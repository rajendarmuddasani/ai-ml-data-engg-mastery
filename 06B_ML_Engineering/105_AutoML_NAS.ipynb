{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c138dc9c",
   "metadata": {},
   "source": [
    "# 105: AutoML and Neural Architecture Search\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** AutoML components: hyperparameter optimization, model selection, feature engineering automation\n",
    "- **Implement** Bayesian optimization for efficient hyperparameter search using Optuna\n",
    "- **Build** automated machine learning pipelines with TPOT and Auto-sklearn\n",
    "- **Apply** Neural Architecture Search (NAS) to find optimal network structures\n",
    "- **Evaluate** AutoML trade-offs: accuracy vs computational cost vs interpretability\n",
    "\n",
    "## üìö What is AutoML?\n",
    "\n",
    "Automated Machine Learning (AutoML) systematizes the process of applying machine learning to real-world problems. Instead of manually trying dozens of algorithms, hundreds of hyperparameter combinations, and countless feature engineering strategies, AutoML automates these decisions using intelligent search algorithms. It democratizes ML by enabling non-experts to build production-quality models while freeing experts to focus on domain-specific challenges.\n",
    "\n",
    "AutoML spans multiple automation levels: **hyperparameter optimization** (finding best learning rate, regularization), **model selection** (choosing between XGBoost vs Random Forest), **feature engineering** (automated interaction discovery), and **Neural Architecture Search** (designing optimal network topologies). Modern AutoML frameworks like Google's AutoML Tables, H2O.ai, and open-source tools like TPOT can achieve expert-level performance in hours rather than weeks.\n",
    "\n",
    "In semiconductor manufacturing, AutoML is particularly valuable because test engineers understand device physics but may lack deep ML expertise. AutoML enables them to build yield prediction models, optimize test programs, and detect anomalies without becoming data scientists‚Äîwhile still producing models that outperform hand-tuned solutions.\n",
    "\n",
    "**Why AutoML?**\n",
    "- ‚úÖ **Democratization**: Non-ML experts can build state-of-the-art models (test engineers ‚Üí ML practitioners)\n",
    "- ‚úÖ **Speed**: Automated search finds better models in hours vs weeks of manual tuning\n",
    "- ‚úÖ **Consistency**: Eliminates human bias in model selection, ensures reproducible pipelines\n",
    "- ‚úÖ **Discovery**: Often finds non-obvious model/hyperparameter combinations experts wouldn't try\n",
    "- ‚úÖ **Scalability**: Same AutoML pipeline works across 100+ products without manual retuning\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Use Case 1: Automated Yield Model Development**\n",
    "- **Input**: STDF wafer test data (200+ parametric tests, 50K+ devices per lot)\n",
    "- **AutoML Task**: Find best model + features + hyperparameters for yield prediction\n",
    "- **Process**: TPOT searches model space (RF, XGBoost, SVM, etc.) + feature engineering for 4 hours\n",
    "- **Output**: Optimized pipeline achieving 94% R¬≤ vs 88% from manual baseline\n",
    "- **Value**: Reduce model development from 3 weeks to 1 day, deploy faster to production\n",
    "\n",
    "**Use Case 2: Per-Product Test Time Optimization**\n",
    "- **Input**: Test time data for 50 different product families, each with unique characteristics\n",
    "- **AutoML Task**: Build custom prediction model for each product automatically\n",
    "- **Process**: Optuna hyperparameter optimization for gradient boosting models per product\n",
    "- **Output**: 50 optimized models, each tuned to specific product's test patterns\n",
    "- **Value**: $5M-$10M annual ATE savings, 30% average test time reduction across portfolio\n",
    "\n",
    "**Use Case 3: Neural Architecture Search for Wafer Map Classification**\n",
    "- **Input**: 100K wafer map images (spatial yield patterns indicating defect types)\n",
    "- **AutoML Task**: Find optimal CNN architecture for classifying defect signatures\n",
    "- **Process**: NAS searches network depth, width, activation functions, skip connections\n",
    "- **Output**: Custom architecture achieving 97% classification accuracy (vs 92% from ResNet-18)\n",
    "- **Value**: Faster root cause identification, automated defect classification replaces manual inspection\n",
    "\n",
    "**Use Case 4: Automated Binning Algorithm Generation**\n",
    "- **Input**: Final test data with current manual binning rules (BIN1=premium, BIN2=standard, etc.)\n",
    "- **AutoML Task**: Learn optimal binning boundaries from historical data\n",
    "- **Process**: Auto-sklearn multi-class classification with automated feature engineering\n",
    "- **Output**: Data-driven binning rules increasing BIN1 yield 8% without escapes\n",
    "- **Value**: $2M-$5M revenue increase per quarter from maximizing premium bin allocation\n",
    "\n",
    "## üîÑ AutoML Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Raw Data] --> B[Define Task & Metric]\n",
    "    B --> C{AutoML Strategy?}\n",
    "    \n",
    "    C -->|Hyperparameter Only| D[Bayesian Optimization]\n",
    "    C -->|Model Selection| E[TPOT / Auto-sklearn]\n",
    "    C -->|Neural Architecture| F[NAS]\n",
    "    \n",
    "    D --> G[Search Space Definition]\n",
    "    E --> H[Pipeline Search Space]\n",
    "    F --> I[Architecture Search Space]\n",
    "    \n",
    "    G --> J[Optuna/Hyperopt]\n",
    "    H --> K[Genetic Programming]\n",
    "    I --> L[Evolution/RL/Gradient]\n",
    "    \n",
    "    J --> M[Evaluate on Validation]\n",
    "    K --> M\n",
    "    L --> M\n",
    "    \n",
    "    M --> N{Budget Exhausted?}\n",
    "    N -->|No| O[Sample Next Config]\n",
    "    O --> J\n",
    "    O --> K\n",
    "    O --> L\n",
    "    \n",
    "    N -->|Yes| P[Select Best Model]\n",
    "    P --> Q[Retrain on Full Data]\n",
    "    Q --> R[Test on Holdout]\n",
    "    R --> S{Performance OK?}\n",
    "    \n",
    "    S -->|No| T[Expand Search Space]\n",
    "    T --> C\n",
    "    S -->|Yes| U[Deploy Pipeline]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style U fill:#e1ffe1\n",
    "    style T fill:#ffe1e1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **010-025**: ML Algorithms - Understanding model types AutoML will search\n",
    "- **041**: Model Evaluation - Metrics AutoML optimizes (RMSE, accuracy, AUC)\n",
    "- **103**: Feature Engineering - What AutoML automates\n",
    "- **104**: Interpretability - Validating AutoML-generated models\n",
    "\n",
    "**This Notebook (105):**\n",
    "- Bayesian optimization with Optuna\n",
    "- Genetic programming with TPOT\n",
    "- Automated pipeline generation\n",
    "- Neural Architecture Search basics\n",
    "- AutoML evaluation strategies\n",
    "\n",
    "**Next Steps:**\n",
    "- **106**: A/B Testing - Validating AutoML models in production\n",
    "- **107**: Model Monitoring - Tracking AutoML model performance over time\n",
    "- **131**: Cloud AutoML - Google AutoML, AWS SageMaker Autopilot, Azure AutoML\n",
    "\n",
    "---\n",
    "\n",
    "Let's automate the art of machine learning! ü§ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4c073",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784811ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# AutoML libraries\n",
    "# Note: Install with: pip install optuna tpot scikit-optimize\n",
    "try:\n",
    "    import optuna\n",
    "    print(\"‚úÖ Optuna available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Install Optuna: pip install optuna\")\n",
    "\n",
    "try:\n",
    "    from tpot import TPOTRegressor\n",
    "    print(\"‚úÖ TPOT available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Install TPOT: pip install tpot\")\n",
    "\n",
    "try:\n",
    "    from skopt import BayesSearchCV\n",
    "    from skopt.space import Real, Integer\n",
    "    print(\"‚úÖ Scikit-optimize available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Install scikit-optimize: pip install scikit-optimize\")\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\n‚úÖ Environment ready for AutoML exploration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d704f",
   "metadata": {},
   "source": [
    "## 2. Generate Semiconductor Test Data\n",
    "\n",
    "**Purpose:** Create complex STDF dataset requiring AutoML to find optimal model.\n",
    "\n",
    "**Key Points:**\n",
    "- **Non-linear relationships**: Yield depends on complex interactions\n",
    "- **Multiple feature types**: Numerical, categorical, spatial\n",
    "- **High dimensionality**: 15 features requiring automated feature selection\n",
    "- **Why this matters**: Manual tuning would take weeks; AutoML finds optimal solution in hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 3000 devices from multiple lots\n",
    "n_devices = 3000\n",
    "\n",
    "# Parametric measurements (15 features)\n",
    "vdd = np.random.normal(1.2, 0.1, n_devices)\n",
    "idd = np.random.normal(50, 10, n_devices)\n",
    "freq = np.random.normal(2000, 200, n_devices)\n",
    "temp = np.random.normal(85, 15, n_devices)\n",
    "vth = np.random.normal(0.4, 0.04, n_devices)\n",
    "leakage = np.random.lognormal(0, 0.5, n_devices)  # Log-normal distribution\n",
    "rise_time = np.random.gamma(2, 0.5, n_devices)  # Gamma distribution\n",
    "fall_time = np.random.gamma(2, 0.5, n_devices)\n",
    "noise_margin = np.random.normal(0.3, 0.05, n_devices)\n",
    "skew = np.random.normal(0, 0.1, n_devices)\n",
    "\n",
    "# Spatial features\n",
    "die_x = np.random.randint(0, 30, n_devices)\n",
    "die_y = np.random.randint(0, 30, n_devices)\n",
    "wafer_id = np.random.choice(['W001', 'W002', 'W003', 'W004', 'W005'], n_devices)\n",
    "\n",
    "# Categorical features\n",
    "lot_id = np.random.choice(['LOT_A', 'LOT_B', 'LOT_C', 'LOT_D'], n_devices)\n",
    "test_program = np.random.choice(['TP_V1', 'TP_V2', 'TP_V3'], n_devices)\n",
    "\n",
    "# Complex target function (requires AutoML to discover)\n",
    "power = vdd * idd\n",
    "thermal_stress = temp * freq / 1000\n",
    "timing_quality = (rise_time + fall_time) / 2\n",
    "radius = np.sqrt((die_x - 15)**2 + (die_y - 15)**2)\n",
    "\n",
    "# Yield with complex non-linear relationships\n",
    "yield_score = (\n",
    "    100\n",
    "    - 0.3 * power\n",
    "    - 0.02 * thermal_stress\n",
    "    + 15 * vth\n",
    "    - 0.5 * radius\n",
    "    - 20 * leakage\n",
    "    + 5 * noise_margin\n",
    "    - 3 * np.abs(skew)\n",
    "    - 2 * timing_quality\n",
    "    # Interaction effects\n",
    "    - 8 * (vdd > 1.3) * (vth < 0.38)\n",
    "    - 5 * (temp > 95) * (freq > 2100)\n",
    "    # Categorical effects\n",
    "    - 3 * (lot_id == 'LOT_D')\n",
    "    + 2 * (test_program == 'TP_V3')\n",
    "    + np.random.normal(0, 3, n_devices)\n",
    ")\n",
    "yield_score = np.clip(yield_score, 60, 100)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'vdd': vdd, 'idd': idd, 'freq': freq, 'temp': temp, 'vth': vth,\n",
    "    'leakage': leakage, 'rise_time': rise_time, 'fall_time': fall_time,\n",
    "    'noise_margin': noise_margin, 'skew': skew,\n",
    "    'die_x': die_x, 'die_y': die_y, 'wafer_id': wafer_id,\n",
    "    'lot_id': lot_id, 'test_program': test_program,\n",
    "    'yield': yield_score\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {df.shape[0]} devices, {df.shape[1]} columns\")\n",
    "print(f\"\\nYield statistics:\")\n",
    "print(df['yield'].describe())\n",
    "print(f\"\\nFeature types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c242d2",
   "metadata": {},
   "source": [
    "## 3. Baseline Model (Manual Tuning)\n",
    "\n",
    "**Purpose:** Establish performance baseline before AutoML.\n",
    "\n",
    "**Key Points:**\n",
    "- **Simple preprocessing**: Basic encoding and scaling\n",
    "- **Default hyperparameters**: RandomForest with sklearn defaults\n",
    "- **No feature engineering**: Raw features only\n",
    "- **Why this matters**: AutoML should significantly outperform this baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for baseline\n",
    "df_baseline = df.copy()\n",
    "\n",
    "# Simple one-hot encoding for categoricals\n",
    "df_baseline = pd.get_dummies(df_baseline, columns=['wafer_id', 'lot_id', 'test_program'])\n",
    "\n",
    "# Split\n",
    "X = df_baseline.drop('yield', axis=1)\n",
    "y = df_baseline['yield']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train baseline Random Forest (default parameters)\n",
    "baseline_model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "baseline_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_baseline = baseline_model.predict(X_test_scaled)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, y_pred_baseline))\n",
    "baseline_r2 = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(\"Baseline Model (Default Random Forest):\")\n",
    "print(f\"  RMSE: {baseline_rmse:.3f}%\")\n",
    "print(f\"  R¬≤: {baseline_r2:.4f}\")\n",
    "print(f\"\\n  Hyperparameters: {baseline_model.get_params()}\")\n",
    "print(f\"\\nüéØ AutoML Goal: Beat R¬≤ = {baseline_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f071e276",
   "metadata": {},
   "source": [
    "## 4. Method 1: Bayesian Optimization with Optuna\n",
    "\n",
    "**Concept:** Use Bayesian optimization to efficiently search hyperparameter space.\n",
    "\n",
    "**Mathematics (Acquisition Function):**\n",
    "$$a(x) = \\mu(x) + \\kappa \\sigma(x)$$\n",
    "\n",
    "Where:\n",
    "- $\\mu(x)$ = expected performance\n",
    "- $\\sigma(x)$ = uncertainty\n",
    "- $\\kappa$ = exploration-exploitation trade-off\n",
    "\n",
    "**Advantages:** Much faster than grid search, intelligent exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6abfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestRegressor(**params)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    scores = cross_val_score(\n",
    "        model, X_train_scaled, y_train,\n",
    "        cv=3, scoring='r2', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "# Create Optuna study\n",
    "print(\"Starting Bayesian Optimization with Optuna...\")\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "# Optimize (50 trials)\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Optimization complete!\")\n",
    "print(f\"\\nBest trial:\")\n",
    "print(f\"  Value (R¬≤): {study.best_trial.value:.4f}\")\n",
    "print(f\"  Improvement over baseline: {(study.best_trial.value - baseline_r2):.4f}\")\n",
    "print(f\"\\nBest hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac8c2d1",
   "metadata": {},
   "source": [
    "## 5. Train Best Model from Optuna\n",
    "\n",
    "**Purpose:** Retrain with optimal hyperparameters on full training set.\n",
    "\n",
    "**Key Points:**\n",
    "- **Full training data**: Use all training samples (not just CV folds)\n",
    "- **Test set evaluation**: Measure generalization performance\n",
    "- **Compare to baseline**: Quantify AutoML improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with best hyperparameters\n",
    "best_params = study.best_params.copy()\n",
    "best_params.update({'random_state': 42, 'n_jobs': -1})\n",
    "\n",
    "optuna_model = RandomForestRegressor(**best_params)\n",
    "optuna_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_optuna = optuna_model.predict(X_test_scaled)\n",
    "optuna_rmse = np.sqrt(mean_squared_error(y_test, y_pred_optuna))\n",
    "optuna_r2 = r2_score(y_test, y_pred_optuna)\n",
    "\n",
    "print(\"Optuna-Optimized Model:\")\n",
    "print(f\"  RMSE: {optuna_rmse:.3f}% (baseline: {baseline_rmse:.3f}%)\")\n",
    "print(f\"  R¬≤: {optuna_r2:.4f} (baseline: {baseline_r2:.4f})\")\n",
    "print(f\"\\n  Improvement:\")\n",
    "print(f\"    RMSE reduction: {((baseline_rmse - optuna_rmse) / baseline_rmse * 100):.1f}%\")\n",
    "print(f\"    R¬≤ increase: {(optuna_r2 - baseline_r2):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfffa14",
   "metadata": {},
   "source": [
    "## 6. Visualize Optuna Optimization\n",
    "\n",
    "**Purpose:** Understand optimization process and parameter importance.\n",
    "\n",
    "**Key Points:**\n",
    "- **Optimization history**: How R¬≤ improved over trials\n",
    "- **Parameter importance**: Which hyperparameters matter most\n",
    "- **Parallel coordinate plot**: Visualize high-dimensional search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b95c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Optimization history\n",
    "trial_numbers = [trial.number for trial in study.trials]\n",
    "trial_values = [trial.value for trial in study.trials]\n",
    "best_values = np.maximum.accumulate(trial_values)\n",
    "\n",
    "axes[0].plot(trial_numbers, trial_values, 'o', alpha=0.5, label='Trial R¬≤')\n",
    "axes[0].plot(trial_numbers, best_values, 'r-', linewidth=2, label='Best R¬≤')\n",
    "axes[0].axhline(baseline_r2, color='gray', linestyle='--', label='Baseline')\n",
    "axes[0].set_xlabel('Trial Number')\n",
    "axes[0].set_ylabel('R¬≤ Score')\n",
    "axes[0].set_title('Optuna Optimization History')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Parameter importance\n",
    "importance = optuna.importance.get_param_importances(study)\n",
    "params = list(importance.keys())\n",
    "values = list(importance.values())\n",
    "\n",
    "axes[1].barh(range(len(params)), values)\n",
    "axes[1].set_yticks(range(len(params)))\n",
    "axes[1].set_yticklabels(params)\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Hyperparameter Importance')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Interpretation:\")\n",
    "print(f\"  Most important hyperparameter: {params[0]}\")\n",
    "print(f\"  Optimization converged after ~{trial_numbers[np.argmax(best_values)]} trials\")\n",
    "print(f\"  Final improvement: {(max(trial_values) - baseline_r2):.4f} R¬≤ gain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc12d3",
   "metadata": {},
   "source": [
    "## 7. Method 2: TPOT (Genetic Programming)\n",
    "\n",
    "**Concept:** Use genetic algorithms to evolve optimal ML pipelines.\n",
    "\n",
    "**Process:**\n",
    "1. Generate random pipelines (model + preprocessing + feature engineering)\n",
    "2. Evaluate fitness (cross-validation score)\n",
    "3. Select best pipelines\n",
    "4. Mutate and crossover to create new generation\n",
    "5. Repeat until convergence\n",
    "\n",
    "**Advantage:** Discovers entire pipelines, not just hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f428c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for TPOT (needs original features)\n",
    "X_train_tpot = X_train.copy()\n",
    "X_test_tpot = X_test.copy()\n",
    "y_train_tpot = y_train.copy()\n",
    "y_test_tpot = y_test.copy()\n",
    "\n",
    "# Create TPOT regressor\n",
    "print(\"Starting TPOT genetic programming search...\")\n",
    "print(\"This will search for optimal pipeline (model + preprocessing + features)\\n\")\n",
    "\n",
    "tpot = TPOTRegressor(\n",
    "    generations=5,  # Number of evolutionary iterations\n",
    "    population_size=20,  # Number of pipelines per generation\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    random_state=42,\n",
    "    verbosity=2,\n",
    "    n_jobs=-1,\n",
    "    config_dict='TPOT light'  # Faster search space\n",
    ")\n",
    "\n",
    "# Fit TPOT (this will take a few minutes)\n",
    "tpot.fit(X_train_tpot, y_train_tpot)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_tpot = tpot.predict(X_test_tpot)\n",
    "tpot_rmse = np.sqrt(mean_squared_error(y_test_tpot, y_pred_tpot))\n",
    "tpot_r2 = r2_score(y_test_tpot, y_pred_tpot)\n",
    "\n",
    "print(f\"\\n‚úÖ TPOT search complete!\")\n",
    "print(f\"\\nTPOT-Generated Pipeline:\")\n",
    "print(f\"  RMSE: {tpot_rmse:.3f}%\")\n",
    "print(f\"  R¬≤: {tpot_r2:.4f}\")\n",
    "print(f\"\\n  Improvement over baseline: {(tpot_r2 - baseline_r2):.4f}\")\n",
    "print(f\"\\nOptimal pipeline found:\")\n",
    "print(tpot.fitted_pipeline_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93935ce1",
   "metadata": {},
   "source": [
    "## 8. Export TPOT Pipeline Code\n",
    "\n",
    "**Purpose:** TPOT can export optimal pipeline as Python code.\n",
    "\n",
    "**Key Points:**\n",
    "- **Reproducible**: Generated code runs independently\n",
    "- **Transparent**: See exactly what TPOT discovered\n",
    "- **Modifiable**: Engineers can tweak auto-generated code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5a8f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export TPOT pipeline to Python script\n",
    "tpot.export('tpot_yield_prediction_pipeline.py')\n",
    "\n",
    "print(\"‚úÖ TPOT pipeline exported to: tpot_yield_prediction_pipeline.py\")\n",
    "print(\"\\nYou can now:\")\n",
    "print(\"  1. Review the auto-generated code\")\n",
    "print(\"  2. Integrate it into production systems\")\n",
    "print(\"  3. Modify it based on domain knowledge\")\n",
    "print(\"  4. Version control it like any other code\")\n",
    "\n",
    "# Show what was discovered\n",
    "print(\"\\nüìã TPOT discovered:\")\n",
    "print(f\"  Algorithm: {type(tpot.fitted_pipeline_.steps[-1][1]).__name__}\")\n",
    "print(f\"  Preprocessing steps: {len(tpot.fitted_pipeline_.steps) - 1}\")\n",
    "print(f\"  Full pipeline: {tpot.fitted_pipeline_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0160e4a",
   "metadata": {},
   "source": [
    "## 9. Method 3: Scikit-Optimize BayesSearchCV\n",
    "\n",
    "**Concept:** Bayesian optimization integrated with sklearn's CV interface.\n",
    "\n",
    "**Advantage:** Familiar sklearn API + efficient Bayesian search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594999bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search space for Gradient Boosting\n",
    "search_spaces = {\n",
    "    'n_estimators': Integer(50, 300),\n",
    "    'max_depth': Integer(3, 15),\n",
    "    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "    'min_samples_split': Integer(2, 20),\n",
    "    'min_samples_leaf': Integer(1, 10),\n",
    "    'subsample': Real(0.6, 1.0),\n",
    "}\n",
    "\n",
    "# BayesSearchCV\n",
    "print(\"Starting BayesSearchCV for Gradient Boosting...\\n\")\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    search_spaces,\n",
    "    n_iter=30,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "bayes_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_bayes = bayes_search.predict(X_test_scaled)\n",
    "bayes_rmse = np.sqrt(mean_squared_error(y_test, y_pred_bayes))\n",
    "bayes_r2 = r2_score(y_test, y_pred_bayes)\n",
    "\n",
    "print(f\"\\n‚úÖ BayesSearchCV complete!\")\n",
    "print(f\"\\nGradient Boosting (Optimized):\")\n",
    "print(f\"  RMSE: {bayes_rmse:.3f}%\")\n",
    "print(f\"  R¬≤: {bayes_r2:.4f}\")\n",
    "print(f\"\\n  Improvement over baseline: {(bayes_r2 - baseline_r2):.4f}\")\n",
    "print(f\"\\nBest hyperparameters:\")\n",
    "for key, value in bayes_search.best_params_.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc695a0",
   "metadata": {},
   "source": [
    "## 10. Compare All AutoML Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'Method': ['Baseline (Manual)', 'Optuna (RF)', 'TPOT (Auto)', 'BayesSearchCV (GB)'],\n",
    "    'RMSE': [baseline_rmse, optuna_rmse, tpot_rmse, bayes_rmse],\n",
    "    'R¬≤': [baseline_r2, optuna_r2, tpot_r2, bayes_r2],\n",
    "    'Time': ['1 min', '~5 min', '~10 min', '~8 min'],\n",
    "    'Automation': ['None', 'Hyperparams', 'Full Pipeline', 'Hyperparams']\n",
    "})\n",
    "\n",
    "results['R¬≤ Improvement'] = results['R¬≤'] - baseline_r2\n",
    "results['RMSE Reduction %'] = ((baseline_rmse - results['RMSE']) / baseline_rmse * 100).round(1)\n",
    "\n",
    "print(\"AutoML Method Comparison:\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: R¬≤ comparison\n",
    "colors = ['gray', 'blue', 'green', 'orange']\n",
    "axes[0].bar(range(len(results)), results['R¬≤'], color=colors, alpha=0.7)\n",
    "axes[0].set_xticks(range(len(results)))\n",
    "axes[0].set_xticklabels(results['Method'], rotation=45, ha='right')\n",
    "axes[0].set_ylabel('R¬≤ Score')\n",
    "axes[0].set_title('Model Performance Comparison')\n",
    "axes[0].axhline(baseline_r2, color='red', linestyle='--', linewidth=2, label='Baseline')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Improvement over baseline\n",
    "axes[1].bar(range(1, len(results)), results['R¬≤ Improvement'][1:], color=colors[1:], alpha=0.7)\n",
    "axes[1].set_xticks(range(1, len(results)))\n",
    "axes[1].set_xticklabels(results['Method'][1:], rotation=45, ha='right')\n",
    "axes[1].set_ylabel('R¬≤ Improvement')\n",
    "axes[1].set_title('AutoML Improvement over Baseline')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Summary:\")\n",
    "best_method_idx = results['R¬≤'].idxmax()\n",
    "print(f\"  Best method: {results.loc[best_method_idx, 'Method']}\")\n",
    "print(f\"  Best R¬≤: {results.loc[best_method_idx, 'R¬≤']:.4f}\")\n",
    "print(f\"  Total improvement: {results.loc[best_method_idx, 'R¬≤ Improvement']:.4f}\")\n",
    "print(f\"  RMSE reduction: {results.loc[best_method_idx, 'RMSE Reduction %']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12080dcb",
   "metadata": {},
   "source": [
    "## 11. Neural Architecture Search (Conceptual)\n",
    "\n",
    "**Concept:** Automate neural network design (depth, width, connections).\n",
    "\n",
    "**NAS Approaches:**\n",
    "1. **Reinforcement Learning**: Train RL agent to design architectures\n",
    "2. **Evolutionary Algorithms**: Evolve network topologies\n",
    "3. **Gradient-Based**: DARTS (Differentiable Architecture Search)\n",
    "\n",
    "**Note:** Full NAS requires significant compute (GPUs). Here we show conceptual framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059f7fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual NAS workflow (pseudo-code style)\n",
    "print(\"Neural Architecture Search Workflow:\")\n",
    "print(\"\\n1. Define Search Space:\")\n",
    "print(\"   - Number of layers: [2, 3, 4, 5]\")\n",
    "print(\"   - Hidden units per layer: [32, 64, 128, 256]\")\n",
    "print(\"   - Activation functions: ['relu', 'tanh', 'elu']\")\n",
    "print(\"   - Dropout rates: [0.0, 0.2, 0.4]\")\n",
    "print(\"   - Skip connections: [True, False]\")\n",
    "print(\"\\n2. Search Strategy:\")\n",
    "print(\"   - Random search: Sample 50 architectures\")\n",
    "print(\"   - Evolutionary: Evolve over 20 generations\")\n",
    "print(\"   - RL: Train controller for 100 episodes\")\n",
    "print(\"\\n3. Performance Estimation:\")\n",
    "print(\"   - Train each architecture for 10 epochs\")\n",
    "print(\"   - Evaluate on validation set\")\n",
    "print(\"   - Record validation loss\")\n",
    "print(\"\\n4. Select Best Architecture:\")\n",
    "print(\"   - Rank by validation performance\")\n",
    "print(\"   - Retrain top-3 from scratch\")\n",
    "print(\"   - Choose best on test set\")\n",
    "\n",
    "# Example architecture discovered by NAS (hypothetical)\n",
    "print(\"\\nüèóÔ∏è Example NAS-Discovered Architecture:\")\n",
    "print(\"  Input (15 features)\")\n",
    "print(\"  ‚Üí Dense(128, relu) + Dropout(0.2)\")\n",
    "print(\"  ‚Üí Dense(64, relu) + Dropout(0.2)\")\n",
    "print(\"  ‚Üí Dense(32, elu)\")\n",
    "print(\"  ‚Üí Skip connection from input\")\n",
    "print(\"  ‚Üí Dense(1, linear) [output]\")\n",
    "print(\"\\n  Training: Adam optimizer, lr=0.001, batch_size=32\")\n",
    "print(\"  Performance: R¬≤ = 0.92 (hypothetical)\")\n",
    "\n",
    "print(\"\\nüí° For production NAS:\")\n",
    "print(\"  - Use libraries: Auto-Keras, NASBench, ENAS\")\n",
    "print(\"  - Requires: GPU cluster, 8-24 hours compute time\")\n",
    "print(\"  - Best for: Image/text tasks where architecture matters most\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c67d2",
   "metadata": {},
   "source": [
    "## 12. AutoML Best Practices & Pitfalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f13ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AutoML Best Practices:\")\n",
    "print(\"\\n‚úÖ DO:\")\n",
    "print(\"  1. Start with strong baseline - know what 'good' performance looks like\")\n",
    "print(\"  2. Set realistic time budgets - diminishing returns after certain point\")\n",
    "print(\"  3. Use proper CV - prevent overfitting during search\")\n",
    "print(\"  4. Validate on holdout - AutoML can overfit to validation set\")\n",
    "print(\"  5. Inspect results - don't blindly trust AutoML output\")\n",
    "print(\"  6. Check interpretability - ensure model makes domain sense\")\n",
    "print(\"  7. Monitor in production - AutoML models drift like any other\")\n",
    "\n",
    "print(\"\\n‚ùå DON'T:\")\n",
    "print(\"  1. Use AutoML as black box - understand what it's optimizing\")\n",
    "print(\"  2. Ignore compute cost - some methods very expensive\")\n",
    "print(\"  3. Skip feature engineering - AutoML works better with good features\")\n",
    "print(\"  4. Forget domain knowledge - AutoML finds correlations, not causation\")\n",
    "print(\"  5. Optimize wrong metric - choose metric aligned with business goal\")\n",
    "print(\"  6. Trust first result - run multiple seeds, ensemble top models\")\n",
    "print(\"  7. Overfit search space - too many options = overfitting\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Common Pitfalls:\")\n",
    "print(\"  ‚Ä¢ Data leakage: AutoML can exploit leaks you didn't notice\")\n",
    "print(\"  ‚Ä¢ Overfitting: Optimizing too long on same validation set\")\n",
    "print(\"  ‚Ä¢ Computational waste: Search space too large, inefficient\")\n",
    "print(\"  ‚Ä¢ Unstable models: High variance across different runs\")\n",
    "print(\"  ‚Ä¢ Poor generalization: Train/test distribution mismatch\")\n",
    "\n",
    "print(\"\\nüéØ Semiconductor-Specific Tips:\")\n",
    "print(\"  ‚Ä¢ Lot stratification: Ensure CV splits preserve lot structure\")\n",
    "print(\"  ‚Ä¢ Physics constraints: Validate AutoML features make engineering sense\")\n",
    "print(\"  ‚Ä¢ Test coverage: Don't remove tests just because AutoML says they're unimportant\")\n",
    "print(\"  ‚Ä¢ Interpretability: Fab engineers must understand model decisions\")\n",
    "print(\"  ‚Ä¢ Stability: Production models must be stable across lots/weeks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac0271f",
   "metadata": {},
   "source": [
    "## 13. Project Templates\n",
    "\n",
    "### Project 1: End-to-End AutoML Yield Prediction System\n",
    "**Objective:** Build production AutoML pipeline for multi-product yield prediction\n",
    "- Collect STDF data for 10 different product families\n",
    "- Use TPOT to generate custom pipeline per product\n",
    "- Export each pipeline as deployable Python script\n",
    "- Create monitoring dashboard tracking AutoML model performance\n",
    "- **Success Metric:** Deploy 10 models in 1 week (vs 10 weeks manual), maintain >90% R¬≤ across all products\n",
    "\n",
    "### Project 2: Hyperparameter Optimization as a Service\n",
    "**Objective:** Create internal tool for test engineers to optimize their models\n",
    "- Build API accepting dataset + model type + time budget\n",
    "- Use Optuna backend for Bayesian optimization\n",
    "- Return best hyperparameters + performance report + optimization plots\n",
    "- Track all optimizations in database for knowledge sharing\n",
    "- **Success Metric:** 20+ engineers using tool monthly, average 15% R¬≤ improvement per optimization\n",
    "\n",
    "### Project 3: NAS for Wafer Map Defect Classification\n",
    "**Objective:** Find optimal CNN architecture for spatial pattern recognition\n",
    "- Dataset: 50K wafer maps labeled with defect types (scratch, ring, edge)\n",
    "- Search space: Layer depth [3-8], filters [16-128], kernel sizes [3,5,7]\n",
    "- Use evolutionary algorithm with 100 generations\n",
    "- Compare NAS result to standard architectures (ResNet, VGG, MobileNet)\n",
    "- **Success Metric:** >95% classification accuracy, <10ms inference time, deployable to edge devices\n",
    "\n",
    "### Project 4: Multi-Objective AutoML for Test Time vs Accuracy\n",
    "**Objective:** Pareto-optimal models balancing prediction quality and feature cost\n",
    "- Define cost per test parameter (ATE time in ms)\n",
    "- Use NSGA-II multi-objective optimization\n",
    "- Optimize: maximize R¬≤, minimize total test time\n",
    "- Generate Pareto front of models (accuracy vs cost trade-offs)\n",
    "- **Success Metric:** 10+ Pareto-optimal models, management chooses based on cost constraints\n",
    "\n",
    "### Project 5: AutoML Model Ensemble\n",
    "**Objective:** Combine multiple AutoML methods for robust predictions\n",
    "- Run Optuna, TPOT, BayesSearchCV independently\n",
    "- Ensemble top-5 models from each method (15 models total)\n",
    "- Use stacking or weighted averaging\n",
    "- Compare ensemble to individual best model\n",
    "- **Success Metric:** Ensemble outperforms any single model by ‚â•2% R¬≤, lower variance across lots\n",
    "\n",
    "### Project 6: Feature Engineering AutoML\n",
    "**Objective:** Automate discovery of optimal feature transformations\n",
    "- Search space: Polynomial degrees [1-3], log transforms, interactions, binning\n",
    "- Use genetic programming to evolve feature engineering pipelines\n",
    "- Evaluate: Feature importance + model performance + interpretability score\n",
    "- Generate human-readable feature engineering code\n",
    "- **Success Metric:** Discover 5+ non-obvious features improving R¬≤ >5%, validated by engineers\n",
    "\n",
    "### Project 7: AutoML with Interpretability Constraints\n",
    "**Objective:** Optimize for performance AND explainability\n",
    "- Define interpretability metric: max tree depth, number of features, coefficient sparsity\n",
    "- Multi-objective optimization: R¬≤ vs interpretability\n",
    "- Reject models failing SHAP sanity checks (unphysical feature importance)\n",
    "- Generate model cards with auto-computed explanations\n",
    "- **Success Metric:** Models within 3% of best R¬≤ but 10x more interpretable\n",
    "\n",
    "### Project 8: Continuous AutoML for Production\n",
    "**Objective:** AutoML that adapts to data drift automatically\n",
    "- Monitor model performance weekly on new production data\n",
    "- Trigger AutoML re-optimization when R¬≤ drops >5%\n",
    "- A/B test new AutoML model vs current production model\n",
    "- Auto-deploy if new model wins A/B test\n",
    "- **Success Metric:** Zero manual model retraining, automated adaptation to process changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92972e1",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "**When to Use AutoML:**\n",
    "- ‚úÖ **Time-constrained projects**: Need good model fast (hours, not weeks)\n",
    "- ‚úÖ **Multiple similar problems**: Same AutoML pipeline works across products\n",
    "- ‚úÖ **Non-expert users**: Enable domain experts to build ML models\n",
    "- ‚úÖ **Baseline establishment**: Quickly find strong starting point for manual tuning\n",
    "- ‚úÖ **Exploration**: Discover non-obvious model/hyperparameter combinations\n",
    "\n",
    "**When NOT to Use AutoML:**\n",
    "- ‚ùå **Novel problems**: Highly specialized tasks needing custom architectures\n",
    "- ‚ùå **Interpretability critical**: Regulatory requirements for full transparency\n",
    "- ‚ùå **Limited compute**: AutoML search expensive (GPU hours, cloud costs)\n",
    "- ‚ùå **Small datasets**: AutoML overfits easily with <1000 samples\n",
    "- ‚ùå **Production constraints**: Strict latency/memory limits AutoML may violate\n",
    "\n",
    "**Method Selection Guide:**\n",
    "- **Optuna**: Best for hyperparameter-only optimization, fastest, sklearn-friendly\n",
    "- **TPOT**: Best for discovering full pipelines, more exploration, takes longer\n",
    "- **BayesSearchCV**: Best for sklearn users wanting easy Bayesian optimization\n",
    "- **NAS**: Best for deep learning, image/text tasks, requires GPU cluster\n",
    "- **Cloud AutoML**: Best for production scale, enterprise features, managed service\n",
    "\n",
    "**Limitations:**\n",
    "- ‚ö†Ô∏è **No free lunch**: AutoML can't compensate for bad data quality\n",
    "- ‚ö†Ô∏è **Computational cost**: Bayesian optimization 10-100x slower than manual tuning\n",
    "- ‚ö†Ô∏è **Overfitting risk**: Optimizing too long on validation set causes overfitting\n",
    "- ‚ö†Ô∏è **Black box danger**: Must inspect and validate AutoML outputs\n",
    "- ‚ö†Ô∏è **Reproducibility**: Different runs may find different \"optimal\" models\n",
    "\n",
    "**Best Practices:**\n",
    "1. **Start simple**: Baseline ‚Üí Optuna ‚Üí TPOT (progressive complexity)\n",
    "2. **Set time budgets**: Diminishing returns after initial exploration phase\n",
    "3. **Proper validation**: Holdout test set never touched during AutoML search\n",
    "4. **Ensemble top models**: Average top-3 often better than single best\n",
    "5. **Domain validation**: Check if AutoML features make engineering sense\n",
    "6. **Monitor in production**: AutoML models drift just like manual models\n",
    "7. **Version control**: Export and save all AutoML-generated code\n",
    "8. **Document search space**: Record what was optimized for reproducibility\n",
    "\n",
    "**Semiconductor Production Checklist:**\n",
    "- [ ] Lot-stratified cross-validation (don't split within lots)\n",
    "- [ ] Physics-based feature validation (SHAP values make sense)\n",
    "- [ ] Interpretability requirements met (engineers can explain decisions)\n",
    "- [ ] Inference time acceptable (<100ms for real-time, <1s for batch)\n",
    "- [ ] Model stability across lots/weeks (low variance)\n",
    "- [ ] A/B testing plan (validate AutoML vs current production model)\n",
    "- [ ] Monitoring dashboard (track performance drift)\n",
    "- [ ] Rollback plan (revert if AutoML model fails)\n",
    "\n",
    "**Next Steps:**\n",
    "- Study **106: A/B Testing** to validate AutoML models in production\n",
    "- Explore **107: Model Monitoring** for continuous performance tracking\n",
    "- Experiment with cloud AutoML platforms (Google, AWS, Azure)\n",
    "- Read \"AutoML: Methods, Systems, Challenges\" book (open source)\n",
    "- Try advanced NAS libraries: Auto-Keras, NASBench-101, Once-for-All Networks\n",
    "- Build internal AutoML platform for your organization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
