{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164f99bd",
   "metadata": {},
   "source": [
    "# 104: Model Interpretability & Explainability\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** the difference between interpretability (global understanding) and explainability (local predictions)\n",
    "- **Implement** SHAP (SHapley Additive exPlanations) values for model-agnostic explanations\n",
    "- **Build** LIME (Local Interpretable Model-agnostic Explanations) for individual prediction analysis\n",
    "- **Apply** interpretability techniques to semiconductor yield prediction models\n",
    "- **Evaluate** feature importance using multiple methods (permutation, tree-based, SHAP)\n",
    "\n",
    "## ðŸ“š What is Model Interpretability?\n",
    "\n",
    "Model interpretability is the ability to understand and explain how a machine learning model makes decisions. In regulated industries like semiconductor manufacturing, healthcare, and finance, interpretability isn't optionalâ€”it's mandatory. A black-box model that predicts 92% yield but can't explain why is often less valuable than a 88% accurate model that clearly identifies root causes.\n",
    "\n",
    "**Interpretability** answers \"How does the model work globally?\" while **Explainability** answers \"Why did the model make this specific prediction?\" For example, knowing that voltage is the most important feature overall (interpretability) helps design better tests, while knowing that Device #47251 failed because its Vdd was 3.2Ïƒ above spec (explainability) enables targeted debug.\n",
    "\n",
    "Modern interpretability techniques like SHAP and LIME work with any modelâ€”from simple linear regression to complex neural networks. They transform opaque algorithms into transparent decision systems that engineers can validate, debug, and trust in production environments where mistakes cost millions.\n",
    "\n",
    "**Why Model Interpretability?**\n",
    "- âœ… **Regulatory Compliance**: FDA, automotive (ISO 26262), and finance require explainable AI for safety-critical decisions\n",
    "- âœ… **Debug Model Failures**: Identify when models rely on spurious correlations or data leakage instead of real physics\n",
    "- âœ… **Build Trust**: Engineers accept ML predictions when they understand the reasoning (\"Yes, Vdd is too high\")\n",
    "- âœ… **Feature Engineering**: Discover which engineered features actually help vs add noise\n",
    "- âœ… **Root Cause Analysis**: Pinpoint why specific devices fail (\"Lot C fails due to high temperature + low Vth combination\")\n",
    "\n",
    "## ðŸ­ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Use Case 1: Yield Loss Root Cause Identification**\n",
    "- **Input**: Failed devices from wafer test with 150+ parametric measurements\n",
    "- **Model Output**: Binary classification (pass/fail) with 94% accuracy\n",
    "- **Explainability**: SHAP values identify top 3 contributing parameters per failed die\n",
    "- **Value**: Reduces root cause analysis time from 2 weeks to 2 days, $500K-$2M savings per yield excursion\n",
    "\n",
    "**Use Case 2: Outlier Device Investigation**\n",
    "- **Input**: Device #X has abnormal power consumption (2.5Ïƒ above mean)\n",
    "- **Model Output**: Power prediction model with 95% RÂ²\n",
    "- **Explainability**: LIME shows Idd (leakage current) contributed +18mW, Freq contributed +7mW\n",
    "- **Value**: Identifies design vs manufacturing issues, prevents false RMAs, saves $50K per misdiagnosed unit\n",
    "\n",
    "**Use Case 3: Test Program Optimization Justification**\n",
    "- **Input**: Random Forest model predicting bin outcome from 500 parametric tests\n",
    "- **Model Output**: 97% bin prediction accuracy\n",
    "- **Explainability**: Permutation importance shows 200 tests have zero impact on predictions\n",
    "- **Value**: Safely remove redundant tests, reduce test time 35%, $3M-$7M annual ATE savings with proof\n",
    "\n",
    "**Use Case 4: Adaptive Test Insertion Confidence**\n",
    "- **Input**: Real-time wafer test results, model predicts next test's outcome\n",
    "- **Model Output**: Probabilistic prediction (85% likely to pass AC parametric tests)\n",
    "- **Explainability**: SHAP force plot shows DC tests already indicate marginal performance\n",
    "- **Value**: Insert additional tests only for high-risk devices, balance coverage vs cost with engineering confidence\n",
    "\n",
    "## ðŸ”„ Model Interpretability Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Trained Model] --> B{Interpretation Goal?}\n",
    "    \n",
    "    B -->|Global Understanding| C[Feature Importance]\n",
    "    B -->|Local Explanation| D[Single Prediction]\n",
    "    \n",
    "    C --> E[Permutation Importance]\n",
    "    C --> F[SHAP Summary Plot]\n",
    "    C --> G[Partial Dependence]\n",
    "    \n",
    "    D --> H[SHAP Force Plot]\n",
    "    D --> I[LIME Explanation]\n",
    "    D --> J[ICE Curves]\n",
    "    \n",
    "    E --> K[Validate with Domain Expert]\n",
    "    F --> K\n",
    "    G --> K\n",
    "    H --> L[Debug Prediction]\n",
    "    I --> L\n",
    "    J --> L\n",
    "    \n",
    "    K --> M{Makes Sense?}\n",
    "    L --> N{Trustworthy?}\n",
    "    \n",
    "    M -->|No| O[Refine Model]\n",
    "    M -->|Yes| P[Deploy with Confidence]\n",
    "    N -->|No| O\n",
    "    N -->|Yes| P\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style P fill:#e1ffe1\n",
    "    style O fill:#ffe1e1\n",
    "```\n",
    "\n",
    "## ðŸ“Š Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **010**: Linear Regression - Coefficient interpretation\n",
    "- **016**: Decision Trees - Tree-based feature importance\n",
    "- **022**: Random Forest - Ensemble feature importance\n",
    "- **103**: Advanced Feature Engineering - Understanding engineered features\n",
    "\n",
    "**This Notebook (104):**\n",
    "- Permutation importance (model-agnostic)\n",
    "- SHAP values (Shapley values from game theory)\n",
    "- LIME (local linear approximations)\n",
    "- Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE)\n",
    "- Feature importance validation and comparison\n",
    "\n",
    "**Next Steps:**\n",
    "- **105**: AutoML & NAS - Automated model selection with interpretability constraints\n",
    "- **107**: Model Monitoring - Detecting when feature importance shifts in production\n",
    "- **054**: Advanced Deep Learning - Attention mechanisms and neural network interpretability\n",
    "\n",
    "---\n",
    "\n",
    "Let's make black-box models transparent and trustworthy! ðŸ”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07223b6",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e340bc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Interpretability libraries\n",
    "# Note: Install with: pip install shap lime\n",
    "try:\n",
    "    import shap\n",
    "    print(\"âœ… SHAP available\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  Install SHAP: pip install shap\")\n",
    "\n",
    "try:\n",
    "    import lime\n",
    "    from lime import lime_tabular\n",
    "    print(\"âœ… LIME available\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  Install LIME: pip install lime\")\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\\nâœ… Environment ready for model interpretability analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090b5fee",
   "metadata": {},
   "source": [
    "## 2. Generate Semiconductor Test Data\n",
    "\n",
    "**Purpose:** Create synthetic STDF parametric test data with known feature-target relationships.\n",
    "\n",
    "**Key Points:**\n",
    "- **Non-linear relationships**: Yield depends on power (vddÃ—idd) and thermal stress (tempÃ—freq)\n",
    "- **Interaction effects**: High Vdd + low Vth = increased leakage, lower yield\n",
    "- **Spatial patterns**: Die position affects yield (wafer edge effect)\n",
    "- **Why this matters**: We know ground truth, can validate if interpretability methods recover it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f725c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2000 devices from wafer test\n",
    "n_devices = 2000\n",
    "\n",
    "# Parametric measurements (STDF-like)\n",
    "vdd = np.random.normal(1.2, 0.08, n_devices)   # Supply voltage (V)\n",
    "idd = np.random.normal(50, 8, n_devices)       # Supply current (mA)\n",
    "freq = np.random.normal(2000, 150, n_devices)  # Max frequency (MHz)\n",
    "temp = np.random.normal(85, 12, n_devices)     # Junction temp (Â°C)\n",
    "vth = np.random.normal(0.4, 0.03, n_devices)   # Threshold voltage (V)\n",
    "\n",
    "# Spatial coordinates\n",
    "die_x = np.random.randint(0, 25, n_devices)\n",
    "die_y = np.random.randint(0, 25, n_devices)\n",
    "radius = np.sqrt((die_x - 12)**2 + (die_y - 12)**2)  # Distance from center\n",
    "\n",
    "# Ground truth relationships (what we want interpretability to discover)\n",
    "power = vdd * idd  # Physical law: P = V Ã— I\n",
    "thermal_stress = temp * freq / 1000  # Thermal stress index\n",
    "overdrive = vdd - vth  # Overdrive voltage affects speed\n",
    "\n",
    "# Target: Yield score (influenced by multiple factors)\n",
    "yield_score = (\n",
    "    100  # Base yield\n",
    "    - 0.4 * power  # High power reduces yield\n",
    "    - 0.015 * thermal_stress  # Thermal stress degrades reliability\n",
    "    + 8 * vth  # Higher Vth = better reliability\n",
    "    - 0.3 * radius  # Edge dies have lower yield\n",
    "    - 5 * (vdd > 1.3) * (vth < 0.38)  # Interaction: high V + low Vth = leakage\n",
    "    + np.random.normal(0, 2.5, n_devices)  # Measurement noise\n",
    ")\n",
    "yield_score = np.clip(yield_score, 65, 100)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'vdd': vdd,\n",
    "    'idd': idd,\n",
    "    'freq': freq,\n",
    "    'temp': temp,\n",
    "    'vth': vth,\n",
    "    'die_x': die_x,\n",
    "    'die_y': die_y,\n",
    "    'radius': radius,\n",
    "    'yield': yield_score\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {df.shape[0]} devices, {df.shape[1]} columns\")\n",
    "print(f\"\\nYield statistics:\")\n",
    "print(df['yield'].describe())\n",
    "print(f\"\\nFeature correlations with yield:\")\n",
    "print(df.corr()['yield'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9355804f",
   "metadata": {},
   "source": [
    "## 3. Train Random Forest Model\n",
    "\n",
    "**Purpose:** Build a non-linear model that we'll interpret using multiple techniques.\n",
    "\n",
    "**Key Points:**\n",
    "- **Random Forest**: Complex ensemble model (not easily interpretable like linear regression)\n",
    "- **High accuracy**: Should achieve RÂ² > 0.85 given strong signal in data\n",
    "- **Black box problem**: 100 trees Ã— 20 max depth = impossible to manually interpret\n",
    "- **Why this matters**: Real production models are complex; need interpretability tools to understand them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53996e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features (exclude radius as it's derived)\n",
    "feature_cols = ['vdd', 'idd', 'freq', 'temp', 'vth', 'die_x', 'die_y']\n",
    "X = df[feature_cols]\n",
    "y = df['yield']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_train = rf_model.predict(X_train)\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(\"Random Forest Model Performance:\")\n",
    "print(f\"  Train RÂ²: {train_r2:.4f}\")\n",
    "print(f\"  Test RÂ²: {test_r2:.4f}\")\n",
    "print(f\"  Test RMSE: {test_rmse:.2f}%\")\n",
    "print(f\"\\n  Model Complexity: {rf_model.n_estimators} trees Ã— max depth {rf_model.max_depth}\")\n",
    "print(f\"  Total leaf nodes: ~{rf_model.n_estimators * (2**rf_model.max_depth)} (impossible to interpret manually!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164eb9c5",
   "metadata": {},
   "source": [
    "## 4. Method 1: Tree-Based Feature Importance (Built-in)\n",
    "\n",
    "**Concept:** Average impurity decrease when splitting on each feature across all trees.\n",
    "\n",
    "**Mathematics:**\n",
    "$$\\text{Importance}(f) = \\frac{1}{N_{trees}} \\sum_{t=1}^{N_{trees}} \\sum_{nodes\\ using\\ f} \\Delta \\text{Impurity}$$\n",
    "\n",
    "**Pros:** Fast, built into sklearn  \n",
    "**Cons:** Biased toward high-cardinality features, unreliable with correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract built-in feature importance\n",
    "tree_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Tree-Based Feature Importance (Gini Impurity):\")\n",
    "print(tree_importance.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(range(len(tree_importance)), tree_importance['importance'])\n",
    "plt.yticks(range(len(tree_importance)), tree_importance['feature'])\n",
    "plt.xlabel('Gini Importance')\n",
    "plt.title('Random Forest Built-in Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Interpretation:\")\n",
    "print(f\"  Top feature: {tree_importance.iloc[0]['feature']} (importance = {tree_importance.iloc[0]['importance']:.3f})\")\n",
    "print(f\"  Model relies most on {tree_importance.iloc[0]['feature']} for split decisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9f53f2",
   "metadata": {},
   "source": [
    "## 5. Method 2: Permutation Importance (Model-Agnostic)\n",
    "\n",
    "**Concept:** Shuffle each feature and measure performance drop. Larger drop = more important.\n",
    "\n",
    "**Mathematics:**\n",
    "$$\\text{Importance}(f) = \\text{Score}_{original} - \\text{Score}_{f\\ permuted}$$\n",
    "\n",
    "**Pros:** Unbiased, works with any model  \n",
    "**Cons:** Slower (requires retraining), can be unstable with correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b43924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute permutation importance on test set\n",
    "perm_importance = permutation_importance(\n",
    "    rf_model, X_test, y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance_mean': perm_importance.importances_mean,\n",
    "    'importance_std': perm_importance.importances_std\n",
    "}).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "print(\"Permutation Importance (RÂ² drop when shuffled):\")\n",
    "print(perm_importance_df.to_string(index=False))\n",
    "\n",
    "# Visualize with error bars\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(\n",
    "    range(len(perm_importance_df)),\n",
    "    perm_importance_df['importance_mean'],\n",
    "    xerr=perm_importance_df['importance_std'],\n",
    "    capsize=5\n",
    ")\n",
    "plt.yticks(range(len(perm_importance_df)), perm_importance_df['feature'])\n",
    "plt.xlabel('Importance (RÂ² Drop)')\n",
    "plt.title('Permutation Importance (10 repeats)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Interpretation:\")\n",
    "print(f\"  If we randomize {perm_importance_df.iloc[0]['feature']}, RÂ² drops by {perm_importance_df.iloc[0]['importance_mean']:.4f}\")\n",
    "print(f\"  This is the most impactful feature for model performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836031d",
   "metadata": {},
   "source": [
    "## 6. Method 3: SHAP Values (Shapley Additive Explanations)\n",
    "\n",
    "**Concept:** Game theory approachâ€”compute each feature's marginal contribution to prediction.\n",
    "\n",
    "**Mathematics (Shapley Value):**\n",
    "$$\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|!(|N|-|S|-1)!}{|N|!} [f(S \\cup \\{i\\}) - f(S)]$$\n",
    "\n",
    "**Properties:**\n",
    "- **Additivity**: Sum of SHAP values = prediction - baseline\n",
    "- **Consistency**: If feature helps more, SHAP value increases\n",
    "- **Local accuracy**: Exact attribution for each prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e4c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer (TreeExplainer for tree models)\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "\n",
    "# Calculate SHAP values for test set (use sample for speed)\n",
    "X_test_sample = X_test.sample(n=min(500, len(X_test)), random_state=42)\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "print(f\"SHAP values computed for {X_test_sample.shape[0]} test samples\")\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "print(f\"\\nBase value (expected model output): {explainer.expected_value:.2f}%\")\n",
    "print(f\"Actual predictions range: {rf_model.predict(X_test_sample).min():.2f}% to {rf_model.predict(X_test_sample).max():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36740cc0",
   "metadata": {},
   "source": [
    "## 7. SHAP Summary Plot (Global Importance)\n",
    "\n",
    "**Purpose:** Visualize feature importance AND feature effects.\n",
    "\n",
    "**Key Points:**\n",
    "- **X-axis**: SHAP value (impact on prediction)\n",
    "- **Y-axis**: Features ranked by importance\n",
    "- **Color**: Feature value (red = high, blue = low)\n",
    "- **Why powerful**: Shows not just importance, but also direction of effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.summary_plot(shap_values, X_test_sample, show=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate mean absolute SHAP values (global importance)\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'mean_abs_shap': np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values('mean_abs_shap', ascending=False)\n",
    "\n",
    "print(\"\\nSHAP Global Importance (mean |SHAP value|):\")\n",
    "print(shap_importance.to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ“Š Interpretation:\")\n",
    "print(f\"  {shap_importance.iloc[0]['feature']} has highest average impact: {shap_importance.iloc[0]['mean_abs_shap']:.3f}\")\n",
    "print(f\"  Red dots (high {shap_importance.iloc[0]['feature']}) push predictions up/down\")\n",
    "print(f\"  Blue dots (low {shap_importance.iloc[0]['feature']}) have opposite effect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee55412",
   "metadata": {},
   "source": [
    "## 8. SHAP Force Plot (Single Prediction Explanation)\n",
    "\n",
    "**Purpose:** Explain one specific predictionâ€”why did Device #X get this yield score?\n",
    "\n",
    "**Key Points:**\n",
    "- **Base value**: Average model prediction (baseline)\n",
    "- **Red arrows**: Features pushing prediction higher\n",
    "- **Blue arrows**: Features pushing prediction lower\n",
    "- **Final value**: Actual prediction = base + sum of SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22adde5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an interesting device to explain (low yield)\n",
    "sample_idx = X_test_sample['vdd'].idxmax()  # Device with highest Vdd\n",
    "sample_device = X_test_sample.loc[sample_idx]\n",
    "sample_prediction = rf_model.predict(sample_device.values.reshape(1, -1))[0]\n",
    "sample_shap = shap_values[X_test_sample.index.get_loc(sample_idx)]\n",
    "\n",
    "print(f\"Explaining Device #{sample_idx}:\")\n",
    "print(f\"\\nFeature values:\")\n",
    "for feat, val in sample_device.items():\n",
    "    print(f\"  {feat}: {val:.3f}\")\n",
    "print(f\"\\nPredicted yield: {sample_prediction:.2f}%\")\n",
    "print(f\"Baseline (average): {explainer.expected_value:.2f}%\")\n",
    "print(f\"Difference: {sample_prediction - explainer.expected_value:.2f}%\")\n",
    "\n",
    "# SHAP force plot\n",
    "shap.force_plot(\n",
    "    explainer.expected_value,\n",
    "    sample_shap,\n",
    "    sample_device,\n",
    "    matplotlib=True,\n",
    "    show=False\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show top contributing features\n",
    "feature_contributions = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'value': sample_device.values,\n",
    "    'shap_value': sample_shap\n",
    "}).sort_values('shap_value', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nTop 3 Contributing Features:\")\n",
    "for i in range(3):\n",
    "    row = feature_contributions.iloc[i]\n",
    "    direction = \"increasing\" if row['shap_value'] > 0 else \"decreasing\"\n",
    "    print(f\"  {i+1}. {row['feature']} = {row['value']:.3f} â†’ {direction} yield by {abs(row['shap_value']):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9701fc",
   "metadata": {},
   "source": [
    "## 9. Method 4: LIME (Local Interpretable Model-agnostic Explanations)\n",
    "\n",
    "**Concept:** Fit a simple linear model locally around the prediction to approximate behavior.\n",
    "\n",
    "**Mathematics:**\n",
    "$$\\text{explanation}(x) = \\arg\\min_{g \\in G} L(f, g, \\pi_x) + \\Omega(g)$$\n",
    "\n",
    "Where:\n",
    "- $f$ = complex model\n",
    "- $g$ = simple linear model\n",
    "- $\\pi_x$ = proximity to instance $x$\n",
    "- $\\Omega(g)$ = complexity penalty\n",
    "\n",
    "**Why useful:** Approximates any model with interpretable linear model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8052bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LIME explainer\n",
    "lime_explainer = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_train.values,\n",
    "    feature_names=feature_cols,\n",
    "    mode='regression',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Explain the same device we used for SHAP\n",
    "lime_explanation = lime_explainer.explain_instance(\n",
    "    data_row=sample_device.values,\n",
    "    predict_fn=rf_model.predict,\n",
    "    num_features=len(feature_cols)\n",
    ")\n",
    "\n",
    "print(f\"LIME Explanation for Device #{sample_idx}:\")\n",
    "print(f\"\\nPrediction: {sample_prediction:.2f}%\")\n",
    "print(f\"Local linear model RÂ²: {lime_explanation.score:.3f}\")\n",
    "print(f\"\\nLinear coefficients (local approximation):\")\n",
    "\n",
    "# Extract feature contributions\n",
    "lime_features = lime_explanation.as_list()\n",
    "for feature_desc, weight in lime_features:\n",
    "    direction = \"â†‘\" if weight > 0 else \"â†“\"\n",
    "    print(f\"  {feature_desc}: {weight:+.3f} {direction}\")\n",
    "\n",
    "# Visualize LIME explanation\n",
    "fig = lime_explanation.as_pyplot_figure()\n",
    "plt.title(f\"LIME Explanation: Device #{sample_idx}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Interpretation:\")\n",
    "print(f\"  LIME fitted a linear model locally (RÂ²={lime_explanation.score:.3f})\")\n",
    "print(f\"  This linear model approximates Random Forest's behavior near this device\")\n",
    "print(f\"  Compare LIME weights to SHAP values for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d5269",
   "metadata": {},
   "source": [
    "## 10. Partial Dependence Plots (PDPs)\n",
    "\n",
    "**Concept:** Show the marginal effect of a feature on predictions, averaging over all other features.\n",
    "\n",
    "**Mathematics:**\n",
    "$$\\text{PDP}(x_s) = \\mathbb{E}_{x_c}[f(x_s, x_c)] = \\frac{1}{n} \\sum_{i=1}^{n} f(x_s, x_c^{(i)})$$\n",
    "\n",
    "Where $x_s$ = feature of interest, $x_c$ = all other features\n",
    "\n",
    "**Why useful:** Reveals non-linear relationships and interaction effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8736e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create partial dependence plots for top features\n",
    "top_features = shap_importance.head(4)['feature'].tolist()\n",
    "feature_indices = [feature_cols.index(f) for f in top_features]\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(14, 10))\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    rf_model,\n",
    "    X_train,\n",
    "    features=feature_indices,\n",
    "    feature_names=feature_cols,\n",
    "    ax=ax.ravel(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "plt.suptitle('Partial Dependence Plots (Top 4 Features)', fontsize=14, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Interpretation:\")\n",
    "print(\"  PDP shows average model response when varying one feature\")\n",
    "print(\"  Flat line = feature has no effect\")\n",
    "print(\"  Curved line = non-linear relationship\")\n",
    "print(\"  Example: If vdd PDP slopes downward, higher voltage â†’ lower yield (as expected from power equation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ab247",
   "metadata": {},
   "source": [
    "## 11. Compare All Importance Methods\n",
    "\n",
    "**Purpose:** Validate consistency across different interpretability techniques.\n",
    "\n",
    "**Key Points:**\n",
    "- **Agreement**: If all methods rank same features top, high confidence\n",
    "- **Disagreement**: Investigateâ€”could indicate correlated features or method-specific biases\n",
    "- **Production strategy**: Use ensemble of methods, not single technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize importance scores to [0, 1] for comparison\n",
    "def normalize_importance(values):\n",
    "    return (values - values.min()) / (values.max() - values.min())\n",
    "\n",
    "# Combine all importance methods\n",
    "comparison_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'tree_importance': normalize_importance(tree_importance.set_index('feature').loc[feature_cols]['importance'].values),\n",
    "    'permutation': normalize_importance(perm_importance_df.set_index('feature').loc[feature_cols]['importance_mean'].values),\n",
    "    'shap_mean_abs': normalize_importance(shap_importance.set_index('feature').loc[feature_cols]['mean_abs_shap'].values)\n",
    "})\n",
    "\n",
    "# Calculate average rank\n",
    "comparison_df['avg_importance'] = comparison_df[['tree_importance', 'permutation', 'shap_mean_abs']].mean(axis=1)\n",
    "comparison_df = comparison_df.sort_values('avg_importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance Comparison (normalized to 0-1):\")\n",
    "print(comparison_df.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(feature_cols))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, comparison_df['tree_importance'], width, label='Tree (Gini)', alpha=0.8)\n",
    "ax.bar(x, comparison_df['permutation'], width, label='Permutation', alpha=0.8)\n",
    "ax.bar(x + width, comparison_df['shap_mean_abs'], width, label='SHAP', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Normalized Importance')\n",
    "ax.set_title('Feature Importance Method Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['feature'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate rank correlation\n",
    "print(\"\\nðŸ“Š Method Agreement:\")\n",
    "from scipy.stats import spearmanr\n",
    "corr_tree_perm = spearmanr(comparison_df['tree_importance'], comparison_df['permutation'])[0]\n",
    "corr_tree_shap = spearmanr(comparison_df['tree_importance'], comparison_df['shap_mean_abs'])[0]\n",
    "corr_perm_shap = spearmanr(comparison_df['permutation'], comparison_df['shap_mean_abs'])[0]\n",
    "\n",
    "print(f\"  Tree vs Permutation: Ï = {corr_tree_perm:.3f}\")\n",
    "print(f\"  Tree vs SHAP: Ï = {corr_tree_shap:.3f}\")\n",
    "print(f\"  Permutation vs SHAP: Ï = {corr_perm_shap:.3f}\")\n",
    "print(f\"\\n  Average correlation: {np.mean([corr_tree_perm, corr_tree_shap, corr_perm_shap]):.3f}\")\n",
    "print(f\"  âœ… High agreement (Ï > 0.8) = trustworthy feature importance rankings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578311d3",
   "metadata": {},
   "source": [
    "## 12. Comprehensive Visualization Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive interpretability dashboard\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Predictions vs Actual\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.scatter(y_test, y_pred_test, alpha=0.5, s=20)\n",
    "ax1.plot([65, 100], [65, 100], 'r--', lw=2)\n",
    "ax1.set_xlabel('Actual Yield (%)')\n",
    "ax1.set_ylabel('Predicted Yield (%)')\n",
    "ax1.set_title(f'Model Performance (RÂ²={test_r2:.3f})')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Feature Importance Comparison\n",
    "ax2 = fig.add_subplot(gs[0, 1:])\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "ax2.barh(x_pos - 0.25, comparison_df['tree_importance'], 0.25, label='Tree', alpha=0.8)\n",
    "ax2.barh(x_pos, comparison_df['permutation'], 0.25, label='Permutation', alpha=0.8)\n",
    "ax2.barh(x_pos + 0.25, comparison_df['shap_mean_abs'], 0.25, label='SHAP', alpha=0.8)\n",
    "ax2.set_yticks(x_pos)\n",
    "ax2.set_yticklabels(comparison_df['feature'])\n",
    "ax2.set_xlabel('Normalized Importance')\n",
    "ax2.set_title('Feature Importance Methods')\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# 3. SHAP Summary (embedded)\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "shap.summary_plot(shap_values, X_test_sample, show=False, plot_size=None)\n",
    "ax3.set_title('SHAP Feature Effects')\n",
    "\n",
    "# 4. Single prediction breakdown\n",
    "ax4 = fig.add_subplot(gs[2, :2])\n",
    "feature_contributions_sorted = feature_contributions.sort_values('shap_value')\n",
    "colors = ['red' if x > 0 else 'blue' for x in feature_contributions_sorted['shap_value']]\n",
    "ax4.barh(range(len(feature_contributions_sorted)), feature_contributions_sorted['shap_value'], color=colors, alpha=0.7)\n",
    "ax4.set_yticks(range(len(feature_contributions_sorted)))\n",
    "ax4.set_yticklabels(feature_contributions_sorted['feature'])\n",
    "ax4.set_xlabel('SHAP Value (% yield impact)')\n",
    "ax4.set_title(f'Device #{sample_idx} Explanation (predicted: {sample_prediction:.1f}%)')\n",
    "ax4.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 5. Feature value distribution\n",
    "ax5 = fig.add_subplot(gs[2, 2])\n",
    "top_feature = comparison_df.iloc[0]['feature']\n",
    "ax5.hist(X_train[top_feature], bins=30, alpha=0.7, edgecolor='black')\n",
    "ax5.axvline(sample_device[top_feature], color='red', linestyle='--', linewidth=2, label=f'Device #{sample_idx}')\n",
    "ax5.set_xlabel(top_feature)\n",
    "ax5.set_ylabel('Frequency')\n",
    "ax5.set_title(f'Distribution: {top_feature}')\n",
    "ax5.legend()\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Model Interpretability Dashboard', fontsize=16, y=0.995)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fcf556",
   "metadata": {},
   "source": [
    "## 13. Project Templates\n",
    "\n",
    "### Project 1: Automated Yield Loss Root Cause System\n",
    "**Objective:** Build real-time SHAP-based root cause identification for failed devices\n",
    "- Load real STDF wafer test data with 200+ parametric tests\n",
    "- Train binary classifier (pass/fail) with Random Forest or XGBoost\n",
    "- For each failed device, compute SHAP values and rank top 5 contributing parameters\n",
    "- Generate automated report: \"Device failed due to: 1) Idd (+3.2Ïƒ), 2) Freq (-1.8Ïƒ), 3) Temp (+2.1Ïƒ)\"\n",
    "- **Success Metric:** Reduce root cause analysis time from 2 weeks to <2 hours, validate with fab engineers on 10 real yield excursions\n",
    "\n",
    "### Project 2: Interpretable Test Time Prediction\n",
    "**Objective:** Explain why specific devices take longer to test\n",
    "- Use test time (seconds) as target variable\n",
    "- Features: device type, lot characteristics, ATE platform, time-of-day, test sequence\n",
    "- Apply LIME to explain outliers (devices taking >2Ã— median time)\n",
    "- Identify systematic issues (e.g., \"Lot B always slow on ATE-7 due to calibration drift\")\n",
    "- **Success Metric:** Find 3+ actionable insights leading to 10% average test time reduction\n",
    "\n",
    "### Project 3: Trusted Bin Prediction with Explanation\n",
    "**Objective:** Deploy bin prediction model with per-device explanations for engineers\n",
    "- Multi-class classification: BIN1 (premium), BIN2 (standard), BIN7 (fail), BIN9 (retest)\n",
    "- Train ensemble model (Random Forest or LightGBM)\n",
    "- Generate SHAP force plots for every prediction in production\n",
    "- Create dashboard showing: prediction, confidence, top 3 reasons, similar historical devices\n",
    "- **Success Metric:** >80% engineer trust score, <5% override rate, feedback validates SHAP reasoning\n",
    "\n",
    "### Project 4: Feature Engineering Validation via Interpretability\n",
    "**Objective:** Verify engineered features are being used correctly by the model\n",
    "- Create 20+ engineered features (interactions, polynomials, domain-specific)\n",
    "- Train model and compute SHAP importance for engineered vs raw features\n",
    "- Check if power (vddÃ—idd) has higher importance than vdd, idd individually\n",
    "- Prune low-SHAP-value features (<0.01 mean |SHAP|)\n",
    "- **Success Metric:** Reduce feature count 40% while maintaining RÂ² within 1%, improved model latency\n",
    "\n",
    "### Project 5: Bias Detection in Wafer-Level Predictions\n",
    "**Objective:** Use interpretability to detect and remove spatial bias in yield models\n",
    "- Train model predicting yield from parametric tests + spatial coordinates\n",
    "- Compute SHAP values for die_x, die_y, radius features\n",
    "- If spatial features dominate (>30% SHAP importance), model learned wafer map patterns\n",
    "- Retrain with spatial features removed, validate performance doesn't drop\n",
    "- **Success Metric:** Model generalizes to new lots without overfitting to historical spatial patterns\n",
    "\n",
    "### Project 6: Partial Dependence for Spec Limit Optimization\n",
    "**Objective:** Use PDPs to set optimal parametric test limits\n",
    "- Train yield prediction model from parametric tests\n",
    "- Generate PDP for each test parameter showing yield vs parameter value\n",
    "- Identify inflection points where yield drops sharply (new spec limit candidates)\n",
    "- Compare PDP-derived limits to current data sheet specs\n",
    "- **Success Metric:** Propose 5+ spec limit changes backed by data, simulate 2-5% yield improvement\n",
    "\n",
    "### Project 7: LIME-Based Anomaly Explanation\n",
    "**Objective:** Explain anomalous devices using local linear approximations\n",
    "- Train autoencoder or isolation forest for anomaly detection on parametric data\n",
    "- For flagged anomalies, use LIME to explain reconstruction error or anomaly score\n",
    "- Generate human-readable explanations: \"Anomalous because: vth=-2.3Ïƒ, idd=+3.1Ïƒ, freq=-1.2Ïƒ\"\n",
    "- Cross-reference with SHAP explanations for validation\n",
    "- **Success Metric:** 90% of LIME-flagged anomaly reasons confirmed by fab engineers as real issues\n",
    "\n",
    "### Project 8: Interactive Interpretability Dashboard\n",
    "**Objective:** Build Streamlit/Dash app for real-time model interpretation\n",
    "- Upload STDF file â†’ predict yield/bin for all devices\n",
    "- Click any device â†’ show SHAP force plot, LIME explanation, feature values\n",
    "- Compare multiple devices side-by-side with SHAP value differences\n",
    "- Export explanations to PDF reports for management\n",
    "- **Success Metric:** Deployed to 5+ test engineers, used weekly for production debug, <5min time-to-insight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c12a0e1",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Key Takeaways\n",
    "\n",
    "**When to Use Each Method:**\n",
    "- **Tree-based importance**: Quick baseline for tree models (RF, XGBoost), but biased\n",
    "- **Permutation importance**: Model-agnostic, unbiased, works for any model\n",
    "- **SHAP values**: Gold standardâ€”both global and local explanations, theoretically sound\n",
    "- **LIME**: When you need simple linear approximations, fast for single predictions\n",
    "- **PDPs**: Understanding non-linear feature effects, but averages over interactions\n",
    "\n",
    "**Limitations:**\n",
    "- âš ï¸ **Correlated features**: All methods struggle when features are highly correlated (multicollinearity)\n",
    "- âš ï¸ **Computational cost**: SHAP can be slow for large datasets (use TreeExplainer or sampling)\n",
    "- âš ï¸ **Interpretation != Causation**: High importance doesn't mean causal relationship\n",
    "- âš ï¸ **Adversarial explanations**: Models can be trained to give misleading explanations\n",
    "- âš ï¸ **Human bias**: Engineers may cherry-pick explanations that confirm existing beliefs\n",
    "\n",
    "**Best Practices:**\n",
    "1. **Use multiple methods**: If SHAP, permutation, and tree importance agree â†’ high confidence\n",
    "2. **Validate with domain experts**: Do top features match physics/engineering knowledge?\n",
    "3. **Check for data leakage**: If importance seems too good, investigate feature-target leakage\n",
    "4. **Sample for speed**: SHAP on 500-1000 samples usually sufficient for global patterns\n",
    "5. **Document explanations**: Include SHAP plots in model cards for production transparency\n",
    "6. **Monitor in production**: Feature importance can shift with data driftâ€”re-compute monthly\n",
    "7. **Use explanations for debugging**: If model fails, SHAP shows which features caused error\n",
    "\n",
    "**Production Checklist:**\n",
    "- [ ] Compute SHAP values on validation set\n",
    "- [ ] Verify top 5 features make engineering sense\n",
    "- [ ] Create SHAP summary plot for model card\n",
    "- [ ] Build API endpoint for single-prediction explanations\n",
    "- [ ] Set up monitoring for feature importance drift\n",
    "- [ ] Train stakeholders on reading SHAP plots\n",
    "- [ ] Document known limitations and failure modes\n",
    "\n",
    "**Next Steps:**\n",
    "- Study **105: AutoML & NAS** for automated model search with interpretability constraints\n",
    "- Explore **107: Model Monitoring** to detect when feature importance changes in production\n",
    "- Apply to **Deep Learning (054)** with attention mechanisms and gradient-based explanations\n",
    "- Read \"Interpretable Machine Learning\" by Christoph Molnar (free online book)\n",
    "- Experiment with SHAP's advanced plots: dependence plots, interaction plots, waterfall plots"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
