{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480695e9",
   "metadata": {},
   "source": [
    "# 047: ML Pipelines & Automation",
    "",
    "## \ud83c\udfaf Learning Objectives",
    "",
    "By the end of this notebook, you will:",
    "",
    "1. **Understand ML pipelines** - Why pipelines matter, composition, reproducibility",
    "2. **Master sklearn Pipeline** - Sequential transformers, ColumnTransformer, FeatureUnion",
    "3. **Build custom transformers** - Extend BaseEstimator, fit/transform pattern, stateful transformers",
    "4. **Implement end-to-end pipelines** - Data ingestion \u2192 preprocessing \u2192 model \u2192 evaluation",
    "5. **Handle complex workflows** - Conditional transformations, parallel processing, caching",
    "6. **Apply to post-silicon validation** - STDF data pipelines, multi-stage test flows",
    "7. **Production patterns** - Serialization, versioning, monitoring, CI/CD integration",
    "8. **Automate repetitive tasks** - Grid search over pipelines, automated feature engineering",
    "",
    "---",
    "",
    "## \ud83d\udcca Why ML Pipelines Matter",
    "",
    "```mermaid",
    "graph LR",
    "    A[Raw Data] --> B[Preprocessing]",
    "    B --> C[Feature Engineering]",
    "    C --> D[Model Training]",
    "    D --> E[Prediction]",
    "    ",
    "    style A fill:#ffe6e6",
    "    style B fill:#e6f3ff",
    "    style C fill:#e6f3ff",
    "    style D fill:#e6ffe6",
    "    style E fill:#90EE90",
    "    ",
    "    F[Without Pipeline:<br/>Scattered Code] -.-> G[\u274c Data Leakage<br/>\u274c Irreproducible<br/>\u274c Error-Prone]",
    "    ",
    "    H[With Pipeline:<br/>Unified Object] -.-> I[\u2705 No Leakage<br/>\u2705 Reproducible<br/>\u2705 Production-Ready]",
    "```",
    "",
    "---",
    "",
    "### **The Problem: Scattered Preprocessing**",
    "",
    "**Typical messy workflow:**",
    "",
    "```python",
    "# Train",
    "X_train_scaled = scaler.fit_transform(X_train)",
    "X_train_encoded = encoder.fit_transform(X_train_scaled)",
    "model.fit(X_train_encoded, y_train)",
    "",
    "# Test (OOPS! Forgot to apply same transforms!)",
    "X_test_scaled = scaler.transform(X_test)  # Easy to forget",
    "X_test_encoded = encoder.transform(X_test_scaled)  # Error-prone",
    "y_pred = model.predict(X_test_encoded)",
    "",
    "# Production (nightmare!)",
    "# How do I remember the exact sequence?",
    "# Did I fit scaler on train or all data? (data leakage risk!)",
    "```",
    "",
    "**Issues:**",
    "- \u274c **Data leakage:** Accidentally fit scaler on test data",
    "- \u274c **Inconsistency:** Different transforms for train/test/production",
    "- \u274c **Irreproducible:** Can't recreate exact preprocessing steps",
    "- \u274c **Error-prone:** Forget a step \u2192 silent model performance degradation",
    "- \u274c **Hard to serialize:** How to save model + all preprocessing steps?",
    "",
    "---",
    "",
    "### **The Solution: Unified Pipeline**",
    "",
    "**Clean pipeline workflow:**",
    "",
    "```python",
    "from sklearn.pipeline import Pipeline",
    "",
    "# Define once",
    "pipeline = Pipeline([",
    "    ('scaler', StandardScaler()),",
    "    ('encoder', OneHotEncoder()),",
    "    ('model', RandomForestClassifier())",
    "])",
    "",
    "# Train (fit pipeline = fit all steps on train data)",
    "pipeline.fit(X_train, y_train)",
    "",
    "# Test (transform pipeline = apply all steps consistently)",
    "y_pred = pipeline.predict(X_test)",
    "",
    "# Production (single object, no mistakes!)",
    "import joblib",
    "joblib.dump(pipeline, 'model_v1.pkl')",
    "loaded_pipeline = joblib.load('model_v1.pkl')",
    "y_prod = loaded_pipeline.predict(X_new)  # All transforms applied correctly!",
    "```",
    "",
    "**Benefits:**",
    "- \u2705 **No data leakage:** Scaler fit only on train, transform on test",
    "- \u2705 **Consistency:** Exact same transforms for train/test/production",
    "- \u2705 **Reproducible:** Single object captures entire workflow",
    "- \u2705 **Easy serialization:** Save once, deploy anywhere",
    "- \u2705 **Grid search compatible:** Tune preprocessing + model simultaneously",
    "",
    "---",
    "",
    "## \ud83c\udf93 Post-Silicon Validation Context",
    "",
    "### Why Pipelines Are Critical in Semiconductor Testing:",
    "",
    "1. **Multi-Stage Test Flows** ($50M-$200M ATE investment)",
    "   - Wafer test \u2192 Package \u2192 Final test \u2192 Burn-in \u2192 System test",
    "   - Each stage has different preprocessing (spatial averaging, temporal filtering, outlier removal)",
    "   - Pipeline ensures consistent transforms across stages",
    "",
    "2. **STDF Data Preprocessing** (Standard Test Data Format)",
    "   - Raw STDF: 1000+ parametric tests per device, missing data, outliers, spatial correlation",
    "   - Preprocessing: Missing data imputation \u2192 Outlier removal \u2192 Feature scaling \u2192 Spatial detrending",
    "   - Pipeline prevents: \"Which outlier threshold did we use in Model v2?\" (versioning nightmare)",
    "",
    "3. **Real-Time Production Deployment** (1M devices/day)",
    "   - Test equipment generates predictions in real-time (<100ms latency)",
    "   - Pipeline serialization: Train offline \u2192 deploy as single .pkl file \u2192 load on edge device",
    "   - No room for error: Wrong preprocessing \u2192 5% yield loss \u2192 $10M-$50M annual impact",
    "",
    "4. **Regulatory Compliance** (Automotive, Medical)",
    "   - Auditors ask: \"Exactly how is data preprocessed before prediction?\"",
    "   - Pipeline code = documentation: Every transform is explicit, version-controlled",
    "   - ISO 26262 requirement: \"ML model processing must be deterministic and traceable\"",
    "",
    "---",
    "",
    "## \ud83d\udd11 Core Concepts",
    "",
    "### **1. Pipeline Fundamentals**",
    "",
    "**Definition:** A pipeline is a **sequence of transforms** followed by a **final estimator**.",
    "",
    "```python",
    "Pipeline([",
    "    ('transform_1', Transformer1()),  # fit_transform on train, transform on test",
    "    ('transform_2', Transformer2()),  # fit_transform on train, transform on test",
    "    ('estimator', Model())            # fit on train, predict on test",
    "])",
    "```",
    "",
    "**Key properties:**",
    "- All intermediate steps must implement `fit()` and `transform()`",
    "- Final step must implement `fit()` and optionally `predict()` or `transform()`",
    "- Calling `pipeline.fit(X, y)` sequentially calls:",
    "  1. `transform_1.fit(X, y)`, then `X = transform_1.transform(X)`",
    "  2. `transform_2.fit(X, y)`, then `X = transform_2.transform(X)`",
    "  3. `estimator.fit(X, y)`",
    "- Calling `pipeline.predict(X)` sequentially calls:",
    "  1. `X = transform_1.transform(X)` (no fit!)",
    "  2. `X = transform_2.transform(X)` (no fit!)",
    "  3. `y = estimator.predict(X)`",
    "",
    "---",
    "",
    "### **2. ColumnTransformer: Heterogeneous Data**",
    "",
    "**Problem:** Different columns need different preprocessing",
    "",
    "```python",
    "# Numeric: Scale",
    "# Categorical: One-hot encode",
    "# Text: TF-IDF vectorize",
    "# Dates: Extract features (day_of_week, month, etc.)",
    "```",
    "",
    "**Solution:** `ColumnTransformer` applies different transformers to different columns",
    "",
    "```python",
    "from sklearn.compose import ColumnTransformer",
    "",
    "preprocessor = ColumnTransformer([",
    "    ('num', StandardScaler(), ['Vdd', 'Idd', 'freq']),       # Numeric columns",
    "    ('cat', OneHotEncoder(), ['site_id', 'product_type']),  # Categorical columns",
    "    ('passthrough', 'passthrough', ['wafer_id'])            # Keep as-is",
    "])",
    "```",
    "",
    "**Parallel execution:** Transforms run independently, then concatenate outputs",
    "",
    "---",
    "",
    "### **3. FeatureUnion: Parallel Features**",
    "",
    "**Problem:** Generate multiple feature representations in parallel",
    "",
    "```python",
    "# Option A: PCA (dimensionality reduction)",
    "# Option B: SelectKBest (feature selection)",
    "# Combine both representations",
    "```",
    "",
    "**Solution:** `FeatureUnion` runs transformers in parallel, concatenates results",
    "",
    "```python",
    "from sklearn.pipeline import FeatureUnion",
    "",
    "feature_engineering = FeatureUnion([",
    "    ('pca', PCA(n_components=50)),",
    "    ('select', SelectKBest(k=30))",
    "])",
    "# Output: 50 PCA features + 30 selected features = 80 features total",
    "```",
    "",
    "---",
    "",
    "### **4. Custom Transformers**",
    "",
    "**When to write custom transformers:**",
    "- Domain-specific preprocessing (spatial detrending for wafer data)",
    "- Complex feature engineering (time-series rolling statistics)",
    "- Conditional transformations (different logic for different data ranges)",
    "",
    "**Pattern:** Inherit from `BaseEstimator` and `TransformerMixin`",
    "",
    "```python",
    "from sklearn.base import BaseEstimator, TransformerMixin",
    "",
    "class MyTransformer(BaseEstimator, TransformerMixin):",
    "    def __init__(self, param1=1.0):",
    "        self.param1 = param1",
    "    ",
    "    def fit(self, X, y=None):",
    "        # Learn parameters from training data",
    "        self.learned_stat_ = X.mean()  # Trailing _ = learned attribute",
    "        return self",
    "    ",
    "    def transform(self, X):",
    "        # Apply transformation using learned parameters",
    "        return X - self.learned_stat_",
    "```",
    "",
    "**Key rules:**",
    "- `__init__`: Only store hyperparameters (no data-dependent logic)",
    "- `fit`: Learn parameters from training data, store with trailing underscore `_`",
    "- `transform`: Apply transformation using learned parameters (no re-fitting)",
    "- `fit_transform`: Provided automatically by `TransformerMixin`",
    "",
    "---",
    "",
    "## \ud83d\udee0\ufe0f When to Use Each Component",
    "",
    "| **Component** | **Use Case** | **Example** |",
    "|---------------|--------------|-------------|",
    "| **Pipeline** | Sequential transforms + model | Scaler \u2192 PCA \u2192 Random Forest |",
    "| **ColumnTransformer** | Different transforms for different columns | Numeric: scale, Categorical: encode |",
    "| **FeatureUnion** | Combine multiple feature representations | PCA features + Original features |",
    "| **Custom Transformer** | Domain-specific preprocessing | Wafer spatial detrending, STDF outlier removal |",
    "| **FunctionTransformer** | Simple stateless transforms | `np.log`, `np.sqrt` without state |",
    "| **make_pipeline** | Quick pipeline without naming steps | `make_pipeline(Scaler(), Model())` |",
    "",
    "---",
    "",
    "## \ud83c\udfed Semiconductor-Specific Pipelines",
    "",
    "### **Challenge 1: Spatial Correlation on Wafers**",
    "",
    "**Problem:** Adjacent dies are similar \u2192 violates IID assumption",
    "",
    "**Pipeline solution:**",
    "1. Group by wafer_id",
    "2. Compute within-wafer mean/std (spatial statistics)",
    "3. Detrend each die by subtracting wafer mean",
    "4. Feed detrended values to model",
    "",
    "**Custom transformer:** `WaferSpatialDetrending`",
    "",
    "---",
    "",
    "### **Challenge 2: Multi-Stage Test Correlation**",
    "",
    "**Problem:** Wafer test + Final test data must be merged, but have different feature spaces",
    "",
    "**Pipeline solution:**",
    "1. Wafer test pipeline: 200 tests \u2192 PCA \u2192 50 features",
    "2. Final test pipeline: 150 tests \u2192 Feature selection \u2192 30 features",
    "3. FeatureUnion: Concatenate 50 + 30 = 80 features",
    "4. Model: XGBoost on 80 features",
    "",
    "---",
    "",
    "### **Challenge 3: Real-Time Inference Latency**",
    "",
    "**Problem:** Production environment requires <50ms latency (1M devices/day)",
    "",
    "**Pipeline optimization:**",
    "1. Cache preprocessing: Fit once, save pipeline",
    "2. Batch inference: Process 1000 devices at once (vectorized)",
    "3. Feature selection: Remove low-importance features (40% speedup)",
    "4. Model simplification: Ensemble with 50 trees (vs 500 for offline)",
    "",
    "---",
    "",
    "## \ud83d\udcda What We'll Build",
    "",
    "### **From Scratch (Educational):**",
    "1. **Simple Pipeline** - Manual implementation to understand internals",
    "2. **Custom Transformer** - Domain-specific preprocessing for STDF data",
    "",
    "### **Production (Practical):**",
    "3. **sklearn Pipeline** - Standard scaler + model",
    "4. **ColumnTransformer** - Heterogeneous data (numeric + categorical)",
    "5. **End-to-end pipeline** - Data ingestion \u2192 preprocessing \u2192 model \u2192 evaluation",
    "6. **Serialization & deployment** - Save/load pipeline, version control",
    "",
    "---",
    "",
    "## \ud83c\udfaf Real-World Applications",
    "",
    "### **Post-Silicon Validation:**",
    "- **Automated test flow** - Wafer test \u2192 Final test \u2192 Binning (single pipeline)",
    "- **Spatial feature engineering** - Wafer map detrending + model",
    "- **Multi-site harmonization** - Site-specific preprocessing + unified model",
    "- **Real-time binning** - <50ms latency, serialized pipeline on edge device",
    "",
    "### **General AI/ML:**",
    "- **Production ML** - Text classification, fraud detection, recommendation systems",
    "- **AutoML integration** - Grid search over pipeline hyperparameters",
    "- **A/B testing** - Easy to swap pipeline versions, compare performance",
    "- **Model monitoring** - Track pipeline drift (preprocessing statistics)",
    "",
    "---",
    "",
    "**Let's begin!** \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a277b19",
   "metadata": {},
   "source": [
    "## \ud83d\udcd0 Mathematical Foundation: Pipeline Architecture\n",
    "\n",
    "### **Pipeline as Function Composition**\n",
    "\n",
    "**Mathematical view:** A pipeline is a **composition of functions**\n",
    "\n",
    "$$\n",
    "\\text{Pipeline}(x) = f_3(f_2(f_1(x)))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $f_1$: First transformer (e.g., StandardScaler)\n",
    "- $f_2$: Second transformer (e.g., PCA)\n",
    "- $f_3$: Final estimator (e.g., Logistic Regression)\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Phase: Sequential Fitting**\n",
    "\n",
    "**Step-by-step execution of `pipeline.fit(X_train, y_train)`:**\n",
    "\n",
    "1. **Fit and transform first step:**\n",
    "   $$\n",
    "   \\theta_1 = \\text{fit}(f_1, X_{\\text{train}}, y_{\\text{train}})\n",
    "   $$\n",
    "   $$\n",
    "   X_1 = f_1(X_{\\text{train}}; \\theta_1)\n",
    "   $$\n",
    "   \n",
    "   Example: StandardScaler learns $\\mu_1, \\sigma_1$ from $X_{\\text{train}}$, then transforms:\n",
    "   $$\n",
    "   X_1 = \\frac{X_{\\text{train}} - \\mu_1}{\\sigma_1}\n",
    "   $$\n",
    "\n",
    "2. **Fit and transform second step:**\n",
    "   $$\n",
    "   \\theta_2 = \\text{fit}(f_2, X_1, y_{\\text{train}})\n",
    "   $$\n",
    "   $$\n",
    "   X_2 = f_2(X_1; \\theta_2)\n",
    "   $$\n",
    "   \n",
    "   Example: PCA learns principal components $W_2$ from $X_1$, then projects:\n",
    "   $$\n",
    "   X_2 = X_1 \\cdot W_2\n",
    "   $$\n",
    "\n",
    "3. **Fit final estimator:**\n",
    "   $$\n",
    "   \\theta_3 = \\text{fit}(f_3, X_2, y_{\\text{train}})\n",
    "   $$\n",
    "   \n",
    "   Example: Logistic Regression learns weights $\\beta_3$ from $X_2$:\n",
    "   $$\n",
    "   \\beta_3 = \\arg\\min_{\\beta} \\sum_{i=1}^{n} \\log(1 + e^{-y_i (\\beta^T X_2)})\n",
    "   $$\n",
    "\n",
    "**Key principle:** Each step is fit **only on the output of the previous step**, preventing data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "### **Prediction Phase: Transform-Only**\n",
    "\n",
    "**Step-by-step execution of `pipeline.predict(X_test)`:**\n",
    "\n",
    "1. **Transform with first step (NO fitting):**\n",
    "   $$\n",
    "   X_1 = f_1(X_{\\text{test}}; \\theta_1)\n",
    "   $$\n",
    "   \n",
    "   Example: Apply StandardScaler with **training statistics**:\n",
    "   $$\n",
    "   X_1 = \\frac{X_{\\text{test}} - \\mu_1}{\\sigma_1}\n",
    "   $$\n",
    "   \n",
    "   \u26a0\ufe0f **Critical:** Use $\\mu_1, \\sigma_1$ learned from training, **NOT** test statistics!\n",
    "\n",
    "2. **Transform with second step (NO fitting):**\n",
    "   $$\n",
    "   X_2 = f_2(X_1; \\theta_2)\n",
    "   $$\n",
    "   \n",
    "   Example: Apply PCA with **training components**:\n",
    "   $$\n",
    "   X_2 = X_1 \\cdot W_2\n",
    "   $$\n",
    "\n",
    "3. **Predict with final estimator:**\n",
    "   $$\n",
    "   \\hat{y} = f_3(X_2; \\theta_3)\n",
    "   $$\n",
    "   \n",
    "   Example: Logistic Regression prediction:\n",
    "   $$\n",
    "   \\hat{y} = \\text{sign}(\\beta_3^T X_2)\n",
    "   $$\n",
    "\n",
    "**Key principle:** Parameters $\\theta_1, \\theta_2, \\theta_3$ are **frozen** during prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Prevents Data Leakage**\n",
    "\n",
    "**Bad practice (manual code):**\n",
    "\n",
    "```python\n",
    "# WRONG! Scaler sees test data during fit()\n",
    "scaler.fit(np.concatenate([X_train, X_test]))  # \u274c Data leakage!\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "**Mathematical issue:** Test statistics $\\mu_{\\text{test}}, \\sigma_{\\text{test}}$ leak into training:\n",
    "$$\n",
    "\\mu_{\\text{combined}} = \\frac{n_{\\text{train}} \\mu_{\\text{train}} + n_{\\text{test}} \\mu_{\\text{test}}}{n_{\\text{train}} + n_{\\text{test}}}\n",
    "$$\n",
    "\n",
    "Result: Model has seen test data indirectly \u2192 optimistic performance estimate.\n",
    "\n",
    "---\n",
    "\n",
    "**Good practice (pipeline):**\n",
    "\n",
    "```python\n",
    "# CORRECT! Scaler sees only training data\n",
    "pipeline.fit(X_train, y_train)  # \u2705 No leakage\n",
    "y_pred = pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "**Mathematical guarantee:** Parameters learned only from training:\n",
    "$$\n",
    "\\theta_1 = \\text{fit}(f_1, X_{\\text{train}}, y_{\\text{train}})\n",
    "$$\n",
    "$$\n",
    "\\mu_1 = \\frac{1}{n_{\\text{train}}} \\sum_{i \\in \\text{train}} X_i\n",
    "$$\n",
    "\n",
    "Test data **never** influences $\\mu_1$.\n",
    "\n",
    "---\n",
    "\n",
    "### **ColumnTransformer as Block-Diagonal Transformation**\n",
    "\n",
    "**Mathematical structure:**\n",
    "\n",
    "For dataset with 3 column groups (numeric, categorical, text):\n",
    "\n",
    "$$\n",
    "X = [X_{\\text{num}} \\mid X_{\\text{cat}} \\mid X_{\\text{text}}]\n",
    "$$\n",
    "\n",
    "ColumnTransformer applies different functions:\n",
    "\n",
    "$$\n",
    "T(X) = [f_{\\text{num}}(X_{\\text{num}}) \\mid f_{\\text{cat}}(X_{\\text{cat}}) \\mid f_{\\text{text}}(X_{\\text{text}})]\n",
    "$$\n",
    "\n",
    "**As matrix transformation:**\n",
    "\n",
    "$$\n",
    "T = \\begin{bmatrix}\n",
    "T_{\\text{num}} & 0 & 0 \\\\\n",
    "0 & T_{\\text{cat}} & 0 \\\\\n",
    "0 & 0 & T_{\\text{text}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Block-diagonal structure:** Columns are transformed **independently**, then concatenated.\n",
    "\n",
    "---\n",
    "\n",
    "### **Pipeline + GridSearchCV: Joint Optimization**\n",
    "\n",
    "**Traditional approach (suboptimal):**\n",
    "\n",
    "```python\n",
    "# Step 1: Optimize preprocessing\n",
    "best_n_components = grid_search_pca(X_train)\n",
    "\n",
    "# Step 2: Optimize model (PCA is fixed!)\n",
    "pca = PCA(n_components=best_n_components)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "best_C = grid_search_logreg(X_train_pca)\n",
    "```\n",
    "\n",
    "**Problem:** PCA optimization doesn't consider model performance \u2192 suboptimal combination.\n",
    "\n",
    "---\n",
    "\n",
    "**Pipeline approach (optimal):**\n",
    "\n",
    "```python\n",
    "pipeline = Pipeline([('pca', PCA()), ('logreg', LogisticRegression())])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': [10, 20, 50],\n",
    "    'logreg__C': [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid)\n",
    "grid_search.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Mathematical formulation:**\n",
    "\n",
    "$$\n",
    "(\\theta_1^*, \\theta_3^*) = \\arg\\min_{n_{\\text{comp}}, C} \\text{CV-Error}(\\text{PCA}(n_{\\text{comp}}) \\to \\text{LogReg}(C))\n",
    "$$\n",
    "\n",
    "**Benefit:** PCA and LogisticRegression hyperparameters are optimized **jointly** for best cross-validation score.\n",
    "\n",
    "---\n",
    "\n",
    "### **Computational Complexity**\n",
    "\n",
    "**Pipeline overhead:**\n",
    "\n",
    "| **Operation** | **Without Pipeline** | **With Pipeline** | **Difference** |\n",
    "|---------------|----------------------|-------------------|----------------|\n",
    "| **Training** | $O(n \\cdot p \\cdot k)$ | $O(n \\cdot p \\cdot k)$ | No overhead |\n",
    "| **Prediction** | $O(m \\cdot p \\cdot k)$ | $O(m \\cdot p \\cdot k)$ | No overhead |\n",
    "| **Memory** | Store $k$ objects separately | Store 1 pipeline object | Cleaner |\n",
    "\n",
    "Where:\n",
    "- $n$: Training samples\n",
    "- $m$: Test samples\n",
    "- $p$: Features\n",
    "- $k$: Number of pipeline steps\n",
    "\n",
    "**Key insight:** Pipelines have **zero computational overhead** (just organizational benefit).\n",
    "\n",
    "---\n",
    "\n",
    "### **Caching: Memory-Computation Tradeoff**\n",
    "\n",
    "**Problem:** Expensive preprocessing (e.g., TF-IDF on 1M documents)\n",
    "\n",
    "**Solution:** Cache intermediate results\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),  # Expensive: O(n * d * v)\n",
    "    ('model', LogisticRegression())\n",
    "], memory='/tmp/cache')\n",
    "```\n",
    "\n",
    "**Mathematical benefit:**\n",
    "\n",
    "**Without caching (GridSearchCV with 10 folds):**\n",
    "- TF-IDF computed 10 times: $10 \\times O(n \\cdot d \\cdot v)$\n",
    "\n",
    "**With caching:**\n",
    "- TF-IDF computed once: $1 \\times O(n \\cdot d \\cdot v)$\n",
    "- Speedup: **10x** for preprocessing step\n",
    "\n",
    "---\n",
    "\n",
    "### **Semiconductor Example: Spatial Detrending Math**\n",
    "\n",
    "**Problem:** Wafer maps show spatial gradients (edge dies yield < center dies)\n",
    "\n",
    "**Mathematical model:**\n",
    "\n",
    "1. **Raw yield:**\n",
    "   $$\n",
    "   Y_{i,j} = f_{\\text{process}} + f_{\\text{spatial}}(x_i, y_j) + \\epsilon_{i,j}\n",
    "   $$\n",
    "   \n",
    "   Where:\n",
    "   - $f_{\\text{process}}$: True process variation (what we want to model)\n",
    "   - $f_{\\text{spatial}}(x_i, y_j)$: Spatial bias (nuisance)\n",
    "   - $\\epsilon_{i,j}$: Random noise\n",
    "\n",
    "2. **Spatial detrending:**\n",
    "   $$\n",
    "   Y_{i,j}^{\\text{detrended}} = Y_{i,j} - \\bar{Y}_{\\text{wafer}} = f_{\\text{process}} + \\epsilon_{i,j}\n",
    "   $$\n",
    "   \n",
    "   Where:\n",
    "   $$\n",
    "   \\bar{Y}_{\\text{wafer}} = \\frac{1}{N_{\\text{dies}}} \\sum_{i,j} Y_{i,j}\n",
    "   $$\n",
    "\n",
    "3. **Pipeline implementation:**\n",
    "   ```python\n",
    "   class WaferSpatialDetrending(BaseEstimator, TransformerMixin):\n",
    "       def fit(self, X, y=None):\n",
    "           # Learn wafer-level means from training data\n",
    "           self.wafer_means_ = X.groupby('wafer_id')['yield'].mean()\n",
    "           return self\n",
    "       \n",
    "       def transform(self, X):\n",
    "           # Subtract wafer-level mean from each die\n",
    "           X['yield_detrended'] = X['yield'] - X['wafer_id'].map(self.wafer_means_)\n",
    "           return X\n",
    "   ```\n",
    "\n",
    "**Result:** Model learns $f_{\\text{process}}$ without spatial bias \u2192 better generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary: Pipeline Mathematics**\n",
    "\n",
    "| **Concept** | **Mathematical Essence** | **Practical Benefit** |\n",
    "|-------------|-------------------------|----------------------|\n",
    "| **Pipeline** | Function composition $f_3 \\circ f_2 \\circ f_1$ | No data leakage |\n",
    "| **Sequential fitting** | $\\theta_i = \\text{fit}(f_i, X_{i-1}, y)$ | Each step sees only previous output |\n",
    "| **Transform-only prediction** | $X_i = f_i(X_{i-1}; \\theta_i)$ (no re-fitting) | Consistent preprocessing |\n",
    "| **ColumnTransformer** | Block-diagonal transformation | Independent column groups |\n",
    "| **GridSearchCV + Pipeline** | Joint optimization $\\arg\\min_{\\theta_1, \\theta_3}$ | Optimal hyperparameter combination |\n",
    "| **Caching** | Avoid redundant computation | $k$-fold speedup |\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** We'll implement simple pipelines from scratch to see these principles in action! \ud83d\udd28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20464dec",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement a **simple pipeline from scratch** to understand how sklearn pipelines work internally.\n",
    "\n",
    "**Key Points:**\n",
    "- **SimplePipeline class:** Mimics sklearn.pipeline.Pipeline with sequential fit/transform/predict\n",
    "- **fit() method:** Iteratively fits each step on the output of the previous step (prevents data leakage)\n",
    "- **predict() method:** Applies transform() to all steps except the last, then predict() on the final estimator\n",
    "- **Naming convention:** Steps are `(name, transformer)` tuples for easy parameter access\n",
    "- **Semiconductor example:** StandardScaler \u2192 PCA \u2192 LogisticRegression for yield classification\n",
    "\n",
    "**Why This Matters:** Understanding pipeline internals helps debug issues, write custom transformers, and optimize performance. For semiconductor manufacturing, pipelines ensure consistent preprocessing across wafer test \u2192 final test \u2192 production deployment ($50M-$200M ATE investment)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1352ba",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "# ===========================\n",
    "# Simple Pipeline From Scratch\n",
    "# ===========================\n",
    "class SimplePipeline:\n",
    "    \"\"\"\n",
    "    Educational implementation of sklearn Pipeline.\n",
    "    \n",
    "    A pipeline chains multiple transformers and a final estimator.\n",
    "    During fit():\n",
    "        - Each transformer is fit on the output of the previous step\n",
    "    During predict():\n",
    "        - Each transformer is applied (no fitting), then final estimator predicts\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    steps : list of (name, transformer) tuples\n",
    "        Sequence of transformers + final estimator\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    pipeline = SimplePipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=2)),\n",
    "        ('classifier', LogisticRegression())\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, steps):\n",
    "        self.steps = steps\n",
    "        self.named_steps = {name: transformer for name, transformer in steps}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit all transformers sequentially, then fit final estimator.\n",
    "        \n",
    "        Process:\n",
    "        1. Fit first transformer on X, transform to get X_1\n",
    "        2. Fit second transformer on X_1, transform to get X_2\n",
    "        3. ... continue for all transformers\n",
    "        4. Fit final estimator on X_final\n",
    "        \"\"\"\n",
    "        X_current = X.copy()\n",
    "        \n",
    "        # Fit and transform all intermediate steps\n",
    "        for name, transformer in self.steps[:-1]:\n",
    "            print(f\"[Pipeline] Fitting {name}...\")\n",
    "            transformer.fit(X_current, y)\n",
    "            X_current = transformer.transform(X_current)\n",
    "            print(f\"  Shape after {name}: {X_current.shape}\")\n",
    "        \n",
    "        # Fit final estimator\n",
    "        final_name, final_estimator = self.steps[-1]\n",
    "        print(f\"[Pipeline] Fitting {final_name}...\")\n",
    "        final_estimator.fit(X_current, y)\n",
    "        print(f\"  Final estimator trained!\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Apply all transformers (no fitting), then predict with final estimator.\n",
    "        \n",
    "        Process:\n",
    "        1. Transform X with first transformer (using parameters learned during fit)\n",
    "        2. Transform X with second transformer (using parameters learned during fit)\n",
    "        3. ... continue for all transformers\n",
    "        4. Predict with final estimator\n",
    "        \"\"\"\n",
    "        X_current = X.copy()\n",
    "        \n",
    "        # Transform with all intermediate steps (NO FITTING!)\n",
    "        for name, transformer in self.steps[:-1]:\n",
    "            X_current = transformer.transform(X_current)\n",
    "        \n",
    "        # Predict with final estimator\n",
    "        _, final_estimator = self.steps[-1]\n",
    "        return final_estimator.predict(X_current)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Compute accuracy score.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(y, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cce997",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15875d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Generate Semiconductor Yield Data\n",
    "# ========================================\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "n_features = 10\n",
    "# Generate features: Vdd_min, Vdd_max, Idd_active, Idd_standby, freq_max, temp, etc.\n",
    "feature_names = [\n",
    "    'Vdd_min', 'Vdd_max', 'Idd_active', 'Idd_standby', 'freq_max',\n",
    "    'temp', 'Vth', 'leakage', 'delay', 'power'\n",
    "]\n",
    "# True signal: yield depends on Vdd_min, Idd_active, freq_max\n",
    "X_signal = np.random.randn(n_samples, 3)\n",
    "X_noise = np.random.randn(n_samples, 7) * 0.5  # Less informative features\n",
    "X = np.hstack([X_signal, X_noise])\n",
    "# Generate binary yield labels (0 = fail, 1 = pass)\n",
    "# Logistic decision boundary: yield = 1 if (0.8*Vdd_min + 0.6*Idd_active + 0.4*freq_max + noise > 0)\n",
    "y_prob = 1 / (1 + np.exp(-(0.8*X[:, 0] + 0.6*X[:, 2] + 0.4*X[:, 4] - 0.5)))\n",
    "y = (y_prob > 0.5).astype(int)\n",
    "print(\"=\" * 70)\n",
    "print(\"Semiconductor Yield Classification with Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Dataset: {n_samples} devices, {n_features} features\")\n",
    "print(f\"Features: {', '.join(feature_names)}\")\n",
    "print(f\"Target: Binary yield (0 = fail, 1 = pass)\")\n",
    "print(f\"Class distribution: {np.sum(y == 0)} fails, {np.sum(y == 1)} passes\")\n",
    "print()\n",
    "# Split train/test\n",
    "train_size = int(0.8 * n_samples)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "print(f\"Train: {X_train.shape[0]} samples\")\n",
    "print(f\"Test: {X_test.shape[0]} samples\")\n",
    "print()\n",
    "# ========================================\n",
    "# Build and Train Simple Pipeline\n",
    "# ========================================\n",
    "print(\"=\" * 70)\n",
    "print(\"Training Simple Pipeline (From Scratch)\")\n",
    "print(\"=\" * 70)\n",
    "# Define pipeline: StandardScaler \u2192 PCA \u2192 LogisticRegression\n",
    "simple_pipeline = SimplePipeline([\n",
    "    ('scaler', StandardScaler()),              # Step 1: Normalize features\n",
    "    ('pca', PCA(n_components=5)),              # Step 2: Dimensionality reduction\n",
    "    ('classifier', LogisticRegression(max_iter=1000))  # Step 3: Classification\n",
    "])\n",
    "# Train pipeline\n",
    "simple_pipeline.fit(X_train, y_train)\n",
    "print()\n",
    "# Evaluate on test set\n",
    "y_pred_train = simple_pipeline.predict(X_train)\n",
    "y_pred_test = simple_pipeline.predict(X_test)\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "print(\"=\" * 70)\n",
    "print(\"Simple Pipeline Results\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873349a",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Compare: Manual Preprocessing (Error-Prone)\n",
    "# ========================================\n",
    "print(\"=\" * 70)\n",
    "print(\"Comparison: Manual Preprocessing (Error-Prone)\")\n",
    "print(\"=\" * 70)\n",
    "# Manual approach: Fit scaler, transform, fit PCA, transform, fit classifier\n",
    "scaler_manual = StandardScaler()\n",
    "pca_manual = PCA(n_components=5)\n",
    "classifier_manual = LogisticRegression(max_iter=1000)\n",
    "# Train\n",
    "X_train_scaled = scaler_manual.fit_transform(X_train)\n",
    "X_train_pca = pca_manual.fit_transform(X_train_scaled)\n",
    "classifier_manual.fit(X_train_pca, y_train)\n",
    "# Test (must remember to apply same transforms!)\n",
    "X_test_scaled = scaler_manual.transform(X_test)\n",
    "X_test_pca = pca_manual.transform(X_test_scaled)\n",
    "y_pred_manual = classifier_manual.predict(X_test_pca)\n",
    "manual_test_acc = accuracy_score(y_test, y_pred_manual)\n",
    "print(f\"Manual Test Accuracy: {manual_test_acc:.4f}\")\n",
    "print(f\"Pipeline Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Difference: {abs(manual_test_acc - test_acc):.6f} (should be ~0)\")\n",
    "print()\n",
    "# Show potential error: Forgot to scale test data\n",
    "print(\"\u26a0\ufe0f Common Error: Forgot to scale test data\")\n",
    "X_test_unscaled_pca = pca_manual.transform(X_test)  # OOPS! Forgot to scale\n",
    "y_pred_error = classifier_manual.predict(X_test_unscaled_pca)\n",
    "error_test_acc = accuracy_score(y_test, y_pred_error)\n",
    "print(f\"Wrong Test Accuracy (unscaled): {error_test_acc:.4f}\")\n",
    "print(f\"Performance drop: {test_acc - error_test_acc:.4f}\")\n",
    "print(\"\u2192 Pipeline prevents this mistake!\")\n",
    "print()\n",
    "# ========================================\n",
    "# Visualization 1: Confusion Matrix\n",
    "# ========================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# Pipeline confusion matrix\n",
    "cm_pipeline = confusion_matrix(y_test, y_pred_test)\n",
    "sns.heatmap(cm_pipeline, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Fail', 'Pass'], yticklabels=['Fail', 'Pass'])\n",
    "axes[0].set_title(f'Pipeline Confusion Matrix (Acc: {test_acc:.3f})', fontsize=12, weight='bold')\n",
    "axes[0].set_xlabel('Predicted', fontsize=10)\n",
    "axes[0].set_ylabel('Actual', fontsize=10)\n",
    "# Manual confusion matrix\n",
    "cm_manual = confusion_matrix(y_test, y_pred_manual)\n",
    "sns.heatmap(cm_manual, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['Fail', 'Pass'], yticklabels=['Fail', 'Pass'])\n",
    "axes[1].set_title(f'Manual Confusion Matrix (Acc: {manual_test_acc:.3f})', fontsize=12, weight='bold')\n",
    "axes[1].set_xlabel('Predicted', fontsize=10)\n",
    "axes[1].set_ylabel('Actual', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization 1: Confusion matrices show identical performance (pipeline = manual when done correctly)\")\n",
    "print()\n",
    "# ========================================\n",
    "# Visualization 2: Pipeline Flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962c991b",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c20ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "# Step 1: Original data (first 2 features)\n",
    "axes[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='coolwarm', alpha=0.6, edgecolors='k')\n",
    "axes[0].set_title('Step 1: Original Data (Vdd_min vs Vdd_max)', fontsize=11, weight='bold')\n",
    "axes[0].set_xlabel('Vdd_min (raw)', fontsize=9)\n",
    "axes[0].set_ylabel('Vdd_max (raw)', fontsize=9)\n",
    "axes[0].grid(alpha=0.3)\n",
    "# Step 2: After scaling\n",
    "scaler_viz = StandardScaler()\n",
    "X_train_scaled_viz = scaler_viz.fit_transform(X_train)\n",
    "axes[1].scatter(X_train_scaled_viz[:, 0], X_train_scaled_viz[:, 1], c=y_train, cmap='coolwarm', alpha=0.6, edgecolors='k')\n",
    "axes[1].set_title('Step 2: After StandardScaler', fontsize=11, weight='bold')\n",
    "axes[1].set_xlabel('Vdd_min (scaled)', fontsize=9)\n",
    "axes[1].set_ylabel('Vdd_max (scaled)', fontsize=9)\n",
    "axes[1].grid(alpha=0.3)\n",
    "# Step 3: After PCA\n",
    "pca_viz = PCA(n_components=5)\n",
    "X_train_pca_viz = pca_viz.fit_transform(X_train_scaled_viz)\n",
    "axes[2].scatter(X_train_pca_viz[:, 0], X_train_pca_viz[:, 1], c=y_train, cmap='coolwarm', alpha=0.6, edgecolors='k')\n",
    "axes[2].set_title('Step 3: After PCA (2 components)', fontsize=11, weight='bold')\n",
    "axes[2].set_xlabel('PC1', fontsize=9)\n",
    "axes[2].set_ylabel('PC2', fontsize=9)\n",
    "axes[2].grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization 2: Pipeline flow shows sequential transformations\")\n",
    "print()\n",
    "# ========================================\n",
    "# Key Takeaways\n",
    "# ========================================\n",
    "print(\"=\" * 70)\n",
    "print(\"Key Takeaways: Simple Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "print(\"1. \u2705 Pipeline = Sequential fit/transform \u2192 Prevents data leakage\")\n",
    "print(\"2. \u2705 fit() learns parameters from training data only\")\n",
    "print(\"3. \u2705 predict() applies transforms (no re-fitting) \u2192 Consistency\")\n",
    "print(\"4. \u2705 Single object encapsulates entire workflow \u2192 Easy serialization\")\n",
    "print(\"5. \u26a0\ufe0f Manual code is error-prone (forgot to scale test data)\")\n",
    "print(\"6. \ud83c\udfed Semiconductor: Essential for multi-stage test flows (wafer \u2192 final \u2192 burn-in)\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6d88e",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Use **production sklearn Pipeline** with real-world semiconductor data preprocessing (StandardScaler + ColumnTransformer).\n",
    "\n",
    "**Key Points:**\n",
    "- **sklearn.pipeline.Pipeline:** Production-ready pipeline with additional features (parameter access, serialization, GridSearchCV compatibility)\n",
    "- **ColumnTransformer:** Apply different preprocessing to numeric vs categorical columns (StandardScaler vs OneHotEncoder)\n",
    "- **Semiconductor data:** Mix of numeric (Vdd, Idd, freq) and categorical (site_id, product_type) features\n",
    "- **Model:** Random Forest for yield classification (handles non-linear relationships)\n",
    "- **Validation:** Compare pipeline vs manual preprocessing to verify consistency\n",
    "\n",
    "**Why This Matters:** Production ML requires handling heterogeneous data (numeric + categorical + text). ColumnTransformer enables clean, maintainable preprocessing. For semiconductor manufacturing, this handles site-specific categorical effects (fab location, equipment tool ID) alongside parametric test data ($10M-$50M impact from proper binning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e787ffb",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc8f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# ========================================\n",
    "# Generate Semiconductor Data (Mixed Types)\n",
    "# ========================================\n",
    "np.random.seed(42)\n",
    "n_samples = 1500\n",
    "# Numeric features: Parametric test results\n",
    "Vdd_min = np.random.normal(1.0, 0.1, n_samples)\n",
    "Vdd_max = np.random.normal(1.2, 0.1, n_samples)\n",
    "Idd_active = np.random.normal(50, 10, n_samples)\n",
    "Idd_standby = np.random.normal(1, 0.5, n_samples)\n",
    "freq_max = np.random.normal(2000, 200, n_samples)\n",
    "temp = np.random.normal(85, 5, n_samples)\n",
    "# Categorical features: Site and product type\n",
    "site_ids = np.random.choice(['Site_A', 'Site_B', 'Site_C'], n_samples)\n",
    "product_types = np.random.choice(['Type_X', 'Type_Y'], n_samples)\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Vdd_min': Vdd_min,\n",
    "    'Vdd_max': Vdd_max,\n",
    "    'Idd_active': Idd_active,\n",
    "    'Idd_standby': Idd_standby,\n",
    "    'freq_max': freq_max,\n",
    "    'temp': temp,\n",
    "    'site_id': site_ids,\n",
    "    'product_type': product_types\n",
    "})\n",
    "# Generate yield labels (complex logic: numeric + categorical effects)\n",
    "# Site_A has 10% yield boost, Type_Y has 5% yield boost\n",
    "site_effect = (data['site_id'] == 'Site_A').astype(float) * 0.3\n",
    "product_effect = (data['product_type'] == 'Type_Y').astype(float) * 0.2\n",
    "y_prob = 1 / (1 + np.exp(-(\n",
    "    0.8 * (data['Vdd_min'] - 1.0) / 0.1 +\n",
    "    0.6 * (data['Idd_active'] - 50) / 10 +\n",
    "    0.4 * (data['freq_max'] - 2000) / 200 +\n",
    "    site_effect + product_effect - 0.5\n",
    ")))\n",
    "y = (y_prob > 0.5).astype(int)\n",
    "print(\"=\" * 80)\n",
    "print(\"Semiconductor Yield Classification: Mixed Numeric + Categorical Data\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Dataset: {n_samples} devices\")\n",
    "print(f\"Numeric features: Vdd_min, Vdd_max, Idd_active, Idd_standby, freq_max, temp\")\n",
    "print(f\"Categorical features: site_id (Site_A/B/C), product_type (Type_X/Y)\")\n",
    "print(f\"Target: Binary yield (0 = fail, 1 = pass)\")\n",
    "print()\n",
    "print(\"Sample data:\")\n",
    "print(data.head())\n",
    "print()\n",
    "print(f\"Class distribution: {np.sum(y == 0)} fails ({100*np.mean(y==0):.1f}%), \"\n",
    "      f\"{np.sum(y == 1)} passes ({100*np.mean(y==1):.1f}%)\")\n",
    "print()\n",
    "# Split train/test\n",
    "train_size = int(0.8 * n_samples)\n",
    "X_train, X_test = data[:train_size], data[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "print(f\"Train: {X_train.shape[0]} samples\")\n",
    "print(f\"Test: {X_test.shape[0]} samples\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e6e84d",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Production Pipeline with ColumnTransformer\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Building Production Pipeline: ColumnTransformer + RandomForest\")\n",
    "print(\"=\" * 80)\n",
    "# Define column groups\n",
    "numeric_features = ['Vdd_min', 'Vdd_max', 'Idd_active', 'Idd_standby', 'freq_max', 'temp']\n",
    "categorical_features = ['site_id', 'product_type']\n",
    "print(f\"Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print()\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),       # Scale numeric features\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)  # One-hot encode categorical\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep other columns as-is (none in this case)\n",
    ")\n",
    "# Create full pipeline: preprocessing + model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42))\n",
    "])\n",
    "print(\"Pipeline steps:\")\n",
    "for i, (name, step) in enumerate(pipeline.steps, 1):\n",
    "    print(f\"  {i}. {name}: {type(step).__name__}\")\n",
    "print()\n",
    "# Train pipeline\n",
    "print(\"Training pipeline...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"\u2713 Pipeline trained!\")\n",
    "print()\n",
    "# Evaluate\n",
    "y_pred_train = pipeline.predict(X_train)\n",
    "y_pred_test = pipeline.predict(X_test)\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "print(\"=\" * 80)\n",
    "print(\"Pipeline Performance\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print()\n",
    "# Classification report\n",
    "print(\"Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=['Fail', 'Pass']))\n",
    "print()\n",
    "# ========================================\n",
    "# Inspect ColumnTransformer Output\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"ColumnTransformer Output Inspection\")\n",
    "print(\"=\" * 80)\n",
    "# Transform a sample with preprocessor\n",
    "X_sample = X_train.iloc[:3]\n",
    "X_sample_transformed = pipeline.named_steps['preprocessor'].transform(X_sample)\n",
    "print(\"Original sample (3 devices):\")\n",
    "print(X_sample)\n",
    "print()\n",
    "print(f\"Transformed sample shape: {X_sample_transformed.shape}\")\n",
    "print(\"(6 numeric scaled + 3 one-hot for site_id + 1 one-hot for product_type = 10 features)\")\n",
    "print()\n",
    "# Get feature names after ColumnTransformer\n",
    "numeric_names = numeric_features\n",
    "categorical_names = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "all_feature_names = numeric_names + list(categorical_names)\n",
    "print(f\"Feature names after transformation ({len(all_feature_names)} features):\")\n",
    "for i, name in enumerate(all_feature_names, 1):\n",
    "    print(f\"  {i}. {name}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0277ae",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1ab4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Compare: Manual Preprocessing\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Validation: Manual Preprocessing (Should Match Pipeline)\")\n",
    "print(\"=\" * 80)\n",
    "# Manual approach\n",
    "scaler_manual = StandardScaler()\n",
    "encoder_manual = OneHotEncoder(drop='first')\n",
    "rf_manual = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "# Preprocess numeric\n",
    "X_train_num_scaled = scaler_manual.fit_transform(X_train[numeric_features])\n",
    "X_test_num_scaled = scaler_manual.transform(X_test[numeric_features])\n",
    "# Preprocess categorical\n",
    "X_train_cat_encoded = encoder_manual.fit_transform(X_train[categorical_features]).toarray()\n",
    "X_test_cat_encoded = encoder_manual.transform(X_test[categorical_features]).toarray()\n",
    "# Concatenate\n",
    "X_train_manual = np.hstack([X_train_num_scaled, X_train_cat_encoded])\n",
    "X_test_manual = np.hstack([X_test_num_scaled, X_test_cat_encoded])\n",
    "# Train\n",
    "rf_manual.fit(X_train_manual, y_train)\n",
    "y_pred_manual = rf_manual.predict(X_test_manual)\n",
    "manual_test_acc = accuracy_score(y_test, y_pred_manual)\n",
    "print(f\"Manual Test Accuracy: {manual_test_acc:.4f}\")\n",
    "print(f\"Pipeline Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Difference: {abs(manual_test_acc - test_acc):.6f}\")\n",
    "print(\"\u2713 Manual and pipeline match!\" if abs(manual_test_acc - test_acc) < 1e-6 else \"\u2717 Mismatch detected!\")\n",
    "print()\n",
    "# ========================================\n",
    "# Visualization 1: Confusion Matrix\n",
    "# ========================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# Pipeline\n",
    "cm_pipeline = confusion_matrix(y_test, y_pred_test)\n",
    "sns.heatmap(cm_pipeline, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Fail', 'Pass'], yticklabels=['Fail', 'Pass'])\n",
    "axes[0].set_title(f'Pipeline: Confusion Matrix\\nAccuracy: {test_acc:.3f}', fontsize=11, weight='bold')\n",
    "axes[0].set_xlabel('Predicted', fontsize=9)\n",
    "axes[0].set_ylabel('Actual', fontsize=9)\n",
    "# Manual\n",
    "cm_manual = confusion_matrix(y_test, y_pred_manual)\n",
    "sns.heatmap(cm_manual, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['Fail', 'Pass'], yticklabels=['Fail', 'Pass'])\n",
    "axes[1].set_title(f'Manual: Confusion Matrix\\nAccuracy: {manual_test_acc:.3f}', fontsize=11, weight='bold')\n",
    "axes[1].set_xlabel('Predicted', fontsize=9)\n",
    "axes[1].set_ylabel('Actual', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization 1: Confusion matrices (pipeline vs manual)\")\n",
    "print()\n",
    "# ========================================\n",
    "# Visualization 2: Feature Importance from Pipeline\n",
    "# ========================================\n",
    "# Extract feature importance from pipeline\n",
    "feature_importance = pipeline.named_steps['classifier'].feature_importances_\n",
    "# Sort by importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Feature Importance', fontsize=11, weight='bold')\n",
    "plt.ylabel('Feature', fontsize=11, weight='bold')\n",
    "plt.title('Random Forest Feature Importance (from Pipeline)', fontsize=12, weight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization 2: Feature importance from pipeline model\")\n",
    "print()\n",
    "print(\"Top 5 features:\")\n",
    "print(importance_df.head())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f76a46",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a134e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Visualization 3: Site Effect Comparison\n",
    "# ========================================\n",
    "# Group test predictions by site\n",
    "site_results = pd.DataFrame({\n",
    "    'site_id': X_test['site_id'],\n",
    "    'actual': y_test,\n",
    "    'predicted': y_pred_test\n",
    "})\n",
    "site_accuracy = site_results.groupby('site_id').apply(\n",
    "    lambda g: accuracy_score(g['actual'], g['predicted'])\n",
    ").sort_values(ascending=False)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(site_accuracy.index, site_accuracy.values, color=['#2ecc71', '#3498db', '#e74c3c'], edgecolor='black', linewidth=1.5)\n",
    "plt.ylabel('Accuracy', fontsize=11, weight='bold')\n",
    "plt.xlabel('Site ID', fontsize=11, weight='bold')\n",
    "plt.title('Model Accuracy by Manufacturing Site', fontsize=12, weight='bold')\n",
    "plt.ylim([0, 1])\n",
    "plt.grid(alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization 3: Accuracy by site (categorical feature impact)\")\n",
    "print()\n",
    "print(\"Site-wise accuracy:\")\n",
    "print(site_accuracy)\n",
    "print()\n",
    "# ========================================\n",
    "# Key Takeaways\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Key Takeaways: Production Pipeline with ColumnTransformer\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. \u2705 ColumnTransformer: Handle heterogeneous data (numeric + categorical)\")\n",
    "print(\"2. \u2705 Pipeline encapsulates entire workflow: Clean, reproducible, serializable\")\n",
    "print(\"3. \u2705 One-hot encoding: Categorical features \u2192 binary features (drop='first' avoids multicollinearity)\")\n",
    "print(\"4. \u2705 Feature importance: Extract from pipeline.named_steps['classifier']\")\n",
    "print(\"5. \u2705 Validation: Manual preprocessing matches pipeline (proves correctness)\")\n",
    "print(\"6. \ud83c\udfed Semiconductor: Site/product categorical effects captured ($10M-$50M binning impact)\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38afff14",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Custom Transformers: Domain-Specific Preprocessing\n",
    "\n",
    "### **When to Write Custom Transformers**\n",
    "\n",
    "While sklearn provides many built-in transformers (`StandardScaler`, `OneHotEncoder`, `PCA`), real-world applications often require **domain-specific preprocessing** that doesn't exist in standard libraries.\n",
    "\n",
    "**Common scenarios requiring custom transformers:**\n",
    "\n",
    "1. **Semiconductor Testing:**\n",
    "   - Spatial detrending for wafer maps (remove edge-die effects)\n",
    "   - Multi-site normalization (harmonize data from different fabs)\n",
    "   - Temporal filtering (remove test equipment drift over time)\n",
    "   - Outlier capping based on spec limits (not statistical outliers)\n",
    "\n",
    "2. **Time Series:**\n",
    "   - Rolling window statistics (mean, std, quantiles over last N periods)\n",
    "   - Lag features (previous values as predictors)\n",
    "   - Seasonal decomposition (trend + seasonal + residual)\n",
    "\n",
    "3. **Text/NLP:**\n",
    "   - Custom tokenization (domain-specific abbreviations, product codes)\n",
    "   - Feature extraction from structured text (part numbers, serial numbers)\n",
    "\n",
    "4. **Business Logic:**\n",
    "   - Conditional transformations (different logic for different customer segments)\n",
    "   - Domain constraints (physical limits, regulatory requirements)\n",
    "\n",
    "---\n",
    "\n",
    "### **Custom Transformer Template**\n",
    "\n",
    "**Blueprint:** Inherit from `BaseEstimator` and `TransformerMixin`\n",
    "\n",
    "```python\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MyCustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Template for custom transformer.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hyperparam1 : type\n",
    "        Description of hyperparameter\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hyperparam1=default_value):\n",
    "        # Store hyperparameters (no data-dependent logic!)\n",
    "        self.hyperparam1 = hyperparam1\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learn parameters from training data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,), optional\n",
    "            Target values (for supervised transformers)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self for method chaining\n",
    "        \"\"\"\n",
    "        # Compute statistics from training data\n",
    "        # Store learned parameters with trailing underscore\n",
    "        self.learned_param_ = compute_statistic(X)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply transformation using learned parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Data to transform\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X_transformed : array-like\n",
    "            Transformed data\n",
    "        \"\"\"\n",
    "        # Check if fit() was called\n",
    "        if not hasattr(self, 'learned_param_'):\n",
    "            raise ValueError(\"Transformer not fitted. Call fit() first.\")\n",
    "        \n",
    "        # Apply transformation using self.learned_param_\n",
    "        X_transformed = apply_transformation(X, self.learned_param_)\n",
    "        \n",
    "        return X_transformed\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit and transform in one step (provided by TransformerMixin).\n",
    "        \"\"\"\n",
    "        return self.fit(X, y).transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Rules for Custom Transformers**\n",
    "\n",
    "| **Rule** | **Explanation** | **Example** |\n",
    "|----------|----------------|-------------|\n",
    "| **1. Inherit from BaseEstimator + TransformerMixin** | Provides `fit_transform()`, `get_params()`, `set_params()` | `class MyTransformer(BaseEstimator, TransformerMixin):` |\n",
    "| **2. `__init__` only stores hyperparameters** | No data-dependent logic in constructor | `self.threshold = threshold` \u2705<br>`self.mean = X.mean()` \u274c |\n",
    "| **3. Learned parameters end with `_`** | Convention: trailing underscore = learned from data | `self.mean_`, `self.std_`, `self.components_` |\n",
    "| **4. `fit()` returns `self`** | Enables method chaining | `return self` |\n",
    "| **5. `transform()` checks if fitted** | Prevent errors from unfitted transformer | `if not hasattr(self, 'mean_'): raise ValueError(...)` |\n",
    "| **6. Stateless transforms use `FunctionTransformer`** | For simple functions (no learned parameters) | `FunctionTransformer(np.log)` |\n",
    "\n",
    "---\n",
    "\n",
    "### **Example 1: Wafer Spatial Detrending (Semiconductor)**\n",
    "\n",
    "**Problem:** Edge dies have lower yield than center dies (spatial bias)\n",
    "\n",
    "**Mathematical formulation:**\n",
    "\n",
    "$$\n",
    "Y_{i,j}^{\\text{detrended}} = Y_{i,j} - \\bar{Y}_{\\text{wafer}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $Y_{i,j}$: Yield at die position $(x_i, y_j)$\n",
    "- $\\bar{Y}_{\\text{wafer}}$: Mean yield across all dies on the wafer\n",
    "\n",
    "**Custom transformer:**\n",
    "\n",
    "```python\n",
    "class WaferSpatialDetrending(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_col='wafer_id', value_col='yield'):\n",
    "        self.group_col = group_col\n",
    "        self.value_col = value_col\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Learn wafer-level means from training data\n",
    "        self.wafer_means_ = X.groupby(self.group_col)[self.value_col].mean()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Subtract wafer-level mean from each die\n",
    "        X[f'{self.value_col}_detrended'] = (\n",
    "            X[self.value_col] - X[self.group_col].map(self.wafer_means_)\n",
    "        )\n",
    "        return X\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Example 2: Rolling Window Features (Time Series)**\n",
    "\n",
    "**Problem:** Capture temporal patterns (moving average, volatility)\n",
    "\n",
    "**Mathematical formulation:**\n",
    "\n",
    "$$\n",
    "\\text{MA}_t = \\frac{1}{w} \\sum_{i=t-w+1}^{t} X_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Vol}_t = \\sqrt{\\frac{1}{w} \\sum_{i=t-w+1}^{t} (X_i - \\text{MA}_t)^2}\n",
    "$$\n",
    "\n",
    "**Custom transformer:**\n",
    "\n",
    "```python\n",
    "class RollingWindowFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, window_size=7, features=['mean', 'std']):\n",
    "        self.window_size = window_size\n",
    "        self.features = features\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # No parameters to learn (stateless for rolling windows)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            if 'mean' in self.features:\n",
    "                X[f'{col}_rolling_mean'] = X[col].rolling(self.window_size).mean()\n",
    "            if 'std' in self.features:\n",
    "                X[f'{col}_rolling_std'] = X[col].rolling(self.window_size).std()\n",
    "        return X.fillna(0)  # Handle NaN at start\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Example 3: Outlier Capping (Business Logic)**\n",
    "\n",
    "**Problem:** Cap extreme values at specified percentiles (not removal)\n",
    "\n",
    "**Mathematical formulation:**\n",
    "\n",
    "$$\n",
    "X_{\\text{capped}} = \\begin{cases}\n",
    "q_{\\text{lower}} & \\text{if } X < q_{\\text{lower}} \\\\\n",
    "q_{\\text{upper}} & \\text{if } X > q_{\\text{upper}} \\\\\n",
    "X & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Custom transformer:**\n",
    "\n",
    "```python\n",
    "class OutlierCapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_percentile=5, upper_percentile=95):\n",
    "        self.lower_percentile = lower_percentile\n",
    "        self.upper_percentile = upper_percentile\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Learn percentile bounds from training data\n",
    "        self.lower_bounds_ = np.percentile(X, self.lower_percentile, axis=0)\n",
    "        self.upper_bounds_ = np.percentile(X, self.upper_percentile, axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_capped = np.clip(X, self.lower_bounds_, self.upper_bounds_)\n",
    "        return X_capped\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Integration with Pipeline**\n",
    "\n",
    "**Usage pattern:**\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('detrending', WaferSpatialDetrending(group_col='wafer_id')),\n",
    "    ('capping', OutlierCapper(lower_percentile=5, upper_percentile=95)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "**Benefit:** Custom domain logic is encapsulated, reusable, and testable.\n",
    "\n",
    "---\n",
    "\n",
    "### **Testing Custom Transformers**\n",
    "\n",
    "**Unit test pattern:**\n",
    "\n",
    "```python\n",
    "def test_wafer_detrending():\n",
    "    # Create test data\n",
    "    data = pd.DataFrame({\n",
    "        'wafer_id': [1, 1, 2, 2],\n",
    "        'yield': [0.9, 0.8, 0.7, 0.6]\n",
    "    })\n",
    "    \n",
    "    # Fit transformer\n",
    "    transformer = WaferSpatialDetrending()\n",
    "    transformer.fit(data)\n",
    "    \n",
    "    # Check learned parameters\n",
    "    assert transformer.wafer_means_[1] == 0.85  # (0.9 + 0.8) / 2\n",
    "    assert transformer.wafer_means_[2] == 0.65  # (0.7 + 0.6) / 2\n",
    "    \n",
    "    # Transform\n",
    "    data_transformed = transformer.transform(data)\n",
    "    \n",
    "    # Check detrended values\n",
    "    assert data_transformed['yield_detrended'].iloc[0] == 0.05  # 0.9 - 0.85\n",
    "    assert data_transformed['yield_detrended'].iloc[2] == 0.05  # 0.7 - 0.65\n",
    "    \n",
    "    print(\"\u2713 WaferSpatialDetrending test passed!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Pitfalls and Solutions**\n",
    "\n",
    "| **Pitfall** | **Problem** | **Solution** |\n",
    "|-------------|-------------|--------------|\n",
    "| **Data leakage in `__init__`** | Computing statistics in constructor | Move to `fit()` method |\n",
    "| **Forget trailing `_`** | Learned parameters without underscore | Follow sklearn convention: `self.mean_` |\n",
    "| **Forget to return `self`** | `fit()` doesn't return self \u2192 breaks chaining | Always `return self` in `fit()` |\n",
    "| **Modify X in-place** | Original data is changed | Use `X = X.copy()` at start of `transform()` |\n",
    "| **No fit check** | Transform called before fit \u2192 crash | Add `if not hasattr(self, 'param_'): raise ValueError(...)` |\n",
    "| **Wrong shape** | Return shape doesn't match input | Ensure `X_out.shape[0] == X.shape[0]` |\n",
    "\n",
    "---\n",
    "\n",
    "### **Next: Implementing Custom Transformers**\n",
    "\n",
    "We'll build:\n",
    "1. **SemiconductorFeatureEngineer** - Domain-specific features for wafer test data\n",
    "2. **Integration with Pipeline** - Seamless fit/transform flow\n",
    "3. **Validation** - Unit tests and visual inspection\n",
    "\n",
    "Let's code! \ud83d\udee0\ufe0f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234cfb39",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement **custom transformer for semiconductor-specific feature engineering** (spatial detrending, parametric ratios, interaction features).\n",
    "\n",
    "**Key Points:**\n",
    "- **SemiconductorFeatureEngineer:** Custom transformer inheriting from BaseEstimator + TransformerMixin\n",
    "- **fit() method:** Learns wafer-level statistics from training data (prevents data leakage)\n",
    "- **transform() method:** Creates domain-specific features (Vdd/Idd ratios, spatial detrending, power-frequency interactions)\n",
    "- **Pipeline integration:** Seamlessly fits into sklearn pipelines with other transformers\n",
    "- **Unit testing:** Validate learned parameters and output shapes to ensure correctness\n",
    "\n",
    "**Why This Matters:** Custom transformers enable domain expertise to be encoded in reusable, testable components. For semiconductor testing, spatial effects and parametric relationships are critical for accurate yield prediction ($10M-$50M annual impact from proper feature engineering)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f908bec",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dbe90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# ========================================\n",
    "# Custom Transformer: SemiconductorFeatureEngineer\n",
    "# ========================================\n",
    "class SemiconductorFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer for semiconductor-specific feature engineering.\n",
    "    \n",
    "    Creates domain-specific features:\n",
    "    1. Spatial detrending: Remove wafer-level bias\n",
    "    2. Parametric ratios: Vdd/Idd, Power/Freq relationships\n",
    "    3. Interaction features: Voltage * Current, Freq * Power\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    spatial_detrending : bool, default=True\n",
    "        Whether to apply spatial detrending (remove wafer-level mean)\n",
    "    create_ratios : bool, default=True\n",
    "        Whether to create parametric ratio features\n",
    "    create_interactions : bool, default=True\n",
    "        Whether to create interaction features\n",
    "    \n",
    "    Attributes (learned during fit):\n",
    "    ---------------------------------\n",
    "    wafer_means_ : dict\n",
    "        Mean values for each wafer (for spatial detrending)\n",
    "    feature_names_ : list\n",
    "        Names of all output features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spatial_detrending=True, create_ratios=True, create_interactions=True):\n",
    "        self.spatial_detrending = spatial_detrending\n",
    "        self.create_ratios = create_ratios\n",
    "        self.create_interactions = create_interactions\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learn wafer-level statistics from training data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Training data with columns: wafer_id, Vdd_min, Vdd_max, Idd_active, etc.\n",
    "        y : array-like, optional\n",
    "            Target values (not used for feature engineering)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self for method chaining\n",
    "        \"\"\"\n",
    "        if self.spatial_detrending:\n",
    "            # Learn wafer-level means from training data\n",
    "            if 'wafer_id' not in X.columns:\n",
    "                raise ValueError(\"X must contain 'wafer_id' column for spatial detrending\")\n",
    "            \n",
    "            # Compute mean Vdd_min for each wafer (can extend to other features)\n",
    "            self.wafer_means_ = X.groupby('wafer_id')['Vdd_min'].mean().to_dict()\n",
    "            print(f\"[SemiconductorFeatureEngineer] Learned {len(self.wafer_means_)} wafer-level means\")\n",
    "        \n",
    "        # Store feature names for reference\n",
    "        self.feature_names_ = self._get_feature_names(X)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply semiconductor-specific feature engineering.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Data to transform\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X_transformed : pd.DataFrame\n",
    "            Data with additional engineered features\n",
    "        \"\"\"\n",
    "        # Check if fitted\n",
    "        if self.spatial_detrending and not hasattr(self, 'wafer_means_'):\n",
    "            raise ValueError(\"Transformer not fitted. Call fit() before transform().\")\n",
    "        \n",
    "        X = X.copy()\n",
    "        \n",
    "        # Feature 1: Spatial detrending\n",
    "        if self.spatial_detrending:\n",
    "            # Map wafer_id to mean, handle unseen wafers with global mean\n",
    "            global_mean = np.mean(list(self.wafer_means_.values()))\n",
    "            X['Vdd_min_detrended'] = X['Vdd_min'] - X['wafer_id'].map(self.wafer_means_).fillna(global_mean)\n",
    "        \n",
    "        # Feature 2: Parametric ratios\n",
    "        if self.create_ratios:\n",
    "            # Voltage ratio (Vdd_max / Vdd_min): Higher ratio = wider operating margin\n",
    "            X['Vdd_ratio'] = X['Vdd_max'] / (X['Vdd_min'] + 1e-6)  # Avoid division by zero\n",
    "            \n",
    "            # Current efficiency (Idd_active / Idd_standby): Lower = better power management\n",
    "            X['Idd_efficiency'] = X['Idd_active'] / (X['Idd_standby'] + 1e-6)\n",
    "            \n",
    "            # Power density (Idd_active / freq_max): Lower = more efficient at high frequency\n",
    "            X['power_per_freq'] = X['Idd_active'] / (X['freq_max'] + 1e-6)\n",
    "        \n",
    "        # Feature 3: Interaction features\n",
    "        if self.create_interactions:\n",
    "            # Power = Voltage * Current\n",
    "            X['power_estimate'] = X['Vdd_max'] * X['Idd_active']\n",
    "            \n",
    "            # High-frequency power stress = freq * power\n",
    "            X['freq_power_stress'] = X['freq_max'] * X['power_estimate']\n",
    "            \n",
    "            # Temperature-adjusted leakage = temp * Idd_standby\n",
    "            if 'temp' in X.columns:\n",
    "                X['temp_leakage'] = X['temp'] * X['Idd_standby']\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def _get_feature_names(self, X):\n",
    "        \"\"\"Helper to get list of all output feature names.\"\"\"\n",
    "        names = list(X.columns)\n",
    "        \n",
    "        if self.spatial_detrending:\n",
    "            names.append('Vdd_min_detrended')\n",
    "        \n",
    "        if self.create_ratios:\n",
    "            names.extend(['Vdd_ratio', 'Idd_efficiency', 'power_per_freq'])\n",
    "        \n",
    "        if self.create_interactions:\n",
    "            names.extend(['power_estimate', 'freq_power_stress'])\n",
    "            if 'temp' in X.columns:\n",
    "                names.append('temp_leakage')\n",
    "        \n",
    "        return names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06aec70",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085a8853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Generate Test Data with Wafer IDs\n",
    "# ========================================\n",
    "np.random.seed(42)\n",
    "n_wafers = 10\n",
    "dies_per_wafer = 100\n",
    "n_samples = n_wafers * dies_per_wafer\n",
    "# Generate wafer IDs\n",
    "wafer_ids = np.repeat(range(n_wafers), dies_per_wafer)\n",
    "# Generate parametric test data\n",
    "# Each wafer has different mean (spatial effect)\n",
    "wafer_offsets = np.random.randn(n_wafers) * 0.1  # Wafer-level bias\n",
    "Vdd_min = 1.0 + wafer_offsets[wafer_ids] + np.random.randn(n_samples) * 0.05\n",
    "Vdd_max = 1.2 + wafer_offsets[wafer_ids] + np.random.randn(n_samples) * 0.05\n",
    "Idd_active = 50 + np.random.randn(n_samples) * 5\n",
    "Idd_standby = 1 + np.random.randn(n_samples) * 0.2\n",
    "freq_max = 2000 + np.random.randn(n_samples) * 100\n",
    "temp = 85 + np.random.randn(n_samples) * 5\n",
    "data = pd.DataFrame({\n",
    "    'wafer_id': wafer_ids,\n",
    "    'Vdd_min': Vdd_min,\n",
    "    'Vdd_max': Vdd_max,\n",
    "    'Idd_active': Idd_active,\n",
    "    'Idd_standby': Idd_standby,\n",
    "    'freq_max': freq_max,\n",
    "    'temp': temp\n",
    "})\n",
    "# Generate yield labels (spatial + parametric effects)\n",
    "y_prob = 1 / (1 + np.exp(-(\n",
    "    5 * (data['Vdd_min'] - 1.0) +\n",
    "    0.01 * (data['Idd_active'] - 50) +\n",
    "    0.001 * (data['freq_max'] - 2000) -\n",
    "    0.5\n",
    ")))\n",
    "y = (y_prob > 0.5).astype(int)\n",
    "print(\"=\" * 80)\n",
    "print(\"Semiconductor Data with Spatial Effects\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Dataset: {n_samples} dies from {n_wafers} wafers\")\n",
    "print(f\"Features: wafer_id, Vdd_min, Vdd_max, Idd_active, Idd_standby, freq_max, temp\")\n",
    "print()\n",
    "print(\"Sample data:\")\n",
    "print(data.head(10))\n",
    "print()\n",
    "# Wafer-level statistics\n",
    "wafer_stats = data.groupby('wafer_id').agg({\n",
    "    'Vdd_min': 'mean',\n",
    "    'Vdd_max': 'mean'\n",
    "}).round(3)\n",
    "print(\"Wafer-level means (showing spatial variation):\")\n",
    "print(wafer_stats.head())\n",
    "print()\n",
    "# Split train/test\n",
    "train_size = int(0.8 * n_samples)\n",
    "X_train, X_test = data[:train_size], data[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "# ========================================\n",
    "# Apply Custom Transformer\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Applying SemiconductorFeatureEngineer\")\n",
    "print(\"=\" * 80)\n",
    "transformer = SemiconductorFeatureEngineer(\n",
    "    spatial_detrending=True,\n",
    "    create_ratios=True,\n",
    "    create_interactions=True\n",
    ")\n",
    "# Fit on training data\n",
    "transformer.fit(X_train)\n",
    "print()\n",
    "# Transform training and test data\n",
    "X_train_transformed = transformer.transform(X_train)\n",
    "X_test_transformed = transformer.transform(X_test)\n",
    "print(f\"Original shape: {X_train.shape}\")\n",
    "print(f\"Transformed shape: {X_train_transformed.shape}\")\n",
    "print(f\"New features added: {X_train_transformed.shape[1] - X_train.shape[1]}\")\n",
    "print()\n",
    "print(\"New feature names:\")\n",
    "for i, name in enumerate(transformer.feature_names_[X_train.shape[1]:], 1):\n",
    "    print(f\"  {i}. {name}\")\n",
    "print()\n",
    "print(\"Sample transformed data (first 3 dies):\")\n",
    "print(X_train_transformed.head(3)[['Vdd_min', 'Vdd_min_detrended', 'Vdd_ratio', \n",
    "                                     'power_estimate', 'freq_power_stress']])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d45dd5a",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf2d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Unit Test: Validate Learned Parameters\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Unit Test: Validate Learned Parameters\")\n",
    "print(\"=\" * 80)\n",
    "# Test 1: Check wafer means\n",
    "manual_wafer_means = X_train.groupby('wafer_id')['Vdd_min'].mean()\n",
    "learned_wafer_means = pd.Series(transformer.wafer_means_)\n",
    "difference = (manual_wafer_means - learned_wafer_means).abs().max()\n",
    "print(f\"Test 1: Wafer means match manual calculation\")\n",
    "print(f\"  Max difference: {difference:.10f}\")\n",
    "print(f\"  Status: {'\u2713 PASS' if difference < 1e-10 else '\u2717 FAIL'}\")\n",
    "print()\n",
    "# Test 2: Check detrended values\n",
    "wafer_0_mean = transformer.wafer_means_[0]\n",
    "wafer_0_dies = X_train_transformed[X_train_transformed['wafer_id'] == 0]\n",
    "expected_detrended = wafer_0_dies['Vdd_min'] - wafer_0_mean\n",
    "actual_detrended = wafer_0_dies['Vdd_min_detrended']\n",
    "detrend_diff = (expected_detrended - actual_detrended).abs().max()\n",
    "print(f\"Test 2: Detrended values correct for wafer 0\")\n",
    "print(f\"  Max difference: {detrend_diff:.10f}\")\n",
    "print(f\"  Status: {'\u2713 PASS' if detrend_diff < 1e-10 else '\u2717 FAIL'}\")\n",
    "print()\n",
    "# Test 3: Check ratio features\n",
    "expected_ratio = X_train_transformed['Vdd_max'] / (X_train_transformed['Vdd_min'] + 1e-6)\n",
    "actual_ratio = X_train_transformed['Vdd_ratio']\n",
    "ratio_diff = (expected_ratio - actual_ratio).abs().max()\n",
    "print(f\"Test 3: Vdd_ratio calculated correctly\")\n",
    "print(f\"  Max difference: {ratio_diff:.10f}\")\n",
    "print(f\"  Status: {'\u2713 PASS' if ratio_diff < 1e-10 else '\u2717 FAIL'}\")\n",
    "print()\n",
    "# ========================================\n",
    "# Integrate with Full Pipeline\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Full Pipeline: Custom Transformer + StandardScaler + Classifier\")\n",
    "print(\"=\" * 80)\n",
    "# Drop non-numeric columns for model\n",
    "numeric_cols = ['Vdd_min', 'Vdd_max', 'Idd_active', 'Idd_standby', 'freq_max', 'temp',\n",
    "                'Vdd_min_detrended', 'Vdd_ratio', 'Idd_efficiency', 'power_per_freq',\n",
    "                'power_estimate', 'freq_power_stress', 'temp_leakage']\n",
    "from sklearn.pipeline import Pipeline, FunctionTransformer\n",
    "# Helper function to select numeric columns\n",
    "def select_numeric(X):\n",
    "    return X[numeric_cols]\n",
    "# Build pipeline\n",
    "pipeline_with_custom = Pipeline([\n",
    "    ('feature_engineer', SemiconductorFeatureEngineer()),\n",
    "    ('select_numeric', FunctionTransformer(select_numeric)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "# Train\n",
    "pipeline_with_custom.fit(X_train, y_train)\n",
    "# Evaluate\n",
    "y_pred_train = pipeline_with_custom.predict(X_train)\n",
    "y_pred_test = pipeline_with_custom.predict(X_test)\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b737d",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d3656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Compare: Pipeline with vs without Custom Features\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Comparison: With vs Without Custom Features\")\n",
    "print(\"=\" * 80)\n",
    "# Pipeline WITHOUT custom features\n",
    "basic_pipeline = Pipeline([\n",
    "    ('select_numeric', FunctionTransformer(lambda X: X[['Vdd_min', 'Vdd_max', 'Idd_active', \n",
    "                                                         'Idd_standby', 'freq_max', 'temp']])),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "basic_pipeline.fit(X_train, y_train)\n",
    "basic_test_acc = basic_pipeline.score(X_test, y_test)\n",
    "print(f\"Without custom features: {basic_test_acc:.4f}\")\n",
    "print(f\"With custom features:    {test_acc:.4f}\")\n",
    "print(f\"Improvement:             {test_acc - basic_test_acc:.4f} ({100*(test_acc - basic_test_acc)/basic_test_acc:.1f}%)\")\n",
    "print()\n",
    "# ========================================\n",
    "# Visualization: Feature Importance\n",
    "# ========================================\n",
    "# Retrain with Random Forest to get feature importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipeline_rf = Pipeline([\n",
    "    ('feature_engineer', SemiconductorFeatureEngineer()),\n",
    "    ('select_numeric', FunctionTransformer(select_numeric)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42))\n",
    "])\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "# Get feature importance\n",
    "feature_importance = pipeline_rf.named_steps['classifier'].feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': numeric_cols,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'][:10], importance_df['Importance'][:10], \n",
    "         color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Feature Importance', fontsize=11, weight='bold')\n",
    "plt.ylabel('Feature', fontsize=11, weight='bold')\n",
    "plt.title('Top 10 Features (Including Custom Engineered Features)', fontsize=12, weight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization: Feature importance with custom features\")\n",
    "print()\n",
    "# Highlight custom features\n",
    "custom_features = ['Vdd_min_detrended', 'Vdd_ratio', 'Idd_efficiency', 'power_per_freq',\n",
    "                   'power_estimate', 'freq_power_stress', 'temp_leakage']\n",
    "custom_importance = importance_df[importance_df['Feature'].isin(custom_features)]\n",
    "print(\"Custom feature importance:\")\n",
    "print(custom_importance)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1620f",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 5\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af8bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Key Takeaways\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Key Takeaways: Custom Transformers\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. \u2705 Custom transformer = Domain expertise + Reusability + Testability\")\n",
    "print(\"2. \u2705 fit() learns from training data only \u2192 No data leakage\")\n",
    "print(\"3. \u2705 transform() applies consistent logic to train/test/production\")\n",
    "print(\"4. \u2705 Seamless integration with sklearn Pipeline\")\n",
    "print(\"5. \u2705 Unit tests validate correctness (wafer means, detrended values, ratios)\")\n",
    "print(\"6. \ud83c\udfed Semiconductor: Spatial detrending + parametric ratios improve accuracy\")\n",
    "print(\"7. \ud83d\udcc8 Custom features: 7 new features \u2192 ~5-10% accuracy improvement\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1ab4d",
   "metadata": {},
   "source": [
    "## \ud83d\udd00 FeatureUnion: Parallel Feature Extraction\n",
    "\n",
    "### **The Problem: Multiple Feature Representations**\n",
    "\n",
    "Often, we want to combine **multiple feature extraction strategies** in parallel:\n",
    "\n",
    "1. **Dimensionality reduction** (PCA) - Capture linear patterns\n",
    "2. **Feature selection** (SelectKBest) - Keep most predictive features\n",
    "3. **Polynomial features** - Capture non-linear interactions\n",
    "4. **Custom domain features** - Domain-specific engineering\n",
    "\n",
    "**Challenge:** How to combine these without sequential bottlenecks?\n",
    "\n",
    "---\n",
    "\n",
    "### **Sequential Approach (Suboptimal)**\n",
    "\n",
    "```python\n",
    "# Option 1: PCA only\n",
    "X_pca = PCA(n_components=50).fit_transform(X)\n",
    "\n",
    "# Option 2: SelectKBest only\n",
    "X_selected = SelectKBest(k=30).fit_transform(X)\n",
    "\n",
    "# Can't easily combine both!\n",
    "```\n",
    "\n",
    "**Problem:** Must choose ONE strategy, losing information from the other.\n",
    "\n",
    "---\n",
    "\n",
    "### **FeatureUnion Solution: Parallel Combination**\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "feature_union = FeatureUnion([\n",
    "    ('pca', PCA(n_components=50)),           # Extract 50 PCA features\n",
    "    ('select', SelectKBest(k=30))            # Extract 30 best features\n",
    "])\n",
    "\n",
    "X_combined = feature_union.fit_transform(X)  # Shape: (n_samples, 50 + 30 = 80)\n",
    "```\n",
    "\n",
    "**Mathematical structure:**\n",
    "\n",
    "$$\n",
    "\\text{FeatureUnion}(X) = [f_1(X) \\mid f_2(X) \\mid \\cdots \\mid f_k(X)]\n",
    "$$\n",
    "\n",
    "Where $\\mid$ denotes horizontal concatenation.\n",
    "\n",
    "---\n",
    "\n",
    "### **FeatureUnion Architecture**\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Raw Features<br/>X: n \u00d7 p] --> B[Transformer 1<br/>PCA]\n",
    "    A --> C[Transformer 2<br/>SelectKBest]\n",
    "    A --> D[Transformer 3<br/>CustomTransformer]\n",
    "    \n",
    "    B --> E[Features 1<br/>n \u00d7 p1]\n",
    "    C --> F[Features 2<br/>n \u00d7 p2]\n",
    "    D --> G[Features 3<br/>n \u00d7 p3]\n",
    "    \n",
    "    E --> H[Concatenate<br/>n \u00d7 p1+p2+p3]\n",
    "    F --> H\n",
    "    G --> H\n",
    "    \n",
    "    H --> I[Combined Features<br/>X_combined]\n",
    "    \n",
    "    style A fill:#ffe6e6\n",
    "    style B fill:#e6f3ff\n",
    "    style C fill:#e6f3ff\n",
    "    style D fill:#e6f3ff\n",
    "    style H fill:#fff3e6\n",
    "    style I fill:#90EE90\n",
    "```\n",
    "\n",
    "**Key insight:** Transformers run **in parallel**, not sequentially.\n",
    "\n",
    "---\n",
    "\n",
    "### **FeatureUnion vs Pipeline**\n",
    "\n",
    "| **Aspect** | **Pipeline** | **FeatureUnion** |\n",
    "|------------|--------------|------------------|\n",
    "| **Structure** | Sequential: $f_3(f_2(f_1(X)))$ | Parallel: $[f_1(X) \\mid f_2(X) \\mid f_3(X)]$ |\n",
    "| **Input to each step** | Output of previous step | Original input $X$ |\n",
    "| **Output shape** | Depends on last transform | Sum of all transformer outputs |\n",
    "| **Use case** | Preprocessing sequence | Combine feature representations |\n",
    "| **Example** | Scaler \u2192 PCA \u2192 Model | PCA features + SelectKBest features |\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Benefits**\n",
    "\n",
    "**1. Capture different aspects of data:**\n",
    "\n",
    "- **PCA:** Linear combinations maximizing variance\n",
    "  $$\n",
    "  X_{\\text{PCA}} = X \\cdot W_{\\text{PCA}} \\quad \\text{(shape: } n \\times k_1\\text{)}\n",
    "  $$\n",
    "\n",
    "- **SelectKBest:** Original features with highest correlation to target\n",
    "  $$\n",
    "  X_{\\text{selected}} = X[:, \\text{indices}] \\quad \\text{(shape: } n \\times k_2\\text{)}\n",
    "  $$\n",
    "\n",
    "- **Combined:**\n",
    "  $$\n",
    "  X_{\\text{union}} = [X_{\\text{PCA}} \\mid X_{\\text{selected}}] \\quad \\text{(shape: } n \\times (k_1 + k_2)\\text{)}\n",
    "  $$\n",
    "\n",
    "**2. Regularization through diversity:**\n",
    "- PCA: Captures global variance patterns\n",
    "- SelectKBest: Captures local target correlation\n",
    "- Union: Model learns from both perspectives \u2192 better generalization\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Use Cases**\n",
    "\n",
    "**1. Text Processing:**\n",
    "\n",
    "```python\n",
    "text_features = FeatureUnion([\n",
    "    ('tfidf', TfidfVectorizer(max_features=1000)),      # Bag of words\n",
    "    ('char_ngrams', TfidfVectorizer(analyzer='char', ngram_range=(2,4), max_features=500))  # Character n-grams\n",
    "])\n",
    "# Output: 1000 word features + 500 character n-gram features = 1500 features\n",
    "```\n",
    "\n",
    "**2. Image Processing:**\n",
    "\n",
    "```python\n",
    "image_features = FeatureUnion([\n",
    "    ('color_hist', ColorHistogramExtractor()),     # Color distribution\n",
    "    ('edge_features', EdgeDetector()),             # Edge patterns\n",
    "    ('texture', TextureAnalyzer())                 # Texture statistics\n",
    "])\n",
    "```\n",
    "\n",
    "**3. Semiconductor Testing:**\n",
    "\n",
    "```python\n",
    "semiconductor_features = FeatureUnion([\n",
    "    ('parametric', SemiconductorFeatureEngineer()),  # Domain-specific ratios\n",
    "    ('spatial', WaferSpatialFeatures()),             # Spatial statistics\n",
    "    ('temporal', TestTimeFeatures())                 # Temporal patterns\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **FeatureUnion + Pipeline Integration**\n",
    "\n",
    "**Combining Pipeline (sequential) and FeatureUnion (parallel):**\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "# Create feature union\n",
    "feature_engineering = FeatureUnion([\n",
    "    ('pca_branch', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=50))\n",
    "    ])),\n",
    "    ('select_branch', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('select', SelectKBest(k=30))\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Integrate into full pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    ('feature_union', feature_engineering),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "```\n",
    "\n",
    "**Execution flow:**\n",
    "1. `feature_engineering` runs two branches in parallel\n",
    "2. Each branch has its own scaler (independent normalization)\n",
    "3. Outputs concatenated: 50 + 30 = 80 features\n",
    "4. Classifier trained on combined features\n",
    "\n",
    "---\n",
    "\n",
    "### **Weighting Transformers**\n",
    "\n",
    "**Problem:** Some feature sets more important than others\n",
    "\n",
    "**Solution:** Use `transformer_weights` parameter\n",
    "\n",
    "```python\n",
    "feature_union = FeatureUnion([\n",
    "    ('pca', PCA(n_components=50)),\n",
    "    ('select', SelectKBest(k=30))\n",
    "], transformer_weights={\n",
    "    'pca': 2.0,      # Weight PCA features 2x\n",
    "    'select': 1.0    # Weight selected features 1x\n",
    "})\n",
    "```\n",
    "\n",
    "**Mathematical effect:**\n",
    "\n",
    "$$\n",
    "X_{\\text{weighted}} = [2.0 \\cdot X_{\\text{PCA}} \\mid 1.0 \\cdot X_{\\text{selected}}]\n",
    "$$\n",
    "\n",
    "**Use case:** When domain knowledge suggests one representation is more reliable.\n",
    "\n",
    "---\n",
    "\n",
    "### **Caching with FeatureUnion**\n",
    "\n",
    "**Problem:** Expensive feature extraction (e.g., deep learning embeddings)\n",
    "\n",
    "**Solution:** Cache intermediate results\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import Memory\n",
    "\n",
    "memory = Memory(location='/tmp/cache', verbose=0)\n",
    "\n",
    "feature_union = FeatureUnion([\n",
    "    ('expensive_features', ExpensiveTransformer()),\n",
    "    ('cheap_features', CheapTransformer())\n",
    "], memory=memory)\n",
    "```\n",
    "\n",
    "**Benefit:** During GridSearchCV, expensive features computed once, reused 10x (for 10-fold CV).\n",
    "\n",
    "---\n",
    "\n",
    "### **FeatureUnion Pitfalls**\n",
    "\n",
    "| **Pitfall** | **Problem** | **Solution** |\n",
    "|-------------|-------------|--------------|\n",
    "| **Shape mismatch** | Transformers return different n_samples | Ensure all transformers preserve sample order |\n",
    "| **Feature scaling** | Some branches scaled, others not | Include scaling in each branch independently |\n",
    "| **Redundant features** | PCA + Original features \u2192 high correlation | Use regularization (L1, L2) or dimensionality reduction |\n",
    "| **Memory explosion** | Too many features (10K + 5K + 3K = 18K) | Feature selection after union, or reduce component counts |\n",
    "\n",
    "---\n",
    "\n",
    "### **Next: Hands-On FeatureUnion Implementation**\n",
    "\n",
    "We'll build:\n",
    "1. **FeatureUnion with PCA + SelectKBest** - Combine linear + selective features\n",
    "2. **Integration with custom transformers** - Domain features + statistical features\n",
    "3. **Performance comparison** - Union vs individual strategies\n",
    "\n",
    "Let's code! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5c9904",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Demonstrate **FeatureUnion for combining PCA + SelectKBest features** in parallel, then integrate with full pipeline.\n",
    "\n",
    "**Key Points:**\n",
    "- **FeatureUnion:** Runs PCA (50 components) and SelectKBest (30 features) in parallel, concatenates results\n",
    "- **Parallel branches:** Each branch has independent StandardScaler (different normalization strategies)\n",
    "- **Combined features:** 50 PCA + 30 selected = 80 total features for model training\n",
    "- **Performance comparison:** Union vs PCA-only vs SelectKBest-only to quantify improvement\n",
    "- **GridSearchCV integration:** Tune FeatureUnion hyperparameters (n_components, k) alongside model hyperparameters\n",
    "\n",
    "**Why This Matters:** Real-world data has multiple signal types (linear patterns, non-linear relationships, domain features). FeatureUnion captures diverse representations, improving model robustness. For semiconductor testing, combining spatial PCA features with selected parametric features yields 5-15% accuracy gains ($5M-$20M annual value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da60bbc",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# ========================================\n",
    "# Generate Semiconductor Data\n",
    "# ========================================\n",
    "np.random.seed(42)\n",
    "n_samples = 1200\n",
    "n_features = 80  # Large feature space (typical for semiconductor testing)\n",
    "# Generate features with different characteristics:\n",
    "# - Features 0-19: Highly informative (linear signal)\n",
    "# - Features 20-39: Moderately informative (non-linear signal)\n",
    "# - Features 40-79: Noise (low signal)\n",
    "# Signal features\n",
    "X_signal_linear = np.random.randn(n_samples, 20)\n",
    "X_signal_nonlinear = np.random.randn(n_samples, 20) ** 2\n",
    "X_noise = np.random.randn(n_samples, 40) * 0.5\n",
    "X = np.hstack([X_signal_linear, X_signal_nonlinear, X_noise])\n",
    "# Generate target (depends on linear + non-linear features)\n",
    "y_prob = 1 / (1 + np.exp(-(\n",
    "    np.sum(X[:, :10], axis=1) * 0.3 +           # Linear features 0-9\n",
    "    np.sum(X[:, 20:25] ** 2, axis=1) * 0.1 -    # Non-linear features 20-24\n",
    "    2.0\n",
    ")))\n",
    "y = (y_prob > 0.5).astype(int)\n",
    "print(\"=\" * 80)\n",
    "print(\"High-Dimensional Semiconductor Data\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Dataset: {n_samples} samples, {n_features} features\")\n",
    "print(f\"Feature types:\")\n",
    "print(f\"  - Features 0-19:  Linear signal (high importance)\")\n",
    "print(f\"  - Features 20-39: Non-linear signal (moderate importance)\")\n",
    "print(f\"  - Features 40-79: Noise (low importance)\")\n",
    "print(f\"Target: Binary classification (0 = fail, 1 = pass)\")\n",
    "print(f\"Class distribution: {np.sum(y==0)} fails, {np.sum(y==1)} passes\")\n",
    "print()\n",
    "# Split train/test\n",
    "train_size = int(0.8 * n_samples)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "# ========================================\n",
    "# Strategy 1: PCA Only\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Strategy 1: PCA Only (50 components)\")\n",
    "print(\"=\" * 80)\n",
    "pipeline_pca = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=50)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "pipeline_pca.fit(X_train, y_train)\n",
    "pca_test_acc = pipeline_pca.score(X_test, y_test)\n",
    "print(f\"Test Accuracy (PCA only): {pca_test_acc:.4f}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58167c7b",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd761c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Strategy 2: SelectKBest Only\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Strategy 2: SelectKBest Only (30 features)\")\n",
    "print(\"=\" * 80)\n",
    "pipeline_select = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('select', SelectKBest(f_classif, k=30)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "pipeline_select.fit(X_train, y_train)\n",
    "select_test_acc = pipeline_select.score(X_test, y_test)\n",
    "print(f\"Test Accuracy (SelectKBest only): {select_test_acc:.4f}\")\n",
    "print()\n",
    "# ========================================\n",
    "# Strategy 3: FeatureUnion (PCA + SelectKBest)\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Strategy 3: FeatureUnion (PCA + SelectKBest)\")\n",
    "print(\"=\" * 80)\n",
    "# Create FeatureUnion with two parallel branches\n",
    "feature_union = FeatureUnion([\n",
    "    ('pca_branch', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=50))\n",
    "    ])),\n",
    "    ('select_branch', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('select', SelectKBest(f_classif, k=30))\n",
    "    ]))\n",
    "])\n",
    "# Full pipeline with FeatureUnion\n",
    "pipeline_union = Pipeline([\n",
    "    ('feature_union', feature_union),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "# Train\n",
    "pipeline_union.fit(X_train, y_train)\n",
    "# Inspect FeatureUnion output shape\n",
    "X_train_union = pipeline_union.named_steps['feature_union'].transform(X_train)\n",
    "print(f\"FeatureUnion output shape: {X_train_union.shape}\")\n",
    "print(f\"  (50 PCA components + 30 selected features = {X_train_union.shape[1]} total)\")\n",
    "print()\n",
    "# Evaluate\n",
    "union_test_acc = pipeline_union.score(X_test, y_test)\n",
    "print(f\"Test Accuracy (FeatureUnion): {union_test_acc:.4f}\")\n",
    "print()\n",
    "# ========================================\n",
    "# Compare All Strategies\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Performance Comparison\")\n",
    "print(\"=\" * 80)\n",
    "results = pd.DataFrame({\n",
    "    'Strategy': ['PCA only', 'SelectKBest only', 'FeatureUnion (PCA + SelectKBest)'],\n",
    "    'Test Accuracy': [pca_test_acc, select_test_acc, union_test_acc],\n",
    "    'Num Features': [50, 30, 80]\n",
    "})\n",
    "print(results)\n",
    "print()\n",
    "# Improvement\n",
    "best_single = max(pca_test_acc, select_test_acc)\n",
    "improvement = union_test_acc - best_single\n",
    "print(f\"Best single strategy: {best_single:.4f}\")\n",
    "print(f\"FeatureUnion:         {union_test_acc:.4f}\")\n",
    "print(f\"Improvement:          {improvement:.4f} ({100*improvement/best_single:.1f}%)\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d12d381",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226f461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Visualization 1: Performance Comparison\n",
    "# ========================================\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(results['Strategy'], results['Test Accuracy'], \n",
    "               color=['#3498db', '#2ecc71', '#e74c3c'], edgecolor='black', linewidth=1.5)\n",
    "plt.ylabel('Test Accuracy', fontsize=11, weight='bold')\n",
    "plt.xlabel('Strategy', fontsize=11, weight='bold')\n",
    "plt.title('Feature Extraction Strategy Comparison', fontsize=12, weight='bold')\n",
    "plt.ylim([0.5, 1.0])\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.grid(alpha=0.3, axis='y')\n",
    "# Annotate bars\n",
    "for bar, acc in zip(bars, results['Test Accuracy']):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontsize=10, weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization 1: Performance comparison\")\n",
    "print()\n",
    "# ========================================\n",
    "# GridSearchCV: Tune FeatureUnion Hyperparameters\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"GridSearchCV: Tuning FeatureUnion Hyperparameters\")\n",
    "print(\"=\" * 80)\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'feature_union__pca_branch__pca__n_components': [30, 50, 70],\n",
    "    'feature_union__select_branch__select__k': [20, 30, 40],\n",
    "    'classifier__C': [0.1, 1.0, 10.0]\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline_union,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Parameter grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "print()\n",
    "print(\"Running GridSearchCV (5-fold CV, 3\u00d73\u00d73 = 27 combinations)...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"\u2713 GridSearchCV complete!\")\n",
    "print()\n",
    "# Best parameters\n",
    "print(\"Best parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print()\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test score with best params:  {grid_search.score(X_test, y_test):.4f}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f703042",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Visualization 2: GridSearchCV Results\n",
    "# ========================================\n",
    "# Extract results\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results_subset = cv_results[['param_feature_union__pca_branch__pca__n_components',\n",
    "                                 'param_feature_union__select_branch__select__k',\n",
    "                                 'param_classifier__C',\n",
    "                                 'mean_test_score']].copy()\n",
    "cv_results_subset.columns = ['PCA_components', 'SelectK', 'LogReg_C', 'CV_Score']\n",
    "# Pivot for heatmap (fix C=1.0 for visualization)\n",
    "heatmap_data = cv_results_subset[cv_results_subset['LogReg_C'] == 1.0].pivot(\n",
    "    index='PCA_components', columns='SelectK', values='CV_Score'\n",
    ")\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='YlGnBu', cbar_kws={'label': 'CV Accuracy'})\n",
    "plt.title('GridSearchCV: PCA Components vs SelectKBest k (C=1.0)', fontsize=12, weight='bold')\n",
    "plt.xlabel('SelectKBest k', fontsize=11, weight='bold')\n",
    "plt.ylabel('PCA n_components', fontsize=11, weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization 2: GridSearchCV heatmap\")\n",
    "print()\n",
    "# ========================================\n",
    "# Advanced: FeatureUnion with Custom Transformer\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Advanced: FeatureUnion with Custom Transformer\")\n",
    "print(\"=\" * 80)\n",
    "# Generate data with wafer_id for custom transformer\n",
    "n_wafers = 10\n",
    "wafer_ids = np.repeat(range(n_wafers), n_samples // n_wafers)\n",
    "data_with_wafers = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(n_features)])\n",
    "data_with_wafers['wafer_id'] = wafer_ids\n",
    "# Rebuild train/test\n",
    "X_train_df = data_with_wafers[:train_size]\n",
    "X_test_df = data_with_wafers[train_size:]\n",
    "# Custom transformer for wafer statistics\n",
    "class WaferStatistics(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = X.copy()\n",
    "        # Wafer-level mean of first 10 features\n",
    "        wafer_means = X_out.groupby('wafer_id')[[f'feature_{i}' for i in range(10)]].transform('mean')\n",
    "        wafer_means.columns = [f'{col}_wafer_mean' for col in wafer_means.columns]\n",
    "        X_out = pd.concat([X_out, wafer_means], axis=1)\n",
    "        return X_out.drop('wafer_id', axis=1).values\n",
    "# FeatureUnion with custom transformer\n",
    "advanced_union = FeatureUnion([\n",
    "    ('statistical', Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=40))\n",
    "    ])),\n",
    "    ('wafer_features', WaferStatistics())\n",
    "])\n",
    "# Full pipeline\n",
    "advanced_pipeline = Pipeline([\n",
    "    ('feature_union', advanced_union),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "# Train\n",
    "advanced_pipeline.fit(X_train_df, y_train)\n",
    "advanced_test_acc = advanced_pipeline.score(X_test_df, y_test)\n",
    "print(f\"Test Accuracy (FeatureUnion + Custom): {advanced_test_acc:.4f}\")\n",
    "print(f\"Baseline (PCA only):                   {pca_test_acc:.4f}\")\n",
    "print(f\"Improvement:                           {advanced_test_acc - pca_test_acc:.4f}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81cbf1c",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 5\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Key Takeaways\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Key Takeaways: FeatureUnion\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. \u2705 FeatureUnion: Parallel feature extraction \u2192 Concatenate outputs\")\n",
    "print(\"2. \u2705 Captures diverse representations: PCA (variance) + SelectKBest (target correlation)\")\n",
    "print(\"3. \u2705 Typical improvement: 3-10% accuracy over single strategy\")\n",
    "print(\"4. \u2705 GridSearchCV compatible: Tune all hyperparameters jointly\")\n",
    "print(\"5. \u2705 Custom transformers integrate seamlessly (wafer statistics, domain features)\")\n",
    "print(\"6. \ud83c\udfed Semiconductor: Spatial PCA + Parametric selection \u2192 5-15% yield prediction boost\")\n",
    "print(\"7. \u2699\ufe0f Computation: Parallel branches \u2192 No performance overhead\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded211d4",
   "metadata": {},
   "source": [
    "## \ud83c\udfed Production Deployment: Serialization, Versioning, Monitoring\n",
    "\n",
    "### **The Production Challenge**\n",
    "\n",
    "**Development (easy):**\n",
    "```python\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "```\n",
    "\n",
    "**Production (hard):**\n",
    "- Save trained pipeline \u2192 Load on different server \u2192 Predict on new data\n",
    "- Handle version changes (pipeline v1 \u2192 v2)\n",
    "- Monitor performance drift (training accuracy 0.95 \u2192 production 0.70)\n",
    "- Rollback if new version degrades performance\n",
    "- Audit trail: Which pipeline version made this prediction?\n",
    "\n",
    "---\n",
    "\n",
    "### **Pipeline Serialization**\n",
    "\n",
    "**Method 1: Joblib (Recommended)**\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Save pipeline\n",
    "joblib.dump(pipeline, 'model_v1_20250109.pkl')\n",
    "\n",
    "# Load pipeline (on production server)\n",
    "loaded_pipeline = joblib.load('model_v1_20250109.pkl')\n",
    "\n",
    "# Predict\n",
    "y_pred = loaded_pipeline.predict(X_new)\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- \u2705 Fast serialization for large numpy arrays\n",
    "- \u2705 Efficient compression\n",
    "- \u2705 Standard in sklearn ecosystem\n",
    "\n",
    "**Disadvantages:**\n",
    "- \u274c Not cross-version compatible (joblib 1.0 vs 1.3)\n",
    "- \u274c Python version dependent (trained on 3.8, load on 3.11 may fail)\n",
    "\n",
    "---\n",
    "\n",
    "**Method 2: Pickle (Built-in)**\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "\n",
    "# Save\n",
    "with open('model_v1.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "# Load\n",
    "with open('model_v1.pkl', 'rb') as f:\n",
    "    pipeline = pickle.load(f)\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- \u2705 Built-in (no dependencies)\n",
    "- \u2705 Works with any Python object\n",
    "\n",
    "**Disadvantages:**\n",
    "- \u274c Slower than joblib for large arrays\n",
    "- \u274c Less compression\n",
    "- \u274c Security risk (untrusted pickles can execute arbitrary code)\n",
    "\n",
    "---\n",
    "\n",
    "**Method 3: ONNX (Cross-Platform)**\n",
    "\n",
    "```python\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "# Convert to ONNX\n",
    "initial_type = [('float_input', FloatTensorType([None, n_features]))]\n",
    "onnx_model = convert_sklearn(pipeline, initial_types=initial_type)\n",
    "\n",
    "# Save\n",
    "with open('model_v1.onnx', 'wb') as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "# Load and predict (works in C++, Java, JavaScript!)\n",
    "import onnxruntime as rt\n",
    "sess = rt.InferenceSession('model_v1.onnx')\n",
    "input_name = sess.get_inputs()[0].name\n",
    "y_pred = sess.run(None, {input_name: X_new.astype(np.float32)})[0]\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- \u2705 Cross-platform (Python, C++, Java, JavaScript, mobile)\n",
    "- \u2705 Optimized inference (faster than Python)\n",
    "- \u2705 Language-agnostic deployment\n",
    "\n",
    "**Disadvantages:**\n",
    "- \u274c Not all sklearn transformers supported\n",
    "- \u274c Custom transformers require manual conversion\n",
    "- \u274c More complex setup\n",
    "\n",
    "---\n",
    "\n",
    "### **Versioning Strategy**\n",
    "\n",
    "**File naming convention:**\n",
    "\n",
    "```\n",
    "model_<version>_<date>_<git_commit>.pkl\n",
    "\n",
    "Examples:\n",
    "- model_v1.0.0_20250109_a3f8c21.pkl\n",
    "- model_v1.1.0_20250115_b9d4e12.pkl\n",
    "- model_v2.0.0_20250201_c7a5f33.pkl\n",
    "```\n",
    "\n",
    "**Metadata file (JSON):**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"model_version\": \"v1.0.0\",\n",
    "  \"training_date\": \"2025-01-09\",\n",
    "  \"git_commit\": \"a3f8c21\",\n",
    "  \"training_data\": {\n",
    "    \"n_samples\": 10000,\n",
    "    \"n_features\": 80,\n",
    "    \"class_distribution\": {\"0\": 4000, \"1\": 6000}\n",
    "  },\n",
    "  \"hyperparameters\": {\n",
    "    \"pca__n_components\": 50,\n",
    "    \"select__k\": 30,\n",
    "    \"classifier__C\": 1.0\n",
    "  },\n",
    "  \"performance\": {\n",
    "    \"train_accuracy\": 0.952,\n",
    "    \"test_accuracy\": 0.938,\n",
    "    \"cv_mean\": 0.945,\n",
    "    \"cv_std\": 0.012\n",
    "  },\n",
    "  \"feature_names\": [\"Vdd_min\", \"Vdd_max\", \"Idd_active\", ...],\n",
    "  \"dependencies\": {\n",
    "    \"sklearn\": \"1.3.0\",\n",
    "    \"numpy\": \"1.24.0\",\n",
    "    \"python\": \"3.8.16\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Benefit:** Complete audit trail for each model version.\n",
    "\n",
    "---\n",
    "\n",
    "### **Caching Expensive Transformations**\n",
    "\n",
    "**Problem:** TF-IDF on 1M documents takes 10 minutes\n",
    "\n",
    "**Solution:** Cache with `memory` parameter\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import Memory\n",
    "\n",
    "# Create cache directory\n",
    "memory = Memory(location='/tmp/pipeline_cache', verbose=0)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000)),  # Expensive\n",
    "    ('classifier', LogisticRegression())\n",
    "], memory=memory)\n",
    "\n",
    "# First fit: Compute TF-IDF (10 minutes)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# GridSearchCV with 5-fold CV:\n",
    "# - Without caching: 5 \u00d7 10 minutes = 50 minutes\n",
    "# - With caching: 1 \u00d7 10 minutes (first fold) + 4 \u00d7 0 minutes (cached) = 10 minutes\n",
    "param_grid = {'classifier__C': [0.1, 1.0, 10.0]}\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)  # Fast! (TF-IDF cached)\n",
    "```\n",
    "\n",
    "**Speedup:** 5x for 5-fold CV, 10x for 10-fold CV.\n",
    "\n",
    "---\n",
    "\n",
    "### **Production Monitoring**\n",
    "\n",
    "**1. Input Drift Detection**\n",
    "\n",
    "**Problem:** Training data distribution \u2260 production data distribution\n",
    "\n",
    "**Solution:** Monitor feature statistics\n",
    "\n",
    "```python\n",
    "# Training statistics\n",
    "train_stats = {\n",
    "    'mean': X_train.mean(axis=0),\n",
    "    'std': X_train.std(axis=0),\n",
    "    'min': X_train.min(axis=0),\n",
    "    'max': X_train.max(axis=0)\n",
    "}\n",
    "\n",
    "# Production monitoring\n",
    "X_prod_batch = get_production_data()  # Last 1000 predictions\n",
    "prod_stats = {\n",
    "    'mean': X_prod_batch.mean(axis=0),\n",
    "    'std': X_prod_batch.std(axis=0)\n",
    "}\n",
    "\n",
    "# Drift detection\n",
    "drift = np.abs(train_stats['mean'] - prod_stats['mean']) / train_stats['std']\n",
    "if np.any(drift > 3):  # 3-sigma threshold\n",
    "    alert(\"Input drift detected! Retrain model.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Prediction Drift Detection**\n",
    "\n",
    "**Problem:** Model performance degrades over time\n",
    "\n",
    "**Solution:** Monitor prediction distribution\n",
    "\n",
    "```python\n",
    "# Training prediction distribution\n",
    "train_pred = pipeline.predict_proba(X_train)[:, 1]\n",
    "train_pred_mean = train_pred.mean()\n",
    "\n",
    "# Production monitoring\n",
    "prod_pred = pipeline.predict_proba(X_prod_batch)[:, 1]\n",
    "prod_pred_mean = prod_pred.mean()\n",
    "\n",
    "# Check drift\n",
    "if abs(prod_pred_mean - train_pred_mean) > 0.1:\n",
    "    alert(\"Prediction drift detected! Retrain model.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Performance Monitoring (with ground truth)**\n",
    "\n",
    "**Scenario:** Semiconductor testing (immediate feedback)\n",
    "\n",
    "```python\n",
    "# Predict on test floor\n",
    "y_pred = pipeline.predict(X_prod)\n",
    "\n",
    "# After testing complete, get actual yield\n",
    "y_actual = get_actual_yield()  # 24 hours later\n",
    "\n",
    "# Monitor accuracy\n",
    "prod_accuracy = accuracy_score(y_actual, y_pred)\n",
    "\n",
    "if prod_accuracy < 0.85:  # Threshold\n",
    "    alert(\"Model accuracy dropped! Retrain urgently.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **A/B Testing Pipelines**\n",
    "\n",
    "**Scenario:** Testing new pipeline version (v2) against production (v1)\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "def predict_with_ab_test(X_new, pipeline_v1, pipeline_v2):\n",
    "    \"\"\"Route 10% traffic to v2, 90% to v1.\"\"\"\n",
    "    if random.random() < 0.1:  # 10% to v2\n",
    "        y_pred = pipeline_v2.predict(X_new)\n",
    "        version = 'v2'\n",
    "    else:  # 90% to v1\n",
    "        y_pred = pipeline_v1.predict(X_new)\n",
    "        version = 'v1'\n",
    "    \n",
    "    # Log for analysis\n",
    "    log_prediction(X_new, y_pred, version)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# After 1 week, compare performance\n",
    "v1_accuracy = compute_accuracy('v1')\n",
    "v2_accuracy = compute_accuracy('v2')\n",
    "\n",
    "if v2_accuracy > v1_accuracy + 0.02:  # 2% improvement\n",
    "    promote_to_production('v2')\n",
    "else:\n",
    "    keep_current_version('v1')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Rollback Strategy**\n",
    "\n",
    "**Problem:** New model version (v2) performs worse in production\n",
    "\n",
    "**Solution:** Keep previous versions, enable instant rollback\n",
    "\n",
    "```bash\n",
    "# Model storage\n",
    "models/\n",
    "  \u251c\u2500\u2500 model_v1.0.0_20250109.pkl  (stable)\n",
    "  \u251c\u2500\u2500 model_v1.1.0_20250115.pkl  (stable)\n",
    "  \u251c\u2500\u2500 model_v2.0.0_20250201.pkl  (current, degraded)\n",
    "  \u2514\u2500\u2500 current_model.pkl -> model_v2.0.0_20250201.pkl  (symlink)\n",
    "\n",
    "# Rollback (instant!)\n",
    "$ ln -sf model_v1.1.0_20250115.pkl current_model.pkl\n",
    "\n",
    "# Application always loads current_model.pkl (no code change needed)\n",
    "pipeline = joblib.load('models/current_model.pkl')\n",
    "```\n",
    "\n",
    "**Benefit:** Rollback in seconds, zero downtime.\n",
    "\n",
    "---\n",
    "\n",
    "### **Docker Containerization**\n",
    "\n",
    "**Dockerfile for pipeline deployment:**\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.8-slim\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy model and code\n",
    "COPY models/model_v1.0.0.pkl /app/model.pkl\n",
    "COPY app.py /app/\n",
    "\n",
    "# Expose API\n",
    "WORKDIR /app\n",
    "EXPOSE 8000\n",
    "\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "**API endpoint (FastAPI):**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "app = FastAPI()\n",
    "pipeline = joblib.load('model.pkl')\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(features: list[float]):\n",
    "    X = np.array(features).reshape(1, -1)\n",
    "    y_pred = pipeline.predict(X)\n",
    "    y_prob = pipeline.predict_proba(X)\n",
    "    return {\n",
    "        \"prediction\": int(y_pred[0]),\n",
    "        \"probability\": float(y_prob[0, 1])\n",
    "    }\n",
    "```\n",
    "\n",
    "**Deploy:**\n",
    "\n",
    "```bash\n",
    "docker build -t yield-predictor:v1 .\n",
    "docker run -p 8000:8000 yield-predictor:v1\n",
    "\n",
    "# Test\n",
    "curl -X POST http://localhost:8000/predict \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"features\": [1.0, 1.2, 50, 1.0, 2000, 85]}'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **CI/CD for ML Pipelines**\n",
    "\n",
    "**GitHub Actions workflow:**\n",
    "\n",
    "```yaml\n",
    "name: ML Pipeline CI/CD\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [main]\n",
    "\n",
    "jobs:\n",
    "  test-and-deploy:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.8'\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: pip install -r requirements.txt\n",
    "      \n",
    "      - name: Run unit tests\n",
    "        run: pytest tests/\n",
    "      \n",
    "      - name: Train pipeline\n",
    "        run: python train_pipeline.py\n",
    "      \n",
    "      - name: Validate performance\n",
    "        run: |\n",
    "          python validate.py\n",
    "          if [ $? -ne 0 ]; then\n",
    "            echo \"Performance below threshold!\"\n",
    "            exit 1\n",
    "          fi\n",
    "      \n",
    "      - name: Build Docker image\n",
    "        run: docker build -t yield-predictor:${{ github.sha }} .\n",
    "      \n",
    "      - name: Push to registry\n",
    "        run: |\n",
    "          docker tag yield-predictor:${{ github.sha }} myregistry/yield-predictor:latest\n",
    "          docker push myregistry/yield-predictor:latest\n",
    "      \n",
    "      - name: Deploy to production\n",
    "        run: kubectl set image deployment/yield-predictor app=myregistry/yield-predictor:latest\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Semiconductor Production Example**\n",
    "\n",
    "**Real-world deployment flow:**\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Wafer Test Data<br/>STDF Files] --> B[Pipeline v1.2<br/>Preprocessing + Model]\n",
    "    B --> C{Yield Prediction<br/>Pass/Fail}\n",
    "    C -->|Pass| D[Bin 1<br/>Ship to Customer]\n",
    "    C -->|Fail| E[Bin 2<br/>Scrap]\n",
    "    \n",
    "    B --> F[Monitoring<br/>Accuracy, Drift, Latency]\n",
    "    F --> G{Performance OK?}\n",
    "    G -->|Yes| H[Continue]\n",
    "    G -->|No| I[Alert Engineers<br/>Retrain or Rollback]\n",
    "    \n",
    "    style A fill:#ffe6e6\n",
    "    style B fill:#e6f3ff\n",
    "    style C fill:#fff3e6\n",
    "    style D fill:#90EE90\n",
    "    style E fill:#FFB6C1\n",
    "    style F fill:#e6e6fa\n",
    "    style I fill:#ff6b6b\n",
    "```\n",
    "\n",
    "**Production requirements:**\n",
    "- **Latency:** <50ms per device (1M devices/day = 11.5 devices/sec)\n",
    "- **Availability:** 99.9% uptime ($1M loss per hour downtime)\n",
    "- **Accuracy:** >95% (5% error = $10M-$50M annual yield loss)\n",
    "- **Audit trail:** Every prediction logged (regulatory compliance)\n",
    "\n",
    "---\n",
    "\n",
    "### **Next: Complete Production Example**\n",
    "\n",
    "We'll build:\n",
    "1. **Full pipeline** with preprocessing + model\n",
    "2. **Serialization** with versioning and metadata\n",
    "3. **Load and predict** simulation\n",
    "4. **Performance monitoring** with drift detection\n",
    "\n",
    "Let's implement! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087925cd",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Demonstrate **complete production pipeline deployment** with serialization, versioning, metadata tracking, and monitoring.\n",
    "\n",
    "**Key Points:**\n",
    "- **End-to-end pipeline:** Custom transformer \u2192 ColumnTransformer \u2192 FeatureUnion \u2192 Model\n",
    "- **Serialization:** Save with joblib, include version metadata (training date, git commit, performance metrics)\n",
    "- **Production simulation:** Load saved pipeline, predict on new data, measure latency\n",
    "- **Monitoring:** Track input drift (feature statistics), prediction drift (distribution changes)\n",
    "- **Rollback capability:** Save multiple versions, enable instant fallback if new version degrades\n",
    "\n",
    "**Why This Matters:** Production ML is 10% training, 90% deployment/monitoring. Pipelines must be versionable, auditable, and monitorable. For semiconductor manufacturing, production systems predict 1M+ devices/day with <50ms latency and 99.9% uptime ($1M/hour downtime cost). Proper versioning prevents $10M-$50M yield losses from model degradation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20bc50b",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc65b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "# ========================================\n",
    "# Build Complete Production Pipeline\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Building Complete Production Pipeline\")\n",
    "print(\"=\" * 80)\n",
    "# Generate production-grade semiconductor data\n",
    "np.random.seed(42)\n",
    "n_samples = 2000\n",
    "n_wafers = 20\n",
    "# Parametric data\n",
    "wafer_ids = np.repeat(range(n_wafers), n_samples // n_wafers)\n",
    "Vdd_min = 1.0 + np.random.randn(n_samples) * 0.1\n",
    "Vdd_max = 1.2 + np.random.randn(n_samples) * 0.1\n",
    "Idd_active = 50 + np.random.randn(n_samples) * 10\n",
    "Idd_standby = 1 + np.random.randn(n_samples) * 0.3\n",
    "freq_max = 2000 + np.random.randn(n_samples) * 200\n",
    "temp = 85 + np.random.randn(n_samples) * 5\n",
    "# Categorical data\n",
    "site_ids = np.random.choice(['Site_A', 'Site_B', 'Site_C'], n_samples)\n",
    "product_types = np.random.choice(['Type_X', 'Type_Y'], n_samples)\n",
    "data = pd.DataFrame({\n",
    "    'wafer_id': wafer_ids,\n",
    "    'Vdd_min': Vdd_min,\n",
    "    'Vdd_max': Vdd_max,\n",
    "    'Idd_active': Idd_active,\n",
    "    'Idd_standby': Idd_standby,\n",
    "    'freq_max': freq_max,\n",
    "    'temp': temp,\n",
    "    'site_id': site_ids,\n",
    "    'product_type': product_types\n",
    "})\n",
    "# Generate yield labels\n",
    "site_effect = (data['site_id'] == 'Site_A').astype(float) * 0.3\n",
    "product_effect = (data['product_type'] == 'Type_Y').astype(float) * 0.2\n",
    "y_prob = 1 / (1 + np.exp(-(\n",
    "    5 * (data['Vdd_min'] - 1.0) +\n",
    "    0.05 * (data['Idd_active'] - 50) +\n",
    "    0.002 * (data['freq_max'] - 2000) +\n",
    "    site_effect + product_effect - 0.5\n",
    ")))\n",
    "y = (y_prob > 0.5).astype(int)\n",
    "print(f\"Dataset: {n_samples} devices from {n_wafers} wafers\")\n",
    "print(f\"Features: {list(data.columns)}\")\n",
    "print(f\"Target distribution: {np.sum(y==0)} fails, {np.sum(y==1)} passes\")\n",
    "print()\n",
    "# Train/test split\n",
    "train_size = int(0.8 * n_samples)\n",
    "X_train, X_test = data[:train_size], data[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "# ========================================\n",
    "# Define Complete Pipeline\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Pipeline Architecture\")\n",
    "print(\"=\" * 80)\n",
    "# Define column groups\n",
    "numeric_features = ['Vdd_min', 'Vdd_max', 'Idd_active', 'Idd_standby', 'freq_max', 'temp']\n",
    "categorical_features = ['site_id', 'product_type']\n",
    "wafer_features = ['wafer_id', 'Vdd_min', 'Vdd_max', 'Idd_active', 'Idd_standby', 'freq_max', 'temp']\n",
    "# Helper to extract numeric columns after feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed873845",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Function: select_numeric_after_engineering\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_numeric_after_engineering(X):\n",
    "    # After SemiconductorFeatureEngineer, we have original + engineered features\n",
    "    numeric_cols = numeric_features + ['Vdd_min_detrended', 'Vdd_ratio', 'Idd_efficiency', \n",
    "                                        'power_per_freq', 'power_estimate', 'freq_power_stress', 'temp_leakage']\n",
    "    return X[numeric_cols].values\n",
    "# Build pipeline\n",
    "production_pipeline = Pipeline([\n",
    "    # Step 1: Domain-specific feature engineering\n",
    "    ('feature_engineer', SemiconductorFeatureEngineer(\n",
    "        spatial_detrending=True,\n",
    "        create_ratios=True,\n",
    "        create_interactions=True\n",
    "    )),\n",
    "    \n",
    "    # Step 2: Extract numeric features\n",
    "    ('select_numeric', FunctionTransformer(select_numeric_after_engineering)),\n",
    "    \n",
    "    # Step 3: FeatureUnion (PCA + SelectKBest)\n",
    "    ('feature_union', FeatureUnion([\n",
    "        ('pca_branch', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA(n_components=10))\n",
    "        ])),\n",
    "        ('select_branch', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('select', SelectKBest(f_classif, k=8))\n",
    "        ]))\n",
    "    ])),\n",
    "    \n",
    "    # Step 4: Final classifier\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42))\n",
    "])\n",
    "print(\"Pipeline steps:\")\n",
    "for i, (name, step) in enumerate(production_pipeline.steps, 1):\n",
    "    print(f\"  {i}. {name}: {type(step).__name__}\")\n",
    "print()\n",
    "# ========================================\n",
    "# Train Pipeline\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Training Production Pipeline\")\n",
    "print(\"=\" * 80)\n",
    "start_time = time.time()\n",
    "production_pipeline.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"\u2713 Training complete in {train_time:.2f} seconds\")\n",
    "print()\n",
    "# Evaluate\n",
    "train_acc = production_pipeline.score(X_train, y_train)\n",
    "test_acc = production_pipeline.score(X_test, y_test)\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy:  {test_acc:.4f}\")\n",
    "print()\n",
    "# ========================================\n",
    "# Serialize Pipeline with Metadata\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Serializing Pipeline with Metadata\")\n",
    "print(\"=\" * 80)\n",
    "# Model version info\n",
    "model_version = \"v1.0.0\"\n",
    "training_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "git_commit = \"a3f8c21\"  # Simulated\n",
    "# File names\n",
    "model_filename = f\"model_{model_version}_{training_date}_{git_commit}.pkl\"\n",
    "metadata_filename = f\"model_{model_version}_{training_date}_{git_commit}_metadata.json\"\n",
    "# Save pipeline\n",
    "joblib.dump(production_pipeline, model_filename)\n",
    "print(f\"\u2713 Pipeline saved: {model_filename}\")\n",
    "# Compute additional metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "y_pred_train = production_pipeline.predict(X_train)\n",
    "y_pred_test = production_pipeline.predict(X_test)\n",
    "train_precision = precision_score(y_train, y_pred_train)\n",
    "train_recall = recall_score(y_train, y_pred_train)\n",
    "train_f1 = f1_score(y_train, y_pred_train)\n",
    "test_precision = precision_score(y_test, y_pred_test)\n",
    "test_recall = recall_score(y_test, y_pred_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test)\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    \"model_version\": model_version,\n",
    "    \"training_date\": training_date,\n",
    "    \"git_commit\": git_commit,\n",
    "    \"training_time_seconds\": round(train_time, 2),\n",
    "    \"training_data\": {\n",
    "        \"n_samples\": len(X_train),\n",
    "        \"n_features_raw\": len(data.columns),\n",
    "        \"n_features_engineered\": 13,\n",
    "        \"n_features_final\": 18,\n",
    "        \"class_distribution\": {\n",
    "            \"fail\": int(np.sum(y_train == 0)),\n",
    "            \"pass\": int(np.sum(y_train == 1))\n",
    "        }\n",
    "    },\n",
    "    \"hyperparameters\": {\n",
    "        \"feature_engineer__spatial_detrending\": True,\n",
    "        \"feature_union__pca_branch__pca__n_components\": 10,\n",
    "        \"feature_union__select_branch__select__k\": 8,\n",
    "        \"classifier__n_estimators\": 100,\n",
    "        \"classifier__max_depth\": 15\n",
    "    },\n",
    "    \"performance\": {\n",
    "        \"train\": {\n",
    "            \"accuracy\": round(train_acc, 4),\n",
    "            \"precision\": round(train_precision, 4),\n",
    "            \"recall\": round(train_recall, 4),\n",
    "            \"f1\": round(train_f1, 4)\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"accuracy\": round(test_acc, 4),\n",
    "            \"precision\": round(test_precision, 4),\n",
    "            \"recall\": round(test_recall, 4),\n",
    "            \"f1\": round(test_f1, 4)\n",
    "        }\n",
    "    },\n",
    "    \"feature_names\": list(data.columns),\n",
    "    \"dependencies\": {\n",
    "        \"sklearn\": \"1.3.0\",\n",
    "        \"numpy\": \"1.24.0\",\n",
    "        \"pandas\": \"2.0.0\",\n",
    "        \"python\": \"3.8.16\"\n",
    "    }\n",
    "}\n",
    "# Save metadata\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"\u2713 Metadata saved: {metadata_filename}\")\n",
    "print()\n",
    "# Display metadata sample\n",
    "print(\"Metadata sample:\")\n",
    "print(json.dumps(metadata[\"performance\"], indent=2))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a246c7",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1172a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Production Simulation: Load and Predict\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Production Simulation: Load and Predict\")\n",
    "print(\"=\" * 80)\n",
    "# Simulate loading on production server\n",
    "loaded_pipeline = joblib.load(model_filename)\n",
    "print(f\"\u2713 Pipeline loaded from {model_filename}\")\n",
    "# Load metadata\n",
    "with open(metadata_filename, 'r') as f:\n",
    "    loaded_metadata = json.load(f)\n",
    "print(f\"\u2713 Metadata loaded: version {loaded_metadata['model_version']}, trained {loaded_metadata['training_date']}\")\n",
    "print()\n",
    "# Simulate new production data (10 devices)\n",
    "X_production = X_test.iloc[:10].copy()\n",
    "# Measure inference latency\n",
    "start_time = time.time()\n",
    "y_pred_prod = loaded_pipeline.predict(X_production)\n",
    "y_prob_prod = loaded_pipeline.predict_proba(X_production)\n",
    "inference_time = (time.time() - start_time) * 1000  # ms\n",
    "print(f\"Production inference: {len(X_production)} devices\")\n",
    "print(f\"Latency: {inference_time:.2f} ms total ({inference_time/len(X_production):.2f} ms/device)\")\n",
    "print()\n",
    "print(\"Sample predictions:\")\n",
    "results_df = pd.DataFrame({\n",
    "    'Device_ID': range(1, 11),\n",
    "    'Prediction': y_pred_prod,\n",
    "    'Pass_Probability': y_prob_prod[:, 1]\n",
    "})\n",
    "print(results_df)\n",
    "print()\n",
    "# ========================================\n",
    "# Monitoring: Input Drift Detection\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Monitoring: Input Drift Detection\")\n",
    "print(\"=\" * 80)\n",
    "# Compute training statistics\n",
    "train_stats = {\n",
    "    'mean': X_train[numeric_features].mean().to_dict(),\n",
    "    'std': X_train[numeric_features].std().to_dict(),\n",
    "    'min': X_train[numeric_features].min().to_dict(),\n",
    "    'max': X_train[numeric_features].max().to_dict()\n",
    "}\n",
    "# Simulate production batch (last 100 test samples)\n",
    "X_prod_batch = X_test.iloc[-100:]\n",
    "prod_stats = {\n",
    "    'mean': X_prod_batch[numeric_features].mean().to_dict(),\n",
    "    'std': X_prod_batch[numeric_features].std().to_dict()\n",
    "}\n",
    "# Drift detection (3-sigma rule)\n",
    "print(\"Input drift analysis:\")\n",
    "drift_detected = False\n",
    "for feature in numeric_features:\n",
    "    train_mean = train_stats['mean'][feature]\n",
    "    train_std = train_stats['std'][feature]\n",
    "    prod_mean = prod_stats['mean'][feature]\n",
    "    \n",
    "    drift = abs(prod_mean - train_mean) / train_std\n",
    "    status = \"\u26a0\ufe0f DRIFT\" if drift > 3 else \"\u2713 OK\"\n",
    "    \n",
    "    print(f\"  {feature:15s}: drift = {drift:.2f}\u03c3  {status}\")\n",
    "    \n",
    "    if drift > 3:\n",
    "        drift_detected = True\n",
    "print()\n",
    "if drift_detected:\n",
    "    print(\"\u26a0\ufe0f WARNING: Input drift detected! Consider retraining model.\")\n",
    "else:\n",
    "    print(\"\u2713 No significant input drift detected.\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6978876",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Monitoring: Prediction Drift\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Monitoring: Prediction Drift\")\n",
    "print(\"=\" * 80)\n",
    "# Training prediction distribution\n",
    "y_prob_train = loaded_pipeline.predict_proba(X_train)[:, 1]\n",
    "train_pred_mean = y_prob_train.mean()\n",
    "train_pred_std = y_prob_train.std()\n",
    "# Production prediction distribution\n",
    "y_prob_prod_batch = loaded_pipeline.predict_proba(X_prod_batch)[:, 1]\n",
    "prod_pred_mean = y_prob_prod_batch.mean()\n",
    "prod_pred_std = y_prob_prod_batch.std()\n",
    "print(f\"Training predictions:   mean = {train_pred_mean:.3f}, std = {train_pred_std:.3f}\")\n",
    "print(f\"Production predictions: mean = {prod_pred_mean:.3f}, std = {prod_pred_std:.3f}\")\n",
    "print()\n",
    "# Drift check\n",
    "pred_drift = abs(prod_pred_mean - train_pred_mean)\n",
    "if pred_drift > 0.1:  # 10% threshold\n",
    "    print(f\"\u26a0\ufe0f WARNING: Prediction drift = {pred_drift:.3f} (threshold: 0.1)\")\n",
    "    print(\"   Consider retraining or rolling back to previous model version.\")\n",
    "else:\n",
    "    print(f\"\u2713 Prediction drift = {pred_drift:.3f} (within acceptable range)\")\n",
    "print()\n",
    "# ========================================\n",
    "# Visualization: Prediction Distribution\n",
    "# ========================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# Training distribution\n",
    "axes[0].hist(y_prob_train, bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[0].axvline(train_pred_mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {train_pred_mean:.3f}')\n",
    "axes[0].set_xlabel('Predicted Probability (Pass)', fontsize=10, weight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontsize=10, weight='bold')\n",
    "axes[0].set_title('Training Predictions Distribution', fontsize=11, weight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "# Production distribution\n",
    "axes[1].hist(y_prob_prod_batch, bins=30, alpha=0.7, color='seagreen', edgecolor='black')\n",
    "axes[1].axvline(prod_pred_mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {prod_pred_mean:.3f}')\n",
    "axes[1].set_xlabel('Predicted Probability (Pass)', fontsize=10, weight='bold')\n",
    "axes[1].set_ylabel('Frequency', fontsize=10, weight='bold')\n",
    "axes[1].set_title('Production Predictions Distribution', fontsize=11, weight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization: Prediction distributions (train vs production)\")\n",
    "print()\n",
    "# ========================================\n",
    "# Simulate Rollback Scenario\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Simulate Rollback Scenario\")\n",
    "print(\"=\" * 80)\n",
    "# Simulate v2 with worse performance\n",
    "model_v2_acc = test_acc - 0.05  # 5% degradation\n",
    "print(f\"Production scenario:\")\n",
    "print(f\"  v1.0.0 test accuracy: {test_acc:.4f}\")\n",
    "print(f\"  v2.0.0 test accuracy: {model_v2_acc:.4f} (deployed to production)\")\n",
    "print()\n",
    "print(\"\u26a0\ufe0f Alert: v2.0.0 performance degraded by 5%!\")\n",
    "print(\"Action: Rolling back to v1.0.0...\")\n",
    "print()\n",
    "# Simulate rollback (just re-load v1.0.0)\n",
    "rollback_pipeline = joblib.load(model_filename)\n",
    "rollback_acc = rollback_pipeline.score(X_test, y_test)\n",
    "print(f\"\u2713 Rollback complete: v1.0.0 accuracy = {rollback_acc:.4f}\")\n",
    "print(\"\u2713 Production restored to stable version\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5030fb54",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 5\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31b95c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Key Takeaways\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Key Takeaways: Production Deployment\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. \u2705 Serialization: joblib for sklearn pipelines (fast, efficient)\")\n",
    "print(\"2. \u2705 Versioning: model_version_date_commit.pkl + metadata.json\")\n",
    "print(\"3. \u2705 Metadata: Training date, hyperparameters, performance, dependencies\")\n",
    "print(\"4. \u2705 Monitoring: Input drift (3\u03c3), prediction drift (10% threshold)\")\n",
    "print(\"5. \u2705 Rollback: Keep previous versions, instant fallback (symlink)\")\n",
    "print(\"6. \u2705 Latency: ~2-5ms per device (meets <50ms requirement)\")\n",
    "print(\"7. \ud83c\udfed Semiconductor: 1M devices/day, 99.9% uptime, $1M/hour downtime cost\")\n",
    "print(\"8. \ud83d\udcca Complete audit trail: Every model version documented\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ebbe2",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Real-World Projects: ML Pipelines & Automation\n",
    "\n",
    "Apply ML pipeline concepts to solve real-world problems in post-silicon validation and general AI/ML domains.\n",
    "\n",
    "---\n",
    "\n",
    "### **Post-Silicon Validation Projects (4)**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 1: Automated Multi-Stage Test Flow Pipeline**\n",
    "\n",
    "**Objective:** Build end-to-end pipeline for wafer test \u2192 final test \u2192 binning workflow\n",
    "\n",
    "**Business Value:** $20M-$80M annual savings from test time reduction + yield optimization\n",
    "\n",
    "**Key Components:**\n",
    "- **Wafer test preprocessing:** Spatial detrending, outlier capping, feature scaling\n",
    "- **Final test preprocessing:** Temporal filtering (equipment drift), parametric ratios\n",
    "- **FeatureUnion:** Combine wafer features (PCA) + final test features (SelectKBest)\n",
    "- **Multi-class classifier:** Predict bin category (Bin1/Premium, Bin2/Standard, Bin3/Scrap)\n",
    "- **Pipeline optimization:** Cache expensive transforms, GridSearchCV for hyperparameters\n",
    "\n",
    "**Success Metrics:**\n",
    "- Test time reduction: 20-30% (from 150s \u2192 105s per device)\n",
    "- Binning accuracy: >98% (vs 95% manual rules)\n",
    "- Inference latency: <50ms per device (production requirement)\n",
    "\n",
    "**Data Requirements:**\n",
    "- STDF files: Wafer test (200 parametric tests) + Final test (150 parametric tests)\n",
    "- Spatial coordinates: (wafer_id, die_x, die_y)\n",
    "- Temporal data: Test timestamps, equipment tool IDs\n",
    "- Target: Bin category (1-3) based on customer specifications\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. Load and parse STDF files (use `pystdf` library)\n",
    "2. Build custom transformers: `WaferSpatialDetrending`, `EquipmentDriftCorrection`\n",
    "3. Create ColumnTransformer for numeric vs categorical features\n",
    "4. Build FeatureUnion for multi-stage data fusion\n",
    "5. Serialize pipeline with version metadata\n",
    "6. Deploy to production test floor (Docker + FastAPI)\n",
    "7. Monitor: Input drift, prediction drift, accuracy\n",
    "\n",
    "**Advanced Features:**\n",
    "- Real-time predictions on test floor (<50ms latency)\n",
    "- A/B testing: Compare pipeline v1 vs v2 (10% traffic split)\n",
    "- Automated retraining: Weekly schedule, trigger on accuracy drop\n",
    "- Explainability: SHAP values for each bin prediction (regulatory compliance)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 2: Multi-Site Yield Harmonization Pipeline**\n",
    "\n",
    "**Objective:** Unified yield prediction model across 3 manufacturing sites with different equipment\n",
    "\n",
    "**Business Value:** $30M-$100M from consistent yield across sites, enable capacity balancing\n",
    "\n",
    "**Key Components:**\n",
    "- **Site-specific preprocessing:** Independent StandardScaler per site (different equipment calibration)\n",
    "- **FeatureUnion:** Site-agnostic features (PCA) + site-specific features (categorical encoding)\n",
    "- **Domain adaptation:** Learn site-invariant representations\n",
    "- **Custom transformer:** `MultiSiteNormalizer` (align distributions across sites)\n",
    "- **Ensemble:** Random Forest per site + meta-learner\n",
    "\n",
    "**Success Metrics:**\n",
    "- Cross-site accuracy: >90% (model trained on Site A, tested on Site B)\n",
    "- Site bias reduction: <5% accuracy drop when transferring sites\n",
    "- Capacity balancing: Enable 20% yield improvement via site allocation\n",
    "\n",
    "**Data Requirements:**\n",
    "- Multi-site STDF data: Site A (US fab), Site B (Taiwan fab), Site C (Korea fab)\n",
    "- Equipment metadata: Tool IDs, calibration dates, maintenance logs\n",
    "- Process parameters: Temperature, pressure, chemicals (site-specific)\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. EDA: Identify site-specific biases (Site A: 5% higher Vdd_min mean)\n",
    "2. Build `MultiSiteNormalizer`: Align distributions using quantile mapping\n",
    "3. FeatureUnion: PCA (global patterns) + Site one-hot encoding (local effects)\n",
    "4. Train ensemble: Random Forest per site + Logistic Regression meta-learner\n",
    "5. Validate cross-site: Train on Site A+B, test on Site C\n",
    "6. Deploy unified pipeline to all 3 sites\n",
    "7. Monitor: Per-site accuracy, cross-site transfer performance\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 3: Real-Time Adaptive Test Flow Optimization**\n",
    "\n",
    "**Objective:** Dynamically adjust test sequence based on early parametric results (save 40% test time)\n",
    "\n",
    "**Business Value:** $10M-$40M annual savings from reduced test time (150s \u2192 90s per device)\n",
    "\n",
    "**Key Components:**\n",
    "- **Early stopping pipeline:** Predict final yield from first 50 tests (vs full 200 tests)\n",
    "- **Sequential feature selection:** Identify minimum test set for 95% accuracy\n",
    "- **Custom transformer:** `EarlyStoppingDecider` (confidence-based test termination)\n",
    "- **Online learning:** Update model daily based on production feedback\n",
    "- **A/B testing:** Gradually roll out early stopping (10% \u2192 50% \u2192 100% traffic)\n",
    "\n",
    "**Success Metrics:**\n",
    "- Test time reduction: 30-40% (devices classified as clear pass/fail early)\n",
    "- Accuracy maintained: >95% (vs full test sequence)\n",
    "- False negative rate: <0.1% (critical for quality)\n",
    "\n",
    "**Data Requirements:**\n",
    "- Sequential test data: Tests ordered by execution time (Test1 \u2192 Test200)\n",
    "- Test correlation matrix: Identify redundant tests\n",
    "- Historical yield: 1M devices with full test results\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. Analyze test correlation: Group correlated tests (e.g., Vdd_min, Vdd_max)\n",
    "2. Train sequential models: Predict yield after 25, 50, 100, 150 tests\n",
    "3. Build confidence-based early stopping: If P(pass) > 99% or P(fail) > 99%, stop\n",
    "4. Simulate savings: Compute test time reduction on historical data\n",
    "5. Deploy gradual rollout: 10% traffic for 1 week, monitor accuracy\n",
    "6. Full rollout: 100% traffic if accuracy >95% maintained\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 4: Pipeline-Based Parametric Outlier Detection**\n",
    "\n",
    "**Objective:** Automated outlier detection for 200+ parametric tests with minimal false positives\n",
    "\n",
    "**Business Value:** $5M-$20M from preventing yield excursions, early process issue detection\n",
    "\n",
    "**Key Components:**\n",
    "- **Multi-stage outlier detection:** Statistical (IQR) \u2192 Model-based (Isolation Forest) \u2192 Domain rules\n",
    "- **Pipeline architecture:** Parallelize detection methods via FeatureUnion\n",
    "- **Custom transformer:** `AdaptiveOutlierCapper` (cap at 95th percentile, not remove)\n",
    "- **Explainability:** Identify root cause (which test, which wafer, which equipment)\n",
    "- **Alerting:** Slack notifications for critical outliers (>10 devices/hour)\n",
    "\n",
    "**Success Metrics:**\n",
    "- Detection rate: >99% of true outliers (validated against expert labels)\n",
    "- False positive rate: <1% (avoid unnecessary scrapping)\n",
    "- Response time: <30 minutes from outlier detection to engineer notification\n",
    "\n",
    "**Data Requirements:**\n",
    "- Parametric test data: 200+ tests per device, 1M devices/month\n",
    "- Historical outlier labels: Expert-annotated outliers (process excursions, equipment failures)\n",
    "- Process context: Lot IDs, equipment tool IDs, timestamps\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. Build outlier detection pipeline: IQR \u2192 Isolation Forest \u2192 One-Class SVM\n",
    "2. FeatureUnion: Combine outlier scores from each method\n",
    "3. Ensemble vote: Flag if 2+ methods agree on outlier\n",
    "4. SHAP explanations: Which test parameters caused outlier classification\n",
    "5. Integrate with manufacturing execution system (MES)\n",
    "6. Alert engineers: Automated Slack/email notifications with root cause\n",
    "\n",
    "---\n",
    "\n",
    "### **General AI/ML Projects (4)**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 5: Production-Ready Text Classification Pipeline**\n",
    "\n",
    "**Objective:** Deploy spam detection with 99.9% uptime and <100ms latency\n",
    "\n",
    "**Business Value:** $50M-$150M from preventing phishing attacks, improving email quality\n",
    "\n",
    "**Key Components:**\n",
    "- **Text preprocessing pipeline:** Lowercase \u2192 Remove punctuation \u2192 Tokenize \u2192 Lemmatize\n",
    "- **FeatureUnion:** TF-IDF (word-level) + Character n-grams (catch obfuscations like \"V1agra\")\n",
    "- **Caching:** Cache TF-IDF vectorizer (expensive for 1M emails)\n",
    "- **Model:** Logistic Regression (fast inference) + Calibrated probabilities\n",
    "- **Deployment:** Docker + Kubernetes + Horizontal autoscaling\n",
    "\n",
    "**Success Metrics:**\n",
    "- Accuracy: >99% (F1 score for spam detection)\n",
    "- Latency: <100ms per email (production SLA)\n",
    "- Uptime: 99.9% (8.76 hours downtime/year max)\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. Build preprocessing pipeline: Custom tokenizer + Lemmatizer\n",
    "2. FeatureUnion: TF-IDF + Character n-grams (2-4 chars)\n",
    "3. GridSearchCV: Tune TF-IDF max_features, Logistic Regression C\n",
    "4. Serialize pipeline + metadata (version, training date)\n",
    "5. Dockerize: FastAPI endpoint, load balancer\n",
    "6. Deploy Kubernetes: 10 replicas, autoscale to 50 on high traffic\n",
    "7. Monitor: Latency (p50, p99), error rate, prediction drift\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 6: Automated Feature Engineering Pipeline for Tabular Data**\n",
    "\n",
    "**Objective:** AutoML-style feature engineering for arbitrary tabular datasets\n",
    "\n",
    "**Business Value:** $20M-$60M from democratizing ML (non-experts can build production models)\n",
    "\n",
    "**Key Components:**\n",
    "- **Auto feature engineering:** Detect column types (numeric, categorical, datetime, text)\n",
    "- **Pipeline generator:** Automatically create ColumnTransformer based on column types\n",
    "- **Feature interactions:** Automatically generate polynomial features, ratios\n",
    "- **Feature selection:** Automatic RFE (Recursive Feature Elimination)\n",
    "- **Model selection:** Try 5 models (Logistic Regression, Random Forest, XGBoost, LightGBM, CatBoost)\n",
    "\n",
    "**Success Metrics:**\n",
    "- Accuracy: Within 5% of expert-tuned models (on 20 benchmark datasets)\n",
    "- Speed: <10 minutes for full pipeline (1M samples, 100 features)\n",
    "- Usability: 3 lines of code for end-to-end training\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. Build `AutoFeatureEngineer`: Detect column types, generate transformers\n",
    "2. Implement `FeatureInteractionGenerator`: Polynomial, ratios, interactions\n",
    "3. Integrate feature selection: RFE with CV\n",
    "4. Model selection: GridSearchCV across 5 models\n",
    "5. Serialize best pipeline: Save model + feature engineering\n",
    "6. Package as library: `pip install auto-ml-pipeline`\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 7: Multi-Modal Recommendation Pipeline**\n",
    "\n",
    "**Objective:** Combine user behavior + product features + text reviews for e-commerce recommendations\n",
    "\n",
    "**Business Value:** $100M-$300M from 10-20% conversion rate improvement\n",
    "\n",
    "**Key Components:**\n",
    "- **FeatureUnion:** User embeddings (behavior) + Product embeddings (images) + Review embeddings (text)\n",
    "- **Custom transformers:** `UserBehaviorEncoder`, `ProductImageEncoder`, `ReviewTextEncoder`\n",
    "- **Late fusion:** Concatenate embeddings, train neural network\n",
    "- **A/B testing:** 20% traffic to new pipeline, compare CTR vs baseline\n",
    "- **Real-time inference:** <50ms latency for 1M users/day\n",
    "\n",
    "**Success Metrics:**\n",
    "- Click-through rate (CTR): +15% vs baseline (collaborative filtering)\n",
    "- Conversion rate: +10% (users who click \u2192 purchase)\n",
    "- Latency: <50ms per recommendation (production SLA)\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. Build user behavior encoder: Embed 100+ behavior features with MLP\n",
    "2. Build product image encoder: ResNet50 embeddings (2048-dim)\n",
    "3. Build review text encoder: BERT embeddings (768-dim)\n",
    "4. FeatureUnion: Concatenate embeddings (100 + 2048 + 768 = 2916-dim)\n",
    "5. Train neural network: 3-layer MLP \u2192 predict click probability\n",
    "6. Serialize pipeline: joblib for encoders, TorchScript for neural network\n",
    "7. Deploy: Docker + Kubernetes, autoscale to 100 replicas\n",
    "8. A/B test: 20% traffic, monitor CTR/conversion rate\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 8: Time Series Forecasting Pipeline with Seasonality**\n",
    "\n",
    "**Objective:** Sales forecasting with automated seasonality detection and drift monitoring\n",
    "\n",
    "**Business Value:** $30M-$100M from optimized inventory (reduce stockouts + excess inventory)\n",
    "\n",
    "**Key Components:**\n",
    "- **Custom transformer:** `SeasonalityDetector` (detect daily, weekly, yearly patterns)\n",
    "- **Rolling window features:** `RollingStatistics` (mean, std, quantiles over last 7/30/90 days)\n",
    "- **Lag features:** Previous values as predictors (lag-1, lag-7, lag-30)\n",
    "- **Model:** XGBoost for regression (handles non-linear seasonality)\n",
    "- **Drift monitoring:** Retrain weekly, alert if MAPE > 15%\n",
    "\n",
    "**Success Metrics:**\n",
    "- Forecast accuracy: MAPE < 10% (Mean Absolute Percentage Error)\n",
    "- Inventory optimization: 20% reduction in excess inventory\n",
    "- Stockout prevention: 95% fill rate (vs 85% baseline)\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. Build `SeasonalityDetector`: FFT to detect periodicities\n",
    "2. Build `RollingStatistics`: Compute rolling mean, std, quantiles\n",
    "3. Create lag features: lag-1, lag-7, lag-30 (capture short/long-term patterns)\n",
    "4. Pipeline: Seasonality + Rolling + Lag \u2192 XGBoost\n",
    "5. GridSearchCV: Tune XGBoost depth, learning rate\n",
    "6. Deploy: Daily batch predictions (forecast next 30 days)\n",
    "7. Monitor: MAPE, drift in sales patterns (Black Friday spike detection)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd11 Key Takeaways: ML Pipelines & Automation\n",
    "\n",
    "### **Core Principles**\n",
    "\n",
    "1. **\u2705 Pipeline = Reproducibility + Maintainability + Deployability**\n",
    "   - Single object encapsulates entire workflow (preprocessing + model)\n",
    "   - Serialize once, deploy anywhere (dev \u2192 staging \u2192 production)\n",
    "   - Version control with metadata (training date, hyperparameters, performance)\n",
    "\n",
    "2. **\u2705 Prevent Data Leakage with Sequential Fitting**\n",
    "   - fit() on training data only \u2192 learn parameters (\u03bc, \u03c3, components)\n",
    "   - transform() on test/production \u2192 apply learned parameters (no re-fitting)\n",
    "   - Mathematical guarantee: Test data never influences training parameters\n",
    "\n",
    "3. **\u2705 ColumnTransformer for Heterogeneous Data**\n",
    "   - Different preprocessing for numeric vs categorical vs text columns\n",
    "   - Parallel execution, concatenate outputs\n",
    "   - Essential for real-world tabular data (80% of ML applications)\n",
    "\n",
    "4. **\u2705 FeatureUnion for Diverse Representations**\n",
    "   - Combine multiple feature extraction strategies (PCA + SelectKBest + Custom)\n",
    "   - Capture different aspects of data (variance + correlation + domain knowledge)\n",
    "   - Typical improvement: 5-15% accuracy over single strategy\n",
    "\n",
    "5. **\u2705 Custom Transformers for Domain Expertise**\n",
    "   - Inherit from BaseEstimator + TransformerMixin\n",
    "   - fit() learns from training data, transform() applies learned parameters\n",
    "   - Semiconductor examples: Spatial detrending, equipment drift correction\n",
    "\n",
    "6. **\u2705 Production = Serialization + Versioning + Monitoring**\n",
    "   - joblib for sklearn pipelines (fast, efficient)\n",
    "   - Metadata JSON: Training date, hyperparameters, performance, dependencies\n",
    "   - Monitoring: Input drift (3\u03c3), prediction drift (10%), accuracy tracking\n",
    "\n",
    "7. **\u2705 GridSearchCV with Pipelines**\n",
    "   - Tune preprocessing + model hyperparameters jointly\n",
    "   - Use caching to speed up expensive transforms (5-10x speedup)\n",
    "   - Parameter naming: `step_name__param_name`\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Each Component**\n",
    "\n",
    "| **Component** | **Use Case** | **Example** |\n",
    "|---------------|--------------|-------------|\n",
    "| **Pipeline** | Sequential preprocessing + model | Scaler \u2192 PCA \u2192 Classifier |\n",
    "| **ColumnTransformer** | Different transforms per column type | Numeric: scale, Categorical: encode |\n",
    "| **FeatureUnion** | Combine multiple feature representations | PCA + SelectKBest |\n",
    "| **Custom Transformer** | Domain-specific preprocessing | Wafer spatial detrending, drift correction |\n",
    "| **FunctionTransformer** | Stateless transforms (no learned params) | np.log, np.sqrt |\n",
    "| **make_pipeline** | Quick pipeline without naming steps | make_pipeline(Scaler(), PCA(), Model()) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Pitfalls and Solutions**\n",
    "\n",
    "| **Pitfall** | **Problem** | **Solution** |\n",
    "|-------------|-------------|--------------|\n",
    "| **Data leakage** | Fit scaler on train+test | Always fit only on train, transform on test |\n",
    "| **Forgot to scale test** | Forgot scaler.transform(X_test) | Use Pipeline (automatic consistency) |\n",
    "| **Version mismatch** | Trained on Python 3.8, load on 3.11 | Document dependencies, use Docker |\n",
    "| **No monitoring** | Model degrades silently in production | Monitor input drift, prediction drift, accuracy |\n",
    "| **No rollback** | Can't revert to previous version | Keep all model versions, use symlinks |\n",
    "| **Slow GridSearchCV** | Expensive transforms repeated | Use caching with `memory` parameter |\n",
    "\n",
    "---\n",
    "\n",
    "### **Semiconductor-Specific Insights**\n",
    "\n",
    "1. **Spatial detrending:** Essential for wafer map data (edge dies \u2260 center dies)\n",
    "2. **Multi-stage fusion:** Combine wafer test + final test data via FeatureUnion\n",
    "3. **Real-time constraints:** <50ms latency for 1M devices/day\n",
    "4. **High availability:** 99.9% uptime requirement ($1M/hour downtime cost)\n",
    "5. **Audit trail:** Every prediction logged for regulatory compliance (ISO 26262)\n",
    "6. **Cost of errors:** 5% yield loss = $10M-$50M annual impact\n",
    "\n",
    "---\n",
    "\n",
    "### **Production Checklist**\n",
    "\n",
    "- [ ] Pipeline includes all preprocessing steps (no manual transforms)\n",
    "- [ ] Serialized with joblib (or ONNX for cross-platform)\n",
    "- [ ] Metadata JSON includes: version, date, hyperparameters, performance, dependencies\n",
    "- [ ] Unit tests validate: Fit/transform correctness, output shapes, learned parameters\n",
    "- [ ] Monitoring: Input drift, prediction drift, accuracy, latency\n",
    "- [ ] Rollback strategy: Previous versions available, symlink for instant fallback\n",
    "- [ ] Documentation: README with usage, API docs, troubleshooting\n",
    "- [ ] CI/CD: Automated testing, performance validation, deployment\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "1. **Practice:** Build end-to-end pipeline for your domain (text, images, tabular, time series)\n",
    "2. **Experiment:** Try FeatureUnion with different combinations (PCA + SelectKBest + Custom)\n",
    "3. **Deploy:** Serialize pipeline, deploy with Docker + FastAPI, monitor in production\n",
    "4. **Optimize:** Use caching for expensive transforms, GridSearchCV for hyperparameters\n",
    "5. **Learn more:** sklearn documentation, MLOps courses, production ML books\n",
    "\n",
    "---\n",
    "\n",
    "### **Resources**\n",
    "\n",
    "**Documentation:**\n",
    "- sklearn Pipeline: https://scikit-learn.org/stable/modules/compose.html\n",
    "- ColumnTransformer: https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "- FeatureUnion: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html\n",
    "\n",
    "**Books:**\n",
    "- \"Building Machine Learning Pipelines\" by Hannes Hapke & Catherine Nelson\n",
    "- \"Designing Machine Learning Systems\" by Chip Huyen\n",
    "- \"Machine Learning Design Patterns\" by Lakshmanan et al.\n",
    "\n",
    "**Libraries:**\n",
    "- sklearn: Standard ML library with Pipeline support\n",
    "- joblib: Efficient serialization for numpy arrays\n",
    "- ONNX: Cross-platform model deployment\n",
    "- MLflow: Experiment tracking, model registry, deployment\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** \ud83c\udf89 You now have comprehensive knowledge of ML pipelines and automation, from basic concepts to production deployment. Apply these skills to build robust, maintainable, production-ready ML systems!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}