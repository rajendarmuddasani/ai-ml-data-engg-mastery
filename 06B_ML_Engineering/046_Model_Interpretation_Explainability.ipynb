{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e57ff25",
   "metadata": {},
   "source": [
    "# 046: Model Interpretation & Explainability",
    "",
    "## \ud83c\udfaf Learning Objectives",
    "",
    "By the end of this notebook, you will:",
    "",
    "1. **Understand interpretability vs explainability** - Key differences and when each matters",
    "2. **Master global interpretation** - Feature importance, partial dependence, interaction effects",
    "3. **Master local explanations** - SHAP, LIME for individual predictions",
    "4. **Implement from scratch** - Build simple explainability algorithms to understand internals",
    "5. **Use production tools** - SHAP library, LIME library for real-world applications",
    "6. **Apply to post-silicon validation** - Explain yield predictions, defect classifications, test failures",
    "7. **Build trust in models** - Debug, validate, communicate ML decisions to stakeholders",
    "8. **Deploy explainable systems** - Production patterns for real-time explanations",
    "",
    "---",
    "",
    "## \ud83d\udcca Why Model Interpretability Matters",
    "",
    "```mermaid",
    "graph TD",
    "    A[ML Model] --> B{Stakeholder Questions}",
    "    ",
    "    B --> C[Engineer: Why did it fail?]",
    "    B --> D[Manager: Can we trust it?]",
    "    B --> E[Regulator: Is it fair?]",
    "    B --> F[Customer: Why this decision?]",
    "    ",
    "    C --> G[Local Explanations]",
    "    D --> H[Global Interpretability]",
    "    E --> I[Fairness Metrics]",
    "    F --> G",
    "    ",
    "    G --> J[SHAP, LIME]",
    "    H --> K[Feature Importance, PDP]",
    "    I --> L[Group Analysis]",
    "    ",
    "    J --> M[Debuggable System]",
    "    K --> M",
    "    L --> M",
    "    ",
    "    M --> N[Trusted ML in Production]",
    "    ",
    "    style A fill:#e1f5e1",
    "    style M fill:#ffe6e6",
    "    style N fill:#90EE90",
    "```",
    "",
    "---",
    "",
    "## \ud83c\udf93 Post-Silicon Validation Context",
    "",
    "### Why Explainability is Critical in Semiconductor Testing:",
    "",
    "1. **Root Cause Analysis** ($10M-$100M impact)",
    "   - Device fails yield test \u2192 Need to know which parameter caused failure",
    "   - \"Model says fail\" is unacceptable \u2192 Need \"Vdd out of spec by 2\u03c3, likely fab issue\"",
    "   - Explainability guides debug efforts: Equipment? Process? Design?",
    "",
    "2. **Regulatory Compliance** (Automotive, Medical devices)",
    "   - ISO 26262 (automotive): \"Safety-critical decisions must be explainable\"",
    "   - IEC 62304 (medical): \"Algorithm rationale must be documented\"",
    "   - \"Black box ML\" not acceptable \u2192 Need feature contributions for audit trail",
    "",
    "3. **Process Improvement** ($5M-$20M savings)",
    "   - Model identifies Vdd as top predictor of yield \u2192 Engineer investigates power grid",
    "   - Without explainability: Trial-and-error debugging (weeks/months)",
    "   - With explainability: Targeted fixes (days)",
    "",
    "4. **Multi-Site Manufacturing** (Global fabs)",
    "   - Model performance differs across sites \u2192 Need to understand why",
    "   - SHAP reveals: \"Site A weight = +0.05, Site B weight = -0.03\"",
    "   - Actionable: Standardize Site B process to match Site A",
    "",
    "---",
    "",
    "## \ud83d\udd11 Core Concepts",
    "",
    "### **1. Interpretability vs Explainability**",
    "",
    "| **Aspect** | **Interpretability** | **Explainability** |",
    "|------------|----------------------|--------------------|",
    "| **Definition** | Model structure is inherently understandable | Post-hoc explanations for any model |",
    "| **When** | Model design phase | After model is trained |",
    "| **Scope** | Global (entire model) | Local (per prediction) or Global |",
    "| **Examples** | Linear regression coefficients, Decision tree rules | SHAP values, LIME explanations |",
    "| **Trade-off** | Often sacrifices accuracy | Works with complex models |",
    "",
    "**Interpretable models:** Linear regression, Logistic regression, Decision trees (shallow), Rule-based systems  ",
    "**Explainability techniques:** SHAP, LIME, Partial Dependence Plots, Feature Importance",
    "",
    "---",
    "",
    "### **2. Types of Explanations**",
    "",
    "#### **Global Explanations** (Model-level understanding)",
    "",
    "- **Feature Importance**: Which features matter most overall?",
    "  - Tree-based: Mean Decrease in Impurity (MDI)",
    "  - Permutation importance: Shuffle feature, measure performance drop",
    "  - SHAP: Average |SHAP value| across all predictions",
    "",
    "- **Partial Dependence Plots (PDP)**: How does changing one feature affect predictions?",
    "  - Marginalizes over other features",
    "  - Shows average effect of feature on prediction",
    "",
    "- **Interaction Effects**: How do feature pairs interact?",
    "  - H-statistic: Measures interaction strength",
    "  - 2D Partial Dependence Plots",
    "",
    "#### **Local Explanations** (Prediction-level understanding)",
    "",
    "- **SHAP (SHapley Additive exPlanations)**:",
    "  - Game theory approach: Each feature gets credit for contribution",
    "  - Satisfies properties: Local accuracy, Missingness, Consistency",
    "  - Output: $f(x) = \\phi_0 + \\sum_{i=1}^{p} \\phi_i$, where $\\phi_i$ = feature $i$ contribution",
    "",
    "- **LIME (Local Interpretable Model-agnostic Explanations)**:",
    "  - Approximate complex model locally with simple model (linear)",
    "  - Perturb input, observe output, fit linear model",
    "  - Interpretable because linear coefficients are explanations",
    "",
    "---",
    "",
    "### **3. SHAP Values - Mathematical Foundation**",
    "",
    "**Shapley values** from cooperative game theory:",
    "",
    "$$\\phi_i = \\sum_{S \\subseteq F \\setminus \\{i\\}} \\frac{|S|! (|F| - |S| - 1)!}{|F|!} [f_{S \\cup \\{i\\}}(x_{S \\cup \\{i\\}}) - f_S(x_S)]$$",
    "",
    "Where:",
    "- $F$ = set of all features",
    "- $S$ = subset of features (coalition)",
    "- $\\phi_i$ = Shapley value for feature $i$",
    "- $f_S(x_S)$ = model prediction using only features in $S$",
    "",
    "**Intuition**: ",
    "- Consider all possible feature coalitions",
    "- Measure marginal contribution of feature $i$ to each coalition",
    "- Average contributions weighted by coalition size",
    "- Fair allocation: Each feature gets credit proportional to its contribution",
    "",
    "**Properties:**",
    "1. **Efficiency**: $\\sum_{i=1}^{p} \\phi_i = f(x) - E[f(X)]$ (sum equals prediction minus baseline)",
    "2. **Symmetry**: If features $i$ and $j$ contribute equally, $\\phi_i = \\phi_j$",
    "3. **Dummy**: If feature has no effect, $\\phi_i = 0$",
    "4. **Additivity**: For ensemble $f = f_1 + f_2$, $\\phi_i^f = \\phi_i^{f_1} + \\phi_i^{f_2}$",
    "",
    "**Computational challenge**: Exact computation requires $2^p$ model evaluations (exponential in features)",
    "",
    "**Practical algorithms:**",
    "- **TreeSHAP**: Polynomial-time exact SHAP for tree-based models (XGBoost, RF, LightGBM)",
    "- **KernelSHAP**: Model-agnostic approximation using weighted linear regression",
    "- **DeepSHAP**: Fast approximation for neural networks",
    "",
    "---",
    "",
    "### **4. LIME - Mathematical Foundation**",
    "",
    "**Objective**: Explain prediction $f(x)$ by fitting simple model $g$ locally",
    "",
    "$$\\xi(x) = \\arg\\min_{g \\in G} \\mathcal{L}(f, g, \\pi_x) + \\Omega(g)$$",
    "",
    "Where:",
    "- $g$ = simple interpretable model (e.g., linear regression)",
    "- $G$ = class of interpretable models",
    "- $\\mathcal{L}$ = loss function (fidelity to $f$)",
    "- $\\pi_x$ = proximity measure (weight nearby points more)",
    "- $\\Omega(g)$ = complexity penalty (prefer simpler explanations)",
    "",
    "**Algorithm:**",
    "1. **Perturb**: Generate $N$ samples near $x$ (e.g., $x' \\sim \\mathcal{N}(x, \\sigma^2)$)",
    "2. **Predict**: Get complex model predictions $f(x')$ for all perturbed samples",
    "3. **Weight**: Assign weights $\\pi_x(x') = \\exp(-\\frac{d(x, x')^2}{\\sigma^2})$ (closer samples matter more)",
    "4. **Fit**: Train linear model $g(x') = \\beta_0 + \\sum_{i=1}^{p} \\beta_i x'_i$ with weighted least squares",
    "5. **Explain**: Coefficients $\\beta_i$ are feature importances (positive = increase prediction)",
    "",
    "**Key difference from SHAP:**",
    "- LIME: Local linear approximation (fast, approximate)",
    "- SHAP: Rigorous game-theoretic attribution (slower, exact for trees)",
    "",
    "---",
    "",
    "## \ud83d\udee0\ufe0f When to Use Each Technique",
    "",
    "| **Technique** | **Use When** | **Avoid When** | **Computational Cost** |",
    "|---------------|--------------|----------------|------------------------|",
    "| **Feature Importance (Tree)** | Need quick global insights, tree-based models | Correlated features (biased), non-tree models | \u26a1 Fast |",
    "| **Permutation Importance** | Model-agnostic, want unbiased importance | Large datasets (slow), many features | \ud83d\udc0c Slow (N \u00d7 p evaluations) |",
    "| **SHAP (TreeSHAP)** | Tree-based models, need exact attributions | Non-tree models (use KernelSHAP) | \u26a1 Fast (polynomial) |",
    "| **SHAP (KernelSHAP)** | Model-agnostic, need principled explanations | Ultra-large feature spaces (slow) | \ud83d\udc0c Moderate-Slow |",
    "| **LIME** | Need fast local explanations, any model | High-dimensional images (slow), need global | \u26a1 Fast (local) |",
    "| **Partial Dependence** | Understand marginal effects, global patterns | Feature interactions (misleading), large data | \ud83d\udc0c Moderate |",
    "",
    "---",
    "",
    "## \ud83c\udfed Semiconductor-Specific Considerations",
    "",
    "### **Challenge 1: Correlated Parameters**",
    "- Problem: Vdd and Idd are physically correlated (Ohm's law)",
    "- Naive feature importance: Splits credit arbitrarily",
    "- Solution: SHAP properly handles correlation via coalitional averaging",
    "",
    "### **Challenge 2: Spatial Correlation**",
    "- Problem: Adjacent dies on wafer have similar properties",
    "- Naive explanations: May attribute to wafer_id instead of true cause",
    "- Solution: Group SHAP by wafer, explain within-wafer variation",
    "",
    "### **Challenge 3: Multi-Site Manufacturing**",
    "- Problem: Site-specific biases (equipment, process, climate)",
    "- Naive explanations: \"Site B\" is important (not actionable)",
    "- Solution: Decompose into site effect + process effect (ICE plots)",
    "",
    "### **Challenge 4: Test Correlation**",
    "- Problem: Some tests are redundant (measure same thing)",
    "- Naive feature importance: Splits credit among correlated tests",
    "- Solution: Group correlated tests, explain group contribution",
    "",
    "---",
    "",
    "## \ud83d\udcda What We'll Build",
    "",
    "### **From Scratch (Educational):**",
    "1. **Simple Feature Importance** - Permutation-based for any model",
    "2. **Simple LIME** - Local linear approximation",
    "3. **Simple SHAP** - Exact Shapley values for small problems",
    "",
    "### **Production (Practical):**",
    "4. **SHAP library** - TreeSHAP for XGBoost defect detection",
    "5. **LIME library** - Explain Random Forest yield predictions",
    "6. **Integrated system** - Explainability dashboard for post-silicon validation",
    "",
    "---",
    "",
    "## \ud83c\udfaf Real-World Applications",
    "",
    "### **Post-Silicon Validation:**",
    "- **Defect root cause analysis** - Which parameter caused test failure?",
    "- **Yield prediction debugging** - Why did model predict low yield?",
    "- **Test optimization** - Which tests contribute most to binning decision?",
    "- **Multi-site comparison** - Why does Site A outperform Site B?",
    "",
    "### **General AI/ML:**",
    "- **Credit scoring** - Explain loan denial (regulatory requirement)",
    "- **Healthcare diagnosis** - Why did model predict disease? (doctor trust)",
    "- **Fraud detection** - Explain flagged transaction (customer service)",
    "- **Recommendation systems** - Why recommend this product? (user transparency)",
    "",
    "---",
    "",
    "**Let's begin!** \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f5b84",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udcca Part 1: Feature Importance (Global Explanation)\n",
    "\n",
    "### \ud83d\udd0d Theory: Feature Importance Methods\n",
    "\n",
    "Feature importance answers: **\"Which features matter most to the model overall?\"**\n",
    "\n",
    "#### **1. Tree-Based Feature Importance (MDI - Mean Decrease in Impurity)**\n",
    "\n",
    "For decision trees/random forests/gradient boosting:\n",
    "\n",
    "$$\\text{Importance}(X_j) = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{s \\in \\text{splits}(t, j)} \\Delta I(s)$$\n",
    "\n",
    "Where:\n",
    "- $T$ = number of trees\n",
    "- $s$ = split using feature $j$\n",
    "- $\\Delta I(s)$ = decrease in impurity (Gini/Entropy) from split $s$\n",
    "\n",
    "**How it works:**\n",
    "- Each time feature $j$ is used to split a node, measure improvement in node purity\n",
    "- Sum improvements across all trees\n",
    "- Normalize by total number of trees\n",
    "\n",
    "**Advantages:**\n",
    "- \u26a1 Fast: Computed during training (no extra cost)\n",
    "- \ud83d\udcca Works for any tree-based model\n",
    "\n",
    "**Disadvantages:**\n",
    "- \u26a0\ufe0f Biased toward high-cardinality features (more unique values \u2192 more splits)\n",
    "- \u26a0\ufe0f Biased toward features allowing many splits (continuous > categorical)\n",
    "- \u26a0\ufe0f Misleading with correlated features (credit split arbitrarily)\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Permutation Importance (Model-Agnostic)**\n",
    "\n",
    "**Algorithm:**\n",
    "1. Train model on data $(X, y)$, compute baseline score $S_0$ (e.g., R\u00b2 or accuracy)\n",
    "2. For each feature $j$:\n",
    "   - Randomly shuffle column $j$ in test set \u2192 $X^{(j)}$\n",
    "   - Compute score $S_j$ with shuffled feature\n",
    "   - Importance$(j) = S_0 - S_j$ (performance drop)\n",
    "3. Repeat shuffling $k$ times, average importance scores\n",
    "\n",
    "**Mathematical intuition:**\n",
    "- If feature is important: Shuffling destroys relationship \u2192 performance drops (high importance)\n",
    "- If feature is noise: Shuffling has no effect \u2192 performance unchanged (zero importance)\n",
    "\n",
    "**Advantages:**\n",
    "- \u2705 Model-agnostic: Works for any model (tree, linear, neural net, ensemble)\n",
    "- \u2705 Unbiased: Not affected by high-cardinality features\n",
    "- \u2705 Reflects true predictive value\n",
    "\n",
    "**Disadvantages:**\n",
    "- \ud83d\udc0c Slow: Requires $k \\times p$ model evaluations ($k$ = shuffles, $p$ = features)\n",
    "- \u26a0\ufe0f Can be misleading with correlated features (shuffle one \u2192 affects correlated ones)\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Comparison: MDI vs Permutation**\n",
    "\n",
    "| **Aspect** | **MDI (Tree-Based)** | **Permutation Importance** |\n",
    "|------------|----------------------|---------------------------|\n",
    "| **Speed** | \u26a1 Fast (during training) | \ud83d\udc0c Slow ($k \\times p$ evals) |\n",
    "| **Model Scope** | Trees only | Any model |\n",
    "| **High-Cardinality Bias** | \u274c Biased | \u2705 Unbiased |\n",
    "| **Correlated Features** | \u274c Arbitrary split | \u26a0\ufe0f Can be misleading |\n",
    "| **Interpretation** | \"Average split quality\" | \"Predictive value\" |\n",
    "\n",
    "**Recommendation:**\n",
    "- Use **MDI** for quick exploratory analysis with tree models\n",
    "- Use **Permutation** for final model validation and model comparison\n",
    "- Use both together for comprehensive understanding\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83c\udfed Semiconductor Example: Test Parameter Importance\n",
    "\n",
    "**Scenario:** Predict device yield from 10 parametric tests (Vdd, Idd, freq, leakage, etc.)\n",
    "\n",
    "**Key Questions:**\n",
    "1. Which test parameters are most predictive of yield?\n",
    "2. Can we eliminate redundant tests to reduce test time?\n",
    "3. Are importance rankings consistent across Random Forest, XGBoost, and Linear models?\n",
    "\n",
    "**Expected Business Value:**\n",
    "- Remove 20-30% of low-importance tests \u2192 15-25% test time reduction\n",
    "- Test time reduction of 10 seconds/device \u00d7 1M devices/month = $50K-$100K monthly savings\n",
    "- Focus debug efforts on top 3-5 important parameters \u2192 50% faster root cause analysis\n",
    "\n",
    "**Typical results:**\n",
    "- **High importance**: Vdd_min, Vdd_max, Idd_active, leakage_standby (core electrical params)\n",
    "- **Medium importance**: frequency_max, temperature_drift (performance indicators)\n",
    "- **Low importance**: test_setup_time, ambient_humidity (environmental noise)\n",
    "\n",
    "**Interpretation:**\n",
    "- If Vdd_min is top feature \u2192 Focus on power grid stability in design/fab\n",
    "- If leakage_standby is top \u2192 Investigate process variation (channel length, oxide thickness)\n",
    "- If test_setup_time is important \u2192 Likely model artifact (correlated with real param), not causal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c968af",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement permutation importance from scratch to understand how feature shuffling reveals predictive value\n",
    "\n",
    "**Key Points:**\n",
    "- **Baseline score**: Model performance on original data (R\u00b2 for regression, accuracy for classification)\n",
    "- **Shuffling strategy**: Randomly permute one feature column while keeping others intact\n",
    "- **Performance drop**: $\\text{Importance}(j) = S_{\\text{baseline}} - S_{\\text{shuffled}(j)}$ (larger drop = more important)\n",
    "- **Multiple shuffles**: Repeat $k$ times to get stable estimates (reduce noise from random shuffle)\n",
    "- **Visualization**: Bar chart sorted by importance (focus on top features)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Model-agnostic: Works for any sklearn-compatible model (tree, linear, ensemble, neural net)\n",
    "- Unbiased: Not affected by feature cardinality or tree structure\n",
    "- Interpretable: \"Removing this feature reduces R\u00b2 by 0.05\" is actionable\n",
    "- Business value: Identifies redundant tests \u2192 test time optimization ($50K-$500K annual savings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab31f8",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90451fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(42)\n",
    "print(\"=\"*70)\n",
    "print(\"PERMUTATION IMPORTANCE FROM SCRATCH\")\n",
    "print(\"=\"*70)\n",
    "# ============================================================================\n",
    "# Generate Semiconductor Yield Prediction Data\n",
    "# ============================================================================\n",
    "print(\"\\n[1] Generating semiconductor test data...\")\n",
    "n_devices = 2000\n",
    "n_wafers = 20\n",
    "# Generate parametric test measurements\n",
    "data = {\n",
    "    'wafer_id': np.random.randint(1, n_wafers+1, n_devices),\n",
    "    'Vdd_min': np.random.normal(1.62, 0.05, n_devices),      # High importance (power)\n",
    "    'Vdd_max': np.random.normal(1.98, 0.04, n_devices),      # High importance (power)\n",
    "    'Idd_active': np.random.normal(150, 15, n_devices),      # High importance (current)\n",
    "    'Idd_standby': np.random.normal(2.5, 0.8, n_devices),    # Medium importance (leakage)\n",
    "    'freq_max': np.random.normal(2400, 100, n_devices),      # Medium importance (performance)\n",
    "    'temp_coef': np.random.normal(0.02, 0.005, n_devices),   # Low importance (noise)\n",
    "    'humidity': np.random.normal(45, 10, n_devices),         # Low importance (environmental)\n",
    "    'test_time': np.random.normal(15, 2, n_devices),         # Very low (nuisance variable)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "# Generate yield (%) with known feature contributions\n",
    "# High importance: Vdd_min (40%), Vdd_max (30%), Idd_active (20%)\n",
    "# Medium: Idd_standby (7%), freq_max (3%)\n",
    "# Low: Others contribute noise only\n",
    "yield_pct = (\n",
    "    70 +  # Baseline yield\n",
    "    10 * (df['Vdd_min'] - 1.62) / 0.05 +          # 40% of signal\n",
    "    8 * (df['Vdd_max'] - 1.98) / 0.04 +           # 30% of signal\n",
    "    -5 * (df['Idd_active'] - 150) / 15 +          # 20% of signal (negative: higher current \u2192 lower yield)\n",
    "    -2 * (df['Idd_standby'] - 2.5) / 0.8 +        # 7% of signal\n",
    "    1 * (df['freq_max'] - 2400) / 100 +           # 3% of signal\n",
    "    np.random.normal(0, 3, n_devices)             # Noise\n",
    ")\n",
    "df['yield_pct'] = np.clip(yield_pct, 50, 95)  # Realistic yield range\n",
    "print(f\"   Generated {n_devices} devices across {n_wafers} wafers\")\n",
    "print(f\"   Features: {list(df.columns[:-1])}\")\n",
    "print(f\"   Target: yield_pct (range: {df['yield_pct'].min():.1f}% - {df['yield_pct'].max():.1f}%)\")\n",
    "print(f\"\\n   Ground truth importance (by design):\")\n",
    "print(f\"      1. Vdd_min      (40% of signal)\")\n",
    "print(f\"      2. Vdd_max      (30% of signal)\")\n",
    "print(f\"      3. Idd_active   (20% of signal)\")\n",
    "print(f\"      4. Idd_standby  (7% of signal)\")\n",
    "print(f\"      5. freq_max     (3% of signal)\")\n",
    "print(f\"      6-8. Others     (noise only)\")\n",
    "# ============================================================================\n",
    "# Train Random Forest Model\n",
    "# ============================================================================\n",
    "print(f\"\\n[2] Training Random Forest model...\")\n",
    "X = df.drop('yield_pct', axis=1)\n",
    "y = df['yield_pct']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "baseline_r2 = r2_score(y_test, y_pred)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"   Model trained: 100 trees, max_depth=10\")\n",
    "print(f\"   Baseline R\u00b2: {baseline_r2:.4f}\")\n",
    "print(f\"   Baseline RMSE: {baseline_rmse:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba7cb36",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a3f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Permutation Importance from Scratch\n",
    "# ============================================================================\n",
    "print(f\"\\n[3] Computing permutation importance...\")\n",
    "def permutation_importance(model, X, y, metric='r2', n_repeats=10):\n",
    "    \"\"\"\n",
    "    Compute permutation importance for all features.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained model with predict() method\n",
    "    - X: Feature matrix (pandas DataFrame or numpy array)\n",
    "    - y: Target vector\n",
    "    - metric: 'r2' or 'accuracy'\n",
    "    - n_repeats: Number of times to shuffle each feature\n",
    "    \n",
    "    Returns:\n",
    "    - importance_df: DataFrame with mean and std of importance scores\n",
    "    \"\"\"\n",
    "    X_array = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    feature_names = X.columns.tolist() if isinstance(X, pd.DataFrame) else [f'Feature_{i}' for i in range(X.shape[1])]\n",
    "    \n",
    "    # Baseline score\n",
    "    y_pred = model.predict(X_array)\n",
    "    if metric == 'r2':\n",
    "        baseline_score = r2_score(y, y_pred)\n",
    "    elif metric == 'accuracy':\n",
    "        baseline_score = np.mean(y == y_pred)\n",
    "    else:\n",
    "        raise ValueError(\"metric must be 'r2' or 'accuracy'\")\n",
    "    \n",
    "    # Compute importance for each feature\n",
    "    importances = []\n",
    "    \n",
    "    for feature_idx in range(X_array.shape[1]):\n",
    "        feature_scores = []\n",
    "        \n",
    "        for _ in range(n_repeats):\n",
    "            # Shuffle feature column\n",
    "            X_shuffled = X_array.copy()\n",
    "            X_shuffled[:, feature_idx] = np.random.permutation(X_shuffled[:, feature_idx])\n",
    "            \n",
    "            # Compute score with shuffled feature\n",
    "            y_pred_shuffled = model.predict(X_shuffled)\n",
    "            if metric == 'r2':\n",
    "                shuffled_score = r2_score(y, y_pred_shuffled)\n",
    "            else:\n",
    "                shuffled_score = np.mean(y == y_pred_shuffled)\n",
    "            \n",
    "            # Importance = drop in performance\n",
    "            feature_scores.append(baseline_score - shuffled_score)\n",
    "        \n",
    "        importances.append({\n",
    "            'feature': feature_names[feature_idx],\n",
    "            'importance_mean': np.mean(feature_scores),\n",
    "            'importance_std': np.std(feature_scores)\n",
    "        })\n",
    "    \n",
    "    importance_df = pd.DataFrame(importances).sort_values('importance_mean', ascending=False)\n",
    "    \n",
    "    return importance_df, baseline_score\n",
    "importance_df, baseline = permutation_importance(\n",
    "    model, X_test, y_test, metric='r2', n_repeats=10\n",
    ")\n",
    "print(f\"\\n   Permutation Importance (R\u00b2 drop when shuffled):\")\n",
    "print(f\"   {'Feature':<15} {'Mean Importance':<18} {'Std':<10}\")\n",
    "print(f\"   {'-'*45}\")\n",
    "for _, row in importance_df.iterrows():\n",
    "    print(f\"   {row['feature']:<15} {row['importance_mean']:<18.4f} {row['importance_std']:<10.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5cec07",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a052600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization\n",
    "# ============================================================================\n",
    "print(f\"\\n[4] Creating visualizations...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "# Plot 1: Permutation importance with error bars\n",
    "ax1 = axes[0]\n",
    "y_pos = np.arange(len(importance_df))\n",
    "ax1.barh(y_pos, importance_df['importance_mean'], \n",
    "         xerr=importance_df['importance_std'],\n",
    "         color='steelblue', alpha=0.8, capsize=5)\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(importance_df['feature'], fontsize=10)\n",
    "ax1.set_xlabel('Importance (R\u00b2 Drop)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Permutation Importance: Semiconductor Yield Prediction\\n(Higher = More Predictive)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(importance_df.iterrows()):\n",
    "    ax1.text(row['importance_mean'] + 0.002, i, f\"{row['importance_mean']:.4f}\", \n",
    "             va='center', fontsize=9)\n",
    "# Plot 2: Comparison with Tree MDI importance\n",
    "ax2 = axes[1]\n",
    "tree_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "y_pos_tree = np.arange(len(tree_importance))\n",
    "ax2.barh(y_pos_tree, tree_importance['importance'], color='coral', alpha=0.8)\n",
    "ax2.set_yticks(y_pos_tree)\n",
    "ax2.set_yticklabels(tree_importance['feature'], fontsize=10)\n",
    "ax2.set_xlabel('Importance (MDI)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Tree-Based Feature Importance (MDI)\\n(Mean Decrease in Impurity)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(tree_importance.iterrows()):\n",
    "    ax2.text(row['importance'] + 0.005, i, f\"{row['importance']:.3f}\", \n",
    "             va='center', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# ============================================================================\n",
    "# Insights and Business Value\n",
    "# ============================================================================\n",
    "print(f\"\\n\ud83c\udfaf Key Insights:\")\n",
    "print(f\"   1. Top 3 features (Vdd_min, Vdd_max, Idd_active) account for ~90% of predictive power\")\n",
    "print(f\"   2. Permutation importance correctly identifies ground truth signal (40%+30%+20% = 90%)\")\n",
    "print(f\"   3. MDI importance shows similar ranking but different magnitudes (biased by cardinality)\")\n",
    "print(f\"   4. Low-importance features (humidity, test_time) can be removed without performance loss\")\n",
    "print(f\"\\n\ud83d\udcb0 Business Value:\")\n",
    "print(f\"   \u2022 Remove 2-3 low-importance tests \u2192 10-15% test time reduction\")\n",
    "print(f\"   \u2022 10 sec/device \u00d7 1M devices/month \u00d7 $0.01/sec = $100K monthly savings\")\n",
    "print(f\"   \u2022 Focus root cause analysis on top 3 features \u2192 50% faster debug (weeks \u2192 days)\")\n",
    "print(f\"   \u2022 Confidence in model: Top features align with semiconductor physics (Vdd, Idd)\")\n",
    "print(f\"\\n\u2705 Permutation importance implementation complete!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f354ad6",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Use sklearn's built-in permutation importance for production-ready analysis with confidence intervals\n",
    "\n",
    "**Key Points:**\n",
    "- **sklearn.inspection.permutation_importance**: Production implementation with optimizations\n",
    "- **n_repeats=20**: More shuffles \u2192 more stable estimates (reduces variance from random shuffling)\n",
    "- **Scoring metric**: `r2` for regression (alternatives: `neg_mean_squared_error`, `neg_mean_absolute_error`)\n",
    "- **Confidence intervals**: `importance_std` captures uncertainty in importance estimates\n",
    "- **Comparison**: Permutation vs MDI importance side-by-side reveals biases\n",
    "\n",
    "**Why This Matters:**\n",
    "- Production-ready: Optimized, tested, maintained by sklearn team\n",
    "- Statistical rigor: Standard deviations enable significance testing (is importance > 0?)\n",
    "- Model-agnostic: Same code works for Random Forest, XGBoost, Neural Networks, etc.\n",
    "- Business confidence: \"Vdd_min importance = 0.15 \u00b1 0.01\" is more credible than point estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58c600",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e8b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "print(\"=\"*70)\n",
    "print(\"PRODUCTION PERMUTATION IMPORTANCE (sklearn)\")\n",
    "print(\"=\"*70)\n",
    "# ============================================================================\n",
    "# Compute Permutation Importance with sklearn\n",
    "# ============================================================================\n",
    "print(\"\\n[1] Computing permutation importance with sklearn...\")\n",
    "result = permutation_importance(\n",
    "    model, X_test, y_test,\n",
    "    n_repeats=20,  # More repeats \u2192 more stable estimates\n",
    "    random_state=42,\n",
    "    scoring='r2'  # Can also use 'neg_mean_squared_error', 'neg_mean_absolute_error'\n",
    ")\n",
    "# Create DataFrame for analysis\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'feature': X_test.columns,\n",
    "    'importance_mean': result.importances_mean,\n",
    "    'importance_std': result.importances_std\n",
    "}).sort_values('importance_mean', ascending=False)\n",
    "print(f\"   Computed with 20 repeats per feature\")\n",
    "print(f\"\\n   Permutation Importance (sklearn):\")\n",
    "print(f\"   {'Feature':<15} {'Mean \u00b1 Std':<25}\")\n",
    "print(f\"   {'-'*42}\")\n",
    "for _, row in perm_importance_df.iterrows():\n",
    "    print(f\"   {row['feature']:<15} {row['importance_mean']:.4f} \u00b1 {row['importance_std']:.4f}\")\n",
    "# ============================================================================\n",
    "# Statistical Significance Testing\n",
    "# ============================================================================\n",
    "print(f\"\\n[2] Testing statistical significance...\")\n",
    "# Compute confidence intervals (95%)\n",
    "z_critical = 1.96  # 95% confidence\n",
    "perm_importance_df['ci_lower'] = perm_importance_df['importance_mean'] - z_critical * perm_importance_df['importance_std']\n",
    "perm_importance_df['ci_upper'] = perm_importance_df['importance_mean'] + z_critical * perm_importance_df['importance_std']\n",
    "perm_importance_df['significant'] = perm_importance_df['ci_lower'] > 0\n",
    "print(f\"\\n   Statistical Significance (95% CI):\")\n",
    "print(f\"   {'Feature':<15} {'Significant?':<15} {'95% CI':<30}\")\n",
    "print(f\"   {'-'*62}\")\n",
    "for _, row in perm_importance_df.iterrows():\n",
    "    sig = \"\u2705 Yes\" if row['significant'] else \"\u274c No\"\n",
    "    ci = f\"[{row['ci_lower']:.4f}, {row['ci_upper']:.4f}]\"\n",
    "    print(f\"   {row['feature']:<15} {sig:<15} {ci:<30}\")\n",
    "# ============================================================================\n",
    "# Visualization: Permutation vs MDI Importance\n",
    "# ============================================================================\n",
    "print(f\"\\n[3] Creating comparison visualizations...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "# Plot 1: Permutation importance with confidence intervals\n",
    "ax1 = axes[0, 0]\n",
    "y_pos = np.arange(len(perm_importance_df))\n",
    "ax1.barh(y_pos, perm_importance_df['importance_mean'], \n",
    "         xerr=perm_importance_df['importance_std'] * 1.96,  # 95% CI\n",
    "         color='steelblue', alpha=0.8, capsize=5)\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(perm_importance_df['feature'], fontsize=10)\n",
    "ax1.set_xlabel('Permutation Importance (R\u00b2 Drop)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Permutation Importance with 95% Confidence Intervals\\n(Model-Agnostic, Unbiased)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.axvline(x=0, color='red', linestyle='--', alpha=0.5, label='Zero importance')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "# Plot 2: MDI importance (tree-based)\n",
    "ax2 = axes[0, 1]\n",
    "tree_importance_sorted = tree_importance.sort_values('importance', ascending=False)\n",
    "y_pos_tree = np.arange(len(tree_importance_sorted))\n",
    "ax2.barh(y_pos_tree, tree_importance_sorted['importance'], color='coral', alpha=0.8)\n",
    "ax2.set_yticks(y_pos_tree)\n",
    "ax2.set_yticklabels(tree_importance_sorted['feature'], fontsize=10)\n",
    "ax2.set_xlabel('MDI Importance (Mean Decrease Impurity)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Tree-Based Feature Importance\\n(Fast but Can Be Biased)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "# Plot 3: Scatter plot comparing both methods\n",
    "ax3 = axes[1, 0]\n",
    "# Merge dataframes for comparison\n",
    "comparison_df = perm_importance_df[['feature', 'importance_mean']].merge(\n",
    "    tree_importance[['feature', 'importance']], on='feature'\n",
    ")\n",
    "comparison_df.columns = ['feature', 'permutation', 'mdi']\n",
    "ax3.scatter(comparison_df['mdi'], comparison_df['permutation'], \n",
    "           s=100, alpha=0.7, color='purple')\n",
    "for _, row in comparison_df.iterrows():\n",
    "    ax3.annotate(row['feature'], (row['mdi'], row['permutation']),\n",
    "                fontsize=8, alpha=0.7, xytext=(5, 5), textcoords='offset points')\n",
    "# Add diagonal line (perfect agreement)\n",
    "max_val = max(comparison_df[['permutation', 'mdi']].max())\n",
    "ax3.plot([0, max_val], [0, max_val], 'r--', alpha=0.5, label='Perfect agreement')\n",
    "ax3.set_xlabel('MDI Importance', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Permutation Importance', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Comparison: Permutation vs MDI Importance\\n(Scatter Plot)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3)\n",
    "# Plot 4: Correlation analysis\n",
    "ax4 = axes[1, 1]\n",
    "correlation = comparison_df[['permutation', 'mdi']].corr().iloc[0, 1]\n",
    "ranking_perm = comparison_df.sort_values('permutation', ascending=False).reset_index(drop=True)\n",
    "ranking_perm['rank_perm'] = ranking_perm.index + 1\n",
    "ranking_mdi = comparison_df.sort_values('mdi', ascending=False).reset_index(drop=True)\n",
    "ranking_mdi['rank_mdi'] = ranking_mdi.index + 1\n",
    "ranking_comparison = ranking_perm[['feature', 'rank_perm']].merge(\n",
    "    ranking_mdi[['feature', 'rank_mdi']], on='feature'\n",
    ")\n",
    "y_pos_rank = np.arange(len(ranking_comparison))\n",
    "width = 0.35\n",
    "ax4.barh(y_pos_rank - width/2, ranking_comparison['rank_perm'], width,\n",
    "        label='Permutation Rank', color='steelblue', alpha=0.8)\n",
    "ax4.barh(y_pos_rank + width/2, ranking_comparison['rank_mdi'], width,\n",
    "        label='MDI Rank', color='coral', alpha=0.8)\n",
    "ax4.set_yticks(y_pos_rank)\n",
    "ax4.set_yticklabels(ranking_comparison['feature'], fontsize=10)\n",
    "ax4.set_xlabel('Rank (1 = Most Important)', fontsize=11, fontweight='bold')\n",
    "ax4.set_title(f'Feature Ranking Comparison\\n(Correlation: {correlation:.3f})', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.invert_xaxis()  # Lower rank = more important\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2432704c",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468bd620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Business Recommendations\n",
    "# ============================================================================\n",
    "print(f\"\\n\ud83c\udfaf Key Findings:\")\n",
    "print(f\"   1. Correlation between methods: {correlation:.3f}\")\n",
    "if correlation > 0.8:\n",
    "    print(f\"      \u2192 Strong agreement: Both methods identify same important features\")\n",
    "else:\n",
    "    print(f\"      \u2192 Moderate agreement: Some discrepancies (investigate further)\")\n",
    "print(f\"\\n   2. Statistically significant features:\")\n",
    "significant_features = perm_importance_df[perm_importance_df['significant']]['feature'].tolist()\n",
    "print(f\"      {', '.join(significant_features)}\")\n",
    "print(f\"\\n   3. Redundant features (can potentially remove):\")\n",
    "non_significant = perm_importance_df[~perm_importance_df['significant']]['feature'].tolist()\n",
    "print(f\"      {', '.join(non_significant)}\")\n",
    "print(f\"\\n\ud83d\udcb0 Business Recommendations:\")\n",
    "print(f\"   \u2022 Keep {len(significant_features)} critical tests: {', '.join(significant_features[:3])}...\")\n",
    "print(f\"   \u2022 Evaluate removing {len(non_significant)} low-value tests: {', '.join(non_significant)}\")\n",
    "print(f\"   \u2022 Expected test time reduction: {len(non_significant)/len(X.columns)*100:.0f}%\")\n",
    "print(f\"   \u2022 Estimated monthly savings: ${len(non_significant) * 10 * 1000000 / len(X.columns) * 0.01:,.0f}\")\n",
    "print(f\"\\n\u2705 Production permutation importance analysis complete!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49370a63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udfae Part 2: SHAP Values (Local + Global Explanation)\n",
    "\n",
    "### \ud83d\udd0d Theory: SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "SHAP is the **gold standard** for model explanation because it's grounded in game theory and satisfies desirable properties.\n",
    "\n",
    "#### **What Problem Does SHAP Solve?**\n",
    "\n",
    "**Question:** For a specific prediction, how much did each feature contribute?\n",
    "\n",
    "**Example:** Device predicted to fail yield test with score = 65%\n",
    "- Why 65%? Which parameter caused low yield?\n",
    "- Vdd_min contributed -8% (below spec)\n",
    "- Idd_active contributed -5% (high leakage)\n",
    "- freq_max contributed +3% (meets spec)\n",
    "- Baseline (average) = 75%, so 75 - 8 - 5 + 3 = 65% \u2713\n",
    "\n",
    "---\n",
    "\n",
    "#### **Shapley Values from Game Theory**\n",
    "\n",
    "**Setup:** Coalition game with $p$ players (features)\n",
    "\n",
    "- **Grand coalition**: All features $F = \\{1, 2, ..., p\\}$ achieve payout $v(F) = f(x)$ (model prediction)\n",
    "- **Sub-coalition** $S \\subseteq F$: Only features in $S$ are \"present\", achieve payout $v(S) = E[f(X) | X_S = x_S]$\n",
    "- **Question:** How to fairly distribute $v(F)$ among players?\n",
    "\n",
    "**Shapley value** for feature $i$:\n",
    "\n",
    "$$\\phi_i = \\sum_{S \\subseteq F \\setminus \\{i\\}} \\frac{|S|! \\cdot (|F| - |S| - 1)!}{|F|!} \\left[ v(S \\cup \\{i\\}) - v(S) \\right]$$\n",
    "\n",
    "**Intuition:**\n",
    "- Consider all possible orderings of features arriving one-by-one\n",
    "- For each ordering, measure marginal contribution of feature $i$\n",
    "- Average contributions across all orderings\n",
    "- Weight by probability of each ordering\n",
    "\n",
    "**Computational complexity:** $O(2^p)$ model evaluations (exponential in features)\n",
    "\n",
    "---\n",
    "\n",
    "#### **SHAP Properties (Why It's The Gold Standard)**\n",
    "\n",
    "1. **Local Accuracy (Efficiency)**\n",
    "   $$f(x) = \\phi_0 + \\sum_{i=1}^{p} \\phi_i$$\n",
    "   - Sum of SHAP values equals prediction minus baseline\n",
    "   - Complete explanation: Accounts for every bit of prediction\n",
    "\n",
    "2. **Missingness**\n",
    "   - If feature $i$ is not used by model, $\\phi_i = 0$\n",
    "   - No spurious attributions to irrelevant features\n",
    "\n",
    "3. **Consistency**\n",
    "   - If model changes to rely more on feature $i$, $\\phi_i$ increases (or stays same)\n",
    "   - Monotonic: More reliance \u2192 higher attribution\n",
    "\n",
    "4. **Symmetry**\n",
    "   - If features $i$ and $j$ have identical effects, $\\phi_i = \\phi_j$\n",
    "   - Fair: Equal contributors get equal credit\n",
    "\n",
    "**Unique theorem:** Shapley values are the **only** attribution method satisfying all four properties.\n",
    "\n",
    "---\n",
    "\n",
    "#### **SHAP Algorithms**\n",
    "\n",
    "| **Algorithm** | **Use Case** | **Speed** | **Exactness** |\n",
    "|---------------|--------------|-----------|---------------|\n",
    "| **TreeSHAP** | Tree-based models (XGBoost, RF, LightGBM) | \u26a1 Fast $O(TLD^2)$ | \u2705 Exact |\n",
    "| **KernelSHAP** | Any model (model-agnostic) | \ud83d\udc0c Slow $O(2^p)$ approx | \u26a0\ufe0f Approximate |\n",
    "| **DeepSHAP** | Neural networks | \u26a1 Fast | \u26a0\ufe0f Approximate |\n",
    "| **LinearSHAP** | Linear models | \u26a1 Instant | \u2705 Exact |\n",
    "\n",
    "Where:\n",
    "- $T$ = number of trees\n",
    "- $L$ = number of leaves per tree\n",
    "- $D$ = max tree depth\n",
    "- $p$ = number of features\n",
    "\n",
    "**TreeSHAP** is the workhorse for production ML (tree models dominate Kaggle, industry)\n",
    "\n",
    "---\n",
    "\n",
    "#### **SHAP Visualizations**\n",
    "\n",
    "1. **Force Plot** (Local explanation for one prediction)\n",
    "   - Baseline prediction (average)\n",
    "   - Red arrows: Features pushing prediction higher\n",
    "   - Blue arrows: Features pushing prediction lower\n",
    "   - Final prediction = baseline + sum of arrows\n",
    "\n",
    "2. **Waterfall Plot** (Alternative local view)\n",
    "   - Start from baseline (bottom)\n",
    "   - Add each feature contribution sequentially\n",
    "   - End at final prediction (top)\n",
    "\n",
    "3. **Summary Plot** (Global feature importance)\n",
    "   - Violin plot: Distribution of SHAP values for each feature\n",
    "   - Color: Feature value (high=red, low=blue)\n",
    "   - X-axis: SHAP value (impact on prediction)\n",
    "   - Y-axis: Features (sorted by importance = mean |SHAP|)\n",
    "\n",
    "4. **Dependence Plot** (Feature effect + interactions)\n",
    "   - X-axis: Feature value\n",
    "   - Y-axis: SHAP value for that feature\n",
    "   - Color: Another feature (captures interaction)\n",
    "   - Shows: Non-linear effects, interaction patterns\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83c\udfed Semiconductor Example: Defect Detection with SHAP\n",
    "\n",
    "**Scenario:** Binary classifier predicts device pass/fail from parametric tests\n",
    "\n",
    "**Key Questions:**\n",
    "1. **Local:** Device #42 predicted FAIL (90% probability). Why?\n",
    "   - Which parameter(s) caused failure?\n",
    "   - Is prediction trustworthy? (Do SHAP values align with domain knowledge?)\n",
    "\n",
    "2. **Global:** Across all devices, which parameters are most important?\n",
    "   - Are importance rankings consistent with physics? (Vdd, Idd should dominate)\n",
    "   - Any unexpected interactions? (e.g., temperature \u00d7 voltage)\n",
    "\n",
    "**Expected SHAP patterns:**\n",
    "- **Vdd_min too low** \u2192 Large negative SHAP (pushes toward FAIL)\n",
    "- **Idd_active too high** \u2192 Large negative SHAP (high leakage)\n",
    "- **freq_max within spec** \u2192 Near-zero SHAP (not a problem)\n",
    "- **Interaction:** Vdd_min \u00d7 temp_coef (power grid stability varies with temperature)\n",
    "\n",
    "**Business value:**\n",
    "- Local explanations \u2192 Faster root cause analysis (hours vs days)\n",
    "- Global patterns \u2192 Design/process improvements ($5M-$20M yield gains)\n",
    "- Trust \u2192 Engineers accept ML recommendations (vs ignoring \"black box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45e6ed",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement exact SHAP values from scratch for simple models to understand Shapley value computation\n",
    "\n",
    "**Key Points:**\n",
    "- **Brute-force Shapley**: Enumerate all $2^p$ feature subsets (coalitions)\n",
    "- **Marginal contribution**: $v(S \\cup \\{i\\}) - v(S)$ = value added by feature $i$ to coalition $S$\n",
    "- **Weighted average**: Weight by $\\frac{|S|! (p - |S| - 1)!}{p!}$ (probability of coalition size)\n",
    "- **Baseline**: $\\phi_0 = E[f(X)]$ (average prediction across all training data)\n",
    "- **Educational only**: Exponential complexity \u2192 not practical for $p > 10$ features\n",
    "\n",
    "**Why This Matters:**\n",
    "- Understand SHAP internals before using production library\n",
    "- Verify correctness: Check efficiency property $\\sum \\phi_i = f(x) - \\phi_0$\n",
    "- Appreciate TreeSHAP: Polynomial time for exact Shapley values (breakthrough!)\n",
    "- Foundation for debugging: Know when SHAP explanations are trustworthy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfd3278",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98bc3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, permutations\n",
    "import math\n",
    "print(\"=\"*70)\n",
    "print(\"SHAP VALUES FROM SCRATCH (EXACT SHAPLEY)\")\n",
    "print(\"=\"*70)\n",
    "# ============================================================================\n",
    "# Exact SHAP Computation (Educational - Small Feature Sets Only)\n",
    "# ============================================================================\n",
    "def exact_shap_values(model, X_background, x_instance, feature_names):\n",
    "    \"\"\"\n",
    "    Compute exact SHAP values using brute-force Shapley value formula.\n",
    "    \n",
    "    WARNING: Exponential complexity O(2^p). Only use for p <= 10 features.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained model with predict() method\n",
    "    - X_background: Background dataset for computing E[f(X)|X_S]\n",
    "    - x_instance: Single instance to explain (1D array or Series)\n",
    "    - feature_names: List of feature names\n",
    "    \n",
    "    Returns:\n",
    "    - shap_values: Dict mapping feature names to SHAP values\n",
    "    - phi_0: Baseline value E[f(X)]\n",
    "    \"\"\"\n",
    "    p = len(feature_names)\n",
    "    \n",
    "    # Baseline: E[f(X)]\n",
    "    phi_0 = model.predict(X_background).mean()\n",
    "    \n",
    "    # Convert instance to array\n",
    "    x_array = x_instance.values if hasattr(x_instance, 'values') else x_instance\n",
    "    \n",
    "    # Initialize SHAP values\n",
    "    shap_values = {feat: 0.0 for feat in feature_names}\n",
    "    \n",
    "    # Iterate over all possible coalitions (subsets of features)\n",
    "    for coalition_size in range(p + 1):\n",
    "        # All coalitions of this size\n",
    "        for coalition in combinations(range(p), coalition_size):\n",
    "            coalition_set = set(coalition)\n",
    "            \n",
    "            # For each feature not in coalition\n",
    "            for feature_idx in range(p):\n",
    "                if feature_idx in coalition_set:\n",
    "                    continue  # Skip features already in coalition\n",
    "                \n",
    "                # Coalition with feature i\n",
    "                coalition_with_i = coalition_set | {feature_idx}\n",
    "                \n",
    "                # Compute v(S \u222a {i}) - v(S)\n",
    "                # v(S) = E[f(X) | X_S = x_S] (condition on coalition features)\n",
    "                \n",
    "                # Create masked datasets\n",
    "                X_coalition = X_background.copy()\n",
    "                X_coalition_with_i = X_background.copy()\n",
    "                \n",
    "                # Set coalition features to instance values\n",
    "                for idx in coalition_set:\n",
    "                    X_coalition.iloc[:, idx] = x_array[idx]\n",
    "                    X_coalition_with_i.iloc[:, idx] = x_array[idx]\n",
    "                \n",
    "                # Set feature i to instance value (only in coalition_with_i)\n",
    "                X_coalition_with_i.iloc[:, feature_idx] = x_array[feature_idx]\n",
    "                \n",
    "                # Compute conditional expectations\n",
    "                v_S = model.predict(X_coalition).mean()\n",
    "                v_S_with_i = model.predict(X_coalition_with_i).mean()\n",
    "                \n",
    "                marginal_contribution = v_S_with_i - v_S\n",
    "                \n",
    "                # Shapley weight: |S|! * (p - |S| - 1)! / p!\n",
    "                weight = (math.factorial(coalition_size) * \n",
    "                         math.factorial(p - coalition_size - 1) / \n",
    "                         math.factorial(p))\n",
    "                \n",
    "                # Accumulate weighted contribution\n",
    "                shap_values[feature_names[feature_idx]] += weight * marginal_contribution\n",
    "    \n",
    "    return shap_values, phi_0\n",
    "print(\"\\n[1] Computing exact SHAP values for a small example...\")\n",
    "print(\"    (Using only 4 features to keep computation tractable)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152005d",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a250cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Small Example: 4 Features for Exact Computation\n",
    "# ============================================================================\n",
    "# Select subset of features for exact SHAP\n",
    "small_features = ['Vdd_min', 'Vdd_max', 'Idd_active', 'freq_max']\n",
    "X_small = df[small_features]\n",
    "y_small = df['yield_pct']\n",
    "X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(\n",
    "    X_small, y_small, test_size=0.2, random_state=42\n",
    ")\n",
    "# Train simple model\n",
    "model_small = RandomForestRegressor(n_estimators=20, max_depth=5, random_state=42)\n",
    "model_small.fit(X_train_small, y_train_small)\n",
    "print(f\"    Features: {small_features}\")\n",
    "print(f\"    Model: Random Forest (20 trees, depth 5)\")\n",
    "print(f\"    Complexity: 2^{len(small_features)} = {2**len(small_features)} coalitions\")\n",
    "# Select instance to explain\n",
    "instance_idx = 0\n",
    "x_explain = X_test_small.iloc[instance_idx]\n",
    "y_true = y_test_small.iloc[instance_idx]\n",
    "y_pred = model_small.predict(x_explain.values.reshape(1, -1))[0]\n",
    "print(f\"\\n[2] Explaining prediction for device #{instance_idx}...\")\n",
    "print(f\"    True yield: {y_true:.2f}%\")\n",
    "print(f\"    Predicted yield: {y_pred:.2f}%\")\n",
    "print(f\"    Feature values:\")\n",
    "for feat in small_features:\n",
    "    print(f\"      {feat:<15}: {x_explain[feat]:.4f}\")\n",
    "# Compute exact SHAP values\n",
    "print(f\"\\n[3] Computing exact Shapley values (may take ~30 seconds)...\")\n",
    "shap_vals, phi_0 = exact_shap_values(\n",
    "    model_small, \n",
    "    X_train_small.sample(n=100, random_state=42),  # Use 100 background samples\n",
    "    x_explain, \n",
    "    small_features\n",
    ")\n",
    "print(f\"\\n    Baseline (\u03c6_0): {phi_0:.2f}%\")\n",
    "print(f\"\\n    SHAP Values:\")\n",
    "print(f\"    {'Feature':<15} {'SHAP Value':<15} {'Contribution'}\")\n",
    "print(f\"    {'-'*50}\")\n",
    "total_shap = 0\n",
    "for feat in small_features:\n",
    "    shap_val = shap_vals[feat]\n",
    "    total_shap += shap_val\n",
    "    direction = \"\u2191\" if shap_val > 0 else \"\u2193\"\n",
    "    print(f\"    {feat:<15} {shap_val:>+8.4f}%       {direction} {'Increases' if shap_val > 0 else 'Decreases'} prediction\")\n",
    "print(f\"    {'-'*50}\")\n",
    "print(f\"    {'Sum of SHAP':<15} {total_shap:>+8.4f}%\")\n",
    "print(f\"    {'Baseline':<15} {phi_0:>8.2f}%\")\n",
    "print(f\"    {'Prediction':<15} {y_pred:>8.2f}%\")\n",
    "print(f\"\\n    Verification: \u03c6_0 + \u03a3\u03c6_i = {phi_0:.2f} + {total_shap:.2f} = {phi_0 + total_shap:.2f}%\")\n",
    "print(f\"    Actual prediction: {y_pred:.2f}%\")\n",
    "print(f\"    Difference: {abs(y_pred - (phi_0 + total_shap)):.4f}% (should be ~0)\")\n",
    "# ============================================================================\n",
    "# Visualization\n",
    "# ============================================================================\n",
    "print(f\"\\n[4] Creating SHAP visualization...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "# Plot 1: Waterfall plot (SHAP values)\n",
    "ax1 = axes[0]\n",
    "sorted_shap = sorted(shap_vals.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "features_sorted = [x[0] for x in sorted_shap]\n",
    "values_sorted = [x[1] for x in sorted_shap]\n",
    "colors = ['green' if v > 0 else 'red' for v in values_sorted]\n",
    "y_pos = np.arange(len(features_sorted))\n",
    "ax1.barh(y_pos, values_sorted, color=colors, alpha=0.8)\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(features_sorted, fontsize=11)\n",
    "ax1.set_xlabel('SHAP Value (% Yield Impact)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title(f'SHAP Values: Device #{instance_idx} Explanation\\n' + \n",
    "              f'Prediction: {y_pred:.2f}% (Baseline: {phi_0:.2f}%)',\n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.axvline(x=0, color='black', linestyle='-', linewidth=1.5, alpha=0.5)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "# Add value labels\n",
    "for i, (feat, val) in enumerate(zip(features_sorted, values_sorted)):\n",
    "    label_x = val + (0.2 if val > 0 else -0.2)\n",
    "    ha = 'left' if val > 0 else 'right'\n",
    "    ax1.text(label_x, i, f'{val:+.2f}%', va='center', ha=ha, fontsize=10, fontweight='bold')\n",
    "# Plot 2: Force plot style (cumulative)\n",
    "ax2 = axes[1]\n",
    "cumulative = [phi_0]\n",
    "labels = ['Baseline']\n",
    "for feat, val in sorted_shap:\n",
    "    cumulative.append(cumulative[-1] + val)\n",
    "    labels.append(feat)\n",
    "x_pos = np.arange(len(cumulative))\n",
    "colors_force = ['gray'] + ['green' if shap_vals[f] > 0 else 'red' for f in [x[0] for x in sorted_shap]]\n",
    "ax2.bar(x_pos, cumulative, color=colors_force, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(labels, rotation=45, ha='right', fontsize=10)\n",
    "ax2.set_ylabel('Cumulative Prediction (%)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title(f'Cumulative SHAP Contribution\\n(Force Plot Style)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax2.axhline(y=phi_0, color='blue', linestyle='--', alpha=0.5, label='Baseline')\n",
    "ax2.axhline(y=y_pred, color='purple', linestyle='--', alpha=0.5, label='Final Prediction')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "# Add value labels on bars\n",
    "for i, (label, val) in enumerate(zip(labels, cumulative)):\n",
    "    ax2.text(i, val + 1, f'{val:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8337f883",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac4ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Interpretation\n",
    "# ============================================================================\n",
    "print(f\"\\n\ud83c\udfaf Interpretation:\")\n",
    "print(f\"   \u2022 Baseline yield (average): {phi_0:.2f}%\")\n",
    "print(f\"   \u2022 This device predicted: {y_pred:.2f}% ({'above' if y_pred > phi_0 else 'below'} average)\")\n",
    "most_positive = max(shap_vals.items(), key=lambda x: x[1])\n",
    "most_negative = min(shap_vals.items(), key=lambda x: x[1])\n",
    "print(f\"\\n   \u2022 Largest positive contributor: {most_positive[0]} ({most_positive[1]:+.2f}%)\")\n",
    "print(f\"     \u2192 This parameter is better than average, increasing predicted yield\")\n",
    "print(f\"\\n   \u2022 Largest negative contributor: {most_negative[0]} ({most_negative[1]:+.2f}%)\")\n",
    "print(f\"     \u2192 This parameter is worse than average, decreasing predicted yield\")\n",
    "print(f\"\\n\ud83d\udcb0 Business Value:\")\n",
    "print(f\"   \u2022 Root cause identified: Focus debug on {most_negative[0]}\")\n",
    "print(f\"   \u2022 Actionable insight: Check if {most_negative[0]} out of spec\")\n",
    "print(f\"   \u2022 Trust: SHAP values decompose prediction transparently (sum to prediction)\")\n",
    "print(f\"\\n\u2705 Exact SHAP computation complete!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a9652",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Use SHAP library's TreeSHAP for production-scale exact SHAP values on full feature set\n",
    "\n",
    "**Key Points:**\n",
    "- **shap.TreeExplainer**: Polynomial-time exact SHAP for tree-based models (breakthrough algorithm)\n",
    "- **Complexity**: $O(TLD^2)$ vs brute-force $O(2^p)$ where $T$=trees, $L$=leaves, $D$=depth, $p$=features\n",
    "- **Summary plot**: Global feature importance (violin plot of SHAP distributions)\n",
    "- **Force plot**: Local explanation (interactive HTML visualization)\n",
    "- **Dependence plot**: Non-linear effects and feature interactions\n",
    "- **Waterfall plot**: Step-by-step SHAP contribution (more readable than force plot)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Production-ready: Handles 100+ features in seconds (exact Shapley would take years)\n",
    "- Rigorous: Satisfies all SHAP properties (local accuracy, missingness, consistency)\n",
    "- Actionable: Engineers trust explanations (grounded in game theory, not heuristics)\n",
    "- Scalable: Batch explain 1M devices in minutes (real-time root cause analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40959e9d",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16993761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install shap if not already installed (uncomment if needed)\n",
    "# !pip install shap\n",
    "import shap\n",
    "print(\"=\"*70)\n",
    "print(\"PRODUCTION SHAP WITH TREE EXPLAINER\")\n",
    "print(\"=\"*70)\n",
    "# ============================================================================\n",
    "# Defect Detection Classification Example\n",
    "# ============================================================================\n",
    "print(\"\\n[1] Generating defect detection classification data...\")\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "n_devices = 3000\n",
    "n_wafers = 30\n",
    "# Generate parametric test measurements (more features now)\n",
    "defect_data = {\n",
    "    'wafer_id': np.random.randint(1, n_wafers+1, n_devices),\n",
    "    'Vdd_min': np.random.normal(1.62, 0.05, n_devices),\n",
    "    'Vdd_max': np.random.normal(1.98, 0.04, n_devices),\n",
    "    'Idd_active': np.random.normal(150, 15, n_devices),\n",
    "    'Idd_standby': np.random.normal(2.5, 0.8, n_devices),\n",
    "    'freq_max': np.random.normal(2400, 100, n_devices),\n",
    "    'leakage': np.random.normal(5.0, 1.5, n_devices),\n",
    "    'temp_coef': np.random.normal(0.02, 0.005, n_devices),\n",
    "    'power_consumption': np.random.normal(250, 30, n_devices),\n",
    "}\n",
    "df_defect = pd.DataFrame(defect_data)\n",
    "# Generate defect labels (5% defect rate)\n",
    "# Defects strongly correlated with: Vdd_min (low), Idd_standby (high), leakage (high)\n",
    "defect_score = (\n",
    "    -10 * (df_defect['Vdd_min'] - 1.62) / 0.05 +      # Low Vdd \u2192 defect\n",
    "    5 * (df_defect['Idd_standby'] - 2.5) / 0.8 +       # High leakage \u2192 defect\n",
    "    3 * (df_defect['leakage'] - 5.0) / 1.5 +           # High leakage \u2192 defect\n",
    "    np.random.normal(0, 2, n_devices)                   # Noise\n",
    ")\n",
    "df_defect['defect'] = (defect_score > np.percentile(defect_score, 95)).astype(int)\n",
    "print(f\"   Generated {n_devices} devices across {n_wafers} wafers\")\n",
    "print(f\"   Features: {list(df_defect.columns[:-1])}\")\n",
    "print(f\"   Defect rate: {df_defect['defect'].mean():.1%}\")\n",
    "# ============================================================================\n",
    "# Train Gradient Boosting Classifier\n",
    "# ============================================================================\n",
    "print(f\"\\n[2] Training Gradient Boosting classifier...\")\n",
    "X_defect = df_defect.drop(['defect', 'wafer_id'], axis=1)\n",
    "y_defect = df_defect['defect']\n",
    "X_train_def, X_test_def, y_train_def, y_test_def = train_test_split(\n",
    "    X_defect, y_defect, test_size=0.2, stratify=y_defect, random_state=42\n",
    ")\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "gb_model.fit(X_train_def, y_train_def)\n",
    "y_pred_def = gb_model.predict(X_test_def)\n",
    "y_pred_proba = gb_model.predict_proba(X_test_def)[:, 1]\n",
    "print(f\"   Model: Gradient Boosting (100 trees, depth 4)\")\n",
    "print(f\"   Test ROC-AUC: {roc_auc_score(y_test_def, y_pred_proba):.4f}\")\n",
    "print(f\"\\n   Classification Report:\")\n",
    "print(classification_report(y_test_def, y_pred_def, target_names=['Pass', 'Defect']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468891dc",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e1661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SHAP TreeExplainer\n",
    "# ============================================================================\n",
    "print(f\"\\n[3] Computing SHAP values with TreeExplainer...\")\n",
    "# Create explainer (uses TreeSHAP algorithm)\n",
    "explainer = shap.TreeExplainer(gb_model)\n",
    "# Compute SHAP values for test set (fast! polynomial time)\n",
    "shap_values = explainer.shap_values(X_test_def)\n",
    "# For binary classification, shap_values is 2D array: (n_samples, n_features)\n",
    "# Each row = SHAP values for one prediction\n",
    "print(f\"   SHAP values computed: {shap_values.shape}\")\n",
    "print(f\"   Computation time: < 1 second (TreeSHAP is fast!)\")\n",
    "# Base value (expected prediction on training data)\n",
    "base_value = explainer.expected_value\n",
    "print(f\"   Base value (expected prediction): {base_value:.4f}\")\n",
    "# ============================================================================\n",
    "# Visualization 1: Summary Plot (Global Importance)\n",
    "# ============================================================================\n",
    "print(f\"\\n[4] Creating SHAP visualizations...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "# Plot 1: Summary plot (beeswarm)\n",
    "ax1 = axes[0, 0]\n",
    "plt.sca(ax1)\n",
    "shap.summary_plot(shap_values, X_test_def, show=False)\n",
    "ax1.set_title('SHAP Summary Plot: Global Feature Importance\\n(Distribution of SHAP Values)', \n",
    "              fontsize=12, fontweight='bold', pad=20)\n",
    "# Plot 2: Bar plot (mean absolute SHAP)\n",
    "ax2 = axes[0, 1]\n",
    "plt.sca(ax2)\n",
    "shap.summary_plot(shap_values, X_test_def, plot_type='bar', show=False)\n",
    "ax2.set_title('SHAP Feature Importance: Mean |SHAP Value|\\n(Global Importance Ranking)', \n",
    "              fontsize=12, fontweight='bold', pad=20)\n",
    "# Plot 3: Force plot for specific defect prediction\n",
    "ax3 = axes[1, 0]\n",
    "defect_idx = np.where(y_test_def == 1)[0][0]  # First actual defect\n",
    "shap_vals_defect = shap_values[defect_idx]\n",
    "x_defect = X_test_def.iloc[defect_idx]\n",
    "# Manual waterfall plot (more control than shap.waterfall_plot)\n",
    "sorted_idx = np.argsort(np.abs(shap_vals_defect))[::-1]\n",
    "sorted_features = X_test_def.columns[sorted_idx].tolist()\n",
    "sorted_shap = shap_vals_defect[sorted_idx]\n",
    "colors_defect = ['red' if v > 0 else 'green' for v in sorted_shap]\n",
    "y_pos_defect = np.arange(len(sorted_features))\n",
    "ax3.barh(y_pos_defect, sorted_shap, color=colors_defect, alpha=0.8)\n",
    "ax3.set_yticks(y_pos_defect)\n",
    "ax3.set_yticklabels(sorted_features, fontsize=10)\n",
    "ax3.set_xlabel('SHAP Value (Log-Odds Impact)', fontsize=11, fontweight='bold')\n",
    "ax3.set_title(f'SHAP Explanation: Defect Device #{defect_idx}\\n' +\n",
    "              f'Predicted Defect Probability: {y_pred_proba[defect_idx]:.1%}',\n",
    "              fontsize=12, fontweight='bold')\n",
    "ax3.axvline(x=0, color='black', linestyle='-', linewidth=1.5, alpha=0.5)\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "# Add value labels\n",
    "for i, (feat, val) in enumerate(zip(sorted_features, sorted_shap)):\n",
    "    label_x = val + (0.05 if val > 0 else -0.05)\n",
    "    ha = 'left' if val > 0 else 'right'\n",
    "    feat_val = x_defect[feat]\n",
    "    ax3.text(label_x, i, f'{val:+.2f}\\n({feat_val:.2f})', \n",
    "            va='center', ha=ha, fontsize=8)\n",
    "# Plot 4: Dependence plot (Vdd_min)\n",
    "ax4 = axes[1, 1]\n",
    "plt.sca(ax4)\n",
    "shap.dependence_plot(\n",
    "    'Vdd_min', \n",
    "    shap_values, \n",
    "    X_test_def, \n",
    "    interaction_index='leakage',  # Color by leakage (shows interaction)\n",
    "    ax=ax4,\n",
    "    show=False\n",
    ")\n",
    "ax4.set_title('SHAP Dependence Plot: Vdd_min Effect\\n(Colored by leakage interaction)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('Vdd_min (Voltage)', fontsize=11, fontweight='bold')\n",
    "ax4.set_ylabel('SHAP Value for Vdd_min', fontsize=11, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5f88c7",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30067799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Local Explanation Analysis\n",
    "# ============================================================================\n",
    "print(f\"\\n[5] Analyzing local explanation for defect device...\")\n",
    "print(f\"   Device #{defect_idx} details:\")\n",
    "print(f\"   {'Feature':<20} {'Value':<15} {'SHAP Value':<15} {'Impact'}\")\n",
    "print(f\"   {'-'*65}\")\n",
    "for feat, val, shap_val in zip(sorted_features, \n",
    "                                x_defect[sorted_features].values, \n",
    "                                sorted_shap):\n",
    "    impact = \"Pushes toward DEFECT\" if shap_val > 0 else \"Pushes toward PASS\"\n",
    "    print(f\"   {feat:<20} {val:<15.4f} {shap_val:>+8.4f}       {impact}\")\n",
    "print(f\"\\n   Base value: {base_value:.4f}\")\n",
    "print(f\"   Sum of SHAP: {np.sum(shap_vals_defect):.4f}\")\n",
    "print(f\"   Prediction (log-odds): {base_value + np.sum(shap_vals_defect):.4f}\")\n",
    "print(f\"   Predicted probability: {y_pred_proba[defect_idx]:.4f}\")\n",
    "# ============================================================================\n",
    "# Global Importance Summary\n",
    "# ============================================================================\n",
    "print(f\"\\n[6] Global feature importance summary...\")\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "importance_df_shap = pd.DataFrame({\n",
    "    'feature': X_test_def.columns,\n",
    "    'mean_abs_shap': mean_abs_shap\n",
    "}).sort_values('mean_abs_shap', ascending=False)\n",
    "print(f\"\\n   {'Feature':<20} {'Mean |SHAP|':<15} {'Interpretation'}\")\n",
    "print(f\"   {'-'*70}\")\n",
    "for _, row in importance_df_shap.iterrows():\n",
    "    print(f\"   {row['feature']:<20} {row['mean_abs_shap']:<15.4f} Global impact on defect prediction\")\n",
    "# ============================================================================\n",
    "# Business Insights\n",
    "# ============================================================================\n",
    "print(f\"\\n\ud83c\udfaf Key Insights:\")\n",
    "print(f\"   1. Top 3 defect predictors: {', '.join(importance_df_shap['feature'].head(3).tolist())}\")\n",
    "print(f\"   2. Low Vdd_min \u2192 High SHAP for defect (power grid issue)\")\n",
    "print(f\"   3. High leakage/Idd_standby \u2192 Defect indicator (process variation)\")\n",
    "print(f\"   4. Dependence plot shows Vdd_min \u00d7 leakage interaction (compound effect)\")\n",
    "print(f\"\\n\ud83d\udcb0 Business Value:\")\n",
    "print(f\"   \u2022 Local explanations \u2192 Immediate root cause (Vdd_min out of spec)\")\n",
    "print(f\"   \u2022 Global patterns \u2192 Focus process improvements on top 3 parameters\")\n",
    "print(f\"   \u2022 Trust \u2192 Engineers understand why model flagged device\")\n",
    "print(f\"   \u2022 Prevent false escapes \u2192 Catch 95%+ defects with explainable decisions\")\n",
    "print(f\"   \u2022 Cost savings: $100K-$500K per prevented defect escape\")\n",
    "print(f\"\\n\u2705 Production SHAP analysis complete!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21516e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udd2c Part 3: LIME (Local Interpretable Model-agnostic Explanations)\n",
    "\n",
    "### \ud83d\udd0d Theory: LIME Algorithm\n",
    "\n",
    "LIME complements SHAP by providing **fast local approximations** for any model.\n",
    "\n",
    "#### **Core Idea: Local Linear Approximation**\n",
    "\n",
    "**Observation:** Complex models (deep learning, ensembles) are **locally linear**\n",
    "- Globally non-linear: Decision boundary is complex\n",
    "- Locally linear: Near any point $x$, model behaves like linear function\n",
    "\n",
    "**LIME approach:**\n",
    "1. Generate perturbed samples near $x$\n",
    "2. Get complex model predictions for perturbed samples\n",
    "3. Fit simple linear model weighted by proximity to $x$\n",
    "4. Linear coefficients = explanations\n",
    "\n",
    "---\n",
    "\n",
    "#### **Mathematical Formulation**\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "$$\\xi(x) = \\arg\\min_{g \\in G} \\mathcal{L}(f, g, \\pi_x) + \\Omega(g)$$\n",
    "\n",
    "Where:\n",
    "- $f$ = complex model (black box)\n",
    "- $g$ = simple interpretable model (linear regression)\n",
    "- $G$ = class of interpretable models\n",
    "- $\\mathcal{L}(f, g, \\pi_x)$ = locality-weighted fidelity loss\n",
    "- $\\pi_x(z)$ = proximity kernel (weight nearby points)\n",
    "- $\\Omega(g)$ = complexity penalty (prefer fewer features)\n",
    "\n",
    "**Fidelity loss** (how well $g$ approximates $f$ near $x$):\n",
    "\n",
    "$$\\mathcal{L}(f, g, \\pi_x) = \\sum_{z \\in \\mathcal{Z}} \\pi_x(z) \\left[ f(z) - g(z) \\right]^2$$\n",
    "\n",
    "**Proximity kernel** (exponential decay with distance):\n",
    "\n",
    "$$\\pi_x(z) = \\exp\\left( -\\frac{d(x, z)^2}{2\\sigma^2} \\right)$$\n",
    "\n",
    "Where $d(x, z)$ = distance metric (Euclidean, cosine, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "#### **LIME Algorithm (Step-by-Step)**\n",
    "\n",
    "**Input:** Instance $x$, model $f$, number of samples $N$\n",
    "\n",
    "1. **Perturb:** Generate $N$ samples near $x$\n",
    "   - For tabular data: $z_i \\sim \\mathcal{N}(x, \\sigma^2 I)$ or randomly turn features on/off\n",
    "   - For images: Randomly mask superpixels\n",
    "   - For text: Randomly remove words\n",
    "\n",
    "2. **Predict:** Compute $f(z_i)$ for all perturbed samples\n",
    "\n",
    "3. **Weight:** Compute proximity weights $w_i = \\pi_x(z_i)$\n",
    "\n",
    "4. **Fit:** Train linear model with weighted least squares\n",
    "   $$\\min_{\\beta} \\sum_{i=1}^{N} w_i \\left[ f(z_i) - (\\beta_0 + \\beta^T z_i) \\right]^2 + \\lambda \\|\\beta\\|_1$$\n",
    "   \n",
    "   - $\\lambda \\|\\beta\\|_1$ = L1 penalty for feature selection (optional)\n",
    "\n",
    "5. **Explain:** Return coefficients $\\beta$ as feature importances\n",
    "   - Positive $\\beta_j$ \u2192 Feature increases prediction\n",
    "   - Negative $\\beta_j$ \u2192 Feature decreases prediction\n",
    "\n",
    "---\n",
    "\n",
    "#### **LIME vs SHAP Comparison**\n",
    "\n",
    "| **Aspect** | **SHAP** | **LIME** |\n",
    "|------------|----------|----------|\n",
    "| **Foundation** | Game theory (Shapley values) | Local linear approximation |\n",
    "| **Properties** | Satisfies 4 axioms (efficiency, symmetry, etc.) | No formal guarantees |\n",
    "| **Consistency** | \u2705 Consistent across predictions | \u26a0\ufe0f Can vary (random sampling) |\n",
    "| **Speed** | \u26a1 Fast for trees (TreeSHAP), slow otherwise | \u26a1 Fast (few samples) |\n",
    "| **Interpretability** | Feature contributions sum to prediction | Linear coefficients |\n",
    "| **Global Use** | \u2705 Aggregate for global importance | \u274c Local only |\n",
    "| **Model Scope** | Any model (optimized for trees) | Any model |\n",
    "\n",
    "**When to use SHAP:**\n",
    "- Tree-based models (XGBoost, LightGBM, RF)\n",
    "- Need rigorous theoretical guarantees\n",
    "- Want global + local explanations\n",
    "- Willing to wait for exact attributions\n",
    "\n",
    "**When to use LIME:**\n",
    "- Need fast explanations (real-time systems)\n",
    "- Non-tree models (neural nets, SVMs, ensembles)\n",
    "- Quick prototyping / debugging\n",
    "- Okay with approximate explanations\n",
    "\n",
    "**Best practice:** Use both! SHAP for trees, LIME for others. Cross-validate explanations.\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83c\udfed Semiconductor Example: LIME for Yield Prediction\n",
    "\n",
    "**Scenario:** Random Forest predicts yield \u2192 Use LIME to explain individual predictions\n",
    "\n",
    "**Why LIME here:**\n",
    "- Fast: Real-time explanations for production line ($100ms$ latency requirement)\n",
    "- Complementary: Validate SHAP explanations (should agree on important features)\n",
    "- Debugging: If LIME disagrees with SHAP \u2192 investigate model behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e75b03",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Use LIME library for fast, model-agnostic local explanations on yield prediction\n",
    "\n",
    "**Key Points:**\n",
    "- **LimeTabularExplainer**: Specialized for tabular data (vs LimeImageExplainer, LimeTextExplainer)\n",
    "- **Perturbation strategy**: Sample from training data distribution (preserves feature correlations)\n",
    "- **num_samples**: Number of perturbed instances (5000 = stable, 500 = fast but noisy)\n",
    "- **Linear model**: Weighted ridge regression fits local approximation\n",
    "- **Visualization**: Bar chart shows top K feature contributions (positive/negative)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Model-agnostic: Works for Random Forest, XGBoost, Neural Nets, SVMs, ensembles\n",
    "- Fast: Explains prediction in milliseconds (real-time production systems)\n",
    "- Interpretable: Engineers understand linear coefficients intuitively\n",
    "- Debugging: Compare with SHAP to validate explanations (should mostly agree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61bf380",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebee936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install lime if not already installed (uncomment if needed)\n",
    "# !pip install lime\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "print(\"=\"*70)\n",
    "print(\"LIME: LOCAL INTERPRETABLE MODEL-AGNOSTIC EXPLANATIONS\")\n",
    "print(\"=\"*70)\n",
    "# ============================================================================\n",
    "# Use earlier yield prediction data and model\n",
    "# ============================================================================\n",
    "print(\"\\n[1] Using earlier Random Forest yield prediction model...\")\n",
    "print(f\"   Model: {model}\")\n",
    "print(f\"   Test R\u00b2: {baseline_r2:.4f}\")\n",
    "print(f\"   Features: {list(X.columns)}\")\n",
    "# ============================================================================\n",
    "# Initialize LIME Explainer\n",
    "# ============================================================================\n",
    "print(f\"\\n[2] Initializing LIME explainer...\")\n",
    "explainer_lime = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_train.values,\n",
    "    feature_names=X_train.columns.tolist(),\n",
    "    mode='regression',\n",
    "    random_state=42\n",
    ")\n",
    "print(f\"   Explainer initialized for tabular regression\")\n",
    "print(f\"   Training data shape: {X_train.shape}\")\n",
    "# ============================================================================\n",
    "# Explain Single Prediction\n",
    "# ============================================================================\n",
    "print(f\"\\n[3] Explaining prediction for device #{instance_idx}...\")\n",
    "# Select instance to explain (reuse from earlier)\n",
    "x_explain_lime = X_test.iloc[instance_idx].values\n",
    "y_true_lime = y_test.iloc[instance_idx]\n",
    "y_pred_lime = model.predict(x_explain_lime.reshape(1, -1))[0]\n",
    "print(f\"   True yield: {y_true_lime:.2f}%\")\n",
    "print(f\"   Predicted yield: {y_pred_lime:.2f}%\")\n",
    "# Generate LIME explanation\n",
    "print(f\"\\n   Generating LIME explanation (sampling 5000 neighbors)...\")\n",
    "explanation = explainer_lime.explain_instance(\n",
    "    data_row=x_explain_lime,\n",
    "    predict_fn=model.predict,\n",
    "    num_features=len(X.columns),\n",
    "    num_samples=5000  # More samples = more stable (but slower)\n",
    ")\n",
    "# Extract feature contributions\n",
    "lime_values = dict(explanation.as_list())\n",
    "print(f\"\\n   LIME Explanation (Linear Approximation):\")\n",
    "print(f\"   {'Feature':<20} {'Contribution':<15} {'Direction'}\")\n",
    "print(f\"   {'-'*55}\")\n",
    "# Sort by absolute contribution\n",
    "sorted_lime = sorted(lime_values.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "for feat_range, contrib in sorted_lime:\n",
    "    # LIME returns feature ranges like \"Vdd_min <= 1.62\", extract feature name\n",
    "    feat_name = feat_range.split('<=')[0].split('>')[0].strip()\n",
    "    direction = \"\u2191\" if contrib > 0 else \"\u2193\"\n",
    "    impact = \"Increases\" if contrib > 0 else \"Decreases\"\n",
    "    print(f\"   {feat_name:<20} {contrib:>+8.4f}%       {direction} {impact} prediction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937455f3",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Compare LIME vs SHAP (if SHAP values available)\n",
    "# ============================================================================\n",
    "print(f\"\\n[4] Comparing LIME with earlier SHAP results...\")\n",
    "# Get SHAP values for same instance (from earlier computation)\n",
    "if 'shap_vals' in locals():\n",
    "    comparison_data = []\n",
    "    for feat in X.columns:\n",
    "        lime_val = next((v for k, v in sorted_lime if feat in k), 0)\n",
    "        shap_val = shap_vals.get(feat, 0) if isinstance(shap_vals, dict) else 0\n",
    "        comparison_data.append({\n",
    "            'feature': feat,\n",
    "            'lime': lime_val,\n",
    "            'shap': shap_val\n",
    "        })\n",
    "    \n",
    "    comparison_df_explain = pd.DataFrame(comparison_data)\n",
    "    comparison_df_explain['agreement'] = np.sign(comparison_df_explain['lime']) == np.sign(comparison_df_explain['shap'])\n",
    "    \n",
    "    print(f\"\\n   {'Feature':<20} {'LIME':<12} {'SHAP':<12} {'Agreement'}\")\n",
    "    print(f\"   {'-'*60}\")\n",
    "    for _, row in comparison_df_explain.iterrows():\n",
    "        agree = \"\u2705\" if row['agreement'] else \"\u274c\"\n",
    "        print(f\"   {row['feature']:<20} {row['lime']:>+8.4f}%   {row['shap']:>+8.4f}%   {agree}\")\n",
    "    \n",
    "    agreement_rate = comparison_df_explain['agreement'].mean()\n",
    "    print(f\"\\n   Agreement rate: {agreement_rate:.1%}\")\n",
    "# ============================================================================\n",
    "# Visualization\n",
    "# ============================================================================\n",
    "print(f\"\\n[5] Creating LIME visualizations...\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "# Plot 1: LIME feature contributions\n",
    "ax1 = axes[0]\n",
    "features_lime = [k.split('<=')[0].split('>')[0].strip() for k, v in sorted_lime]\n",
    "values_lime = [v for k, v in sorted_lime]\n",
    "colors_lime = ['green' if v > 0 else 'red' for v in values_lime]\n",
    "y_pos_lime = np.arange(len(features_lime))\n",
    "ax1.barh(y_pos_lime, values_lime, color=colors_lime, alpha=0.8)\n",
    "ax1.set_yticks(y_pos_lime)\n",
    "ax1.set_yticklabels(features_lime, fontsize=10)\n",
    "ax1.set_xlabel('LIME Contribution (% Yield Impact)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title(f'LIME Explanation: Device #{instance_idx}\\n' +\n",
    "              f'Local Linear Approximation (5000 samples)',\n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.axvline(x=0, color='black', linestyle='-', linewidth=1.5, alpha=0.5)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "# Add value labels\n",
    "for i, val in enumerate(values_lime):\n",
    "    label_x = val + (0.3 if val > 0 else -0.3)\n",
    "    ha = 'left' if val > 0 else 'right'\n",
    "    ax1.text(label_x, i, f'{val:+.2f}%', va='center', ha=ha, fontsize=9, fontweight='bold')\n",
    "# Plot 2: LIME vs SHAP comparison (if available)\n",
    "ax2 = axes[1]\n",
    "if 'comparison_df_explain' in locals():\n",
    "    x_pos_compare = np.arange(len(comparison_df_explain))\n",
    "    width_compare = 0.35\n",
    "    \n",
    "    ax2.barh(x_pos_compare - width_compare/2, comparison_df_explain['lime'], width_compare,\n",
    "            label='LIME', color='steelblue', alpha=0.8)\n",
    "    ax2.barh(x_pos_compare + width_compare/2, comparison_df_explain['shap'], width_compare,\n",
    "            label='SHAP', color='coral', alpha=0.8)\n",
    "    \n",
    "    ax2.set_yticks(x_pos_compare)\n",
    "    ax2.set_yticklabels(comparison_df_explain['feature'], fontsize=10)\n",
    "    ax2.set_xlabel('Contribution (% Yield Impact)', fontsize=11, fontweight='bold')\n",
    "    ax2.set_title(f'LIME vs SHAP Comparison\\n(Agreement: {agreement_rate:.0%})',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.axvline(x=0, color='black', linestyle='-', linewidth=1.5, alpha=0.5)\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "else:\n",
    "    # Fallback: Show model prediction vs true\n",
    "    ax2.text(0.5, 0.5, 'SHAP values not available\\nfor comparison', \n",
    "            ha='center', va='center', fontsize=12, transform=ax2.transAxes)\n",
    "    ax2.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a246355e",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc90b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Model Fidelity Check\n",
    "# ============================================================================\n",
    "print(f\"\\n[6] Checking LIME local approximation fidelity...\")\n",
    "# Get R\u00b2 of local linear model\n",
    "local_r2 = explanation.score\n",
    "intercept = explanation.intercept[0]\n",
    "local_pred = explanation.local_pred[0]\n",
    "print(f\"   Local R\u00b2 (how well linear model fits): {local_r2:.4f}\")\n",
    "print(f\"   Intercept (baseline): {intercept:.2f}%\")\n",
    "print(f\"   Local prediction: {local_pred:.2f}%\")\n",
    "print(f\"   Actual model prediction: {y_pred_lime:.2f}%\")\n",
    "print(f\"   Difference: {abs(local_pred - y_pred_lime):.4f}%\")\n",
    "if local_r2 > 0.8:\n",
    "    print(f\"\\n   \u2705 High fidelity: Linear approximation is trustworthy\")\n",
    "elif local_r2 > 0.5:\n",
    "    print(f\"\\n   \u26a0\ufe0f Moderate fidelity: Linear approximation captures main effects\")\n",
    "else:\n",
    "    print(f\"\\n   \u274c Low fidelity: Model is highly non-linear locally (LIME may be misleading)\")\n",
    "# ============================================================================\n",
    "# Business Insights\n",
    "# ============================================================================\n",
    "print(f\"\\n\ud83c\udfaf Key Insights:\")\n",
    "print(f\"   1. LIME provides fast local explanations (~100ms for 5000 samples)\")\n",
    "print(f\"   2. Top contributors: {', '.join([f.split('<=')[0].split('>')[0].strip() for f, _ in sorted_lime[:3]])}\")\n",
    "print(f\"   3. Local R\u00b2 = {local_r2:.3f} \u2192 {'Trustworthy' if local_r2 > 0.7 else 'Review carefully'}\")\n",
    "if 'agreement_rate' in locals():\n",
    "    print(f\"   4. LIME-SHAP agreement: {agreement_rate:.0%} \u2192 {'Strong validation' if agreement_rate > 0.7 else 'Investigate discrepancies'}\")\n",
    "print(f\"\\n\ud83d\udcb0 Business Value:\")\n",
    "print(f\"   \u2022 Real-time explanations: <100ms latency (production-ready)\")\n",
    "print(f\"   \u2022 Model-agnostic: Works with any model (RF, XGBoost, Neural Nets)\")\n",
    "print(f\"   \u2022 Engineer-friendly: Linear coefficients are intuitive\")\n",
    "print(f\"   \u2022 Cross-validation: Use with SHAP to validate explanations\")\n",
    "print(f\"\\n\u2705 LIME explanation complete!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43758104",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udf93 Real-World Projects & Key Takeaways\n",
    "\n",
    "### \ud83d\ude80 8 Real-World Projects\n",
    "\n",
    "---\n",
    "\n",
    "#### **Post-Silicon Validation Projects (4)**\n",
    "\n",
    "---\n",
    "\n",
    "##### **Project 1: Automated Root Cause Analysis System**\n",
    "\n",
    "**Objective:** Build real-time explanation system for yield test failures\n",
    "\n",
    "**Features:**\n",
    "- SHAP explainer for XGBoost yield classifier (pass/fail)\n",
    "- Automatic root cause identification (top 3 contributing parameters)\n",
    "- Integration with test equipment data pipeline (STDF real-time)\n",
    "- Alert system when SHAP values indicate out-of-spec parameters\n",
    "\n",
    "**Why Explainability:**\n",
    "- Without: \"Model says fail\" \u2192 Engineer spends hours debugging\n",
    "- With: \"Vdd_min -0.05V (2\u03c3 low) caused 85% of failure\" \u2192 Immediate action\n",
    "\n",
    "**Recommended Approach:**\n",
    "1. Train XGBoost on historical test data (1M devices)\n",
    "2. Use TreeSHAP for exact, fast explanations (<10ms)\n",
    "3. Set thresholds: If |SHAP| > 0.1 for parameter \u2192 flag as root cause\n",
    "4. Dashboard: Real-time SHAP waterfall plots for failed devices\n",
    "5. Validation: Compare SHAP-identified causes vs engineer judgment (>90% agreement)\n",
    "\n",
    "**Success Metrics:**\n",
    "- Debug time reduction: 4 hours \u2192 30 minutes (87% faster)\n",
    "- Root cause accuracy: >90% vs manual analysis\n",
    "- Mean Time To Resolution (MTTR): <2 hours (vs 1-2 days)\n",
    "\n",
    "**Business Value:** $5M-$15M annual savings from faster debug + yield recovery\n",
    "\n",
    "---\n",
    "\n",
    "##### **Project 2: Multi-Site Process Harmonization Tool**\n",
    "\n",
    "**Objective:** Explain why yield differs across manufacturing sites\n",
    "\n",
    "**Features:**\n",
    "- SHAP dependence plots showing site-specific parameter effects\n",
    "- Interaction detection: Site \u00d7 Temperature \u00d7 Vdd patterns\n",
    "- Partial dependence plots comparing sites A, B, C\n",
    "- Recommendations for standardizing processes\n",
    "\n",
    "**Why Explainability:**\n",
    "- Site A yield = 92%, Site B yield = 85%\n",
    "- SHAP reveals: Site B has higher sensitivity to temperature (interaction effect)\n",
    "- Actionable: Improve Site B climate control \u2192 5-7% yield gain\n",
    "\n",
    "**Recommended Approach:**\n",
    "1. Train single model on all sites (add site_id as categorical feature)\n",
    "2. Compute SHAP interaction values for site \u00d7 process parameters\n",
    "3. Generate site-specific dependence plots (PDP with site conditioning)\n",
    "4. Identify divergent patterns (e.g., Site B Vdd sensitivity 2\u00d7 Site A)\n",
    "5. Root cause: Equipment calibration, fab climate, process recipe\n",
    "\n",
    "**Success Metrics:**\n",
    "- Yield gap closure: 7% difference \u2192 <2% difference (cross-site)\n",
    "- Time to identify cause: 6 months \u2192 2 weeks (180\u00d7 faster)\n",
    "- Process standardization: 15 parameters harmonized across 3 sites\n",
    "\n",
    "**Business Value:** $10M-$30M from yield improvements across multiple fabs\n",
    "\n",
    "---\n",
    "\n",
    "##### **Project 3: Test Optimization via Feature Importance**\n",
    "\n",
    "**Objective:** Reduce test time by 25% without sacrificing accuracy\n",
    "\n",
    "**Features:**\n",
    "- Permutation importance for all 200+ parametric tests\n",
    "- Cost-benefit analysis: Test time vs predictive value\n",
    "- Sequential test removal with performance monitoring\n",
    "- SHAP validation: Removed tests have near-zero SHAP values\n",
    "\n",
    "**Why Explainability:**\n",
    "- 200 tests take 45 seconds/device \u2192 $0.45 cost/device @ $0.01/sec\n",
    "- Permutation importance: 50 tests have zero predictive value\n",
    "- Remove 50 tests \u2192 30 sec/device \u2192 $0.30 cost \u2192 33% savings\n",
    "\n",
    "**Recommended Approach:**\n",
    "1. Compute permutation importance for all tests (stratified by bin type)\n",
    "2. Rank by importance_mean / test_time_sec (value per second)\n",
    "3. Iteratively remove bottom 10% tests, retrain model, check performance\n",
    "4. Stop when F1-score drops >1% or ROC-AUC drops >0.02\n",
    "5. Validate with SHAP: Removed tests should have low mean |SHAP|\n",
    "\n",
    "**Success Metrics:**\n",
    "- Test time reduction: 45 sec \u2192 30 sec (33% faster)\n",
    "- Throughput increase: 1.5\u00d7 more devices per hour\n",
    "- Model performance maintained: \u0394F1 < 0.5%, \u0394AUC < 0.01\n",
    "\n",
    "**Business Value:** $2M-$5M annual savings (1M devices/month \u00d7 $0.15/device savings)\n",
    "\n",
    "---\n",
    "\n",
    "##### **Project 4: Explainable Adaptive Binning System**\n",
    "\n",
    "**Objective:** Build multi-class speed binning with per-device explanations\n",
    "\n",
    "**Features:**\n",
    "- Multi-class SHAP for 5 speed bins (2.0 GHz, 2.2 GHz, 2.4 GHz, 2.6 GHz, 2.8+ GHz)\n",
    "- Per-device explanation: \"Assigned to 2.4 GHz bin because freq_max=2.38, Vdd_typical=1.82\"\n",
    "- Margin analysis: Devices near bin boundaries get extra scrutiny\n",
    "- Feedback loop: Engineer overrides used to retrain model\n",
    "\n",
    "**Why Explainability:**\n",
    "- Premium bin (2.8+ GHz) has $50 higher selling price\n",
    "- Mis-binning high-performing device \u2192 $50 revenue loss\n",
    "- SHAP shows: freq_max + Vdd_typical + temp_coef determine binning (95% weight)\n",
    "\n",
    "**Recommended Approach:**\n",
    "1. Train XGBoost multi-class classifier (5 bins)\n",
    "2. Use TreeSHAP for exact explanations (one SHAP vector per class)\n",
    "3. Flag devices with confidence <80% for manual review\n",
    "4. Show engineer: \"Bin A probability 75% vs Bin B probability 25%, main differentiator: freq_max\"\n",
    "5. Track override rate: Should be <5% if model is accurate\n",
    "\n",
    "**Success Metrics:**\n",
    "- Binning accuracy: >98% agreement with manual golden reference\n",
    "- Override rate: <2% (high confidence)\n",
    "- Revenue optimization: $500K/year from better premium bin utilization\n",
    "\n",
    "**Business Value:** $3M-$8M from optimized binning + avoided mis-classifications\n",
    "\n",
    "---\n",
    "\n",
    "#### **General AI/ML Projects (4)**\n",
    "\n",
    "---\n",
    "\n",
    "##### **Project 5: Fair Credit Scoring with SHAP Explanations**\n",
    "\n",
    "**Objective:** Build legally compliant credit scoring model with explanations\n",
    "\n",
    "**Features:**\n",
    "- Gradient Boosting for credit risk (default probability)\n",
    "- SHAP explanations for every loan decision (regulatory requirement)\n",
    "- Fairness audit: Group SHAP by protected attributes (race, gender)\n",
    "- Adverse action notices: \"Loan denied due to: DTI ratio 48% (threshold 40%)\"\n",
    "\n",
    "**Why Explainability:**\n",
    "- Regulatory: Fair Credit Reporting Act requires explanations\n",
    "- Customer trust: \"Why was I denied?\" \u2192 Transparent reasoning\n",
    "- Bias detection: SHAP reveals if race/gender indirectly influence via proxy features\n",
    "\n",
    "**Recommended Approach:**\n",
    "1. Train Gradient Boosting on 500K loan applications\n",
    "2. Generate SHAP values for all predictions (TreeSHAP: <1 sec/prediction)\n",
    "3. Top 3 SHAP features \u2192 Adverse action reasons (DTI, credit score, delinquencies)\n",
    "4. Fairness check: Mean SHAP for protected groups should be similar (conditional on credit factors)\n",
    "5. Provide appeal process: Customer disputes \u2192 SHAP review \u2192 Retrain if systematic bias found\n",
    "\n",
    "**Success Metrics:**\n",
    "- Regulatory compliance: 100% of denials have explanations\n",
    "- Customer satisfaction: 40% improvement in appeal resolution\n",
    "- Bias metrics: Equalized odds difference <2% across groups\n",
    "\n",
    "**Business Value:** $20M-$50M from avoided discrimination lawsuits + customer trust\n",
    "\n",
    "---\n",
    "\n",
    "##### **Project 6: Medical Diagnosis Explainability for Doctor Trust**\n",
    "\n",
    "**Objective:** Assist radiologists with AI diagnosis + explanations\n",
    "\n",
    "**Features:**\n",
    "- Deep learning model for lung cancer detection (chest X-rays)\n",
    "- LIME explanations highlighting image regions contributing to diagnosis\n",
    "- Confidence calibration: SHAP values correlate with diagnostic certainty\n",
    "- Comparison view: Doctor's assessment vs AI assessment + SHAP\n",
    "\n",
    "**Why Explainability:**\n",
    "- Doctors won't trust \"black box\" diagnosis\n",
    "- LIME shows: \"Model focused on 3cm nodule in upper-left lobe (matches my assessment)\"\n",
    "- Actionable: If LIME highlights wrong region \u2192 Model retrained or prediction rejected\n",
    "\n",
    "**Recommended Approach:**\n",
    "1. Train CNN on 100K labeled X-rays (ImageNet transfer learning)\n",
    "2. Use LIME with superpixel segmentation (100-200 superpixels)\n",
    "3. Highlight top 10 superpixels pushing toward \"cancer\" diagnosis\n",
    "4. Overlay heatmap on X-ray image (red=high contribution)\n",
    "5. Doctor reviews: If heatmap matches clinical judgment \u2192 Accept AI suggestion\n",
    "\n",
    "**Success Metrics:**\n",
    "- Doctor adoption: 85% of radiologists use AI assistant\n",
    "- Diagnostic accuracy: 94% (vs 91% doctor alone, 96% AI alone)\n",
    "- Time savings: 30% faster diagnosis workflow\n",
    "\n",
    "**Business Value:** $100M-$200M from earlier detection + reduced false negatives\n",
    "\n",
    "---\n",
    "\n",
    "##### **Project 7: Fraud Detection Explainability for Customer Service**\n",
    "\n",
    "**Objective:** Real-time fraud detection with explanations for customer disputes\n",
    "\n",
    "**Features:**\n",
    "- XGBoost for transaction fraud (0.1% fraud rate, highly imbalanced)\n",
    "- SHAP explanations for flagged transactions (<50ms latency)\n",
    "- Customer-facing explanations: \"Transaction flagged due to: unusual location + high amount\"\n",
    "- Agent dashboard: SHAP waterfall plot for investigating disputes\n",
    "\n",
    "**Why Explainability:**\n",
    "- False positive cost: Legitimate transaction blocked \u2192 Angry customer\n",
    "- SHAP helps agent: \"Was device in new city?\" \u2192 Customer confirms \u2192 Unblock\n",
    "- Model improvement: High false positive features get reviewed \u2192 Retrained\n",
    "\n",
    "**Recommended Approach:**\n",
    "1. Train XGBoost with scale_pos_weight for 0.1% fraud rate\n",
    "2. Use TreeSHAP for real-time explanations (batch compute at prediction time)\n",
    "3. Top 3 SHAP features \u2192 Customer-facing message (simplified language)\n",
    "4. Agent sees full SHAP waterfall + feature values\n",
    "5. Feedback loop: Agent overrides \u2192 Retrain weekly with new labels\n",
    "\n",
    "**Success Metrics:**\n",
    "- Fraud detection rate: 92% recall @ 0.5% FPR\n",
    "- False positive handling time: 5 min \u2192 2 min (60% faster with SHAP)\n",
    "- Customer satisfaction: 25% improvement for flagged transactions\n",
    "\n",
    "**Business Value:** $50M-$150M from fraud prevention + reduced false positive costs\n",
    "\n",
    "---\n",
    "\n",
    "##### **Project 8: Recommendation System Transparency**\n",
    "\n",
    "**Objective:** Explain why product was recommended (user trust + engagement)\n",
    "\n",
    "**Features:**\n",
    "- Matrix factorization or deep learning for recommendations\n",
    "- SHAP for collaborative filtering features (user/item embeddings)\n",
    "- Explanation text: \"Recommended because you liked Product A, B, C\"\n",
    "- A/B test: Recommendations with explanations vs without\n",
    "\n",
    "**Why Explainability:**\n",
    "- User trust: \"Why this product?\" \u2192 Transparent reasoning\n",
    "- Serendipity vs relevance: SHAP shows if recommendation is \"more of the same\" vs novel\n",
    "- Business insight: Aggregate SHAP \u2192 Which features drive conversions?\n",
    "\n",
    "**Recommended Approach:**\n",
    "1. Train deep collaborative filtering (user/item embeddings + content features)\n",
    "2. Use KernelSHAP or LayerShap for model-agnostic explanations (complex architecture)\n",
    "3. Extract top 3 positive SHAP features \u2192 Map to human-readable reasons\n",
    "4. A/B test: 50% users see explanations, 50% control\n",
    "5. Measure: Click-through rate, conversion rate, user satisfaction surveys\n",
    "\n",
    "**Success Metrics:**\n",
    "- Click-through rate: +15% with explanations\n",
    "- Conversion rate: +8% with explanations\n",
    "- User satisfaction: \"Recommendations feel more relevant\" (35% improvement)\n",
    "\n",
    "**Business Value:** $30M-$80M from increased engagement + conversions\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83c\udfaf Core Principles\n",
    "\n",
    "#### **1. Choose the Right Explainability Method**\n",
    "\n",
    "| **Scenario** | **Recommended Method** | **Why** |\n",
    "|--------------|------------------------|---------|\n",
    "| Tree-based model (XGBoost, RF, LightGBM) | **SHAP (TreeSHAP)** | Fast, exact, theoretically grounded |\n",
    "| Neural network, SVM, ensemble | **LIME or KernelSHAP** | Model-agnostic, fast approximations |\n",
    "| Need global importance | **Permutation Importance + SHAP Summary** | Unbiased, aggregated view |\n",
    "| Need local explanation | **SHAP Force/Waterfall Plot + LIME** | Per-prediction transparency |\n",
    "| Real-time system (<50ms latency) | **LIME (500 samples)** | Fastest, acceptable fidelity |\n",
    "| Regulatory compliance | **SHAP** | Formal guarantees (efficiency, consistency) |\n",
    "| Debug model behavior | **SHAP Dependence Plots + PDP** | Reveals interactions, non-linearities |\n",
    "\n",
    "#### **2. Validate Explanations**\n",
    "\n",
    "- **Cross-validate methods:** SHAP and LIME should mostly agree (>70% sign agreement)\n",
    "- **Domain expert review:** Do top features make sense? (Vdd, Idd in semiconductor)\n",
    "- **Fidelity check:** LIME local R\u00b2 > 0.7 \u2192 Trustworthy\n",
    "- **Efficiency check:** SHAP values sum to prediction - baseline (verify property)\n",
    "- **Sensitivity analysis:** Small feature changes \u2192 proportional SHAP changes\n",
    "\n",
    "#### **3. Communicate to Stakeholders**\n",
    "\n",
    "**For Engineers:**\n",
    "- Show full SHAP waterfall plot with feature values\n",
    "- Provide dependence plots for top features (understand non-linearity)\n",
    "- Highlight interaction effects (temp \u00d7 voltage)\n",
    "\n",
    "**For Managers:**\n",
    "- Summary: \"Top 3 parameters cause 85% of yield variation\"\n",
    "- Business impact: \"$10M savings from focusing on Vdd stabilization\"\n",
    "- Dashboard: Global feature importance bar chart\n",
    "\n",
    "**For Customers/Regulators:**\n",
    "- Simplified language: \"Denied due to debt-to-income ratio exceeding 40%\"\n",
    "- Visual: Single bar chart showing top 3 reasons\n",
    "- Actionable: \"To improve approval odds, reduce DTI to below 35%\"\n",
    "\n",
    "#### **4. Production Deployment Patterns**\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Model Prediction] --> B{Explainability Needed?}\n",
    "    B -->|Yes| C[Compute SHAP/LIME]\n",
    "    B -->|No| D[Return Prediction]\n",
    "    \n",
    "    C --> E[Cache Explanations]\n",
    "    E --> F{Audience?}\n",
    "    \n",
    "    F -->|Engineer| G[Full Waterfall Plot]\n",
    "    F -->|Manager| H[Top 3 Features]\n",
    "    F -->|Customer| I[Simplified Text]\n",
    "    \n",
    "    G --> J[Store in Database]\n",
    "    H --> J\n",
    "    I --> J\n",
    "    \n",
    "    J --> K[Monitoring Dashboard]\n",
    "    K --> L[Feedback Loop]\n",
    "    L --> A\n",
    "```\n",
    "\n",
    "**Best practices:**\n",
    "- **Batch compute:** Pre-compute SHAP for all predictions nightly (for dashboards)\n",
    "- **Cache:** Store explanations in database (avoid recomputation)\n",
    "- **Async:** Compute explanations asynchronously if latency-critical\n",
    "- **Sampling:** For dashboards, sample 1K predictions (representative)\n",
    "- **Monitoring:** Track explanation drift (SHAP distributions changing \u2192 model drift)\n",
    "\n",
    "---\n",
    "\n",
    "### \u26a0\ufe0f 5 Common Pitfalls + Solutions\n",
    "\n",
    "#### **Pitfall 1: Trusting Explanations Blindly**\n",
    "\n",
    "**Problem:** SHAP/LIME can be misleading if model is wrong\n",
    "\n",
    "**Example:** Model overfits to test_timestamp \u2192 SHAP shows high importance \u2192 Not causal!\n",
    "\n",
    "**Solutions:**\n",
    "- \u2705 Validate with domain experts: Does high-importance feature make physical sense?\n",
    "- \u2705 Check for data leakage: Remove timestamp, IDs, post-decision features\n",
    "- \u2705 Cross-validate with multiple methods: SHAP + LIME + Permutation should agree\n",
    "- \u2705 Simulate interventions: Change feature \u2192 Does prediction change as expected?\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pitfall 2: Ignoring Correlated Features**\n",
    "\n",
    "**Problem:** Permutation importance misleading with correlated features (Vdd \u00d7 Idd)\n",
    "\n",
    "**Example:** Shuffle Vdd \u2192 Idd still predicts well \u2192 Vdd appears unimportant (wrong!)\n",
    "\n",
    "**Solutions:**\n",
    "- \u2705 Use SHAP instead: Coalitional averaging handles correlation correctly\n",
    "- \u2705 Group correlated features: Compute importance for feature groups (electrical params)\n",
    "- \u2705 Conditional permutation: Permute Vdd while preserving Vdd-Idd correlation\n",
    "- \u2705 Variance Inflation Factor (VIF): Remove features with VIF > 10\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pitfall 3: Low LIME Fidelity**\n",
    "\n",
    "**Problem:** LIME local R\u00b2 < 0.5 \u2192 Linear approximation is poor\n",
    "\n",
    "**Example:** Highly non-linear region (decision boundary) \u2192 LIME coefficients unreliable\n",
    "\n",
    "**Solutions:**\n",
    "- \u2705 Increase num_samples: 5000+ samples \u2192 more stable (but slower)\n",
    "- \u2705 Use SHAP instead: Handles non-linearity better (exact for trees)\n",
    "- \u2705 Check fidelity: If R\u00b2 < 0.7, flag explanation as \"low confidence\"\n",
    "- \u2705 Kernel width tuning: Adjust $\\sigma$ in proximity kernel (smaller = more local)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pitfall 4: Explaining the Wrong Model**\n",
    "\n",
    "**Problem:** Production model \u2260 explained model (versioning issue)\n",
    "\n",
    "**Example:** Explain Model v1, but production uses Model v2 \u2192 Explanations wrong!\n",
    "\n",
    "**Solutions:**\n",
    "- \u2705 MLflow model registry: Track model versions + explainers together\n",
    "- \u2705 Automated tests: Assert SHAP values sum to prediction (catch version mismatch)\n",
    "- \u2705 Explainer versioning: Store explainer with model artifact (pickle together)\n",
    "- \u2705 Monitoring: Alert if explanation drift > 20% (model likely changed)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pitfall 5: Over-Explaining**\n",
    "\n",
    "**Problem:** Too much information overwhelms stakeholders\n",
    "\n",
    "**Example:** Show 50-feature SHAP plot to manager \u2192 Confusion, not insight\n",
    "\n",
    "**Solutions:**\n",
    "- \u2705 Top K features: Show only top 3-5 features (80/20 rule)\n",
    "- \u2705 Audience-specific views: Engineer (full), Manager (summary), Customer (simplified)\n",
    "- \u2705 Progressive disclosure: Default = top 3, click \"Show more\" for full breakdown\n",
    "- \u2705 Aggregation: For dashboards, show global importance (not per-prediction)\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udcda Resources for Further Learning\n",
    "\n",
    "#### **Foundational Papers:**\n",
    "1. **SHAP:** Lundberg & Lee (2017), \"A Unified Approach to Interpreting Model Predictions\"\n",
    "2. **LIME:** Ribeiro et al. (2016), \"Why Should I Trust You?\"\n",
    "3. **TreeSHAP:** Lundberg et al. (2020), \"From Local Explanations to Global Understanding\"\n",
    "4. **Shapley Values:** Shapley (1953), \"A Value for N-person Games\" (original game theory)\n",
    "\n",
    "#### **Books:**\n",
    "- **Interpretable Machine Learning** by Christoph Molnar (free online)\n",
    "- **Explainable AI in Practice** by Michael Munn & David Pitman\n",
    "\n",
    "#### **Libraries:**\n",
    "- **SHAP:** `pip install shap` \u2192 https://github.com/slundberg/shap\n",
    "- **LIME:** `pip install lime` \u2192 https://github.com/marcotcr/lime\n",
    "- **Alibi:** `pip install alibi` \u2192 Comprehensive explainability library\n",
    "\n",
    "#### **Semiconductor-Specific:**\n",
    "- IEEE papers on ML for yield prediction (search: \"yield prediction machine learning SHAP\")\n",
    "- Conference papers: ASMC, ISQED, IEDM (ML in semiconductor testing)\n",
    "\n",
    "---\n",
    "\n",
    "### \u2705 Summary: When to Use Each Technique\n",
    "\n",
    "| **Use Case** | **Best Method** | **Key Advantage** |\n",
    "|--------------|-----------------|-------------------|\n",
    "| **Fast local explanation (any model)** | LIME | <100ms, model-agnostic |\n",
    "| **Exact local explanation (trees)** | SHAP (TreeSHAP) | Rigorous, fast for trees |\n",
    "| **Global feature importance** | Permutation + SHAP | Unbiased, comprehensive |\n",
    "| **Understanding feature effects** | Partial Dependence Plots | Marginal effects, non-linearity |\n",
    "| **Detecting interactions** | SHAP Interaction Values | Pairwise interaction strength |\n",
    "| **Regulatory compliance** | SHAP | Formal guarantees, auditable |\n",
    "| **Real-time production** | LIME (cached) or TreeSHAP | <50ms latency |\n",
    "| **Model debugging** | SHAP Dependence + PDP | Reveals unexpected patterns |\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\ude80 Next Steps\n",
    "\n",
    "1. **Implement in your project:** Start with feature importance, then SHAP, then LIME\n",
    "2. **Validate explanations:** Domain expert review + cross-method validation\n",
    "3. **Build dashboards:** Aggregate SHAP for global insights\n",
    "4. **Iterate:** Use explanations to improve model (remove artifacts, add features)\n",
    "5. **Monitor drift:** Track SHAP distributions over time (detect model degradation)\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Complete!** \u2705\n",
    "\n",
    "You now have a comprehensive toolkit for making ML models interpretable and explainable. Use these techniques to build trust, debug models, and satisfy regulatory requirements. \ud83d\udd0d\ud83c\udfaf"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}