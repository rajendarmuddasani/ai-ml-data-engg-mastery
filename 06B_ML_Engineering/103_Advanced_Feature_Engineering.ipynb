{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a89ba45e",
   "metadata": {},
   "source": [
    "# 103: Advanced Feature Engineering\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** mathematical foundations of feature interactions, polynomial expansion, and encoding strategies\n",
    "- **Implement** feature engineering techniques from scratch using NumPy and pandas\n",
    "- **Build** production-ready feature pipelines with sklearn transformers\n",
    "- **Apply** domain-specific feature engineering to semiconductor parametric test data\n",
    "- **Evaluate** feature importance and engineering impact on model performance\n",
    "\n",
    "## ðŸ“š What is Feature Engineering?\n",
    "\n",
    "Feature engineering is the process of creating new input variables (features) from existing raw data to improve machine learning model performance. It combines domain knowledge, mathematical transformations, and automated discovery methods to expose patterns that algorithms can exploit.\n",
    "\n",
    "Unlike automated feature learning in deep learning, traditional ML models (linear regression, tree-based methods, SVMs) benefit enormously from well-engineered features. A carefully crafted feature can capture complex relationships that would require thousands of model parameters to learn automatically.\n",
    "\n",
    "In post-silicon validation, feature engineering transforms raw parametric test measurements (voltage, current, frequency) into meaningful device characteristics (power consumption, thermal stress, efficiency metrics) that directly correlate with yield and reliability outcomes.\n",
    "\n",
    "**Why Feature Engineering?**\n",
    "- âœ… **Boost Model Performance**: Well-engineered features can improve RÂ² from 0.70 to 0.95+ with same algorithm\n",
    "- âœ… **Reduce Model Complexity**: Fewer, better features = faster training, simpler models, easier deployment\n",
    "- âœ… **Encode Domain Knowledge**: Capture physics-based relationships (power = voltage Ã— current) that data alone may not reveal\n",
    "- âœ… **Handle Non-Linearity**: Polynomial and interaction terms allow linear models to fit curved decision boundaries\n",
    "- âœ… **Manage Categoricals**: Advanced encoding (target, hash) handles high-cardinality variables efficiently\n",
    "\n",
    "## ðŸ­ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Use Case 1: Power Consumption Prediction**\n",
    "- **Input**: Vdd (supply voltage), Idd (supply current), frequency, temperature from parametric tests\n",
    "- **Engineered Features**: Power (VddÃ—Idd), power density, thermal coefficient, efficiency ratio\n",
    "- **Output**: Predicted device power consumption with Â±5% accuracy\n",
    "- **Value**: Early detection of high-power outliers before customer shipment saves $50K-$500K per RMA\n",
    "\n",
    "**Use Case 2: Wafer-Level Yield Modeling**\n",
    "- **Input**: Die coordinates (x, y), test parameters, lot ID from wafer test STDF\n",
    "- **Engineered Features**: Radial distance from center, edge proximity, spatial clusters, lot-level statistics\n",
    "- **Output**: Spatial yield map identifying systematic defect patterns\n",
    "- **Value**: Root cause localization (lithography, CMP, implant issues) reduces time-to-fix from weeks to days\n",
    "\n",
    "**Use Case 3: Test Time Optimization**\n",
    "- **Input**: 500+ parametric test measurements per device, test execution timestamps\n",
    "- **Engineered Features**: Test correlations, redundancy scores, temporal patterns, equipment drift indicators\n",
    "- **Output**: Pruned test suite maintaining 99.5% defect coverage at 40% reduced test time\n",
    "- **Value**: $2M-$5M annual savings per product line from reduced ATE (Automated Test Equipment) hours\n",
    "\n",
    "**Use Case 4: Bin Prediction (Pass/Fail Classification)**\n",
    "- **Input**: Raw parametric measurements, environmental conditions, device metadata\n",
    "- **Engineered Features**: Polynomial expansions capturing non-linear spec limits, interaction terms for correlated failures\n",
    "- **Output**: Probabilistic bin prediction (BIN1=premium, BIN2=standard, BIN7=fail)\n",
    "- **Value**: Adaptive test insertion based on early predictions reduces escapes 30-50%\n",
    "\n",
    "## ðŸ”„ Feature Engineering Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Raw STDF Data] --> B[Domain Analysis]\n",
    "    B --> C{Feature Type?}\n",
    "    \n",
    "    C -->|Numerical| D[Interactions & Polynomials]\n",
    "    C -->|Categorical| E[Encoding Strategy]\n",
    "    C -->|Spatial| F[Coordinate Transforms]\n",
    "    C -->|Temporal| G[Time-Based Features]\n",
    "    \n",
    "    D --> H[Feature Selection]\n",
    "    E --> H\n",
    "    F --> H\n",
    "    G --> H\n",
    "    \n",
    "    H --> I[Validate on Holdout]\n",
    "    I --> J{Performance OK?}\n",
    "    \n",
    "    J -->|No| K[Refine Features]\n",
    "    K --> C\n",
    "    J -->|Yes| L[Production Pipeline]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style L fill:#e1ffe1\n",
    "    style I fill:#fff4e1\n",
    "```\n",
    "\n",
    "## ðŸ“Š Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **010**: Linear Regression - Feature-target relationships\n",
    "- **016**: Decision Trees - Feature importance concepts\n",
    "- **022**: Random Forest - Ensemble feature selection\n",
    "- **041**: Model Evaluation - Validation strategies\n",
    "\n",
    "**This Notebook (103):**\n",
    "- Feature interaction terms (multiplicative combinations)\n",
    "- Polynomial feature expansion (degree 2, 3)\n",
    "- Target encoding for categoricals (smoothed mean encoding)\n",
    "- Feature hashing for high-cardinality variables\n",
    "- Automated feature engineering concepts\n",
    "\n",
    "**Next Steps:**\n",
    "- **104**: Model Interpretability - Understand which engineered features matter (SHAP, LIME)\n",
    "- **105**: AutoML & NAS - Automated feature discovery at scale\n",
    "- **108**: Feature Stores - Production feature management with Feast\n",
    "\n",
    "---\n",
    "\n",
    "Let's engineer features that unlock hidden patterns in semiconductor test data! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a64f66d",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ffe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Environment ready for advanced feature engineering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27da4fc",
   "metadata": {},
   "source": [
    "## 2. Generate Semiconductor Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic STDF-like parametric test data\n",
    "n_devices = 1000\n",
    "\n",
    "# Test parameters\n",
    "vdd = np.random.normal(1.2, 0.05, n_devices)  # Supply voltage\n",
    "idd = np.random.normal(50, 5, n_devices)      # Supply current (mA)\n",
    "freq = np.random.normal(2000, 100, n_devices) # Frequency (MHz)\n",
    "temp = np.random.normal(85, 10, n_devices)    # Temperature (Â°C)\n",
    "vth = np.random.normal(0.4, 0.02, n_devices)  # Threshold voltage\n",
    "\n",
    "# Wafer position\n",
    "die_x = np.random.randint(0, 20, n_devices)\n",
    "die_y = np.random.randint(0, 20, n_devices)\n",
    "\n",
    "# Lot IDs (categorical)\n",
    "lot_ids = np.random.choice(['LOT_A', 'LOT_B', 'LOT_C', 'LOT_D'], n_devices)\n",
    "\n",
    "# Target: yield (influenced by interactions)\n",
    "power = vdd * idd\n",
    "thermal_stress = temp * freq / 1000\n",
    "yield_score = 95 - 0.3 * power - 0.01 * thermal_stress + 2 * vth + np.random.normal(0, 2, n_devices)\n",
    "yield_score = np.clip(yield_score, 70, 100)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'vdd': vdd,\n",
    "    'idd': idd,\n",
    "    'freq': freq,\n",
    "    'temp': temp,\n",
    "    'vth': vth,\n",
    "    'die_x': die_x,\n",
    "    'die_y': die_y,\n",
    "    'lot_id': lot_ids,\n",
    "    'yield': yield_score\n",
    "})\n",
    "\n",
    "print(f\"Dataset: {df.shape[0]} devices, {df.shape[1]} columns\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad4d86d",
   "metadata": {},
   "source": [
    "## 3. Feature Interaction Terms (From Scratch)\n",
    "\n",
    "**Concept:** Multiply features to capture their combined effect.\n",
    "\n",
    "**Mathematics:**\n",
    "$$f_{interaction}(x_i, x_j) = x_i \\times x_j$$\n",
    "\n",
    "**Why It Works:** Many real-world phenomena are multiplicative (e.g., power = voltage Ã— current)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4faa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From-scratch implementation of feature interactions\n",
    "def create_interaction_features(df, feature_pairs):\n",
    "    \"\"\"\n",
    "    Create interaction features from specified pairs.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - feature_pairs: List of tuples (feature1, feature2)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with new interaction columns\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for feat1, feat2 in feature_pairs:\n",
    "        interaction_name = f\"{feat1}_x_{feat2}\"\n",
    "        df_copy[interaction_name] = df_copy[feat1] * df_copy[feat2]\n",
    "        print(f\"Created: {interaction_name}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Define important interactions for semiconductor devices\n",
    "interactions = [\n",
    "    ('vdd', 'idd'),      # Power consumption\n",
    "    ('temp', 'freq'),    # Thermal stress\n",
    "    ('vdd', 'vth'),      # Overdrive voltage\n",
    "    ('die_x', 'die_y'),  # Spatial correlation\n",
    "]\n",
    "\n",
    "df_interact = create_interaction_features(df, interactions)\n",
    "print(f\"\\nNew shape: {df_interact.shape}\")\n",
    "print(f\"New columns: {[col for col in df_interact.columns if '_x_' in col]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfac862",
   "metadata": {},
   "source": [
    "## 4. Polynomial Features (Production)\n",
    "\n",
    "**Concept:** Create polynomial combinations of features (xÂ², xÂ³, xâ‚Ã—xâ‚‚, etc.)\n",
    "\n",
    "**sklearn Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec09d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features for polynomial expansion\n",
    "num_features = ['vdd', 'idd', 'freq', 'temp', 'vth']\n",
    "X_num = df[num_features]\n",
    "\n",
    "# Create polynomial features (degree=2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_num)\n",
    "\n",
    "# Get feature names\n",
    "poly_feature_names = poly.get_feature_names_out(num_features)\n",
    "\n",
    "# Create DataFrame\n",
    "df_poly = pd.DataFrame(X_poly, columns=poly_feature_names)\n",
    "\n",
    "print(f\"Original features: {X_num.shape[1]}\")\n",
    "print(f\"Polynomial features: {df_poly.shape[1]}\")\n",
    "print(f\"\\nSample polynomial features:\")\n",
    "print(poly_feature_names[:10])\n",
    "\n",
    "# Visualize impact on model performance\n",
    "y = df['yield']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_num, y, test_size=0.2, random_state=42)\n",
    "X_train_poly, X_test_poly, _, _ = train_test_split(df_poly, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model with original features\n",
    "model_linear = Ridge(alpha=1.0)\n",
    "model_linear.fit(X_train, y_train)\n",
    "y_pred_linear = model_linear.predict(X_test)\n",
    "r2_linear = r2_score(y_test, y_pred_linear)\n",
    "\n",
    "# Model with polynomial features\n",
    "model_poly = Ridge(alpha=1.0)\n",
    "model_poly.fit(X_train_poly, y_train)\n",
    "y_pred_poly = model_poly.predict(X_test_poly)\n",
    "r2_poly = r2_score(y_test, y_pred_poly)\n",
    "\n",
    "print(f\"\\nRÂ² Score - Linear features: {r2_linear:.4f}\")\n",
    "print(f\"RÂ² Score - Polynomial features: {r2_poly:.4f}\")\n",
    "print(f\"Improvement: {(r2_poly - r2_linear) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee97768",
   "metadata": {},
   "source": [
    "## 5. Target Encoding for Categorical Variables\n",
    "\n",
    "**Concept:** Encode categories by their mean target value.\n",
    "\n",
    "**Mathematics:**\n",
    "$$\\text{encoding}(c) = \\frac{\\sum_{i \\in c} y_i + \\alpha \\cdot \\mu_{global}}{n_c + \\alpha}$$\n",
    "\n",
    "Where:\n",
    "- $c$ = category\n",
    "- $\\alpha$ = smoothing parameter\n",
    "- $\\mu_{global}$ = global mean of target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6089511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From-scratch target encoding with smoothing\n",
    "class TargetEncoder:\n",
    "    def __init__(self, smoothing=1.0):\n",
    "        self.smoothing = smoothing\n",
    "        self.encoding_map = {}\n",
    "        self.global_mean = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Learn encoding from training data\"\"\"\n",
    "        self.global_mean = y.mean()\n",
    "        \n",
    "        # Calculate smoothed mean for each category\n",
    "        for category in X.unique():\n",
    "            category_mask = (X == category)\n",
    "            category_target_sum = y[category_mask].sum()\n",
    "            category_count = category_mask.sum()\n",
    "            \n",
    "            # Smoothed mean\n",
    "            smoothed_mean = (\n",
    "                (category_target_sum + self.smoothing * self.global_mean) /\n",
    "                (category_count + self.smoothing)\n",
    "            )\n",
    "            \n",
    "            self.encoding_map[category] = smoothed_mean\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply encoding\"\"\"\n",
    "        return X.map(self.encoding_map).fillna(self.global_mean)\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "# Apply to lot_id\n",
    "encoder = TargetEncoder(smoothing=10.0)\n",
    "df['lot_id_encoded'] = encoder.fit_transform(df['lot_id'], df['yield'])\n",
    "\n",
    "print(\"Target Encoding Results:\")\n",
    "print(df.groupby('lot_id').agg({\n",
    "    'yield': ['mean', 'count'],\n",
    "    'lot_id_encoded': 'mean'\n",
    "}).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f4b8bb",
   "metadata": {},
   "source": [
    "## 6. Feature Hashing for High-Cardinality Categoricals\n",
    "\n",
    "**Concept:** Hash categorical values into fixed-size feature space.\n",
    "\n",
    "**Benefits:**\n",
    "- Handles unlimited categories with fixed memory\n",
    "- No need to store mapping dictionaries\n",
    "- Fast encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fa0de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate high-cardinality categorical (device serial numbers)\n",
    "device_ids = [f\"DEV_{i:06d}\" for i in range(len(df))]\n",
    "df['device_id'] = device_ids\n",
    "\n",
    "# Feature hashing\n",
    "hasher = FeatureHasher(n_features=10, input_type='string')\n",
    "hashed_features = hasher.transform([[did] for did in df['device_id']])\n",
    "hashed_df = pd.DataFrame(\n",
    "    hashed_features.toarray(),\n",
    "    columns=[f'hash_{i}' for i in range(10)]\n",
    ")\n",
    "\n",
    "print(f\"Original cardinality: {df['device_id'].nunique()}\")\n",
    "print(f\"Hashed features: {hashed_df.shape[1]}\")\n",
    "print(f\"\\nSample hashed features:\")\n",
    "print(hashed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce594c19",
   "metadata": {},
   "source": [
    "## 7. Automated Feature Engineering with Featuretools\n",
    "\n",
    "**Concept:** Automatically discover and create features using deep feature synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89668b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Featuretools requires installation: pip install featuretools\n",
    "# For demonstration, we'll show the conceptual approach\n",
    "\n",
    "print(\"Automated Feature Engineering Workflow:\")\n",
    "print(\"\\n1. Define entity (table) with index\")\n",
    "print(\"2. Specify feature primitives (operations)\")\n",
    "print(\"3. Run deep feature synthesis\")\n",
    "print(\"4. Select top features by importance\")\n",
    "\n",
    "# Manual example: Create time-based aggregations\n",
    "# (simulating what Featuretools would do automatically)\n",
    "df['power'] = df['vdd'] * df['idd']\n",
    "df['power_density'] = df['power'] / (df['die_x'] + df['die_y'] + 1)\n",
    "df['thermal_index'] = df['temp'] / df['freq'] * 1000\n",
    "df['efficiency'] = df['freq'] / df['power']\n",
    "\n",
    "print(\"\\nAuto-generated features:\")\n",
    "for col in ['power', 'power_density', 'thermal_index', 'efficiency']:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d580c1",
   "metadata": {},
   "source": [
    "## 8. Complete Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad60727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all engineered features\n",
    "feature_cols = [\n",
    "    # Original\n",
    "    'vdd', 'idd', 'freq', 'temp', 'vth',\n",
    "    # Interactions\n",
    "    'vdd_x_idd', 'temp_x_freq', 'vdd_x_vth',\n",
    "    # Target encoded\n",
    "    'lot_id_encoded',\n",
    "    # Auto-generated\n",
    "    'power', 'power_density', 'thermal_index', 'efficiency'\n",
    "]\n",
    "\n",
    "X_full = df_interact[feature_cols]\n",
    "y = df_interact['yield']\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "model_final = Ridge(alpha=1.0)\n",
    "model_final.fit(X_train_scaled, y_train)\n",
    "y_pred = model_final.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Performance with Engineered Features:\")\n",
    "print(f\"  RMSE: {rmse:.2f}%\")\n",
    "print(f\"  RÂ² Score: {r2:.4f}\")\n",
    "print(f\"\\nTop 5 Most Important Features:\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'coefficient': np.abs(model_final.coef_)\n",
    "}).sort_values('coefficient', ascending=False)\n",
    "print(feature_importance.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa09afb5",
   "metadata": {},
   "source": [
    "## 9. Visualization: Feature Engineering Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c24b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Predictions vs Actual\n",
    "axes[0, 0].scatter(y_test, y_pred, alpha=0.5)\n",
    "axes[0, 0].plot([70, 100], [70, 100], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Yield (%)')\n",
    "axes[0, 0].set_ylabel('Predicted Yield (%)')\n",
    "axes[0, 0].set_title(f'Predictions (RÂ²={r2:.3f})')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# 2. Feature Importance\n",
    "top_features = feature_importance.head(10)\n",
    "axes[0, 1].barh(range(len(top_features)), top_features['coefficient'])\n",
    "axes[0, 1].set_yticks(range(len(top_features)))\n",
    "axes[0, 1].set_yticklabels(top_features['feature'])\n",
    "axes[0, 1].set_xlabel('|Coefficient|')\n",
    "axes[0, 1].set_title('Top 10 Feature Importance')\n",
    "axes[0, 1].invert_yaxis()\n",
    "\n",
    "# 3. Interaction feature: power vs yield\n",
    "axes[1, 0].scatter(df_interact['vdd_x_idd'], df_interact['yield'], alpha=0.5)\n",
    "axes[1, 0].set_xlabel('Power (vdd Ã— idd)')\n",
    "axes[1, 0].set_ylabel('Yield (%)')\n",
    "axes[1, 0].set_title('Interaction Feature: Power vs Yield')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# 4. Target encoding effect\n",
    "lot_stats = df_interact.groupby('lot_id')['yield'].agg(['mean', 'std']).reset_index()\n",
    "axes[1, 1].bar(range(len(lot_stats)), lot_stats['mean'], yerr=lot_stats['std'], capsize=5)\n",
    "axes[1, 1].set_xticks(range(len(lot_stats)))\n",
    "axes[1, 1].set_xticklabels(lot_stats['lot_id'])\n",
    "axes[1, 1].set_ylabel('Yield (%)')\n",
    "axes[1, 1].set_title('Target Encoding: Lot-Level Patterns')\n",
    "axes[1, 1].grid(True, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed05bd24",
   "metadata": {},
   "source": [
    "## 10. Project Templates\n",
    "\n",
    "### Project 1: Wafer-Level Feature Engineering\n",
    "**Objective:** Create spatial features from die coordinates to predict yield patterns\n",
    "- Extract radial distance from wafer center\n",
    "- Calculate neighborhood aggregations (8 surrounding dies)\n",
    "- Detect edge effects using distance-to-edge features\n",
    "- Train model with spatial features vs baseline\n",
    "- **Success Metric:** >15% improvement in yield prediction accuracy\n",
    "\n",
    "### Project 2: Multi-Parameter Interaction Discovery\n",
    "**Objective:** Automatically find predictive parameter interactions in STDF data\n",
    "- Implement exhaustive pairwise interaction search\n",
    "- Rank interactions by mutual information with yield\n",
    "- Create top-K interaction features\n",
    "- Compare with domain-expert selected interactions\n",
    "- **Success Metric:** Discover 3+ non-obvious interactions with high predictive power\n",
    "\n",
    "### Project 3: Categorical Feature Optimization\n",
    "**Objective:** Compare encoding strategies for high-cardinality test program IDs\n",
    "- Implement one-hot, target, ordinal, and hash encoding\n",
    "- Measure encoding time and memory usage\n",
    "- Evaluate model performance with each encoding\n",
    "- Handle rare categories and missing values\n",
    "- **Success Metric:** Find optimal encoding with <5% performance loss vs one-hot at 50% memory\n",
    "\n",
    "### Project 4: Temporal Feature Engineering\n",
    "**Objective:** Create time-based features from test timestamp data\n",
    "- Extract hour-of-day, day-of-week patterns\n",
    "- Calculate time-since-last-test for equipment\n",
    "- Detect test sequence dependencies\n",
    "- Build rolling window aggregations\n",
    "- **Success Metric:** Identify temporal patterns explaining 10%+ yield variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d7a791",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Key Takeaways\n",
    "\n",
    "**When to Use Advanced Feature Engineering:**\n",
    "- Complex, non-linear relationships in data\n",
    "- Domain knowledge suggests specific interactions\n",
    "- High-cardinality categoricals (>100 levels)\n",
    "- Performance matters more than interpretability\n",
    "\n",
    "**Limitations:**\n",
    "- âš ï¸ Curse of dimensionality with polynomial features\n",
    "- âš ï¸ Target encoding can leak information if not done carefully\n",
    "- âš ï¸ Feature hashing loses interpretability\n",
    "- âš ï¸ Automated methods may create spurious features\n",
    "\n",
    "**Best Practices:**\n",
    "1. Start with domain-guided interactions before automation\n",
    "2. Use regularization (Ridge/Lasso) with many features\n",
    "3. Validate encoding on separate fold to prevent leakage\n",
    "4. Monitor feature importance and prune low-value features\n",
    "5. Document feature engineering logic for production\n",
    "\n",
    "**Next Steps:**\n",
    "- Study 104: Model Interpretability to understand engineered features\n",
    "- Explore AutoML (105) for automated feature discovery\n",
    "- Apply to real STDF data with >100 test parameters"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
