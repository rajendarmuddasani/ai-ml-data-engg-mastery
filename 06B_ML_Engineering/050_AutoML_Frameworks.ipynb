{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea603eb4",
   "metadata": {},
   "source": [
    "# 050: AutoML Frameworks",
    "",
    "## \ud83d\udcda Learning Objectives",
    "",
    "By the end of this comprehensive notebook, you will master:",
    "",
    "1. **Understand AutoML Philosophy** - Why automated ML matters, when to use vs manual ML, trade-offs (speed vs control)",
    "2. **Master Auto-sklearn** - Bayesian optimization, meta-learning, ensemble construction, automated preprocessing",
    "3. **Implement TPOT** - Genetic programming for pipeline optimization, evolutionary algorithms, fitness functions",
    "4. **Deploy H2O AutoML** - Distributed AutoML, leaderboard, explainability, production deployment",
    "5. **Build Custom AutoML** - Create simplified AutoML from scratch (search space, optimization, evaluation)",
    "6. **Apply to Semiconductor Testing** - Automated model selection for parametric test optimization, yield prediction",
    "7. **Optimize for Production** - Time budgets, resource constraints, model interpretability, deployment patterns",
    "8. **Evaluate Trade-offs** - AutoML vs manual ML (performance, time, interpretability, cost)",
    "",
    "---",
    "",
    "## \ud83c\udfaf What is AutoML?",
    "",
    "**AutoML (Automated Machine Learning)** automates the end-to-end process of applying machine learning to real-world problems, including:",
    "",
    "1. **Data Preprocessing**: Handling missing values, encoding categoricals, scaling features",
    "2. **Feature Engineering**: Creating new features, selecting relevant features",
    "3. **Model Selection**: Choosing best algorithm (Linear, Tree-based, Neural Networks)",
    "4. **Hyperparameter Optimization**: Finding optimal hyperparameters via search (Grid, Random, Bayesian)",
    "5. **Ensemble Construction**: Combining multiple models for better performance",
    "6. **Pipeline Optimization**: End-to-end workflow from raw data to predictions",
    "",
    "---",
    "",
    "## \ud83d\udca1 Why AutoML Matters",
    "",
    "### **Traditional ML Workflow Pain Points:**",
    "",
    "| **Manual Step** | **Challenge** | **Time Required** | **Expertise Needed** |",
    "|----------------|--------------|-------------------|---------------------|",
    "| Data preprocessing | Handle missing values, encoding, scaling | 2-5 hours | Intermediate |",
    "| Feature engineering | Domain knowledge, trial-and-error | 5-20 hours | Advanced |",
    "| Model selection | Try 10-20 algorithms manually | 3-10 hours | Advanced |",
    "| Hyperparameter tuning | Grid search over large space | 5-50 hours (compute) | Expert |",
    "| Ensemble construction | Combine models, avoid overfitting | 2-5 hours | Expert |",
    "| **Total** | **Manual ML project** | **20-90 hours** | **Expert required** |",
    "",
    "### **AutoML Solution:**",
    "",
    "| **Automated Step** | **AutoML Approach** | **Time Required** | **Expertise Needed** |",
    "|-------------------|-------------------|-------------------|---------------------|",
    "| Data preprocessing | Automatic detection and handling | Automatic | None |",
    "| Feature engineering | Automated feature generation | Automatic | None |",
    "| Model selection | Intelligent search (Bayesian, GP) | 1-10 hours (compute) | Basic |",
    "| Hyperparameter tuning | Efficient search algorithms | Automatic | None |",
    "| Ensemble construction | Automatic model stacking | Automatic | None |",
    "| **Total** | **AutoML project** | **1-10 hours** | **Basic ML knowledge** |",
    "",
    "**Key Benefits:**",
    "- \u26a1 **Speed**: 10-90x faster than manual ML (hours vs days/weeks)",
    "- \ud83c\udfaf **Performance**: Often matches or beats manual ML (90-95% of expert performance)",
    "- \ud83d\ude80 **Accessibility**: Democratizes ML (non-experts can build production models)",
    "- \ud83d\udd04 **Reproducibility**: Consistent results, versioned experiments",
    "- \ud83d\udcca **Exploration**: Tries algorithms you might not consider manually",
    "",
    "---",
    "",
    "## \ud83c\udfed Semiconductor Testing Use Case",
    "",
    "**Challenge:** Optimize parametric test flow for 100+ test parameters, predict device yield",
    "",
    "**Manual Approach:**",
    "- Engineer tries Linear Regression \u2192 75% accuracy",
    "- Try Random Forest \u2192 82% accuracy",
    "- Tune hyperparameters for 2 days \u2192 85% accuracy",
    "- **Total time: 1 week, final accuracy: 85%**",
    "",
    "**AutoML Approach:**",
    "- Set time budget: 4 hours",
    "- AutoML tries 50+ pipelines (preprocessing + models + hyperparameters)",
    "- Best model: XGBoost with custom preprocessing \u2192 88% accuracy",
    "- Ensemble top 5 models \u2192 90% accuracy",
    "- **Total time: 4 hours (mostly compute), final accuracy: 90%**",
    "",
    "**Impact:** 5% accuracy improvement = 2-5% yield gain = **$50M-$200M annually** for high-volume production",
    "",
    "---",
    "",
    "## \ud83d\udd0d AutoML Frameworks Comparison",
    "",
    "```mermaid",
    "graph TB",
    "    subgraph \"AutoML Landscape\"",
    "        A[AutoML Frameworks] --> B[Auto-sklearn]",
    "        A --> C[TPOT]",
    "        A --> D[H2O AutoML]",
    "        A --> E[Google AutoML]",
    "        A --> F[Azure AutoML]",
    "        ",
    "        B --> B1[Bayesian Optimization]",
    "        B --> B2[Meta-Learning]",
    "        B --> B3[Ensemble Selection]",
    "        ",
    "        C --> C1[Genetic Programming]",
    "        C --> C2[Pipeline Evolution]",
    "        C --> C3[Tree-based Models]",
    "        ",
    "        D --> D1[Distributed Computing]",
    "        D --> D2[Stacked Ensembles]",
    "        D --> D3[Explainability]",
    "        ",
    "        E --> E1[Cloud-based]",
    "        E --> E2[Neural Architecture Search]",
    "        E --> E3[Transfer Learning]",
    "        ",
    "        F --> F1[Azure Integration]",
    "        F --> F2[Automated ML]",
    "        F --> F3[Responsible AI]",
    "    end",
    "    ",
    "    style A fill:#3498db,color:#fff",
    "    style B fill:#2ecc71,color:#fff",
    "    style C fill:#e74c3c,color:#fff",
    "    style D fill:#f39c12,color:#fff",
    "```",
    "",
    "---",
    "",
    "## \ud83d\udcca Framework Comparison Table",
    "",
    "| **Framework** | **Algorithm** | **Strength** | **Best For** | **Speed** | **Interpretability** |",
    "|--------------|--------------|-------------|--------------|-----------|-------------------|",
    "| **Auto-sklearn** | Bayesian Optimization + Meta-Learning | Best accuracy | Tabular data, classification/regression | Medium | Low-Medium |",
    "| **TPOT** | Genetic Programming | Pipeline optimization | Feature engineering, tree models | Slow | Medium |",
    "| **H2O AutoML** | Grid/Random + Stacking | Scalability, deployment | Large datasets, production | Fast | High |",
    "| **Google AutoML** | Neural Architecture Search | Deep learning, images | Vision, NLP, structured data | Medium | Low |",
    "| **Azure AutoML** | Ensemble methods | Cloud integration | Enterprise, Azure users | Fast | Medium-High |",
    "| **PyCaret** | Grid Search + Ensembles | Ease of use | Rapid prototyping | Fast | High |",
    "",
    "---",
    "",
    "## \u2696\ufe0f When to Use AutoML vs Manual ML",
    "",
    "### **Use AutoML When:**",
    "- \u2705 Tight deadline (need model in hours/days, not weeks)",
    "- \u2705 Limited ML expertise (data scientists busy, analysts need models)",
    "- \u2705 Tabular data (AutoML excels at structured data)",
    "- \u2705 Baseline needed quickly (AutoML as starting point)",
    "- \u2705 Multiple similar projects (manufacturing, A/B testing)",
    "- \u2705 Reproducibility critical (versioned, documented experiments)",
    "",
    "### **Use Manual ML When:**",
    "- \u2705 Custom architecture needed (novel problem, research)",
    "- \u2705 Interpretability critical (medical, legal, high-stakes)",
    "- \u2705 Domain-specific features (complex feature engineering)",
    "- \u2705 Extreme constraints (latency < 1ms, model < 1MB)",
    "- \u2705 Cutting-edge required (AutoML lags research by 1-2 years)",
    "- \u2705 Learning/control desired (educational, full control)",
    "",
    "### **Hybrid Approach (Best of Both):**",
    "1. **Start with AutoML** \u2192 Get baseline in hours (e.g., 85% accuracy)",
    "2. **Analyze AutoML results** \u2192 Understand what works (XGBoost? Feature importance?)",
    "3. **Manual refinement** \u2192 Add domain features, tune further (85% \u2192 90%)",
    "4. **Production optimization** \u2192 Compress, quantize, deploy (latency, memory)",
    "",
    "---",
    "",
    "## \ud83d\udd2c AutoML Under the Hood",
    "",
    "### **Key Components:**",
    "",
    "1. **Search Space Definition:**",
    "   ```",
    "   Pipeline = Preprocessor + Feature Engineering + Model + Hyperparameters",
    "   ",
    "   Preprocessors: [None, StandardScaler, MinMaxScaler, RobustScaler]",
    "   Feature Engineering: [PCA, PolynomialFeatures, SelectKBest]",
    "   Models: [Linear, Tree-based, Ensemble, Neural Nets]",
    "   Hyperparameters: {learning_rate: [0.001, 0.01, 0.1], ...}",
    "   ",
    "   Total combinations: 4 \u00d7 3 \u00d7 10 \u00d7 1000 = 120,000 possibilities",
    "   ```",
    "",
    "2. **Optimization Strategy:**",
    "   - **Random Search**: Try random configurations (baseline)",
    "   - **Grid Search**: Exhaustive search (infeasible for large spaces)",
    "   - **Bayesian Optimization**: Model performance as Gaussian Process, select next trial intelligently",
    "   - **Genetic Programming**: Evolve pipelines via mutation and crossover",
    "   - **Meta-Learning**: Warm-start with configurations that worked on similar datasets",
    "",
    "3. **Evaluation:**",
    "   - **Cross-Validation**: Stratified K-Fold (default: 5-fold)",
    "   - **Holdout Set**: Final test set (never touched during optimization)",
    "   - **Metrics**: Accuracy, AUC, F1, RMSE (user-specified)",
    "",
    "4. **Ensemble Construction:**",
    "   - **Select top N models** (e.g., top 10 from 100 trials)",
    "   - **Stack with meta-learner** (Logistic Regression, XGBoost)",
    "   - **Weighted voting** (weight by validation performance)",
    "",
    "---",
    "",
    "## \ud83c\udfed Semiconductor Testing Applications",
    "",
    "### **Application 1: Parametric Test Optimization**",
    "- **Problem**: 100+ parametric tests, expensive test time ($50-$200/device-hour)",
    "- **AutoML Goal**: Predict device pass/fail from first 20 tests \u2192 Skip remaining 80 tests if high confidence",
    "- **Impact**: 60% test time reduction = $30M-$100M annual savings",
    "",
    "### **Application 2: Yield Prediction**",
    "- **Problem**: Predict wafer/lot yield from in-line metrology and process parameters",
    "- **AutoML Goal**: Automated feature engineering from 500+ process parameters \u2192 90%+ yield prediction accuracy",
    "- **Impact**: 2-5% yield improvement = $50M-$200M annually",
    "",
    "### **Application 3: Adaptive Test Insertion**",
    "- **Problem**: Which optional tests to run based on wafer/lot characteristics?",
    "- **AutoML Goal**: Real-time decision on test insertion (< 50ms latency)",
    "- **Impact**: Optimize quality vs cost trade-off = $10M-$50M annually",
    "",
    "### **Application 4: Failure Mode Classification**",
    "- **Problem**: Classify defects into 10-20 failure modes from parametric signatures",
    "- **AutoML Goal**: Multiclass classification with imbalanced classes (1-5% per mode)",
    "- **Impact**: Faster root cause analysis, 30-50% faster time-to-fix = $5M-$20M/incident",
    "",
    "---",
    "",
    "## \ud83c\udfaf AutoML General AI/ML Applications",
    "",
    "1. **E-commerce**: Product recommendation, price optimization, demand forecasting",
    "2. **Finance**: Credit scoring, fraud detection, algorithmic trading",
    "3. **Healthcare**: Disease diagnosis, readmission prediction, treatment optimization",
    "4. **Marketing**: Customer segmentation, churn prediction, campaign optimization",
    "5. **Manufacturing**: Quality control, predictive maintenance, supply chain optimization",
    "6. **Energy**: Load forecasting, renewable energy optimization, grid management",
    "",
    "---",
    "",
    "## \ud83d\udcc8 Performance Expectations",
    "",
    "**Typical AutoML vs Manual ML Performance:**",
    "",
    "| **Dataset Type** | **Manual ML (Expert)** | **AutoML (4-hour budget)** | **Gap** |",
    "|-----------------|---------------------|--------------------------|---------|",
    "| Tabular (small, < 10K rows) | 90% | 85-88% | 2-5% |",
    "| Tabular (medium, 10K-1M) | 92% | 90-92% | 0-2% |",
    "| Tabular (large, > 1M) | 93% | 92-94% | -1% to +1% (AutoML can win!) |",
    "| Images (CNNs) | 95% | 88-92% | 3-7% (AutoML lags on vision) |",
    "| Text (NLP) | 94% | 87-91% | 3-7% (AutoML lags on NLP) |",
    "",
    "**Key Insight:** AutoML is closest to expert performance on **tabular data** (structured, numerical/categorical features).",
    "",
    "---",
    "",
    "## \ud83d\ude80 What We'll Build",
    "",
    "In this notebook, you'll implement:",
    "",
    "1. **Auto-sklearn**: Bayesian optimization for semiconductor yield prediction",
    "2. **TPOT**: Genetic programming for parametric test optimization",
    "3. **H2O AutoML**: Distributed AutoML for large-scale device testing",
    "4. **Custom AutoML**: Build simplified AutoML from scratch (educational)",
    "5. **Production Deployment**: Export, serve, monitor AutoML models",
    "6. **Comparative Analysis**: AutoML vs manual ML on real semiconductor datasets",
    "",
    "**By the end, you'll confidently choose and deploy AutoML for production workloads.**",
    "",
    "---",
    "",
    "Let's dive in! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8340b2",
   "metadata": {},
   "source": [
    "## \ud83d\udcd0 Mathematical Foundation: AutoML Optimization\n",
    "\n",
    "### **1. The AutoML Optimization Problem**\n",
    "\n",
    "Given a machine learning pipeline space $\\mathcal{P}$ and dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$, find the optimal pipeline $p^*$ that minimizes loss:\n",
    "\n",
    "$$\n",
    "p^* = \\underset{p \\in \\mathcal{P}}{\\arg\\min} \\; \\mathbb{E}_{(x,y) \\sim \\mathcal{D}} \\left[ \\mathcal{L}(f_p(x), y) \\right]\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "- $\\mathcal{P}$: Pipeline search space (preprocessors \u00d7 feature engineering \u00d7 models \u00d7 hyperparameters)\n",
    "- $f_p$: Function defined by pipeline $p$\n",
    "- $\\mathcal{L}$: Loss function (cross-entropy, MSE, etc.)\n",
    "- $\\mathbb{E}[\\cdot]$: Expected loss (estimated via cross-validation)\n",
    "\n",
    "**Challenge:** $|\\mathcal{P}|$ can be $10^6$ to $10^{12}$ configurations \u2192 exhaustive search infeasible\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Bayesian Optimization (Auto-sklearn Core)**\n",
    "\n",
    "#### **Key Idea:**\n",
    "Model the objective function $f(p)$ (validation performance) as a **Gaussian Process** and select next configuration to evaluate via **acquisition function**.\n",
    "\n",
    "#### **Gaussian Process (GP):**\n",
    "\n",
    "A GP defines a distribution over functions:\n",
    "\n",
    "$$\n",
    "f(p) \\sim \\mathcal{GP}(\\mu(p), k(p, p'))\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "- $\\mu(p)$: Mean function (typically 0 or prior estimate)\n",
    "- $k(p, p')$: Kernel (covariance) function measuring similarity between pipelines\n",
    "\n",
    "**After observing $t$ configurations** $\\{p_1, p_2, \\ldots, p_t\\}$ with losses $\\{f(p_1), f(p_2), \\ldots, f(p_t)\\}$:\n",
    "\n",
    "$$\n",
    "f(p_{t+1}) | \\mathcal{D}_{1:t} \\sim \\mathcal{N}(\\mu_t(p_{t+1}), \\sigma_t^2(p_{t+1}))\n",
    "$$\n",
    "\n",
    "**Predictive mean:**\n",
    "$$\n",
    "\\mu_t(p) = k(p)^T (K + \\sigma_n^2 I)^{-1} \\mathbf{f}_{1:t}\n",
    "$$\n",
    "\n",
    "**Predictive variance:**\n",
    "$$\n",
    "\\sigma_t^2(p) = k(p, p) - k(p)^T (K + \\sigma_n^2 I)^{-1} k(p)\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "- $K$: Covariance matrix $K_{ij} = k(p_i, p_j)$\n",
    "- $k(p) = [k(p, p_1), k(p, p_2), \\ldots, k(p, p_t)]^T$\n",
    "- $\\mathbf{f}_{1:t} = [f(p_1), f(p_2), \\ldots, f(p_t)]^T$\n",
    "- $\\sigma_n^2$: Observation noise\n",
    "\n",
    "**Interpretation:**\n",
    "- $\\mu_t(p)$: Best guess of performance at $p$ given past observations\n",
    "- $\\sigma_t^2(p)$: Uncertainty about performance (high uncertainty \u2192 worth exploring)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Acquisition Functions:**\n",
    "\n",
    "Select next pipeline $p_{t+1}$ by maximizing acquisition function $\\alpha(p)$:\n",
    "\n",
    "$$\n",
    "p_{t+1} = \\underset{p \\in \\mathcal{P}}{\\arg\\max} \\; \\alpha(p | \\mathcal{D}_{1:t})\n",
    "$$\n",
    "\n",
    "**1. Expected Improvement (EI):**\n",
    "\n",
    "$$\n",
    "\\text{EI}(p) = \\mathbb{E}\\left[ \\max(0, f_{\\text{best}} - f(p)) \\right]\n",
    "$$\n",
    "\n",
    "Where $f_{\\text{best}} = \\min_{i=1}^t f(p_i)$ (best performance so far)\n",
    "\n",
    "**Closed form (under GP):**\n",
    "$$\n",
    "\\text{EI}(p) = \n",
    "\\begin{cases}\n",
    "(\\mu_t(p) - f_{\\text{best}} - \\xi) \\Phi(Z) + \\sigma_t(p) \\phi(Z) & \\text{if } \\sigma_t(p) > 0 \\\\\n",
    "0 & \\text{if } \\sigma_t(p) = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "Z = \\frac{\\mu_t(p) - f_{\\text{best}} - \\xi}{\\sigma_t(p)}\n",
    "$$\n",
    "\n",
    "- $\\Phi(\\cdot)$: Standard normal CDF\n",
    "- $\\phi(\\cdot)$: Standard normal PDF\n",
    "- $\\xi$: Exploration parameter (typically 0.01)\n",
    "\n",
    "**Intuition:** Balance exploitation (low $\\mu_t$, good predicted performance) and exploration (high $\\sigma_t$, uncertain regions)\n",
    "\n",
    "**2. Upper Confidence Bound (UCB):**\n",
    "\n",
    "$$\n",
    "\\text{UCB}(p) = \\mu_t(p) - \\kappa \\cdot \\sigma_t(p)\n",
    "$$\n",
    "\n",
    "Where $\\kappa$ controls exploration-exploitation trade-off (typically $\\kappa = 2$)\n",
    "\n",
    "**Intuition:** Optimistic estimate (assume performance is mean minus uncertainty)\n",
    "\n",
    "**3. Probability of Improvement (PI):**\n",
    "\n",
    "$$\n",
    "\\text{PI}(p) = \\Phi\\left( \\frac{\\mu_t(p) - f_{\\text{best}} - \\xi}{\\sigma_t(p)} \\right)\n",
    "$$\n",
    "\n",
    "**Intuition:** Probability that $p$ improves over current best\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Meta-Learning (Warm-Start)**\n",
    "\n",
    "Instead of starting Bayesian optimization from scratch, use **meta-features** to predict promising configurations.\n",
    "\n",
    "#### **Meta-Features:**\n",
    "\n",
    "Extract dataset characteristics:\n",
    "- **Statistical**: Mean, std, skewness, kurtosis per feature\n",
    "- **Information-theoretic**: Entropy, mutual information\n",
    "- **Model-based**: Linear model R\u00b2, tree depth\n",
    "- **Data**: # samples, # features, # classes, class imbalance ratio\n",
    "\n",
    "$$\n",
    "\\mathbf{m} = [\\text{n\\_samples}, \\text{n\\_features}, \\text{n\\_classes}, \\text{imbalance\\_ratio}, \\ldots] \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "#### **Meta-Learning Algorithm:**\n",
    "\n",
    "1. **Build meta-dataset** from past AutoML runs:\n",
    "   $$\n",
    "   \\mathcal{M} = \\{(\\mathbf{m}_j, p_j^*, f(p_j^*))\\}_{j=1}^M\n",
    "   $$\n",
    "   Where $(\\mathbf{m}_j, p_j^*, f(p_j^*))$ = (meta-features, best pipeline, best performance) for dataset $j$\n",
    "\n",
    "2. **For new dataset** with meta-features $\\mathbf{m}_{\\text{new}}$:\n",
    "   - Find $K$ most similar datasets: $\\mathcal{N}_K(\\mathbf{m}_{\\text{new}}) = \\{j_1, j_2, \\ldots, j_K\\}$ (K-NN in meta-feature space)\n",
    "   - Initialize Bayesian optimization with their best pipelines: $\\{p_{j_1}^*, p_{j_2}^*, \\ldots, p_{j_K}^*\\}$\n",
    "\n",
    "**Benefit:** Converge 5-10x faster (start with good configurations instead of random)\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Genetic Programming (TPOT Core)**\n",
    "\n",
    "#### **Representation:**\n",
    "\n",
    "Encode ML pipeline as a **tree**:\n",
    "\n",
    "```\n",
    "         Classifier (XGBoost)\n",
    "                |\n",
    "          Feature Union\n",
    "           /          \\\n",
    "    SelectKBest      PCA\n",
    "         |             |\n",
    "   StandardScaler  MinMaxScaler\n",
    "         |             |\n",
    "     Raw Features  Raw Features\n",
    "```\n",
    "\n",
    "**Genotype:** Tree structure with operators (preprocessing, feature engineering, models)\n",
    "\n",
    "**Phenotype:** Executable sklearn pipeline\n",
    "\n",
    "---\n",
    "\n",
    "#### **Evolutionary Algorithm:**\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Generate population of $N$ random pipelines: $\\mathcal{P}^{(0)} = \\{p_1^{(0)}, p_2^{(0)}, \\ldots, p_N^{(0)}\\}$\n",
    "\n",
    "2. **Fitness Evaluation:**\n",
    "   - Evaluate each pipeline via cross-validation: $f(p_i) = \\text{CV\\_Score}(p_i, \\mathcal{D})$\n",
    "\n",
    "3. **Selection:**\n",
    "   - Select top $M$ pipelines (tournament selection, roulette wheel)\n",
    "   - Probability of selection proportional to fitness: $P(\\text{select } p_i) \\propto \\exp(f(p_i) / T)$\n",
    "\n",
    "4. **Crossover:**\n",
    "   - Combine two parent pipelines $p_a, p_b$ \u2192 two offspring $p_c, p_d$\n",
    "   - Swap subtrees at random crossover points\n",
    "   \n",
    "   Example:\n",
    "   ```\n",
    "   Parent A: SelectKBest \u2192 XGBoost\n",
    "   Parent B: PCA \u2192 RandomForest\n",
    "   \n",
    "   Offspring C: SelectKBest \u2192 RandomForest (crossover)\n",
    "   Offspring D: PCA \u2192 XGBoost (crossover)\n",
    "   ```\n",
    "\n",
    "5. **Mutation:**\n",
    "   - Randomly modify pipeline with probability $p_{\\text{mut}}$ (typically 0.1-0.2)\n",
    "   - Mutations: Change operator, add/remove node, modify hyperparameter\n",
    "   \n",
    "   Example:\n",
    "   ```\n",
    "   Before: StandardScaler \u2192 XGBoost(depth=5)\n",
    "   After:  MinMaxScaler \u2192 XGBoost(depth=8)  (mutated)\n",
    "   ```\n",
    "\n",
    "6. **Repeat** for $G$ generations (typically 50-100)\n",
    "\n",
    "**Convergence:** Best pipeline in final generation $\\mathcal{P}^{(G)}$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Fitness Function:**\n",
    "\n",
    "$$\n",
    "\\text{Fitness}(p) = \\frac{1}{K} \\sum_{k=1}^K \\text{Score}_k(p) - \\lambda \\cdot \\text{Complexity}(p)\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "- $\\text{Score}_k(p)$: Performance on fold $k$ (accuracy, F1, AUC)\n",
    "- $\\text{Complexity}(p)$: Pipeline complexity penalty (tree depth, # operators)\n",
    "- $\\lambda$: Regularization weight (prevent overly complex pipelines)\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Ensemble Construction**\n",
    "\n",
    "After optimization (Bayesian or Genetic), construct ensemble from top $M$ models.\n",
    "\n",
    "#### **Ensemble Selection Algorithm (Caruana et al.):**\n",
    "\n",
    "1. **Sort models** by validation performance: $\\{p_1, p_2, \\ldots, p_M\\}$ (best to worst)\n",
    "\n",
    "2. **Greedy forward selection**:\n",
    "   - Initialize ensemble: $\\mathcal{E} = \\{\\}$\n",
    "   - For $t = 1, 2, \\ldots, T$ iterations:\n",
    "     - For each model $p_i$:\n",
    "       - Compute ensemble performance: $f(\\mathcal{E} \\cup \\{p_i\\})$ (weighted voting)\n",
    "     - Add model that improves ensemble most: $\\mathcal{E} \\leftarrow \\mathcal{E} \\cup \\{p^*\\}$\n",
    "     - **Allow duplicates** (model can be added multiple times \u2192 higher weight)\n",
    "\n",
    "3. **Final ensemble**: $\\mathcal{E} = \\{p_{i_1}, p_{i_2}, \\ldots, p_{i_T}\\}$ (may have duplicates)\n",
    "\n",
    "4. **Prediction**: Weighted voting\n",
    "   $$\n",
    "   \\hat{y} = \\frac{1}{T} \\sum_{t=1}^T f_{p_{i_t}}(x)\n",
    "   $$\n",
    "\n",
    "**Why allow duplicates?** Naturally assigns higher weight to better models\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Time Budget Allocation**\n",
    "\n",
    "Given total time budget $T_{\\text{total}}$, allocate time across optimization phases:\n",
    "\n",
    "$$\n",
    "T_{\\text{total}} = T_{\\text{meta}} + T_{\\text{search}} + T_{\\text{ensemble}}\n",
    "$$\n",
    "\n",
    "**Typical allocation (Auto-sklearn):**\n",
    "- $T_{\\text{meta}} = 0.05 \\cdot T_{\\text{total}}$ (meta-learning initialization)\n",
    "- $T_{\\text{search}} = 0.85 \\cdot T_{\\text{total}}$ (Bayesian optimization)\n",
    "- $T_{\\text{ensemble}} = 0.10 \\cdot T_{\\text{total}}$ (ensemble construction)\n",
    "\n",
    "**Within search phase**, allocate time per trial dynamically:\n",
    "- Early trials: Small time budget (quick exploration)\n",
    "- Later trials: Larger time budget (deeper evaluation of promising regions)\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Hyperparameter Importance (fANOVA)**\n",
    "\n",
    "After optimization, analyze which hyperparameters matter most using **functional ANOVA**:\n",
    "\n",
    "$$\n",
    "f(p) = f_0 + \\sum_{i} f_i(p_i) + \\sum_{i<j} f_{ij}(p_i, p_j) + \\ldots\n",
    "$$\n",
    "\n",
    "**Variance decomposition:**\n",
    "$$\n",
    "V[f] = \\sum_{i} V[f_i] + \\sum_{i<j} V[f_{ij}] + \\ldots\n",
    "$$\n",
    "\n",
    "**Importance of hyperparameter $i$:**\n",
    "$$\n",
    "\\text{Importance}(p_i) = \\frac{V[f_i] + \\sum_{j \\neq i} V[f_{ij}]}{V[f]}\n",
    "$$\n",
    "\n",
    "**Interpretation:** Which hyperparameters explain most variance in model performance?\n",
    "\n",
    "**Semiconductor Example:**\n",
    "- For yield prediction, tree depth (importance = 35%) > learning rate (25%) > # features (20%)\n",
    "- Focus manual tuning on high-importance hyperparameters\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Summary Table: Optimization Algorithms**\n",
    "\n",
    "| **Algorithm** | **Strategy** | **Complexity** | **Convergence** | **Best For** |\n",
    "|--------------|-------------|----------------|----------------|--------------|\n",
    "| **Random Search** | Sample uniformly | $O(N)$ | Slow (N trials) | Baseline |\n",
    "| **Grid Search** | Exhaustive | $O(d^k)$ exponential | Guaranteed (if exhaustive) | Small spaces |\n",
    "| **Bayesian Optimization** | GP + EI | $O(N^3)$ per iteration | Fast (50-200 trials) | Expensive evaluations |\n",
    "| **Genetic Programming** | Evolution | $O(N \\cdot G \\cdot C)$ | Medium (100-500 evals) | Pipeline structure search |\n",
    "| **Hyperband** | Successive halving | $O(N \\log N)$ | Fast (adaptive) | Limited budget |\n",
    "\n",
    "**Where:**\n",
    "- $N$: # trials\n",
    "- $d$: # hyperparameters\n",
    "- $k$: Grid size per hyperparameter\n",
    "- $G$: # generations (GP)\n",
    "- $C$: Population size (GP)\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Semiconductor-Specific Formulations**\n",
    "\n",
    "#### **Test-Time Optimization:**\n",
    "\n",
    "Objective: Minimize test cost + yield loss\n",
    "\n",
    "$$\n",
    "p^* = \\underset{p \\in \\mathcal{P}}{\\arg\\min} \\; C_{\\text{test}}(p) + C_{\\text{yield}} \\cdot (1 - \\text{Recall}(p))\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "- $C_{\\text{test}}(p)$: Test time cost ($50-$200/device-hour)\n",
    "- $C_{\\text{yield}}$: Yield loss per missed defect ($10M-$100M annually)\n",
    "- $\\text{Recall}(p)$: Defect detection rate\n",
    "\n",
    "**AutoML modification:** Multi-objective optimization (Pareto frontier of cost vs recall)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Adaptive Test Insertion:**\n",
    "\n",
    "Sequential decision: After each test stage, predict final outcome\n",
    "\n",
    "$$\n",
    "\\text{Stop}(p, s) = \n",
    "\\begin{cases}\n",
    "\\text{True} & \\text{if } P(\\text{defect} | \\text{tests}_{1:s}) > \\theta_{\\text{high}} \\\\\n",
    "\\text{True} & \\text{if } P(\\text{pass} | \\text{tests}_{1:s}) > \\theta_{\\text{low}} \\\\\n",
    "\\text{False} & \\text{otherwise (continue testing)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**AutoML goal:** Learn optimal stopping thresholds $\\theta_{\\text{high}}, \\theta_{\\text{low}}$ via policy search\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "1. \u2705 **Bayesian Optimization**: Models $f(p)$ as GP, selects next trial via EI (exploitation + exploration)\n",
    "2. \u2705 **Meta-Learning**: Warm-start with configurations from similar datasets (5-10x speedup)\n",
    "3. \u2705 **Genetic Programming**: Evolves pipeline structure via crossover and mutation\n",
    "4. \u2705 **Ensemble Selection**: Greedy forward selection with replacement (natural weighting)\n",
    "5. \u2705 **Time Budget**: Allocate 5% meta, 85% search, 10% ensemble\n",
    "6. \u2705 **Hyperparameter Importance**: fANOVA identifies which hyperparameters matter most\n",
    "7. \u2705 **Multi-Objective**: Semiconductor often requires cost-performance trade-offs (Pareto optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24e147a",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b446bf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Auto-sklearn: Bayesian Optimization for AutoML\n",
    "# ========================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Install auto-sklearn if not available\n",
    "try:\n",
    "    import autosklearn\n",
    "    import autosklearn.classification\n",
    "    import autosklearn.metrics\n",
    "except ImportError:\n",
    "    print(\"Installing auto-sklearn (this may take a few minutes)...\")\n",
    "    import sys\n",
    "    import subprocess\n",
    "    # Note: auto-sklearn requires specific dependencies, may need conda install\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"auto-sklearn\"])\n",
    "    import autosklearn\n",
    "    import autosklearn.classification\n",
    "    import autosklearn.metrics\n",
    "print(\"=\" * 80)\n",
    "print(\"Auto-sklearn: Automated Machine Learning with Bayesian Optimization\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"auto-sklearn version: {autosklearn.__version__}\")\n",
    "print()\n",
    "# ========================================\n",
    "# Generate Semiconductor Yield Dataset\n",
    "# ========================================\n",
    "print(\"Generating Semiconductor Wafer Yield Dataset...\")\n",
    "print(\"-\" * 80)\n",
    "np.random.seed(42)\n",
    "n_samples = 5000\n",
    "n_wafers = 100\n",
    "devices_per_wafer = n_samples // n_wafers\n",
    "# Parametric test features (20 features)\n",
    "feature_names = [\n",
    "    'Vdd_min', 'Vdd_max', 'Vdd_typ',\n",
    "    'Idd_active', 'Idd_standby', 'Idd_sleep',\n",
    "    'freq_min', 'freq_max', 'freq_typ',\n",
    "    'temp_min', 'temp_max', 'temp_typ',\n",
    "    'power_active', 'power_standby',\n",
    "    'leakage_current',\n",
    "    'setup_time', 'hold_time',\n",
    "    'rise_time', 'fall_time',\n",
    "    'noise_margin'\n",
    "]\n",
    "# Generate features with realistic correlations\n",
    "X = np.random.randn(n_samples, 20)\n",
    "# Add realistic correlations (Vdd correlates with Idd, frequency, power)\n",
    "X[:, 3] = X[:, 0] * 0.7 + np.random.randn(n_samples) * 0.3  # Idd_active ~ Vdd_min\n",
    "X[:, 12] = X[:, 0] * 0.6 + X[:, 6] * 0.4 + np.random.randn(n_samples) * 0.2  # power ~ Vdd + freq\n",
    "# Spatial features (wafer-level)\n",
    "wafer_ids = np.repeat(np.arange(n_wafers), devices_per_wafer)\n",
    "die_x = np.random.uniform(0, 300, n_samples)  # mm\n",
    "die_y = np.random.uniform(0, 300, n_samples)  # mm\n",
    "distance_from_center = np.sqrt((die_x - 150)**2 + (die_y - 150)**2)\n",
    "# Add spatial features to X\n",
    "X_with_spatial = np.column_stack([X, distance_from_center])\n",
    "feature_names_with_spatial = feature_names + ['dist_from_center']\n",
    "# Generate yield labels (pass/fail)\n",
    "# Failures correlate with: low Vdd_min, high Idd, edge dies, specific wafer batches\n",
    "fail_prob = 1 / (1 + np.exp(-(\n",
    "    -5.0 +  # Bias (2-3% failure rate)\n",
    "    -2.0 * X[:, 0] +  # Low Vdd_min \u2192 higher failure\n",
    "    1.5 * X[:, 3] +   # High Idd_active \u2192 higher failure\n",
    "    0.01 * distance_from_center +  # Edge dies \u2192 higher failure\n",
    "    0.5 * (wafer_ids % 10 == 0)  # Every 10th wafer batch has issues\n",
    ")))\n",
    "y = (fail_prob > 0.5).astype(int)\n",
    "# Adjust to exactly 2-3% failure rate\n",
    "n_fails_target = int(n_samples * 0.025)\n",
    "fail_indices = np.argsort(fail_prob)[-n_fails_target:]\n",
    "y = np.zeros(n_samples, dtype=int)\n",
    "y[fail_indices] = 1\n",
    "print(f\"Dataset generated:\")\n",
    "print(f\"  Total devices: {n_samples}\")\n",
    "print(f\"  Features: {len(feature_names_with_spatial)}\")\n",
    "print(f\"  Pass (class 0): {np.sum(y==0)} ({100*np.mean(y==0):.1f}%)\")\n",
    "print(f\"  Fail (class 1): {np.sum(y==1)} ({100*np.mean(y==1):.1f}%)\")\n",
    "print(f\"  Imbalance ratio: 1:{int(np.sum(y==0)/np.sum(y==1))}\")\n",
    "print()\n",
    "# Create DataFrame for easier inspection\n",
    "df = pd.DataFrame(X_with_spatial, columns=feature_names_with_spatial)\n",
    "df['wafer_id'] = wafer_ids\n",
    "df['yield_label'] = y\n",
    "print(\"Sample data:\")\n",
    "print(df.head(10))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7844a8f",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b96e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Split Data (Stratified)\n",
    "# ========================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_with_spatial, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Train set: {len(X_train)} samples ({np.sum(y_train==1)} failures)\")\n",
    "print(f\"Test set:  {len(X_test)} samples ({np.sum(y_test==1)} failures)\")\n",
    "print()\n",
    "# ========================================\n",
    "# Baseline: Manual ML (Random Forest)\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Baseline: Manual ML (Random Forest with default hyperparameters)\")\n",
    "print(\"=\" * 80)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Simple manual pipeline\n",
    "manual_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=10, \n",
    "                                         class_weight='balanced', random_state=42))\n",
    "])\n",
    "manual_pipeline.fit(X_train, y_train)\n",
    "y_pred_manual = manual_pipeline.predict(X_test)\n",
    "y_proba_manual = manual_pipeline.predict_proba(X_test)[:, 1]\n",
    "# Metrics\n",
    "acc_manual = accuracy_score(y_test, y_pred_manual)\n",
    "prec_manual = precision_score(y_test, y_pred_manual, zero_division=0)\n",
    "rec_manual = recall_score(y_test, y_pred_manual)\n",
    "f1_manual = f1_score(y_test, y_pred_manual)\n",
    "auc_manual = roc_auc_score(y_test, y_proba_manual)\n",
    "print(f\"Manual RF Performance:\")\n",
    "print(f\"  Accuracy:  {acc_manual:.4f}\")\n",
    "print(f\"  Precision: {prec_manual:.4f}\")\n",
    "print(f\"  Recall:    {rec_manual:.4f}\")\n",
    "print(f\"  F1 Score:  {f1_manual:.4f}\")\n",
    "print(f\"  AUC:       {auc_manual:.4f}\")\n",
    "print()\n",
    "cm_manual = confusion_matrix(y_test, y_pred_manual)\n",
    "print(f\"Confusion Matrix:\")\n",
    "print(cm_manual)\n",
    "print(f\"  Missed failures: {cm_manual[1,0]} out of {np.sum(y_test==1)}\")\n",
    "print()\n",
    "# ========================================\n",
    "# Auto-sklearn: Automated ML\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Auto-sklearn: Automated Machine Learning\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Configuration:\")\n",
    "print(\"  Time budget: 300 seconds (5 minutes for demo, use 1-4 hours in production)\")\n",
    "print(\"  Per-run time limit: 30 seconds\")\n",
    "print(\"  Memory limit: 3072 MB\")\n",
    "print(\"  Ensemble size: 50\")\n",
    "print(\"  Metric: Recall (critical for defect detection)\")\n",
    "print()\n",
    "# Create Auto-sklearn classifier\n",
    "automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "    time_left_for_this_task=300,  # Total time budget (seconds)\n",
    "    per_run_time_limit=30,        # Max time per model trial\n",
    "    memory_limit=3072,            # Memory limit (MB)\n",
    "    ensemble_size=50,             # Ensemble from top 50 models\n",
    "    ensemble_nbest=200,           # Consider top 200 for ensemble\n",
    "    initial_configurations_via_metalearning=25,  # Use meta-learning\n",
    "    metric=autosklearn.metrics.recall,  # Optimize for recall (defect detection)\n",
    "    n_jobs=4,                     # Parallel trials\n",
    "    seed=42\n",
    ")\n",
    "print(\"Starting Auto-sklearn optimization...\")\n",
    "print(\"(This will take ~5 minutes. In production, use 1-4 hours for best results)\")\n",
    "print()\n",
    "import time\n",
    "start_time = time.time()\n",
    "# Fit Auto-sklearn\n",
    "automl.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\u2705 Auto-sklearn completed in {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995711d4",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee9d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Auto-sklearn Results\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Auto-sklearn: Results & Analysis\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "# Predict\n",
    "y_pred_automl = automl.predict(X_test)\n",
    "y_proba_automl = automl.predict_proba(X_test)[:, 1]\n",
    "# Metrics\n",
    "acc_automl = accuracy_score(y_test, y_pred_automl)\n",
    "prec_automl = precision_score(y_test, y_pred_automl, zero_division=0)\n",
    "rec_automl = recall_score(y_test, y_pred_automl)\n",
    "f1_automl = f1_score(y_test, y_pred_automl)\n",
    "auc_automl = roc_auc_score(y_test, y_proba_automl)\n",
    "print(f\"Auto-sklearn Performance:\")\n",
    "print(f\"  Accuracy:  {acc_automl:.4f}\")\n",
    "print(f\"  Precision: {prec_automl:.4f}\")\n",
    "print(f\"  Recall:    {rec_automl:.4f}\")\n",
    "print(f\"  F1 Score:  {f1_automl:.4f}\")\n",
    "print(f\"  AUC:       {auc_automl:.4f}\")\n",
    "print()\n",
    "cm_automl = confusion_matrix(y_test, y_pred_automl)\n",
    "print(f\"Confusion Matrix:\")\n",
    "print(cm_automl)\n",
    "print(f\"  Missed failures: {cm_automl[1,0]} out of {np.sum(y_test==1)}\")\n",
    "print()\n",
    "# Comparison\n",
    "print(\"Performance Comparison:\")\n",
    "print(f\"  Recall improvement: {rec_automl - rec_manual:.4f} ({100*(rec_automl - rec_manual):.1f}%)\")\n",
    "print(f\"  F1 improvement: {f1_automl - f1_manual:.4f} ({100*(f1_automl - f1_manual):.1f}%)\")\n",
    "print(f\"  Missed failures reduced: {cm_manual[1,0]} \u2192 {cm_automl[1,0]} (saved {cm_manual[1,0] - cm_automl[1,0]} devices)\")\n",
    "print()\n",
    "# ========================================\n",
    "# Show Models Tried\n",
    "# ========================================\n",
    "print(\"Models Explored by Auto-sklearn:\")\n",
    "print(\"-\" * 80)\n",
    "# Get statistics\n",
    "stats = automl.sprint_statistics()\n",
    "print(stats)\n",
    "print()\n",
    "# Show final ensemble\n",
    "print(\"Final Ensemble:\")\n",
    "print(\"-\" * 80)\n",
    "print(automl.show_models())\n",
    "print()\n",
    "# ========================================\n",
    "# Leaderboard (Top Models)\n",
    "# ========================================\n",
    "print(\"Leaderboard: Top 10 Models\")\n",
    "print(\"-\" * 80)\n",
    "# Get leaderboard from automl results\n",
    "leaderboard = automl.leaderboard()\n",
    "print(leaderboard.head(10))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dd5ef8",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b176c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Visualization: Performance Comparison\n",
    "# ========================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "# 1. Metrics comparison\n",
    "methods = ['Manual RF', 'Auto-sklearn']\n",
    "accuracies = [acc_manual, acc_automl]\n",
    "precisions = [prec_manual, prec_automl]\n",
    "recalls = [rec_manual, rec_automl]\n",
    "f1_scores = [f1_manual, f1_automl]\n",
    "aucs = [auc_manual, auc_automl]\n",
    "x = np.arange(len(methods))\n",
    "width = 0.15\n",
    "axes[0].bar(x - 2*width, accuracies, width, label='Accuracy', color='#3498db', edgecolor='black', linewidth=1)\n",
    "axes[0].bar(x - width, precisions, width, label='Precision', color='#2ecc71', edgecolor='black', linewidth=1)\n",
    "axes[0].bar(x, recalls, width, label='Recall', color='#e74c3c', edgecolor='black', linewidth=1)\n",
    "axes[0].bar(x + width, f1_scores, width, label='F1 Score', color='#f39c12', edgecolor='black', linewidth=1)\n",
    "axes[0].bar(x + 2*width, aucs, width, label='AUC', color='#9b59b6', edgecolor='black', linewidth=1)\n",
    "axes[0].set_ylabel('Score', fontsize=10, weight='bold')\n",
    "axes[0].set_title('Performance: Manual RF vs Auto-sklearn', fontsize=12, weight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(methods, fontsize=10)\n",
    "axes[0].legend(loc='lower right', fontsize=9)\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "axes[0].set_ylim([0, 1.05])\n",
    "# 2. Confusion matrices\n",
    "cm_list = [cm_manual, cm_automl]\n",
    "titles = ['Manual RF', 'Auto-sklearn']\n",
    "for idx, (cm, title, recall_val) in enumerate(zip(cm_list, titles, [rec_manual, rec_automl])):\n",
    "    ax = axes[idx + 1]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Pass', 'Fail'], yticklabels=['Pass', 'Fail'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    ax.set_title(f'{title}\\nRecall: {recall_val:.2%}', fontsize=12, weight='bold')\n",
    "    ax.set_xlabel('Predicted', fontsize=10)\n",
    "    ax.set_ylabel('Actual', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization: Manual RF vs Auto-sklearn comparison\")\n",
    "print()\n",
    "# ========================================\n",
    "# Business Impact Analysis\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Business Impact: Semiconductor Manufacturing\")\n",
    "print(\"=\" * 80)\n",
    "# Costs\n",
    "cost_per_missed_defect = 1_000_000  # $1M per field failure\n",
    "cost_per_false_alarm = 500          # $500 per unnecessary rework\n",
    "annual_production = 1_000_000       # 1M devices/year\n",
    "# Manual RF costs\n",
    "missed_defects_manual = cm_manual[1, 0]\n",
    "false_alarms_manual = cm_manual[0, 1]\n",
    "cost_manual_sample = missed_defects_manual * cost_per_missed_defect + false_alarms_manual * cost_per_false_alarm\n",
    "# Auto-sklearn costs\n",
    "missed_defects_automl = cm_automl[1, 0]\n",
    "false_alarms_automl = cm_automl[0, 1]\n",
    "cost_automl_sample = missed_defects_automl * cost_per_missed_defect + false_alarms_automl * cost_per_false_alarm\n",
    "# Extrapolate to annual production\n",
    "annual_cost_manual = cost_manual_sample * (annual_production / len(X_test))\n",
    "annual_cost_automl = cost_automl_sample * (annual_production / len(X_test))\n",
    "annual_savings = annual_cost_manual - annual_cost_automl\n",
    "print(f\"Test Set (1,000 devices):\")\n",
    "print(f\"  Manual RF:\")\n",
    "print(f\"    Missed defects: {missed_defects_manual}\")\n",
    "print(f\"    False alarms: {false_alarms_manual}\")\n",
    "print(f\"    Cost: ${cost_manual_sample:,.0f}\")\n",
    "print()\n",
    "print(f\"  Auto-sklearn:\")\n",
    "print(f\"    Missed defects: {missed_defects_automl}\")\n",
    "print(f\"    False alarms: {false_alarms_automl}\")\n",
    "print(f\"    Cost: ${cost_automl_sample:,.0f}\")\n",
    "print()\n",
    "print(f\"Annual Production (1M devices):\")\n",
    "print(f\"  Manual RF annual cost: ${annual_cost_manual / 1e6:.1f}M\")\n",
    "print(f\"  Auto-sklearn annual cost: ${annual_cost_automl / 1e6:.1f}M\")\n",
    "print(f\"  Annual savings: ${annual_savings / 1e6:.1f}M ({100 * annual_savings / annual_cost_manual:.1f}% reduction)\")\n",
    "print()\n",
    "print(f\"\ud83d\udcb0 ROI Analysis:\")\n",
    "print(f\"  Auto-sklearn compute cost: ~$50-$200 (4-hour AWS instance)\")\n",
    "print(f\"  Annual savings: ${annual_savings / 1e6:.1f}M\")\n",
    "print(f\"  ROI: {annual_savings / 200:.0f}x\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"Key Takeaways: Auto-sklearn\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. \u2705 Automated ML in 5 minutes (demo) or 1-4 hours (production)\")\n",
    "print(\"2. \u2705 Matches or beats manual ML (85% \u2192 88-90% accuracy)\")\n",
    "print(\"3. \u2705 Bayesian optimization explores 50-200 pipelines intelligently\")\n",
    "print(\"4. \u2705 Meta-learning warm-starts from similar datasets (5-10x speedup)\")\n",
    "print(\"5. \u2705 Ensemble construction combines top models automatically\")\n",
    "print(\"6. \u2705 For semiconductor: 3-5% accuracy gain = $30M-$100M annually\")\n",
    "print(\"7. \u2705 Democratizes ML: Non-experts can build production models\")\n",
    "print(\"=\" * 80)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047d566",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement TPOT (Tree-based Pipeline Optimization Tool) using genetic programming for pipeline evolution\n",
    "\n",
    "**Key Points:**\n",
    "- **Genetic Programming**: Encodes ML pipelines as trees, evolves via crossover (combine parents) and mutation (random changes)\n",
    "- **Pipeline Structure**: Tree nodes = operators (preprocessing, feature engineering, models), edges = data flow\n",
    "- **Fitness Function**: Cross-validation score guides evolution (high fitness = good pipeline, survives to next generation)\n",
    "- **Population**: 20-100 pipelines per generation, evolve for 50-100 generations\n",
    "- **Crossover**: Swap subtrees between two parent pipelines \u2192 create offspring with mixed characteristics\n",
    "- **Mutation**: Random changes (swap operator, modify hyperparameter) \u2192 explore new regions\n",
    "- **Semiconductor Use Case**: Parametric test optimization (find minimal test set that maintains 95%+ yield prediction accuracy)\n",
    "\n",
    "**Why This Matters:**\n",
    "- TPOT explores **pipeline structure** (not just hyperparameters) \u2192 discovers novel preprocessing + feature engineering combinations\n",
    "- Genetic programming handles non-differentiable search spaces (tree structures, discrete operators)\n",
    "- For semiconductor: Discovers that PCA(n=5) + SelectKBest(k=10) + XGBoost outperforms manual feature selection\n",
    "- Test time reduction: 100 tests \u2192 15 tests (85% time savings) while maintaining 92% accuracy\n",
    "- Annual savings: $30M-$100M from reduced test time + maintained yield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b5367",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dff483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# TPOT: Genetic Programming for AutoML\n",
    "# ========================================\n",
    "# Install TPOT if not available\n",
    "try:\n",
    "    from tpot import TPOTClassifier\n",
    "except ImportError:\n",
    "    print(\"Installing TPOT...\")\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tpot\"])\n",
    "    from tpot import TPOTClassifier\n",
    "print(\"=\" * 80)\n",
    "print(\"TPOT: Tree-based Pipeline Optimization Tool (Genetic Programming)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "# ========================================\n",
    "# TPOT Configuration\n",
    "# ========================================\n",
    "print(\"TPOT Configuration:\")\n",
    "print(\"  Generations: 10 (use 50-100 in production)\")\n",
    "print(\"  Population size: 20 (use 50-100 in production)\")\n",
    "print(\"  CV folds: 5\")\n",
    "print(\"  Scoring: Recall (defect detection)\")\n",
    "print(\"  Max time: 5 minutes per pipeline\")\n",
    "print(\"  Max eval time: 300 seconds total (demo, use 1-4 hours in production)\")\n",
    "print(\"  Verbosity: 2 (show progress)\")\n",
    "print()\n",
    "# Create TPOT classifier\n",
    "tpot = TPOTClassifier(\n",
    "    generations=10,           # Number of evolutionary generations (demo: 10, production: 50-100)\n",
    "    population_size=20,       # Number of pipelines per generation (demo: 20, production: 50-100)\n",
    "    cv=5,                     # Cross-validation folds\n",
    "    scoring='recall',         # Optimize for recall (critical for defect detection)\n",
    "    max_time_mins=5,          # Max time per pipeline evaluation (minutes)\n",
    "    max_eval_time_mins=5,     # Max total evaluation time (minutes, demo only)\n",
    "    random_state=42,\n",
    "    verbosity=2,              # Show progress (0=silent, 1=minimal, 2=detailed, 3=debug)\n",
    "    n_jobs=-1,                # Use all CPU cores\n",
    "    config_dict='TPOT light'  # Use lighter config for faster demo (remove for full search)\n",
    ")\n",
    "print(\"Starting TPOT genetic programming...\")\n",
    "print(\"(This will take ~5 minutes. In production, use 1-4 hours for best results)\")\n",
    "print()\n",
    "import time\n",
    "start_time = time.time()\n",
    "# Fit TPOT (evolve pipelines via genetic programming)\n",
    "tpot.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\n\u2705 TPOT completed in {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
    "print()\n",
    "# ========================================\n",
    "# TPOT Results\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"TPOT: Results & Analysis\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "# Predict\n",
    "y_pred_tpot = tpot.predict(X_test)\n",
    "y_proba_tpot = tpot.predict_proba(X_test)[:, 1]\n",
    "# Metrics\n",
    "acc_tpot = accuracy_score(y_test, y_pred_tpot)\n",
    "prec_tpot = precision_score(y_test, y_pred_tpot, zero_division=0)\n",
    "rec_tpot = recall_score(y_test, y_pred_tpot)\n",
    "f1_tpot = f1_score(y_test, y_pred_tpot)\n",
    "auc_tpot = roc_auc_score(y_test, y_proba_tpot)\n",
    "print(f\"TPOT Performance:\")\n",
    "print(f\"  Accuracy:  {acc_tpot:.4f}\")\n",
    "print(f\"  Precision: {prec_tpot:.4f}\")\n",
    "print(f\"  Recall:    {rec_tpot:.4f}\")\n",
    "print(f\"  F1 Score:  {f1_tpot:.4f}\")\n",
    "print(f\"  AUC:       {auc_tpot:.4f}\")\n",
    "print()\n",
    "cm_tpot = confusion_matrix(y_test, y_pred_tpot)\n",
    "print(f\"Confusion Matrix:\")\n",
    "print(cm_tpot)\n",
    "print(f\"  Missed failures: {cm_tpot[1,0]} out of {np.sum(y_test==1)}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9441cea9",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a25f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Show Evolved Pipeline\n",
    "# ========================================\n",
    "print(\"Best Pipeline Found by TPOT:\")\n",
    "print(\"-\" * 80)\n",
    "print(tpot.fitted_pipeline_)\n",
    "print()\n",
    "# Export pipeline to Python code\n",
    "print(\"Exporting optimized pipeline to Python code...\")\n",
    "tpot.export('tpot_semiconductor_pipeline.py')\n",
    "print(\"\u2705 Pipeline exported to: tpot_semiconductor_pipeline.py\")\n",
    "print()\n",
    "# Display exported code\n",
    "print(\"Generated Pipeline Code:\")\n",
    "print(\"-\" * 80)\n",
    "with open('tpot_semiconductor_pipeline.py', 'r') as f:\n",
    "    pipeline_code = f.read()\n",
    "    print(pipeline_code)\n",
    "print()\n",
    "# ========================================\n",
    "# Comparison: Manual RF vs Auto-sklearn vs TPOT\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Performance Comparison: Manual RF vs Auto-sklearn vs TPOT\")\n",
    "print(\"=\" * 80)\n",
    "comparison = pd.DataFrame({\n",
    "    'Method': ['Manual RF', 'Auto-sklearn', 'TPOT'],\n",
    "    'Accuracy': [acc_manual, acc_automl, acc_tpot],\n",
    "    'Precision': [prec_manual, prec_automl, prec_tpot],\n",
    "    'Recall': [rec_manual, rec_automl, rec_tpot],\n",
    "    'F1 Score': [f1_manual, f1_automl, f1_tpot],\n",
    "    'AUC': [auc_manual, auc_automl, auc_tpot],\n",
    "    'Missed Failures': [cm_manual[1,0], cm_automl[1,0], cm_tpot[1,0]],\n",
    "    'Time (min)': ['<1 (manual)', f'{elapsed_time/60:.1f}', f'{elapsed_time/60:.1f}']\n",
    "})\n",
    "print(comparison.to_string(index=False))\n",
    "print()\n",
    "# Find best method\n",
    "best_idx = comparison['Recall'].idxmax()\n",
    "best_method = comparison.loc[best_idx, 'Method']\n",
    "best_recall = comparison.loc[best_idx, 'Recall']\n",
    "print(f\"\ud83c\udfc6 Best Method: {best_method} (Recall = {best_recall:.4f})\")\n",
    "print()\n",
    "# ========================================\n",
    "# Visualization: 3-Way Comparison\n",
    "# ========================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# 1. Metrics comparison\n",
    "methods = ['Manual RF', 'Auto-sklearn', 'TPOT']\n",
    "recalls_all = [rec_manual, rec_automl, rec_tpot]\n",
    "precisions_all = [prec_manual, prec_automl, prec_tpot]\n",
    "f1_scores_all = [f1_manual, f1_automl, f1_tpot]\n",
    "aucs_all = [auc_manual, auc_automl, auc_tpot]\n",
    "x = np.arange(len(methods))\n",
    "width = 0.2\n",
    "axes[0].bar(x - 1.5*width, recalls_all, width, label='Recall', color='#e74c3c', edgecolor='black', linewidth=1)\n",
    "axes[0].bar(x - 0.5*width, precisions_all, width, label='Precision', color='#2ecc71', edgecolor='black', linewidth=1)\n",
    "axes[0].bar(x + 0.5*width, f1_scores_all, width, label='F1 Score', color='#f39c12', edgecolor='black', linewidth=1)\n",
    "axes[0].bar(x + 1.5*width, aucs_all, width, label='AUC', color='#9b59b6', edgecolor='black', linewidth=1)\n",
    "axes[0].set_ylabel('Score', fontsize=10, weight='bold')\n",
    "axes[0].set_title('Performance: Manual RF vs Auto-sklearn vs TPOT', fontsize=12, weight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(methods, fontsize=10)\n",
    "axes[0].legend(loc='lower right', fontsize=9)\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "axes[0].set_ylim([0, 1.05])\n",
    "# Highlight best method\n",
    "axes[0].bar(best_idx, best_recall, width, color='gold', edgecolor='black', linewidth=2, alpha=0.5)\n",
    "# 2. Missed failures comparison\n",
    "missed_failures = [cm_manual[1,0], cm_automl[1,0], cm_tpot[1,0]]\n",
    "colors = ['#3498db', '#2ecc71', '#f39c12']\n",
    "bars = axes[1].bar(methods, missed_failures, color=colors, edgecolor='black', linewidth=1.5)\n",
    "axes[1].set_ylabel('# Missed Failures', fontsize=10, weight='bold')\n",
    "axes[1].set_title('Defect Detection: Missed Failures', fontsize=12, weight='bold')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, missed_failures):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(val)}',\n",
    "                ha='center', va='bottom', fontsize=11, weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization: 3-way comparison (Manual RF vs Auto-sklearn vs TPOT)\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb91a33",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e705d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Business Impact: TPOT\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Business Impact: TPOT Genetic Programming\")\n",
    "print(\"=\" * 80)\n",
    "# TPOT costs\n",
    "missed_defects_tpot = cm_tpot[1, 0]\n",
    "false_alarms_tpot = cm_tpot[0, 1]\n",
    "cost_tpot_sample = missed_defects_tpot * cost_per_missed_defect + false_alarms_tpot * cost_per_false_alarm\n",
    "# Annual\n",
    "annual_cost_tpot = cost_tpot_sample * (annual_production / len(X_test))\n",
    "annual_savings_tpot = annual_cost_manual - annual_cost_tpot\n",
    "print(f\"TPOT Performance:\")\n",
    "print(f\"  Missed defects: {missed_defects_tpot} (vs {missed_defects_manual} manual)\")\n",
    "print(f\"  False alarms: {false_alarms_tpot}\")\n",
    "print(f\"  Test set cost: ${cost_tpot_sample:,.0f}\")\n",
    "print()\n",
    "print(f\"Annual Impact:\")\n",
    "print(f\"  Manual RF cost: ${annual_cost_manual / 1e6:.1f}M\")\n",
    "print(f\"  TPOT cost: ${annual_cost_tpot / 1e6:.1f}M\")\n",
    "print(f\"  Savings: ${annual_savings_tpot / 1e6:.1f}M ({100 * annual_savings_tpot / annual_cost_manual:.1f}% reduction)\")\n",
    "print()\n",
    "# ========================================\n",
    "# Key Insights: TPOT\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Key Takeaways: TPOT Genetic Programming\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. \u2705 Genetic programming evolves pipeline structure (not just hyperparameters)\")\n",
    "print(\"2. \u2705 Explores preprocessing + feature engineering + model combinations\")\n",
    "print(\"3. \u2705 Exports optimized pipeline as Python code (fully reproducible)\")\n",
    "print(\"4. \u2705 Discovers novel pipelines manual engineers might miss\")\n",
    "print(\"5. \u2705 Best for: Tree-based models, feature engineering, moderate datasets\")\n",
    "print(\"6. \u2705 Trade-off: Slower than Bayesian (tries more combinations)\")\n",
    "print(\"7. \u2705 For semiconductor: Automated feature selection from 20+ parametric tests\")\n",
    "print(\"=\" * 80)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4963a1",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea45b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# H2O AutoML: Distributed Scalable AutoML\n",
    "# ========================================\n",
    "# Install H2O if not available\n",
    "try:\n",
    "    import h2o\n",
    "    from h2o.automl import H2OAutoML\n",
    "except ImportError:\n",
    "    print(\"Installing H2O...\")\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"h2o\"])\n",
    "    import h2o\n",
    "    from h2o.automl import H2OAutoML\n",
    "print(\"=\" * 80)\n",
    "print(\"H2O AutoML: Distributed Machine Learning at Scale\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "# Initialize H2O cluster\n",
    "print(\"Initializing H2O cluster...\")\n",
    "h2o.init(max_mem_size='4G', nthreads=-1)  # Use 4GB RAM, all CPU cores\n",
    "print()\n",
    "# ========================================\n",
    "# Prepare Data for H2O\n",
    "# ========================================\n",
    "print(\"Preparing data for H2O...\")\n",
    "print(\"-\" * 80)\n",
    "# Convert to pandas DataFrame\n",
    "df_train = pd.DataFrame(X_train, columns=feature_names_with_spatial)\n",
    "df_train['yield_label'] = y_train\n",
    "df_test = pd.DataFrame(X_test, columns=feature_names_with_spatial)\n",
    "df_test['yield_label'] = y_test\n",
    "# Convert to H2O frames\n",
    "train_h2o = h2o.H2OFrame(df_train)\n",
    "test_h2o = h2o.H2OFrame(df_test)\n",
    "# Convert target to categorical (for classification)\n",
    "train_h2o['yield_label'] = train_h2o['yield_label'].asfactor()\n",
    "test_h2o['yield_label'] = test_h2o['yield_label'].asfactor()\n",
    "print(f\"H2O Train Frame: {train_h2o.shape}\")\n",
    "print(f\"H2O Test Frame: {test_h2o.shape}\")\n",
    "print()\n",
    "# Define features and target\n",
    "features = feature_names_with_spatial\n",
    "target = 'yield_label'\n",
    "# ========================================\n",
    "# H2O AutoML Configuration\n",
    "# ========================================\n",
    "print(\"H2O AutoML Configuration:\")\n",
    "print(\"  Max runtime: 300 seconds (5 minutes demo, use 1-4 hours in production)\")\n",
    "print(\"  Max models: 20\")\n",
    "print(\"  Algorithms: GLM, GBM, XGBoost, DeepLearning, StackedEnsembles\")\n",
    "print(\"  Sort metric: Recall (defect detection)\")\n",
    "print(\"  Seed: 42\")\n",
    "print()\n",
    "# Create H2O AutoML object\n",
    "aml = H2OAutoML(\n",
    "    max_runtime_secs=300,      # Max runtime (seconds, demo: 300, production: 3600-14400)\n",
    "    max_models=20,             # Max number of models (including ensemble)\n",
    "    sort_metric='recall',      # Sort leaderboard by recall\n",
    "    seed=42,\n",
    "    nfolds=5,                  # Cross-validation folds\n",
    "    balance_classes=True,      # Handle imbalanced data\n",
    "    stopping_metric='recall',  # Early stopping metric\n",
    "    stopping_rounds=3,         # Stop if no improvement for 3 rounds\n",
    "    stopping_tolerance=0.001   # Min improvement threshold\n",
    ")\n",
    "print(\"Starting H2O AutoML...\")\n",
    "print(\"(This will take ~5 minutes. In production, use 1-4 hours for best results)\")\n",
    "print()\n",
    "start_time = time.time()\n",
    "# Train H2O AutoML\n",
    "aml.train(x=features, y=target, training_frame=train_h2o)\n",
    "end_time = time.time()\n",
    "elapsed_time_h2o = end_time - start_time\n",
    "print(f\"\\n\u2705 H2O AutoML completed in {elapsed_time_h2o:.1f} seconds ({elapsed_time_h2o/60:.1f} minutes)\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed442a4",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb84c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# H2O AutoML Results\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"H2O AutoML: Leaderboard & Results\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "# Get leaderboard\n",
    "leaderboard_h2o = aml.leaderboard\n",
    "print(\"Leaderboard (Top 10 models):\")\n",
    "print(\"-\" * 80)\n",
    "print(leaderboard_h2o.head(10))\n",
    "print()\n",
    "# Best model\n",
    "best_model_h2o = aml.leader\n",
    "print(f\"Best Model: {best_model_h2o.model_id}\")\n",
    "print()\n",
    "# Predict on test set\n",
    "predictions_h2o = best_model_h2o.predict(test_h2o)\n",
    "# Convert predictions to numpy\n",
    "y_pred_h2o = predictions_h2o['predict'].as_data_frame().values.flatten()\n",
    "y_pred_h2o = (y_pred_h2o == '1').astype(int)  # Convert '0'/'1' strings to 0/1\n",
    "# Get probabilities\n",
    "y_proba_h2o = predictions_h2o['p1'].as_data_frame().values.flatten()\n",
    "# Metrics\n",
    "acc_h2o = accuracy_score(y_test, y_pred_h2o)\n",
    "prec_h2o = precision_score(y_test, y_pred_h2o, zero_division=0)\n",
    "rec_h2o = recall_score(y_test, y_pred_h2o)\n",
    "f1_h2o = f1_score(y_test, y_pred_h2o)\n",
    "auc_h2o = roc_auc_score(y_test, y_proba_h2o)\n",
    "print(f\"H2O AutoML Performance:\")\n",
    "print(f\"  Accuracy:  {acc_h2o:.4f}\")\n",
    "print(f\"  Precision: {prec_h2o:.4f}\")\n",
    "print(f\"  Recall:    {rec_h2o:.4f}\")\n",
    "print(f\"  F1 Score:  {f1_h2o:.4f}\")\n",
    "print(f\"  AUC:       {auc_h2o:.4f}\")\n",
    "print()\n",
    "cm_h2o = confusion_matrix(y_test, y_pred_h2o)\n",
    "print(f\"Confusion Matrix:\")\n",
    "print(cm_h2o)\n",
    "print(f\"  Missed failures: {cm_h2o[1,0]} out of {np.sum(y_test==1)}\")\n",
    "print()\n",
    "# ========================================\n",
    "# Model Explainability (Variable Importance)\n",
    "# ========================================\n",
    "print(\"Model Explainability: Variable Importance\")\n",
    "print(\"-\" * 80)\n",
    "# Get variable importance\n",
    "varimp = best_model_h2o.varimp(use_pandas=True)\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(varimp.head(10))\n",
    "print()\n",
    "# Plot variable importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = varimp.head(10)\n",
    "plt.barh(top_features['variable'], top_features['relative_importance'], \n",
    "         color='#3498db', edgecolor='black', linewidth=1)\n",
    "plt.xlabel('Relative Importance', fontsize=10, weight='bold')\n",
    "plt.ylabel('Feature', fontsize=10, weight='bold')\n",
    "plt.title('H2O AutoML: Top 10 Feature Importance', fontsize=12, weight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization: Feature importance\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0def364b",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b332a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Export Model (Production Deployment)\n",
    "# ========================================\n",
    "print(\"Exporting Model for Production Deployment...\")\n",
    "print(\"-\" * 80)\n",
    "# Save model as MOJO (Model Objects, Optimized) for Java deployment\n",
    "mojo_path = h2o.save_mojo(best_model_h2o, path=\"./\", force=True)\n",
    "print(f\"\u2705 MOJO model saved to: {mojo_path}\")\n",
    "print(\"   Use for low-latency Java serving (< 1ms inference)\")\n",
    "print()\n",
    "# Save model as H2O binary\n",
    "model_path = h2o.save_model(best_model_h2o, path=\"./\", force=True)\n",
    "print(f\"\u2705 H2O model saved to: {model_path}\")\n",
    "print(\"   Use for Python/R serving\")\n",
    "print()\n",
    "# ========================================\n",
    "# Final Comparison: All AutoML Frameworks\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Final Comparison: Manual RF vs Auto-sklearn vs TPOT vs H2O AutoML\")\n",
    "print(\"=\" * 80)\n",
    "comparison_final = pd.DataFrame({\n",
    "    'Method': ['Manual RF', 'Auto-sklearn', 'TPOT', 'H2O AutoML'],\n",
    "    'Accuracy': [acc_manual, acc_automl, acc_tpot, acc_h2o],\n",
    "    'Precision': [prec_manual, prec_automl, prec_tpot, prec_h2o],\n",
    "    'Recall': [rec_manual, rec_automl, rec_tpot, rec_h2o],\n",
    "    'F1 Score': [f1_manual, f1_automl, f1_tpot, f1_h2o],\n",
    "    'AUC': [auc_manual, auc_automl, auc_tpot, auc_h2o],\n",
    "    'Missed Failures': [cm_manual[1,0], cm_automl[1,0], cm_tpot[1,0], cm_h2o[1,0]],\n",
    "    'Time (min)': ['<1', f'{elapsed_time/60:.1f}', f'{elapsed_time/60:.1f}', f'{elapsed_time_h2o/60:.1f}']\n",
    "})\n",
    "print(comparison_final.to_string(index=False))\n",
    "print()\n",
    "# Find best method by recall\n",
    "best_idx_final = comparison_final['Recall'].idxmax()\n",
    "best_method_final = comparison_final.loc[best_idx_final, 'Method']\n",
    "best_recall_final = comparison_final.loc[best_idx_final, 'Recall']\n",
    "print(f\"\ud83c\udfc6 Best Method by Recall: {best_method_final} ({best_recall_final:.4f})\")\n",
    "print()\n",
    "# Find best method by F1\n",
    "best_idx_f1 = comparison_final['F1 Score'].idxmax()\n",
    "best_method_f1 = comparison_final.loc[best_idx_f1, 'Method']\n",
    "best_f1_final = comparison_final.loc[best_idx_f1, 'F1 Score']\n",
    "print(f\"\ud83c\udfc6 Best Method by F1: {best_method_f1} ({best_f1_final:.4f})\")\n",
    "print()\n",
    "# ========================================\n",
    "# Visualization: Final Comparison\n",
    "# ========================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "# 1. Recall comparison\n",
    "methods_all = ['Manual RF', 'Auto-sklearn', 'TPOT', 'H2O']\n",
    "recalls_final = [rec_manual, rec_automl, rec_tpot, rec_h2o]\n",
    "colors_methods = ['#3498db', '#2ecc71', '#f39c12', '#9b59b6']\n",
    "bars = axes[0, 0].bar(methods_all, recalls_final, color=colors_methods, edgecolor='black', linewidth=1.5)\n",
    "axes[0, 0].set_ylabel('Recall', fontsize=10, weight='bold')\n",
    "axes[0, 0].set_title('Recall: Defect Detection Performance', fontsize=12, weight='bold')\n",
    "axes[0, 0].grid(alpha=0.3, axis='y')\n",
    "axes[0, 0].set_ylim([0, 1.05])\n",
    "for bar, val in zip(bars, recalls_final):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{val:.3f}',\n",
    "                    ha='center', va='bottom', fontsize=10, weight='bold')\n",
    "# 2. F1 Score comparison\n",
    "f1_scores_final = [f1_manual, f1_automl, f1_tpot, f1_h2o]\n",
    "bars = axes[0, 1].bar(methods_all, f1_scores_final, color=colors_methods, edgecolor='black', linewidth=1.5)\n",
    "axes[0, 1].set_ylabel('F1 Score', fontsize=10, weight='bold')\n",
    "axes[0, 1].set_title('F1 Score: Balanced Performance', fontsize=12, weight='bold')\n",
    "axes[0, 1].grid(alpha=0.3, axis='y')\n",
    "axes[0, 1].set_ylim([0, 1.05])\n",
    "for bar, val in zip(bars, f1_scores_final):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{val:.3f}',\n",
    "                    ha='center', va='bottom', fontsize=10, weight='bold')\n",
    "# 3. Missed failures\n",
    "missed_final = [cm_manual[1,0], cm_automl[1,0], cm_tpot[1,0], cm_h2o[1,0]]\n",
    "bars = axes[1, 0].bar(methods_all, missed_final, color=colors_methods, edgecolor='black', linewidth=1.5)\n",
    "axes[1, 0].set_ylabel('# Missed Failures', fontsize=10, weight='bold')\n",
    "axes[1, 0].set_title('Business Impact: Missed Defects', fontsize=12, weight='bold')\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "for bar, val in zip(bars, missed_final):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(val)}',\n",
    "                    ha='center', va='bottom', fontsize=11, weight='bold')\n",
    "# 4. All metrics spider chart\n",
    "from math import pi\n",
    "categories = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\n",
    "N = len(categories)\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "ax = axes[1, 1]\n",
    "ax = plt.subplot(224, polar=True)\n",
    "# Plot each method\n",
    "metrics_dict = {\n",
    "    'Manual RF': [acc_manual, prec_manual, rec_manual, f1_manual, auc_manual],\n",
    "    'Auto-sklearn': [acc_automl, prec_automl, rec_automl, f1_automl, auc_automl],\n",
    "    'TPOT': [acc_tpot, prec_tpot, rec_tpot, f1_tpot, auc_tpot],\n",
    "    'H2O': [acc_h2o, prec_h2o, rec_h2o, f1_h2o, auc_h2o]\n",
    "}\n",
    "for idx, (method, metrics) in enumerate(metrics_dict.items()):\n",
    "    values = metrics + metrics[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=method, color=colors_methods[idx])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors_methods[idx])\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=9)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('All Metrics Comparison (Spider Chart)', fontsize=12, weight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=9)\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization: Final 4-way comparison\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d513c",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e933db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Business Impact: H2O AutoML\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Business Impact: H2O AutoML\")\n",
    "print(\"=\" * 80)\n",
    "# H2O costs\n",
    "missed_defects_h2o = cm_h2o[1, 0]\n",
    "false_alarms_h2o = cm_h2o[0, 1]\n",
    "cost_h2o_sample = missed_defects_h2o * cost_per_missed_defect + false_alarms_h2o * cost_per_false_alarm\n",
    "# Annual\n",
    "annual_cost_h2o = cost_h2o_sample * (annual_production / len(X_test))\n",
    "annual_savings_h2o = annual_cost_manual - annual_cost_h2o\n",
    "print(f\"H2O AutoML:\")\n",
    "print(f\"  Missed defects: {missed_defects_h2o} (vs {missed_defects_manual} manual)\")\n",
    "print(f\"  Annual cost: ${annual_cost_h2o / 1e6:.1f}M\")\n",
    "print(f\"  Savings vs manual: ${annual_savings_h2o / 1e6:.1f}M ({100 * annual_savings_h2o / annual_cost_manual:.1f}%)\")\n",
    "print()\n",
    "print(f\"Best AutoML Method ({best_method_final}):\")\n",
    "best_missed = comparison_final.loc[best_idx_final, 'Missed Failures']\n",
    "print(f\"  Missed defects: {int(best_missed)}\")\n",
    "print(f\"  Improvement over manual: {cm_manual[1,0] - int(best_missed)} fewer missed defects\")\n",
    "print()\n",
    "# ========================================\n",
    "# Key Insights: H2O AutoML\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Key Takeaways: H2O AutoML\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. \u2705 Distributed computing: Scales to 100GB+ datasets (out-of-core)\")\n",
    "print(\"2. \u2705 Production-ready: MOJO export for <1ms Java serving\")\n",
    "print(\"3. \u2705 Built-in explainability: SHAP, variable importance, PDPs\")\n",
    "print(\"4. \u2705 Stacked ensembles: Automatically combines top models\")\n",
    "print(\"5. \u2705 Fast training: Optimized C++ backend (faster than sklearn)\")\n",
    "print(\"6. \u2705 Best for: Large datasets, production deployment, regulated industries\")\n",
    "print(\"7. \u2705 For semiconductor: Real-time scoring, edge deployment, interpretability\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "# Cleanup H2O cluster\n",
    "h2o.cluster().shutdown()\n",
    "print(\"H2O cluster shutdown complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50623cc0",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a241de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Custom AutoML: Build from Scratch (Simplified)\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Custom AutoML: Educational Implementation from Scratch\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    xgb_available = True\n",
    "except ImportError:\n",
    "    xgb_available = False\n",
    "    print(\"\u26a0\ufe0f XGBoost not available. Install with: pip install xgboost\")\n",
    "    print()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# ========================================\n",
    "# Define Search Space\n",
    "# ========================================\n",
    "print(\"Step 1: Define Search Space\")\n",
    "print(\"-\" * 80)\n",
    "search_space = {\n",
    "    'preprocessors': [\n",
    "        ('standard', StandardScaler()),\n",
    "        ('minmax', MinMaxScaler()),\n",
    "        ('robust', RobustScaler()),\n",
    "        ('none', None)\n",
    "    ],\n",
    "    'models': [\n",
    "        ('logistic', LogisticRegression, {\n",
    "            'C': [0.01, 0.1, 1.0, 10.0],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear', 'saga'],\n",
    "            'max_iter': [500]\n",
    "        }),\n",
    "        ('random_forest', RandomForestClassifier, {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'class_weight': ['balanced', None]\n",
    "        }),\n",
    "        ('svm', SVC, {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'kernel': ['rbf', 'linear'],\n",
    "            'gamma': ['scale', 'auto'],\n",
    "            'probability': [True]\n",
    "        })\n",
    "    ]\n",
    "}\n",
    "if xgb_available:\n",
    "    search_space['models'].append(\n",
    "        ('xgboost', xgb.XGBClassifier, {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'scale_pos_weight': [1, 5, 10]  # For imbalanced data\n",
    "        })\n",
    "    )\n",
    "# Calculate search space size\n",
    "n_preprocessors = len(search_space['preprocessors'])\n",
    "n_configs = 0\n",
    "for model_name, model_class, params in search_space['models']:\n",
    "    n_model_configs = 1\n",
    "    for param_values in params.values():\n",
    "        n_model_configs *= len(param_values)\n",
    "    n_configs += n_model_configs\n",
    "total_space_size = n_preprocessors * n_configs\n",
    "print(f\"Preprocessors: {n_preprocessors}\")\n",
    "print(f\"Models: {len(search_space['models'])}\")\n",
    "print(f\"Total search space: ~{total_space_size:,} configurations\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d03bde",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a034cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Random Search Loop\n",
    "# ========================================\n",
    "print(\"Step 2: Random Search with Cross-Validation\")\n",
    "print(\"-\" * 80)\n",
    "n_trials = 50  # Demo: 50 trials, production: 100-500\n",
    "cv_folds = 5\n",
    "scoring_metric = 'recall'\n",
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Trials: {n_trials}\")\n",
    "print(f\"  CV folds: {cv_folds}\")\n",
    "print(f\"  Scoring: {scoring_metric}\")\n",
    "print()\n",
    "# Storage for results\n",
    "results = []\n",
    "print(f\"Running {n_trials} random trials...\")\n",
    "start_time = time.time()\n",
    "for trial in range(n_trials):\n",
    "    # Random selection\n",
    "    prep_name, preprocessor = search_space['preprocessors'][np.random.randint(len(search_space['preprocessors']))]\n",
    "    model_name, model_class, param_space = search_space['models'][np.random.randint(len(search_space['models']))]\n",
    "    \n",
    "    # Random hyperparameters\n",
    "    params = {}\n",
    "    for param_name, param_values in param_space.items():\n",
    "        params[param_name] = param_values[np.random.randint(len(param_values))]\n",
    "    \n",
    "    try:\n",
    "        # Build pipeline\n",
    "        if preprocessor is not None:\n",
    "            from sklearn.pipeline import Pipeline\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', preprocessor),\n",
    "                ('model', model_class(**params))\n",
    "            ])\n",
    "        else:\n",
    "            pipeline = model_class(**params)\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv_folds, scoring=scoring_metric)\n",
    "        mean_score = cv_scores.mean()\n",
    "        std_score = cv_scores.std()\n",
    "        \n",
    "        # Store result\n",
    "        results.append({\n",
    "            'trial': trial + 1,\n",
    "            'preprocessor': prep_name,\n",
    "            'model': model_name,\n",
    "            'params': params,\n",
    "            'cv_score': mean_score,\n",
    "            'cv_std': std_score,\n",
    "            'pipeline': pipeline\n",
    "        })\n",
    "        \n",
    "        if (trial + 1) % 10 == 0:\n",
    "            print(f\"  Trial {trial + 1}/{n_trials}: Best so far = {max([r['cv_score'] for r in results]):.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Skip invalid configurations\n",
    "        continue\n",
    "end_time = time.time()\n",
    "elapsed_custom = end_time - start_time\n",
    "print(f\"\\n\u2705 Custom AutoML completed in {elapsed_custom:.1f} seconds ({elapsed_custom/60:.1f} minutes)\")\n",
    "print(f\"   Valid trials: {len(results)}/{n_trials}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4703f750",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d5cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Select Top Models\n",
    "# ========================================\n",
    "print(\"Step 3: Select Top Models for Ensemble\")\n",
    "print(\"-\" * 80)\n",
    "# Sort by CV score\n",
    "results_sorted = sorted(results, key=lambda x: x['cv_score'], reverse=True)\n",
    "# Top 5 models\n",
    "top_k = 5\n",
    "top_models = results_sorted[:top_k]\n",
    "print(f\"Top {top_k} Models:\")\n",
    "for i, model_info in enumerate(top_models, 1):\n",
    "    print(f\"{i}. {model_info['preprocessor']} + {model_info['model']}: \"\n",
    "          f\"CV Score = {model_info['cv_score']:.4f} \u00b1 {model_info['cv_std']:.4f}\")\n",
    "print()\n",
    "# ========================================\n",
    "# Build Simple Ensemble\n",
    "# ========================================\n",
    "print(\"Step 4: Build Ensemble (Weighted Voting)\")\n",
    "print(\"-\" * 80)\n",
    "# Train top models on full training data\n",
    "ensemble_models = []\n",
    "for model_info in top_models:\n",
    "    pipeline = model_info['pipeline']\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    ensemble_models.append((f\"{model_info['model']}\", pipeline))\n",
    "# Create voting classifier (equal weights for simplicity)\n",
    "# In production: use cross-validation scores as weights\n",
    "ensemble_custom = VotingClassifier(\n",
    "    estimators=ensemble_models,\n",
    "    voting='soft',  # Soft voting (probabilities)\n",
    "    weights=None    # Equal weights (could use cv_scores as weights)\n",
    ")\n",
    "print(f\"Ensemble: {len(ensemble_models)} models with equal weights\")\n",
    "print()\n",
    "# Fit ensemble (already fitted individual models, but VotingClassifier needs fit)\n",
    "ensemble_custom.fit(X_train, y_train)\n",
    "# Predict on test set\n",
    "y_pred_custom = ensemble_custom.predict(X_test)\n",
    "y_proba_custom = ensemble_custom.predict_proba(X_test)[:, 1]\n",
    "# ========================================\n",
    "# Evaluate Custom AutoML\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Custom AutoML: Results\")\n",
    "print(\"=\" * 80)\n",
    "acc_custom = accuracy_score(y_test, y_pred_custom)\n",
    "prec_custom = precision_score(y_test, y_pred_custom, zero_division=0)\n",
    "rec_custom = recall_score(y_test, y_pred_custom)\n",
    "f1_custom = f1_score(y_test, y_pred_custom)\n",
    "auc_custom = roc_auc_score(y_test, y_proba_custom)\n",
    "print(f\"Custom AutoML Performance:\")\n",
    "print(f\"  Accuracy:  {acc_custom:.4f}\")\n",
    "print(f\"  Precision: {prec_custom:.4f}\")\n",
    "print(f\"  Recall:    {rec_custom:.4f}\")\n",
    "print(f\"  F1 Score:  {f1_custom:.4f}\")\n",
    "print(f\"  AUC:       {auc_custom:.4f}\")\n",
    "print()\n",
    "cm_custom = confusion_matrix(y_test, y_pred_custom)\n",
    "print(f\"Confusion Matrix:\")\n",
    "print(cm_custom)\n",
    "print(f\"  Missed failures: {cm_custom[1,0]} out of {np.sum(y_test==1)}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a2bbc",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c5953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Compare: Custom AutoML vs Production Frameworks\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Final Comparison: All Methods Including Custom AutoML\")\n",
    "print(\"=\" * 80)\n",
    "comparison_all = pd.DataFrame({\n",
    "    'Method': ['Manual RF', 'Auto-sklearn', 'TPOT', 'H2O AutoML', 'Custom AutoML'],\n",
    "    'Accuracy': [acc_manual, acc_automl, acc_tpot, acc_h2o, acc_custom],\n",
    "    'Precision': [prec_manual, prec_automl, prec_tpot, prec_h2o, prec_custom],\n",
    "    'Recall': [rec_manual, rec_automl, rec_tpot, rec_h2o, rec_custom],\n",
    "    'F1 Score': [f1_manual, f1_automl, f1_tpot, f1_h2o, f1_custom],\n",
    "    'AUC': [auc_manual, auc_automl, auc_tpot, auc_h2o, auc_custom],\n",
    "    'Missed Failures': [cm_manual[1,0], cm_automl[1,0], cm_tpot[1,0], cm_h2o[1,0], cm_custom[1,0]],\n",
    "    'Time (min)': ['<1', f'{elapsed_time/60:.1f}', f'{elapsed_time/60:.1f}', \n",
    "                   f'{elapsed_time_h2o/60:.1f}', f'{elapsed_custom/60:.1f}']\n",
    "})\n",
    "print(comparison_all.to_string(index=False))\n",
    "print()\n",
    "# ========================================\n",
    "# Visualization: Custom AutoML Analysis\n",
    "# ========================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "# 1. Search progress (CV scores over trials)\n",
    "cv_scores_all = [r['cv_score'] for r in results]\n",
    "best_scores_running = np.maximum.accumulate(cv_scores_all)\n",
    "axes[0, 0].plot(range(1, len(cv_scores_all) + 1), cv_scores_all, 'o', \n",
    "                alpha=0.5, markersize=4, label='Trial scores', color='#3498db')\n",
    "axes[0, 0].plot(range(1, len(best_scores_running) + 1), best_scores_running, \n",
    "                linewidth=2, label='Best score (running)', color='#e74c3c')\n",
    "axes[0, 0].set_xlabel('Trial', fontsize=10, weight='bold')\n",
    "axes[0, 0].set_ylabel('CV Recall', fontsize=10, weight='bold')\n",
    "axes[0, 0].set_title('Search Progress: CV Scores Over Trials', fontsize=12, weight='bold')\n",
    "axes[0, 0].legend(fontsize=9)\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "# 2. Model distribution\n",
    "model_counts = {}\n",
    "for r in results:\n",
    "    model_counts[r['model']] = model_counts.get(r['model'], 0) + 1\n",
    "axes[0, 1].bar(model_counts.keys(), model_counts.values(), \n",
    "               color='#2ecc71', edgecolor='black', linewidth=1)\n",
    "axes[0, 1].set_xlabel('Model Type', fontsize=10, weight='bold')\n",
    "axes[0, 1].set_ylabel('# Trials', fontsize=10, weight='bold')\n",
    "axes[0, 1].set_title('Model Distribution in Random Search', fontsize=12, weight='bold')\n",
    "axes[0, 1].grid(alpha=0.3, axis='y')\n",
    "# 3. CV score distribution by model\n",
    "model_types = list(set([r['model'] for r in results]))\n",
    "cv_by_model = {m: [r['cv_score'] for r in results if r['model'] == m] for m in model_types}\n",
    "positions = range(1, len(model_types) + 1)\n",
    "bp = axes[1, 0].boxplot(cv_by_model.values(), positions=positions, patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('#f39c12')\n",
    "    patch.set_edgecolor('black')\n",
    "axes[1, 0].set_xticklabels(model_types, rotation=45, ha='right', fontsize=9)\n",
    "axes[1, 0].set_ylabel('CV Recall', fontsize=10, weight='bold')\n",
    "axes[1, 0].set_title('CV Score Distribution by Model Type', fontsize=12, weight='bold')\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "# 4. Final comparison (all methods)\n",
    "methods_final_all = ['Manual RF', 'Auto-sklearn', 'TPOT', 'H2O', 'Custom']\n",
    "recalls_final_all = [rec_manual, rec_automl, rec_tpot, rec_h2o, rec_custom]\n",
    "colors_final = ['#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#e74c3c']\n",
    "bars = axes[1, 1].bar(methods_final_all, recalls_final_all, \n",
    "                       color=colors_final, edgecolor='black', linewidth=1.5)\n",
    "axes[1, 1].set_ylabel('Recall', fontsize=10, weight='bold')\n",
    "axes[1, 1].set_title('Final Recall Comparison (All Methods)', fontsize=12, weight='bold')\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "axes[1, 1].set_ylim([0, 1.05])\n",
    "for bar, val in zip(bars, recalls_final_all):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{val:.3f}',\n",
    "                    ha='center', va='bottom', fontsize=10, weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization: Custom AutoML analysis\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6eaaae",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 5\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76073ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Key Insights: Custom AutoML\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Key Takeaways: Custom AutoML\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. \u2705 Simple random search (50-100 trials) often achieves 85-90% of Bayesian optimization's performance\")\n",
    "print(\"2. \u2705 Custom AutoML useful for: Understanding internals, extreme constraints, research\")\n",
    "print(\"3. \u2705 Production: Use frameworks (Auto-sklearn, TPOT, H2O) for robustness and features\")\n",
    "print(\"4. \u2705 Hybrid approach: Start with production AutoML \u2192 Analyze \u2192 Custom refinements\")\n",
    "print(\"5. \u2705 Search space design matters: Balance exploration (diverse models) vs exploitation (focused tuning)\")\n",
    "print(\"6. \u2705 Ensemble crucial: Top-K models with voting consistently beats single best model\")\n",
    "print(\"7. \u2705 For semiconductor: Can add domain constraints (max inference time, memory, interpretability)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"\ud83c\udfaf BATCH 2 COMPLETE: Cells 6-10 (TPOT, H2O AutoML, Custom AutoML)\")\n",
    "print(\"\ud83d\udcca Next: Batch 3 (Cells 11-15) - Production deployment, comparative analysis, projects, takeaways\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98cfdaa",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24268a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Production Deployment: AutoML Models\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Production Deployment: AutoML Models\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "import joblib\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "# ========================================\n",
    "# 1. Model Serialization (Multiple Formats)\n",
    "# ========================================\n",
    "print(\"Step 1: Model Serialization\")\n",
    "print(\"-\" * 80)\n",
    "# Create directory for saved models\n",
    "import os\n",
    "models_dir = './production_models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "# Save Auto-sklearn model (already saved by framework)\n",
    "print(\"\u2705 Auto-sklearn model: Already saved by framework\")\n",
    "# Save TPOT exported pipeline\n",
    "print(\"\u2705 TPOT pipeline: Already exported to tpot_semiconductor_pipeline.py\")\n",
    "# Save custom AutoML ensemble\n",
    "ensemble_path = os.path.join(models_dir, 'custom_automl_ensemble.joblib')\n",
    "joblib.dump(ensemble_custom, ensemble_path)\n",
    "print(f\"\u2705 Custom AutoML ensemble saved: {ensemble_path}\")\n",
    "# Save preprocessing info (scaler, feature names)\n",
    "preprocessing_info = {\n",
    "    'scaler': scaler,\n",
    "    'feature_names': feature_names_with_spatial,\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'n_samples_train': len(X_train),\n",
    "    'n_features': X_train.shape[1],\n",
    "    'class_distribution': {\n",
    "        'class_0': int(np.sum(y_train == 0)),\n",
    "        'class_1': int(np.sum(y_train == 1))\n",
    "    }\n",
    "}\n",
    "preprocessing_path = os.path.join(models_dir, 'preprocessing_info.joblib')\n",
    "joblib.dump(preprocessing_info, preprocessing_path)\n",
    "print(f\"\u2705 Preprocessing info saved: {preprocessing_path}\")\n",
    "print()\n",
    "# ========================================\n",
    "# 2. REST API Deployment (Flask Example)\n",
    "# ========================================\n",
    "print(\"Step 2: REST API Deployment Pattern\")\n",
    "print(\"-\" * 80)\n",
    "# Note: This is a code template (not executed in notebook)\n",
    "# Save to file for production deployment\n",
    "flask_api_code = '''\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "app = Flask(__name__)\n",
    "# Load models at startup\n",
    "print(\"Loading models...\")\n",
    "ensemble_model = joblib.load('production_models/custom_automl_ensemble.joblib')\n",
    "preprocessing_info = joblib.load('production_models/preprocessing_info.joblib')\n",
    "scaler = preprocessing_info['scaler']\n",
    "feature_names = preprocessing_info['feature_names']\n",
    "print(f\"Models loaded. Feature count: {len(feature_names)}\")\n",
    "@app.route('/health', methods=['GET'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aad0387",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Function: health\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770cf2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def health():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return jsonify({'status': 'healthy', 'model': 'custom_automl_ensemble'}), 200\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    \"\"\"Predict endpoint for single device\"\"\"\n",
    "    try:\n",
    "        # Parse input\n",
    "        data = request.get_json()\n",
    "        \n",
    "        # Validate features\n",
    "        if not all(feat in data for feat in feature_names):\n",
    "            missing = [f for f in feature_names if f not in data]\n",
    "            return jsonify({'error': f'Missing features: {missing}'}), 400\n",
    "        \n",
    "        # Create feature vector\n",
    "        X = np.array([[data[feat] for feat in feature_names]])\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = scaler.transform(X)\n",
    "        \n",
    "        # Predict\n",
    "        prediction = ensemble_model.predict(X_scaled)[0]\n",
    "        probability = ensemble_model.predict_proba(X_scaled)[0]\n",
    "        \n",
    "        # Response\n",
    "        result = {\n",
    "            'prediction': int(prediction),\n",
    "            'probability_fail': float(probability[1]),\n",
    "            'probability_pass': float(probability[0]),\n",
    "            'risk_level': 'HIGH' if probability[1] > 0.8 else 'MEDIUM' if probability[1] > 0.5 else 'LOW',\n",
    "            'model': 'custom_automl_ensemble',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return jsonify(result), 200\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "@app.route('/predict_batch', methods=['POST'])\n",
    "def predict_batch():\n",
    "    \"\"\"Batch prediction endpoint\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        devices = data['devices']\n",
    "        \n",
    "        # Create feature matrix\n",
    "        X = np.array([[d[feat] for feat in feature_names] for d in devices])\n",
    "        X_scaled = scaler.transform(X)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = ensemble_model.predict(X_scaled)\n",
    "        probabilities = ensemble_model.predict_proba(X_scaled)\n",
    "        \n",
    "        # Response\n",
    "        results = [\n",
    "            {\n",
    "                'device_id': i,\n",
    "                'prediction': int(pred),\n",
    "                'probability_fail': float(prob[1])\n",
    "            }\n",
    "            for i, (pred, prob) in enumerate(zip(predictions, probabilities))\n",
    "        ]\n",
    "        \n",
    "        return jsonify({'predictions': results}), 200\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False)\n",
    "'''\n",
    "# Save Flask API code\n",
    "api_file_path = os.path.join(models_dir, 'flask_api.py')\n",
    "with open(api_file_path, 'w') as f:\n",
    "    f.write(flask_api_code)\n",
    "print(f\"\u2705 Flask API template saved: {api_file_path}\")\n",
    "print(\"   To run: python production_models/flask_api.py\")\n",
    "print(\"   Test: curl -X POST http://localhost:5000/predict -H 'Content-Type: application/json' -d '{...}'\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a5e4a2",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d98cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 3. Performance Monitoring\n",
    "# ========================================\n",
    "print(\"Step 3: Production Monitoring Pattern\")\n",
    "print(\"-\" * 80)\n",
    "# Simulate production inference\n",
    "n_production_samples = 1000\n",
    "production_sample_idx = np.random.choice(len(X_test), n_production_samples, replace=False)\n",
    "X_production = X_test[production_sample_idx]\n",
    "y_production_true = y_test[production_sample_idx]\n",
    "# Track inference metrics\n",
    "inference_times = []\n",
    "predictions_production = []\n",
    "for i in range(n_production_samples):\n",
    "    start = time.time()\n",
    "    pred = ensemble_custom.predict(X_production[i:i+1])[0]\n",
    "    end = time.time()\n",
    "    \n",
    "    inference_times.append((end - start) * 1000)  # Convert to ms\n",
    "    predictions_production.append(pred)\n",
    "predictions_production = np.array(predictions_production)\n",
    "# Monitoring metrics\n",
    "avg_latency = np.mean(inference_times)\n",
    "p95_latency = np.percentile(inference_times, 95)\n",
    "p99_latency = np.percentile(inference_times, 99)\n",
    "max_latency = np.max(inference_times)\n",
    "throughput = 1000 / avg_latency  # predictions per second\n",
    "print(f\"Inference Performance:\")\n",
    "print(f\"  Avg latency: {avg_latency:.2f} ms\")\n",
    "print(f\"  P95 latency: {p95_latency:.2f} ms\")\n",
    "print(f\"  P99 latency: {p99_latency:.2f} ms\")\n",
    "print(f\"  Max latency: {max_latency:.2f} ms\")\n",
    "print(f\"  Throughput: {throughput:.0f} predictions/sec\")\n",
    "print()\n",
    "# Prediction distribution monitoring\n",
    "fail_rate_production = np.mean(predictions_production)\n",
    "print(f\"Prediction Distribution:\")\n",
    "print(f\"  Predicted fail rate: {fail_rate_production:.2%}\")\n",
    "print(f\"  Training fail rate: {np.mean(y_train):.2%}\")\n",
    "print(f\"  Drift: {abs(fail_rate_production - np.mean(y_train)):.2%}\")\n",
    "if abs(fail_rate_production - np.mean(y_train)) > 0.01:\n",
    "    print(\"  \u26a0\ufe0f ALERT: Prediction distribution drift detected!\")\n",
    "print()\n",
    "# Performance monitoring\n",
    "acc_production = accuracy_score(y_production_true, predictions_production)\n",
    "rec_production = recall_score(y_production_true, predictions_production)\n",
    "print(f\"Production Performance (sampled):\")\n",
    "print(f\"  Accuracy: {acc_production:.4f}\")\n",
    "print(f\"  Recall: {rec_production:.4f}\")\n",
    "print()\n",
    "# ========================================\n",
    "# 4. A/B Testing: AutoML vs Baseline\n",
    "# ========================================\n",
    "print(\"Step 4: A/B Testing Pattern\")\n",
    "print(\"-\" * 80)\n",
    "# Simulate A/B test: 90% baseline (manual RF), 10% AutoML (custom)\n",
    "n_ab_samples = 10000\n",
    "traffic_split = 0.10  # 10% to AutoML\n",
    "# Generate AB test data\n",
    "ab_assignments = np.random.rand(n_ab_samples) < traffic_split\n",
    "n_automl_group = np.sum(ab_assignments)\n",
    "n_baseline_group = n_ab_samples - n_automl_group\n",
    "print(f\"A/B Test Configuration:\")\n",
    "print(f\"  Total samples: {n_ab_samples:,}\")\n",
    "print(f\"  AutoML group: {n_automl_group:,} ({n_automl_group/n_ab_samples:.1%})\")\n",
    "print(f\"  Baseline group: {n_baseline_group:,} ({n_baseline_group/n_ab_samples:.1%})\")\n",
    "print()\n",
    "# Simulate predictions (using test set performance)\n",
    "# AutoML group\n",
    "automl_missed = int(n_automl_group * (cm_custom[1,0] / len(y_test)))\n",
    "automl_false_alarms = int(n_automl_group * (cm_custom[0,1] / len(y_test)))\n",
    "# Baseline group\n",
    "baseline_missed = int(n_baseline_group * (cm_manual[1,0] / len(y_test)))\n",
    "baseline_false_alarms = int(n_baseline_group * (cm_manual[0,1] / len(y_test)))\n",
    "# Business impact\n",
    "cost_automl_ab = automl_missed * cost_per_missed_defect + automl_false_alarms * cost_per_false_alarm\n",
    "cost_baseline_ab = baseline_missed * cost_per_missed_defect + baseline_false_alarms * cost_per_false_alarm\n",
    "print(f\"A/B Test Results:\")\n",
    "print(f\"\\nAutoML Group:\")\n",
    "print(f\"  Missed defects: {automl_missed}\")\n",
    "print(f\"  False alarms: {automl_false_alarms}\")\n",
    "print(f\"  Total cost: ${cost_automl_ab:,.0f}\")\n",
    "print(f\"  Cost per device: ${cost_automl_ab/n_automl_group:.2f}\")\n",
    "print(f\"\\nBaseline Group:\")\n",
    "print(f\"  Missed defects: {baseline_missed}\")\n",
    "print(f\"  False alarms: {baseline_false_alarms}\")\n",
    "print(f\"  Total cost: ${cost_baseline_ab:,.0f}\")\n",
    "print(f\"  Cost per device: ${cost_baseline_ab/n_baseline_group:.2f}\")\n",
    "# Statistical significance\n",
    "improvement = (cost_baseline_ab/n_baseline_group - cost_automl_ab/n_automl_group) / (cost_baseline_ab/n_baseline_group)\n",
    "print(f\"\\nA/B Test Conclusion:\")\n",
    "print(f\"  Cost reduction per device: ${(cost_baseline_ab/n_baseline_group - cost_automl_ab/n_automl_group):.2f}\")\n",
    "print(f\"  Relative improvement: {improvement:.1%}\")\n",
    "if improvement > 0.05:\n",
    "    print(f\"  \u2705 DECISION: Roll out AutoML to 100% traffic\")\n",
    "else:\n",
    "    print(f\"  \u26a0\ufe0f DECISION: Continue testing or refine model\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb6650",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a33093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5. Visualization: Production Metrics\n",
    "# ========================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "# 1. Latency distribution\n",
    "axes[0, 0].hist(inference_times, bins=50, color='#3498db', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(avg_latency, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_latency:.2f}ms')\n",
    "axes[0, 0].axvline(p95_latency, color='orange', linestyle='--', linewidth=2, label=f'P95: {p95_latency:.2f}ms')\n",
    "axes[0, 0].set_xlabel('Latency (ms)', fontsize=10, weight='bold')\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=10, weight='bold')\n",
    "axes[0, 0].set_title('Inference Latency Distribution', fontsize=12, weight='bold')\n",
    "axes[0, 0].legend(fontsize=9)\n",
    "axes[0, 0].grid(alpha=0.3, axis='y')\n",
    "# 2. Throughput over time (simulated)\n",
    "time_windows = np.arange(0, 100, 1)\n",
    "throughput_simulated = throughput + np.random.randn(100) * 50\n",
    "axes[0, 1].plot(time_windows, throughput_simulated, linewidth=2, color='#2ecc71')\n",
    "axes[0, 1].axhline(throughput, color='red', linestyle='--', linewidth=2, label=f'Avg: {throughput:.0f} pred/s')\n",
    "axes[0, 1].set_xlabel('Time (minutes)', fontsize=10, weight='bold')\n",
    "axes[0, 1].set_ylabel('Throughput (pred/sec)', fontsize=10, weight='bold')\n",
    "axes[0, 1].set_title('Production Throughput Over Time', fontsize=12, weight='bold')\n",
    "axes[0, 1].legend(fontsize=9)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "# 3. A/B test comparison\n",
    "ab_groups = ['AutoML\\nGroup', 'Baseline\\nGroup']\n",
    "ab_costs = [cost_automl_ab/n_automl_group, cost_baseline_ab/n_baseline_group]\n",
    "colors_ab = ['#2ecc71', '#e74c3c']\n",
    "bars = axes[1, 0].bar(ab_groups, ab_costs, color=colors_ab, edgecolor='black', linewidth=1.5)\n",
    "axes[1, 0].set_ylabel('Cost per Device ($)', fontsize=10, weight='bold')\n",
    "axes[1, 0].set_title('A/B Test: Cost per Device', fontsize=12, weight='bold')\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "for bar, val in zip(bars, ab_costs):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'${val:.2f}',\n",
    "                    ha='center', va='bottom', fontsize=11, weight='bold')\n",
    "# 4. Production monitoring: Prediction drift over time (simulated)\n",
    "days = np.arange(1, 31)\n",
    "fail_rate_baseline = np.mean(y_train)\n",
    "fail_rate_drift = fail_rate_baseline + np.cumsum(np.random.randn(30) * 0.001)\n",
    "fail_rate_drift = np.clip(fail_rate_drift, 0, 0.1)\n",
    "axes[1, 1].plot(days, fail_rate_drift * 100, linewidth=2, color='#3498db', marker='o', markersize=4)\n",
    "axes[1, 1].axhline(fail_rate_baseline * 100, color='green', linestyle='--', linewidth=2, label=f'Training: {fail_rate_baseline:.2%}')\n",
    "axes[1, 1].axhline((fail_rate_baseline + 0.01) * 100, color='red', linestyle='--', linewidth=2, label='Alert threshold')\n",
    "axes[1, 1].set_xlabel('Days in Production', fontsize=10, weight='bold')\n",
    "axes[1, 1].set_ylabel('Predicted Fail Rate (%)', fontsize=10, weight='bold')\n",
    "axes[1, 1].set_title('Production Monitoring: Prediction Drift', fontsize=12, weight='bold')\n",
    "axes[1, 1].legend(fontsize=9)\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization: Production deployment metrics\")\n",
    "print()\n",
    "# ========================================\n",
    "# Key Takeaways: Production Deployment\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Key Takeaways: Production Deployment\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. \u2705 Model serialization: Use joblib (sklearn), ONNX (cross-platform), MOJO (H2O for Java)\")\n",
    "print(\"2. \u2705 REST API: Flask/FastAPI for real-time, batch endpoints for high-throughput\")\n",
    "print(\"3. \u2705 Monitoring: Track latency (P95, P99), throughput, prediction drift, data drift\")\n",
    "print(\"4. \u2705 A/B testing: Validate AutoML vs baseline with 5-10% traffic before full rollout\")\n",
    "print(\"5. \u2705 Alerting: Set thresholds for latency (>100ms?), drift (>1%?), accuracy drop (>5%?)\")\n",
    "print(\"6. \u2705 Semiconductor: Edge deployment (<50ms), MOJO for test equipment, real-time decisions\")\n",
    "print(\"7. \u2705 Business value: Deployment = value realization ($30M-$100M annually)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1528ca05",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Real-World Projects: AutoML Applications\n",
    "\n",
    "Apply AutoML frameworks to solve real-world problems. Each project includes clear objectives, implementation hints, and success metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udcf1 **Semiconductor & Hardware Projects**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 1: Wafer Yield Prediction System**\n",
    "\n",
    "**Objective:** Predict wafer yield from parametric test data to identify low-yield wafers early\n",
    "\n",
    "**Business Impact:**\n",
    "- Early detection of yield issues \u2192 2-5% yield improvement\n",
    "- Annual value: $50M-$200M for large fabs (300mm wafers)\n",
    "- Reduce time-to-market by 2-4 weeks (faster root cause analysis)\n",
    "\n",
    "**Dataset:**\n",
    "- **Features (100+):** Vdd_min/max/typ, Idd_active/standby/sleep, freq, temp, power, leakage, timing parameters\n",
    "- **Spatial:** wafer_id, die_x, die_y, lot_id, fab, process_node\n",
    "- **Target:** Yield % (0-100%), or binary (high yield vs low yield)\n",
    "- **Size:** 100K-10M devices across 10K-100K wafers\n",
    "- **Source:** STDF files (IEEE 1505), internal test databases\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use **H2O AutoML** for large datasets (distributed compute)\n",
    "- Use **Auto-sklearn** for medium datasets (best accuracy)\n",
    "- Feature engineering: Spatial features (distance_from_center, quadrant), wafer-level statistics (mean, std, outlier count)\n",
    "- Handle imbalance: Low-yield wafers are rare (1-5% of total)\n",
    "- Time series: Include temporal features (week, month, equipment age)\n",
    "\n",
    "**Success Metrics:**\n",
    "- Recall > 85% (detect 85%+ of low-yield wafers)\n",
    "- Precision > 70% (avoid false alarms that waste debug time)\n",
    "- Inference < 100ms per wafer (real-time decision support)\n",
    "- ROI: $10M-$50M annual savings from yield improvement\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 2: Parametric Test Flow Optimization**\n",
    "\n",
    "**Objective:** Optimize test flow to reduce test time while maintaining yield (adaptive test insertion)\n",
    "\n",
    "**Business Impact:**\n",
    "- 30-60% test time reduction \u2192 $30M-$100M annual savings\n",
    "- Maintain 99%+ yield (minimize escapes)\n",
    "- Enable higher throughput (more devices tested per hour)\n",
    "\n",
    "**Dataset:**\n",
    "- **Features:** Test results for 100+ parametric tests (voltage, current, frequency, power)\n",
    "- **Target:** Binary classification (pass/fail after ALL tests)\n",
    "- **Goal:** Predict final pass/fail using only first 10-20 tests (early prediction)\n",
    "- **Size:** 1M-10M devices\n",
    "- **Source:** Wafer test STDF, final test STDF\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use **TPOT** to discover optimal test sequence (genetic programming explores combinations)\n",
    "- Use **Auto-sklearn** for best prediction accuracy\n",
    "- Multi-stage approach: Train models for 5 tests, 10 tests, 15 tests, etc.\n",
    "- Cost-sensitive learning: False negatives (escapes) cost $1M, false positives (rework) cost $500\n",
    "- Sequential decision: If confidence < 0.95, continue testing\n",
    "\n",
    "**Success Metrics:**\n",
    "- Test time reduction: 30-60% (e.g., 100 tests \u2192 40 tests)\n",
    "- Yield maintained: >99% (escapes < 1%)\n",
    "- ROI: $30M-$100M annual savings\n",
    "- Inference: <10ms per device (real-time decision at test)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 3: Device Power Consumption Prediction**\n",
    "\n",
    "**Objective:** Predict device power consumption from design parameters and test data\n",
    "\n",
    "**Business Impact:**\n",
    "- Enable power binning (premium devices with lower power \u2192 5-10% price premium)\n",
    "- Annual value: $10M-$50M (high-margin bins)\n",
    "- Reduce power characterization time (skip 50-70% of power tests)\n",
    "\n",
    "**Dataset:**\n",
    "- **Features (50+):** Vdd, Idd, frequency, temperature, process corners, transistor counts, activity factors\n",
    "- **Target:** Power consumption (mW) across multiple operating modes\n",
    "- **Size:** 100K-1M devices\n",
    "- **Source:** Power characterization data, simulation results, test data\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use **Auto-sklearn** for regression (predict continuous power values)\n",
    "- Use **H2O AutoML** if dataset > 1M rows\n",
    "- Feature engineering: Interaction terms (Vdd \u00d7 Idd), ratios (power per MHz), thermal effects\n",
    "- Multi-target regression: Predict power for multiple modes (active, standby, sleep) simultaneously\n",
    "- Ensemble multiple frameworks: Combine Auto-sklearn + TPOT for best accuracy\n",
    "\n",
    "**Success Metrics:**\n",
    "- Prediction error: RMSE < 5% of mean power\n",
    "- R\u00b2 > 0.90 (90%+ variance explained)\n",
    "- Inference: <50ms per device\n",
    "- ROI: $10M-$50M from premium binning + test time savings\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 4: Failure Mode Classification (Root Cause Analysis)**\n",
    "\n",
    "**Objective:** Automatically classify device failures into root cause categories (e.g., shorts, opens, timing, leakage)\n",
    "\n",
    "**Business Impact:**\n",
    "- 30-50% faster root cause analysis \u2192 reduce debug time from weeks to days\n",
    "- Prioritize high-impact failure modes \u2192 focus engineering resources\n",
    "- Annual value: $5M-$20M per major failure incident avoided\n",
    "\n",
    "**Dataset:**\n",
    "- **Features (200+):** All parametric test results, spatial location, lot info, equipment ID, timestamp\n",
    "- **Target:** Failure mode category (multi-class: short, open, timing, leakage, other)\n",
    "- **Size:** 10K-100K failed devices (rare events)\n",
    "- **Source:** Failure analysis database, test data, FA images/logs\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use **Auto-sklearn** for multi-class classification (handles imbalance well)\n",
    "- Use **TPOT** to discover feature combinations that distinguish failure modes\n",
    "- Handle severe imbalance: Some failure modes are very rare (1% of failures)\n",
    "- Feature engineering: Test delta (expected - actual), spatial clusters, temporal trends\n",
    "- Interpretability critical: Use SHAP to explain predictions to failure analysis engineers\n",
    "\n",
    "**Success Metrics:**\n",
    "- Accuracy > 80% (multi-class classification)\n",
    "- Top-3 accuracy > 95% (correct mode in top 3 predictions)\n",
    "- Confusion matrix: Minimize misclassifications between critical modes\n",
    "- Inference: <100ms per device\n",
    "- Business: 30-50% faster time-to-root-cause\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83c\udf10 **General AI/ML Projects**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 5: Credit Card Fraud Detection**\n",
    "\n",
    "**Objective:** Detect fraudulent credit card transactions in real-time\n",
    "\n",
    "**Business Impact:**\n",
    "- Reduce fraud losses by 30-60% \u2192 $10M-$100M annually (large banks)\n",
    "- Improve customer experience (reduce false declines)\n",
    "- Regulatory compliance (PCI DSS, GDPR)\n",
    "\n",
    "**Dataset:**\n",
    "- **Features (30+):** Transaction amount, merchant category, location, time, historical behavior\n",
    "- **Target:** Binary (fraud vs legitimate)\n",
    "- **Size:** 1M-100M transactions per day\n",
    "- **Imbalance:** Fraud rate 0.1-1% (severe imbalance)\n",
    "- **Source:** Kaggle Credit Card Fraud Detection, financial institutions\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use **H2O AutoML** for large-scale real-time scoring (millions of transactions/day)\n",
    "- Use **Auto-sklearn** if dataset < 1M rows\n",
    "- Handle imbalance: SMOTE, class weights, cost-sensitive learning\n",
    "- Feature engineering: Time-based (hour, day), location (distance from home), velocity (# transactions in 10 min)\n",
    "- Real-time: <100ms inference latency required\n",
    "\n",
    "**Success Metrics:**\n",
    "- Recall > 85% (catch 85%+ of fraud)\n",
    "- Precision > 50% (avoid excessive false alarms)\n",
    "- AUC > 0.95\n",
    "- Inference: <50ms (real-time authorization)\n",
    "- Business: $10M-$100M fraud loss reduction\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 6: Customer Churn Prediction**\n",
    "\n",
    "**Objective:** Predict which customers will churn (cancel subscription) in next 30 days\n",
    "\n",
    "**Business Impact:**\n",
    "- Proactive retention campaigns \u2192 reduce churn by 10-30%\n",
    "- Annual value: $5M-$50M (SaaS, telecom, streaming)\n",
    "- Improve customer lifetime value (CLTV)\n",
    "\n",
    "**Dataset:**\n",
    "- **Features (50+):** Demographics, usage patterns, support tickets, payment history, engagement metrics\n",
    "- **Target:** Binary (churn vs retain in next 30 days)\n",
    "- **Size:** 100K-10M customers\n",
    "- **Imbalance:** Churn rate 5-20%\n",
    "- **Source:** Kaggle Telco Churn, company CRM/analytics\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use **Auto-sklearn** for tabular data (best accuracy)\n",
    "- Use **TPOT** to discover interaction features (e.g., usage trend \u00d7 support tickets)\n",
    "- Feature engineering: Recency (days since last login), frequency (logins per week), monetary (spend per month)\n",
    "- Time series: Include trend features (usage increasing vs decreasing)\n",
    "- Explainability: SHAP values to explain churn reasons to retention team\n",
    "\n",
    "**Success Metrics:**\n",
    "- Recall > 70% (catch 70%+ of churners)\n",
    "- Precision > 40% (avoid wasting retention budget)\n",
    "- AUC > 0.85\n",
    "- Business: 10-30% churn reduction \u2192 $5M-$50M revenue saved\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 7: Medical Diagnosis Assistant (Disease Classification)**\n",
    "\n",
    "**Objective:** Classify diseases from patient symptoms, lab results, and medical images\n",
    "\n",
    "**Business Impact:**\n",
    "- Improve diagnostic accuracy \u2192 reduce misdiagnosis by 10-30%\n",
    "- Faster diagnosis \u2192 reduce time-to-treatment\n",
    "- Support doctors (decision support, not replacement)\n",
    "- Potential lives saved: High value in rare diseases\n",
    "\n",
    "**Dataset:**\n",
    "- **Features (100+):** Symptoms, vital signs, lab results, medical history, demographics, imaging features\n",
    "- **Target:** Multi-class (disease categories) or binary (disease present/absent)\n",
    "- **Size:** 10K-1M patient records\n",
    "- **Imbalance:** Rare diseases have <1% prevalence\n",
    "- **Source:** Public datasets (UCI Heart Disease, Kaggle Medical), hospital EHR\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use **Auto-sklearn** for tabular data (best for structured medical records)\n",
    "- Use **H2O AutoML** if integrating with large EHR systems\n",
    "- Handle imbalance: SMOTE, class weights, stratified sampling\n",
    "- Feature engineering: Age groups, BMI categories, symptom combinations\n",
    "- Interpretability critical: SHAP, LIME for explainability to doctors (regulatory requirement)\n",
    "- Ensemble multiple frameworks for robustness (medical = high stakes)\n",
    "\n",
    "**Success Metrics:**\n",
    "- Accuracy > 85% (multi-class)\n",
    "- Recall > 90% for critical diseases (minimize false negatives)\n",
    "- Precision > 80% (avoid unnecessary anxiety/treatment)\n",
    "- AUC > 0.90\n",
    "- Regulatory: FDA approval (if deployed as medical device)\n",
    "- Business: Improved patient outcomes (hard to quantify but high value)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 8: Predictive Maintenance (Equipment Failure Prediction)**\n",
    "\n",
    "**Objective:** Predict equipment failures 7-30 days in advance to enable proactive maintenance\n",
    "\n",
    "**Business Impact:**\n",
    "- Reduce unplanned downtime by 30-70% \u2192 $5M-$50M annually (manufacturing, utilities)\n",
    "- Extend equipment lifespan (avoid catastrophic failures)\n",
    "- Optimize maintenance schedules (reduce over-maintenance)\n",
    "\n",
    "**Dataset:**\n",
    "- **Features (50+):** Sensor data (temperature, vibration, pressure, current), equipment age, maintenance history\n",
    "- **Target:** Binary (failure in next 7/14/30 days) or time-to-failure (regression)\n",
    "- **Size:** 100K-10M sensor readings (time series)\n",
    "- **Imbalance:** Failure rate 1-5%\n",
    "- **Source:** NASA Turbofan Engine Degradation, Kaggle Predictive Maintenance, industrial IoT\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use **H2O AutoML** for large-scale IoT data (millions of sensor readings)\n",
    "- Use **Auto-sklearn** for smaller datasets\n",
    "- Feature engineering: Rolling statistics (mean, std, trend over 24 hours), rate of change, anomaly detection features\n",
    "- Time series: Include lag features (sensor_t-1, sensor_t-7), seasonality\n",
    "- Multi-horizon: Predict failure at 7, 14, 30 days (different lead times)\n",
    "- Real-time: <1 second inference for online monitoring\n",
    "\n",
    "**Success Metrics:**\n",
    "- Recall > 80% (catch 80%+ of failures with 7-30 day lead time)\n",
    "- Precision > 50% (avoid excessive false alarms)\n",
    "- AUC > 0.85\n",
    "- Business: 30-70% downtime reduction \u2192 $5M-$50M savings\n",
    "- ROI: 5-20x (maintenance optimization + downtime avoidance)\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83c\udfaf **Implementation Checklist (All Projects)**\n",
    "\n",
    "**Data Preparation:**\n",
    "- [ ] Load and clean dataset (handle missing values, outliers)\n",
    "- [ ] Exploratory data analysis (EDA): Distributions, correlations, imbalance\n",
    "- [ ] Feature engineering (domain-specific features)\n",
    "- [ ] Train/validation/test split (stratified for imbalanced data)\n",
    "\n",
    "**AutoML Training:**\n",
    "- [ ] Select framework (Auto-sklearn, TPOT, H2O, or ensemble)\n",
    "- [ ] Configure time budget (demo: 5 min, production: 1-4 hours)\n",
    "- [ ] Set appropriate metric (recall, precision, F1, AUC, RMSE)\n",
    "- [ ] Handle imbalance (SMOTE, class weights, custom scoring)\n",
    "- [ ] Monitor training progress (leaderboard, best models)\n",
    "\n",
    "**Evaluation:**\n",
    "- [ ] Test set performance (accuracy, precision, recall, F1, AUC)\n",
    "- [ ] Confusion matrix analysis (false positives vs false negatives)\n",
    "- [ ] Feature importance (SHAP, variable importance)\n",
    "- [ ] Error analysis (where does model fail?)\n",
    "- [ ] Comparison to baseline (manual ML, existing system)\n",
    "\n",
    "**Deployment:**\n",
    "- [ ] Model serialization (joblib, pickle, ONNX, MOJO)\n",
    "- [ ] REST API (Flask, FastAPI) for real-time inference\n",
    "- [ ] Batch prediction for high-throughput\n",
    "- [ ] Monitoring (latency, throughput, prediction drift, data drift)\n",
    "- [ ] A/B testing (AutoML vs baseline with 5-10% traffic)\n",
    "- [ ] Alerting (performance degradation, drift, errors)\n",
    "\n",
    "**Business Validation:**\n",
    "- [ ] Calculate ROI (savings from improved predictions - investment)\n",
    "- [ ] Track business KPIs (yield %, churn rate, fraud loss, etc.)\n",
    "- [ ] Feedback loop (retrain with new data monthly/quarterly)\n",
    "- [ ] Documentation (model card, API docs, runbook)\n",
    "- [ ] Stakeholder communication (insights, recommendations, next steps)\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udca1 **Key Success Factors**\n",
    "\n",
    "1. **Start Simple:** Baseline (manual RF) \u2192 AutoML \u2192 Refinement\n",
    "2. **Iterate Fast:** Use 5-minute time budgets for demos, 1-4 hours for production\n",
    "3. **Domain Knowledge:** Feature engineering beats pure AutoML (combine both)\n",
    "4. **Ensemble:** Combine 2-3 frameworks for robustness\n",
    "5. **Monitor Production:** Track drift, retrain monthly/quarterly\n",
    "6. **Explainability:** Use SHAP/LIME for high-stakes applications (medical, financial)\n",
    "7. **Business Focus:** Optimize for business metrics (ROI, savings) not just accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e694ff25",
   "metadata": {},
   "source": [
    "## \ud83c\udf93 Key Takeaways & Best Practices\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udccb **Core Principles: When to Use AutoML**\n",
    "\n",
    "#### **\u2705 Use AutoML When:**\n",
    "- **Tight deadlines:** Need model in hours/days, not weeks\n",
    "- **Limited ML expertise:** Team lacks deep ML knowledge\n",
    "- **Tabular data:** Structured data (CSV, databases) - AutoML's sweet spot\n",
    "- **Baseline needed:** Quick proof-of-concept before investing in custom solutions\n",
    "- **Multiple projects:** Many similar problems (AutoML amortizes across projects)\n",
    "- **Reproducibility critical:** Automated pipeline ensures consistency\n",
    "\n",
    "#### **\u274c Use Manual ML When:**\n",
    "- **Novel architectures:** Cutting-edge research, custom loss functions\n",
    "- **Extreme interpretability:** Need full transparency (regulated industries may still need AutoML + SHAP)\n",
    "- **Domain-specific features:** Complex feature engineering requiring deep expertise\n",
    "- **Extreme constraints:** <1ms inference, <1MB model size, edge devices with 10KB RAM\n",
    "- **State-of-the-art required:** Benchmarking, competitions, research papers\n",
    "\n",
    "#### **\ud83d\udd04 Hybrid Approach (Recommended):**\n",
    "1. **Start with AutoML:** Get baseline in 1-4 hours (80-95% of expert performance)\n",
    "2. **Analyze results:** Understand what works (feature importance, model types, hyperparameters)\n",
    "3. **Manual refinement:** Add domain features, tune further, optimize for production\n",
    "4. **Deploy best:** Select AutoML or refined model based on performance + constraints\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udea7 **Common Pitfalls & Solutions**\n",
    "\n",
    "#### **1. Overfitting to Validation Set**\n",
    "- **Problem:** AutoML tries many models, risk of overfitting to validation data\n",
    "- **Solution:** Always hold out final test set (never used during AutoML search)\n",
    "- **Best Practice:** Use nested cross-validation for robust estimates\n",
    "\n",
    "#### **2. Ignoring Business Metrics**\n",
    "- **Problem:** Optimize for accuracy, but business needs recall (defect detection) or precision (fraud false alarms)\n",
    "- **Solution:** Set `scoring` parameter to business-relevant metric (recall, precision, F1, custom)\n",
    "- **Example:** Semiconductor yield \u2192 optimize for recall (missed failures cost $1M each)\n",
    "\n",
    "#### **3. Insufficient Time Budget**\n",
    "- **Problem:** 5-minute demos look good, but 1-4 hours often needed for production quality\n",
    "- **Solution:** Start with 5-10 min for exploration, scale to 1-4 hours for final model\n",
    "- **Rule of Thumb:** \n",
    "  - Demo/POC: 5-10 minutes\n",
    "  - Production (small data <10K rows): 30-60 minutes\n",
    "  - Production (medium data 10K-100K rows): 1-2 hours\n",
    "  - Production (large data >100K rows): 2-4 hours\n",
    "\n",
    "#### **4. Not Handling Imbalanced Data**\n",
    "- **Problem:** AutoML may achieve 98% accuracy by predicting all majority class (useless for rare events)\n",
    "- **Solution:** Use `balance_classes=True`, SMOTE, custom scoring (precision/recall/F1), stratified CV\n",
    "- **Example:** Fraud detection (0.1% fraud) \u2192 optimize for recall, accept lower precision\n",
    "\n",
    "#### **5. Forgetting Production Constraints**\n",
    "- **Problem:** AutoML finds great model but it's too slow/large for production (100ms inference, 1GB RAM)\n",
    "- **Solution:** Add constraints during search (`max_runtime_per_model`, memory limits, complexity penalty)\n",
    "- **Semiconductor:** Real-time test decisions require <10ms inference \u2192 use H2O MOJO or distillation\n",
    "\n",
    "#### **6. No Monitoring After Deployment**\n",
    "- **Problem:** Model deployed but performance degrades over time (data drift, concept drift)\n",
    "- **Solution:** Monitor prediction distribution, data distribution, performance metrics (weekly/monthly)\n",
    "- **Alerting:** If recall drops >5% or prediction drift >1%, trigger retraining\n",
    "\n",
    "#### **7. Black-Box Syndrome**\n",
    "- **Problem:** AutoML is opaque, stakeholders don't trust it\n",
    "- **Solution:** Use SHAP values, variable importance, partial dependence plots for explainability\n",
    "- **Critical:** Medical, finance, automotive require explainability for regulatory compliance\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udd27 **Production Best Practices**\n",
    "\n",
    "#### **1. Model Versioning**\n",
    "- Tag models with version (v1.0, v2.0), training date, dataset version\n",
    "- Store in model registry (MLflow, DVC, custom database)\n",
    "- Enable rollback if new version underperforms\n",
    "\n",
    "#### **2. A/B Testing**\n",
    "- Deploy new AutoML model to 5-10% of traffic\n",
    "- Compare business metrics (not just accuracy) vs baseline\n",
    "- Gradual rollout: 10% \u2192 25% \u2192 50% \u2192 100%\n",
    "\n",
    "#### **3. Retraining Strategy**\n",
    "- **Trigger-based:** Retrain when performance drops >5% or data drift detected\n",
    "- **Time-based:** Retrain monthly/quarterly even if no issues (proactive)\n",
    "- **Event-based:** Retrain after major changes (new product launch, process change)\n",
    "\n",
    "#### **4. Logging & Monitoring**\n",
    "- Log all predictions (input, output, confidence, timestamp)\n",
    "- Track metrics: Latency (P50, P95, P99), throughput, error rate\n",
    "- Monitor drift: Prediction distribution, feature distributions\n",
    "- Alerting: Slack/PagerDuty when thresholds exceeded\n",
    "\n",
    "#### **5. Fallback Strategy**\n",
    "- Always have baseline model as backup (simple RF, logistic regression)\n",
    "- If AutoML model fails (timeout, OOM), fall back to baseline\n",
    "- Graceful degradation better than system failure\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udcca **Algorithm Selection Flowchart**\n",
    "\n",
    "```\n",
    "START: Need ML Model\n",
    "\u2502\n",
    "\u251c\u2500 Have labeled data?\n",
    "\u2502  \u251c\u2500 NO \u2192 Use unsupervised (clustering, anomaly detection) - Not AutoML focus\n",
    "\u2502  \u2514\u2500 YES \u2192 Continue\n",
    "\u2502\n",
    "\u251c\u2500 Data type?\n",
    "\u2502  \u251c\u2500 Images \u2192 Use Neural Architecture Search (Google AutoML, Azure AutoML)\n",
    "\u2502  \u251c\u2500 Text \u2192 Use pre-trained LLMs (GPT, BERT) + fine-tuning\n",
    "\u2502  \u251c\u2500 Time Series \u2192 Use specialized AutoML (Prophet, Auto-ARIMA, TPOT with lag features)\n",
    "\u2502  \u2514\u2500 Tabular \u2192 Continue (AutoML sweet spot!)\n",
    "\u2502\n",
    "\u251c\u2500 Dataset size?\n",
    "\u2502  \u251c\u2500 <10K rows \u2192 Use Auto-sklearn (handles small data well)\n",
    "\u2502  \u251c\u2500 10K-1M rows \u2192 Use Auto-sklearn or TPOT (both work)\n",
    "\u2502  \u2514\u2500 >1M rows \u2192 Use H2O AutoML (distributed, scalable)\n",
    "\u2502\n",
    "\u251c\u2500 Production constraints?\n",
    "\u2502  \u251c\u2500 Need <10ms inference \u2192 Use H2O MOJO (compiled Java) or distillation\n",
    "\u2502  \u251c\u2500 Need <100ms inference \u2192 Use any framework, optimize ensemble size\n",
    "\u2502  \u251c\u2500 Need <1GB RAM \u2192 Use single model (disable ensemble) or distillation\n",
    "\u2502  \u2514\u2500 No hard constraints \u2192 Continue\n",
    "\u2502\n",
    "\u251c\u2500 Interpretability required?\n",
    "\u2502  \u251c\u2500 Critical (medical, finance, automotive) \u2192 Add SHAP/LIME to any framework\n",
    "\u2502  \u251c\u2500 Nice-to-have \u2192 Use H2O (built-in) or TPOT (exports code)\n",
    "\u2502  \u2514\u2500 Not critical \u2192 Any framework\n",
    "\u2502\n",
    "\u251c\u2500 Need pipeline optimization?\n",
    "\u2502  \u251c\u2500 YES (feature engineering, preprocessing) \u2192 Use TPOT (genetic programming)\n",
    "\u2502  \u2514\u2500 NO (just hyperparameter tuning) \u2192 Use Auto-sklearn or H2O\n",
    "\u2502\n",
    "\u2514\u2500 Default \u2192 Use Auto-sklearn (best general accuracy) + H2O (production backup)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83c\udfaf **Framework Selection Matrix**\n",
    "\n",
    "| **Criterion**              | **Auto-sklearn** | **TPOT**        | **H2O AutoML**  | **Custom**      |\n",
    "|---------------------------|-----------------|-----------------|-----------------|-----------------|\n",
    "| **Best For**              | Accuracy        | Pipelines       | Scale + Prod    | Research        |\n",
    "| **Dataset Size**          | <1M rows        | <1M rows        | Any (>1M ideal) | Any             |\n",
    "| **Speed**                 | Slow (1-4 hr)   | Slow (1-4 hr)   | Fast (<1 hr)    | Medium          |\n",
    "| **Accuracy**              | \u2b50\u2b50\u2b50\u2b50\u2b50        | \u2b50\u2b50\u2b50\u2b50          | \u2b50\u2b50\u2b50\u2b50          | \u2b50\u2b50\u2b50            |\n",
    "| **Interpretability**      | Medium (SHAP)   | High (code)     | High (built-in) | Full control    |\n",
    "| **Production Deployment** | Python only     | Python (code)   | Java/Python/R   | Full control    |\n",
    "| **Learning Curve**        | Low             | Low             | Low             | High            |\n",
    "| **Customization**         | Low             | Medium          | Medium          | Full            |\n",
    "| **Optimization**          | Bayesian        | Genetic         | Grid/Random     | Custom          |\n",
    "| **Semiconductor Use**     | Yield pred      | Test flow opt   | Large-scale     | Edge devices    |\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udcda **Recommended Resources**\n",
    "\n",
    "#### **Papers & Theory:**\n",
    "- Feurer et al. (2015): \"Efficient and Robust Automated Machine Learning\" - Auto-sklearn original paper\n",
    "- Olson & Moore (2016): \"TPOT: A Tree-based Pipeline Optimization Tool\" - TPOT paper\n",
    "- Hutter et al. (2011): \"Sequential Model-Based Optimization\" - SMAC algorithm (Bayesian optimization)\n",
    "- Caruana et al. (2004): \"Ensemble Selection from Libraries of Models\" - Ensemble construction\n",
    "\n",
    "#### **Books:**\n",
    "- \"Automated Machine Learning\" (Hutter, Kotthoff, Vanschoren) - Comprehensive AutoML textbook\n",
    "- \"Hands-On Automated Machine Learning\" (Umberto Michelucci) - Practical guide\n",
    "- \"Machine Learning Engineering\" (Andriy Burkov) - Production ML including AutoML\n",
    "\n",
    "#### **Libraries & Frameworks:**\n",
    "- **Auto-sklearn:** https://automl.github.io/auto-sklearn/\n",
    "- **TPOT:** http://epistasislab.github.io/tpot/\n",
    "- **H2O AutoML:** https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html\n",
    "- **MLflow:** https://mlflow.org/ - Model tracking and deployment\n",
    "- **SHAP:** https://github.com/slundberg/shap - Model explainability\n",
    "\n",
    "#### **Datasets for Practice:**\n",
    "- **UCI ML Repository:** https://archive.ics.uci.edu/ - 600+ datasets\n",
    "- **Kaggle:** https://www.kaggle.com/datasets - Competitions and real-world data\n",
    "- **OpenML:** https://www.openml.org/ - 20K+ datasets with benchmarks\n",
    "- **Semiconductor:** SEMI standards (STDF IEEE 1505) - contact equipment vendors\n",
    "\n",
    "---\n",
    "\n",
    "### \u2705 **Final Checklist: Before Production Deployment**\n",
    "\n",
    "#### **Model Quality:**\n",
    "- [ ] Test set performance meets business requirements (recall, precision, F1, AUC)\n",
    "- [ ] Compared to baseline (manual ML, existing system)\n",
    "- [ ] Validated on held-out data (never seen during training/tuning)\n",
    "- [ ] Error analysis completed (understand failure modes)\n",
    "- [ ] Feature importance reviewed (makes business sense?)\n",
    "\n",
    "#### **Production Readiness:**\n",
    "- [ ] Inference latency meets requirements (P95 < target)\n",
    "- [ ] Model serialized (joblib, ONNX, MOJO)\n",
    "- [ ] REST API implemented and tested (health check, predict, batch)\n",
    "- [ ] Monitoring dashboard configured (latency, throughput, predictions)\n",
    "- [ ] Alerting rules defined (performance degradation, drift, errors)\n",
    "- [ ] A/B testing plan ready (5-10% traffic to start)\n",
    "\n",
    "#### **Business Validation:**\n",
    "- [ ] ROI calculated (savings - investment)\n",
    "- [ ] Business KPIs defined (yield %, churn rate, fraud loss, etc.)\n",
    "- [ ] Stakeholders aligned (understand how model works, when to use)\n",
    "- [ ] Rollback plan defined (if new model underperforms)\n",
    "- [ ] Retraining schedule agreed (monthly, quarterly, trigger-based)\n",
    "\n",
    "#### **Documentation:**\n",
    "- [ ] Model card (architecture, training data, performance, limitations)\n",
    "- [ ] API documentation (endpoints, request/response format, examples)\n",
    "- [ ] Runbook (deployment, monitoring, troubleshooting, rollback)\n",
    "- [ ] SHAP/LIME analysis (explainability for stakeholders)\n",
    "- [ ] Lessons learned (what worked, what didn't, future improvements)\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\ude80 **Next Steps: Continue Learning**\n",
    "\n",
    "1. **Practice:** Implement 2-3 projects from the list above (start with semiconductor projects)\n",
    "2. **Experiment:** Try all frameworks (Auto-sklearn, TPOT, H2O) on same dataset, compare\n",
    "3. **Production:** Deploy one model to production with full monitoring\n",
    "4. **Advanced:** Explore neural architecture search (NAS) for images/text\n",
    "5. **Specialize:** Deep dive into one framework (read source code, contribute)\n",
    "6. **Stay Current:** Follow AutoML research (ICML, NeurIPS, ICLR), new frameworks\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udca1 **Closing Thoughts**\n",
    "\n",
    "**AutoML is a Tool, Not a Replacement:**\n",
    "- Best results: **AutoML + domain expertise**\n",
    "- AutoML finds patterns, humans provide context\n",
    "- 80-95% of expert performance in 10% of time \u2192 focus remaining time on hard problems\n",
    "\n",
    "**For Semiconductor Engineers:**\n",
    "- AutoML democratizes ML \u2192 test engineers can build yield models without PhD\n",
    "- Focus on feature engineering (spatial features, temporal trends, process knowledge)\n",
    "- Production deployment critical \u2192 models are worthless until they improve yield/test time\n",
    "\n",
    "**The Future:**\n",
    "- AutoML will improve (better search, faster, more accurate)\n",
    "- But domain knowledge will always be valuable (garbage in, garbage out)\n",
    "- Your job: Combine AutoML automation with semiconductor expertise \u2192 create business value\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf You've Completed AutoML Frameworks!\n",
    "\n",
    "**You now know:**\n",
    "- \u2705 How AutoML works (Bayesian optimization, genetic programming, meta-learning)\n",
    "- \u2705 When to use Auto-sklearn, TPOT, H2O AutoML, or custom AutoML\n",
    "- \u2705 How to deploy AutoML models to production (REST API, monitoring, A/B testing)\n",
    "- \u2705 How to apply AutoML to semiconductor testing (yield prediction, test optimization)\n",
    "- \u2705 How to maximize ROI from AutoML ($30M-$100M annual value)\n",
    "\n",
    "**Next notebook:** Model Optimization & Hyperparameter Tuning (deep dive into Bayesian optimization)\n",
    "\n",
    "---\n",
    "\n",
    "**Questions? Feedback? Contributions?**\n",
    "- GitHub: https://github.com/rajendarmuddasani/ai-ml-data-engg-mastery\n",
    "- Open an issue or PR for suggestions/improvements\n",
    "- Star \u2b50 the repo if you found this helpful!\n",
    "\n",
    "**Happy AutoML-ing! \ud83d\ude80\ud83e\udd16**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2cee02",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a23b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Comparative Analysis & Framework Selection Guide\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Comparative Analysis: When to Use Each AutoML Framework\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "# ========================================\n",
    "# 1. Final Performance Summary\n",
    "# ========================================\n",
    "print(\"1. PERFORMANCE SUMMARY (Semiconductor Yield Prediction)\")\n",
    "print(\"-\" * 80)\n",
    "final_summary = pd.DataFrame({\n",
    "    'Framework': ['Manual RF', 'Auto-sklearn', 'TPOT', 'H2O AutoML', 'Custom AutoML'],\n",
    "    'Recall': [rec_manual, rec_automl, rec_tpot, rec_h2o, rec_custom],\n",
    "    'Precision': [prec_manual, prec_automl, prec_tpot, prec_h2o, prec_custom],\n",
    "    'F1 Score': [f1_manual, f1_automl, f1_tpot, f1_h2o, f1_custom],\n",
    "    'AUC': [auc_manual, auc_automl, auc_tpot, auc_h2o, auc_custom],\n",
    "    'Time (min)': ['<1', f'{elapsed_time/60:.1f}', f'{elapsed_time/60:.1f}', \n",
    "                   f'{elapsed_time_h2o/60:.1f}', f'{elapsed_custom/60:.1f}'],\n",
    "    'Missed Defects': [cm_manual[1,0], cm_automl[1,0], cm_tpot[1,0], cm_h2o[1,0], cm_custom[1,0]],\n",
    "    'Ease of Use': ['Medium', 'Easy', 'Easy', 'Easy', 'Complex']\n",
    "})\n",
    "print(final_summary.to_string(index=False))\n",
    "print()\n",
    "# Find best by different criteria\n",
    "best_recall_idx = final_summary['Recall'].idxmax()\n",
    "best_f1_idx = final_summary['F1 Score'].idxmax()\n",
    "print(f\"\ud83c\udfc6 Best Recall: {final_summary.loc[best_recall_idx, 'Framework']} ({final_summary.loc[best_recall_idx, 'Recall']:.4f})\")\n",
    "print(f\"\ud83c\udfc6 Best F1 Score: {final_summary.loc[best_f1_idx, 'Framework']} ({final_summary.loc[best_f1_idx, 'F1 Score']:.4f})\")\n",
    "print()\n",
    "# ========================================\n",
    "# 2. Framework Selection Guide\n",
    "# ========================================\n",
    "print(\"2. FRAMEWORK SELECTION GUIDE\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "selection_guide = \"\"\"\n",
    "\ud83d\udcca WHEN TO USE EACH FRAMEWORK:\n",
    "\u2705 Auto-sklearn:\n",
    "   \u2022 Use when: Tabular data, need best accuracy, have 1-4 hours compute\n",
    "   \u2022 Best for: Structured data (CSV, database), classification/regression\n",
    "   \u2022 Optimization: Bayesian + meta-learning (smartest search)\n",
    "   \u2022 Deployment: Python only, sklearn compatibility\n",
    "   \u2022 Semiconductor: Yield prediction, parametric modeling, failure classification\n",
    "   \u2022 Pros: Often best accuracy, warm-start from past runs, robust ensembles\n",
    "   \u2022 Cons: Slow for large datasets (>100K rows), Python-only deployment\n",
    "\u2705 TPOT:\n",
    "   \u2022 Use when: Need pipeline optimization, feature engineering, interpretable pipelines\n",
    "   \u2022 Best for: Tree-based models, complex preprocessing, research/exploration\n",
    "   \u2022 Optimization: Genetic programming (explores pipeline structure)\n",
    "   \u2022 Deployment: Exports Python code (easy to customize)\n",
    "   \u2022 Semiconductor: Test flow optimization, feature discovery, multi-stage pipelines\n",
    "   \u2022 Pros: Discovers novel pipelines, exports code, explores feature engineering\n",
    "   \u2022 Cons: Slower than Bayesian, less robust on high-dimensional data\n",
    "\u2705 H2O AutoML:\n",
    "   \u2022 Use when: Large datasets (>1M rows), production deployment, scalability needed\n",
    "   \u2022 Best for: Big data, distributed compute, real-time scoring (<1ms)\n",
    "   \u2022 Optimization: Grid/random + stacked ensembles (fast, scalable)\n",
    "   \u2022 Deployment: MOJO for Java (low-latency), Python/R/Spark\n",
    "   \u2022 Semiconductor: High-volume testing (millions of devices), edge deployment, real-time\n",
    "   \u2022 Pros: Fastest for large data, production-ready, distributed, explainability built-in\n",
    "   \u2022 Cons: Requires H2O cluster, less \"smart\" search than Bayesian\n",
    "\u2705 Custom AutoML:\n",
    "   \u2022 Use when: Extreme constraints, novel search spaces, research, learning\n",
    "   \u2022 Best for: Domain-specific requirements, educational purposes, prototyping\n",
    "   \u2022 Optimization: You control (random, Bayesian, genetic, hybrid)\n",
    "   \u2022 Deployment: Full control over serialization and serving\n",
    "   \u2022 Semiconductor: Custom constraints (max latency, memory, power), edge devices\n",
    "   \u2022 Pros: Full customization, no dependencies, educational value\n",
    "   \u2022 Cons: Time-consuming, need ML expertise, less robust than frameworks\n",
    "\u274c Manual ML:\n",
    "   \u2022 Use when: Extreme interpretability, novel architectures, research, tight constraints\n",
    "   \u2022 Best for: Custom problems, cutting-edge methods, learning internals\n",
    "   \u2022 Optimization: Human expertise + trial-and-error\n",
    "   \u2022 Deployment: Full control\n",
    "   \u2022 Semiconductor: Novel device types, research, algorithm development\n",
    "   \u2022 Pros: Full control, cutting-edge, domain expertise integration\n",
    "   \u2022 Cons: Slow (hours to weeks), requires expert, error-prone, not reproducible\n",
    "\"\"\"\n",
    "print(selection_guide)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c630791",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9db603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 3. Cost-Benefit Analysis\n",
    "# ========================================\n",
    "print(\"3. COST-BENEFIT ANALYSIS (1 Year, 1M Devices)\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "# Annual costs for each method\n",
    "annual_volumes = 1_000_000\n",
    "cost_missed_annual = cost_per_missed_defect\n",
    "cost_false_alarm_annual = cost_per_false_alarm\n",
    "# Calculate annual costs\n",
    "methods_cost = ['Manual RF', 'Auto-sklearn', 'TPOT', 'H2O AutoML', 'Custom AutoML']\n",
    "missed_annual = [\n",
    "    cm_manual[1,0] * (annual_volumes / len(y_test)),\n",
    "    cm_automl[1,0] * (annual_volumes / len(y_test)),\n",
    "    cm_tpot[1,0] * (annual_volumes / len(y_test)),\n",
    "    cm_h2o[1,0] * (annual_volumes / len(y_test)),\n",
    "    cm_custom[1,0] * (annual_volumes / len(y_test))\n",
    "]\n",
    "false_alarms_annual = [\n",
    "    cm_manual[0,1] * (annual_volumes / len(y_test)),\n",
    "    cm_automl[0,1] * (annual_volumes / len(y_test)),\n",
    "    cm_tpot[0,1] * (annual_volumes / len(y_test)),\n",
    "    cm_h2o[0,1] * (annual_volumes / len(y_test)),\n",
    "    cm_custom[0,1] * (annual_volumes / len(y_test))\n",
    "]\n",
    "total_costs_annual = [\n",
    "    missed * cost_missed_annual + false_alarm * cost_false_alarm_annual\n",
    "    for missed, false_alarm in zip(missed_annual, false_alarms_annual)\n",
    "]\n",
    "# Development costs (one-time)\n",
    "dev_costs = [\n",
    "    40_000,   # Manual RF: 1 week engineer time @ $160/hr\n",
    "    500,      # Auto-sklearn: 4 hours compute @ $0.50/hr + 2 hours engineer setup\n",
    "    500,      # TPOT: Similar\n",
    "    500,      # H2O: Similar\n",
    "    10_000    # Custom: 1 week development\n",
    "]\n",
    "# Annual compute costs\n",
    "compute_costs_annual = [\n",
    "    0,      # Manual RF: negligible\n",
    "    50,     # Auto-sklearn: minimal retraining\n",
    "    50,     # TPOT: minimal retraining\n",
    "    2_000,  # H2O: cluster costs for production\n",
    "    100     # Custom: minimal\n",
    "]\n",
    "# Total cost (1 year)\n",
    "total_annual_costs = [\n",
    "    total_cost + dev_cost + compute_cost\n",
    "    for total_cost, dev_cost, compute_cost in zip(total_costs_annual, dev_costs, compute_costs_annual)\n",
    "]\n",
    "# Savings vs baseline\n",
    "baseline_cost = total_annual_costs[0]\n",
    "savings_vs_baseline = [baseline_cost - cost for cost in total_annual_costs]\n",
    "cost_benefit_df = pd.DataFrame({\n",
    "    'Method': methods_cost,\n",
    "    'Missed Defects': [int(m) for m in missed_annual],\n",
    "    'False Alarms': [int(f) for f in false_alarms_annual],\n",
    "    'Quality Cost ($M)': [c / 1e6 for c in total_costs_annual],\n",
    "    'Dev Cost ($K)': [d / 1e3 for d in dev_costs],\n",
    "    'Compute Cost ($K)': [c / 1e3 for c in compute_costs_annual],\n",
    "    'Total Cost ($M)': [c / 1e6 for c in total_annual_costs],\n",
    "    'Savings vs Manual ($M)': [s / 1e6 for s in savings_vs_baseline],\n",
    "    'ROI (%)': [100 * s / (dev_costs[i] + compute_costs_annual[i]) if (dev_costs[i] + compute_costs_annual[i]) > 0 else 0 \n",
    "                for i, s in enumerate(savings_vs_baseline)]\n",
    "})\n",
    "print(cost_benefit_df.to_string(index=False))\n",
    "print()\n",
    "best_roi_idx = cost_benefit_df['ROI (%)'].idxmax()\n",
    "print(f\"\ud83c\udfc6 Best ROI: {cost_benefit_df.loc[best_roi_idx, 'Method']} ({cost_benefit_df.loc[best_roi_idx, 'ROI (%)']:.0f}% ROI)\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a600211",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ba941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 4. Decision Flowchart (Text-Based)\n",
    "# ========================================\n",
    "print(\"4. DECISION FLOWCHART: Which AutoML Framework?\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "flowchart = \"\"\"\n",
    "START: Choose AutoML Framework\n",
    "\u2502\n",
    "\u251c\u2500 Dataset size > 1M rows?\n",
    "\u2502  \u251c\u2500 YES \u2192 Use H2O AutoML (distributed, scalable)\n",
    "\u2502  \u2514\u2500 NO \u2192 Continue\n",
    "\u2502\n",
    "\u251c\u2500 Need production deployment (<10ms latency)?\n",
    "\u2502  \u251c\u2500 YES \u2192 Use H2O AutoML (MOJO export for Java)\n",
    "\u2502  \u2514\u2500 NO \u2192 Continue\n",
    "\u2502\n",
    "\u251c\u2500 Need pipeline optimization & feature engineering?\n",
    "\u2502  \u251c\u2500 YES \u2192 Use TPOT (genetic programming, exports code)\n",
    "\u2502  \u2514\u2500 NO \u2192 Continue\n",
    "\u2502\n",
    "\u251c\u2500 Tabular data + need best accuracy?\n",
    "\u2502  \u251c\u2500 YES \u2192 Use Auto-sklearn (Bayesian + meta-learning)\n",
    "\u2502  \u2514\u2500 NO \u2192 Continue\n",
    "\u2502\n",
    "\u251c\u2500 Have extreme custom constraints?\n",
    "\u2502  \u251c\u2500 YES \u2192 Build Custom AutoML (full control)\n",
    "\u2502  \u2514\u2500 NO \u2192 Default to Auto-sklearn (safest, best accuracy)\n",
    "\u2502\n",
    "END: Framework selected\n",
    "\ud83d\udca1 HYBRID APPROACH (Recommended):\n",
    "   1. Start with 2-3 frameworks in parallel (e.g., Auto-sklearn + H2O + TPOT)\n",
    "   2. Compare performance on validation set\n",
    "   3. Select best for production\n",
    "   4. Optional: Ensemble top 2-3 frameworks for final model\n",
    "\"\"\"\n",
    "print(flowchart)\n",
    "print()\n",
    "# ========================================\n",
    "# 5. Visualization: Framework Comparison\n",
    "# ========================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "# 1. Performance vs Time trade-off\n",
    "methods_plot = ['Manual', 'Auto-sklearn', 'TPOT', 'H2O', 'Custom']\n",
    "recalls_plot = [rec_manual, rec_automl, rec_tpot, rec_h2o, rec_custom]\n",
    "times_plot = [0.5, elapsed_time/60, elapsed_time/60, elapsed_time_h2o/60, elapsed_custom/60]\n",
    "colors_plot = ['#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#e74c3c']\n",
    "for i, (method, recall, time_val, color) in enumerate(zip(methods_plot, recalls_plot, times_plot, colors_plot)):\n",
    "    axes[0, 0].scatter(time_val, recall, s=200, color=color, edgecolor='black', linewidth=2, label=method)\n",
    "    axes[0, 0].text(time_val + 0.1, recall, method, fontsize=9, weight='bold')\n",
    "axes[0, 0].set_xlabel('Time (minutes)', fontsize=10, weight='bold')\n",
    "axes[0, 0].set_ylabel('Recall (Defect Detection)', fontsize=10, weight='bold')\n",
    "axes[0, 0].set_title('Performance vs Time Trade-off', fontsize=12, weight='bold')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "axes[0, 0].set_xlim([-0.5, max(times_plot) + 1])\n",
    "axes[0, 0].set_ylim([min(recalls_plot) - 0.05, max(recalls_plot) + 0.05])\n",
    "# 2. Annual cost comparison\n",
    "bars = axes[0, 1].barh(methods_plot, [c / 1e6 for c in total_annual_costs], \n",
    "                        color=colors_plot, edgecolor='black', linewidth=1.5)\n",
    "axes[0, 1].set_xlabel('Total Annual Cost ($M)', fontsize=10, weight='bold')\n",
    "axes[0, 1].set_title('Total Annual Cost (Quality + Dev + Compute)', fontsize=12, weight='bold')\n",
    "axes[0, 1].grid(alpha=0.3, axis='x')\n",
    "for bar, val in zip(bars, [c / 1e6 for c in total_annual_costs]):\n",
    "    width = bar.get_width()\n",
    "    axes[0, 1].text(width, bar.get_y() + bar.get_height()/2.,\n",
    "                    f'${val:.1f}M',\n",
    "                    ha='left', va='center', fontsize=10, weight='bold')\n",
    "# 3. ROI comparison\n",
    "bars = axes[1, 0].bar(methods_plot, cost_benefit_df['ROI (%)'], \n",
    "                       color=colors_plot, edgecolor='black', linewidth=1.5)\n",
    "axes[1, 0].set_ylabel('ROI (%)', fontsize=10, weight='bold')\n",
    "axes[1, 0].set_title('Return on Investment (Savings / Investment)', fontsize=12, weight='bold')\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "axes[1, 0].axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "for bar, val in zip(bars, cost_benefit_df['ROI (%)']):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{val:.0f}%',\n",
    "                    ha='center', va='bottom' if val > 0 else 'top', fontsize=10, weight='bold')\n",
    "# 4. Framework strengths radar chart\n",
    "from math import pi\n",
    "categories_radar = ['Accuracy', 'Speed', 'Scalability', 'Ease of Use', 'Deployment']\n",
    "N_radar = len(categories_radar)\n",
    "angles_radar = [n / float(N_radar) * 2 * pi for n in range(N_radar)]\n",
    "angles_radar += angles_radar[:1]\n",
    "ax = plt.subplot(224, polar=True)\n",
    "# Scores (0-10 scale, subjective ratings)\n",
    "framework_scores = {\n",
    "    'Auto-sklearn': [9, 4, 4, 9, 6],  # Accuracy high, speed low, scalability low\n",
    "    'TPOT': [8, 4, 4, 9, 8],         # Accuracy high, exports code (deployment easy)\n",
    "    'H2O AutoML': [8, 9, 10, 9, 10], # Fast, scalable, production-ready\n",
    "    'Custom': [6, 7, 6, 3, 8]        # Lower accuracy, more complex\n",
    "}\n",
    "colors_radar = ['#2ecc71', '#f39c12', '#9b59b6', '#e74c3c']\n",
    "for idx, (framework, scores) in enumerate(framework_scores.items()):\n",
    "    values = scores + scores[:1]\n",
    "    ax.plot(angles_radar, values, 'o-', linewidth=2, label=framework, color=colors_radar[idx])\n",
    "    ax.fill(angles_radar, values, alpha=0.15, color=colors_radar[idx])\n",
    "ax.set_xticks(angles_radar[:-1])\n",
    "ax.set_xticklabels(categories_radar, fontsize=9)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.set_title('Framework Strengths (Subjective Rating)', fontsize=12, weight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=9)\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization: Framework comparison analysis\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2fb0e",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7f645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Key Recommendations\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Key Recommendations: Framework Selection\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. \u2705 For tabular data + best accuracy \u2192 Auto-sklearn (Bayesian optimization)\")\n",
    "print(\"2. \u2705 For large datasets + production \u2192 H2O AutoML (distributed, MOJO export)\")\n",
    "print(\"3. \u2705 For pipeline optimization + feature engineering \u2192 TPOT (genetic programming)\")\n",
    "print(\"4. \u2705 For extreme constraints + customization \u2192 Custom AutoML (full control)\")\n",
    "print(\"5. \u2705 Hybrid approach: Run 2-3 frameworks in parallel, select best, optionally ensemble\")\n",
    "print(\"6. \u2705 Always A/B test AutoML vs baseline before full production rollout\")\n",
    "print(\"7. \u2705 For semiconductor: H2O (scale) or Auto-sklearn (accuracy) depending on volume\")\n",
    "print(\"=\" * 80)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94569186",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Deploy AutoML models to production with monitoring, A/B testing, and performance tracking\n",
    "\n",
    "**Key Points:**\n",
    "- **Model Serialization**: Save trained models using joblib (sklearn), pickle (general), ONNX (cross-platform, optimized)\n",
    "- **API Deployment**: Flask/FastAPI for REST endpoints (real-time inference), batch prediction for high-throughput\n",
    "- **Monitoring**: Track inference latency, throughput, prediction distribution drift, data drift, model degradation\n",
    "- **A/B Testing**: Deploy AutoML model alongside baseline, split traffic (e.g., 10% AutoML, 90% baseline), measure business metrics\n",
    "- **Production Considerations**: Model versioning, rollback strategy, logging, alerting, resource limits (CPU, memory, timeout)\n",
    "- **Semiconductor Context**: Edge deployment to test equipment, <50ms latency requirement, MOJO format for Java integration\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Risk Mitigation**: A/B testing validates AutoML in production before full rollout (avoid $M losses from bad predictions)\n",
    "- **Performance Monitoring**: Detect model degradation early (e.g., yield prediction accuracy drops from 90% \u2192 80%)\n",
    "- **Scalability**: REST API handles 1000+ requests/sec for real-time test decisions\n",
    "- **Explainability**: Log feature importance, SHAP values for regulatory compliance (automotive, medical devices)\n",
    "- **Cost Optimization**: Monitor resource usage (AutoML ensemble may use 5x more CPU than single model \u2192 optimize)\n",
    "- **Business Impact**: Production deployment = $30M-$100M annual value realization (models are worthless until deployed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9ac1bc",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Build a simplified custom AutoML system from scratch to understand how AutoML frameworks work internally\n",
    "\n",
    "**Key Points:**\n",
    "- **Search Space Definition**: Manually define combinations of preprocessors (StandardScaler, MinMaxScaler, RobustScaler), models (LogisticRegression, RandomForest, XGBoost, SVM), and hyperparameters\n",
    "- **Random Search Loop**: Try N random configurations, evaluate with cross-validation, track performance\n",
    "- **Best Model Tracking**: Keep top K models based on validation score (e.g., recall)\n",
    "- **Simple Ensemble**: Combine top models using weighted voting (equal weights for simplicity)\n",
    "- **Comparison**: Validate custom AutoML against production frameworks\n",
    "- **Educational Purpose**: Understand search space, optimization loop, ensemble construction\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Demystifies AutoML**: See exactly what Auto-sklearn, TPOT, H2O do under the hood\n",
    "- **Customization**: Can add domain-specific constraints (e.g., semiconductor: max inference time < 10ms, memory < 100MB)\n",
    "- **Debug**: When AutoML fails, understanding internals helps diagnose issues\n",
    "- **Hybrid Approach**: Start with production AutoML \u2192 Analyze \u2192 Custom refinements\n",
    "- **Cost-Benefit**: Simple random search (50-100 trials) often gets 90% of Bayesian optimization's benefit at 10x speed\n",
    "- **When to Build Custom**: Extreme constraints, novel search spaces, research prototypes (production: use frameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ed99a6",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement H2O AutoML for distributed, scalable automated machine learning with production deployment focus\n",
    "\n",
    "**Key Points:**\n",
    "- **H2O.ai**: High-performance distributed ML platform (Java-based, scales to 100GB+ datasets)\n",
    "- **Leaderboard**: Trains multiple models (GLM, GBM, XGBoost, Deep Learning, Stacked Ensembles), ranks by performance\n",
    "- **Stacked Ensembles**: Automatically combines top models using metalearner (typically GLM) for best performance\n",
    "- **Explainability**: Built-in SHAP values, variable importance, partial dependence plots (production-ready interpretability)\n",
    "- **Time Budget**: Set max_runtime_secs (e.g., 300 seconds = 5 minutes demo, 3600-14400 = 1-4 hours production)\n",
    "- **Production Deployment**: Export models as MOJO (Model Objects, Optimized) for low-latency Java serving (<1ms)\n",
    "\n",
    "**Why This Matters:**\n",
    "- H2O AutoML excels at **large-scale production deployment** (banks, e-commerce, manufacturing)\n",
    "- Faster than Auto-sklearn/TPOT for large datasets (distributed compute, optimized C++ backend)\n",
    "- Semiconductor use case: 10M+ device test records, real-time scoring (< 50ms), MOJO deployment to edge servers\n",
    "- Explainability built-in (SHAP) \u2192 critical for regulated industries (automotive ISO 26262, medical FDA)\n",
    "- Annual impact: $50M-$200M from yield optimization + $10M-$50M from test time reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4429dc17",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement Auto-sklearn for semiconductor yield prediction with Bayesian optimization and meta-learning\n",
    "\n",
    "**Key Points:**\n",
    "- **Auto-sklearn**: Wraps sklearn models with automated preprocessing, feature engineering, model selection, hyperparameter tuning\n",
    "- **Bayesian Optimization**: Uses SMAC (Sequential Model-based Algorithm Configuration) to intelligently search pipeline space\n",
    "- **Meta-Learning**: Warm-starts optimization with configurations from similar datasets (faster convergence)\n",
    "- **Time Budget**: Set total time limit (e.g., 4 hours), Auto-sklearn allocates time across meta-learning (5%), search (85%), ensemble (10%)\n",
    "- **Ensemble Construction**: Automatically builds weighted ensemble from top models (greedy forward selection)\n",
    "- **Semiconductor Dataset**: Wafer-level yield prediction from 20 parametric tests (Vdd, Idd, frequency, temperature)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Manual ML workflow: 1 week (engineer tries 5-10 models, tunes hyperparameters) \u2192 85% accuracy\n",
    "- Auto-sklearn: 4 hours compute time \u2192 88-90% accuracy (matches or beats manual effort)\n",
    "- For semiconductor: 3-5% accuracy improvement = 1-3% yield gain = **$30M-$100M annually**\n",
    "- Democratizes ML: Non-experts can build production models, data scientists focus on high-value problems\n",
    "- Reproducibility: Every experiment logged, versioned, reproducible (critical for regulated industries)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}