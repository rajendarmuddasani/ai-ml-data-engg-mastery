{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d61aaea",
   "metadata": {},
   "source": [
    "## üî¢ K-Fold Cross-Validation: The Foundation\n",
    "\n",
    "**K-Fold CV** is the most common cross-validation strategy. It works by:\n",
    "\n",
    "1. **Split data into K equal-sized folds** (typically K=5 or K=10)\n",
    "2. **For each fold k = 1 to K:**\n",
    "   - Use fold k as **validation set**\n",
    "   - Use remaining K-1 folds as **training set**\n",
    "   - Train model and compute metric on validation fold\n",
    "3. **Average metrics across all K folds** ‚Üí robust estimate\n",
    "4. **Report mean ¬± standard deviation**\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Let $D$ be the full dataset with $n$ samples. Split into K folds: $D_1, D_2, ..., D_K$ (each has $\\approx n/K$ samples).\n",
    "\n",
    "For each fold $k$:\n",
    "- **Training set**: $D_{train}^{(k)} = D \\setminus D_k$ (all folds except k)\n",
    "- **Validation set**: $D_{val}^{(k)} = D_k$\n",
    "- Train model $f_k$ on $D_{train}^{(k)}$\n",
    "- Compute metric: $M_k = \\text{metric}(f_k, D_{val}^{(k)})$\n",
    "\n",
    "**Final estimate**:\n",
    "$$\n",
    "M_{CV} = \\frac{1}{K} \\sum_{k=1}^{K} M_k \\quad \\text{(mean)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_{CV} = \\sqrt{\\frac{1}{K-1} \\sum_{k=1}^{K} (M_k - M_{CV})^2} \\quad \\text{(std)}\n",
    "$$\n",
    "\n",
    "**Report as**: $M_{CV} \\pm \\sigma_{CV}$ (e.g., \"Accuracy = 0.90 ¬± 0.03\")\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "#### Training Set Size\n",
    "Each fold trains on $(K-1)/K$ of data:\n",
    "- K=5: 80% training (4000 samples from 5000)\n",
    "- K=10: 90% training (4500 samples from 5000)\n",
    "- K=n (LOO): ~100% training (n-1 samples)\n",
    "\n",
    "**Trade-off**: Larger K ‚Üí more training data per fold (less bias), but more folds (higher variance, more computation)\n",
    "\n",
    "#### Computational Cost\n",
    "K-Fold requires training **K models**:\n",
    "- K=5: 5√ó cost of single train/test\n",
    "- K=10: 10√ó cost\n",
    "- K=n (LOO): n√ó cost (prohibitive for large datasets)\n",
    "\n",
    "**Typical choice**: K=5 for faster iteration, K=10 for final evaluation\n",
    "\n",
    "#### Variance of Estimate\n",
    "Standard error of the mean:\n",
    "$$\n",
    "SE = \\frac{\\sigma_{CV}}{\\sqrt{K}}\n",
    "$$\n",
    "\n",
    "- Larger K ‚Üí smaller SE (more precise estimate)\n",
    "- But folds are not independent (overlap in training sets) ‚Üí SE underestimates true variance\n",
    "\n",
    "### When to Use K-Fold\n",
    "\n",
    "‚úÖ **Use K-Fold when:**\n",
    "- Data is **i.i.d.** (independent and identically distributed)\n",
    "- Classes are **balanced** (or use Stratified K-Fold)\n",
    "- No temporal/spatial ordering in data\n",
    "- Need robust estimate with confidence intervals\n",
    "\n",
    "‚ùå **Do NOT use K-Fold when:**\n",
    "- Data has **temporal ordering** (time series) ‚Üí use Time Series Split\n",
    "- Data has **group structure** (multiple samples from same patient/wafer) ‚Üí use Group K-Fold\n",
    "- Classes are **extremely imbalanced** ‚Üí use Stratified K-Fold\n",
    "\n",
    "### Semiconductor Example: Yield Prediction\n",
    "\n",
    "**Scenario**: Predict device pass/fail from parametric test data (10,000 devices from 50 wafers)\n",
    "\n",
    "**Single split approach:**\n",
    "- Random 80/20 split: Accuracy = 92%\n",
    "- **Problem**: Maybe test set happened to be from \"easy\" wafers?\n",
    "\n",
    "**5-Fold CV approach:**\n",
    "- Fold 1: Accuracy = 91%\n",
    "- Fold 2: Accuracy = 93%\n",
    "- Fold 3: Accuracy = 89%\n",
    "- Fold 4: Accuracy = 92%\n",
    "- Fold 5: Accuracy = 90%\n",
    "- **Mean ¬± Std**: 91% ¬± 1.4%\n",
    "\n",
    "**Interpretation**: \n",
    "- Expected accuracy is 91% (more reliable than single 92%)\n",
    "- Variability is ¬±1.4% (performance is stable)\n",
    "- 95% confidence interval: 91% ¬± 2√ó1.4% = [88.2%, 93.8%]\n",
    "\n",
    "### Choosing K\n",
    "\n",
    "| **K** | **Training Size** | **Bias** | **Variance** | **Computation** | **When to Use** |\n",
    "|-------|-------------------|----------|--------------|-----------------|----------------|\n",
    "| **K=3** | 67% | High | Low | 3√ó | Quick experiments, very small datasets |\n",
    "| **K=5** | 80% | Medium | Medium | 5√ó | **Standard choice**, good bias-variance trade-off |\n",
    "| **K=10** | 90% | Low | High | 10√ó | **Final evaluation**, low-bias estimate |\n",
    "| **K=20** | 95% | Very Low | Very High | 20√ó | Large datasets, need low bias |\n",
    "| **K=n (LOO)** | ~100% | Minimal | Maximum | n√ó | Small datasets (<1000), maximum data usage |\n",
    "\n",
    "**Rule of thumb**: \n",
    "- Small dataset (<1000): K=10 or LOO\n",
    "- Medium dataset (1K-100K): K=5 or K=10\n",
    "- Large dataset (>100K): K=3 or K=5 (computational cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7079bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "class KFoldEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive K-Fold cross-validation evaluator.\n",
    "    \n",
    "    Provides detailed analysis of model performance across folds,\n",
    "    including mean/std metrics, confidence intervals, and visualizations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits: int = 5, random_state: int = 42, shuffle: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize K-Fold cross-validator.\n",
    "        \n",
    "        Args:\n",
    "            n_splits: Number of folds (K)\n",
    "            random_state: Random seed for reproducibility\n",
    "            shuffle: Whether to shuffle data before splitting\n",
    "        \"\"\"\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.shuffle = shuffle\n",
    "        self.kfold = KFold(n_splits=n_splits, random_state=random_state, shuffle=shuffle)\n",
    "        self.results = {}\n",
    "    \n",
    "    def evaluate(self, model, X, y, scoring: str = 'accuracy') -> Dict:\n",
    "        \"\"\"\n",
    "        Perform K-Fold cross-validation.\n",
    "        \n",
    "        Args:\n",
    "            model: Sklearn-compatible model\n",
    "            X: Features (n_samples, n_features)\n",
    "            y: Target (n_samples,)\n",
    "            scoring: Metric to compute ('accuracy', 'f1', 'roc_auc', etc.)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with scores and statistics\n",
    "        \"\"\"\n",
    "        # Perform cross-validation\n",
    "        cv_results = cross_validate(\n",
    "            model, X, y, \n",
    "            cv=self.kfold, \n",
    "            scoring=scoring,\n",
    "            return_train_score=True,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Extract scores\n",
    "        test_scores = cv_results['test_score']\n",
    "        train_scores = cv_results['train_score']\n",
    "        fit_times = cv_results['fit_time']\n",
    "        \n",
    "        # Compute statistics\n",
    "        results = {\n",
    "            'test_scores': test_scores,\n",
    "            'train_scores': train_scores,\n",
    "            'fit_times': fit_times,\n",
    "            'test_mean': np.mean(test_scores),\n",
    "            'test_std': np.std(test_scores),\n",
    "            'test_min': np.min(test_scores),\n",
    "            'test_max': np.max(test_scores),\n",
    "            'train_mean': np.mean(train_scores),\n",
    "            'train_std': np.std(train_scores),\n",
    "            'mean_fit_time': np.mean(fit_times),\n",
    "            'scoring': scoring\n",
    "        }\n",
    "        \n",
    "        # Compute confidence interval (95%)\n",
    "        # Using t-distribution for small sample size (K folds)\n",
    "        from scipy.stats import t\n",
    "        confidence = 0.95\n",
    "        dof = len(test_scores) - 1  # Degrees of freedom\n",
    "        t_critical = t.ppf((1 + confidence) / 2, dof)\n",
    "        margin_of_error = t_critical * (results['test_std'] / np.sqrt(len(test_scores)))\n",
    "        \n",
    "        results['ci_lower'] = results['test_mean'] - margin_of_error\n",
    "        results['ci_upper'] = results['test_mean'] + margin_of_error\n",
    "        \n",
    "        self.results = results\n",
    "        return results\n",
    "    \n",
    "    def print_summary(self) -> None:\n",
    "        \"\"\"\n",
    "        Print formatted summary of cross-validation results.\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results available. Run evaluate() first.\")\n",
    "            return\n",
    "        \n",
    "        r = self.results\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(f\"{self.n_splits}-FOLD CROSS-VALIDATION RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Metric: {r['scoring']}\")\n",
    "        print(f\"Number of folds: {self.n_splits}\")\n",
    "        print(f\"Shuffle: {self.shuffle}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"TEST SET PERFORMANCE\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"Mean:      {r['test_mean']:.6f}\")\n",
    "        print(f\"Std Dev:   {r['test_std']:.6f}\")\n",
    "        print(f\"Min:       {r['test_min']:.6f}\")\n",
    "        print(f\"Max:       {r['test_max']:.6f}\")\n",
    "        print(f\"Range:     {r['test_max'] - r['test_min']:.6f}\")\n",
    "        print(f\"\\n95% Confidence Interval: [{r['ci_lower']:.6f}, {r['ci_upper']:.6f}]\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"TRAIN SET PERFORMANCE (checking for overfitting)\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"Mean:      {r['train_mean']:.6f}\")\n",
    "        print(f\"Std Dev:   {r['train_std']:.6f}\")\n",
    "        print(f\"\\nTrain-Test Gap: {r['train_mean'] - r['test_mean']:.6f}\")\n",
    "        \n",
    "        if r['train_mean'] - r['test_mean'] > 0.1:\n",
    "            print(\"‚ö†Ô∏è  WARNING: Large train-test gap suggests overfitting!\")\n",
    "        elif r['train_mean'] - r['test_mean'] < 0.02:\n",
    "            print(\"‚úÖ Good: Small train-test gap (low overfitting)\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"PER-FOLD SCORES\")\n",
    "        print(\"-\"*80)\n",
    "        for i, (train_score, test_score) in enumerate(zip(r['train_scores'], r['test_scores']), 1):\n",
    "            print(f\"Fold {i}: Train = {train_score:.6f}, Test = {test_score:.6f}, \"\n",
    "                  f\"Gap = {train_score - test_score:.6f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"COMPUTATIONAL COST\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"Mean fit time per fold: {r['mean_fit_time']:.4f} seconds\")\n",
    "        print(f\"Total CV time: {r['mean_fit_time'] * self.n_splits:.4f} seconds\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def plot_fold_comparison(self, figsize: Tuple[int, int] = (12, 5)) -> None:\n",
    "        \"\"\"\n",
    "        Visualize performance across folds.\n",
    "        \n",
    "        Args:\n",
    "            figsize: Figure size (width, height)\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results available. Run evaluate() first.\")\n",
    "            return\n",
    "        \n",
    "        r = self.results\n",
    "        folds = np.arange(1, self.n_splits + 1)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Plot 1: Train vs Test scores per fold\n",
    "        axes[0].plot(folds, r['train_scores'], 'o-', label='Train Score', \n",
    "                    color='green', linewidth=2, markersize=8)\n",
    "        axes[0].plot(folds, r['test_scores'], 'o-', label='Test Score', \n",
    "                    color='blue', linewidth=2, markersize=8)\n",
    "        axes[0].axhline(r['test_mean'], color='red', linestyle='--', linewidth=2, \n",
    "                       label=f'Test Mean = {r[\"test_mean\"]:.4f}')\n",
    "        axes[0].fill_between(folds, \n",
    "                            r['test_mean'] - r['test_std'], \n",
    "                            r['test_mean'] + r['test_std'], \n",
    "                            alpha=0.2, color='blue', label='¬± 1 Std Dev')\n",
    "        \n",
    "        axes[0].set_xlabel('Fold Number', fontsize=11, fontweight='bold')\n",
    "        axes[0].set_ylabel(f'{r[\"scoring\"].capitalize()}', fontsize=11, fontweight='bold')\n",
    "        axes[0].set_title('Performance Across Folds', fontsize=12, fontweight='bold')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(alpha=0.3)\n",
    "        axes[0].set_xticks(folds)\n",
    "        \n",
    "        # Plot 2: Box plot of test scores\n",
    "        bp = axes[1].boxplot([r['test_scores']], labels=['Test Scores'], patch_artist=True)\n",
    "        bp['boxes'][0].set_facecolor('lightblue')\n",
    "        bp['medians'][0].set_color('red')\n",
    "        bp['medians'][0].set_linewidth(2)\n",
    "        \n",
    "        # Add individual points\n",
    "        axes[1].scatter([1] * len(r['test_scores']), r['test_scores'], \n",
    "                       alpha=0.6, s=50, color='blue', zorder=3)\n",
    "        \n",
    "        # Add mean line\n",
    "        axes[1].axhline(r['test_mean'], color='green', linestyle='--', linewidth=2, \n",
    "                       label=f'Mean = {r[\"test_mean\"]:.4f}')\n",
    "        \n",
    "        axes[1].set_ylabel(f'{r[\"scoring\"].capitalize()}', fontsize=11, fontweight='bold')\n",
    "        axes[1].set_title(f'Distribution of Test Scores\\n(Std = {r[\"test_std\"]:.4f})', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"EXAMPLE: K-Fold Cross-Validation on Semiconductor Yield Prediction\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Generate synthetic semiconductor data\n",
    "    n_devices = 1000\n",
    "    n_features = 5\n",
    "    \n",
    "    # Features: VDD, IDD, Freq, Temp, Radial_distance\n",
    "    X = np.random.randn(n_devices, n_features)\n",
    "    X[:, 0] = np.random.normal(1.8, 0.05, n_devices)  # VDD\n",
    "    X[:, 1] = np.random.normal(50, 5, n_devices)      # IDD\n",
    "    X[:, 2] = np.random.normal(2000, 100, n_devices)  # Freq\n",
    "    X[:, 3] = np.random.normal(85, 5, n_devices)      # Temp\n",
    "    X[:, 4] = np.random.uniform(0, 5, n_devices)      # Radial distance\n",
    "    \n",
    "    # Target: fail more likely at edge (high radial distance) and extreme parameters\n",
    "    fail_prob = 0.05 + 0.1 * (X[:, 4] / 5.0) + 0.1 * (np.abs(X[:, 0] - 1.8) > 0.1)\n",
    "    y = (np.random.random(n_devices) < fail_prob).astype(int)\n",
    "    \n",
    "    print(f\"Dataset: {n_devices} devices, {n_features} features\")\n",
    "    print(f\"Class distribution: {(1-y.mean())*100:.1f}% pass, {y.mean()*100:.1f}% fail\\n\")\n",
    "    \n",
    "    # Create model\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    \n",
    "    # Perform 5-Fold CV\n",
    "    evaluator = KFoldEvaluator(n_splits=5, random_state=42)\n",
    "    results = evaluator.evaluate(model, X, y, scoring='accuracy')\n",
    "    \n",
    "    # Print summary\n",
    "    evaluator.print_summary()\n",
    "    \n",
    "    # Visualize\n",
    "    evaluator.plot_fold_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f3081",
   "metadata": {},
   "source": [
    "## üéØ Stratified K-Fold: Preserving Class Distribution\n",
    "\n",
    "**Problem with standard K-Fold**: For **imbalanced datasets**, random splits can create folds with very different class distributions:\n",
    "\n",
    "**Example**: 1000 samples with 10% positive class (100 positives, 900 negatives)\n",
    "- K=5 random splits: Each fold should have ~20 positives, ~180 negatives\n",
    "- **But random chance**: Fold 1 might have 10 positives, Fold 2 might have 30 positives!\n",
    "- This creates **high variance** in metrics across folds\n",
    "\n",
    "**Stratified K-Fold solution**: Ensures each fold has **approximately the same class distribution** as the full dataset.\n",
    "\n",
    "### How Stratified K-Fold Works\n",
    "\n",
    "1. **Separate samples by class**: Positives and negatives\n",
    "2. **Split each class into K folds independently**\n",
    "3. **Combine corresponding folds**: Fold 1 = positives_fold1 + negatives_fold1\n",
    "4. **Result**: Each fold has same positive/negative ratio as full dataset\n",
    "\n",
    "### Mathematical Guarantee\n",
    "\n",
    "For binary classification with positive class proportion $p$:\n",
    "\n",
    "**Standard K-Fold**: Each fold has $\\approx p \\pm \\sqrt{p(1-p)/n_k}$ positives (binomial variance)\n",
    "\n",
    "**Stratified K-Fold**: Each fold has **exactly** $p$ positives (within 1 sample due to rounding)\n",
    "\n",
    "**Example**:\n",
    "- 1000 samples, 10% positive, K=5\n",
    "- Standard K-Fold: Each fold could have 8-12% positives (variance)\n",
    "- Stratified K-Fold: Each fold has exactly 10% positives (200 samples, 20 positive)\n",
    "\n",
    "### When to Use Stratified K-Fold\n",
    "\n",
    "‚úÖ **Always use for classification problems**, especially when:\n",
    "- **Imbalanced classes** (minority class < 20%)\n",
    "- **Small datasets** (where random variation is high)\n",
    "- **Multi-class problems** (ensures all classes in every fold)\n",
    "- **Need low-variance estimates** (reduces fold-to-fold variability)\n",
    "\n",
    "‚ùå **Do NOT use for:**\n",
    "- **Regression problems** (no classes to stratify by)\n",
    "- **Time series data** (breaks temporal ordering)\n",
    "\n",
    "### Semiconductor Example: Defect Detection\n",
    "\n",
    "**Scenario**: Predict device defects (2% defect rate, 10,000 devices)\n",
    "\n",
    "**Standard 5-Fold**:\n",
    "- Fold 1: 15 defects (0.75%) ‚Üê Too few!\n",
    "- Fold 2: 25 defects (1.25%)\n",
    "- Fold 3: 50 defects (2.5%) ‚Üê Too many!\n",
    "- Fold 4: 40 defects (2.0%)\n",
    "- Fold 5: 70 defects (3.5%) ‚Üê Very unbalanced!\n",
    "- **Result**: High variance in metrics (Recall varies wildly)\n",
    "\n",
    "**Stratified 5-Fold**:\n",
    "- Fold 1: 40 defects (2.0%) ‚úì\n",
    "- Fold 2: 40 defects (2.0%) ‚úì\n",
    "- Fold 3: 40 defects (2.0%) ‚úì\n",
    "- Fold 4: 40 defects (2.0%) ‚úì\n",
    "- Fold 5: 40 defects (2.0%) ‚úì\n",
    "- **Result**: Low variance, reliable estimates\n",
    "\n",
    "### Multi-Class Stratification\n",
    "\n",
    "Stratified K-Fold also works for **multi-class problems**:\n",
    "\n",
    "**Example**: Device binning (4 bins: BIN1=30%, BIN2=40%, BIN3=20%, BIN4=10%)\n",
    "\n",
    "Each fold maintains proportions:\n",
    "- BIN1: 30% in every fold\n",
    "- BIN2: 40% in every fold\n",
    "- BIN3: 20% in every fold\n",
    "- BIN4: 10% in every fold\n",
    "\n",
    "This ensures **all bins are present** in every fold (important for rare classes!).\n",
    "\n",
    "### Variance Reduction\n",
    "\n",
    "**Empirical observation**: Stratified K-Fold typically reduces variance by **30-50%** compared to standard K-Fold for imbalanced problems.\n",
    "\n",
    "**Example metrics**:\n",
    "- Standard K-Fold: F1 = 0.75 ¬± 0.08 (high variance)\n",
    "- Stratified K-Fold: F1 = 0.76 ¬± 0.04 (low variance, more reliable)\n",
    "\n",
    "### Edge Case: Extremely Rare Classes\n",
    "\n",
    "**Problem**: If minority class has fewer samples than K, impossible to put in every fold!\n",
    "\n",
    "**Example**: 5 defects in 1000 devices, K=10\n",
    "- Can't put 0.5 defects in each fold!\n",
    "- Stratified K-Fold will **fail** or distribute unevenly\n",
    "\n",
    "**Solutions**:\n",
    "1. Reduce K (use K=5 or K=3 instead)\n",
    "2. Use Leave-One-Out CV (K=n)\n",
    "3. Oversample minority class before CV\n",
    "4. Use Group K-Fold with wafer/lot grouping\n",
    "\n",
    "### Implementation Note\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Automatically maintains class distribution\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_idx, val_idx in skf.split(X, y):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # y_train and y_val have same class distribution as full y\n",
    "```\n",
    "\n",
    "**sklearn default**: `cross_val_score(cv=5)` uses **StratifiedKFold** for classification automatically! üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d021ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from typing import Tuple\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def compare_stratified_vs_regular_kfold(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    n_splits: int = 5,\n",
    "    figsize: Tuple[int, int] = (14, 10)\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Compare Stratified K-Fold vs Regular K-Fold on imbalanced data.\n",
    "    \n",
    "    Demonstrates:\n",
    "    1. Class distribution across folds\n",
    "    2. Performance variance\n",
    "    3. Why stratification matters for imbalanced data\n",
    "    \n",
    "    Args:\n",
    "        X: Features (n_samples, n_features)\n",
    "        y: Target (n_samples,) - binary classification\n",
    "        n_splits: Number of folds\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    # Create both splitters\n",
    "    regular_kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    stratified_kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Track class distributions\n",
    "    regular_train_dist = []\n",
    "    regular_val_dist = []\n",
    "    stratified_train_dist = []\n",
    "    stratified_val_dist = []\n",
    "    \n",
    "    # Regular K-Fold analysis\n",
    "    for train_idx, val_idx in regular_kf.split(X, y):\n",
    "        regular_train_dist.append(y[train_idx].mean())\n",
    "        regular_val_dist.append(y[val_idx].mean())\n",
    "    \n",
    "    # Stratified K-Fold analysis\n",
    "    for train_idx, val_idx in stratified_kf.split(X, y):\n",
    "        stratified_train_dist.append(y[train_idx].mean())\n",
    "        stratified_val_dist.append(y[val_idx].mean())\n",
    "    \n",
    "    # Compute performance metrics\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    \n",
    "    regular_scores = cross_val_score(model, X, y, cv=regular_kf, scoring='f1')\n",
    "    stratified_scores = cross_val_score(model, X, y, cv=stratified_kf, scoring='f1')\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"=\"*80)\n",
    "    print(\"STRATIFIED vs REGULAR K-FOLD COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Dataset: {len(y)} samples\")\n",
    "    print(f\"Overall positive class rate: {y.mean()*100:.2f}%\")\n",
    "    print(f\"Number of folds: {n_splits}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"CLASS DISTRIBUTION ACROSS FOLDS (Validation Sets)\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'Fold':<10} {'Regular K-Fold':<20} {'Stratified K-Fold':<20}\")\n",
    "    print(\"-\"*80)\n",
    "    for i in range(n_splits):\n",
    "        print(f\"Fold {i+1:<5} {regular_val_dist[i]*100:>6.2f}% positive     \"\n",
    "              f\"{stratified_val_dist[i]*100:>6.2f}% positive\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"VARIANCE IN CLASS DISTRIBUTION\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Regular K-Fold:    Std = {np.std(regular_val_dist)*100:.4f}%\")\n",
    "    print(f\"Stratified K-Fold: Std = {np.std(stratified_val_dist)*100:.4f}%\")\n",
    "    print(f\"Variance reduction: {(1 - np.std(stratified_val_dist)/np.std(regular_val_dist))*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"PERFORMANCE METRICS (F1 Score)\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Regular K-Fold:    {regular_scores.mean():.6f} ¬± {regular_scores.std():.6f}\")\n",
    "    print(f\"Stratified K-Fold: {stratified_scores.mean():.6f} ¬± {stratified_scores.std():.6f}\")\n",
    "    print(f\"Variance reduction: {(1 - stratified_scores.std()/regular_scores.std())*100:.1f}%\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    folds = np.arange(1, n_splits + 1)\n",
    "    overall_rate = y.mean()\n",
    "    \n",
    "    # Plot 1: Regular K-Fold class distribution\n",
    "    axes[0, 0].bar(folds, np.array(regular_val_dist)*100, alpha=0.7, \n",
    "                   color='orange', edgecolor='black')\n",
    "    axes[0, 0].axhline(overall_rate*100, color='red', linestyle='--', \n",
    "                       linewidth=2, label=f'Overall: {overall_rate*100:.2f}%')\n",
    "    axes[0, 0].set_xlabel('Fold Number', fontsize=11, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Positive Class %', fontsize=11, fontweight='bold')\n",
    "    axes[0, 0].set_title(f'Regular K-Fold: Class Distribution\\nStd = {np.std(regular_val_dist)*100:.4f}%', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3, axis='y')\n",
    "    axes[0, 0].set_xticks(folds)\n",
    "    \n",
    "    # Plot 2: Stratified K-Fold class distribution\n",
    "    axes[0, 1].bar(folds, np.array(stratified_val_dist)*100, alpha=0.7, \n",
    "                   color='green', edgecolor='black')\n",
    "    axes[0, 1].axhline(overall_rate*100, color='red', linestyle='--', \n",
    "                       linewidth=2, label=f'Overall: {overall_rate*100:.2f}%')\n",
    "    axes[0, 1].set_xlabel('Fold Number', fontsize=11, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Positive Class %', fontsize=11, fontweight='bold')\n",
    "    axes[0, 1].set_title(f'Stratified K-Fold: Class Distribution\\nStd = {np.std(stratified_val_dist)*100:.4f}%', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3, axis='y')\n",
    "    axes[0, 1].set_xticks(folds)\n",
    "    \n",
    "    # Plot 3: Performance comparison (F1 scores)\n",
    "    axes[1, 0].plot(folds, regular_scores, 'o-', color='orange', \n",
    "                    linewidth=2, markersize=8, label='Regular K-Fold')\n",
    "    axes[1, 0].plot(folds, stratified_scores, 'o-', color='green', \n",
    "                    linewidth=2, markersize=8, label='Stratified K-Fold')\n",
    "    axes[1, 0].set_xlabel('Fold Number', fontsize=11, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('F1 Score', fontsize=11, fontweight='bold')\n",
    "    axes[1, 0].set_title('Performance Across Folds', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "    axes[1, 0].set_xticks(folds)\n",
    "    \n",
    "    # Plot 4: Box plots of F1 scores\n",
    "    bp = axes[1, 1].boxplot([regular_scores, stratified_scores], \n",
    "                            labels=['Regular K-Fold', 'Stratified K-Fold'],\n",
    "                            patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('orange')\n",
    "    bp['boxes'][1].set_facecolor('lightgreen')\n",
    "    \n",
    "    # Add scatter points\n",
    "    for i, scores in enumerate([regular_scores, stratified_scores], 1):\n",
    "        axes[1, 1].scatter([i] * len(scores), scores, alpha=0.6, s=50, \n",
    "                          color='blue', zorder=3)\n",
    "    \n",
    "    axes[1, 1].set_ylabel('F1 Score', fontsize=11, fontweight='bold')\n",
    "    axes[1, 1].set_title('F1 Score Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nDEMONSTRATION: Stratified K-Fold for Imbalanced Semiconductor Defect Detection\\n\")\n",
    "    \n",
    "    # Generate imbalanced dataset (5% defect rate)\n",
    "    n_devices = 2000\n",
    "    n_features = 5\n",
    "    \n",
    "    X = np.random.randn(n_devices, n_features)\n",
    "    X[:, 0] = np.random.normal(1.8, 0.05, n_devices)  # VDD\n",
    "    X[:, 1] = np.random.normal(50, 5, n_devices)      # IDD\n",
    "    X[:, 2] = np.random.normal(2000, 100, n_devices)  # Freq\n",
    "    X[:, 3] = np.random.normal(85, 5, n_devices)      # Temp\n",
    "    X[:, 4] = np.random.uniform(0, 5, n_devices)      # Radial distance\n",
    "    \n",
    "    # Create imbalanced target (5% defect rate)\n",
    "    fail_prob = 0.03 + 0.05 * (X[:, 4] / 5.0)  # Edge effect\n",
    "    y = (np.random.random(n_devices) < fail_prob).astype(int)\n",
    "    \n",
    "    # Run comparison\n",
    "    compare_stratified_vs_regular_kfold(X, y, n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d84c178",
   "metadata": {},
   "source": [
    "## ‚è∞ Time Series Cross-Validation: Respecting Temporal Order\n",
    "\n",
    "**Critical Problem**: Standard K-Fold and Stratified K-Fold **randomly shuffle** data before splitting. This is **catastrophic for time series** because:\n",
    "\n",
    "1. **Data leakage**: Training on future data to predict past (impossible in production!)\n",
    "2. **Unrealistic evaluation**: Model sees future patterns it won't have in deployment\n",
    "3. **Overly optimistic metrics**: Performance much better than real-world\n",
    "\n",
    "**Example of the problem**:\n",
    "- You have test data from January-December 2024\n",
    "- Standard K-Fold might train on June-December to predict January-May\n",
    "- **In production**: Model deployed in January 2025 has NO access to future months!\n",
    "- **Result**: Real performance much worse than CV suggests\n",
    "\n",
    "### Time Series Split: Forward Chaining\n",
    "\n",
    "**Solution**: Use **expanding window** or **rolling window** approach where validation is always **after** training:\n",
    "\n",
    "```\n",
    "Split 1:  [Train: Month 1-3]  ‚Üí  [Val: Month 4]\n",
    "Split 2:  [Train: Month 1-4]  ‚Üí  [Val: Month 5]\n",
    "Split 3:  [Train: Month 1-5]  ‚Üí  [Val: Month 6]\n",
    "Split 4:  [Train: Month 1-6]  ‚Üí  [Val: Month 7]\n",
    "...\n",
    "```\n",
    "\n",
    "**Key property**: Training data is always **before** validation data (no future leakage).\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given time-ordered dataset $D = \\\\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\\\}$ where index represents time:\n",
    "\n",
    "**For k-th split**:\n",
    "- **Training set**: $D_{train}^{(k)} = \\\\{(x_i, y_i) : i \\\\leq t_k\\\\}$ (all data up to time $t_k$)\n",
    "- **Validation set**: $D_{val}^{(k)} = \\\\{(x_i, y_i) : t_k < i \\\\leq t_{k+1}\\\\}$ (data from $t_k$ to $t_{k+1}$)\n",
    "\n",
    "Where $t_1 < t_2 < ... < t_K$ are split time points.\n",
    "\n",
    "### Two Variants\n",
    "\n",
    "#### 1. Expanding Window (sklearn default)\n",
    "\n",
    "Training set **grows** with each split:\n",
    "\n",
    "```\n",
    "Split 1:  [‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë]  ‚Üí  Val\n",
    "Split 2:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë]  ‚Üí  Val\n",
    "Split 3:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë]  ‚Üí  Val\n",
    "Split 4:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë]  ‚Üí  Val\n",
    "Split 5:  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë]  ‚Üí  Val\n",
    "```\n",
    "\n",
    "**Pros**:\n",
    "- Uses all historical data (no waste)\n",
    "- More training data ‚Üí better model\n",
    "- Reflects production scenario (retrain with all historical data)\n",
    "\n",
    "**Cons**:\n",
    "- Later folds have more training data (bias)\n",
    "- Computational cost increases (larger training sets)\n",
    "\n",
    "#### 2. Rolling Window (fixed size)\n",
    "\n",
    "Training set **slides** with fixed size:\n",
    "\n",
    "```\n",
    "Split 1:  [‚ñà‚ñà‚ñà‚ñà]‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ‚Üí  Val\n",
    "Split 2:  ‚ñë[‚ñà‚ñà‚ñà‚ñà]‚ñë‚ñë‚ñë‚ñë‚ñë  ‚Üí  Val\n",
    "Split 3:  ‚ñë‚ñë[‚ñà‚ñà‚ñà‚ñà]‚ñë‚ñë‚ñë‚ñë  ‚Üí  Val\n",
    "Split 4:  ‚ñë‚ñë‚ñë[‚ñà‚ñà‚ñà‚ñà]‚ñë‚ñë‚ñë  ‚Üí  Val\n",
    "Split 5:  ‚ñë‚ñë‚ñë‚ñë[‚ñà‚ñà‚ñà‚ñà]‚ñë‚ñë  ‚Üí  Val\n",
    "```\n",
    "\n",
    "**Pros**:\n",
    "- Consistent training set size (fair comparison)\n",
    "- Focuses on recent data (if older data less relevant)\n",
    "- Faster training (fixed size)\n",
    "\n",
    "**Cons**:\n",
    "- Wastes early data (not used in later folds)\n",
    "- May not reflect production (usually have all history)\n",
    "\n",
    "### Post-Silicon Validation Example\n",
    "\n",
    "**Scenario**: Test time prediction across production batches\n",
    "\n",
    "- **Data**: 52 weeks of test data (Week 1-52, 2024)\n",
    "- **Goal**: Deploy model in Week 1, 2025 ‚Üí predict Week 2-52, 2025\n",
    "\n",
    "**Wrong approach (Standard K-Fold)**:\n",
    "```python\n",
    "# WRONG: Randomly splits weeks, trains on future to predict past!\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "```\n",
    "\n",
    "Result: Train on Week 40-52 to predict Week 1-10 ‚Üí **unrealistic!**\n",
    "\n",
    "**Correct approach (Time Series Split)**:\n",
    "```python\n",
    "# CORRECT: Always train on past to predict future\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "```\n",
    "\n",
    "Splits:\n",
    "- Split 1: Train [Week 1-42]  ‚Üí Val [Week 43-44]\n",
    "- Split 2: Train [Week 1-44]  ‚Üí Val [Week 45-46]\n",
    "- Split 3: Train [Week 1-46]  ‚Üí Val [Week 47-48]\n",
    "- Split 4: Train [Week 1-48]  ‚Üí Val [Week 49-50]\n",
    "- Split 5: Train [Week 1-50]  ‚Üí Val [Week 51-52]\n",
    "\n",
    "**Interpretation**: Simulates deploying model at Week 42, 44, 46, 48, 50 and evaluating on next 2 weeks.\n",
    "\n",
    "### When to Use Time Series Split\n",
    "\n",
    "‚úÖ **Always use for temporal data**:\n",
    "- **Time series forecasting** (stock prices, demand, sensor data)\n",
    "- **Sequential test data** (device tests ordered by time/batch)\n",
    "- **Longitudinal studies** (patient outcomes over time)\n",
    "- **Manufacturing data** (production runs, process drift)\n",
    "- **Any data with temporal ordering**\n",
    "\n",
    "‚ùå **Do NOT use for**:\n",
    "- **i.i.d. data** (no temporal correlation) ‚Üí use K-Fold\n",
    "- **Small datasets** (not enough splits for reliable estimate)\n",
    "\n",
    "### Special Considerations\n",
    "\n",
    "#### Gap Between Train and Validation\n",
    "\n",
    "Sometimes you need a **gap** to avoid leakage:\n",
    "\n",
    "```\n",
    "Train [Month 1-3]  ‚Üí  [Gap: Month 4]  ‚Üí  Val [Month 5]\n",
    "```\n",
    "\n",
    "**Why gap?**\n",
    "- Avoid autocorrelation (today's value correlated with yesterday)\n",
    "- Realistic: Model trained on Monday, deployed Wednesday (2-day gap)\n",
    "- Example: Stock prediction (can't use today to predict tomorrow due to execution delay)\n",
    "\n",
    "**sklearn doesn't support gaps natively** ‚Üí implement custom splitter.\n",
    "\n",
    "#### Minimum Training Size\n",
    "\n",
    "Early splits have **small training sets** ‚Üí poor model quality.\n",
    "\n",
    "**Solution**: Set minimum training size:\n",
    "```python\n",
    "TimeSeriesSplit(n_splits=5, max_train_size=None)  # Expanding window\n",
    "TimeSeriesSplit(n_splits=5, max_train_size=1000)  # Rolling window (max 1000)\n",
    "```\n",
    "\n",
    "Or skip early splits with insufficient data.\n",
    "\n",
    "#### Choosing Number of Splits\n",
    "\n",
    "**Trade-off**:\n",
    "- **More splits** (K=10): Better variance estimate, but each validation set is smaller\n",
    "- **Fewer splits** (K=3): Larger validation sets, but higher variance estimate\n",
    "\n",
    "**Rule of thumb**:\n",
    "- Short time series (<100 points): K=3-5\n",
    "- Medium (100-1000): K=5-10\n",
    "- Long (>1000): K=10+\n",
    "\n",
    "### Comparison: Standard K-Fold vs Time Series Split\n",
    "\n",
    "| **Aspect** | **Standard K-Fold** | **Time Series Split** |\n",
    "|-----------|-------------------|---------------------|\n",
    "| **Data order** | Random shuffle | Temporal order preserved |\n",
    "| **Train/Val relationship** | Random split | Train always before Val |\n",
    "| **Training set size** | Constant (~(K-1)/K) | Growing (expanding window) |\n",
    "| **Realistic for time series?** | ‚ùå No (data leakage) | ‚úÖ Yes (no leakage) |\n",
    "| **When to use** | i.i.d. data | Temporal data |\n",
    "| **Metrics** | Overly optimistic | Realistic |\n",
    "\n",
    "### Production Deployment Pattern\n",
    "\n",
    "Time Series CV mimics **production retraining schedule**:\n",
    "\n",
    "```\n",
    "Week 1-52:  Train model on historical data  ‚Üí  Deploy Week 53\n",
    "Week 53 ends: Retrain with Week 1-53        ‚Üí  Deploy Week 54\n",
    "Week 54 ends: Retrain with Week 1-54        ‚Üí  Deploy Week 55\n",
    "...\n",
    "```\n",
    "\n",
    "Each CV fold simulates one retraining cycle. Average metric across folds = expected production performance.\n",
    "\n",
    "### Semiconductor Example: Process Drift Detection\n",
    "\n",
    "**Scenario**: Predict device yield across 100 production lots (ordered by time)\n",
    "\n",
    "**Wrong (Standard K-Fold)**:\n",
    "- Metric: Accuracy = 95%\n",
    "- **Problem**: Trained on Lot 80-100 to predict Lot 1-20 (impossible!)\n",
    "\n",
    "**Correct (Time Series Split, K=5)**:\n",
    "- Split 1: Train [Lot 1-70]  ‚Üí Val [Lot 71-74]  : Accuracy = 92%\n",
    "- Split 2: Train [Lot 1-74]  ‚Üí Val [Lot 75-78]  : Accuracy = 90%\n",
    "- Split 3: Train [Lot 1-78]  ‚Üí Val [Lot 79-82]  : Accuracy = 88%\n",
    "- Split 4: Train [Lot 1-82]  ‚Üí Val [Lot 83-86]  : Accuracy = 86%\n",
    "- Split 5: Train [Lot 1-86]  ‚Üí Val [Lot 87-90]  : Accuracy = 85%\n",
    "- **Mean ¬± Std**: 88.2% ¬± 2.8%\n",
    "\n",
    "**Key insight**: Performance **degrades over time** (88% ‚Üí 85%) due to process drift!\n",
    "\n",
    "This tells you:\n",
    "1. **Realistic performance**: 88.2% (not 95%)\n",
    "2. **Model staleness**: Accuracy drops 7% over 20 lots\n",
    "3. **Retraining schedule**: Retrain every 10-15 lots to maintain >90% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from typing import Tuple, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "class TimeSeriesValidator:\n",
    "    \"\"\"\n",
    "    Time Series Cross-Validation with visualization and analysis.\n",
    "    \n",
    "    Implements expanding window strategy (sklearn TimeSeriesSplit)\n",
    "    with detailed performance tracking and trend analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits: int = 5, max_train_size: int = None, test_size: int = None):\n",
    "        \"\"\"\n",
    "        Initialize Time Series cross-validator.\n",
    "        \n",
    "        Args:\n",
    "            n_splits: Number of splits\n",
    "            max_train_size: Maximum size of training set (None = expanding window)\n",
    "            test_size: Size of validation set (None = auto)\n",
    "        \"\"\"\n",
    "        self.n_splits = n_splits\n",
    "        self.max_train_size = max_train_size\n",
    "        self.test_size = test_size\n",
    "        self.tscv = TimeSeriesSplit(\n",
    "            n_splits=n_splits, \n",
    "            max_train_size=max_train_size,\n",
    "            test_size=test_size\n",
    "        )\n",
    "        self.results = {}\n",
    "    \n",
    "    def evaluate(self, model, X, y, scoring='r2') -> dict:\n",
    "        \"\"\"\n",
    "        Perform time series cross-validation.\n",
    "        \n",
    "        Args:\n",
    "            model: Sklearn-compatible model\n",
    "            X: Features (n_samples, n_features) - time-ordered\n",
    "            y: Target (n_samples,) - time-ordered\n",
    "            scoring: Metric to compute\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with detailed results\n",
    "        \"\"\"\n",
    "        train_scores = []\n",
    "        test_scores = []\n",
    "        train_sizes = []\n",
    "        test_sizes = []\n",
    "        split_indices = []\n",
    "        \n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(self.tscv.split(X), 1):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Compute scores\n",
    "            if scoring == 'r2':\n",
    "                train_score = model.score(X_train, y_train)\n",
    "                test_score = model.score(X_test, y_test)\n",
    "            else:\n",
    "                from sklearn.metrics import get_scorer\n",
    "                scorer = get_scorer(scoring)\n",
    "                train_score = scorer(model, X_train, y_train)\n",
    "                test_score = scorer(model, X_test, y_test)\n",
    "            \n",
    "            train_scores.append(train_score)\n",
    "            test_scores.append(test_score)\n",
    "            train_sizes.append(len(train_idx))\n",
    "            test_sizes.append(len(test_idx))\n",
    "            split_indices.append((train_idx, test_idx))\n",
    "        \n",
    "        # Store results\n",
    "        self.results = {\n",
    "            'train_scores': np.array(train_scores),\n",
    "            'test_scores': np.array(test_scores),\n",
    "            'train_sizes': np.array(train_sizes),\n",
    "            'test_sizes': np.array(test_sizes),\n",
    "            'split_indices': split_indices,\n",
    "            'test_mean': np.mean(test_scores),\n",
    "            'test_std': np.std(test_scores),\n",
    "            'test_min': np.min(test_scores),\n",
    "            'test_max': np.max(test_scores),\n",
    "            'scoring': scoring,\n",
    "            'n_splits': self.n_splits\n",
    "        }\n",
    "        \n",
    "        # Detect trend in performance\n",
    "        from scipy.stats import linregress\n",
    "        folds = np.arange(1, self.n_splits + 1)\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(folds, test_scores)\n",
    "        \n",
    "        self.results['trend_slope'] = slope\n",
    "        self.results['trend_pvalue'] = p_value\n",
    "        self.results['trend_significant'] = p_value < 0.05\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def print_summary(self) -> None:\n",
    "        \"\"\"\n",
    "        Print formatted summary with trend analysis.\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results available. Run evaluate() first.\")\n",
    "            return\n",
    "        \n",
    "        r = self.results\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(f\"TIME SERIES CROSS-VALIDATION ({self.n_splits} SPLITS)\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Metric: {r['scoring']}\")\n",
    "        print(f\"Strategy: {'Expanding window' if self.max_train_size is None else f'Rolling window (max train={self.max_train_size})'}\")\n",
    "        \n",
    "        print(\"\\\\n\" + \"-\"*80)\n",
    "        print(\"OVERALL PERFORMANCE\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"Mean:      {r['test_mean']:.6f}\")\n",
    "        print(f\"Std Dev:   {r['test_std']:.6f}\")\n",
    "        print(f\"Min:       {r['test_min']:.6f}\")\n",
    "        print(f\"Max:       {r['test_max']:.6f}\")\n",
    "        print(f\"Range:     {r['test_max'] - r['test_min']:.6f}\")\n",
    "        \n",
    "        print(\"\\\\n\" + \"-\"*80)\n",
    "        print(\"PERFORMANCE TREND ANALYSIS\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"Trend slope: {r['trend_slope']:.6f} per fold\")\n",
    "        print(f\"P-value: {r['trend_pvalue']:.6f}\")\n",
    "        \n",
    "        if r['trend_significant']:\n",
    "            if r['trend_slope'] < 0:\n",
    "                print(\"‚ö†Ô∏è  SIGNIFICANT DOWNWARD TREND: Performance degrading over time!\")\n",
    "                print(\"    ‚Üí Model staleness detected (consider retraining schedule)\")\n",
    "                print(f\"    ‚Üí Expected drop: {abs(r['trend_slope'] * self.n_splits):.4f} over {self.n_splits} folds\")\n",
    "            else:\n",
    "                print(\"‚úÖ SIGNIFICANT UPWARD TREND: Performance improving over time\")\n",
    "                print(\"    ‚Üí More training data helps (expanding window working well)\")\n",
    "        else:\n",
    "            print(\"‚úÖ NO SIGNIFICANT TREND: Performance stable over time\")\n",
    "        \n",
    "        print(\"\\\\n\" + \"-\"*80)\n",
    "        print(\"PER-FOLD DETAILS\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"{'Fold':<6} {'Train Size':<12} {'Test Size':<12} {'Train Score':<14} {'Test Score':<14} {'Gap':<10}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for i in range(self.n_splits):\n",
    "            gap = r['train_scores'][i] - r['test_scores'][i]\n",
    "            print(f\"{i+1:<6} {r['train_sizes'][i]:<12} {r['test_sizes'][i]:<12} \"\n",
    "                  f\"{r['train_scores'][i]:<14.6f} {r['test_scores'][i]:<14.6f} {gap:<10.6f}\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def plot_time_series_splits(self, figsize: Tuple[int, int] = (14, 10)) -> None:\n",
    "        \"\"\"\n",
    "        Visualize time series splits and performance.\n",
    "        \n",
    "        Args:\n",
    "            figsize: Figure size\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results available. Run evaluate() first.\")\n",
    "            return\n",
    "        \n",
    "        r = self.results\n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        \n",
    "        # Plot 1: Visual representation of splits\n",
    "        for i, (train_idx, test_idx) in enumerate(r['split_indices']):\n",
    "            # Train data\n",
    "            axes[0, 0].barh(i, len(train_idx), left=train_idx[0], \n",
    "                           color='green', alpha=0.6, edgecolor='black')\n",
    "            # Test data\n",
    "            axes[0, 0].barh(i, len(test_idx), left=test_idx[0], \n",
    "                           color='orange', alpha=0.6, edgecolor='black')\n",
    "        \n",
    "        axes[0, 0].set_xlabel('Sample Index (Time ‚Üí)', fontsize=11, fontweight='bold')\n",
    "        axes[0, 0].set_ylabel('Fold Number', fontsize=11, fontweight='bold')\n",
    "        axes[0, 0].set_title('Time Series Split Visualization\\\\n(Green=Train, Orange=Test)', \n",
    "                            fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].set_yticks(range(self.n_splits))\n",
    "        axes[0, 0].set_yticklabels([f'Fold {i+1}' for i in range(self.n_splits)])\n",
    "        axes[0, 0].grid(alpha=0.3, axis='x')\n",
    "        \n",
    "        # Plot 2: Performance across folds\n",
    "        folds = np.arange(1, self.n_splits + 1)\n",
    "        axes[0, 1].plot(folds, r['train_scores'], 'o-', color='green', \n",
    "                       linewidth=2, markersize=8, label='Train Score')\n",
    "        axes[0, 1].plot(folds, r['test_scores'], 'o-', color='blue', \n",
    "                       linewidth=2, markersize=8, label='Test Score')\n",
    "        axes[0, 1].axhline(r['test_mean'], color='red', linestyle='--', \n",
    "                          linewidth=2, label=f'Test Mean = {r[\\\"test_mean\\\"]:.4f}')\n",
    "        \n",
    "        # Add trend line\n",
    "        from scipy.stats import linregress\n",
    "        slope, intercept, _, _, _ = linregress(folds, r['test_scores'])\n",
    "        trend_line = slope * folds + intercept\n",
    "        axes[0, 1].plot(folds, trend_line, 'r--', linewidth=1.5, alpha=0.7, \n",
    "                       label=f'Trend (slope={slope:.4f})')\n",
    "        \n",
    "        axes[0, 1].set_xlabel('Fold Number (Time ‚Üí)', fontsize=11, fontweight='bold')\n",
    "        axes[0, 1].set_ylabel(f'{r[\\\"scoring\\\"].capitalize()}', fontsize=11, fontweight='bold')\n",
    "        axes[0, 1].set_title('Performance Over Time', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(alpha=0.3)\n",
    "        axes[0, 1].set_xticks(folds)\n",
    "        \n",
    "        # Plot 3: Training set size evolution\n",
    "        axes[1, 0].bar(folds, r['train_sizes'], alpha=0.7, color='green', \n",
    "                      edgecolor='black', label='Train Size')\n",
    "        axes[1, 0].bar(folds, r['test_sizes'], bottom=r['train_sizes'], \n",
    "                      alpha=0.7, color='orange', edgecolor='black', label='Test Size')\n",
    "        \n",
    "        axes[1, 0].set_xlabel('Fold Number', fontsize=11, fontweight='bold')\n",
    "        axes[1, 0].set_ylabel('Number of Samples', fontsize=11, fontweight='bold')\n",
    "        axes[1, 0].set_title('Dataset Split Sizes', fontsize=12, fontweight='bold')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "        axes[1, 0].set_xticks(folds)\n",
    "        \n",
    "        # Plot 4: Train-test gap\n",
    "        gap = r['train_scores'] - r['test_scores']\n",
    "        colors = ['red' if g > 0.1 else 'green' for g in gap]\n",
    "        axes[1, 1].bar(folds, gap, alpha=0.7, color=colors, edgecolor='black')\n",
    "        axes[1, 1].axhline(0, color='black', linestyle='-', linewidth=1)\n",
    "        axes[1, 1].axhline(0.1, color='red', linestyle='--', linewidth=1, \n",
    "                          label='Overfitting threshold (0.1)')\n",
    "        \n",
    "        axes[1, 1].set_xlabel('Fold Number', fontsize=11, fontweight='bold')\n",
    "        axes[1, 1].set_ylabel('Train - Test Gap', fontsize=11, fontweight='bold')\n",
    "        axes[1, 1].set_title('Overfitting Detection\\\\n(Red = High gap)', \n",
    "                            fontsize=12, fontweight='bold')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "        axes[1, 1].set_xticks(folds)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def compare_kfold_vs_timeseries(X, y, n_splits=5, figsize=(14, 5)):\n",
    "    \"\"\"\n",
    "    Compare Standard K-Fold (WRONG) vs Time Series Split (CORRECT) for temporal data.\n",
    "    \n",
    "    Demonstrates why K-Fold fails on time series and Time Series Split succeeds.\n",
    "    \n",
    "    Args:\n",
    "        X: Features (time-ordered)\n",
    "        y: Target (time-ordered)\n",
    "        n_splits: Number of folds\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    model = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42)\n",
    "    \n",
    "    # Standard K-Fold (WRONG for time series)\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    kfold_scores = cross_val_score(model, X, y, cv=kfold, scoring='r2')\n",
    "    \n",
    "    # Time Series Split (CORRECT)\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    tscv_scores = cross_val_score(model, X, y, cv=tscv, scoring='r2')\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPARISON: K-FOLD vs TIME SERIES SPLIT\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Dataset size: {len(y)} samples (time-ordered)\")\n",
    "    print(f\"Number of splits: {n_splits}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"-\"*80)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'Method':<25} {'Mean R¬≤':<15} {'Std R¬≤':<15} {'Assessment':<30}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'K-Fold (WRONG)':<25} {kfold_scores.mean():<15.6f} {kfold_scores.std():<15.6f} \"\n",
    "          f\"{'OVERLY OPTIMISTIC ‚ùå':<30}\")\n",
    "    print(f\"{'Time Series Split':<25} {tscv_scores.mean():<15.6f} {tscv_scores.std():<15.6f} \"\n",
    "          f\"{'REALISTIC ‚úÖ':<30}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"\\\\nOptimism bias: {(kfold_scores.mean() - tscv_scores.mean()):.6f}\")\n",
    "    print(f\"Percentage overestimation: {((kfold_scores.mean() - tscv_scores.mean()) / tscv_scores.mean() * 100):.2f}%\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    folds = np.arange(1, n_splits + 1)\n",
    "    \n",
    "    # Plot scores\n",
    "    axes[0].plot(folds, kfold_scores, 'o-', color='red', linewidth=2, \n",
    "                markersize=8, label=f'K-Fold (Mean={kfold_scores.mean():.4f})')\n",
    "    axes[0].plot(folds, tscv_scores, 'o-', color='green', linewidth=2, \n",
    "                markersize=8, label=f'Time Series Split (Mean={tscv_scores.mean():.4f})')\n",
    "    \n",
    "    axes[0].set_xlabel('Fold Number', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_ylabel('R¬≤ Score', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_title('Performance Comparison\\\\n(K-Fold overestimates!)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    axes[0].set_xticks(folds)\n",
    "    \n",
    "    # Box plot\n",
    "    bp = axes[1].boxplot([kfold_scores, tscv_scores], \n",
    "                         labels=['K-Fold\\\\n(WRONG)', 'Time Series\\\\n(CORRECT)'],\n",
    "                         patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightcoral')\n",
    "    bp['boxes'][1].set_facecolor('lightgreen')\n",
    "    \n",
    "    axes[1].set_ylabel('R¬≤ Score', fontsize=11, fontweight='bold')\n",
    "    axes[1].set_title('Distribution Comparison', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\\\nEXAMPLE: Time Series Cross-Validation for Semiconductor Test Time Prediction\\\\n\")\n",
    "    \n",
    "    # Generate time-ordered test time data (with trend)\n",
    "    n_weeks = 52\n",
    "    n_devices_per_week = 50\n",
    "    n_samples = n_weeks * n_devices_per_week\n",
    "    \n",
    "    # Time-varying features (simulating process drift)\n",
    "    time = np.arange(n_samples)\n",
    "    complexity = 0.5 + 0.01 * (time / n_samples)  # Increasing complexity over time\n",
    "    n_test_points = 20 + 10 * np.sin(2 * np.pi * time / (n_devices_per_week * 4))  # Seasonal\n",
    "    \n",
    "    X = np.column_stack([\n",
    "        complexity + 0.1 * np.random.randn(n_samples),\n",
    "        n_test_points + 5 * np.random.randn(n_samples),\n",
    "        np.random.normal(2000, 200, n_samples),  # Frequency\n",
    "    ])\n",
    "    \n",
    "    # Test time increases over time (process drift) + noise\n",
    "    y = (10 + 30 * complexity + 0.5 * n_test_points + \n",
    "         5 * np.random.randn(n_samples))\n",
    "    \n",
    "    print(f\"Dataset: {n_samples} samples ({n_weeks} weeks)\")\n",
    "    print(f\"Features: complexity (trending up), n_test_points (seasonal), frequency\")\n",
    "    print(f\"Target: test_time_ms (with process drift)\\\\n\")\n",
    "    \n",
    "    # Time Series CV\n",
    "    validator = TimeSeriesValidator(n_splits=5)\n",
    "    results = validator.evaluate(LinearRegression(), X, y, scoring='r2')\n",
    "    validator.print_summary()\n",
    "    validator.plot_time_series_splits()\n",
    "    \n",
    "    # Comparison\n",
    "    print(\"\\\\n\\\\n\")\n",
    "    compare_kfold_vs_timeseries(X, y, n_splits=5)\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {},\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc9142b",
   "metadata": {},
   "source": [
    "## üîÅ Nested Cross-Validation: Unbiased Hyperparameter Tuning\n",
    "\n",
    "**Critical Problem**: When you tune hyperparameters using CV and report those CV scores, you get **optimistically biased estimates**.\n",
    "\n",
    "### The Hyperparameter Tuning Bias\n",
    "\n",
    "**Naive approach** (WRONG):\n",
    "```python\n",
    "# WRONG: Optimistic bias!\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "print(f\"Best score: {grid_search.best_score_}\")  # ‚Üê BIASED (too optimistic)\n",
    "```\n",
    "\n",
    "**Problem**: `best_score_` was obtained by selecting the best hyperparameters from 5 folds. This is **data snooping** - you peeked at validation performance to choose parameters!\n",
    "\n",
    "**Result**: Reported score is higher than true generalization performance.\n",
    "\n",
    "### Nested Cross-Validation Solution\n",
    "\n",
    "**Idea**: Use **two nested CV loops**:\n",
    "- **Outer loop**: Estimates true performance (unbiased)\n",
    "- **Inner loop**: Tunes hyperparameters (on training set only)\n",
    "\n",
    "```\n",
    "Outer Fold 1:\n",
    "    Inner CV on Train ‚Üí Find best params ‚Üí Evaluate on Outer Val ‚Üí Score 1\n",
    "Outer Fold 2:\n",
    "    Inner CV on Train ‚Üí Find best params ‚Üí Evaluate on Outer Val ‚Üí Score 2\n",
    "...\n",
    "Outer Fold K:\n",
    "    Inner CV on Train ‚Üí Find best params ‚Üí Evaluate on Outer Val ‚Üí Score K\n",
    "\n",
    "Average scores ‚Üí Unbiased performance estimate\n",
    "```\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given dataset $D$, split into K outer folds: $D = D_1 \\\\cup D_2 \\\\cup ... \\\\cup D_K$\n",
    "\n",
    "**For each outer fold k**:\n",
    "1. **Outer training set**: $D_{train}^{outer} = D \\\\setminus D_k$\n",
    "2. **Inner CV** on $D_{train}^{outer}$:\n",
    "   - Split into J inner folds\n",
    "   - For each hyperparameter config $\\\\theta$:\n",
    "     - Compute inner CV score: $S_{inner}(\\\\theta)$\n",
    "   - Select best: $\\\\theta_k^* = \\\\arg\\\\max_\\\\theta S_{inner}(\\\\theta)$\n",
    "3. **Train final model** on $D_{train}^{outer}$ with $\\\\theta_k^*$\n",
    "4. **Evaluate** on $D_k$ ‚Üí get $S_k^{outer}$\n",
    "\n",
    "**Final unbiased estimate**:\n",
    "$$\n",
    "S_{nested} = \\\\frac{1}{K} \\\\sum_{k=1}^{K} S_k^{outer}\n",
    "$$\n",
    "\n",
    "This is **unbiased** because outer validation sets were never used for hyperparameter selection.\n",
    "\n",
    "### Why Nested CV is Necessary\n",
    "\n",
    "**Experiment**: Compare naive CV vs nested CV\n",
    "\n",
    "**Naive CV** (5-fold with hyperparameter tuning):\n",
    "- Reports: Accuracy = 92%\n",
    "- **But this used all data for hyperparameter selection!**\n",
    "\n",
    "**Nested CV** (5√ó3: 5 outer, 3 inner):\n",
    "- Reports: Accuracy = 88%\n",
    "- **This is the true expected performance on new data**\n",
    "\n",
    "**Bias**: 92% - 88% = 4% optimistic bias!\n",
    "\n",
    "The more hyperparameters you tune, the larger the bias.\n",
    "\n",
    "### Computational Cost\n",
    "\n",
    "**Nested CV is expensive**:\n",
    "- Outer folds: K\n",
    "- Inner folds: J\n",
    "- Total models trained: K √ó J √ó (number of hyperparameter configs)\n",
    "\n",
    "**Example**:\n",
    "- K=5, J=3, 100 hyperparameter configs\n",
    "- Total: 5 √ó 3 √ó 100 = **1,500 model trainings**!\n",
    "\n",
    "**Mitigation strategies**:\n",
    "1. Use fewer outer folds (K=3) for final estimate\n",
    "2. Use fewer inner folds (J=3) for hyperparameter tuning\n",
    "3. Use RandomizedSearchCV instead of GridSearchCV (fewer configs)\n",
    "4. Cache models if possible\n",
    "\n",
    "### When to Use Nested CV\n",
    "\n",
    "‚úÖ **Use nested CV when**:\n",
    "- Publishing research (need unbiased estimates)\n",
    "- Comparing multiple models fairly\n",
    "- Reporting final performance to stakeholders\n",
    "- Hyperparameter tuning is part of workflow\n",
    "\n",
    "‚ùå **Skip nested CV when**:\n",
    "- Just exploring/prototyping (too slow)\n",
    "- Hyperparameters are fixed (no tuning)\n",
    "- Dataset is huge (computational cost prohibitive)\n",
    "\n",
    "### Nested CV vs Hold-Out Test Set\n",
    "\n",
    "**Alternative approach**: Use 3-way split\n",
    "\n",
    "```\n",
    "Data ‚Üí [Train: 60%] [Validation: 20%] [Test: 20%]\n",
    "\n",
    "1. Tune hyperparameters on Train+Val\n",
    "2. Report final performance on Test (never used before)\n",
    "```\n",
    "\n",
    "**Nested CV advantages**:\n",
    "- Uses all data (no held-out test set)\n",
    "- More reliable estimate (averaged over folds)\n",
    "\n",
    "**Hold-out advantages**:\n",
    "- Much faster (1 test evaluation vs K)\n",
    "- Simpler to implement\n",
    "- Test set truly unseen\n",
    "\n",
    "**Rule of thumb**:\n",
    "- Small data (<10K): Use nested CV (can't afford to hold out 20%)\n",
    "- Large data (>100K): Use hold-out test set (faster, simpler)\n",
    "\n",
    "### Semiconductor Example\n",
    "\n",
    "**Scenario**: Tune Random Forest for yield prediction (5,000 devices)\n",
    "\n",
    "**Hyperparameters to tune**: `n_estimators`, `max_depth`, `min_samples_split` (10 configs)\n",
    "\n",
    "**Naive CV** (WRONG):\n",
    "```python\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
    "grid.fit(X, y)\n",
    "print(grid.best_score_)  # Reports: 0.92 (optimistic!)\n",
    "```\n",
    "\n",
    "**Nested CV** (CORRECT):\n",
    "```python\n",
    "# Outer loop: 5-fold\n",
    "# Inner loop: 3-fold for hyperparameter tuning\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "nested_scores = []\n",
    "for train_idx, test_idx in outer_cv.split(X, y):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Inner CV for hyperparameter tuning\n",
    "    grid = GridSearchCV(RandomForestClassifier(), param_grid, \n",
    "                        cv=inner_cv, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate best model on outer test set\n",
    "    score = grid.best_estimator_.score(X_test, y_test)\n",
    "    nested_scores.append(score)\n",
    "\n",
    "print(f\"Nested CV: {np.mean(nested_scores):.4f} ¬± {np.std(nested_scores):.4f}\")\n",
    "# Reports: 0.88 ¬± 0.03 (unbiased!)\n",
    "```\n",
    "\n",
    "**Cost**: 5 outer √ó 3 inner √ó 10 configs = 150 model trainings\n",
    "\n",
    "### Reporting Guidelines\n",
    "\n",
    "When using nested CV, report:\n",
    "\n",
    "1. **Nested CV score** (unbiased estimate): \"Accuracy = 0.88 ¬± 0.03\"\n",
    "2. **Best hyperparameters** (from each outer fold): Shows stability\n",
    "3. **Final model**: Retrain on ALL data with most common best params\n",
    "\n",
    "**Example report**:\n",
    "```\n",
    "Nested Cross-Validation Results (5 outer √ó 3 inner folds):\n",
    "- Accuracy: 0.88 ¬± 0.03 (unbiased estimate)\n",
    "- Best hyperparameters selected per fold:\n",
    "  * Fold 1: n_estimators=100, max_depth=10\n",
    "  * Fold 2: n_estimators=150, max_depth=10\n",
    "  * Fold 3: n_estimators=100, max_depth=12\n",
    "  * Fold 4: n_estimators=100, max_depth=10\n",
    "  * Fold 5: n_estimators=100, max_depth=10\n",
    "- Most common: n_estimators=100, max_depth=10\n",
    "- Final model: Retrained on all data with n_estimators=100, max_depth=10\n",
    "```\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "For **production ML workflows**:\n",
    "\n",
    "1. **Development phase**: Use simple CV or train/val split (fast iteration)\n",
    "2. **Model selection**: Use nested CV to compare models fairly\n",
    "3. **Final deployment**: \n",
    "   - Use nested CV to get unbiased performance estimate\n",
    "   - Retrain on all data with best hyperparameters\n",
    "   - Monitor production metrics (may differ from CV!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42de4a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def nested_cross_validation(\n",
    "    X, y,\n",
    "    model_class,\n",
    "    param_grid: Dict,\n",
    "    outer_cv_splits: int = 5,\n",
    "    inner_cv_splits: int = 3,\n",
    "    scoring: str = 'accuracy'\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Perform nested cross-validation for unbiased performance estimation.\n",
    "    \n",
    "    Args:\n",
    "        X: Features\n",
    "        y: Target\n",
    "        model_class: Sklearn model class (e.g., RandomForestClassifier)\n",
    "        param_grid: Hyperparameter grid for tuning\n",
    "        outer_cv_splits: Number of outer folds\n",
    "        inner_cv_splits: Number of inner folds\n",
    "        scoring: Metric to optimize\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with nested CV results\n",
    "    \"\"\"\n",
    "    outer_cv = StratifiedKFold(n_splits=outer_cv_splits, shuffle=True, random_state=42)\n",
    "    inner_cv = StratifiedKFold(n_splits=inner_cv_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    outer_scores = []\n",
    "    best_params_per_fold = []\n",
    "    inner_best_scores = []\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"NESTED CROSS-VALIDATION: {outer_cv_splits} Outer √ó {inner_cv_splits} Inner Folds\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(outer_cv.split(X, y), 1):\n",
    "        print(f\"\\nOuter Fold {fold_idx}/{outer_cv_splits}:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Inner CV for hyperparameter tuning\n",
    "        grid_search = GridSearchCV(\n",
    "            model_class(),\n",
    "            param_grid,\n",
    "            cv=inner_cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best params and score from inner CV\n",
    "        best_params = grid_search.best_params_\n",
    "        inner_score = grid_search.best_score_\n",
    "        \n",
    "        # Evaluate on outer test set (never used in inner CV)\n",
    "        outer_score = grid_search.best_estimator_.score(X_test, y_test)\n",
    "        \n",
    "        outer_scores.append(outer_score)\n",
    "        best_params_per_fold.append(best_params)\n",
    "        inner_best_scores.append(inner_score)\n",
    "        \n",
    "        print(f\"  Best params: {best_params}\")\n",
    "        print(f\"  Inner CV score: {inner_score:.6f}\")\n",
    "        print(f\"  Outer test score: {outer_score:.6f}\")\n",
    "        print(f\"  Gap (inner - outer): {inner_score - outer_score:.6f}\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Compute statistics\n",
    "    results = {\n",
    "        'outer_scores': np.array(outer_scores),\n",
    "        'inner_scores': np.array(inner_best_scores),\n",
    "        'best_params_per_fold': best_params_per_fold,\n",
    "        'mean_outer_score': np.mean(outer_scores),\n",
    "        'std_outer_score': np.std(outer_scores),\n",
    "        'mean_inner_score': np.mean(inner_best_scores),\n",
    "        'std_inner_score': np.std(inner_best_scores),\n",
    "        'optimism_bias': np.mean(inner_best_scores) - np.mean(outer_scores),\n",
    "        'elapsed_time': elapsed_time,\n",
    "        'outer_cv_splits': outer_cv_splits,\n",
    "        'inner_cv_splits': inner_cv_splits\n",
    "    }\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"NESTED CV SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Outer CV (unbiased): {results['mean_outer_score']:.6f} ¬± {results['std_outer_score']:.6f}\")\n",
    "    print(f\"Inner CV (biased):   {results['mean_inner_score']:.6f} ¬± {results['std_inner_score']:.6f}\")\n",
    "    print(f\"Optimism bias:       {results['optimism_bias']:.6f}\")\n",
    "    print(f\"Total time:          {elapsed_time:.2f} seconds\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compare_naive_vs_nested_cv(\n",
    "    X, y,\n",
    "    model_class,\n",
    "    param_grid: Dict,\n",
    "    outer_cv_splits: int = 5,\n",
    "    inner_cv_splits: int = 3,\n",
    "    scoring: str = 'accuracy'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Compare naive CV (biased) vs nested CV (unbiased).\n",
    "    \n",
    "    Args:\n",
    "        X: Features\n",
    "        y: Target\n",
    "        model_class: Sklearn model class\n",
    "        param_grid: Hyperparameter grid\n",
    "        outer_cv_splits: Outer folds\n",
    "        inner_cv_splits: Inner folds\n",
    "        scoring: Metric\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARISON: NAIVE CV vs NESTED CV\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Naive CV (WRONG: uses all data for hyperparameter tuning)\n",
    "    print(\"\\n[1] Running NAIVE CV (biased)...\")\n",
    "    naive_start = time.time()\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model_class(),\n",
    "        param_grid,\n",
    "        cv=outer_cv_splits,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    naive_time = time.time() - naive_start\n",
    "    naive_score = grid_search.best_score_\n",
    "    naive_params = grid_search.best_params_\n",
    "    \n",
    "    print(f\"  Best score (OPTIMISTIC): {naive_score:.6f}\")\n",
    "    print(f\"  Best params: {naive_params}\")\n",
    "    print(f\"  Time: {naive_time:.2f} seconds\")\n",
    "    \n",
    "    # Nested CV (CORRECT: outer folds never used for hyperparameter tuning)\n",
    "    print(\"\\n[2] Running NESTED CV (unbiased)...\")\n",
    "    nested_results = nested_cross_validation(\n",
    "        X, y, model_class, param_grid, \n",
    "        outer_cv_splits, inner_cv_splits, scoring\n",
    "    )\n",
    "    \n",
    "    # Comparison\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Method':<20} {'Score':<20} {'Assessment':<30}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'Naive CV':<20} {naive_score:<20.6f} {'OPTIMISTIC BIAS ‚ùå':<30}\")\n",
    "    print(f\"{'Nested CV':<20} {nested_results['mean_outer_score']:<20.6f} {'UNBIASED ESTIMATE ‚úÖ':<30}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"\\nOptimism bias: {naive_score - nested_results['mean_outer_score']:.6f}\")\n",
    "    print(f\"Percentage overestimation: {((naive_score - nested_results['mean_outer_score']) / nested_results['mean_outer_score'] * 100):.2f}%\")\n",
    "    print(f\"\\nComputation time:\")\n",
    "    print(f\"  Naive CV:  {naive_time:.2f} seconds\")\n",
    "    print(f\"  Nested CV: {nested_results['elapsed_time']:.2f} seconds (√ó{nested_results['elapsed_time']/naive_time:.1f})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Score comparison\n",
    "    methods = ['Naive CV\\\\n(Biased)', 'Nested CV\\\\n(Unbiased)']\n",
    "    scores = [naive_score, nested_results['mean_outer_score']]\n",
    "    colors = ['red', 'green']\n",
    "    \n",
    "    bars = axes[0].bar(methods, scores, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_title('Score Comparison\\\\n(Naive CV overestimates!)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, scores):\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                    f'{score:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Inner vs Outer scores in nested CV\n",
    "    folds = np.arange(1, outer_cv_splits + 1)\n",
    "    axes[1].plot(folds, nested_results['inner_scores'], 'o-', \n",
    "                color='orange', linewidth=2, markersize=8, \n",
    "                label=f'Inner CV (Mean={nested_results[\"mean_inner_score\"]:.4f})')\n",
    "    axes[1].plot(folds, nested_results['outer_scores'], 'o-', \n",
    "                color='green', linewidth=2, markersize=8, \n",
    "                label=f'Outer CV (Mean={nested_results[\"mean_outer_score\"]:.4f})')\n",
    "    \n",
    "    axes[1].set_xlabel('Fold Number', fontsize=11, fontweight='bold')\n",
    "    axes[1].set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "    axes[1].set_title('Nested CV: Inner vs Outer Scores\\\\n(Gap shows optimism bias)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    axes[1].set_xticks(folds)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nEXAMPLE: Nested CV for Semiconductor Yield Prediction with Hyperparameter Tuning\\n\")\n",
    "    \n",
    "    # Generate synthetic semiconductor data\n",
    "    X, y = make_classification(\n",
    "        n_samples=2000,\n",
    "        n_features=10,\n",
    "        n_informative=8,\n",
    "        n_redundant=2,\n",
    "        n_classes=2,\n",
    "        weights=[0.85, 0.15],  # Imbalanced (15% defect rate)\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset: {len(y)} devices\")\n",
    "    print(f\"Features: 10 parametric test measurements\")\n",
    "    print(f\"Target: Pass/Fail (imbalanced: {(y==0).sum()} pass, {(y==1).sum()} fail)\")\n",
    "    print(f\"Defect rate: {y.mean()*100:.1f}%\")\n",
    "    \n",
    "    # Define hyperparameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nHyperparameter grid: {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split'])} configurations\")\n",
    "    \n",
    "    # Run comparison\n",
    "    compare_naive_vs_nested_cv(\n",
    "        X, y,\n",
    "        RandomForestClassifier,\n",
    "        param_grid,\n",
    "        outer_cv_splits=5,\n",
    "        inner_cv_splits=3,\n",
    "        scoring='f1'\n",
    "    )\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üéØ CV Strategy Selection Guide\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Decision Flowchart\\n\",\n",
    "    \"\\n\",\n",
    "    \"```mermaid\\n\",\n",
    "    \"graph TD\\n\",\n",
    "    \"    A[Need to evaluate model] --> B{Data has<br/>temporal order?}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    B -->|Yes| C[Use Time Series Split]\\n\",\n",
    "    \"    C --> C1{Process drift<br/>expected?}\\n\",\n",
    "    \"    C1 -->|Yes| C2[Monitor performance<br/>trend across folds]\\n\",\n",
    "    \"    C1 -->|No| C3[Standard Time Series CV]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    B -->|No| D{Data has<br/>group structure?}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    D -->|Yes| E[Use Group K-Fold]\\n\",\n",
    "    \"    E --> E1[Keep groups together<br/>in same fold]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    D -->|No| F{Classification<br/>or Regression?}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    F -->|Classification| G{Classes<br/>balanced?}\\n\",\n",
    "    \"    G -->|Yes| H[Use K-Fold]\\n\",\n",
    "    \"    G -->|No| I[Use Stratified K-Fold]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    F -->|Regression| H\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    H --> J{Need to tune<br/>hyperparameters?}\\n\",\n",
    "    \"    I --> J\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    J -->|Yes| K[Use Nested CV]\\n\",\n",
    "    \"    J -->|No| L[Use Simple CV]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    K --> M[Report unbiased<br/>performance]\\n\",\n",
    "    \"    L --> M\\n\",\n",
    "    \"```\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Quick Reference Table\\n\",\n",
    "    \"\\n\",\n",
    "    \"| **Data Characteristic** | **Recommended Strategy** | **Why** |\\n\",\n",
    "    \"|------------------------|-------------------------|----------|\\n\",\n",
    "    \"| **Temporal ordering** | Time Series Split | Prevents data leakage, realistic |\\n\",\n",
    "    \"| **Imbalanced classes** | Stratified K-Fold | Maintains class distribution |\\n\",\n",
    "    \"| **Group structure** (e.g., multiple samples per patient/wafer) | Group K-Fold | Prevents group leakage |\\n\",\n",
    "    \"| **i.i.d. data, balanced** | K-Fold | Simple, standard |\\n\",\n",
    "    \"| **Need hyperparameter tuning** | Nested CV | Unbiased performance |\\n\",\n",
    "    \"| **Small dataset (<1000)** | K=10 or LOO | Maximizes training data |\\n\",\n",
    "    \"| **Large dataset (>100K)** | K=3 or K=5 | Reduces computation |\\n\",\n",
    "    \"| **Multi-class** | Stratified K-Fold | Ensures all classes in every fold |\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Semiconductor-Specific Guidelines\\n\",\n",
    "    \"\\n\",\n",
    "    \"| **Scenario** | **CV Strategy** | **Specific Considerations** |\\n\",\n",
    "    \"|--------------|----------------|---------------------------|\\n\",\n",
    "    \"| **Wafer-level models** | Group K-Fold by wafer_id | Multiple dies from same wafer correlated |\\n\",\n",
    "    \"| **Lot-based analysis** | Group K-Fold by lot_id | Manufacturing lots share process conditions |\\n\",\n",
    "    \"| **Production time series** | Time Series Split | Process drift, equipment aging |\\n\",\n",
    "    \"| **Spatial yield models** | Stratified K-Fold by yield bins | Maintain yield distribution |\\n\",\n",
    "    \"| **Test time prediction** | Time Series Split | Test program evolves over time |\\n\",\n",
    "    \"| **Defect detection** (rare) | Stratified K-Fold | Maintain low defect rate in each fold |\\n\",\n",
    "    \"| **Multi-fab comparison** | Group K-Fold by fab_id | Fab-specific characteristics |\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Practical Recommendations\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### Development Phase (Fast Iteration)\\n\",\n",
    "    \"- Use simple train/val split (80/20) or 3-Fold CV\\n\",\n",
    "    \"- Focus on model development, not rigorous evaluation\\n\",\n",
    "    \"- Accept higher variance for speed\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### Model Selection (Compare Algorithms)\\n\",\n",
    "    \"- Use 5-Fold CV (or stratified/time series variant)\\n\",\n",
    "    \"- Report mean ¬± std for each model\\n\",\n",
    "    \"- Use statistical tests (paired t-test, McNemar) to compare\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### Final Evaluation (Production Readiness)\\n\",\n",
    "    \"- Use 10-Fold CV or nested CV\\n\",\n",
    "    \"- Report confidence intervals\\n\",\n",
    "    \"- Include multiple metrics (not just accuracy)\\n\",\n",
    "    \"- Document CV strategy in model card\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### Research/Publication\\n\",\n",
    "    \"- Use nested CV for hyperparameter tuning\\n\",\n",
    "    \"- Report both inner and outer CV scores\\n\",\n",
    "    \"- Use multiple random seeds to verify stability\\n\",\n",
    "    \"- Provide full reproducibility details\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Common Mistakes to Avoid\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### ‚ùå Mistake 1: Using K-Fold on Time Series\\n\",\n",
    "    \"**Problem**: Data leakage (training on future)\\n\",\n",
    "    \"**Solution**: Always use Time Series Split\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### ‚ùå Mistake 2: Reporting Inner CV Score as Final Performance\\n\",\n",
    "    \"**Problem**: Optimistic bias from hyperparameter tuning\\n\",\n",
    "    \"**Solution**: Use nested CV and report outer CV score\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### ‚ùå Mistake 3: Not Stratifying Imbalanced Classes\\n\",\n",
    "    \"**Problem**: High variance in metrics across folds\\n\",\n",
    "    \"**Solution**: Use Stratified K-Fold\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### ‚ùå Mistake 4: Splitting Groups Across Folds\\n\",\n",
    "    \"**Problem**: Leakage from correlated samples (same wafer, patient)\\n\",\n",
    "    \"**Solution**: Use Group K-Fold\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### ‚ùå Mistake 5: Not Checking for Process Drift\\n\",\n",
    "    \"**Problem**: Model performs well in CV but degrades in production\\n\",\n",
    "    \"**Solution**: Use Time Series Split and monitor trend\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Computational Considerations\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### Time Complexity\\n\",\n",
    "    \"- **K-Fold**: K √ó T (where T = model training time)\\n\",\n",
    "    \"- **Nested CV**: K_outer √ó K_inner √ó N_configs √ó T\\n\",\n",
    "    \"- **LOO**: n √ó T (where n = number of samples)\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### Memory Requirements\\n\",\n",
    "    \"- K-Fold: Single model in memory\\n\",\n",
    "    \"- Nested CV: Single model (sequential)\\n\",\n",
    "    \"- Parallelization: Can run folds in parallel (increases memory)\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### Speed Optimization Tips\\n\",\n",
    "    \"1. Use `n_jobs=-1` for parallel fold execution\\n\",\n",
    "    \"2. Reduce K for large datasets (K=3 or K=5)\\n\",\n",
    "    \"3. Use RandomizedSearchCV instead of GridSearchCV\\n\",\n",
    "    \"4. Cache computations when possible\\n\",\n",
    "    \"5. Use early stopping for iterative models\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Integration with Production Workflow\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### Model Card Documentation\\n\",\n",
    "    \"Include in model card:\\n\",\n",
    "    \"```yaml\\n\",\n",
    "    \"validation:\\n\",\n",
    "    \"  strategy: Stratified 5-Fold Cross-Validation\\n\",\n",
    "    \"  outer_folds: 5\\n\",\n",
    "    \"  inner_folds: 3  # if nested\\n\",\n",
    "    \"  metric: F1-Score\\n\",\n",
    "    \"  performance: 0.88 ¬± 0.03\\n\",\n",
    "    \"  confidence_interval: [0.82, 0.94] (95%)\\n\",\n",
    "    \"  hyperparameters: \\n\",\n",
    "    \"    tuning_method: GridSearchCV\\n\",\n",
    "    \"    best_params: {n_estimators: 100, max_depth: 10}\\n\",\n",
    "    \"  notes: Imbalanced dataset (15% positive class)\\n\",\n",
    "    \"```\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### Monitoring in Production\\n\",\n",
    "    \"After deployment:\\n\",\n",
    "    \"1. Compare production metrics to CV estimates\\n\",\n",
    "    \"2. Alert if performance drops below CV - 2œÉ\\n\",\n",
    "    \"3. Re-run CV periodically on new data\\n\",\n",
    "    \"4. Retrain when CV performance degrades\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {},\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517daad7",
   "metadata": {},
   "source": [
    "## üë• Group K-Fold: Handling Clustered Data\n",
    "\n",
    "### The Problem: Group Leakage\n",
    "\n",
    "Many real-world datasets have **group structure** where multiple samples belong to the same underlying entity:\n",
    "- **Medical**: Multiple measurements from same patient\n",
    "- **Semiconductor**: Multiple dies from same wafer\n",
    "- **Finance**: Multiple transactions from same customer\n",
    "- **Education**: Multiple test scores from same student\n",
    "\n",
    "**Critical issue**: If samples from the same group appear in both training and test sets, the model learns group-specific patterns rather than generalizable patterns.\n",
    "\n",
    "### How Group K-Fold Works\n",
    "\n",
    "1. **Group identification**: Each sample has a group label (wafer_id, patient_id, etc.)\n",
    "2. **Group-level splitting**: Groups (not samples) are divided into K folds\n",
    "3. **No group leakage**: All samples from a group stay in the same fold\n",
    "4. **Cross-validation**: Standard K-Fold procedure on group assignments\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Let $G = \\{g_1, g_2, ..., g_m\\}$ be the set of groups.\n",
    "\n",
    "**Standard K-Fold** (WRONG):\n",
    "- Randomly split samples ‚Üí Group $g_i$ may appear in both train and test\n",
    "\n",
    "**Group K-Fold** (CORRECT):\n",
    "- Split groups into K folds: $G = F_1 \\cup F_2 \\cup ... \\cup F_K$\n",
    "- For fold $k$: Train on $\\bigcup_{i \\neq k} F_i$, test on $F_k$\n",
    "- Guarantee: $F_i \\cap F_j = \\emptyset$ for $i \\neq j$\n",
    "\n",
    "### Semiconductor Example: Wafer-Level Yield Prediction\n",
    "\n",
    "**Scenario**: Predict device yield from parametric tests\n",
    "\n",
    "**Data structure**:\n",
    "```\n",
    "Wafer 001: 100 dies ‚Üí Die A, Die B, Die C, ...\n",
    "Wafer 002: 100 dies ‚Üí Die A, Die B, Die C, ...\n",
    "Wafer 003: 100 dies ‚Üí Die A, Die B, Die C, ...\n",
    "...\n",
    "```\n",
    "\n",
    "**Problem with Standard K-Fold**:\n",
    "- Dies from Wafer 001 in training\n",
    "- Dies from Wafer 001 in testing\n",
    "- Model learns wafer-specific patterns (spatial correlations, fab conditions)\n",
    "- **Production failure**: New wafer comes ‚Üí model doesn't generalize\n",
    "\n",
    "**Example**:\n",
    "- Standard K-Fold: 95% accuracy (overly optimistic)\n",
    "- Group K-Fold: 88% accuracy (realistic, generalizes to new wafers)\n",
    "\n",
    "### When to Use Group K-Fold\n",
    "\n",
    "| **Use Case** | **Group By** | **Why** |\n",
    "|--------------|--------------|---------|\n",
    "| ‚úÖ **Medical data** | patient_id | Multiple visits/measurements per patient |\n",
    "| ‚úÖ **Semiconductor** | wafer_id, lot_id | Spatial/process correlations |\n",
    "| ‚úÖ **Finance** | customer_id | Customer-specific behavior patterns |\n",
    "| ‚úÖ **Image classification** | scene_id | Multiple images from same scene |\n",
    "| ‚úÖ **Time series** | entity_id | Multiple time points per entity |\n",
    "| ‚ùå **i.i.d. samples** | (none) | Use standard K-Fold |\n",
    "| ‚ùå **Single measurement per entity** | (none) | No group structure |\n",
    "\n",
    "### Comparison: Standard vs Group K-Fold\n",
    "\n",
    "| **Aspect** | **Standard K-Fold** | **Group K-Fold** |\n",
    "|------------|---------------------|------------------|\n",
    "| **Splitting** | Random samples | Random groups |\n",
    "| **Leakage risk** | High (if groups exist) | None |\n",
    "| **Performance estimate** | Optimistic | Realistic |\n",
    "| **Production generalization** | Poor | Good |\n",
    "| **Variance** | Lower | Higher (fewer \"effective\" samples) |\n",
    "| **Fold sizes** | Balanced samples | May be imbalanced (group sizes vary) |\n",
    "\n",
    "### Implementation Considerations\n",
    "\n",
    "#### Unbalanced Folds\n",
    "**Problem**: Groups have different sizes\n",
    "```\n",
    "Group A: 5 samples\n",
    "Group B: 100 samples\n",
    "Group C: 10 samples\n",
    "```\n",
    "- Fold 1 may have 115 samples, Fold 2 may have 5 samples\n",
    "\n",
    "**Solution**:\n",
    "- Use `StratifiedGroupKFold` to balance target distribution\n",
    "- Monitor fold size variance\n",
    "- Consider using more folds (K=10 instead of K=5)\n",
    "\n",
    "#### Small Number of Groups\n",
    "**Problem**: If only 10 groups exist, K=5 means only 2 groups per fold\n",
    "- High variance in estimates\n",
    "\n",
    "**Solution**:\n",
    "- Use Leave-One-Group-Out (LOGO)\n",
    "- Collect more groups if possible\n",
    "- Report uncertainty honestly\n",
    "\n",
    "#### Hierarchical Groups\n",
    "**Example**: Dies within wafers, wafers within lots\n",
    "- Group by highest level (lot_id)\n",
    "- Alternative: Nested CV (outer=lots, inner=wafers)\n",
    "\n",
    "### Semiconductor-Specific Patterns\n",
    "\n",
    "#### Spatial Correlation (Wafer Map)\n",
    "```\n",
    "Wafer layout:\n",
    "[Good] [Good] [Fail] [Fail]\n",
    "[Good] [Good] [Fail] [Fail]\n",
    "[Good] [Fail] [Fail] [Fail]\n",
    "```\n",
    "- Dies near each other have correlated outcomes\n",
    "- Must group by wafer to avoid spatial leakage\n",
    "\n",
    "#### Lot-Based Process Drift\n",
    "```\n",
    "Lot 001 (Week 1): High yield (98%)\n",
    "Lot 002 (Week 2): Medium yield (95%)\n",
    "Lot 003 (Week 3): Low yield (92%)\n",
    "```\n",
    "- Group by lot_id to test generalization across process conditions\n",
    "- Mimics production: Model trained on past lots ‚Üí predicts new lots\n",
    "\n",
    "#### Multi-Fab Analysis\n",
    "```\n",
    "Fab A: 1000 wafers\n",
    "Fab B: 800 wafers\n",
    "Fab C: 500 wafers\n",
    "```\n",
    "- Group by fab_id to test cross-fab generalization\n",
    "- Critical for models deployed across multiple fabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde75f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold, LeaveOneGroupOut, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "class GroupCVEvaluator:\n",
    "    \"\"\"\n",
    "    Cross-validation evaluator for grouped data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits: int = 5, cv_type: str = 'group'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_splits: Number of folds (ignored for LOGO)\n",
    "            cv_type: 'group' (GroupKFold) or 'logo' (LeaveOneGroupOut)\n",
    "        \"\"\"\n",
    "        self.n_splits = n_splits\n",
    "        self.cv_type = cv_type\n",
    "        \n",
    "        if cv_type == 'group':\n",
    "            self.cv = GroupKFold(n_splits=n_splits)\n",
    "        elif cv_type == 'logo':\n",
    "            self.cv = LeaveOneGroupOut()\n",
    "        else:\n",
    "            raise ValueError(\"cv_type must be 'group' or 'logo'\")\n",
    "    \n",
    "    def evaluate(self, model, X, y, groups, scoring='r2') -> Dict:\n",
    "        \"\"\"\n",
    "        Perform group cross-validation.\n",
    "        \n",
    "        Args:\n",
    "            model: Sklearn model\n",
    "            X: Features\n",
    "            y: Target\n",
    "            groups: Group labels for each sample\n",
    "            scoring: 'r2' or 'mae'\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with CV results\n",
    "        \"\"\"\n",
    "        train_scores = []\n",
    "        test_scores = []\n",
    "        group_info = []\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"GROUP CROSS-VALIDATION: {self.cv_type.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Total samples: {len(y)}\")\n",
    "        print(f\"Total groups: {len(np.unique(groups))}\")\n",
    "        print(f\"Samples per group (avg): {len(y) / len(np.unique(groups)):.1f}\")\n",
    "        \n",
    "        for fold_idx, (train_idx, test_idx) in enumerate(self.cv.split(X, y, groups), 1):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            groups_train = groups[train_idx]\n",
    "            groups_test = groups[test_idx]\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            if scoring == 'r2':\n",
    "                train_score = model.score(X_train, y_train)\n",
    "                test_score = model.score(X_test, y_test)\n",
    "            elif scoring == 'mae':\n",
    "                train_score = -mean_absolute_error(y_train, model.predict(X_train))\n",
    "                test_score = -mean_absolute_error(y_test, model.predict(X_test))\n",
    "            \n",
    "            train_scores.append(train_score)\n",
    "            test_scores.append(test_score)\n",
    "            \n",
    "            # Group info\n",
    "            group_info.append({\n",
    "                'fold': fold_idx,\n",
    "                'train_samples': len(train_idx),\n",
    "                'test_samples': len(test_idx),\n",
    "                'train_groups': len(np.unique(groups_train)),\n",
    "                'test_groups': len(np.unique(groups_test)),\n",
    "                'train_groups_list': list(np.unique(groups_train)),\n",
    "                'test_groups_list': list(np.unique(groups_test))\n",
    "            })\n",
    "            \n",
    "            print(f\"\\nFold {fold_idx}:\")\n",
    "            print(f\"  Train: {len(train_idx)} samples, {len(np.unique(groups_train))} groups\")\n",
    "            print(f\"  Test:  {len(test_idx)} samples, {len(np.unique(groups_test))} groups\")\n",
    "            print(f\"  Train score: {train_score:.6f}\")\n",
    "            print(f\"  Test score:  {test_score:.6f}\")\n",
    "        \n",
    "        results = {\n",
    "            'train_scores': np.array(train_scores),\n",
    "            'test_scores': np.array(test_scores),\n",
    "            'group_info': group_info,\n",
    "            'mean_train': np.mean(train_scores),\n",
    "            'std_train': np.std(train_scores),\n",
    "            'mean_test': np.mean(test_scores),\n",
    "            'std_test': np.std(test_scores),\n",
    "            'cv_type': self.cv_type,\n",
    "            'n_splits': len(train_scores)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Train: {results['mean_train']:.6f} ¬± {results['std_train']:.6f}\")\n",
    "        print(f\"Test:  {results['mean_test']:.6f} ¬± {results['std_test']:.6f}\")\n",
    "        print(f\"Generalization gap: {results['mean_train'] - results['mean_test']:.6f}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_results(self, results: Dict, comparison_results: Dict = None):\n",
    "        \"\"\"\n",
    "        Visualize group CV results.\n",
    "        \n",
    "        Args:\n",
    "            results: Group CV results\n",
    "            comparison_results: Optional standard K-Fold results for comparison\n",
    "        \"\"\"\n",
    "        if comparison_results:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        else:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            axes = axes.reshape(1, 2)\n",
    "        \n",
    "        # Plot 1: Fold performance (Group CV)\n",
    "        folds = np.arange(1, results['n_splits'] + 1)\n",
    "        axes[0, 0].plot(folds, results['train_scores'], 'o-', \n",
    "                       color='blue', linewidth=2, markersize=8, label='Train')\n",
    "        axes[0, 0].plot(folds, results['test_scores'], 'o-', \n",
    "                       color='green', linewidth=2, markersize=8, label='Test')\n",
    "        axes[0, 0].axhline(results['mean_test'], color='green', \n",
    "                          linestyle='--', alpha=0.7, label=f'Mean Test ({results[\"mean_test\"]:.4f})')\n",
    "        axes[0, 0].fill_between(folds, \n",
    "                                results['mean_test'] - results['std_test'],\n",
    "                                results['mean_test'] + results['std_test'],\n",
    "                                alpha=0.2, color='green')\n",
    "        axes[0, 0].set_xlabel('Fold Number', fontsize=11, fontweight='bold')\n",
    "        axes[0, 0].set_ylabel('Score (R¬≤)', fontsize=11, fontweight='bold')\n",
    "        axes[0, 0].set_title(f'Group CV Performance\\\\n({results[\"cv_type\"].upper()})', \n",
    "                            fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(alpha=0.3)\n",
    "        axes[0, 0].set_xticks(folds)\n",
    "        \n",
    "        # Plot 2: Fold size distribution (Group CV)\n",
    "        fold_nums = [info['fold'] for info in results['group_info']]\n",
    "        train_samples = [info['train_samples'] for info in results['group_info']]\n",
    "        test_samples = [info['test_samples'] for info in results['group_info']]\n",
    "        \n",
    "        x = np.arange(len(fold_nums))\n",
    "        width = 0.35\n",
    "        axes[0, 1].bar(x - width/2, train_samples, width, label='Train', \n",
    "                      color='blue', alpha=0.7, edgecolor='black')\n",
    "        axes[0, 1].bar(x + width/2, test_samples, width, label='Test', \n",
    "                      color='green', alpha=0.7, edgecolor='black')\n",
    "        axes[0, 1].set_xlabel('Fold Number', fontsize=11, fontweight='bold')\n",
    "        axes[0, 1].set_ylabel('Number of Samples', fontsize=11, fontweight='bold')\n",
    "        axes[0, 1].set_title('Fold Size Distribution\\\\n(May be imbalanced with groups)', \n",
    "                            fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].set_xticks(x)\n",
    "        axes[0, 1].set_xticklabels(fold_nums)\n",
    "        axes[0, 1].grid(alpha=0.3, axis='y')\n",
    "        \n",
    "        if comparison_results:\n",
    "            # Plot 3: Comparison - Performance\n",
    "            methods = ['Standard K-Fold\\\\n(Biased)', f'Group CV\\\\n(Unbiased)']\n",
    "            scores = [comparison_results['mean_test'], results['mean_test']]\n",
    "            colors = ['red', 'green']\n",
    "            \n",
    "            bars = axes[1, 0].bar(methods, scores, color=colors, alpha=0.7, edgecolor='black')\n",
    "            axes[1, 0].set_ylabel('Test Score (R¬≤)', fontsize=11, fontweight='bold')\n",
    "            axes[1, 0].set_title('Performance Comparison\\\\n(Group CV shows realistic performance)', \n",
    "                                fontsize=12, fontweight='bold')\n",
    "            axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "            \n",
    "            for bar, score in zip(bars, scores):\n",
    "                height = bar.get_height()\n",
    "                axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                               f'{score:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            # Add optimism bias annotation\n",
    "            optimism = comparison_results['mean_test'] - results['mean_test']\n",
    "            axes[1, 0].annotate(f'Optimism bias: {optimism:.4f}', \n",
    "                               xy=(0.5, max(scores) * 0.95), \n",
    "                               ha='center', fontsize=10, \n",
    "                               bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "            \n",
    "            # Plot 4: Distribution comparison\n",
    "            data_to_plot = [comparison_results['test_scores'], results['test_scores']]\n",
    "            bp = axes[1, 1].boxplot(data_to_plot, labels=methods, patch_artist=True)\n",
    "            for patch, color in zip(bp['boxes'], colors):\n",
    "                patch.set_facecolor(color)\n",
    "                patch.set_alpha(0.7)\n",
    "            axes[1, 1].set_ylabel('Test Score (R¬≤)', fontsize=11, fontweight='bold')\n",
    "            axes[1, 1].set_title('Score Distribution\\\\n(Group CV has higher variance)', \n",
    "                                fontsize=12, fontweight='bold')\n",
    "            axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def generate_wafer_data(n_wafers: int = 20, dies_per_wafer: int = 100) -> Tuple:\n",
    "    \"\"\"\n",
    "    Generate synthetic semiconductor wafer data with spatial correlation.\n",
    "    \n",
    "    Args:\n",
    "        n_wafers: Number of wafers\n",
    "        dies_per_wafer: Number of dies per wafer\n",
    "        \n",
    "    Returns:\n",
    "        X, y, groups (wafer IDs)\n",
    "    \"\"\"\n",
    "    n_samples = n_wafers * dies_per_wafer\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    groups = []\n",
    "    \n",
    "    for wafer_id in range(n_wafers):\n",
    "        # Wafer-level effects (process variation)\n",
    "        wafer_offset = np.random.normal(0, 0.15)\n",
    "        \n",
    "        for die_id in range(dies_per_wafer):\n",
    "            # Die-level features (parametric tests)\n",
    "            features = np.random.normal(0, 1, 5)\n",
    "            \n",
    "            # Target with wafer-level correlation\n",
    "            target = (\n",
    "                2.0 * features[0] +\n",
    "                1.5 * features[1] +\n",
    "                1.0 * features[2] +\n",
    "                wafer_offset +  # Wafer-level effect!\n",
    "                np.random.normal(0, 0.1)\n",
    "            )\n",
    "            \n",
    "            X.append(features)\n",
    "            y.append(target)\n",
    "            groups.append(wafer_id)\n",
    "    \n",
    "    return np.array(X), np.array(y), np.array(groups)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nEXAMPLE: Group CV for Wafer-Level Yield Prediction\\n\")\n",
    "    \n",
    "    # Generate wafer data\n",
    "    X, y, groups = generate_wafer_data(n_wafers=20, dies_per_wafer=100)\n",
    "    \n",
    "    print(f\"Dataset: {len(y)} dies from {len(np.unique(groups))} wafers\")\n",
    "    print(f\"Dies per wafer: {len(y) // len(np.unique(groups))}\")\n",
    "    print(f\"Features: 5 parametric test measurements\")\n",
    "    print(f\"Target: Yield-related metric\")\n",
    "    print(f\"Key: Data has wafer-level spatial correlation\")\n",
    "    \n",
    "    # Standard K-Fold (WRONG - suffers from group leakage)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[1] STANDARD K-FOLD (WRONG - Group Leakage!)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    standard_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    standard_scores = []\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(standard_cv.split(X), 1):\n",
    "        model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "        model.fit(X[train_idx], y[train_idx])\n",
    "        score = model.score(X[test_idx], y[test_idx])\n",
    "        standard_scores.append(score)\n",
    "        \n",
    "        # Check for group leakage\n",
    "        train_groups = set(groups[train_idx])\n",
    "        test_groups = set(groups[test_idx])\n",
    "        overlap = train_groups & test_groups\n",
    "        \n",
    "        print(f\"Fold {fold_idx}: R¬≤ = {score:.6f}, Group overlap = {len(overlap)} wafers ‚ùå\")\n",
    "    \n",
    "    standard_results = {\n",
    "        'test_scores': np.array(standard_scores),\n",
    "        'mean_test': np.mean(standard_scores),\n",
    "        'std_test': np.std(standard_scores)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nStandard K-Fold: {standard_results['mean_test']:.6f} ¬± {standard_results['std_test']:.6f}\")\n",
    "    print(\"WARNING: This is OPTIMISTIC due to group leakage!\")\n",
    "    \n",
    "    # Group K-Fold (CORRECT - no group leakage)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"[2] GROUP K-FOLD (CORRECT - No Group Leakage)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "    evaluator = GroupCVEvaluator(n_splits=5, cv_type='group')\n",
    "    group_results = evaluator.evaluate(model, X, y, groups, scoring='r2')\n",
    "    \n",
    "    # Verify no group overlap\n",
    "    print(\"\\nVerifying no group leakage:\")\n",
    "    for info in group_results['group_info']:\n",
    "        train_groups_set = set(info['train_groups_list'])\n",
    "        test_groups_set = set(info['test_groups_list'])\n",
    "        overlap = train_groups_set & test_groups_set\n",
    "        print(f\"  Fold {info['fold']}: Overlap = {len(overlap)} ‚úÖ\")\n",
    "    \n",
    "    # Comparison\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Standard K-Fold: {standard_results['mean_test']:.6f} ¬± {standard_results['std_test']:.6f} ‚ùå (Optimistic)\")\n",
    "    print(f\"Group K-Fold:    {group_results['mean_test']:.6f} ¬± {group_results['std_test']:.6f} ‚úÖ (Realistic)\")\n",
    "    print(f\"Optimism bias:   {standard_results['mean_test'] - group_results['mean_test']:.6f}\")\n",
    "    print(f\"Production generalization: Group CV gives realistic estimate for NEW WAFERS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Visualization\n",
    "    evaluator.plot_results(group_results, standard_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb86b4b",
   "metadata": {},
   "source": [
    "## üî¨ Complete Example 1: Semiconductor Test Time Optimization\n",
    "\n",
    "### Problem Statement\n",
    "A semiconductor test engineer needs to predict final test time for new devices based on early parametric measurements. The goal is to optimize test scheduling and resource allocation.\n",
    "\n",
    "### Dataset Characteristics\n",
    "- **Temporal data**: 52 weeks of production data\n",
    "- **Features**: Device complexity score, number of test points, operating frequency, power consumption, temperature\n",
    "- **Target**: Test time in milliseconds\n",
    "- **Challenge**: Process drift over time (equipment aging, test program updates)\n",
    "\n",
    "### Why Standard K-Fold Fails\n",
    "Standard K-Fold randomly shuffles data, training on Week 40-52 to predict Week 1-10. This is **impossible in production** where you can only use past data to predict future.\n",
    "\n",
    "### Solution: Time Series Cross-Validation\n",
    "Use forward chaining with expanding window to realistically simulate production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2838a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate realistic test time data with temporal patterns\n",
    "def generate_test_time_data(n_weeks=52, devices_per_week=50):\n",
    "    \"\"\"Generate semiconductor test time data with process drift.\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for week in range(n_weeks):\n",
    "        # Process drift: Equipment aging (test time increases over time)\n",
    "        drift = 0.005 * week\n",
    "        \n",
    "        # Seasonal pattern: Quarterly maintenance cycles\n",
    "        seasonal = 0.1 * np.sin(2 * np.pi * week / 13)\n",
    "        \n",
    "        for device in range(devices_per_week):\n",
    "            # Device features\n",
    "            complexity = np.random.uniform(50, 150)  # Complexity score\n",
    "            n_test_points = np.random.randint(100, 500)  # Number of tests\n",
    "            frequency = np.random.uniform(1.0, 3.5)  # GHz\n",
    "            power = np.random.uniform(5, 25)  # Watts\n",
    "            temperature = np.random.normal(25, 2)  # Celsius\n",
    "            \n",
    "            # Test time model\n",
    "            base_time = (\n",
    "                0.5 * complexity +\n",
    "                0.3 * n_test_points +\n",
    "                50 * frequency +\n",
    "                10 * power +\n",
    "                2 * temperature\n",
    "            )\n",
    "            \n",
    "            # Add drift and seasonal effects\n",
    "            test_time = base_time * (1 + drift + seasonal) + np.random.normal(0, 50)\n",
    "            \n",
    "            data.append({\n",
    "                'week': week,\n",
    "                'complexity': complexity,\n",
    "                'n_test_points': n_test_points,\n",
    "                'frequency': frequency,\n",
    "                'power': power,\n",
    "                'temperature': temperature,\n",
    "                'test_time_ms': test_time\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate data\n",
    "print(\"=\"*80)\n",
    "print(\"COMPLETE EXAMPLE: SEMICONDUCTOR TEST TIME PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n[1] Generating Data...\")\n",
    "\n",
    "df = generate_test_time_data(n_weeks=52, devices_per_week=50)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(df)} device measurements\")\n",
    "print(f\"   Timespan: {df['week'].min()} to {df['week'].max()} weeks\")\n",
    "print(f\"   Features: complexity, n_test_points, frequency, power, temperature\")\n",
    "print(f\"   Target: test_time_ms (mean={df['test_time_ms'].mean():.1f} ms)\")\n",
    "\n",
    "# Prepare data\n",
    "X = df[['complexity', 'n_test_points', 'frequency', 'power', 'temperature']].values\n",
    "y = df['test_time_ms'].values\n",
    "\n",
    "print(\"\\n[2] Comparing Cross-Validation Strategies...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Strategy 1: Standard K-Fold (WRONG for time series)\n",
    "print(\"\\nüìâ STRATEGY 1: Standard K-Fold (WRONG - Data Leakage!)\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kfold_scores = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(X), 1):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X[train_idx])\n",
    "    X_test_scaled = scaler.transform(X[test_idx])\n",
    "    \n",
    "    model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_scaled, y[train_idx])\n",
    "    \n",
    "    score = model.score(X_test_scaled, y[test_idx])\n",
    "    mae = mean_absolute_error(y[test_idx], model.predict(X_test_scaled))\n",
    "    \n",
    "    kfold_scores.append(score)\n",
    "    \n",
    "    # Check temporal leakage\n",
    "    train_weeks = df.iloc[train_idx]['week']\n",
    "    test_weeks = df.iloc[test_idx]['week']\n",
    "    print(f\"  Fold {fold_idx}: R¬≤={score:.4f}, MAE={mae:.1f} ms\")\n",
    "    print(f\"    Train weeks: {train_weeks.min()}-{train_weeks.max()}\")\n",
    "    print(f\"    Test weeks:  {test_weeks.min()}-{test_weeks.max()}\")\n",
    "    print(f\"    ‚ùå Training on future data! (Week {train_weeks.max()} > Week {test_weeks.min()})\")\n",
    "\n",
    "kfold_mean = np.mean(kfold_scores)\n",
    "kfold_std = np.std(kfold_scores)\n",
    "print(f\"\\n  K-Fold Result: R¬≤ = {kfold_mean:.4f} ¬± {kfold_std:.4f}\")\n",
    "print(f\"  ‚ö†Ô∏è  OPTIMISTIC due to data leakage!\")\n",
    "\n",
    "# Strategy 2: Time Series Split (CORRECT)\n",
    "print(\"\\nüìà STRATEGY 2: Time Series Split (CORRECT - No Leakage)\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "ts_scores = []\n",
    "ts_maes = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X[train_idx])\n",
    "    X_test_scaled = scaler.transform(X[test_idx])\n",
    "    \n",
    "    model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_scaled, y[train_idx])\n",
    "    \n",
    "    score = model.score(X_test_scaled, y[test_idx])\n",
    "    mae = mean_absolute_error(y[test_idx], model.predict(X_test_scaled))\n",
    "    \n",
    "    ts_scores.append(score)\n",
    "    ts_maes.append(mae)\n",
    "    \n",
    "    train_weeks = df.iloc[train_idx]['week']\n",
    "    test_weeks = df.iloc[test_idx]['week']\n",
    "    print(f\"  Fold {fold_idx}: R¬≤={score:.4f}, MAE={mae:.1f} ms\")\n",
    "    print(f\"    Train weeks: {train_weeks.min()}-{train_weeks.max()}\")\n",
    "    print(f\"    Test weeks:  {test_weeks.min()}-{test_weeks.max()}\")\n",
    "    print(f\"    ‚úÖ Train < Test (Week {train_weeks.max()} < Week {test_weeks.min()})\")\n",
    "\n",
    "ts_mean = np.mean(ts_scores)\n",
    "ts_std = np.std(ts_scores)\n",
    "print(f\"\\n  Time Series CV Result: R¬≤ = {ts_mean:.4f} ¬± {ts_std:.4f}\")\n",
    "print(f\"  ‚úÖ REALISTIC estimate for production!\")\n",
    "\n",
    "# Detect performance trend\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(range(1, 6), ts_scores)\n",
    "print(f\"\\n  Performance trend: slope={slope:.6f}, p-value={p_value:.4f}\")\n",
    "if p_value < 0.05:\n",
    "    if slope < 0:\n",
    "        print(f\"  ‚ö†Ô∏è  Significant DOWNWARD trend detected!\")\n",
    "        print(f\"     ‚Üí Model staleness: Performance degrades {ts_scores[0]:.4f} ‚Üí {ts_scores[-1]:.4f}\")\n",
    "        print(f\"     ‚Üí Recommendation: Retrain model periodically or use online learning\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Upward trend: More data improves performance\")\n",
    "else:\n",
    "    print(f\"  ‚úÖ Stable performance across time\")\n",
    "\n",
    "# Final comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Method':<25} {'R¬≤ Score':<20} {'Assessment':<30}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Standard K-Fold':<25} {kfold_mean:.4f} ¬± {kfold_std:.4f}    {'OPTIMISTIC ‚ùå':<30}\")\n",
    "print(f\"{'Time Series Split':<25} {ts_mean:.4f} ¬± {ts_std:.4f}    {'REALISTIC ‚úÖ':<30}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Optimism bias: {kfold_mean - ts_mean:.4f} ({(kfold_mean - ts_mean)/ts_mean*100:.1f}%)\")\n",
    "print(f\"\\nProduction Recommendation:\")\n",
    "print(f\"  - Expected R¬≤: {ts_mean:.4f} (use Time Series CV estimate)\")\n",
    "print(f\"  - Expected MAE: {np.mean(ts_maes):.1f} ¬± {np.std(ts_maes):.1f} ms\")\n",
    "print(f\"  - Monitor for model staleness (performance degrading over time)\")\n",
    "print(f\"  - Retrain model every {52 // 5} weeks based on CV splits\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Score comparison\n",
    "methods = ['Standard K-Fold\\\\n(Biased)', 'Time Series Split\\\\n(Unbiased)']\n",
    "scores_mean = [kfold_mean, ts_mean]\n",
    "scores_std = [kfold_std, ts_std]\n",
    "colors = ['red', 'green']\n",
    "\n",
    "bars = axes[0, 0].bar(methods, scores_mean, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].errorbar(range(len(methods)), scores_mean, yerr=scores_std, \n",
    "                    fmt='none', color='black', capsize=5)\n",
    "axes[0, 0].set_ylabel('R¬≤ Score', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_title('Cross-Validation Strategy Comparison\\\\n(Time Series Split shows realistic performance)', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "for bar, score in zip(bars, scores_mean):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                   f'{score:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Performance trend over time\n",
    "folds = np.arange(1, 6)\n",
    "axes[0, 1].plot(folds, ts_scores, 'o-', color='green', linewidth=2, markersize=8, label='Actual')\n",
    "axes[0, 1].plot(folds, slope * folds + intercept, '--', color='red', \n",
    "               linewidth=2, label=f'Trend (slope={slope:.4f})')\n",
    "axes[0, 1].set_xlabel('Fold Number (Time ‚Üí)', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('R¬≤ Score', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_title('Time Series CV: Performance Over Time\\\\n(Check for model staleness)', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "axes[0, 1].set_xticks(folds)\n",
    "\n",
    "# Plot 3: MAE over time\n",
    "axes[1, 0].plot(folds, ts_maes, 'o-', color='orange', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_xlabel('Fold Number (Time ‚Üí)', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('MAE (milliseconds)', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_title('Prediction Error Over Time\\\\n(Monitor for drift)', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "axes[1, 0].set_xticks(folds)\n",
    "\n",
    "# Plot 4: Distribution comparison\n",
    "data_to_plot = [kfold_scores, ts_scores]\n",
    "bp = axes[1, 1].boxplot(data_to_plot, labels=methods, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "axes[1, 1].set_ylabel('R¬≤ Score', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_title('Score Distribution\\\\n(Higher variance in Time Series CV is normal)', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Complete example finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a06c5c",
   "metadata": {},
   "source": [
    "## üéØ 8 Real-World Project Ideas\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "#### 1. **Wafer Yield Prediction with Spatial Cross-Validation**\n",
    "**Objective**: Predict wafer-level yield using parametric test data while accounting for spatial correlations.\n",
    "\n",
    "**Why This Matters**: Standard CV causes spatial leakage. Group K-Fold by wafer_id ensures model generalizes to new wafers, not just new dies on same wafer.\n",
    "\n",
    "**Key Features**:\n",
    "- Parametric measurements: Vdd, Idd, frequency, power\n",
    "- Spatial features: die_x, die_y coordinates\n",
    "- Process features: lot_id, fab_id, equipment_id\n",
    "- Target: Yield percentage or pass/fail\n",
    "\n",
    "**Implementation Hints**:\n",
    "- Use `GroupKFold` with groups=wafer_id\n",
    "- Alternative: Stratified Group K-Fold to maintain yield distribution\n",
    "- Visualize wafer maps to understand spatial patterns\n",
    "- Compare Group CV vs standard K-Fold to quantify spatial leakage\n",
    "\n",
    "**Success Metrics**:\n",
    "- Group CV R¬≤ > 0.85 (realistic for new wafers)\n",
    "- MAE < 2% yield (actionable for manufacturing decisions)\n",
    "- No performance degradation when deployed to new wafers\n",
    "- Spatial leakage quantified: Standard CV - Group CV\n",
    "\n",
    "**Business Value**: $500K+ annual savings by predicting low-yield wafers early and adjusting process parameters.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Test Time Optimization with Temporal Cross-Validation**\n",
    "**Objective**: Predict final test time to optimize test scheduling, accounting for process drift over time.\n",
    "\n",
    "**Why This Matters**: Test programs evolve, equipment ages ‚Üí model trained on old data may not work on new lots. Time Series CV reveals realistic performance.\n",
    "\n",
    "**Key Features**:\n",
    "- Device complexity score\n",
    "- Number of test points\n",
    "- Operating frequency, power, temperature\n",
    "- Historical test time trends\n",
    "- Equipment age (implicit via timestamp)\n",
    "\n",
    "**Implementation Hints**:\n",
    "- Use `TimeSeriesSplit` with n_splits=5-10\n",
    "- Monitor performance trend across folds (check for staleness)\n",
    "- Consider rolling window if equipment replaced periodically\n",
    "- Include gap between train/test if test program updated in batches\n",
    "\n",
    "**Success Metrics**:\n",
    "- Time Series CV MAE < 50ms (test scheduling precision)\n",
    "- Performance trend stable or upward (model doesn't degrade)\n",
    "- Detect when retraining needed (significant downward trend)\n",
    "- 30% reduction in test resource idle time\n",
    "\n",
    "**Business Value**: $200K+ annual savings via optimized test scheduling and reduced equipment downtime.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Multi-Fab Yield Model with Nested Cross-Validation**\n",
    "**Objective**: Build yield prediction model that works across multiple fabs, with rigorous hyperparameter tuning.\n",
    "\n",
    "**Why This Matters**: Naive hyperparameter tuning gives optimistic estimates. Nested CV provides unbiased performance for cross-fab deployment.\n",
    "\n",
    "**Key Features**:\n",
    "- Parametric test measurements (standardized across fabs)\n",
    "- Fab-specific features: process node, equipment type\n",
    "- Environmental: temperature, humidity\n",
    "- Target: Yield or defect density\n",
    "\n",
    "**Implementation Hints**:\n",
    "- Outer loop: Group K-Fold by fab_id (generalization to new fabs)\n",
    "- Inner loop: Stratified K-Fold for hyperparameter tuning\n",
    "- Tune: Model type, feature engineering, threshold selection\n",
    "- Report both inner (optimistic) and outer (realistic) scores\n",
    "\n",
    "**Success Metrics**:\n",
    "- Nested CV R¬≤ > 0.80 (realistic cross-fab performance)\n",
    "- Inner-outer gap < 0.05 (low optimism bias)\n",
    "- Best hyperparameters consistent across outer folds\n",
    "- Model performs within 5% when deployed to new fab\n",
    "\n",
    "**Business Value**: Enable standardized yield models across 3-5 fabs, saving $1M+ in duplicated development effort.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Parametric Outlier Detection with Stratified Cross-Validation**\n",
    "**Objective**: Detect rare parametric outliers (0.5-2% rate) that indicate process excursions.\n",
    "\n",
    "**Why This Matters**: Imbalanced data (98% normal, 2% outliers) ‚Üí standard K-Fold creates variable class distributions across folds. Stratified K-Fold maintains consistent 2% rate.\n",
    "\n",
    "**Key Features**:\n",
    "- All parametric measurements from test data\n",
    "- Statistical features: Z-scores, Mahalanobis distance\n",
    "- Temporal features: Time since last outlier\n",
    "- Spatial features: Neighboring die measurements\n",
    "\n",
    "**Implementation Hints**:\n",
    "- Use `StratifiedKFold` to maintain outlier rate in each fold\n",
    "- Consider SMOTE or class weighting for extreme imbalance\n",
    "- Use F1-score or AUPRC (not accuracy) due to imbalance\n",
    "- Compare Stratified vs Regular K-Fold variance\n",
    "\n",
    "**Success Metrics**:\n",
    "- Stratified CV F1 > 0.85 with std < 0.03 (low variance)\n",
    "- AUPRC > 0.90 (good precision-recall tradeoff)\n",
    "- 50% reduction in variance vs regular K-Fold\n",
    "- Catch 95% of outliers with <1% false positive rate\n",
    "\n",
    "**Business Value**: $300K+ savings by detecting process excursions early, preventing scrap of entire lots.\n",
    "\n",
    "---\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "#### 5. **Customer Churn Prediction with Stratified Nested CV**\n",
    "**Objective**: Predict customer churn (10-15% rate) with rigorous model selection and unbiased performance estimate.\n",
    "\n",
    "**Why This Matters**: Imbalanced classes + hyperparameter tuning ‚Üí double optimism bias. Use Stratified K-Fold + Nested CV.\n",
    "\n",
    "**Key Features**:\n",
    "- Customer demographics: age, location, tenure\n",
    "- Usage patterns: login frequency, feature usage\n",
    "- Support interactions: ticket count, resolution time\n",
    "- Billing: payment history, plan changes\n",
    "\n",
    "**Implementation Hints**:\n",
    "- Outer loop: Stratified K-Fold (maintain churn rate)\n",
    "- Inner loop: Stratified K-Fold (hyperparameter tuning)\n",
    "- Tune: Model type, class weighting, threshold\n",
    "- Report confidence intervals for churn rate impact\n",
    "\n",
    "**Success Metrics**:\n",
    "- Nested CV AUPRC > 0.75 (realistic performance)\n",
    "- Inner-outer gap < 0.05 (low optimism)\n",
    "- Churn rate consistent across folds (within 1%)\n",
    "- 25% reduction in customer acquisition cost via retention\n",
    "\n",
    "**Business Value**: $500K+ annual revenue retention by proactively targeting at-risk customers.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Stock Price Prediction with Rolling Window CV**\n",
    "**Objective**: Predict next-day stock price movement using time series with non-stationary patterns.\n",
    "\n",
    "**Why This Matters**: Financial markets have regime changes. Rolling window CV (fixed training size) better mimics production than expanding window.\n",
    "\n",
    "**Key Features**:\n",
    "- Technical indicators: Moving averages, RSI, MACD\n",
    "- Fundamental: P/E ratio, earnings, volume\n",
    "- Sentiment: News sentiment scores\n",
    "- Market features: Index movements, sector performance\n",
    "\n",
    "**Implementation Hints**:\n",
    "- Use custom `TimeSeriesSplit` with `max_train_size` (rolling window)\n",
    "- Training window: 252 days (1 trading year)\n",
    "- Test window: 21 days (1 month)\n",
    "- Monitor performance trend to detect regime changes\n",
    "\n",
    "**Success Metrics**:\n",
    "- Time Series CV accuracy > 55% (statistically significant)\n",
    "- Performance stable across folds (regime-invariant)\n",
    "- Sharpe ratio > 1.5 in backtest\n",
    "- Detect regime changes when performance drops >10%\n",
    "\n",
    "**Business Value**: 15-20% annual returns above market benchmark via systematic trading strategy.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. **Medical Diagnosis with Patient-Level Group CV**\n",
    "**Objective**: Predict disease diagnosis from medical images, ensuring model generalizes to new patients (not just new images from same patients).\n",
    "\n",
    "**Why This Matters**: Multiple images per patient ‚Üí standard CV causes patient leakage. Group K-Fold by patient_id ensures generalization.\n",
    "\n",
    "**Key Features**:\n",
    "- Image features: CNN embeddings, texture, shape\n",
    "- Patient metadata: Age, sex, medical history\n",
    "- Temporal: Disease progression stage\n",
    "- Clinical: Lab results, vitals\n",
    "\n",
    "**Implementation Hints**:\n",
    "- Use `GroupKFold` with groups=patient_id\n",
    "- Ensure train/test have no overlapping patients\n",
    "- Consider Leave-One-Group-Out if few patients\n",
    "- Stratify by diagnosis if possible (StratifiedGroupKFold)\n",
    "\n",
    "**Success Metrics**:\n",
    "- Group CV AUROC > 0.90 (clinically useful)\n",
    "- No patient leakage (verify group separation)\n",
    "- Performance within 5% when deployed to new hospital\n",
    "- Sensitivity > 0.95 (catch most positive cases)\n",
    "\n",
    "**Business Value**: Enable early diagnosis, reducing treatment costs by $10K+ per patient and improving outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. **Sales Forecasting with Hierarchical Time Series CV**\n",
    "**Objective**: Forecast sales across multiple product categories and regions, accounting for temporal and group structure.\n",
    "\n",
    "**Why This Matters**: Sales data has both temporal ordering and hierarchy (products within categories, stores within regions). Need hybrid CV strategy.\n",
    "\n",
    "**Key Features**:\n",
    "- Historical sales: Past 2 years daily\n",
    "- Seasonality: Day of week, month, holidays\n",
    "- Promotions: Discount percentage, campaign type\n",
    "- External: Weather, economic indicators\n",
    "- Hierarchy: Product ‚Üí Category, Store ‚Üí Region\n",
    "\n",
    "**Implementation Hints**:\n",
    "- Outer loop: Time Series Split (temporal)\n",
    "- Consider grouping by region for cross-region validation\n",
    "- Use separate models per category or hierarchical model\n",
    "- Aggregate forecasts to ensure consistency (bottom-up or top-down)\n",
    "\n",
    "**Success Metrics**:\n",
    "- Time Series CV MAPE < 15% (industry standard)\n",
    "- Performance stable across seasons\n",
    "- Forecast accuracy within 10% at category level\n",
    "- Enable 20% inventory reduction via better planning\n",
    "\n",
    "**Business Value**: $2M+ savings annually through optimized inventory management and reduced stockouts/overstock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ad63d4",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways and Best Practices\n",
    "\n",
    "### Core Principles\n",
    "\n",
    "#### 1. **Match CV Strategy to Data Structure**\n",
    "- ‚úÖ **Temporal data** ‚Üí Time Series Split (forward chaining)\n",
    "- ‚úÖ **Imbalanced classes** ‚Üí Stratified K-Fold\n",
    "- ‚úÖ **Group structure** ‚Üí Group K-Fold\n",
    "- ‚úÖ **Hyperparameter tuning** ‚Üí Nested CV\n",
    "- ‚úÖ **i.i.d. data** ‚Üí Standard K-Fold\n",
    "\n",
    "**Golden Rule**: Your CV strategy should mimic how the model will be used in production.\n",
    "\n",
    "#### 2. **Understand the Bias-Variance Tradeoff in CV**\n",
    "- **More folds (K=10)**: Lower bias, higher variance, more computation\n",
    "- **Fewer folds (K=3)**: Higher bias, lower variance, less computation\n",
    "- **LOO (K=n)**: Lowest bias, highest variance, expensive\n",
    "- **Typical choice**: K=5 (good balance)\n",
    "\n",
    "**Recommendation**: Start with K=5, increase to K=10 for final evaluation.\n",
    "\n",
    "#### 3. **Report Uncertainty Honestly**\n",
    "- Always report **mean ¬± std** (not just mean)\n",
    "- Include **confidence intervals** (95% CI using t-distribution)\n",
    "- Show **per-fold results** (check for outliers)\n",
    "- Document **CV strategy** in model card\n",
    "\n",
    "**Bad reporting**: \"Model achieves 92% accuracy\"  \n",
    "**Good reporting**: \"Model achieves 88.2% ¬± 2.8% accuracy (95% CI: [82.7%, 93.7%]) using 5-Fold Stratified CV on imbalanced dataset (15% positive class)\"\n",
    "\n",
    "---\n",
    "\n",
    "### Common Pitfalls and Solutions\n",
    "\n",
    "#### ‚ùå **Pitfall 1: Data Leakage Through Time**\n",
    "**Problem**: Using K-Fold on time series data  \n",
    "**Consequence**: Training on future to predict past ‚Üí overly optimistic  \n",
    "**Solution**: Always use Time Series Split for temporal data  \n",
    "**Detection**: Check if max(train_dates) > min(test_dates) in any fold\n",
    "\n",
    "#### ‚ùå **Pitfall 2: Group Leakage**\n",
    "**Problem**: Same patient/wafer/customer in train and test  \n",
    "**Consequence**: Model learns entity-specific patterns, not generalizable  \n",
    "**Solution**: Use Group K-Fold, ensure no group overlap  \n",
    "**Detection**: Check if train_groups ‚à© test_groups ‚â† ‚àÖ\n",
    "\n",
    "#### ‚ùå **Pitfall 3: Optimistic Hyperparameter Tuning**\n",
    "**Problem**: Reporting best_score_ from GridSearchCV  \n",
    "**Consequence**: 3-5% optimism bias from data snooping  \n",
    "**Solution**: Use Nested CV for unbiased estimate  \n",
    "**Detection**: Compare inner CV score to outer CV score (gap = bias)\n",
    "\n",
    "#### ‚ùå **Pitfall 4: Not Stratifying Imbalanced Data**\n",
    "**Problem**: Some folds have 1% positive, others 5%  \n",
    "**Consequence**: High variance in metrics across folds  \n",
    "**Solution**: Use Stratified K-Fold for classification  \n",
    "**Detection**: Check class distribution per fold (should be consistent)\n",
    "\n",
    "#### ‚ùå **Pitfall 5: Preprocessing Leakage**\n",
    "**Problem**: Fitting scaler on full dataset before CV  \n",
    "**Consequence**: Test set statistics leak into training  \n",
    "**Solution**: Fit preprocessing inside CV loop (use sklearn Pipeline)  \n",
    "**Detection**: Check if preprocessing uses test set information\n",
    "\n",
    "---\n",
    "\n",
    "### Production Deployment Guidelines\n",
    "\n",
    "#### **Phase 1: Development (Fast Iteration)**\n",
    "- Use simple train/test split (80/20)\n",
    "- CV not required for rapid prototyping\n",
    "- Focus: Model architecture, feature engineering\n",
    "- Speed > Rigor\n",
    "\n",
    "#### **Phase 2: Model Selection (Compare Algorithms)**\n",
    "- Use 5-Fold CV (stratified/time series as appropriate)\n",
    "- Compare multiple models with same CV strategy\n",
    "- Report mean ¬± std for each model\n",
    "- Use statistical tests (paired t-test) to compare\n",
    "\n",
    "#### **Phase 3: Final Evaluation (Production Readiness)**\n",
    "- Use 10-Fold CV or Nested CV\n",
    "- Report confidence intervals\n",
    "- Include multiple metrics (accuracy, precision, recall, AUROC, etc.)\n",
    "- Document CV strategy in model card\n",
    "- Validate on hold-out test set (if available)\n",
    "\n",
    "#### **Phase 4: Monitoring (Post-Deployment)**\n",
    "- Compare production metrics to CV estimates\n",
    "- Alert if performance drops below CV - 2œÉ\n",
    "- Re-run CV periodically on new data\n",
    "- Retrain when CV performance degrades significantly\n",
    "\n",
    "---\n",
    "\n",
    "### Computational Optimization\n",
    "\n",
    "#### **Speed vs Accuracy Tradeoffs**\n",
    "1. **Reduce K**: Use K=3 instead of K=10 (3.3√ó speedup)\n",
    "2. **Parallelize**: Use `n_jobs=-1` in CV functions\n",
    "3. **Subsample**: Use stratified subset for large datasets\n",
    "4. **Early stopping**: For iterative models (GBM, neural networks)\n",
    "5. **RandomizedSearchCV**: Instead of GridSearchCV (10-100√ó speedup)\n",
    "6. **Cache**: Use `memory` parameter in sklearn Pipeline\n",
    "\n",
    "#### **When to Use Each Strategy**\n",
    "- **Development**: K=3, single random seed\n",
    "- **Model selection**: K=5, multiple metrics\n",
    "- **Final evaluation**: K=10 or Nested CV, multiple seeds\n",
    "- **Research/Publication**: Nested CV, comprehensive metrics, reproducibility details\n",
    "\n",
    "---\n",
    "\n",
    "### Semiconductor-Specific Best Practices\n",
    "\n",
    "#### **Wafer-Level Models**\n",
    "- Always group by wafer_id (avoid spatial leakage)\n",
    "- Consider stratifying by yield bins\n",
    "- Visualize wafer maps to understand spatial patterns\n",
    "- Report performance per fab if multi-fab deployment\n",
    "\n",
    "#### **Lot-Based Models**\n",
    "- Group by lot_id (avoid process correlation leakage)\n",
    "- Use Time Series Split if modeling across production time\n",
    "- Monitor for process drift (performance trend analysis)\n",
    "- Consider separate models per product family\n",
    "\n",
    "#### **Test Time Models**\n",
    "- Use Time Series Split (test programs evolve)\n",
    "- Monitor for equipment aging effects\n",
    "- Consider rolling window if equipment replaced\n",
    "- Include gap between train/test for test program updates\n",
    "\n",
    "#### **Defect Detection Models**\n",
    "- Use Stratified K-Fold (maintain defect rate)\n",
    "- Consider SMOTE or class weighting for extreme imbalance\n",
    "- Report AUPRC (not accuracy) due to imbalance\n",
    "- Tune threshold separately for production (precision vs recall tradeoff)\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced Topics (Beyond This Notebook)\n",
    "\n",
    "#### **Custom Cross-Validation Strategies**\n",
    "- Implement custom splitters for domain-specific needs\n",
    "- Example: Block CV for spatial data, Walk-Forward CV for finance\n",
    "- Inherit from `sklearn.model_selection.BaseCrossValidator`\n",
    "\n",
    "#### **Ensemble Cross-Validation**\n",
    "- Train ensemble where each member trained on different CV fold\n",
    "- Average predictions for better generalization\n",
    "- Requires K models (memory/computation tradeoff)\n",
    "\n",
    "#### **Cross-Validation for Deep Learning**\n",
    "- Use K-Fold for small datasets (<10K samples)\n",
    "- Use single train/val split for large datasets (>100K)\n",
    "- Consider stratified split by class for image classification\n",
    "- Use TimeSeriesSplit for sequential data (RNN, LSTM)\n",
    "\n",
    "#### **Multi-Objective Cross-Validation**\n",
    "- Optimize multiple metrics simultaneously\n",
    "- Example: Maximize accuracy while minimizing inference time\n",
    "- Use Pareto-optimal solutions\n",
    "\n",
    "---\n",
    "\n",
    "### Final Recommendations\n",
    "\n",
    "#### **For Beginners**\n",
    "1. Start with standard K-Fold (K=5)\n",
    "2. Learn to recognize data structure violations (time, groups, imbalance)\n",
    "3. Switch to appropriate CV strategy when needed\n",
    "4. Always report mean ¬± std\n",
    "\n",
    "#### **For Practitioners**\n",
    "1. Match CV strategy to production deployment\n",
    "2. Use Nested CV for hyperparameter tuning\n",
    "3. Monitor for data leakage (time, groups, preprocessing)\n",
    "4. Document CV strategy in model card\n",
    "\n",
    "#### **For Researchers**\n",
    "1. Use Nested CV for unbiased estimates\n",
    "2. Report both inner and outer CV scores\n",
    "3. Use multiple random seeds to verify stability\n",
    "4. Provide full reproducibility details\n",
    "\n",
    "#### **For Semiconductor Engineers**\n",
    "1. Group by wafer_id or lot_id (avoid spatial/process leakage)\n",
    "2. Use Time Series Split for production time series\n",
    "3. Stratify by yield bins or defect classes\n",
    "4. Monitor for process drift post-deployment\n",
    "\n",
    "---\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "#### **Sklearn Documentation**\n",
    "- [Cross-validation guide](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- [Model evaluation metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "\n",
    "#### **Academic Papers**\n",
    "- Cawley, G. C., & Talbot, N. L. (2010). \"On over-fitting in model selection and subsequent selection bias in performance evaluation.\" JMLR.\n",
    "- Bergmeir, C., & Ben√≠tez, J. M. (2012). \"On the use of cross-validation for time series predictor evaluation.\" Information Sciences.\n",
    "\n",
    "#### **Practical Guides**\n",
    "- Raschka, S. (2018). \"Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning.\"\n",
    "- Kuhn, M., & Johnson, K. (2013). \"Applied Predictive Modeling.\" Chapter 4: Over-Fitting and Model Tuning.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "**Cross-validation is not optional‚Äîit's the foundation of reliable machine learning.**\n",
    "\n",
    "- ‚úÖ Choose CV strategy based on data structure\n",
    "- ‚úÖ Report uncertainty (mean ¬± std, confidence intervals)\n",
    "- ‚úÖ Avoid data leakage (time, groups, preprocessing)\n",
    "- ‚úÖ Use Nested CV for hyperparameter tuning\n",
    "- ‚úÖ Monitor production performance vs CV estimates\n",
    "\n",
    "**Remember**: The goal is not to maximize CV score‚Äîit's to get an **honest, unbiased estimate** of how your model will perform in production. A lower but realistic CV score is infinitely more valuable than a high but optimistic one.\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now have a comprehensive understanding of cross-validation strategies. Use this knowledge to build robust, reliable machine learning models that generalize well to production environments.\n",
    "\n",
    "**Next Steps**: Apply these techniques to real datasets, experiment with different CV strategies, and always validate your assumptions. Cross-validation is both an art and a science‚Äîmaster it, and you'll build models that stand the test of time (and production!)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
