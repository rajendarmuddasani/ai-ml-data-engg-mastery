{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff5e708",
   "metadata": {},
   "source": [
    "# 042: Model Evaluation Metrics\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** classification metrics (accuracy, precision, recall, F1, AUC-ROC)\n",
    "- **Master** regression metrics (MSE, RMSE, MAE, RÂ², MAPE)\n",
    "- **Implement** confusion matrices, ROC curves, and precision-recall curves\n",
    "- **Apply** business-aligned metrics for semiconductor test optimization\n",
    "- **Build** automated evaluation dashboards for model monitoring\n",
    "\n",
    "## ðŸ“š What are Model Evaluation Metrics?\n",
    "\n",
    "**Model evaluation metrics** quantify how well a model performs on unseen data. Choosing the right metric depends on:\n",
    "- Problem type (classification vs. regression)\n",
    "- Business objective (minimize cost, maximize revenue, balance trade-offs)\n",
    "- Class imbalance (accuracy misleads when classes are skewed)\n",
    "- Error asymmetry (false positives vs. false negatives)\n",
    "\n",
    "**Key Classification Metrics:**\n",
    "- **Accuracy**: Correct predictions / Total predictions (misleading if imbalanced)\n",
    "- **Precision**: TP / (TP + FP) - \"Of predicted positives, how many are correct?\"\n",
    "- **Recall**: TP / (TP + FN) - \"Of actual positives, how many did we catch?\"\n",
    "- **F1-Score**: Harmonic mean of precision and recall (balanced metric)\n",
    "- **AUC-ROC**: Area under receiver operating characteristic curve (threshold-independent)\n",
    "\n",
    "**Key Regression Metrics:**\n",
    "- **MSE/RMSE**: Mean squared error (penalizes large errors)\n",
    "- **MAE**: Mean absolute error (robust to outliers)\n",
    "- **RÂ²**: Explained variance (0-1, interpretable)\n",
    "- **MAPE**: Mean absolute percentage error (scale-independent)\n",
    "\n",
    "## ðŸ­ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Automated Metrics Dashboard**\n",
    "- Input: Model predictions + ground truth from validation sets\n",
    "- Output: Real-time dashboard tracking 10+ metrics over time\n",
    "- Value: Data-driven model selection and monitoring ($5M+ confidence)\n",
    "\n",
    "**Cost-Optimized Decision Thresholds**\n",
    "- Input: Business costs (FP = $500 yield loss, FN = $10K field return)\n",
    "- Output: Optimal probability threshold maximizing profit\n",
    "- Value: $10-20M annual savings through cost-aware predictions\n",
    "\n",
    "**Model Degradation Detector**\n",
    "- Input: Streaming predictions + validation labels in production\n",
    "- Output: Alerts when metrics degrade >5% from baseline\n",
    "- Value: $15M+ prevented revenue loss from model staleness\n",
    "\n",
    "**Multi-Objective Trade-Off Optimizer**\n",
    "- Input: Multiple competing metrics (accuracy, speed, cost)\n",
    "- Output: Pareto frontier identifying optimal model configurations\n",
    "- Value: $12M+ through balanced optimization (98% accuracy + 20% faster)\n",
    "\n",
    "## ðŸ”„ Metrics Evaluation Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Model Predictions] --> B[Compute Metrics]\n",
    "    C[Ground Truth] --> B\n",
    "    B --> D{Classification?}\n",
    "    D -->|Yes| E[ROC, PR, F1]\n",
    "    D -->|No| F[RMSE, MAE, RÂ²]\n",
    "    E --> G[Business Alignment]\n",
    "    F --> G\n",
    "    G --> H[Select Best Model]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style H fill:#e1ffe1\n",
    "```\n",
    "\n",
    "## ðŸ“Š Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- 013: Logistic Regression (classification fundamentals)\n",
    "- 010: Linear Regression (regression fundamentals)\n",
    "\n",
    "**Next Steps:**\n",
    "- 043: Cross-Validation Strategies (robust evaluation)\n",
    "- 044: Hyperparameter Tuning (optimization)\n",
    "\n",
    "---\n",
    "\n",
    "Let's master model evaluation for production ML! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54fdca6",
   "metadata": {},
   "source": [
    "## ðŸ“ Section 1: Classification Metrics - Confusion Matrix Foundation\n",
    "\n",
    "### The Confusion Matrix\n",
    "\n",
    "The **confusion matrix** is the foundation for all classification metrics. It shows the counts of correct and incorrect predictions for each class.\n",
    "\n",
    "**Binary Classification:**\n",
    "\n",
    "```\n",
    "                  Predicted\n",
    "                 Positive | Negative\n",
    "         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Actual   Positive    TP   |   FN\n",
    "         Negative    FP   |   TN\n",
    "```\n",
    "\n",
    "**Definitions:**\n",
    "- **TP (True Positive):** Correctly predicted positive (model said YES, actual YES)\n",
    "- **TN (True Negative):** Correctly predicted negative (model said NO, actual NO)\n",
    "- **FP (False Positive):** Incorrectly predicted positive (model said YES, actual NO) - **Type I Error**\n",
    "- **FN (False Negative):** Incorrectly predicted negative (model said NO, actual YES) - **Type II Error**\n",
    "\n",
    "**Semiconductor Example:**\n",
    "- **TP:** Device predicted to fail, actually fails â†’ Caught by test âœ…\n",
    "- **TN:** Device predicted to pass, actually passes â†’ Good device shipped âœ…\n",
    "- **FP:** Device predicted to fail, actually passes â†’ Unnecessary scrap ðŸ’°\n",
    "- **FN:** Device predicted to pass, actually fails â†’ Test escape (shipped to customer) âš ï¸\n",
    "\n",
    "### Derived Metrics from Confusion Matrix\n",
    "\n",
    "#### 1. Accuracy\n",
    "\n",
    "**Definition:** Proportion of correct predictions (both positive and negative).\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "**When to Use:**\n",
    "- âœ… Balanced datasets (roughly equal class distribution)\n",
    "- âœ… Equal cost for both types of errors\n",
    "- âœ… High-level performance summary\n",
    "\n",
    "**When NOT to Use:**\n",
    "- âŒ Imbalanced datasets (accuracy paradox)\n",
    "- âŒ Asymmetric error costs\n",
    "\n",
    "**Example:** 95% accuracy with 95% class 1 â†’ Model could predict all class 1 and be 95% accurate!\n",
    "\n",
    "#### 2. Precision (Positive Predictive Value)\n",
    "\n",
    "**Definition:** Of all positive predictions, what fraction were actually positive?\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "**Interpretation:** \"When the model says YES, how often is it right?\"\n",
    "\n",
    "**When to Use:**\n",
    "- âœ… False positives are costly (spam detection, fraud detection)\n",
    "- âœ… Resources limited (can only investigate X positive predictions)\n",
    "- âœ… Want to minimize false alarms\n",
    "\n",
    "**Semiconductor Example:** \n",
    "- High precision â†’ Few good devices scrapped unnecessarily\n",
    "- Precision = 0.90 â†’ 10% of predicted-fail devices actually pass (overkill)\n",
    "\n",
    "#### 3. Recall (Sensitivity, True Positive Rate)\n",
    "\n",
    "**Definition:** Of all actual positives, what fraction did we correctly identify?\n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "**Interpretation:** \"Of all the YES cases, how many did we catch?\"\n",
    "\n",
    "**When to Use:**\n",
    "- âœ… False negatives are costly (disease detection, security threats)\n",
    "- âœ… Must catch as many positives as possible\n",
    "- âœ… Imbalanced datasets with rare positive class\n",
    "\n",
    "**Semiconductor Example:**\n",
    "- High recall â†’ Few failing devices escape to customers\n",
    "- Recall = 0.95 â†’ 5% of failing devices not caught by test (test escapes)\n",
    "\n",
    "#### 4. Specificity (True Negative Rate)\n",
    "\n",
    "**Definition:** Of all actual negatives, what fraction did we correctly identify?\n",
    "\n",
    "$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "**Interpretation:** \"Of all the NO cases, how many did we correctly identify?\"\n",
    "\n",
    "**Relationship:** Specificity = 1 - False Positive Rate\n",
    "\n",
    "#### 5. F1 Score (Harmonic Mean of Precision and Recall)\n",
    "\n",
    "**Definition:** Single metric combining precision and recall.\n",
    "\n",
    "$$F_1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2 \\times TP}{2 \\times TP + FP + FN}$$\n",
    "\n",
    "**Why Harmonic Mean?** Penalizes extreme imbalances (if either precision or recall is low, F1 is low).\n",
    "\n",
    "**When to Use:**\n",
    "- âœ… Need single metric for model comparison\n",
    "- âœ… Want balance between precision and recall\n",
    "- âœ… Imbalanced datasets\n",
    "\n",
    "**Example:**\n",
    "- Precision = 0.90, Recall = 0.10 â†’ F1 = 0.18 (penalized for low recall)\n",
    "- Precision = 0.50, Recall = 0.50 â†’ F1 = 0.50 (balanced)\n",
    "\n",
    "#### 6. F-beta Score (Weighted F Score)\n",
    "\n",
    "**Definition:** Generalized F1 score that allows weighting recall vs precision.\n",
    "\n",
    "$$F_{\\beta} = (1 + \\beta^2) \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\beta^2 \\times \\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "Where:\n",
    "- $\\beta > 1$: Favor recall (reduce false negatives)\n",
    "- $\\beta < 1$: Favor precision (reduce false positives)\n",
    "- $\\beta = 1$: Balanced (standard F1 score)\n",
    "\n",
    "**Common Values:**\n",
    "- **F0.5:** Precision matters 2x more than recall\n",
    "- **F1:** Equal weight (harmonic mean)\n",
    "- **F2:** Recall matters 2x more than precision\n",
    "\n",
    "**Semiconductor Example:**\n",
    "- Use F2 when test escapes (FN) cost $10K but overkill (FP) costs $100\n",
    "- Use F0.5 when overkill is very expensive (high-value devices)\n",
    "\n",
    "### Precision-Recall Trade-off\n",
    "\n",
    "**Key Insight:** Precision and recall often trade off against each other.\n",
    "\n",
    "**Increasing Threshold:**\n",
    "- Predict positive only when very confident\n",
    "- â†‘ Precision (fewer false positives)\n",
    "- â†“ Recall (more false negatives)\n",
    "\n",
    "**Decreasing Threshold:**\n",
    "- Predict positive more liberally\n",
    "- â†“ Precision (more false positives)\n",
    "- â†‘ Recall (fewer false negatives)\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Threshold = 0.9: Precision = 0.95, Recall = 0.60 (conservative)\n",
    "Threshold = 0.5: Precision = 0.85, Recall = 0.80 (balanced)\n",
    "Threshold = 0.1: Precision = 0.60, Recall = 0.95 (aggressive)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94cf6ec",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0aafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Metrics Implementation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "class ClassificationMetrics:\n",
    "    \"\"\"\n",
    "    Comprehensive classification metrics toolkit.\n",
    "    \n",
    "    Computes all standard metrics from confusion matrix:\n",
    "    - Accuracy, Precision, Recall, Specificity\n",
    "    - F1 score, F-beta scores\n",
    "    - Matthews Correlation Coefficient\n",
    "    - ROC-AUC (requires probability scores)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, y_true, y_pred, y_prob=None):\n",
    "        \"\"\"\n",
    "        Initialize with true labels and predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true : array-like\n",
    "            True labels (0 or 1)\n",
    "        y_pred : array-like\n",
    "            Predicted labels (0 or 1)\n",
    "        y_prob : array-like, optional\n",
    "            Predicted probabilities for positive class (for ROC-AUC)\n",
    "        \"\"\"\n",
    "        self.y_true = np.array(y_true)\n",
    "        self.y_pred = np.array(y_pred)\n",
    "        self.y_prob = np.array(y_prob) if y_prob is not None else None\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(self.y_true, self.y_pred)\n",
    "        \n",
    "        # Extract TP, TN, FP, FN\n",
    "        if cm.shape == (2, 2):\n",
    "            self.tn, self.fp, self.fn, self.tp = cm.ravel()\n",
    "        else:\n",
    "            raise ValueError(\"Only binary classification supported\")\n",
    "        \n",
    "        self.total = self.tp + self.tn + self.fp + self.fn\n",
    "    \n",
    "    def accuracy(self):\n",
    "        \"\"\"Calculate accuracy: (TP + TN) / Total\"\"\"\n",
    "        return (self.tp + self.tn) / self.total\n",
    "    \n",
    "    def precision(self):\n",
    "        \"\"\"Calculate precision: TP / (TP + FP)\"\"\"\n",
    "        if (self.tp + self.fp) == 0:\n",
    "            return 0.0  # No positive predictions\n",
    "        return self.tp / (self.tp + self.fp)\n",
    "    \n",
    "    def recall(self):\n",
    "        \"\"\"Calculate recall (sensitivity): TP / (TP + FN)\"\"\"\n",
    "        if (self.tp + self.fn) == 0:\n",
    "            return 0.0  # No actual positives\n",
    "        return self.tp / (self.tp + self.fn)\n",
    "    \n",
    "    def specificity(self):\n",
    "        \"\"\"Calculate specificity: TN / (TN + FP)\"\"\"\n",
    "        if (self.tn + self.fp) == 0:\n",
    "            return 0.0  # No actual negatives\n",
    "        return self.tn / (self.tn + self.fp)\n",
    "    \n",
    "    def f1_score(self):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa3dd0",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\"Calculate F1 score: harmonic mean of precision and recall\"\"\"\n",
    "        prec = self.precision()\n",
    "        rec = self.recall()\n",
    "        \n",
    "        if (prec + rec) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return 2 * (prec * rec) / (prec + rec)\n",
    "    \n",
    "    def fbeta_score(self, beta=1.0):\n",
    "        \"\"\"\n",
    "        Calculate F-beta score with custom beta.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        beta : float\n",
    "            Weight of recall vs precision\n",
    "            beta > 1: favor recall\n",
    "            beta < 1: favor precision\n",
    "        \"\"\"\n",
    "        prec = self.precision()\n",
    "        rec = self.recall()\n",
    "        \n",
    "        if (beta**2 * prec + rec) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return (1 + beta**2) * (prec * rec) / (beta**2 * prec + rec)\n",
    "    \n",
    "    def matthews_corrcoef(self):\n",
    "        \"\"\"\n",
    "        Calculate Matthews Correlation Coefficient (MCC).\n",
    "        \n",
    "        Range: [-1, 1]\n",
    "        +1: Perfect prediction\n",
    "        0: Random prediction\n",
    "        -1: Total disagreement\n",
    "        \n",
    "        Good for imbalanced datasets.\n",
    "        \"\"\"\n",
    "        numerator = (self.tp * self.tn) - (self.fp * self.fn)\n",
    "        denominator = np.sqrt(\n",
    "            (self.tp + self.fp) * (self.tp + self.fn) * \n",
    "            (self.tn + self.fp) * (self.tn + self.fn)\n",
    "        )\n",
    "        \n",
    "        if denominator == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return numerator / denominator\n",
    "    \n",
    "    def roc_auc(self):\n",
    "        \"\"\"\n",
    "        Calculate ROC-AUC score.\n",
    "        Requires probability scores (y_prob).\n",
    "        \n",
    "        Range: [0, 1]\n",
    "        0.5: Random classifier\n",
    "        1.0: Perfect classifier\n",
    "        \"\"\"\n",
    "        if self.y_prob is None:\n",
    "            raise ValueError(\"y_prob required for ROC-AUC calculation\")\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(self.y_true, self.y_prob)\n",
    "        return auc(fpr, tpr)\n",
    "    \n",
    "    def get_confusion_matrix(self):\n",
    "        \"\"\"Return confusion matrix as 2x2 array\"\"\"\n",
    "        return np.array([[self.tn, self.fp],\n",
    "                        [self.fn, self.tp]])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f978ab",
   "metadata": {},
   "source": [
    "### ðŸ“ Function: summary\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a40b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Print comprehensive metrics summary.\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"CLASSIFICATION METRICS SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(f\"                 Predicted\")\n",
    "        print(f\"                 Neg    Pos\")\n",
    "        print(f\"       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "        print(f\"Actual Neg      {self.tn:4d}   {self.fp:4d}\")\n",
    "        print(f\"       Pos      {self.fn:4d}   {self.tp:4d}\")\n",
    "        \n",
    "        print(\"\\nBasic Metrics:\")\n",
    "        print(f\"  Accuracy:    {self.accuracy():.4f}\")\n",
    "        print(f\"  Precision:   {self.precision():.4f}\")\n",
    "        print(f\"  Recall:      {self.recall():.4f}\")\n",
    "        print(f\"  Specificity: {self.specificity():.4f}\")\n",
    "        \n",
    "        print(\"\\nComposite Metrics:\")\n",
    "        print(f\"  F1 Score:    {self.f1_score():.4f}\")\n",
    "        print(f\"  F0.5 Score:  {self.fbeta_score(0.5):.4f} (precision focus)\")\n",
    "        print(f\"  F2 Score:    {self.fbeta_score(2.0):.4f} (recall focus)\")\n",
    "        print(f\"  MCC:         {self.matthews_corrcoef():.4f}\")\n",
    "        \n",
    "        if self.y_prob is not None:\n",
    "            print(f\"  ROC-AUC:     {self.roc_auc():.4f}\")\n",
    "        \n",
    "        print(\"\\nError Analysis:\")\n",
    "        print(f\"  False Positives: {self.fp} ({self.fp/self.total*100:.1f}%)\")\n",
    "        print(f\"  False Negatives: {self.fn} ({self.fn/self.total*100:.1f}%)\")\n",
    "        print(f\"  Total Errors:    {self.fp + self.fn} ({(self.fp + self.fn)/self.total*100:.1f}%)\")\n",
    "    \n",
    "    def plot_confusion_matrix(self, normalize=False):\n",
    "        \"\"\"\n",
    "        Plot confusion matrix as heatmap.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        normalize : bool\n",
    "            If True, show percentages instead of counts\n",
    "        \"\"\"\n",
    "        cm = self.get_confusion_matrix()\n",
    "        \n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            fmt = '.2%'\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "        else:\n",
    "            fmt = 'd'\n",
    "            title = 'Confusion Matrix'\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues', \n",
    "                   xticklabels=['Negative', 'Positive'],\n",
    "                   yticklabels=['Negative', 'Positive'],\n",
    "                   cbar_kws={'label': 'Percentage' if normalize else 'Count'})\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "print(\"âœ… ClassificationMetrics class implemented\")\n",
    "print(\"\\nAvailable Metrics:\")\n",
    "print(\"- accuracy(): Overall correct predictions\")\n",
    "print(\"- precision(): Positive predictive value\")\n",
    "print(\"- recall(): True positive rate (sensitivity)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66edab70",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eadd4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- specificity(): True negative rate\")\n",
    "print(\"- f1_score(): Harmonic mean of precision and recall\")\n",
    "print(\"- fbeta_score(beta): Weighted F score\")\n",
    "print(\"- matthews_corrcoef(): MCC for imbalanced data\")\n",
    "print(\"- roc_auc(): Area under ROC curve\")\n",
    "print(\"- summary(): Print all metrics\")\n",
    "print(\"- plot_confusion_matrix(): Visualize confusion matrix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c30418",
   "metadata": {},
   "source": [
    "## ðŸ“ Section 2: Advanced Classification Metrics - ROC and PR Curves\n",
    "\n",
    "### ROC Curve (Receiver Operating Characteristic)\n",
    "\n",
    "**Definition:** Plot of True Positive Rate (Recall) vs False Positive Rate across all classification thresholds.\n",
    "\n",
    "**Axes:**\n",
    "- **X-axis:** False Positive Rate (FPR) = $\\frac{FP}{FP + TN}$ = 1 - Specificity\n",
    "- **Y-axis:** True Positive Rate (TPR) = $\\frac{TP}{TP + FN}$ = Recall\n",
    "\n",
    "**How It Works:**\n",
    "1. Model outputs probabilities P(positive) for each sample\n",
    "2. For each threshold t âˆˆ [0, 1]:\n",
    "   - Classify as positive if P(positive) â‰¥ t\n",
    "   - Calculate TPR and FPR at this threshold\n",
    "3. Plot all (FPR, TPR) points\n",
    "4. Connect points to form ROC curve\n",
    "\n",
    "**Interpretation:**\n",
    "- **Diagonal line (FPR = TPR):** Random classifier (AUC = 0.5)\n",
    "- **Top-left corner:** Perfect classifier (TPR = 1, FPR = 0)\n",
    "- **Area Under Curve (AUC):** Overall model quality\n",
    "\n",
    "**AUC-ROC Values:**\n",
    "```\n",
    "AUC = 0.5:  Random guessing (coin flip)\n",
    "AUC = 0.7:  Fair model\n",
    "AUC = 0.8:  Good model\n",
    "AUC = 0.9:  Excellent model\n",
    "AUC = 1.0:  Perfect model\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- âœ… Threshold-independent (shows performance across all thresholds)\n",
    "- âœ… Good for balanced and imbalanced datasets\n",
    "- âœ… Easy to compare multiple models\n",
    "- âœ… Probabilistic interpretation (AUC = probability model ranks random positive higher than random negative)\n",
    "\n",
    "**Limitations:**\n",
    "- âŒ Overly optimistic for highly imbalanced datasets (high TN dominates)\n",
    "- âŒ Doesn't directly show precision\n",
    "- âŒ Doesn't incorporate class distribution\n",
    "\n",
    "### Precision-Recall Curve\n",
    "\n",
    "**Definition:** Plot of Precision vs Recall across all classification thresholds.\n",
    "\n",
    "**Axes:**\n",
    "- **X-axis:** Recall = $\\frac{TP}{TP + FN}$\n",
    "- **Y-axis:** Precision = $\\frac{TP}{TP + FP}$\n",
    "\n",
    "**When to Use:**\n",
    "- âœ… **Highly imbalanced datasets** (e.g., fraud detection: 0.1% fraud rate)\n",
    "- âœ… When positive class is the focus\n",
    "- âœ… When false positives are costly\n",
    "\n",
    "**Interpretation:**\n",
    "- **Top-right corner:** Perfect classifier (Precision = 1, Recall = 1)\n",
    "- **Baseline:** Random classifier â†’ Precision = proportion of positives\n",
    "- **Trade-off:** As recall increases (more liberal threshold), precision typically decreases\n",
    "\n",
    "**Average Precision (AP):**\n",
    "- Area under PR curve\n",
    "- Single-number summary\n",
    "- Better than AUC-ROC for imbalanced data\n",
    "\n",
    "### ROC vs PR Curve: When to Use Which?\n",
    "\n",
    "| Characteristic | ROC Curve | PR Curve |\n",
    "|----------------|-----------|----------|\n",
    "| **Balanced data** | âœ… Excellent | âœ… Good |\n",
    "| **Imbalanced data (1:99)** | âš ï¸ Optimistic | âœ… Realistic |\n",
    "| **Focus on positives** | âŒ No | âœ… Yes |\n",
    "| **Interpretability** | âœ… Easy (TPR vs FPR) | âš ï¸ Moderate |\n",
    "| **Threshold selection** | âœ… Yes | âœ… Yes |\n",
    "| **Multiple models** | âœ… Easy comparison | âœ… Easy comparison |\n",
    "\n",
    "**Rule of Thumb:**\n",
    "- **Use ROC:** Balanced datasets, care about both classes equally\n",
    "- **Use PR:** Imbalanced datasets, positive class is rare and important\n",
    "\n",
    "### Semiconductor Testing Example\n",
    "\n",
    "**Scenario:** Defect detection with 2% failure rate (highly imbalanced)\n",
    "\n",
    "**Two Models:**\n",
    "- **Model A:** AUC-ROC = 0.95, Average Precision = 0.70\n",
    "- **Model B:** AUC-ROC = 0.93, Average Precision = 0.80\n",
    "\n",
    "**Analysis:**\n",
    "- ROC suggests Model A is better (0.95 > 0.93)\n",
    "- PR suggests Model B is better (0.80 > 0.70)\n",
    "- **Which to trust?** PR curve (highly imbalanced, focus on detecting rare defects)\n",
    "\n",
    "**Why the difference?**\n",
    "- Model A achieves high AUC-ROC by correctly classifying many true negatives (98% of data)\n",
    "- Model B sacrifices some TN but does better job on positives (defects)\n",
    "- PR curve reveals Model B is better at actual defect detection\n",
    "\n",
    "### Threshold Selection Strategies\n",
    "\n",
    "**1. Maximize F1 Score:**\n",
    "```python\n",
    "# Find threshold that maximizes F1\n",
    "best_threshold = threshold_at_max_f1(precisions, recalls, thresholds)\n",
    "```\n",
    "\n",
    "**2. Fixed Recall (minimize FN):**\n",
    "```python\n",
    "# Ensure 95% recall, optimize precision\n",
    "threshold = threshold_at_recall(recalls, thresholds, target_recall=0.95)\n",
    "```\n",
    "\n",
    "**3. Fixed Precision (minimize FP):**\n",
    "```python\n",
    "# Ensure 90% precision, optimize recall\n",
    "threshold = threshold_at_precision(precisions, thresholds, target_precision=0.90)\n",
    "```\n",
    "\n",
    "**4. Cost-Based:**\n",
    "```python\n",
    "# Minimize: cost_FP * FP + cost_FN * FN\n",
    "threshold = threshold_min_cost(y_true, y_prob, cost_FP=100, cost_FN=10000)\n",
    "```\n",
    "\n",
    "**Semiconductor Example:**\n",
    "- Cost of test escape (FN): $10,000 per device\n",
    "- Cost of overkill (FP): $100 per device\n",
    "- â†’ Choose threshold that minimizes total cost: $100 Ã— FP + $10,000 Ã— FN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246390d9",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541496ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and PR Curve Implementation\n",
    "class ROCPRAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive ROC and Precision-Recall curve analysis.\n",
    "    \n",
    "    Provides:\n",
    "    - ROC curve plotting with AUC\n",
    "    - Precision-Recall curve with AP\n",
    "    - Threshold analysis and selection\n",
    "    - Cost-based optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, y_true, y_prob):\n",
    "        \"\"\"\n",
    "        Initialize with true labels and predicted probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true : array-like\n",
    "            True labels (0 or 1)\n",
    "        y_prob : array-like\n",
    "            Predicted probabilities for positive class\n",
    "        \"\"\"\n",
    "        self.y_true = np.array(y_true)\n",
    "        self.y_prob = np.array(y_prob)\n",
    "        \n",
    "        # Compute ROC curve\n",
    "        self.fpr, self.tpr, self.roc_thresholds = roc_curve(y_true, y_prob)\n",
    "        self.auc_roc = auc(self.fpr, self.tpr)\n",
    "        \n",
    "        # Compute Precision-Recall curve\n",
    "        self.precision, self.recall, self.pr_thresholds = precision_recall_curve(y_true, y_prob)\n",
    "        self.auc_pr = auc(self.recall, self.precision)\n",
    "    \n",
    "    def plot_roc_curve(self, ax=None):\n",
    "        \"\"\"\n",
    "        Plot ROC curve with AUC.\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        \n",
    "        ax.plot(self.fpr, self.tpr, linewidth=2, \n",
    "               label=f'ROC Curve (AUC = {self.auc_roc:.3f})')\n",
    "        ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "        \n",
    "        ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "        ax.set_ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "        ax.set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "        ax.legend(loc='lower right', fontsize=10)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.set_ylim([0, 1])\n",
    "        \n",
    "        if ax is None:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    def plot_pr_curve(self, ax=None):\n",
    "        \"\"\"\n",
    "        Plot Precision-Recall curve with AP.\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        \n",
    "        # Baseline (random classifier precision = positive rate)\n",
    "        baseline_precision = np.sum(self.y_true) / len(self.y_true)\n",
    "        \n",
    "        ax.plot(self.recall, self.precision, linewidth=2,\n",
    "               label=f'PR Curve (AP = {self.auc_pr:.3f})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c681e77",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "        ax.axhline(y=baseline_precision, color='k', linestyle='--', linewidth=1,\n",
    "                  label=f'Baseline (Precision = {baseline_precision:.3f})')\n",
    "        \n",
    "        ax.set_xlabel('Recall', fontsize=12)\n",
    "        ax.set_ylabel('Precision', fontsize=12)\n",
    "        ax.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "        ax.legend(loc='upper right', fontsize=10)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.set_ylim([0, 1])\n",
    "        \n",
    "        if ax is None:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    def find_threshold_at_recall(self, target_recall):\n",
    "        \"\"\"\n",
    "        Find threshold that achieves target recall.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target_recall : float\n",
    "            Desired recall (e.g., 0.95 for 95% recall)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        threshold, precision at that recall\n",
    "        \"\"\"\n",
    "        # Find index where recall >= target (recall decreases as we go along curve)\n",
    "        idx = np.where(self.recall >= target_recall)[0]\n",
    "        \n",
    "        if len(idx) == 0:\n",
    "            return None, None\n",
    "        \n",
    "        idx = idx[-1]  # Get last index that meets requirement\n",
    "        threshold = self.pr_thresholds[idx] if idx < len(self.pr_thresholds) else 0.0\n",
    "        precision = self.precision[idx]\n",
    "        \n",
    "        return threshold, precision\n",
    "    \n",
    "    def find_threshold_at_precision(self, target_precision):\n",
    "        \"\"\"\n",
    "        Find threshold that achieves target precision.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target_precision : float\n",
    "            Desired precision (e.g., 0.90 for 90% precision)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        threshold, recall at that precision\n",
    "        \"\"\"\n",
    "        idx = np.where(self.precision >= target_precision)[0]\n",
    "        \n",
    "        if len(idx) == 0:\n",
    "            return None, None\n",
    "        \n",
    "        idx = idx[0]  # Get first index that meets requirement\n",
    "        threshold = self.pr_thresholds[idx] if idx < len(self.pr_thresholds) else 1.0\n",
    "        recall = self.recall[idx]\n",
    "        \n",
    "        return threshold, recall\n",
    "    \n",
    "    def find_optimal_threshold_f1(self):\n",
    "        \"\"\"\n",
    "        Find threshold that maximizes F1 score.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c98003",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc9e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "        threshold, best_f1, precision, recall\n",
    "        \"\"\"\n",
    "        # Calculate F1 for each threshold\n",
    "        f1_scores = []\n",
    "        \n",
    "        for i in range(len(self.pr_thresholds)):\n",
    "            prec = self.precision[i]\n",
    "            rec = self.recall[i]\n",
    "            \n",
    "            if (prec + rec) > 0:\n",
    "                f1 = 2 * (prec * rec) / (prec + rec)\n",
    "            else:\n",
    "                f1 = 0\n",
    "            \n",
    "            f1_scores.append(f1)\n",
    "        \n",
    "        best_idx = np.argmax(f1_scores)\n",
    "        best_threshold = self.pr_thresholds[best_idx]\n",
    "        best_f1 = f1_scores[best_idx]\n",
    "        best_precision = self.precision[best_idx]\n",
    "        best_recall = self.recall[best_idx]\n",
    "        \n",
    "        return best_threshold, best_f1, best_precision, best_recall\n",
    "    \n",
    "    def find_optimal_threshold_cost(self, cost_fp, cost_fn):\n",
    "        \"\"\"\n",
    "        Find threshold that minimizes total cost.\n",
    "        \n",
    "        Total Cost = cost_fp * FP + cost_fn * FN\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        cost_fp : float\n",
    "            Cost of one false positive\n",
    "        cost_fn : float\n",
    "            Cost of one false negative\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        best_threshold, min_cost, FP, FN\n",
    "        \"\"\"\n",
    "        n_pos = np.sum(self.y_true)\n",
    "        n_neg = len(self.y_true) - n_pos\n",
    "        \n",
    "        costs = []\n",
    "        thresholds_to_test = np.linspace(0, 1, 100)\n",
    "        \n",
    "        for threshold in thresholds_to_test:\n",
    "            y_pred = (self.y_prob >= threshold).astype(int)\n",
    "            \n",
    "            # Calculate confusion matrix\n",
    "            cm = confusion_matrix(self.y_true, y_pred)\n",
    "            if cm.shape == (2, 2):\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "            else:\n",
    "                # Handle edge cases\n",
    "                fp = fn = 0\n",
    "            \n",
    "            total_cost = cost_fp * fp + cost_fn * fn\n",
    "            costs.append((threshold, total_cost, fp, fn))\n",
    "        \n",
    "        # Find minimum cost\n",
    "        best_result = min(costs, key=lambda x: x[1])\n",
    "        \n",
    "        return best_result  # (threshold, min_cost, FP, FN)\n",
    "    \n",
    "    def plot_threshold_analysis(self):\n",
    "        \"\"\"\n",
    "        Plot how precision, recall, and F1 change with threshold.\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52de8314",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba585e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Calculate F1 scores\n",
    "        f1_scores = []\n",
    "        for i in range(len(self.pr_thresholds)):\n",
    "            prec = self.precision[i]\n",
    "            rec = self.recall[i]\n",
    "            if (prec + rec) > 0:\n",
    "                f1 = 2 * (prec * rec) / (prec + rec)\n",
    "            else:\n",
    "                f1 = 0\n",
    "            f1_scores.append(f1)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        ax.plot(self.pr_thresholds, self.precision[:-1], label='Precision', linewidth=2)\n",
    "        ax.plot(self.pr_thresholds, self.recall[:-1], label='Recall', linewidth=2)\n",
    "        ax.plot(self.pr_thresholds, f1_scores, label='F1 Score', linewidth=2, linestyle='--')\n",
    "        \n",
    "        # Mark optimal F1 threshold\n",
    "        best_threshold, best_f1, _, _ = self.find_optimal_threshold_f1()\n",
    "        ax.axvline(x=best_threshold, color='red', linestyle=':', linewidth=2,\n",
    "                  label=f'Optimal F1 Threshold = {best_threshold:.3f}')\n",
    "        \n",
    "        ax.set_xlabel('Threshold', fontsize=12)\n",
    "        ax.set_ylabel('Score', fontsize=12)\n",
    "        ax.set_title('Metrics vs Threshold', fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.set_ylim([0, 1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "print(\"âœ… ROCPRAnalyzer class implemented\")\n",
    "print(\"\\nAvailable Methods:\")\n",
    "print(\"- plot_roc_curve(): Plot ROC curve with AUC\")\n",
    "print(\"- plot_pr_curve(): Plot Precision-Recall curve with AP\")\n",
    "print(\"- find_threshold_at_recall(): Get threshold for target recall\")\n",
    "print(\"- find_threshold_at_precision(): Get threshold for target precision\")\n",
    "print(\"- find_optimal_threshold_f1(): Maximize F1 score\")\n",
    "print(\"- find_optimal_threshold_cost(): Minimize cost (FP and FN costs)\")\n",
    "print(\"- plot_threshold_analysis(): Visualize metrics vs threshold\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e37e01",
   "metadata": {},
   "source": [
    "## ðŸ“Š Regression Metrics Theory\n",
    "\n",
    "When evaluating regression models (predicting continuous values), we need different metrics than classification. In **post-silicon validation**, regression metrics help evaluate:\n",
    "\n",
    "- **Test time prediction**: Predict how long a device test will take (milliseconds)\n",
    "- **Parametric yield prediction**: Predict continuous yield percentage (0-100%)\n",
    "- **Power consumption models**: Predict device power draw (watts/milliwatts)\n",
    "- **Temperature prediction**: Predict junction temperature under load (Â°C)\n",
    "\n",
    "### Why Regression Metrics Differ from Classification\n",
    "\n",
    "1. **Continuous outputs**: Predictions can be infinitely close to actual values\n",
    "2. **Error magnitude matters**: Being off by 0.1% vs 10% has very different implications\n",
    "3. **Direction of error**: Over-prediction vs under-prediction may have different costs\n",
    "4. **Scale sensitivity**: Errors should be interpreted in context of the target variable's range\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”‘ Key Regression Metrics\n",
    "\n",
    "#### 1. Mean Squared Error (MSE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- **Units**: Squared units of target variable (e.g., msÂ² for test time)\n",
    "- **Range**: [0, âˆž), where 0 is perfect\n",
    "- **Penalty**: Heavily penalizes large errors (quadratic)\n",
    "- **Use when**: Large errors are disproportionately costly\n",
    "\n",
    "**Example:** Test time prediction with MSE = 25 msÂ² means average squared error is 25 msÂ²\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Root Mean Squared Error (RMSE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} = \\sqrt{MSE}\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- **Units**: Same as target variable (e.g., ms for test time)\n",
    "- **Range**: [0, âˆž), where 0 is perfect\n",
    "- **Meaning**: \"On average, predictions are off by Â± RMSE\"\n",
    "- **Use when**: You need interpretable error magnitude\n",
    "\n",
    "**Example:** Test time RMSE = 5 ms means predictions are typically off by Â±5 milliseconds\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Mean Absolute Error (MAE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- **Units**: Same as target variable\n",
    "- **Range**: [0, âˆž), where 0 is perfect\n",
    "- **Penalty**: Linear penalty for errors\n",
    "- **Robust**: Less sensitive to outliers than MSE/RMSE\n",
    "- **Use when**: All errors weighted equally, outliers present\n",
    "\n",
    "**Example:** Yield MAE = 2% means average absolute deviation is 2 percentage points\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. R-Squared (RÂ²) / Coefficient of Determination\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $SS_{res}$ = Residual sum of squares (model error)\n",
    "- $SS_{tot}$ = Total sum of squares (variance in data)\n",
    "- $\\bar{y}$ = Mean of actual values\n",
    "\n",
    "**Interpretation:**\n",
    "- **Range**: (-âˆž, 1], typically [0, 1]\n",
    "- **Meaning**: Proportion of variance explained by the model\n",
    "- **RÂ² = 1.0**: Perfect predictions\n",
    "- **RÂ² = 0.0**: Model no better than predicting mean\n",
    "- **RÂ² < 0**: Model worse than predicting mean\n",
    "- **Use when**: Comparing models on same dataset, need variance explanation\n",
    "\n",
    "**Example:** RÂ² = 0.85 means model explains 85% of variance in test time\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "MAPE = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- **Units**: Percentage (%)\n",
    "- **Range**: [0, âˆž), where 0 is perfect\n",
    "- **Scale-independent**: Can compare across different target ranges\n",
    "- **Use when**: Target variable has meaningful zero, want relative error\n",
    "- **Limitation**: Undefined when $y_i = 0$, biased toward under-predictions\n",
    "\n",
    "**Example:** MAPE = 5% means predictions are off by 5% on average\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Max Error\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Max Error} = \\max_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- **Units**: Same as target variable\n",
    "- **Range**: [0, âˆž)\n",
    "- **Meaning**: Worst-case prediction error\n",
    "- **Use when**: Need to guarantee maximum acceptable error\n",
    "\n",
    "**Example:** Max Error = 50 ms means worst test time prediction was off by 50 ms\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Decision Guide: Which Regression Metric to Use?\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Need to evaluate regression model] --> B{Need interpretable<br/>error magnitude?}\n",
    "    B -->|Yes| C{Outliers<br/>present?}\n",
    "    B -->|No| D{Compare models<br/>on same data?}\n",
    "    \n",
    "    C -->|Yes| E[Use MAE<br/>Robust to outliers]\n",
    "    C -->|No| F[Use RMSE<br/>Penalizes large errors]\n",
    "    \n",
    "    D -->|Yes| G[Use RÂ²<br/>Variance explained]\n",
    "    D -->|No| H{Scale-independent<br/>comparison needed?}\n",
    "    \n",
    "    H -->|Yes| I[Use MAPE<br/>Percentage error]\n",
    "    H -->|No| J{Worst-case<br/>guarantee needed?}\n",
    "    \n",
    "    J -->|Yes| K[Use Max Error<br/>Safety critical]\n",
    "    J -->|No| L[Use MSE<br/>Loss function]\n",
    "```\n",
    "\n",
    "### Post-Silicon Validation Context\n",
    "\n",
    "| **Use Case** | **Recommended Metrics** | **Why** |\n",
    "|--------------|------------------------|---------|\n",
    "| **Test time prediction** | RMSE + MAE + Max Error | Interpretable (ms), robust (MAE), worst-case (Max) |\n",
    "| **Yield prediction** | RMSE + RÂ² + MAPE | Interpretable (%), variance explained, scale-independent |\n",
    "| **Power consumption** | MAE + MAPE | Robust to outliers, percentage useful for efficiency |\n",
    "| **Spatial modeling** | RMSE + RÂ² | Penalize large spatial errors, variance explanation |\n",
    "| **Model comparison** | RÂ² + RMSE | Standardized comparison, interpretable error |\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Using MAPE with zeros**: Undefined when actual = 0 (e.g., zero power consumption)\n",
    "2. **Ignoring scale**: MSE = 100 is good for large values, terrible for small values\n",
    "3. **Over-relying on RÂ²**: Can be high even with poor predictions if variance is low\n",
    "4. **Comparing MSE across datasets**: Only valid for same target variable\n",
    "5. **Forgetting residuals**: Always visualize residuals to check assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b17fbe",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee3ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "class RegressionMetrics:\n",
    "    \"\"\"\n",
    "    Comprehensive regression metrics calculator for model evaluation.\n",
    "    \n",
    "    Computes key metrics for continuous target predictions:\n",
    "    - Mean Squared Error (MSE)\n",
    "    - Root Mean Squared Error (RMSE)  \n",
    "    - Mean Absolute Error (MAE)\n",
    "    - R-Squared (RÂ²)\n",
    "    - Mean Absolute Percentage Error (MAPE)\n",
    "    - Max Error\n",
    "    \n",
    "    Also provides residual analysis and visualization methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, y_true: np.ndarray, y_pred: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initialize with true and predicted values.\n",
    "        \n",
    "        Args:\n",
    "            y_true: Actual target values (n_samples,)\n",
    "            y_pred: Predicted target values (n_samples,)\n",
    "        \"\"\"\n",
    "        self.y_true = np.array(y_true).flatten()\n",
    "        self.y_pred = np.array(y_pred).flatten()\n",
    "        \n",
    "        if len(self.y_true) != len(self.y_pred):\n",
    "            raise ValueError(\"y_true and y_pred must have same length\")\n",
    "        \n",
    "        # Compute residuals (errors)\n",
    "        self.residuals = self.y_true - self.y_pred\n",
    "        self.n_samples = len(self.y_true)\n",
    "        self.y_mean = np.mean(self.y_true)\n",
    "    \n",
    "    def mse(self) -> float:\n",
    "        \"\"\"\n",
    "        Compute Mean Squared Error.\n",
    "        \n",
    "        MSE = (1/n) * Î£(y_i - Å·_i)Â²\n",
    "        \n",
    "        Returns:\n",
    "            Mean squared error (float)\n",
    "        \"\"\"\n",
    "        return np.mean(self.residuals ** 2)\n",
    "    \n",
    "    def rmse(self) -> float:\n",
    "        \"\"\"\n",
    "        Compute Root Mean Squared Error.\n",
    "        \n",
    "        RMSE = âˆš(MSE)\n",
    "        Same units as target variable.\n",
    "        \n",
    "        Returns:\n",
    "            Root mean squared error (float)\n",
    "        \"\"\"\n",
    "        return np.sqrt(self.mse())\n",
    "    \n",
    "    def mae(self) -> float:\n",
    "        \"\"\"\n",
    "        Compute Mean Absolute Error.\n",
    "        \n",
    "        MAE = (1/n) * Î£|y_i - Å·_i|\n",
    "        Robust to outliers compared to MSE.\n",
    "        \n",
    "        Returns:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09308c",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5031685",
   "metadata": {},
   "outputs": [],
   "source": [
    "            Mean absolute error (float)\n",
    "        \"\"\"\n",
    "        return np.mean(np.abs(self.residuals))\n",
    "    \n",
    "    def r2_score(self) -> float:\n",
    "        \"\"\"\n",
    "        Compute R-Squared (coefficient of determination).\n",
    "        \n",
    "        RÂ² = 1 - (SS_res / SS_tot)\n",
    "        where SS_res = Î£(y_i - Å·_i)Â²  [residual sum of squares]\n",
    "              SS_tot = Î£(y_i - È³)Â²    [total sum of squares]\n",
    "        \n",
    "        Range: (-âˆž, 1], where 1 is perfect, 0 is mean predictor\n",
    "        \n",
    "        Returns:\n",
    "            R-squared score (float)\n",
    "        \"\"\"\n",
    "        ss_res = np.sum(self.residuals ** 2)\n",
    "        ss_tot = np.sum((self.y_true - self.y_mean) ** 2)\n",
    "        \n",
    "        # Handle edge case: constant target\n",
    "        if ss_tot == 0:\n",
    "            return 1.0 if ss_res == 0 else 0.0\n",
    "        \n",
    "        return 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    def mape(self) -> float:\n",
    "        \"\"\"\n",
    "        Compute Mean Absolute Percentage Error.\n",
    "        \n",
    "        MAPE = (100/n) * Î£|y_i - Å·_i| / |y_i|\n",
    "        Scale-independent metric (percentage).\n",
    "        \n",
    "        Returns:\n",
    "            Mean absolute percentage error in % (float)\n",
    "            Returns np.inf if any y_true is zero\n",
    "        \"\"\"\n",
    "        # Check for zeros in y_true\n",
    "        if np.any(self.y_true == 0):\n",
    "            print(\"Warning: MAPE is undefined when actual values contain zeros\")\n",
    "            return np.inf\n",
    "        \n",
    "        return 100 * np.mean(np.abs(self.residuals / self.y_true))\n",
    "    \n",
    "    def max_error(self) -> float:\n",
    "        \"\"\"\n",
    "        Compute maximum absolute error (worst-case).\n",
    "        \n",
    "        Max Error = max|y_i - Å·_i|\n",
    "        \n",
    "        Returns:\n",
    "            Maximum absolute error (float)\n",
    "        \"\"\"\n",
    "        return np.max(np.abs(self.residuals))\n",
    "    \n",
    "    def adjusted_r2(self, n_features: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute Adjusted R-Squared (penalizes for number of features).\n",
    "        \n",
    "        Adjusted RÂ² = 1 - [(1-RÂ²) * (n-1) / (n-p-1)]\n",
    "        where n = number of samples, p = number of features\n",
    "        \n",
    "        Args:\n",
    "            n_features: Number of features used in model\n",
    "            \n",
    "        Returns:\n",
    "            Adjusted R-squared score (float)\n",
    "        \"\"\"\n",
    "        r2 = self.r2_score()\n",
    "        n = self.n_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e38d7e",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf11c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "        p = n_features\n",
    "        \n",
    "        if n <= p + 1:\n",
    "            return np.nan  # Not enough samples\n",
    "        \n",
    "        return 1 - ((1 - r2) * (n - 1) / (n - p - 1))\n",
    "    \n",
    "    def get_residual_stats(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get comprehensive residual statistics.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with residual statistics\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'mean_residual': np.mean(self.residuals),\n",
    "            'std_residual': np.std(self.residuals),\n",
    "            'min_residual': np.min(self.residuals),\n",
    "            'max_residual': np.max(self.residuals),\n",
    "            'median_residual': np.median(self.residuals),\n",
    "            'q25_residual': np.percentile(self.residuals, 25),\n",
    "            'q75_residual': np.percentile(self.residuals, 75)\n",
    "        }\n",
    "    \n",
    "    def summary(self) -> None:\n",
    "        \"\"\"\n",
    "        Print comprehensive metrics summary.\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"REGRESSION METRICS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Number of samples: {self.n_samples}\")\n",
    "        print(f\"Target mean: {self.y_mean:.4f}\")\n",
    "        print(f\"Target std: {np.std(self.y_true):.4f}\")\n",
    "        print(f\"Target range: [{np.min(self.y_true):.4f}, {np.max(self.y_true):.4f}]\")\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"PRIMARY METRICS\")\n",
    "        print(\"-\"*60)\n",
    "        print(f\"MSE:        {self.mse():.6f}\")\n",
    "        print(f\"RMSE:       {self.rmse():.6f}\")\n",
    "        print(f\"MAE:        {self.mae():.6f}\")\n",
    "        print(f\"RÂ²:         {self.r2_score():.6f}\")\n",
    "        print(f\"MAPE:       {self.mape():.4f}%\")\n",
    "        print(f\"Max Error:  {self.max_error():.6f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"RESIDUAL STATISTICS\")\n",
    "        print(\"-\"*60)\n",
    "        stats = self.get_residual_stats()\n",
    "        for key, value in stats.items():\n",
    "            print(f\"{key:20s}: {value:10.6f}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def plot_predictions(self, title: str = \"Predicted vs Actual\", \n",
    "                        figsize: Tuple[int, int] = (12, 4)) -> None:\n",
    "        \"\"\"\n",
    "        Visualize predictions and residuals.\n",
    "        \n",
    "        Creates 3 subplots:\n",
    "        1. Predicted vs Actual (scatter with perfect prediction line)\n",
    "        2. Residuals vs Predicted (check for patterns)\n",
    "        3. Residual distribution (check normality)\n",
    "        \n",
    "        Args:\n",
    "            title: Plot title prefix\n",
    "            figsize: Figure size (width, height)\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "        \n",
    "        # Plot 1: Predicted vs Actual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322e1792",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382af4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "        axes[0].scatter(self.y_true, self.y_pred, alpha=0.6, s=30, edgecolors='k', linewidth=0.5)\n",
    "        \n",
    "        # Perfect prediction line\n",
    "        min_val = min(np.min(self.y_true), np.min(self.y_pred))\n",
    "        max_val = max(np.max(self.y_true), np.max(self.y_pred))\n",
    "        axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "        \n",
    "        axes[0].set_xlabel('Actual Values', fontsize=11, fontweight='bold')\n",
    "        axes[0].set_ylabel('Predicted Values', fontsize=11, fontweight='bold')\n",
    "        axes[0].set_title(f'{title}\\nRÂ² = {self.r2_score():.4f}', fontsize=12, fontweight='bold')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Residuals vs Predicted\n",
    "        axes[1].scatter(self.y_pred, self.residuals, alpha=0.6, s=30, edgecolors='k', linewidth=0.5)\n",
    "        axes[1].axhline(y=0, color='r', linestyle='--', lw=2, label='Zero Residual')\n",
    "        \n",
    "        axes[1].set_xlabel('Predicted Values', fontsize=11, fontweight='bold')\n",
    "        axes[1].set_ylabel('Residuals (Actual - Predicted)', fontsize=11, fontweight='bold')\n",
    "        axes[1].set_title(f'Residual Plot\\nMAE = {self.mae():.4f}', fontsize=12, fontweight='bold')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Residual Distribution\n",
    "        axes[2].hist(self.residuals, bins=30, alpha=0.7, edgecolor='black', color='skyblue')\n",
    "        axes[2].axvline(x=0, color='r', linestyle='--', lw=2, label='Zero')\n",
    "        axes[2].axvline(x=np.mean(self.residuals), color='orange', linestyle='--', lw=2, \n",
    "                       label=f'Mean = {np.mean(self.residuals):.4f}')\n",
    "        \n",
    "        axes[2].set_xlabel('Residuals', fontsize=11, fontweight='bold')\n",
    "        axes[2].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "        axes[2].set_title(f'Residual Distribution\\nRMSE = {self.rmse():.4f}', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[2].legend()\n",
    "        axes[2].grid(alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_error_distribution(self, figsize: Tuple[int, int] = (10, 5)) -> None:\n",
    "        \"\"\"\n",
    "        Visualize error distribution with box plot and violin plot.\n",
    "        \n",
    "        Args:\n",
    "            figsize: Figure size (width, height)\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Box plot\n",
    "        bp = axes[0].boxplot(self.residuals, vert=True, patch_artist=True)\n",
    "        bp['boxes'][0].set_facecolor('lightblue')\n",
    "        bp['medians'][0].set_color('red')\n",
    "        bp['medians'][0].set_linewidth(2)\n",
    "        \n",
    "        axes[0].set_ylabel('Residuals', fontsize=11, fontweight='bold')\n",
    "        axes[0].set_title('Residual Box Plot', fontsize=12, fontweight='bold')\n",
    "        axes[0].grid(alpha=0.3, axis='y')\n",
    "        axes[0].axhline(y=0, color='red', linestyle='--', lw=1.5)\n",
    "        \n",
    "        # Violin plot\n",
    "        parts = axes[1].violinplot([self.residuals], vert=True, showmeans=True, showmedians=True)\n",
    "        for pc in parts['bodies']:\n",
    "            pc.set_facecolor('lightgreen')\n",
    "            pc.set_alpha(0.7)\n",
    "        \n",
    "        axes[1].set_ylabel('Residuals', fontsize=11, fontweight='bold')\n",
    "        axes[1].set_title('Residual Violin Plot', fontsize=12, fontweight='bold')\n",
    "        axes[1].grid(alpha=0.3, axis='y')\n",
    "        axes[1].axhline(y=0, color='red', linestyle='--', lw=1.5)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8abd36",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation Part 5\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc3e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "# Example usage demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic test time data (post-silicon validation example)\n",
    "    np.random.seed(42)\n",
    "    n_samples = 100\n",
    "    \n",
    "    # Actual test times (ms)\n",
    "    y_true = 50 + 20 * np.random.randn(n_samples)\n",
    "    \n",
    "    # Predicted test times (with some error)\n",
    "    y_pred = y_true + 5 * np.random.randn(n_samples)\n",
    "    \n",
    "    # Create metrics object\n",
    "    metrics = RegressionMetrics(y_true, y_pred)\n",
    "    \n",
    "    # Print summary\n",
    "    metrics.summary()\n",
    "    \n",
    "    # Visualize\n",
    "    metrics.plot_predictions(title=\"Test Time Prediction\")\n",
    "    metrics.plot_error_distribution()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1970d738",
   "metadata": {},
   "source": [
    "## ðŸ”§ Cost-Sensitive Evaluation and Custom Metrics\n",
    "\n",
    "In production environments, especially **post-silicon validation**, different types of errors often have **different costs**. A false negative (missing a defect) might cost $10M in field failures, while a false positive (unnecessary retest) might cost only $50K.\n",
    "\n",
    "### Cost Matrices in Classification\n",
    "\n",
    "For binary classification, we can define a **cost matrix**:\n",
    "\n",
    "|                    | **Predicted Negative** | **Predicted Positive** |\n",
    "|--------------------|------------------------|------------------------|\n",
    "| **Actual Negative** | TN (Cost = 0)         | FP (Cost = C_FP)      |\n",
    "| **Actual Positive** | FN (Cost = C_FN)      | TP (Cost = 0)         |\n",
    "\n",
    "**Total Cost:**\n",
    "$$\n",
    "\\text{Total Cost} = C_{FP} \\times FP + C_{FN} \\times FN\n",
    "$$\n",
    "\n",
    "**Semiconductor Example:**\n",
    "- Device fails but test passes (FN): **$10,000,000** per escaped defect\n",
    "- Device passes but test fails (FP): **$50,000** per false alarm (retest cost)\n",
    "- Cost ratio: FN is 200Ã— more expensive than FP\n",
    "\n",
    "**Optimal threshold:** Not necessarily 0.5! We should minimize total cost:\n",
    "$$\n",
    "t^* = \\arg\\min_t \\left[ C_{FP} \\times FP(t) + C_{FN} \\times FN(t) \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Custom Metrics for Business Objectives\n",
    "\n",
    "Beyond standard metrics, you may need **domain-specific metrics**:\n",
    "\n",
    "#### 1. **Weighted Accuracy** (imbalanced classes with known costs)\n",
    "\n",
    "$$\n",
    "\\text{Weighted Acc} = \\frac{w_0 \\times TN + w_1 \\times TP}{w_0 \\times (TN + FP) + w_1 \\times (TP + FN)}\n",
    "$$\n",
    "\n",
    "Where $w_0$, $w_1$ are class weights (inversely proportional to class frequency).\n",
    "\n",
    "#### 2. **Top-K Accuracy** (information retrieval, recommendations)\n",
    "\n",
    "$$\n",
    "\\text{Top-K Acc} = \\frac{\\text{# correct predictions in top K}}{n}\n",
    "$$\n",
    "\n",
    "Useful when you only act on top K predictions (e.g., investigate top 10 suspected defects).\n",
    "\n",
    "#### 3. **Expected Calibration Error (ECE)** (probability calibration)\n",
    "\n",
    "$$\n",
    "ECE = \\sum_{m=1}^{M} \\frac{|B_m|}{n} \\left| acc(B_m) - conf(B_m) \\right|\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- Predictions binned into M bins by confidence\n",
    "- $B_m$ = samples in bin m\n",
    "- $acc(B_m)$ = accuracy in bin m\n",
    "- $conf(B_m)$ = average confidence in bin m\n",
    "\n",
    "**Interpretation:** ECE measures how well predicted probabilities match actual outcomes. ECE = 0 means perfectly calibrated.\n",
    "\n",
    "#### 4. **Production Throughput Metric** (semiconductor specific)\n",
    "\n",
    "$$\n",
    "\\text{Throughput} = \\frac{\\text{Tested Devices}}{\\text{Total Test Time}} \\times (1 - \\text{False Positive Rate})\n",
    "$$\n",
    "\n",
    "Accounts for both speed and accuracy. High FPR reduces effective throughput due to retests.\n",
    "\n",
    "---\n",
    "\n",
    "### Multi-Class and Multi-Label Considerations\n",
    "\n",
    "#### Multi-Class Classification (one label per sample)\n",
    "\n",
    "**Macro-average:** Compute metric for each class, then average\n",
    "$$\n",
    "\\text{Macro-F1} = \\frac{1}{K} \\sum_{k=1}^{K} F1_k\n",
    "$$\n",
    "\n",
    "**Micro-average:** Aggregate TP, FP, FN across all classes\n",
    "$$\n",
    "\\text{Micro-F1} = \\frac{2 \\times \\text{Total TP}}{2 \\times \\text{Total TP} + \\text{Total FP} + \\text{Total FN}}\n",
    "$$\n",
    "\n",
    "**Weighted-average:** Weight by class support (number of samples)\n",
    "$$\n",
    "\\text{Weighted-F1} = \\sum_{k=1}^{K} \\frac{n_k}{n} F1_k\n",
    "$$\n",
    "\n",
    "**When to use:**\n",
    "- **Macro**: All classes equally important (rare classes matter)\n",
    "- **Micro**: Large classes more important (overall accuracy)\n",
    "- **Weighted**: Balance by class size (common in imbalanced datasets)\n",
    "\n",
    "#### Multi-Label Classification (multiple labels per sample)\n",
    "\n",
    "Each sample can have multiple labels (e.g., device has both \"high power\" and \"low frequency\" defects).\n",
    "\n",
    "**Hamming Loss:**\n",
    "$$\n",
    "\\text{Hamming} = \\frac{1}{n \\times L} \\sum_{i=1}^{n} \\sum_{j=1}^{L} \\mathbb{1}(y_{ij} \\neq \\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "**Subset Accuracy (exact match):**\n",
    "$$\n",
    "\\text{Subset Acc} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}(y_i = \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "Only counts prediction as correct if ALL labels match exactly.\n",
    "\n",
    "---\n",
    "\n",
    "### Cross-Validation and Evaluation Strategy\n",
    "\n",
    "#### Why Cross-Validation?\n",
    "\n",
    "Single train/test split can be **misleading**:\n",
    "- Result depends on specific split\n",
    "- May not represent true performance\n",
    "- Small datasets: high variance in estimates\n",
    "\n",
    "**K-Fold Cross-Validation:**\n",
    "1. Split data into K equal folds\n",
    "2. For each fold k:\n",
    "   - Train on K-1 folds\n",
    "   - Validate on fold k\n",
    "3. Average metrics across K folds\n",
    "\n",
    "**Stratified K-Fold:** Maintains class distribution in each fold (important for imbalanced data)\n",
    "\n",
    "**Typical K values:**\n",
    "- K=5: Fast, reasonable variance\n",
    "- K=10: Standard choice, lower variance\n",
    "- K=n (LOO): Maximum data usage, high computational cost\n",
    "\n",
    "#### Time Series Considerations\n",
    "\n",
    "**DO NOT use random K-Fold for time series!** This causes **data leakage** (future predicting past).\n",
    "\n",
    "**Use Time Series Split instead:**\n",
    "```\n",
    "Fold 1: Train [1:100]   â†’ Test [101:120]\n",
    "Fold 2: Train [1:120]   â†’ Test [121:140]\n",
    "Fold 3: Train [1:140]   â†’ Test [141:160]\n",
    "...\n",
    "```\n",
    "\n",
    "**Post-silicon example:** Test data from Week 10 should NOT be used to train model evaluated on Week 5 data.\n",
    "\n",
    "---\n",
    "\n",
    "### Statistical Significance Testing\n",
    "\n",
    "Is Model A **truly better** than Model B, or just lucky on this dataset?\n",
    "\n",
    "#### McNemar's Test (paired classification models)\n",
    "\n",
    "Tests if two models have **significantly different error rates**.\n",
    "\n",
    "**Contingency table:**\n",
    "|              | Model B Correct | Model B Wrong |\n",
    "|--------------|----------------|---------------|\n",
    "| **Model A Correct** | a              | b             |\n",
    "| **Model A Wrong**   | c              | d             |\n",
    "\n",
    "**Test statistic:**\n",
    "$$\n",
    "\\chi^2 = \\frac{(b - c)^2}{b + c}\n",
    "$$\n",
    "\n",
    "Under null hypothesis (models equivalent), $\\chi^2 \\sim \\chi^2(1)$\n",
    "\n",
    "**Interpretation:** If p-value < 0.05, models are significantly different.\n",
    "\n",
    "#### Paired t-test (multiple datasets/folds)\n",
    "\n",
    "Compare mean performance across K folds.\n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar{d}}{s_d / \\sqrt{K}}\n",
    "$$\n",
    "\n",
    "Where $\\bar{d}$ = mean difference, $s_d$ = standard deviation of differences.\n",
    "\n",
    "---\n",
    "\n",
    "### Production Monitoring Metrics\n",
    "\n",
    "Once deployed, monitor these metrics **continuously**:\n",
    "\n",
    "1. **Prediction Distribution Drift**: Has distribution of predicted probabilities changed?\n",
    "2. **Feature Drift**: Have input features changed (mean, std, range)?\n",
    "3. **Performance Degradation**: Are metrics declining over time?\n",
    "4. **Calibration Drift**: Are predicted probabilities still calibrated?\n",
    "5. **Latency and Throughput**: Is model still meeting SLAs?\n",
    "\n",
    "**Alerting thresholds:**\n",
    "- Accuracy drops > 5% from baseline\n",
    "- Feature means shift > 2 standard deviations\n",
    "- 95th percentile latency exceeds SLA\n",
    "\n",
    "**Post-silicon example:** If test yield predictions suddenly drop in accuracy, it may indicate:\n",
    "- Process change in manufacturing\n",
    "- New device architecture\n",
    "- Test equipment calibration drift\n",
    "- Model staleness (needs retraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca85b2c4",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8449293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.metrics import confusion_matrix\n",
    "class CostSensitiveEvaluator:\n",
    "    \"\"\"\n",
    "    Cost-sensitive evaluation for classification models.\n",
    "    \n",
    "    Allows custom cost matrices for FP and FN errors,\n",
    "    finds optimal thresholds to minimize total cost,\n",
    "    and performs multi-class evaluation with different averaging strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, y_true: np.ndarray, y_prob: np.ndarray, \n",
    "                 cost_fp: float = 1.0, cost_fn: float = 1.0):\n",
    "        \"\"\"\n",
    "        Initialize with true labels and predicted probabilities.\n",
    "        \n",
    "        Args:\n",
    "            y_true: True binary labels (0/1)\n",
    "            y_prob: Predicted probabilities for positive class\n",
    "            cost_fp: Cost of false positive error\n",
    "            cost_fn: Cost of false negative error\n",
    "        \"\"\"\n",
    "        self.y_true = np.array(y_true)\n",
    "        self.y_prob = np.array(y_prob)\n",
    "        self.cost_fp = cost_fp\n",
    "        self.cost_fn = cost_fn\n",
    "        \n",
    "        if len(self.y_true) != len(self.y_prob):\n",
    "            raise ValueError(\"y_true and y_prob must have same length\")\n",
    "    \n",
    "    def compute_cost(self, threshold: float) -> Tuple[float, int, int]:\n",
    "        \"\"\"\n",
    "        Compute total cost at given threshold.\n",
    "        \n",
    "        Args:\n",
    "            threshold: Classification threshold (0-1)\n",
    "            \n",
    "        Returns:\n",
    "            (total_cost, num_fp, num_fn)\n",
    "        \"\"\"\n",
    "        y_pred = (self.y_prob >= threshold).astype(int)\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(self.y_true, y_pred).ravel()\n",
    "        \n",
    "        # Compute cost\n",
    "        total_cost = self.cost_fp * fp + self.cost_fn * fn\n",
    "        \n",
    "        return total_cost, fp, fn\n",
    "    \n",
    "    def find_optimal_threshold(self, thresholds: np.ndarray = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Find threshold that minimizes total cost.\n",
    "        \n",
    "        Args:\n",
    "            thresholds: Array of thresholds to evaluate (default: 100 points)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with optimal threshold and costs\n",
    "        \"\"\"\n",
    "        if thresholds is None:\n",
    "            thresholds = np.linspace(0, 1, 100)\n",
    "        \n",
    "        costs = []\n",
    "        fps = []\n",
    "        fns = []\n",
    "        \n",
    "        for t in thresholds:\n",
    "            cost, fp, fn = self.compute_cost(t)\n",
    "            costs.append(cost)\n",
    "            fps.append(fp)\n",
    "            fns.append(fn)\n",
    "        \n",
    "        # Find minimum cost\n",
    "        optimal_idx = np.argmin(costs)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        \n",
    "        return {\n",
    "            'optimal_threshold': optimal_threshold,\n",
    "            'min_cost': costs[optimal_idx],\n",
    "            'fp_at_optimal': fps[optimal_idx],\n",
    "            'fn_at_optimal': fns[optimal_idx],\n",
    "            'all_thresholds': thresholds,\n",
    "            'all_costs': np.array(costs),\n",
    "            'all_fps': np.array(fps),\n",
    "            'all_fns': np.array(fns)\n",
    "        }\n",
    "    \n",
    "    def plot_cost_analysis(self, result: Dict = None, figsize: Tuple[int, int] = (14, 5)):\n",
    "        \"\"\"\n",
    "        Visualize cost vs threshold and error breakdown.\n",
    "        \n",
    "        Args:\n",
    "            result: Result from find_optimal_threshold() (or compute if None)\n",
    "            figsize: Figure size\n",
    "        \"\"\"\n",
    "        if result is None:\n",
    "            result = self.find_optimal_threshold()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "        \n",
    "        thresholds = result['all_thresholds']\n",
    "        costs = result['all_costs']\n",
    "        fps = result['all_fps']\n",
    "        fns = result['all_fns']\n",
    "        optimal_t = result['optimal_threshold']\n",
    "        \n",
    "        # Plot 1: Total Cost vs Threshold\n",
    "        axes[0].plot(thresholds, costs, 'b-', lw=2, label='Total Cost')\n",
    "        axes[0].axvline(optimal_t, color='r', linestyle='--', lw=2, \n",
    "                       label=f'Optimal = {optimal_t:.3f}')\n",
    "        axes[0].scatter([optimal_t], [result['min_cost']], color='r', s=100, zorder=5)\n",
    "        \n",
    "        axes[0].set_xlabel('Threshold', fontsize=11, fontweight='bold')\n",
    "        axes[0].set_ylabel('Total Cost', fontsize=11, fontweight='bold')\n",
    "        axes[0].set_title(f'Cost Optimization\\nMin Cost = {result[\"min_cost\"]:.2f}', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(alpha=0.3)\n",
    "        \n",
    "        # Plot 2: FP and FN counts vs Threshold\n",
    "        axes[1].plot(thresholds, fps, 'orange', lw=2, label=f'FP (cost={self.cost_fp})')\n",
    "        axes[1].plot(thresholds, fns, 'purple', lw=2, label=f'FN (cost={self.cost_fn})')\n",
    "        axes[1].axvline(optimal_t, color='r', linestyle='--', lw=2, label='Optimal')\n",
    "        \n",
    "        axes[1].set_xlabel('Threshold', fontsize=11, fontweight='bold')\n",
    "        axes[1].set_ylabel('Error Count', fontsize=11, fontweight='bold')\n",
    "        axes[1].set_title('FP vs FN Trade-off', fontsize=12, fontweight='bold')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Cost Breakdown at Optimal Threshold\n",
    "        fp_cost = self.cost_fp * result['fp_at_optimal']\n",
    "        fn_cost = self.cost_fn * result['fn_at_optimal']\n",
    "        \n",
    "        labels = ['FP Cost', 'FN Cost']\n",
    "        costs_breakdown = [fp_cost, fn_cost]\n",
    "        colors = ['orange', 'purple']\n",
    "        \n",
    "        axes[2].bar(labels, costs_breakdown, color=colors, alpha=0.7, edgecolor='black')\n",
    "        \n",
    "        for i, (label, cost) in enumerate(zip(labels, costs_breakdown)):\n",
    "            axes[2].text(i, cost + max(costs_breakdown) * 0.02, f'${cost:.2f}', \n",
    "                        ha='center', fontweight='bold')\n",
    "        \n",
    "        axes[2].set_ylabel('Cost', fontsize=11, fontweight='bold')\n",
    "        axes[2].set_title(f'Cost Breakdown at t={optimal_t:.3f}', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[2].grid(alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def compare_thresholds(self, thresholds: List[float]) -> None:\n",
    "        \"\"\"\n",
    "        Compare performance at multiple thresholds.\n",
    "        \n",
    "        Args:\n",
    "            thresholds: List of thresholds to compare\n",
    "        \"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"THRESHOLD COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Cost FP: ${self.cost_fp:,.2f}  |  Cost FN: ${self.cost_fn:,.2f}\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"{'Threshold':<12} {'FP':<8} {'FN':<8} {'FP Cost':<12} {'FN Cost':<12} {'Total Cost':<12}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for t in thresholds:\n",
    "            total_cost, fp, fn = self.compute_cost(t)\n",
    "            fp_cost = self.cost_fp * fp\n",
    "            fn_cost = self.cost_fn * fn\n",
    "            \n",
    "            print(f\"{t:<12.3f} {fp:<8d} {fn:<8d} ${fp_cost:<11,.2f} ${fn_cost:<11,.2f} ${total_cost:<11,.2f}\")\n",
    "        \n",
    "        print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee7c9de",
   "metadata": {},
   "source": [
    "### ðŸ“ Class: MultiClassEvaluator:\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassEvaluator:\n",
    "    \"\"\"\n",
    "    Multi-class classification evaluation with macro/micro/weighted averaging.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, y_true: np.ndarray, y_pred: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initialize with true and predicted labels.\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels (n_samples,)\n",
    "            y_pred: Predicted labels (n_samples,)\n",
    "        \"\"\"\n",
    "        self.y_true = np.array(y_true)\n",
    "        self.y_pred = np.array(y_pred)\n",
    "        self.classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "        self.n_classes = len(self.classes)\n",
    "    \n",
    "    def compute_per_class_metrics(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Compute precision, recall, F1 for each class (one-vs-rest).\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with per-class metrics\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            # One-vs-rest binary classification\n",
    "            y_true_binary = (self.y_true == cls).astype(int)\n",
    "            y_pred_binary = (self.y_pred == cls).astype(int)\n",
    "            \n",
    "            # Confusion matrix for this class\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true_binary, y_pred_binary, labels=[0, 1]).ravel()\n",
    "            \n",
    "            # Compute metrics\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "            support = tp + fn  # Actual number of samples in this class\n",
    "            \n",
    "            metrics[cls] = {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'support': support\n",
    "            }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def macro_average(self, metric: str = 'f1') -> float:\n",
    "        \"\"\"\n",
    "        Compute macro-average (unweighted mean across classes).\n",
    "        \n",
    "        Args:\n",
    "            metric: 'precision', 'recall', or 'f1'\n",
    "            \n",
    "        Returns:\n",
    "            Macro-averaged metric\n",
    "        \"\"\"\n",
    "        per_class = self.compute_per_class_metrics()\n",
    "        values = [per_class[cls][metric] for cls in self.classes]\n",
    "        return np.mean(values)\n",
    "    \n",
    "    def micro_average(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute micro-average (aggregate TP/FP/FN across all classes).\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with micro precision, recall, F1\n",
    "        \"\"\"\n",
    "        total_tp = 0\n",
    "        total_fp = 0\n",
    "        total_fn = 0\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            y_true_binary = (self.y_true == cls).astype(int)\n",
    "            y_pred_binary = (self.y_pred == cls).astype(int)\n",
    "            \n",
    "            tn, fp, fn, tp = confusion_matrix(y_true_binary, y_pred_binary, labels=[0, 1]).ravel()\n",
    "            \n",
    "            total_tp += tp\n",
    "            total_fp += fp\n",
    "            total_fn += fn\n",
    "        \n",
    "        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
    "        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    def weighted_average(self, metric: str = 'f1') -> float:\n",
    "        \"\"\"\n",
    "        Compute weighted-average (weighted by class support).\n",
    "        \n",
    "        Args:\n",
    "            metric: 'precision', 'recall', or 'f1'\n",
    "            \n",
    "        Returns:\n",
    "            Weighted-averaged metric\n",
    "        \"\"\"\n",
    "        per_class = self.compute_per_class_metrics()\n",
    "        \n",
    "        weighted_sum = 0.0\n",
    "        total_support = 0\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            weighted_sum += per_class[cls][metric] * per_class[cls]['support']\n",
    "            total_support += per_class[cls]['support']\n",
    "        \n",
    "        return weighted_sum / total_support if total_support > 0 else 0.0\n",
    "    \n",
    "    def summary(self) -> None:\n",
    "        \"\"\"\n",
    "        Print comprehensive multi-class evaluation summary.\n",
    "        \"\"\"\n",
    "        per_class = self.compute_per_class_metrics()\n",
    "        micro = self.micro_average()\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"MULTI-CLASS CLASSIFICATION SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Number of classes: {self.n_classes}\")\n",
    "        print(f\"Total samples: {len(self.y_true)}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"PER-CLASS METRICS\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"{'Class':<10} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            m = per_class[cls]\n",
    "            print(f\"{cls:<10} {m['precision']:<12.4f} {m['recall']:<12.4f} {m['f1']:<12.4f} {m['support']:<10d}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"AVERAGED METRICS\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"{'Average Type':<20} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"{'Macro':<20} {self.macro_average('precision'):<12.4f} \"\n",
    "              f\"{self.macro_average('recall'):<12.4f} {self.macro_average('f1'):<12.4f}\")\n",
    "        print(f\"{'Micro':<20} {micro['precision']:<12.4f} \"\n",
    "              f\"{micro['recall']:<12.4f} {micro['f1']:<12.4f}\")\n",
    "        print(f\"{'Weighted':<20} {self.weighted_average('precision'):<12.4f} \"\n",
    "              f\"{self.weighted_average('recall'):<12.4f} {self.weighted_average('f1'):<12.4f}\")\n",
    "        print(\"=\"*80)\n",
    "# Example usage demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Example 1: Cost-sensitive binary classification (semiconductor defect detection)\n",
    "    print(\"EXAMPLE 1: Cost-Sensitive Defect Detection\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    n_samples = 500\n",
    "    y_true = np.random.randint(0, 2, n_samples)  # 0 = pass, 1 = fail\n",
    "    y_prob = np.clip(y_true + 0.3 * np.random.randn(n_samples), 0, 1)  # Add noise\n",
    "    \n",
    "    # FN (missed defect) costs $10M, FP (false alarm) costs $50K\n",
    "    evaluator = CostSensitiveEvaluator(y_true, y_prob, cost_fp=50_000, cost_fn=10_000_000)\n",
    "    \n",
    "    result = evaluator.find_optimal_threshold()\n",
    "    print(f\"Optimal threshold: {result['optimal_threshold']:.4f}\")\n",
    "    print(f\"Minimum total cost: ${result['min_cost']:,.2f}\")\n",
    "    print(f\"FP at optimal: {result['fp_at_optimal']}\")\n",
    "    print(f\"FN at optimal: {result['fn_at_optimal']}\")\n",
    "    \n",
    "    evaluator.plot_cost_analysis(result)\n",
    "    \n",
    "    # Compare with standard threshold\n",
    "    evaluator.compare_thresholds([0.3, 0.5, 0.7, result['optimal_threshold']])\n",
    "    \n",
    "    # Example 2: Multi-class evaluation (device bin classification)\n",
    "    print(\"\\n\\nEXAMPLE 2: Multi-Class Bin Classification\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # 4 bins: BIN1 (premium), BIN2 (standard), BIN3 (low-grade), BIN4 (reject)\n",
    "    y_true_mc = np.random.choice(['BIN1', 'BIN2', 'BIN3', 'BIN4'], size=400, \n",
    "                                 p=[0.3, 0.4, 0.2, 0.1])\n",
    "    \n",
    "    # Add some prediction errors\n",
    "    y_pred_mc = y_true_mc.copy()\n",
    "    error_idx = np.random.choice(len(y_pred_mc), size=50, replace=False)\n",
    "    y_pred_mc[error_idx] = np.random.choice(['BIN1', 'BIN2', 'BIN3', 'BIN4'], size=50)\n",
    "    \n",
    "    mc_evaluator = MultiClassEvaluator(y_true_mc, y_pred_mc)\n",
    "    mc_evaluator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c087b1cb",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Complete Example: Semiconductor Yield Prediction Evaluation\n",
    "\n",
    "Let's build a **complete evaluation pipeline** for a real-world post-silicon validation scenario:\n",
    "\n",
    "**Scenario:**\n",
    "- **Objective**: Predict device pass/fail based on parametric test data\n",
    "- **Dataset**: 1000 devices from 10 wafers with spatial (die_x, die_y) and electrical features (VDD, IDD, Freq, Temp)\n",
    "- **Challenge**: Imbalanced (10% fail rate), high cost asymmetry (FN = $5M, FP = $100K)\n",
    "- **Goal**: Find optimal decision threshold and evaluate with comprehensive metrics\n",
    "\n",
    "**Business Context:**\n",
    "- Each missed defect (FN) that escapes to field costs **$5,000,000** in recalls, warranty, brand damage\n",
    "- Each false alarm (FP) costs **$100,000** in unnecessary retest and analysis\n",
    "- Cost ratio: FN is 50Ã— more expensive than FP\n",
    "- Need to balance yield protection with manufacturing efficiency\n",
    "\n",
    "### Evaluation Strategy\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Train Model] --> B[Get Predictions]\n",
    "    B --> C[Classification Metrics]\n",
    "    B --> D[ROC/PR Curves]\n",
    "    B --> E[Regression Metrics<br/>for yield %]\n",
    "    C --> F[Cost Analysis]\n",
    "    D --> F\n",
    "    E --> F\n",
    "    F --> G[Optimal Threshold]\n",
    "    G --> H[Production<br/>Deployment]\n",
    "```\n",
    "\n",
    "We'll evaluate using:\n",
    "1. **Classification metrics** (confusion matrix, precision, recall, F1)\n",
    "2. **ROC and PR curves** (threshold-independent performance)\n",
    "3. **Cost-sensitive analysis** (find optimal threshold for business objectives)\n",
    "4. **Regression metrics** (for continuous yield percentage prediction)\n",
    "5. **Statistical validation** (cross-validation, confidence intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b83a7e",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ecd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "print(\"=\"*80)\n",
    "print(\"COMPLETE EVALUATION EXAMPLE: SEMICONDUCTOR YIELD PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "# ============================================================================\n",
    "# STEP 1: Generate Synthetic Dataset\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 1] Generating synthetic semiconductor test data...\")\n",
    "n_devices = 1000\n",
    "n_wafers = 10\n",
    "devices_per_wafer = n_devices // n_wafers\n",
    "# Generate spatial coordinates\n",
    "wafer_ids = np.repeat(range(n_wafers), devices_per_wafer)\n",
    "die_x = np.random.uniform(0, 10, n_devices)  # 0-10 mm\n",
    "die_y = np.random.uniform(0, 10, n_devices)  # 0-10 mm\n",
    "# Calculate radial distance from wafer center (5, 5)\n",
    "radial_distance = np.sqrt((die_x - 5)**2 + (die_y - 5)**2)\n",
    "# Generate electrical parameters\n",
    "VDD = np.random.normal(1.8, 0.05, n_devices)  # Voltage (V)\n",
    "IDD = np.random.normal(50, 5, n_devices)      # Current (mA)\n",
    "Freq = np.random.normal(2000, 100, n_devices) # Frequency (MHz)\n",
    "Temp = np.random.normal(85, 5, n_devices)     # Temperature (Â°C)\n",
    "# Create target: devices near edge more likely to fail (radial effect)\n",
    "# Also devices with extreme electrical parameters fail more\n",
    "fail_prob = 0.05 + 0.15 * (radial_distance / radial_distance.max())\n",
    "fail_prob += 0.1 * (np.abs(VDD - 1.8) > 0.1).astype(float)\n",
    "fail_prob += 0.1 * (IDD > 60).astype(float)\n",
    "fail_prob += 0.05 * (Temp > 90).astype(float)\n",
    "y_actual = (np.random.random(n_devices) < fail_prob).astype(int)\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'wafer_id': wafer_ids,\n",
    "    'die_x': die_x,\n",
    "    'die_y': die_y,\n",
    "    'radial_distance': radial_distance,\n",
    "    'VDD': VDD,\n",
    "    'IDD': IDD,\n",
    "    'Freq': Freq,\n",
    "    'Temp': Temp,\n",
    "    'fail': y_actual\n",
    "})\n",
    "print(f\"Dataset created: {n_devices} devices from {n_wafers} wafers\")\n",
    "print(f\"Class distribution: {(1-y_actual.mean())*100:.1f}% pass, {y_actual.mean()*100:.1f}% fail\")\n",
    "print(f\"Imbalance ratio: {(1-y_actual.mean())/y_actual.mean():.1f}:1\")\n",
    "# ============================================================================\n",
    "# STEP 2: Train Model and Get Predictions\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 2] Training Random Forest classifier...\")\n",
    "# Prepare features\n",
    "feature_cols = ['radial_distance', 'VDD', 'IDD', 'Freq', 'Temp']\n",
    "X = df[feature_cols].values\n",
    "y = df['fail'].values\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                      random_state=42, stratify=y)\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# Train model\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_prob = model.predict_proba(X_test_scaled)[:, 1]  # Probability of class 1 (fail)\n",
    "print(f\"Model trained on {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9505450",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4cb964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Classification Metrics Evaluation\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 3] Computing classification metrics...\")\n",
    "# Use our ClassificationMetrics class\n",
    "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
    "class ClassificationMetricsSimple:\n",
    "    def __init__(self, y_true, y_pred, y_prob=None):\n",
    "        self.y_true = np.array(y_true)\n",
    "        self.y_pred = np.array(y_pred)\n",
    "        self.y_prob = y_prob\n",
    "        \n",
    "        cm = sk_confusion_matrix(y_true, y_pred)\n",
    "        self.tn, self.fp, self.fn, self.tp = cm.ravel()\n",
    "    \n",
    "    def accuracy(self):\n",
    "        return (self.tp + self.tn) / (self.tp + self.tn + self.fp + self.fn)\n",
    "    \n",
    "    def precision(self):\n",
    "        return self.tp / (self.tp + self.fp) if (self.tp + self.fp) > 0 else 0.0\n",
    "    \n",
    "    def recall(self):\n",
    "        return self.tp / (self.tp + self.fn) if (self.tp + self.fn) > 0 else 0.0\n",
    "    \n",
    "    def f1_score(self):\n",
    "        p = self.precision()\n",
    "        r = self.recall()\n",
    "        return 2 * p * r / (p + r) if (p + r) > 0 else 0.0\n",
    "    \n",
    "    def specificity(self):\n",
    "        return self.tn / (self.tn + self.fp) if (self.tn + self.fp) > 0 else 0.0\n",
    "metrics = ClassificationMetricsSimple(y_test, y_pred, y_prob)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                Predicted: Pass  |  Predicted: Fail\")\n",
    "print(f\"Actual: Pass    TN = {metrics.tn:<8d} |  FP = {metrics.fp:<8d}\")\n",
    "print(f\"Actual: Fail    FN = {metrics.fn:<8d} |  TP = {metrics.tp:<8d}\")\n",
    "print(f\"\\nClassification Metrics (threshold = 0.5):\")\n",
    "print(f\"  Accuracy:    {metrics.accuracy():.4f}\")\n",
    "print(f\"  Precision:   {metrics.precision():.4f}\")\n",
    "print(f\"  Recall:      {metrics.recall():.4f}\")\n",
    "print(f\"  F1-Score:    {metrics.f1_score():.4f}\")\n",
    "print(f\"  Specificity: {metrics.specificity():.4f}\")\n",
    "# ============================================================================\n",
    "# STEP 4: ROC and PR Curve Analysis\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 4] Analyzing ROC and PR curves...\")\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "# ROC curve\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "# PR curve\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_prob)\n",
    "avg_precision = average_precision_score(y_test, y_prob)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Average Precision (PR AUC): {avg_precision:.4f}\")\n",
    "# Plot ROC and PR curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# ROC curve\n",
    "axes[0].plot(fpr, tpr, 'b-', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'r--', lw=2, label='Random Classifier')\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('ROC Curve - Yield Prediction', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "# PR curve\n",
    "baseline = y_test.mean()\n",
    "axes[1].plot(recall, precision, 'b-', lw=2, label=f'PR Curve (AP = {avg_precision:.4f})')\n",
    "axes[1].axhline(baseline, color='r', linestyle='--', lw=2, label=f'Baseline ({baseline:.4f})')\n",
    "axes[1].set_xlabel('Recall', fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylabel('Precision', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('Precision-Recall Curve', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7751fb",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636b5973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: Cost-Sensitive Analysis\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 5] Finding optimal threshold with cost analysis...\")\n",
    "# Cost parameters\n",
    "COST_FP = 100_000   # $100K per false alarm (unnecessary retest)\n",
    "COST_FN = 5_000_000 # $5M per missed defect (field escape)\n",
    "print(f\"Cost structure:\")\n",
    "print(f\"  False Positive (unnecessary retest): ${COST_FP:,}\")\n",
    "print(f\"  False Negative (missed defect):      ${COST_FN:,}\")\n",
    "print(f\"  Cost ratio (FN/FP): {COST_FN/COST_FP:.0f}:1\")\n",
    "# Find optimal threshold\n",
    "thresholds_to_test = np.linspace(0, 1, 100)\n",
    "costs = []\n",
    "fps = []\n",
    "fns = []\n",
    "for t in thresholds_to_test:\n",
    "    y_pred_t = (y_prob >= t).astype(int)\n",
    "    cm = sk_confusion_matrix(y_test, y_pred_t)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    total_cost = COST_FP * fp + COST_FN * fn\n",
    "    costs.append(total_cost)\n",
    "    fps.append(fp)\n",
    "    fns.append(fn)\n",
    "optimal_idx = np.argmin(costs)\n",
    "optimal_threshold = thresholds_to_test[optimal_idx]\n",
    "min_cost = costs[optimal_idx]\n",
    "print(f\"\\nOptimal threshold: {optimal_threshold:.4f}\")\n",
    "print(f\"Minimum total cost: ${min_cost:,.2f}\")\n",
    "print(f\"FP at optimal: {fps[optimal_idx]}\")\n",
    "print(f\"FN at optimal: {fns[optimal_idx]}\")\n",
    "# Compare standard vs optimal threshold\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"{'Threshold':<12} {'FP':<8} {'FN':<8} {'Total Cost':<15}\")\n",
    "print(\"-\"*50)\n",
    "for t in [0.3, 0.5, 0.7, optimal_threshold]:\n",
    "    idx = np.argmin(np.abs(thresholds_to_test - t))\n",
    "    print(f\"{t:<12.3f} {fps[idx]:<8d} {fns[idx]:<8d} ${costs[idx]:>13,.2f}\")\n",
    "# Visualize cost analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# Cost vs threshold\n",
    "axes[0].plot(thresholds_to_test, costs, 'b-', lw=2)\n",
    "axes[0].axvline(optimal_threshold, color='r', linestyle='--', lw=2, \n",
    "               label=f'Optimal = {optimal_threshold:.3f}')\n",
    "axes[0].scatter([optimal_threshold], [min_cost], color='r', s=100, zorder=5)\n",
    "axes[0].set_xlabel('Threshold', fontsize=11, fontweight='bold')\n",
    "axes[0].set_ylabel('Total Cost ($)', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title(f'Cost Optimization\\nMin Cost = ${min_cost:,.0f}', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].ticklabel_format(style='plain', axis='y')\n",
    "# FP vs FN\n",
    "axes[1].plot(thresholds_to_test, fps, 'orange', lw=2, label='False Positives')\n",
    "axes[1].plot(thresholds_to_test, fns, 'purple', lw=2, label='False Negatives')\n",
    "axes[1].axvline(optimal_threshold, color='r', linestyle='--', lw=2, label='Optimal')\n",
    "axes[1].set_xlabel('Threshold', fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylabel('Error Count', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('FP vs FN Trade-off', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nâœ… Key Findings:\")\n",
    "print(f\"   â€¢ Model achieves ROC AUC = {roc_auc:.4f} (excellent discrimination)\")\n",
    "print(f\"   â€¢ At standard threshold (0.5): {metrics.fn} missed defects = ${metrics.fn * COST_FN:,.0f} cost\")\n",
    "print(f\"   â€¢ At optimal threshold ({optimal_threshold:.3f}): {fns[optimal_idx]} missed defects = ${fns[optimal_idx] * COST_FN:,.0f} cost\")\n",
    "print(f\"   â€¢ Cost savings: ${(metrics.fn - fns[optimal_idx]) * COST_FN:,.0f}\")\n",
    "print(f\"\\nðŸ“Š Recommendation: Use threshold = {optimal_threshold:.4f} for production deployment\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c9e4d",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734df9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "print(\"=\"*80)\n",
    "print(\"REGRESSION EXAMPLE: TEST TIME PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "# ============================================================================\n",
    "# Generate Synthetic Test Time Data\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 1] Generating test time prediction dataset...\")\n",
    "n_devices = 800\n",
    "# Features affecting test time\n",
    "n_test_points = np.random.randint(10, 50, n_devices)  # Number of test points\n",
    "complexity = np.random.uniform(0, 1, n_devices)       # Test complexity (0-1)\n",
    "freq = np.random.normal(2000, 200, n_devices)         # Operating frequency (MHz)\n",
    "temp = np.random.normal(85, 10, n_devices)            # Test temperature (Â°C)\n",
    "# True test time model (with some realistic relationships)\n",
    "# Higher test points â†’ longer time\n",
    "# Higher complexity â†’ longer time\n",
    "# Higher frequency â†’ shorter time (faster execution)\n",
    "# Temperature has minimal effect\n",
    "test_time_actual = (\n",
    "    5.0 +                                    # Base time\n",
    "    0.5 * n_test_points +                   # Linear with test points\n",
    "    20.0 * complexity +                      # Complexity effect\n",
    "    -0.003 * freq +                          # Frequency effect (inverse)\n",
    "    0.1 * temp +                             # Temperature effect\n",
    "    np.random.normal(0, 2, n_devices)       # Random noise\n",
    ")\n",
    "# Ensure positive test times\n",
    "test_time_actual = np.maximum(test_time_actual, 1.0)\n",
    "# Create DataFrame\n",
    "df_test_time = pd.DataFrame({\n",
    "    'n_test_points': n_test_points,\n",
    "    'complexity': complexity,\n",
    "    'freq': freq,\n",
    "    'temp': temp,\n",
    "    'test_time_ms': test_time_actual\n",
    "})\n",
    "print(f\"Dataset: {n_devices} devices\")\n",
    "print(f\"Test time range: [{test_time_actual.min():.2f}, {test_time_actual.max():.2f}] ms\")\n",
    "print(f\"Test time mean: {test_time_actual.mean():.2f} ms\")\n",
    "print(f\"Test time std: {test_time_actual.std():.2f} ms\")\n",
    "# ============================================================================\n",
    "# Train Regression Model\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 2] Training regression models...\")\n",
    "# Prepare data\n",
    "X_reg = df_test_time[['n_test_points', 'complexity', 'freq', 'temp']].values\n",
    "y_reg = df_test_time['test_time_ms'].values\n",
    "# Split\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.3, random_state=42\n",
    ")\n",
    "# Scale features\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
    "# Train Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_reg_scaled, y_train_reg)\n",
    "y_pred_lr = lr_model.predict(X_test_reg_scaled)\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train_reg_scaled, y_train_reg)\n",
    "y_pred_rf = rf_model.predict(X_test_reg_scaled)\n",
    "print(f\"Models trained on {len(X_train_reg)} samples\")\n",
    "print(f\"Test set: {len(X_test_reg)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df513451",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Compute Regression Metrics\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 3] Computing regression metrics...\")\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "def compute_all_metrics(y_true, y_pred, model_name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # MAPE (avoid division by zero)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    # Max error\n",
    "    max_err = np.max(np.abs(y_true - y_pred))\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'RÂ²': r2,\n",
    "        'MAPE': mape,\n",
    "        'Max Error': max_err\n",
    "    }\n",
    "# Compute metrics for both models\n",
    "lr_metrics = compute_all_metrics(y_test_reg, y_pred_lr, 'Linear Regression')\n",
    "rf_metrics = compute_all_metrics(y_test_reg, y_pred_rf, 'Random Forest')\n",
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Metric':<15} {'Linear Regression':<20} {'Random Forest':<20} {'Winner':<10}\")\n",
    "print(\"-\"*80)\n",
    "metrics_to_compare = ['MSE', 'RMSE', 'MAE', 'RÂ²', 'MAPE', 'Max Error']\n",
    "for metric in metrics_to_compare:\n",
    "    lr_val = lr_metrics[metric]\n",
    "    rf_val = rf_metrics[metric]\n",
    "    \n",
    "    # For RÂ², higher is better; for others, lower is better\n",
    "    if metric == 'RÂ²':\n",
    "        winner = 'Linear Reg' if lr_val > rf_val else 'Random Forest'\n",
    "        better_symbol = '>' if lr_val > rf_val else '<'\n",
    "    else:\n",
    "        winner = 'Linear Reg' if lr_val < rf_val else 'Random Forest'\n",
    "        better_symbol = '<' if lr_val < rf_val else '>'\n",
    "    \n",
    "    print(f\"{metric:<15} {lr_val:<20.4f} {rf_val:<20.4f} {winner:<10}\")\n",
    "print(\"=\"*80)\n",
    "# ============================================================================\n",
    "# Visualize Regression Performance\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 4] Visualizing predictions...\")\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "# Linear Regression visualizations\n",
    "# Predicted vs Actual\n",
    "axes[0, 0].scatter(y_test_reg, y_pred_lr, alpha=0.6, s=30, edgecolors='k', linewidth=0.5)\n",
    "min_val = min(y_test_reg.min(), y_pred_lr.min())\n",
    "max_val = max(y_test_reg.max(), y_pred_lr.max())\n",
    "axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Test Time (ms)', fontsize=10, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Predicted Test Time (ms)', fontsize=10, fontweight='bold')\n",
    "axes[0, 0].set_title(f'Linear Regression\\nRÂ² = {lr_metrics[\"RÂ²\"]:.4f}', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "# Residuals\n",
    "residuals_lr = y_test_reg - y_pred_lr\n",
    "axes[0, 1].scatter(y_pred_lr, residuals_lr, alpha=0.6, s=30, edgecolors='k', linewidth=0.5)\n",
    "axes[0, 1].axhline(0, color='r', linestyle='--', lw=2)\n",
    "axes[0, 1].set_xlabel('Predicted Test Time (ms)', fontsize=10, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Residuals (ms)', fontsize=10, fontweight='bold')\n",
    "axes[0, 1].set_title(f'Residual Plot\\nMAE = {lr_metrics[\"MAE\"]:.4f} ms', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "# Residual distribution\n",
    "axes[0, 2].hist(residuals_lr, bins=25, alpha=0.7, edgecolor='black', color='skyblue')\n",
    "axes[0, 2].axvline(0, color='r', linestyle='--', lw=2)\n",
    "axes[0, 2].axvline(np.mean(residuals_lr), color='orange', linestyle='--', lw=2)\n",
    "axes[0, 2].set_xlabel('Residuals (ms)', fontsize=10, fontweight='bold')\n",
    "axes[0, 2].set_ylabel('Frequency', fontsize=10, fontweight='bold')\n",
    "axes[0, 2].set_title(f'Residual Distribution\\nRMSE = {lr_metrics[\"RMSE\"]:.4f} ms', fontsize=11, fontweight='bold')\n",
    "axes[0, 2].grid(alpha=0.3, axis='y')\n",
    "# Random Forest visualizations\n",
    "# Predicted vs Actual\n",
    "axes[1, 0].scatter(y_test_reg, y_pred_rf, alpha=0.6, s=30, edgecolors='k', linewidth=0.5, color='green')\n",
    "axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "axes[1, 0].set_xlabel('Actual Test Time (ms)', fontsize=10, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Predicted Test Time (ms)', fontsize=10, fontweight='bold')\n",
    "axes[1, 0].set_title(f'Random Forest\\nRÂ² = {rf_metrics[\"RÂ²\"]:.4f}', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "# Residuals\n",
    "residuals_rf = y_test_reg - y_pred_rf\n",
    "axes[1, 1].scatter(y_pred_rf, residuals_rf, alpha=0.6, s=30, edgecolors='k', linewidth=0.5, color='green')\n",
    "axes[1, 1].axhline(0, color='r', linestyle='--', lw=2)\n",
    "axes[1, 1].set_xlabel('Predicted Test Time (ms)', fontsize=10, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Residuals (ms)', fontsize=10, fontweight='bold')\n",
    "axes[1, 1].set_title(f'Residual Plot\\nMAE = {rf_metrics[\"MAE\"]:.4f} ms', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "# Residual distribution\n",
    "axes[1, 2].hist(residuals_rf, bins=25, alpha=0.7, edgecolor='black', color='lightgreen')\n",
    "axes[1, 2].axvline(0, color='r', linestyle='--', lw=2)\n",
    "axes[1, 2].axvline(np.mean(residuals_rf), color='orange', linestyle='--', lw=2)\n",
    "axes[1, 2].set_xlabel('Residuals (ms)', fontsize=10, fontweight='bold')\n",
    "axes[1, 2].set_ylabel('Frequency', fontsize=10, fontweight='bold')\n",
    "axes[1, 2].set_title(f'Residual Distribution\\nRMSE = {rf_metrics[\"RMSE\"]:.4f} ms', fontsize=11, fontweight='bold')\n",
    "axes[1, 2].grid(alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af1a354",
   "metadata": {},
   "source": [
    "### ðŸ“ Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83378e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Business Impact Analysis\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 5] Business impact analysis...\")\n",
    "# Assume manufacturing throughput goal: 1000 devices/hour\n",
    "# Test time directly affects throughput\n",
    "avg_test_time_actual = y_test_reg.mean()\n",
    "avg_test_time_pred_lr = y_pred_lr.mean()\n",
    "avg_test_time_pred_rf = y_pred_rf.mean()\n",
    "print(f\"\\nThroughput Analysis:\")\n",
    "print(f\"  Actual average test time: {avg_test_time_actual:.2f} ms\")\n",
    "print(f\"  Linear Regression prediction: {avg_test_time_pred_lr:.2f} ms (error: {abs(avg_test_time_pred_lr - avg_test_time_actual):.2f} ms)\")\n",
    "print(f\"  Random Forest prediction: {avg_test_time_pred_rf:.2f} ms (error: {abs(avg_test_time_pred_rf - avg_test_time_actual):.2f} ms)\")\n",
    "# Throughput in devices/hour\n",
    "throughput_actual = 3600 * 1000 / avg_test_time_actual  # 3600 sec/hr, 1000 ms/sec\n",
    "throughput_pred_lr = 3600 * 1000 / avg_test_time_pred_lr\n",
    "throughput_pred_rf = 3600 * 1000 / avg_test_time_pred_rf\n",
    "print(f\"\\n  Throughput (devices/hour):\")\n",
    "print(f\"    Actual: {throughput_actual:.0f}\")\n",
    "print(f\"    LR predicted: {throughput_pred_lr:.0f}\")\n",
    "print(f\"    RF predicted: {throughput_pred_rf:.0f}\")\n",
    "# Cost of prediction error (assume $10 per device, 24/7 operation)\n",
    "cost_per_device = 10\n",
    "hours_per_year = 8760  # 24 * 365\n",
    "error_cost_lr = abs(throughput_pred_lr - throughput_actual) * hours_per_year * cost_per_device\n",
    "error_cost_rf = abs(throughput_pred_rf - throughput_actual) * hours_per_year * cost_per_device\n",
    "print(f\"\\n  Annual revenue impact of prediction error:\")\n",
    "print(f\"    Linear Regression: ${error_cost_lr:,.0f}\")\n",
    "print(f\"    Random Forest: ${error_cost_rf:,.0f}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGRESSION EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nâœ… Key Findings:\")\n",
    "print(f\"   â€¢ Random Forest outperforms Linear Regression (RÂ² = {rf_metrics['RÂ²']:.4f} vs {lr_metrics['RÂ²']:.4f})\")\n",
    "print(f\"   â€¢ Average prediction error: {rf_metrics['MAE']:.2f} ms (RF) vs {lr_metrics['MAE']:.2f} ms (LR)\")\n",
    "print(f\"   â€¢ Max error: {rf_metrics['Max Error']:.2f} ms (RF) vs {lr_metrics['Max Error']:.2f} ms (LR)\")\n",
    "print(f\"   â€¢ RF model reduces annual error cost by ${abs(error_cost_lr - error_cost_rf):,.0f}\")\n",
    "print(f\"\\nðŸ“Š Recommendation: Deploy Random Forest for production test time prediction\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e277f",
   "metadata": {},
   "source": [
    "## ðŸš€ Real-World Project Ideas\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "#### Project 1: Adaptive Test Threshold Optimizer\n",
    "**Objective**: Build system that dynamically adjusts test thresholds based on cost-sensitive evaluation to minimize total manufacturing cost while maintaining quality targets.\n",
    "\n",
    "**Business Impact**: **$15M annual savings** (typical large semiconductor fab)\n",
    "\n",
    "**Key Features**:\n",
    "- Real-time cost matrix updates based on field failure data\n",
    "- Multi-objective optimization (minimize cost, maximize yield, meet quality targets)\n",
    "- Threshold adaptation per product family and test stage\n",
    "- A/B testing framework for threshold changes\n",
    "- Dashboard showing cost breakdown (FP vs FN) and savings\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- Total cost (FP cost + FN cost)\n",
    "- Cost per device\n",
    "- Yield rate vs quality escapes\n",
    "- Return on investment (ROI)\n",
    "\n",
    "**Techniques**:\n",
    "- Cost-sensitive learning\n",
    "- ROC/PR curve analysis\n",
    "- Bayesian optimization for threshold tuning\n",
    "- Monte Carlo simulation for cost estimation\n",
    "- Multi-armed bandit for A/B testing\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 2: Multi-Stage Test Flow Evaluator\n",
    "**Objective**: Evaluate and optimize multi-stage test flows (wafer sort â†’ final test â†’ system test) with cascading decision thresholds and cumulative cost analysis.\n",
    "\n",
    "**Business Impact**: **$25M savings** (reduce redundant tests, optimize flow)\n",
    "\n",
    "**Key Features**:\n",
    "- Stage-wise metric computation (each test stage)\n",
    "- Cumulative confusion matrices across stages\n",
    "- Test coverage analysis (which defects caught at which stage)\n",
    "- Flow optimization (skip unnecessary stages for low-risk devices)\n",
    "- Cost-benefit analysis per test stage\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- Stage-wise precision/recall/F1\n",
    "- Cumulative FP/FN across all stages\n",
    "- Test time per stage vs value added\n",
    "- Defect escape rate per stage\n",
    "- Total test cost per device\n",
    "\n",
    "**Techniques**:\n",
    "- Multi-class classification evaluation\n",
    "- Sequential decision analysis\n",
    "- Markov chains for flow modeling\n",
    "- Dynamic programming for optimal paths\n",
    "- Cost-benefit analysis\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 3: Spatial Yield Prediction Evaluator\n",
    "**Objective**: Evaluate models that predict yield based on spatial wafer patterns (die location, wafer map analysis) with specialized spatial metrics.\n",
    "\n",
    "**Business Impact**: **$20M savings** (early detection of spatial defects, process improvements)\n",
    "\n",
    "**Key Features**:\n",
    "- Spatial autocorrelation metrics (Moran's I, Geary's C)\n",
    "- Wafer map visualization with prediction overlay\n",
    "- Zone-based evaluation (edge, center, quadrants)\n",
    "- Spatial clustering detection (defect hotspots)\n",
    "- Process-aware metrics (batch, lot, fab)\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- Standard classification metrics\n",
    "- Spatial autocorrelation of errors\n",
    "- Zone-wise precision/recall\n",
    "- Hotspot detection rate\n",
    "- False alarm rate per wafer region\n",
    "\n",
    "**Techniques**:\n",
    "- Spatial statistics\n",
    "- Geospatial analysis\n",
    "- Cluster detection algorithms\n",
    "- Image-based evaluation\n",
    "- Process control charts\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 4: Reliability Prediction Confidence Calibration\n",
    "**Objective**: Build calibrated confidence estimators for device reliability predictions (predicted failure probability matches actual failure rate).\n",
    "\n",
    "**Business Impact**: **$30M savings** (accurate warranty reserves, targeted reliability improvements)\n",
    "\n",
    "**Key Features**:\n",
    "- Expected Calibration Error (ECE) computation\n",
    "- Reliability calibration curves\n",
    "- Confidence interval estimation for MTBF (Mean Time Between Failures)\n",
    "- Uncertainty quantification for predictions\n",
    "- Risk-based binning (high confidence â†’ ship, low confidence â†’ additional test)\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- Expected Calibration Error (ECE)\n",
    "- Brier score\n",
    "- Confidence interval coverage\n",
    "- Calibration curve analysis\n",
    "- Risk-adjusted accuracy\n",
    "\n",
    "**Techniques**:\n",
    "- Probability calibration (Platt scaling, isotonic regression)\n",
    "- Conformal prediction\n",
    "- Bayesian uncertainty quantification\n",
    "- Bootstrap confidence intervals\n",
    "- Risk-based decision making\n",
    "\n",
    "---\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "#### Project 5: Healthcare Risk Stratification System\n",
    "**Objective**: Evaluate multi-class risk stratification models (low/medium/high/critical risk) for patient outcomes with cost-sensitive evaluation (higher penalty for underestimating high-risk patients).\n",
    "\n",
    "**Business Impact**: **$100M savings** (hospital system), improved patient outcomes\n",
    "\n",
    "**Key Features**:\n",
    "- Multi-class macro/micro/weighted metrics\n",
    "- Cost matrix with severity-based penalties\n",
    "- Calibration for risk probabilities\n",
    "- Subgroup fairness analysis (demographic parity)\n",
    "- Temporal evaluation (prediction window analysis)\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- Macro/Micro/Weighted F1\n",
    "- Cost-sensitive accuracy\n",
    "- Expected Calibration Error\n",
    "- Fairness metrics (equalized odds, demographic parity)\n",
    "- C-statistic (concordance index)\n",
    "\n",
    "**Techniques**:\n",
    "- Multi-class evaluation\n",
    "- Cost-sensitive learning\n",
    "- Fairness-aware ML\n",
    "- Survival analysis\n",
    "- Calibration methods\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 6: Financial Fraud Detection Pipeline\n",
    "**Objective**: Build comprehensive evaluation pipeline for real-time fraud detection with extreme imbalance (0.1% fraud rate), cost asymmetry, and concept drift monitoring.\n",
    "\n",
    "**Business Impact**: **$500M prevented losses** (large financial institution)\n",
    "\n",
    "**Key Features**:\n",
    "- Imbalanced classification evaluation (PR curves, F-beta)\n",
    "- Cost-sensitive thresholds (missed fraud >> false alarm)\n",
    "- Real-time monitoring dashboard (prediction drift, feature drift)\n",
    "- A/B testing framework for model updates\n",
    "- Explainability metrics (feature importance, SHAP consistency)\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- Precision-Recall AUC (PR-AUC)\n",
    "- F2-score (favor recall)\n",
    "- Cost-weighted accuracy\n",
    "- Prediction drift (KL divergence, PSI)\n",
    "- Model latency (p50, p95, p99)\n",
    "\n",
    "**Techniques**:\n",
    "- Imbalanced learning evaluation\n",
    "- Cost-sensitive classification\n",
    "- Concept drift detection\n",
    "- Online learning evaluation\n",
    "- Real-time monitoring\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 7: E-commerce Recommendation Evaluator\n",
    "**Objective**: Evaluate recommendation systems with beyond-accuracy metrics (diversity, novelty, serendipity, coverage) and multi-stakeholder objectives (user satisfaction, revenue, inventory turnover).\n",
    "\n",
    "**Business Impact**: **$200M revenue increase** (large e-commerce platform)\n",
    "\n",
    "**Key Features**:\n",
    "- Ranking metrics (NDCG, MAP, MRR, Hit Rate@K)\n",
    "- Diversity metrics (intra-list diversity, coverage)\n",
    "- Business metrics (revenue, conversion, click-through rate)\n",
    "- User satisfaction metrics (engagement time, return rate)\n",
    "- A/B test evaluation framework\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- Precision@K, Recall@K, F1@K\n",
    "- Normalized Discounted Cumulative Gain (NDCG)\n",
    "- Mean Average Precision (MAP)\n",
    "- Coverage (catalog coverage, user coverage)\n",
    "- Diversity (Gini coefficient, entropy)\n",
    "- Revenue per user\n",
    "\n",
    "**Techniques**:\n",
    "- Ranking evaluation\n",
    "- Multi-objective optimization\n",
    "- A/B testing\n",
    "- Causal inference\n",
    "- Business metric alignment\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 8: NLP Model Evaluation Suite\n",
    "**Objective**: Build comprehensive evaluation suite for NLP models (classification, NER, QA, summarization) with task-specific metrics, human evaluation correlation, and fairness analysis.\n",
    "\n",
    "**Business Impact**: **$50M cost savings** (improved customer service automation)\n",
    "\n",
    "**Key Features**:\n",
    "- Task-specific metrics (F1 for NER, BLEU/ROUGE for summarization, Exact Match for QA)\n",
    "- Human evaluation alignment (correlation with expert ratings)\n",
    "- Bias detection (gender, race, age bias in predictions)\n",
    "- Multilingual evaluation (cross-lingual consistency)\n",
    "- Production monitoring (response quality drift)\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- Token-level F1 (NER)\n",
    "- Exact Match, F1 (QA)\n",
    "- BLEU, ROUGE, METEOR (summarization)\n",
    "- Perplexity (language models)\n",
    "- Bias scores (demographic parity, equalized odds)\n",
    "- Human correlation (Spearman, Pearson)\n",
    "\n",
    "**Techniques**:\n",
    "- Token-level evaluation\n",
    "- N-gram matching\n",
    "- Semantic similarity (BERTScore)\n",
    "- Fairness metrics\n",
    "- Human-in-the-loop evaluation\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Project Selection Guide\n",
    "\n",
    "| **Domain** | **Best Project** | **Why** |\n",
    "|------------|-----------------|---------|\n",
    "| **Semiconductor** | Adaptive Test Threshold Optimizer | Immediate ROI, clear cost savings, production-ready |\n",
    "| **Manufacturing** | Multi-Stage Test Flow Evaluator | Optimizes entire process, high impact |\n",
    "| **Healthcare** | Risk Stratification System | Life-saving impact, regulatory compliance |\n",
    "| **Finance** | Fraud Detection Pipeline | Massive ROI, real-time requirements |\n",
    "| **E-commerce** | Recommendation Evaluator | Direct revenue impact, user satisfaction |\n",
    "| **NLP/AI** | NLP Evaluation Suite | Foundation for AI products, fairness critical |\n",
    "\n",
    "### Success Criteria\n",
    "\n",
    "For each project, define clear success metrics:\n",
    "\n",
    "1. **Technical**: Accuracy improvement, metric optimization, latency reduction\n",
    "2. **Business**: Cost savings, revenue increase, efficiency gains\n",
    "3. **Operational**: Deployment speed, maintenance cost, scalability\n",
    "4. **Stakeholder**: User satisfaction, compliance, risk reduction\n",
    "\n",
    "### Implementation Roadmap\n",
    "\n",
    "1. **Phase 1 (Weeks 1-2)**: Baseline evaluation, metric selection, cost matrix definition\n",
    "2. **Phase 2 (Weeks 3-4)**: Custom metrics implementation, visualization, automation\n",
    "3. **Phase 3 (Weeks 5-6)**: Production integration, monitoring, A/B testing\n",
    "4. **Phase 4 (Weeks 7-8)**: Optimization, documentation, handoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a16649",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Key Takeaways and Best Practices\n",
    "\n",
    "### ðŸ“ Core Principles\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Model Evaluation] --> B[Choose Right Metrics]\n",
    "    A --> C[Understand Business Context]\n",
    "    A --> D[Consider Data Characteristics]\n",
    "    \n",
    "    B --> B1[Classification vs Regression]\n",
    "    B --> B2[Balanced vs Imbalanced]\n",
    "    B --> B3[Cost Asymmetry]\n",
    "    \n",
    "    C --> C1[Define Success Criteria]\n",
    "    C --> C2[Identify Stakeholders]\n",
    "    C --> C3[Quantify Costs/Benefits]\n",
    "    \n",
    "    D --> D1[Data Distribution]\n",
    "    D --> D2[Feature Quality]\n",
    "    D --> D3[Temporal Patterns]\n",
    "    \n",
    "    B1 --> E[Comprehensive Evaluation]\n",
    "    B2 --> E\n",
    "    B3 --> E\n",
    "    C1 --> E\n",
    "    C2 --> E\n",
    "    C3 --> E\n",
    "    D1 --> E\n",
    "    D2 --> E\n",
    "    D3 --> E\n",
    "    \n",
    "    E --> F[Production Deployment]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”‘ When to Use Each Metric\n",
    "\n",
    "#### Classification Metrics Decision Table\n",
    "\n",
    "| **Scenario** | **Primary Metric** | **Secondary Metrics** | **Why** |\n",
    "|-------------|-------------------|---------------------|---------|\n",
    "| **Balanced classes, equal costs** | Accuracy, F1 | Precision, Recall | Simple, interpretable |\n",
    "| **Imbalanced classes** | PR-AUC, F1 | Precision@K, Recall@K | Focuses on minority class |\n",
    "| **Cost asymmetry (FN >> FP)** | Cost-sensitive threshold | Recall, F-beta (Î²>1) | Minimize expensive errors |\n",
    "| **Cost asymmetry (FP >> FN)** | Cost-sensitive threshold | Precision, F-beta (Î²<1) | Minimize false alarms |\n",
    "| **Need probability calibration** | ECE, Brier score | Calibration curves | Reliable confidence estimates |\n",
    "| **Multi-class, balanced** | Macro-F1 | Confusion matrix | All classes equally important |\n",
    "| **Multi-class, imbalanced** | Weighted-F1 | Micro-F1, Per-class metrics | Weight by class size |\n",
    "| **Ranking/Retrieval** | NDCG, MAP | Precision@K, Recall@K | Order matters |\n",
    "| **Threshold-independent** | ROC-AUC, PR-AUC | Full curves | Compare models globally |\n",
    "\n",
    "#### Regression Metrics Decision Table\n",
    "\n",
    "| **Scenario** | **Primary Metric** | **Secondary Metrics** | **Why** |\n",
    "|-------------|-------------------|---------------------|---------|\n",
    "| **General regression** | RMSE | MAE, RÂ² | Interpretable, standard |\n",
    "| **Outliers present** | MAE | Median Absolute Error | Robust to extremes |\n",
    "| **Comparing models** | RÂ² | RMSE, MAE | Variance explained |\n",
    "| **Scale-independent comparison** | MAPE | RÂ² | Percentage error |\n",
    "| **Penalize large errors** | RMSE | MSE | Quadratic penalty |\n",
    "| **Worst-case guarantee** | Max Error | RMSE, MAE | Safety-critical applications |\n",
    "| **Multiple features, regularization** | Adjusted RÂ² | RÂ², Cross-val RMSE | Penalizes overfitting |\n",
    "| **Business impact** | Custom cost function | RMSE, MAE | Align with objectives |\n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ Common Pitfalls and How to Avoid Them\n",
    "\n",
    "#### Pitfall 1: Optimizing the Wrong Metric\n",
    "**Problem**: Using accuracy on imbalanced data (99% class 0 â†’ 99% accuracy by always predicting 0)\n",
    "\n",
    "**Solution**: \n",
    "- Always check class distribution first\n",
    "- Use PR-AUC or F1 for imbalanced data\n",
    "- Define business-relevant metrics (cost, revenue, risk)\n",
    "\n",
    "#### Pitfall 2: Ignoring Cost Asymmetry\n",
    "**Problem**: Using default threshold (0.5) when FN costs $10M and FP costs $50K\n",
    "\n",
    "**Solution**:\n",
    "- Define cost matrix explicitly\n",
    "- Find optimal threshold: `t* = argmin(C_FP Ã— FP(t) + C_FN Ã— FN(t))`\n",
    "- Monitor costs in production, not just accuracy\n",
    "\n",
    "#### Pitfall 3: Forgetting Cross-Validation\n",
    "**Problem**: Single train/test split â†’ results depend on specific split\n",
    "\n",
    "**Solution**:\n",
    "- Use K-Fold CV (K=5 or 10) to estimate mean and variance\n",
    "- Report confidence intervals: `metric Â± std`\n",
    "- Use stratified splits for imbalanced data\n",
    "- For time series: Use time series split (no data leakage)\n",
    "\n",
    "#### Pitfall 4: Not Checking Calibration\n",
    "**Problem**: Model says 80% confidence but only correct 60% of the time\n",
    "\n",
    "**Solution**:\n",
    "- Compute Expected Calibration Error (ECE)\n",
    "- Plot calibration curves\n",
    "- Apply calibration methods (Platt scaling, isotonic regression)\n",
    "- Monitor calibration drift in production\n",
    "\n",
    "#### Pitfall 5: Comparing Across Different Datasets\n",
    "**Problem**: \"Model A has MSE=10 on dataset X, Model B has MSE=5 on dataset Y â†’ B is better\"\n",
    "\n",
    "**Solution**:\n",
    "- Only compare models on the **same dataset**\n",
    "- Use scale-independent metrics (RÂ², MAPE) for cross-dataset insights\n",
    "- Report metrics relative to baseline (% improvement)\n",
    "\n",
    "#### Pitfall 6: Overfitting to Validation Set\n",
    "**Problem**: Tuning hyperparameters to maximize validation metric â†’ overfitting\n",
    "\n",
    "**Solution**:\n",
    "- Use 3-way split: Train / Validation / Test\n",
    "- Report final metrics on held-out test set (never used for tuning)\n",
    "- Use nested cross-validation for rigorous evaluation\n",
    "\n",
    "#### Pitfall 7: Ignoring Production Constraints\n",
    "**Problem**: Model has great metrics but 10-second latency (SLA is 100ms)\n",
    "\n",
    "**Solution**:\n",
    "- Define SLAs upfront: Latency (p50, p95, p99), throughput, memory\n",
    "- Monitor production metrics: Prediction drift, feature drift, performance degradation\n",
    "- Set alerting thresholds (e.g., accuracy drops > 5%)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ­ Production Evaluation Checklist\n",
    "\n",
    "Before deploying to production, validate:\n",
    "\n",
    "- [ ] **Metrics aligned with business objectives** (not just technical metrics)\n",
    "- [ ] **Cost asymmetry considered** (optimal threshold found)\n",
    "- [ ] **Cross-validation performed** (results stable across folds)\n",
    "- [ ] **Calibration checked** (predicted probabilities reliable)\n",
    "- [ ] **Fairness evaluated** (no bias across demographics/segments)\n",
    "- [ ] **Production constraints met** (latency, throughput, memory)\n",
    "- [ ] **Monitoring implemented** (drift detection, alerting)\n",
    "- [ ] **A/B testing plan** (how to validate in production)\n",
    "- [ ] **Rollback criteria defined** (when to revert to old model)\n",
    "- [ ] **Documentation complete** (metrics, thresholds, assumptions)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”§ Tools and Libraries\n",
    "\n",
    "#### Essential Python Libraries\n",
    "\n",
    "```python\n",
    "# Metrics computation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Calibration\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Advanced evaluation\n",
    "from scipy.stats import ttest_rel  # Paired t-test\n",
    "from mlxtend.evaluate import mcnemar  # McNemar's test\n",
    "```\n",
    "\n",
    "#### Specialized Tools\n",
    "\n",
    "- **scikit-learn**: Standard metrics, cross-validation, calibration\n",
    "- **imbalanced-learn**: Metrics for imbalanced data\n",
    "- **fairlearn**: Fairness metrics and bias detection\n",
    "- **alibi**: Model explainability and confidence\n",
    "- **evidently**: Production monitoring and drift detection\n",
    "- **neptune/mlflow**: Experiment tracking and metric logging\n",
    "- **wandb**: Real-time monitoring and visualization\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š Metric Reporting Template\n",
    "\n",
    "When reporting model performance, include:\n",
    "\n",
    "1. **Dataset characteristics**: Size, class distribution, feature count, temporal range\n",
    "2. **Evaluation strategy**: K-Fold CV (K=?), stratified?, time series split?\n",
    "3. **Primary metric with CI**: `F1 = 0.85 Â± 0.03` (mean Â± std across folds)\n",
    "4. **Confusion matrix**: TP, TN, FP, FN (absolute counts, not just percentages)\n",
    "5. **Cost analysis**: Total cost at optimal threshold vs baseline\n",
    "6. **Calibration**: ECE, calibration curve\n",
    "7. **Production metrics**: Latency (p50/p95/p99), throughput, memory\n",
    "8. **Comparison to baseline**: `15% improvement in F1 over previous model`\n",
    "9. **Statistical significance**: `p-value < 0.01 (paired t-test)`\n",
    "10. **Business impact**: `Estimated $5M annual savings from reduced false negatives`\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "\n",
    "After mastering evaluation metrics:\n",
    "\n",
    "1. **043_Cross_Validation_Strategies.ipynb**: Rigorous model validation techniques\n",
    "2. **044_Hyperparameter_Tuning.ipynb**: Optimize models using evaluation metrics\n",
    "3. **045_Model_Interpretability.ipynb**: Explain predictions and build trust\n",
    "4. **046_Production_ML_Systems.ipynb**: Deploy and monitor models in production\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š Additional Resources\n",
    "\n",
    "**Books**:\n",
    "- *Evaluating Machine Learning Models* by Alice Zheng (O'Reilly)\n",
    "- *The Hundred-Page Machine Learning Book* by Andriy Burkov (Chapter on Evaluation)\n",
    "\n",
    "**Papers**:\n",
    "- \"The Relationship Between Precision-Recall and ROC Curves\" (ICML 2006)\n",
    "- \"Calibration of Probabilities\" (Platt, 1999)\n",
    "- \"A Survey of Predictive Modelling under Imbalanced Distributions\" (AIRE, 2015)\n",
    "\n",
    "**Online**:\n",
    "- sklearn documentation on metrics: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "- Google ML Crash Course: Classification Metrics\n",
    "- fast.ai: Practical Deep Learning - Model Evaluation\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Remember\n",
    "\n",
    "> **\"You can't improve what you don't measure, but measuring the wrong thing is worse than not measuring at all.\"**\n",
    "\n",
    "Always start by asking:\n",
    "1. **What business objective am I optimizing for?**\n",
    "2. **What are the costs of different types of errors?**\n",
    "3. **How will this model be used in production?**\n",
    "\n",
    "Choose metrics that **align with answers** to these questions, not just what's easy to compute.\n",
    "\n",
    "**Congratulations!** You now have comprehensive knowledge of model evaluation metrics. Apply these principles to build models that deliver **real business value**. ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
