{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48bb239f",
   "metadata": {},
   "source": [
    "## üìù Section 1: Handling Missing Data\n",
    "\n",
    "### Why Missing Data Occurs\n",
    "\n",
    "**Common Causes:**\n",
    "1. **Measurement Failures**: Sensor malfunctions, test equipment errors\n",
    "2. **Data Collection Issues**: Logging failures, transmission errors\n",
    "3. **Not Applicable**: Conditional tests that only run in certain scenarios\n",
    "4. **Intentional Omissions**: Privacy concerns, cost constraints\n",
    "\n",
    "**Types of Missing Data:**\n",
    "- **MCAR (Missing Completely At Random)**: Missing values are independent of all data\n",
    "- **MAR (Missing At Random)**: Missing values depend on observed data\n",
    "- **MNAR (Missing Not At Random)**: Missing values depend on unobserved data\n",
    "\n",
    "### Strategies for Handling Missing Data\n",
    "\n",
    "**1. Mean/Median/Mode Imputation:**\n",
    "- **Mean**: For normally distributed numeric features\n",
    "- **Median**: For numeric features with outliers (more robust)\n",
    "- **Mode**: For categorical features\n",
    "\n",
    "$$\\text{Imputed Value} = \\begin{cases} \n",
    "\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i & \\text{(Mean)} \\\\\n",
    "\\text{median}(x) & \\text{(Median)} \\\\\n",
    "\\text{mode}(x) & \\text{(Mode)}\n",
    "\\end{cases}$$\n",
    "\n",
    "**2. Constant Imputation:**\n",
    "- Replace with a specific value (e.g., 0, -1, 'Missing')\n",
    "- Useful when missing values have semantic meaning\n",
    "\n",
    "**3. Forward Fill / Backward Fill:**\n",
    "- **Forward Fill**: Use previous valid value (time series)\n",
    "- **Backward Fill**: Use next valid value (time series)\n",
    "\n",
    "**4. Advanced Methods:**\n",
    "- **KNN Imputation**: Use k-nearest neighbors' values\n",
    "- **Iterative Imputation**: Model each feature as a function of others\n",
    "- **Deep Learning**: Autoencoders for complex imputation\n",
    "\n",
    "**When to Use Each:**\n",
    "- **< 5% missing**: Any method works reasonably well\n",
    "- **5-20% missing**: Use median/mode or KNN imputation\n",
    "- **20-40% missing**: Consider iterative imputation or drop feature\n",
    "- **> 40% missing**: Usually better to drop the feature entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa35bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Data Handling Implementation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class MissingDataHandler:\n",
    "    \"\"\"\n",
    "    Comprehensive missing data handling toolkit.\n",
    "    \n",
    "    Supports multiple imputation strategies:\n",
    "    - Statistical: mean, median, mode\n",
    "    - Fixed: constant value\n",
    "    - Sequential: forward fill, backward fill\n",
    "    - Advanced: KNN imputation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, strategy='median'):\n",
    "        \"\"\"\n",
    "        Initialize with default strategy.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        strategy : str\n",
    "            Default imputation strategy: 'mean', 'median', 'mode', 'constant', \n",
    "            'forward_fill', 'backward_fill', 'knn'\n",
    "        \"\"\"\n",
    "        self.strategy = strategy\n",
    "        self.imputer = None\n",
    "        self.fill_values = {}\n",
    "        \n",
    "    def analyze_missing(self, df):\n",
    "        \"\"\"\n",
    "        Analyze missing data patterns.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with missing value statistics per column\n",
    "        \"\"\"\n",
    "        missing_stats = pd.DataFrame({\n",
    "            'column': df.columns,\n",
    "            'missing_count': df.isnull().sum(),\n",
    "            'missing_pct': (df.isnull().sum() / len(df)) * 100,\n",
    "            'dtype': df.dtypes\n",
    "        })\n",
    "        \n",
    "        missing_stats = missing_stats[missing_stats['missing_count'] > 0]\n",
    "        missing_stats = missing_stats.sort_values('missing_pct', ascending=False)\n",
    "        \n",
    "        return missing_stats\n",
    "    \n",
    "    def impute_simple(self, df, strategy='median', fill_value=None):\n",
    "        \"\"\"\n",
    "        Simple imputation using sklearn SimpleImputer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            Input data with missing values\n",
    "        strategy : str\n",
    "            'mean', 'median', 'most_frequent', 'constant'\n",
    "        fill_value : any\n",
    "            Value to use when strategy='constant'\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with imputed values\n",
    "        \"\"\"\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        if strategy == 'constant':\n",
    "            self.imputer = SimpleImputer(strategy=strategy, fill_value=fill_value)\n",
    "        else:\n",
    "            self.imputer = SimpleImputer(strategy=strategy)\n",
    "        \n",
    "        df_imputed = df.copy()\n",
    "        df_imputed[numeric_cols] = self.imputer.fit_transform(df[numeric_cols])\n",
    "        \n",
    "        # Store fill values for inspection\n",
    "        self.fill_values = dict(zip(numeric_cols, self.imputer.statistics_))\n",
    "        \n",
    "        return df_imputed\n",
    "    \n",
    "    def impute_forward_fill(self, df):\n",
    "        \"\"\"Forward fill (use previous valid value).\"\"\"\n",
    "        return df.fillna(method='ffill')\n",
    "    \n",
    "    def impute_backward_fill(self, df):\n",
    "        \"\"\"Backward fill (use next valid value).\"\"\"\n",
    "        return df.fillna(method='bfill')\n",
    "    \n",
    "    def impute_knn(self, df, n_neighbors=5):\n",
    "        \"\"\"\n",
    "        KNN imputation - uses k-nearest neighbors' values.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            Input data with missing values\n",
    "        n_neighbors : int\n",
    "            Number of neighbors to use\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with KNN-imputed values\n",
    "        \"\"\"\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        self.imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        \n",
    "        df_imputed = df.copy()\n",
    "        df_imputed[numeric_cols] = self.imputer.fit_transform(df[numeric_cols])\n",
    "        \n",
    "        return df_imputed\n",
    "    \n",
    "    def visualize_missing(self, df):\n",
    "        \"\"\"\n",
    "        Visualize missing data patterns.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Missing data heatmap\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
    "        plt.title('Missing Data Pattern')\n",
    "        plt.xlabel('Features')\n",
    "        plt.ylabel('Samples')\n",
    "        \n",
    "        # Missing percentage bar chart\n",
    "        plt.subplot(1, 2, 2)\n",
    "        missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "        missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=True)\n",
    "        missing_pct.plot(kind='barh', color='coral')\n",
    "        plt.title('Missing Data Percentage by Feature')\n",
    "        plt.xlabel('Missing %')\n",
    "        plt.ylabel('Features')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"‚úÖ MissingDataHandler class implemented\")\n",
    "print(\"\\nKey Methods:\")\n",
    "print(\"- analyze_missing(): Get missing data statistics\")\n",
    "print(\"- impute_simple(): Mean/median/mode/constant imputation\")\n",
    "print(\"- impute_forward_fill(): Use previous valid value\")\n",
    "print(\"- impute_backward_fill(): Use next valid value\")\n",
    "print(\"- impute_knn(): K-nearest neighbors imputation\")\n",
    "print(\"- visualize_missing(): Plot missing data patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb943e",
   "metadata": {},
   "source": [
    "## üìù Section 2: Feature Scaling and Normalization\n",
    "\n",
    "### Why Scale Features?\n",
    "\n",
    "**Problem:** Features with different ranges can dominate the learning process.\n",
    "\n",
    "**Example:**\n",
    "- Frequency: 1000-3000 MHz (range: 2000)\n",
    "- Voltage: 0.8-1.2 V (range: 0.4)\n",
    "- Temperature: 25-85¬∞C (range: 60)\n",
    "\n",
    "Without scaling, frequency would dominate distance calculations in algorithms like KNN, K-Means.\n",
    "\n",
    "### Scaling Methods\n",
    "\n",
    "**1. Standardization (Z-Score Normalization):**\n",
    "\n",
    "Transforms features to have mean=0 and standard deviation=1.\n",
    "\n",
    "$$x_{\\text{scaled}} = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ = mean of feature\n",
    "- $\\sigma$ = standard deviation of feature\n",
    "\n",
    "**When to Use:**\n",
    "- Features follow normal distribution\n",
    "- Algorithms assume normally distributed data (Linear Regression, Logistic Regression)\n",
    "- Presence of outliers is acceptable (they remain outliers)\n",
    "\n",
    "**2. Min-Max Normalization:**\n",
    "\n",
    "Scales features to a fixed range [0, 1] (or custom range).\n",
    "\n",
    "$$x_{\\text{scaled}} = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}$$\n",
    "\n",
    "**When to Use:**\n",
    "- Bounded ranges needed (e.g., neural networks with sigmoid activation)\n",
    "- Data doesn't follow normal distribution\n",
    "- No significant outliers (outliers compress the scale)\n",
    "\n",
    "**3. Robust Scaling:**\n",
    "\n",
    "Uses median and interquartile range (IQR), robust to outliers.\n",
    "\n",
    "$$x_{\\text{scaled}} = \\frac{x - \\text{median}(x)}{\\text{IQR}(x)}$$\n",
    "\n",
    "Where: $\\text{IQR} = Q_3 - Q_1$ (75th percentile - 25th percentile)\n",
    "\n",
    "**When to Use:**\n",
    "- Data contains significant outliers\n",
    "- Median is more representative than mean\n",
    "\n",
    "**4. Log Transformation:**\n",
    "\n",
    "Applies logarithm to compress large values.\n",
    "\n",
    "$$x_{\\text{scaled}} = \\log(x + 1)$$\n",
    "\n",
    "(Add 1 to handle x=0 values)\n",
    "\n",
    "**When to Use:**\n",
    "- Highly skewed distributions (right-skewed)\n",
    "- Power-law relationships\n",
    "- Making multiplicative relationships additive\n",
    "\n",
    "### Decision Guide\n",
    "\n",
    "```\n",
    "Does data have outliers?\n",
    "‚îú‚îÄ YES: Use Robust Scaler\n",
    "‚îî‚îÄ NO: Is data normally distributed?\n",
    "       ‚îú‚îÄ YES: Use Standard Scaler\n",
    "       ‚îî‚îÄ NO: Is data highly skewed?\n",
    "              ‚îú‚îÄ YES: Use Log Transform + Standard Scaler\n",
    "              ‚îî‚îÄ NO: Use Min-Max Scaler\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362e8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling Implementation\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from scipy import stats\n",
    "\n",
    "class FeatureScaler:\n",
    "    \"\"\"\n",
    "    Comprehensive feature scaling toolkit.\n",
    "    \n",
    "    Supports multiple scaling methods:\n",
    "    - Standard scaling (z-score normalization)\n",
    "    - Min-Max scaling (0-1 normalization)\n",
    "    - Robust scaling (median and IQR)\n",
    "    - Log transformation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='standard'):\n",
    "        \"\"\"\n",
    "        Initialize with scaling method.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        method : str\n",
    "            Scaling method: 'standard', 'minmax', 'robust', 'log'\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.scaler = None\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def fit(self, X, feature_names=None):\n",
    "        \"\"\"\n",
    "        Fit scaler to data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like or DataFrame\n",
    "            Training data\n",
    "        feature_names : list\n",
    "            Names of features (for DataFrame output)\n",
    "        \"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names = X.columns.tolist()\n",
    "            X = X.values\n",
    "        else:\n",
    "            self.feature_names = feature_names\n",
    "        \n",
    "        if self.method == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "        elif self.method == 'minmax':\n",
    "            self.scaler = MinMaxScaler()\n",
    "        elif self.method == 'robust':\n",
    "            self.scaler = RobustScaler()\n",
    "        elif self.method == 'log':\n",
    "            self.scaler = None  # Log transform doesn't need fitting\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scaling method: {self.method}\")\n",
    "        \n",
    "        if self.scaler is not None:\n",
    "            self.scaler.fit(X)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform data using fitted scaler.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like or DataFrame\n",
    "            Data to transform\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame or array with scaled features\n",
    "        \"\"\"\n",
    "        is_dataframe = isinstance(X, pd.DataFrame)\n",
    "        \n",
    "        if is_dataframe:\n",
    "            X_values = X.values\n",
    "        else:\n",
    "            X_values = X\n",
    "        \n",
    "        if self.method == 'log':\n",
    "            X_scaled = np.log1p(X_values)  # log(1 + x) to handle zeros\n",
    "        else:\n",
    "            X_scaled = self.scaler.transform(X_values)\n",
    "        \n",
    "        if is_dataframe or self.feature_names is not None:\n",
    "            feature_names = self.feature_names if self.feature_names else X.columns\n",
    "            return pd.DataFrame(X_scaled, columns=feature_names, index=X.index if is_dataframe else None)\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def fit_transform(self, X, feature_names=None):\n",
    "        \"\"\"\n",
    "        Fit scaler and transform data in one step.\n",
    "        \"\"\"\n",
    "        return self.fit(X, feature_names).transform(X)\n",
    "    \n",
    "    def inverse_transform(self, X_scaled):\n",
    "        \"\"\"\n",
    "        Inverse transform scaled data back to original scale.\n",
    "        \"\"\"\n",
    "        if self.method == 'log':\n",
    "            return np.expm1(X_scaled)  # exp(x) - 1\n",
    "        \n",
    "        is_dataframe = isinstance(X_scaled, pd.DataFrame)\n",
    "        \n",
    "        if is_dataframe:\n",
    "            X_values = X_scaled.values\n",
    "        else:\n",
    "            X_values = X_scaled\n",
    "        \n",
    "        X_original = self.scaler.inverse_transform(X_values)\n",
    "        \n",
    "        if is_dataframe or self.feature_names is not None:\n",
    "            feature_names = self.feature_names if self.feature_names else X_scaled.columns\n",
    "            return pd.DataFrame(X_original, columns=feature_names, \n",
    "                              index=X_scaled.index if is_dataframe else None)\n",
    "        \n",
    "        return X_original\n",
    "    \n",
    "    def get_scaling_params(self):\n",
    "        \"\"\"\n",
    "        Get scaling parameters for inspection.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dictionary with scaling parameters\n",
    "        \"\"\"\n",
    "        if self.method == 'log':\n",
    "            return {'method': 'log', 'note': 'No parameters (element-wise transform)'}\n",
    "        \n",
    "        params = {'method': self.method}\n",
    "        \n",
    "        if hasattr(self.scaler, 'mean_'):\n",
    "            params['mean'] = self.scaler.mean_\n",
    "        if hasattr(self.scaler, 'scale_'):\n",
    "            params['scale'] = self.scaler.scale_\n",
    "        if hasattr(self.scaler, 'center_'):\n",
    "            params['center'] = self.scaler.center_\n",
    "        if hasattr(self.scaler, 'data_min_'):\n",
    "            params['data_min'] = self.scaler.data_min_\n",
    "        if hasattr(self.scaler, 'data_max_'):\n",
    "            params['data_max'] = self.scaler.data_max_\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def visualize_scaling_effect(self, X_original, X_scaled, feature_idx=0):\n",
    "        \"\"\"\n",
    "        Visualize the effect of scaling on a feature.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_original : array-like\n",
    "            Original data before scaling\n",
    "        X_scaled : array-like\n",
    "            Scaled data after transformation\n",
    "        feature_idx : int\n",
    "            Index of feature to visualize\n",
    "        \"\"\"\n",
    "        if isinstance(X_original, pd.DataFrame):\n",
    "            feature_name = X_original.columns[feature_idx]\n",
    "            X_orig_values = X_original.iloc[:, feature_idx].values\n",
    "        else:\n",
    "            feature_name = f\"Feature {feature_idx}\"\n",
    "            X_orig_values = X_original[:, feature_idx]\n",
    "        \n",
    "        if isinstance(X_scaled, pd.DataFrame):\n",
    "            X_scaled_values = X_scaled.iloc[:, feature_idx].values\n",
    "        else:\n",
    "            X_scaled_values = X_scaled[:, feature_idx]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Original distribution\n",
    "        axes[0, 0].hist(X_orig_values, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "        axes[0, 0].set_title(f'{feature_name} - Original Distribution')\n",
    "        axes[0, 0].set_xlabel('Value')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].axvline(np.mean(X_orig_values), color='red', linestyle='--', label='Mean')\n",
    "        axes[0, 0].axvline(np.median(X_orig_values), color='green', linestyle='--', label='Median')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # Scaled distribution\n",
    "        axes[0, 1].hist(X_scaled_values, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "        axes[0, 1].set_title(f'{feature_name} - Scaled Distribution ({self.method})')\n",
    "        axes[0, 1].set_xlabel('Scaled Value')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].axvline(np.mean(X_scaled_values), color='red', linestyle='--', label='Mean')\n",
    "        axes[0, 1].axvline(np.median(X_scaled_values), color='green', linestyle='--', label='Median')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Original box plot\n",
    "        axes[1, 0].boxplot(X_orig_values, vert=False)\n",
    "        axes[1, 0].set_title(f'{feature_name} - Original Box Plot')\n",
    "        axes[1, 0].set_xlabel('Value')\n",
    "        \n",
    "        # Scaled box plot\n",
    "        axes[1, 1].boxplot(X_scaled_values, vert=False)\n",
    "        axes[1, 1].set_title(f'{feature_name} - Scaled Box Plot')\n",
    "        axes[1, 1].set_xlabel('Scaled Value')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"\\n{feature_name} Statistics:\")\n",
    "        print(f\"\\nOriginal:\")\n",
    "        print(f\"  Mean: {np.mean(X_orig_values):.4f}\")\n",
    "        print(f\"  Std: {np.std(X_orig_values):.4f}\")\n",
    "        print(f\"  Min: {np.min(X_orig_values):.4f}\")\n",
    "        print(f\"  Max: {np.max(X_orig_values):.4f}\")\n",
    "        \n",
    "        print(f\"\\nScaled ({self.method}):\")\n",
    "        print(f\"  Mean: {np.mean(X_scaled_values):.4f}\")\n",
    "        print(f\"  Std: {np.std(X_scaled_values):.4f}\")\n",
    "        print(f\"  Min: {np.min(X_scaled_values):.4f}\")\n",
    "        print(f\"  Max: {np.max(X_scaled_values):.4f}\")\n",
    "\n",
    "print(\"‚úÖ FeatureScaler class implemented\")\n",
    "print(\"\\nSupported Scaling Methods:\")\n",
    "print(\"- 'standard': Z-score normalization (mean=0, std=1)\")\n",
    "print(\"- 'minmax': Scale to [0, 1] range\")\n",
    "print(\"- 'robust': Use median and IQR (robust to outliers)\")\n",
    "print(\"- 'log': Log transformation (for skewed distributions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2fa08",
   "metadata": {},
   "source": [
    "## üìù Section 3: Categorical Encoding\n",
    "\n",
    "### Why Encode Categorical Variables?\n",
    "\n",
    "**Problem:** Most machine learning algorithms require numeric input, but real-world data contains categorical features (device_type, wafer_lot, test_site, etc.).\n",
    "\n",
    "**Example Categorical Features:**\n",
    "- **Nominal**: No inherent order (color: red, blue, green)\n",
    "- **Ordinal**: Natural order (temperature: low, medium, high)\n",
    "- **High Cardinality**: Many unique values (product_id: 1000+ categories)\n",
    "\n",
    "### Encoding Methods\n",
    "\n",
    "**1. One-Hot Encoding (Dummy Variables):**\n",
    "\n",
    "Creates binary column for each category.\n",
    "\n",
    "**Original:**\n",
    "```\n",
    "| Color  |\n",
    "|--------|\n",
    "| Red    |\n",
    "| Blue   |\n",
    "| Green  |\n",
    "```\n",
    "\n",
    "**One-Hot Encoded:**\n",
    "```\n",
    "| Color_Red | Color_Blue | Color_Green |\n",
    "|-----------|------------|-------------|\n",
    "| 1         | 0          | 0           |\n",
    "| 0         | 1          | 0           |\n",
    "| 0         | 0          | 1           |\n",
    "```\n",
    "\n",
    "**Mathematical Representation:**\n",
    "For category $c_i$ in feature $C$ with $k$ categories:\n",
    "\n",
    "$$\\text{OneHot}(c_i) = [0, 0, ..., 1, ..., 0] \\in \\{0,1\\}^k$$\n",
    "\n",
    "Where 1 is at position $i$.\n",
    "\n",
    "**When to Use:**\n",
    "- Nominal categories (no order)\n",
    "- Low cardinality (< 15 categories)\n",
    "- Tree-based models or linear models\n",
    "\n",
    "**Limitations:**\n",
    "- High dimensionality with many categories\n",
    "- Sparse matrices (mostly zeros)\n",
    "- Multicollinearity in linear models (drop one column)\n",
    "\n",
    "**2. Label Encoding (Integer Encoding):**\n",
    "\n",
    "Maps categories to integers: {Red: 0, Blue: 1, Green: 2}\n",
    "\n",
    "$$\\text{LabelEncode}(c_i) = i \\in \\{0, 1, ..., k-1\\}$$\n",
    "\n",
    "**When to Use:**\n",
    "- Ordinal categories (temperature: low < medium < high)\n",
    "- Tree-based models (can handle integers naturally)\n",
    "- Target variable in classification\n",
    "\n",
    "**Limitations:**\n",
    "- Implies ordering (Red < Blue < Green) when none exists\n",
    "- Linear models interpret as numeric distance\n",
    "\n",
    "**3. Target Encoding (Mean Encoding):**\n",
    "\n",
    "Replaces category with mean of target variable for that category.\n",
    "\n",
    "$$\\text{TargetEncode}(c_i) = \\frac{1}{n_i} \\sum_{j: x_j = c_i} y_j$$\n",
    "\n",
    "Where:\n",
    "- $n_i$ = count of samples with category $c_i$\n",
    "- $y_j$ = target value for sample $j$\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "| Device_Type | Yield (target) |\n",
    "|-------------|----------------|\n",
    "| A           | 85%            |\n",
    "| A           | 90%            |\n",
    "| B           | 70%            |\n",
    "| B           | 75%            |\n",
    "```\n",
    "\n",
    "Target Encoding: {A: 87.5%, B: 72.5%}\n",
    "\n",
    "**When to Use:**\n",
    "- High cardinality features (many categories)\n",
    "- Strong correlation between category and target\n",
    "- Gradient boosting models (XGBoost, LightGBM)\n",
    "\n",
    "**Limitations:**\n",
    "- Risk of overfitting (use smoothing or cross-validation)\n",
    "- Data leakage if not done carefully\n",
    "- Requires target variable (supervised only)\n",
    "\n",
    "**4. Frequency Encoding:**\n",
    "\n",
    "Replaces category with its occurrence frequency.\n",
    "\n",
    "$$\\text{FreqEncode}(c_i) = \\frac{n_i}{n}$$\n",
    "\n",
    "**5. Binary Encoding:**\n",
    "\n",
    "Converts categories to binary digits, creates log‚ÇÇ(k) columns instead of k.\n",
    "\n",
    "**Decision Guide:**\n",
    "\n",
    "```\n",
    "What type of categorical feature?\n",
    "‚îú‚îÄ Ordinal (ordered): Use Label Encoding\n",
    "‚îî‚îÄ Nominal (no order):\n",
    "    ‚îú‚îÄ Low cardinality (< 15): Use One-Hot Encoding\n",
    "    ‚îî‚îÄ High cardinality (> 15):\n",
    "        ‚îú‚îÄ Tree-based model: Target Encoding\n",
    "        ‚îî‚îÄ Linear model: One-Hot + dimensionality reduction\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79003d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Encoding Implementation\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from category_encoders import TargetEncoder  # pip install category-encoders\n",
    "\n",
    "class CategoricalEncoder:\n",
    "    \"\"\"\n",
    "    Comprehensive categorical encoding toolkit.\n",
    "    \n",
    "    Supports multiple encoding methods:\n",
    "    - One-hot encoding (binary columns)\n",
    "    - Label encoding (integer mapping)\n",
    "    - Target encoding (mean of target per category)\n",
    "    - Frequency encoding (category occurrence rate)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='onehot'):\n",
    "        \"\"\"\n",
    "        Initialize with encoding method.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        method : str\n",
    "            Encoding method: 'onehot', 'label', 'target', 'frequency'\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.encoders = {}\n",
    "        self.encoded_columns = []\n",
    "        \n",
    "    def fit(self, df, categorical_cols, target_col=None):\n",
    "        \"\"\"\n",
    "        Fit encoders to categorical columns.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            Training data\n",
    "        categorical_cols : list\n",
    "            Names of categorical columns to encode\n",
    "        target_col : str\n",
    "            Target column name (required for target encoding)\n",
    "        \"\"\"\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.target_col = target_col\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if self.method == 'label':\n",
    "                encoder = LabelEncoder()\n",
    "                encoder.fit(df[col])\n",
    "                self.encoders[col] = encoder\n",
    "                \n",
    "            elif self.method == 'target':\n",
    "                if target_col is None:\n",
    "                    raise ValueError(\"target_col required for target encoding\")\n",
    "                encoder = TargetEncoder()\n",
    "                encoder.fit(df[col], df[target_col])\n",
    "                self.encoders[col] = encoder\n",
    "                \n",
    "            elif self.method == 'frequency':\n",
    "                # Store frequency mapping\n",
    "                freq_map = df[col].value_counts(normalize=True).to_dict()\n",
    "                self.encoders[col] = freq_map\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Transform categorical columns using fitted encoders.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            Data to encode\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with encoded categorical features\n",
    "        \"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        \n",
    "        if self.method == 'onehot':\n",
    "            # One-hot encoding using pandas get_dummies\n",
    "            df_encoded = pd.get_dummies(df_encoded, columns=self.categorical_cols, \n",
    "                                       prefix=self.categorical_cols, drop_first=False)\n",
    "            self.encoded_columns = [col for col in df_encoded.columns \n",
    "                                   if any(cat in col for cat in self.categorical_cols)]\n",
    "        \n",
    "        elif self.method == 'label':\n",
    "            for col in self.categorical_cols:\n",
    "                encoder = self.encoders[col]\n",
    "                df_encoded[col] = encoder.transform(df[col])\n",
    "        \n",
    "        elif self.method == 'target':\n",
    "            for col in self.categorical_cols:\n",
    "                encoder = self.encoders[col]\n",
    "                df_encoded[col] = encoder.transform(df[col])\n",
    "        \n",
    "        elif self.method == 'frequency':\n",
    "            for col in self.categorical_cols:\n",
    "                freq_map = self.encoders[col]\n",
    "                df_encoded[col] = df[col].map(freq_map)\n",
    "                # Handle unseen categories\n",
    "                df_encoded[col].fillna(0, inplace=True)\n",
    "        \n",
    "        return df_encoded\n",
    "    \n",
    "    def fit_transform(self, df, categorical_cols, target_col=None):\n",
    "        \"\"\"\n",
    "        Fit encoders and transform data in one step.\n",
    "        \"\"\"\n",
    "        return self.fit(df, categorical_cols, target_col).transform(df)\n",
    "    \n",
    "    def inverse_transform(self, df, encoded_cols=None):\n",
    "        \"\"\"\n",
    "        Inverse transform encoded data back to original categories.\n",
    "        Only works for label encoding.\n",
    "        \"\"\"\n",
    "        if self.method != 'label':\n",
    "            raise ValueError(\"Inverse transform only supported for label encoding\")\n",
    "        \n",
    "        df_decoded = df.copy()\n",
    "        \n",
    "        for col in self.categorical_cols:\n",
    "            if col in df_decoded.columns:\n",
    "                encoder = self.encoders[col]\n",
    "                df_decoded[col] = encoder.inverse_transform(df_decoded[col].astype(int))\n",
    "        \n",
    "        return df_decoded\n",
    "    \n",
    "    def get_encoding_info(self):\n",
    "        \"\"\"\n",
    "        Get information about encodings applied.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dictionary with encoding details\n",
    "        \"\"\"\n",
    "        info = {\n",
    "            'method': self.method,\n",
    "            'categorical_columns': self.categorical_cols,\n",
    "            'encoders': {}\n",
    "        }\n",
    "        \n",
    "        for col in self.categorical_cols:\n",
    "            if self.method == 'label':\n",
    "                encoder = self.encoders[col]\n",
    "                info['encoders'][col] = {\n",
    "                    'classes': encoder.classes_.tolist(),\n",
    "                    'n_classes': len(encoder.classes_)\n",
    "                }\n",
    "            elif self.method == 'frequency':\n",
    "                info['encoders'][col] = self.encoders[col]\n",
    "        \n",
    "        if self.method == 'onehot':\n",
    "            info['encoded_columns'] = self.encoded_columns\n",
    "            info['n_new_columns'] = len(self.encoded_columns)\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    def visualize_encoding(self, df_original, df_encoded, col_name):\n",
    "        \"\"\"\n",
    "        Visualize the effect of encoding on a categorical column.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df_original : DataFrame\n",
    "            Original data before encoding\n",
    "        df_encoded : DataFrame\n",
    "            Encoded data after transformation\n",
    "        col_name : str\n",
    "            Name of categorical column to visualize\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Original categories distribution\n",
    "        value_counts = df_original[col_name].value_counts()\n",
    "        axes[0].bar(range(len(value_counts)), value_counts.values, color='skyblue', edgecolor='black')\n",
    "        axes[0].set_xticks(range(len(value_counts)))\n",
    "        axes[0].set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
    "        axes[0].set_title(f'{col_name} - Original Categories')\n",
    "        axes[0].set_xlabel('Category')\n",
    "        axes[0].set_ylabel('Count')\n",
    "        axes[0].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Encoded representation\n",
    "        if self.method == 'onehot':\n",
    "            # Show one-hot encoded columns\n",
    "            encoded_cols = [c for c in df_encoded.columns if c.startswith(f'{col_name}_')]\n",
    "            encoded_sample = df_encoded[encoded_cols].head(10)\n",
    "            \n",
    "            sns.heatmap(encoded_sample.T, cmap='YlOrRd', cbar=True, \n",
    "                       linewidths=0.5, linecolor='gray', ax=axes[1])\n",
    "            axes[1].set_title(f'{col_name} - One-Hot Encoded (First 10 Samples)')\n",
    "            axes[1].set_xlabel('Sample Index')\n",
    "            axes[1].set_ylabel('Encoded Columns')\n",
    "        \n",
    "        else:\n",
    "            # Show encoded values distribution\n",
    "            axes[1].hist(df_encoded[col_name], bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "            axes[1].set_title(f'{col_name} - Encoded Values ({self.method})')\n",
    "            axes[1].set_xlabel('Encoded Value')\n",
    "            axes[1].set_ylabel('Frequency')\n",
    "            axes[1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print encoding mapping\n",
    "        print(f\"\\n{col_name} Encoding Details:\")\n",
    "        if self.method == 'label':\n",
    "            encoder = self.encoders[col_name]\n",
    "            mapping = dict(zip(encoder.classes_, range(len(encoder.classes_))))\n",
    "            print(f\"Label Mapping: {mapping}\")\n",
    "        elif self.method == 'frequency':\n",
    "            print(f\"Frequency Mapping: {self.encoders[col_name]}\")\n",
    "        elif self.method == 'onehot':\n",
    "            encoded_cols = [c for c in df_encoded.columns if c.startswith(f'{col_name}_')]\n",
    "            print(f\"Created {len(encoded_cols)} binary columns: {encoded_cols[:5]}...\")\n",
    "\n",
    "print(\"‚úÖ CategoricalEncoder class implemented\")\n",
    "print(\"\\nSupported Encoding Methods:\")\n",
    "print(\"- 'onehot': Binary columns for each category\")\n",
    "print(\"- 'label': Integer mapping (0, 1, 2, ...)\")\n",
    "print(\"- 'target': Mean of target variable per category\")\n",
    "print(\"- 'frequency': Category occurrence frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e92a16",
   "metadata": {},
   "source": [
    "## üìù Section 4: Feature Creation and Engineering\n",
    "\n",
    "### Why Create New Features?\n",
    "\n",
    "**Core Principle:** Raw features rarely represent the underlying patterns optimally. Creating derived features helps models learn relationships more easily.\n",
    "\n",
    "**Types of Feature Creation:**\n",
    "\n",
    "### 1. Polynomial Features (Interactions and Powers)\n",
    "\n",
    "**Interactions:** Combine two features to capture their joint effect.\n",
    "\n",
    "$$\\text{Interaction}(x_1, x_2) = x_1 \\times x_2$$\n",
    "\n",
    "**Example (Semiconductor):**\n",
    "- $x_1$ = Voltage (VDD)\n",
    "- $x_2$ = Frequency\n",
    "- Interaction = VDD √ó Frequency (captures dynamic power relationship)\n",
    "\n",
    "**Powers:** Capture non-linear relationships.\n",
    "\n",
    "$$\\text{Polynomial}(x, d) = [x, x^2, x^3, ..., x^d]$$\n",
    "\n",
    "**Full Polynomial Features (degree 2):**\n",
    "For features $[x_1, x_2]$, creates: $[1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]$\n",
    "\n",
    "**When to Use:**\n",
    "- Non-linear relationships in linear models\n",
    "- Interactions between features are important\n",
    "- Sufficient data to avoid overfitting (curse of dimensionality)\n",
    "\n",
    "**Trade-off:**\n",
    "- Degree 2 with 10 features ‚Üí 66 features\n",
    "- Degree 3 with 10 features ‚Üí 286 features\n",
    "- Can lead to overfitting with limited data\n",
    "\n",
    "### 2. Domain-Specific Features\n",
    "\n",
    "**Definition:** Features created using domain expertise and physical/business understanding.\n",
    "\n",
    "**Semiconductor Testing Examples:**\n",
    "\n",
    "**Power Efficiency:**\n",
    "$$\\text{Efficiency} = \\frac{\\text{Frequency}}{\\text{Power}}$$\n",
    "\n",
    "Higher efficiency = better performance per watt\n",
    "\n",
    "**Power Density:**\n",
    "$$\\text{Power Density} = \\frac{\\text{Power}}{\\text{VDD}}$$\n",
    "\n",
    "Normalized power consumption\n",
    "\n",
    "**Leakage Current (Normalized):**\n",
    "$$\\text{Leakage Norm} = \\frac{\\text{IDD}_{\\text{static}}}{\\text{VDD}}$$\n",
    "\n",
    "Resistance-like metric for leakage\n",
    "\n",
    "**Spatial Distance:**\n",
    "$$\\text{Radial Distance} = \\sqrt{x^2 + y^2}$$\n",
    "\n",
    "Distance from wafer center (edge dies often different)\n",
    "\n",
    "**Temperature Coefficient:**\n",
    "$$\\text{Temp Coeff} = \\frac{\\text{Performance}}{\\text{Temperature}}$$\n",
    "\n",
    "How performance changes with temperature\n",
    "\n",
    "### 3. Statistical Aggregations\n",
    "\n",
    "**Group-Level Features:** Aggregate statistics within groups.\n",
    "\n",
    "**Example:** For dies on the same wafer:\n",
    "- Mean VDD per wafer\n",
    "- Std deviation of frequency per wafer\n",
    "- Deviation from wafer mean\n",
    "\n",
    "$$\\text{Deviation from Wafer Mean} = x_{\\text{die}} - \\bar{x}_{\\text{wafer}}$$\n",
    "\n",
    "**Why This Matters:**\n",
    "- Captures process variation patterns\n",
    "- Identifies outlier dies within wafer\n",
    "- Helps predict wafer-level yield\n",
    "\n",
    "### 4. Temporal Features (Time Series)\n",
    "\n",
    "**From timestamps, extract:**\n",
    "- Hour of day, day of week, month\n",
    "- Days since reference event\n",
    "- Rolling statistics (moving average, std)\n",
    "- Lag features (previous N values)\n",
    "\n",
    "### 5. Text Features\n",
    "\n",
    "**From text data, extract:**\n",
    "- Text length, word count\n",
    "- Presence of keywords\n",
    "- Sentiment scores\n",
    "- TF-IDF vectors\n",
    "\n",
    "### Feature Creation Strategy\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Raw Features] --> B{Domain Knowledge?}\n",
    "    B -->|Yes| C[Create Domain Features]\n",
    "    B -->|No| D[Try Polynomial Features]\n",
    "    C --> E[Check Correlation]\n",
    "    D --> E\n",
    "    E --> F{New Features Useful?}\n",
    "    F -->|Yes| G[Keep Features]\n",
    "    F -->|No| H[Discard Features]\n",
    "    G --> I[Feature Selection]\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "1. **Start Simple:** Linear combinations first\n",
    "2. **Use Domain Knowledge:** Physics, business logic\n",
    "3. **Validate Usefulness:** Check correlation with target\n",
    "4. **Regularization:** Use L1/L2 to handle many features\n",
    "5. **Feature Importance:** Remove low-importance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Creation Implementation\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from itertools import combinations\n",
    "\n",
    "class FeatureCreator:\n",
    "    \"\"\"\n",
    "    Comprehensive feature creation toolkit.\n",
    "    \n",
    "    Supports:\n",
    "    - Polynomial features (powers and interactions)\n",
    "    - Domain-specific features (custom recipes)\n",
    "    - Statistical aggregations (group-level features)\n",
    "    - Mathematical transformations (log, sqrt, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize feature creator.\"\"\"\n",
    "        self.created_features = []\n",
    "        self.poly_features = None\n",
    "        \n",
    "    def create_polynomial_features(self, df, numeric_cols, degree=2, interaction_only=False):\n",
    "        \"\"\"\n",
    "        Create polynomial features (powers and interactions).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            Input data\n",
    "        numeric_cols : list\n",
    "            Numeric columns to create polynomial features from\n",
    "        degree : int\n",
    "            Polynomial degree (2 = squares and interactions, 3 = cubes, etc.)\n",
    "        interaction_only : bool\n",
    "            If True, only create interaction terms (no powers)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with original + polynomial features\n",
    "        \"\"\"\n",
    "        # Extract numeric data\n",
    "        X = df[numeric_cols].values\n",
    "        \n",
    "        # Create polynomial features\n",
    "        poly = PolynomialFeatures(degree=degree, interaction_only=interaction_only, \n",
    "                                  include_bias=False)\n",
    "        X_poly = poly.fit_transform(X)\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = poly.get_feature_names_out(numeric_cols)\n",
    "        \n",
    "        # Create DataFrame with polynomial features\n",
    "        df_poly = pd.DataFrame(X_poly, columns=feature_names, index=df.index)\n",
    "        \n",
    "        # Store polynomial transformer\n",
    "        self.poly_features = poly\n",
    "        \n",
    "        # Combine with original dataframe (keep non-numeric columns)\n",
    "        non_numeric_cols = [col for col in df.columns if col not in numeric_cols]\n",
    "        df_result = pd.concat([df[non_numeric_cols], df_poly], axis=1)\n",
    "        \n",
    "        # Track created features\n",
    "        new_features = [f for f in feature_names if f not in numeric_cols]\n",
    "        self.created_features.extend(new_features)\n",
    "        \n",
    "        print(f\"‚úÖ Created {len(new_features)} polynomial features (degree={degree})\")\n",
    "        print(f\"   Original: {len(numeric_cols)} ‚Üí New: {len(feature_names)}\")\n",
    "        \n",
    "        return df_result\n",
    "    \n",
    "    def create_domain_features(self, df, recipes):\n",
    "        \"\"\"\n",
    "        Create domain-specific features using custom recipes.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            Input data\n",
    "        recipes : dict\n",
    "            Dictionary mapping new feature names to lambda functions\n",
    "            Example: {'efficiency': lambda x: x['freq'] / x['power']}\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with original + domain features\n",
    "        \"\"\"\n",
    "        df_result = df.copy()\n",
    "        \n",
    "        for feature_name, recipe_func in recipes.items():\n",
    "            try:\n",
    "                df_result[feature_name] = recipe_func(df)\n",
    "                self.created_features.append(feature_name)\n",
    "                print(f\"‚úÖ Created domain feature: {feature_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to create {feature_name}: {e}\")\n",
    "        \n",
    "        return df_result\n",
    "    \n",
    "    def create_aggregation_features(self, df, group_col, agg_cols, agg_funcs=['mean', 'std', 'min', 'max']):\n",
    "        \"\"\"\n",
    "        Create group-level aggregation features.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            Input data\n",
    "        group_col : str\n",
    "            Column to group by (e.g., 'wafer_id')\n",
    "        agg_cols : list\n",
    "            Columns to aggregate\n",
    "        agg_funcs : list\n",
    "            Aggregation functions to apply\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with original + aggregation features\n",
    "        \"\"\"\n",
    "        df_result = df.copy()\n",
    "        \n",
    "        for col in agg_cols:\n",
    "            for func in agg_funcs:\n",
    "                # Create aggregation\n",
    "                agg_name = f'{col}_{func}_by_{group_col}'\n",
    "                agg_values = df.groupby(group_col)[col].transform(func)\n",
    "                df_result[agg_name] = agg_values\n",
    "                \n",
    "                self.created_features.append(agg_name)\n",
    "                \n",
    "                # Also create deviation from group mean\n",
    "                if func == 'mean':\n",
    "                    dev_name = f'{col}_deviation_from_{group_col}'\n",
    "                    df_result[dev_name] = df[col] - agg_values\n",
    "                    self.created_features.append(dev_name)\n",
    "        \n",
    "        print(f\"‚úÖ Created {len(agg_cols) * len(agg_funcs)} aggregation features\")\n",
    "        print(f\"   Grouped by: {group_col}\")\n",
    "        \n",
    "        return df_result\n",
    "    \n",
    "    def create_interaction_features(self, df, col_pairs):\n",
    "        \"\"\"\n",
    "        Create interaction features for specific column pairs.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            Input data\n",
    "        col_pairs : list of tuples\n",
    "            Pairs of columns to create interactions\n",
    "            Example: [('voltage', 'frequency'), ('power', 'temp')]\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with original + interaction features\n",
    "        \"\"\"\n",
    "        df_result = df.copy()\n",
    "        \n",
    "        for col1, col2 in col_pairs:\n",
    "            # Multiplication interaction\n",
    "            mult_name = f'{col1}_x_{col2}'\n",
    "            df_result[mult_name] = df[col1] * df[col2]\n",
    "            self.created_features.append(mult_name)\n",
    "            \n",
    "            # Ratio features (both directions)\n",
    "            ratio_name_1 = f'{col1}_div_{col2}'\n",
    "            ratio_name_2 = f'{col2}_div_{col1}'\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            df_result[ratio_name_1] = df[col1] / (df[col2] + 1e-10)\n",
    "            df_result[ratio_name_2] = df[col2] / (df[col1] + 1e-10)\n",
    "            \n",
    "            self.created_features.extend([ratio_name_1, ratio_name_2])\n",
    "            \n",
    "            print(f\"‚úÖ Created interactions for ({col1}, {col2}): multiply + 2 ratios\")\n",
    "        \n",
    "        return df_result\n",
    "    \n",
    "    def create_mathematical_transforms(self, df, numeric_cols, transforms=['log', 'sqrt', 'square']):\n",
    "        \"\"\"\n",
    "        Create mathematical transformations of numeric features.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            Input data\n",
    "        numeric_cols : list\n",
    "            Columns to transform\n",
    "        transforms : list\n",
    "            Transformations to apply: 'log', 'sqrt', 'square', 'inverse'\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with original + transformed features\n",
    "        \"\"\"\n",
    "        df_result = df.copy()\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if 'log' in transforms:\n",
    "                # Log transform (handle zeros and negatives)\n",
    "                log_name = f'{col}_log'\n",
    "                df_result[log_name] = np.log1p(df[col].clip(lower=0))\n",
    "                self.created_features.append(log_name)\n",
    "            \n",
    "            if 'sqrt' in transforms:\n",
    "                # Square root (handle negatives)\n",
    "                sqrt_name = f'{col}_sqrt'\n",
    "                df_result[sqrt_name] = np.sqrt(df[col].clip(lower=0))\n",
    "                self.created_features.append(sqrt_name)\n",
    "            \n",
    "            if 'square' in transforms:\n",
    "                # Square\n",
    "                square_name = f'{col}_square'\n",
    "                df_result[square_name] = df[col] ** 2\n",
    "                self.created_features.append(square_name)\n",
    "            \n",
    "            if 'inverse' in transforms:\n",
    "                # Inverse (handle zeros)\n",
    "                inv_name = f'{col}_inverse'\n",
    "                df_result[inv_name] = 1 / (df[col] + 1e-10)\n",
    "                self.created_features.append(inv_name)\n",
    "        \n",
    "        print(f\"‚úÖ Created {len(numeric_cols) * len(transforms)} mathematical transform features\")\n",
    "        \n",
    "        return df_result\n",
    "    \n",
    "    def get_created_features(self):\n",
    "        \"\"\"\n",
    "        Get list of all created features.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        List of feature names created by this instance\n",
    "        \"\"\"\n",
    "        return self.created_features\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset created features list.\"\"\"\n",
    "        self.created_features = []\n",
    "        self.poly_features = None\n",
    "\n",
    "print(\"‚úÖ FeatureCreator class implemented\")\n",
    "print(\"\\nSupported Feature Creation Methods:\")\n",
    "print(\"- create_polynomial_features(): Powers and interactions\")\n",
    "print(\"- create_domain_features(): Custom recipes (domain knowledge)\")\n",
    "print(\"- create_aggregation_features(): Group-level statistics\")\n",
    "print(\"- create_interaction_features(): Specific column pair interactions\")\n",
    "print(\"- create_mathematical_transforms(): Log, sqrt, square, inverse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be71bd",
   "metadata": {},
   "source": [
    "## üìù Section 5: Feature Selection Methods\n",
    "\n",
    "### Why Feature Selection?\n",
    "\n",
    "**Problems with Too Many Features:**\n",
    "1. **Curse of Dimensionality:** Model performance degrades with too many features relative to samples\n",
    "2. **Overfitting:** Model learns noise instead of patterns\n",
    "3. **Computational Cost:** Training and inference become expensive\n",
    "4. **Interpretability:** Harder to understand which features matter\n",
    "5. **Multicollinearity:** Correlated features cause instability in linear models\n",
    "\n",
    "**Goal:** Keep only the most informative features that contribute to prediction accuracy.\n",
    "\n",
    "### Feature Selection Categories\n",
    "\n",
    "**1. Filter Methods:** Use statistical measures independent of model\n",
    "**2. Wrapper Methods:** Use model performance to evaluate feature subsets\n",
    "**3. Embedded Methods:** Feature selection during model training (L1 regularization)\n",
    "\n",
    "### Filter Methods\n",
    "\n",
    "#### A. Variance Threshold\n",
    "\n",
    "**Principle:** Remove features with low variance (near-constant values).\n",
    "\n",
    "$$\\text{Variance}(x) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2$$\n",
    "\n",
    "**When to Use:**\n",
    "- Quick preprocessing step\n",
    "- Remove features that don't vary much across samples\n",
    "- Binary features with 95%+ same value\n",
    "\n",
    "**Example:**\n",
    "- Feature with values [1, 1, 1, 1, 2, 1, 1, 1] ‚Üí Low variance, likely uninformative\n",
    "- Feature with values [1, 5, 2, 8, 3, 9, 1, 6] ‚Üí High variance, potentially useful\n",
    "\n",
    "**Sklearn Implementation:**\n",
    "```python\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=0.01)  # Remove if variance < 0.01\n",
    "```\n",
    "\n",
    "#### B. Correlation-Based Selection\n",
    "\n",
    "**Principle:** Remove highly correlated features (redundant information).\n",
    "\n",
    "**Pearson Correlation:**\n",
    "$$r_{xy} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2 \\sum(y_i - \\bar{y})^2}}$$\n",
    "\n",
    "Range: $r \\in [-1, 1]$\n",
    "- $r = 1$: Perfect positive correlation\n",
    "- $r = -1$: Perfect negative correlation\n",
    "- $r = 0$: No linear correlation\n",
    "\n",
    "**Strategy:**\n",
    "1. Calculate correlation matrix\n",
    "2. For pairs with $|r| > 0.95$ (highly correlated), remove one feature\n",
    "3. Keep feature with higher correlation to target (if available)\n",
    "\n",
    "**When to Use:**\n",
    "- Linear models (affected by multicollinearity)\n",
    "- Many features with redundant information\n",
    "- After creating polynomial features\n",
    "\n",
    "#### C. Mutual Information\n",
    "\n",
    "**Principle:** Measure mutual dependence between feature and target.\n",
    "\n",
    "**Definition:**\n",
    "$$I(X; Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)}$$\n",
    "\n",
    "Where:\n",
    "- $I(X; Y)$ = mutual information between X and Y\n",
    "- $p(x, y)$ = joint probability\n",
    "- $p(x), p(y)$ = marginal probabilities\n",
    "\n",
    "**Properties:**\n",
    "- $I(X; Y) \\geq 0$ (always non-negative)\n",
    "- $I(X; Y) = 0$ if X and Y are independent\n",
    "- Captures non-linear relationships (unlike correlation)\n",
    "\n",
    "**When to Use:**\n",
    "- Non-linear relationships between features and target\n",
    "- Classification problems (discrete target)\n",
    "- Ranking features by importance\n",
    "\n",
    "**Sklearn Implementation:**\n",
    "```python\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "mi_scores = mutual_info_classif(X, y)  # Classification\n",
    "mi_scores = mutual_info_regression(X, y)  # Regression\n",
    "```\n",
    "\n",
    "#### D. Chi-Square Test\n",
    "\n",
    "**Principle:** Test independence between categorical feature and target.\n",
    "\n",
    "$$\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}$$\n",
    "\n",
    "Where:\n",
    "- $O_i$ = observed frequency\n",
    "- $E_i$ = expected frequency (if independent)\n",
    "\n",
    "**When to Use:**\n",
    "- Categorical features\n",
    "- Classification problems\n",
    "- Quick statistical test for relevance\n",
    "\n",
    "### Wrapper Methods\n",
    "\n",
    "#### Recursive Feature Elimination (RFE)\n",
    "\n",
    "**Algorithm:**\n",
    "1. Train model on all features\n",
    "2. Rank features by importance\n",
    "3. Remove least important feature\n",
    "4. Repeat until desired number of features\n",
    "\n",
    "**When to Use:**\n",
    "- Small to medium feature sets\n",
    "- When computational cost is acceptable\n",
    "- Linear models or tree-based models (have feature importance)\n",
    "\n",
    "**Trade-off:** More accurate but computationally expensive (trains many models)\n",
    "\n",
    "### Embedded Methods\n",
    "\n",
    "#### L1 Regularization (Lasso)\n",
    "\n",
    "**Objective Function:**\n",
    "$$\\min_{\\beta} \\left\\{ \\sum_{i=1}^{n} (y_i - \\beta^T x_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\right\\}$$\n",
    "\n",
    "**Effect:** L1 penalty drives some coefficients to exactly zero ‚Üí automatic feature selection\n",
    "\n",
    "**When to Use:**\n",
    "- Linear models (regression, logistic regression)\n",
    "- High-dimensional data (more features than samples)\n",
    "- Want sparse solution (few non-zero coefficients)\n",
    "\n",
    "### Feature Selection Strategy\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[All Features] --> B{How many features?}\n",
    "    B -->|< 100| C[Try All Methods]\n",
    "    B -->|100-1000| D[Filter Methods First]\n",
    "    B -->|> 1000| E[Variance Threshold + Correlation]\n",
    "    \n",
    "    C --> F{Model Type?}\n",
    "    D --> F\n",
    "    E --> F\n",
    "    \n",
    "    F -->|Linear| G[Correlation + Lasso]\n",
    "    F -->|Tree-based| H[Feature Importance + RFE]\n",
    "    F -->|Neural Net| I[Mutual Information + PCA]\n",
    "    \n",
    "    G --> J[Final Feature Set]\n",
    "    H --> J\n",
    "    I --> J\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection Implementation\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif, mutual_info_regression, chi2, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "class FeatureSelector:\n",
    "    \"\"\"\n",
    "    Comprehensive feature selection toolkit.\n",
    "    \n",
    "    Supports multiple selection methods:\n",
    "    - Variance threshold (remove low-variance features)\n",
    "    - Correlation-based (remove highly correlated features)\n",
    "    - Mutual information (rank by MI score)\n",
    "    - Chi-square test (categorical features)\n",
    "    - Recursive Feature Elimination (wrapper method)\n",
    "    - Feature importance (from tree-based models)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='variance'):\n",
    "        \"\"\"\n",
    "        Initialize with selection method.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        method : str\n",
    "            Selection method: 'variance', 'correlation', 'mutual_info', \n",
    "            'chi2', 'rfe', 'importance'\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.selector = None\n",
    "        self.selected_features = []\n",
    "        self.feature_scores = {}\n",
    "        \n",
    "    def select_by_variance(self, df, threshold=0.01):\n",
    "        \"\"\"\n",
    "        Remove features with variance below threshold.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            Input data\n",
    "        threshold : float\n",
    "            Minimum variance required (features below this are removed)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with low-variance features removed\n",
    "        \"\"\"\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        # Calculate variance for each feature\n",
    "        variances = df[numeric_cols].var()\n",
    "        \n",
    "        # Select features above threshold\n",
    "        selected = variances[variances > threshold].index.tolist()\n",
    "        removed = variances[variances <= threshold].index.tolist()\n",
    "        \n",
    "        self.selected_features = selected\n",
    "        self.feature_scores = variances.to_dict()\n",
    "        \n",
    "        print(f\"‚úÖ Variance Threshold Selection:\")\n",
    "        print(f\"   Threshold: {threshold}\")\n",
    "        print(f\"   Kept: {len(selected)} features\")\n",
    "        print(f\"   Removed: {len(removed)} features\")\n",
    "        if removed:\n",
    "            print(f\"   Removed features: {removed[:5]}...\" if len(removed) > 5 else f\"   Removed features: {removed}\")\n",
    "        \n",
    "        # Keep selected numeric + all non-numeric columns\n",
    "        non_numeric_cols = [col for col in df.columns if col not in numeric_cols]\n",
    "        return df[non_numeric_cols + selected]\n",
    "    \n",
    "    def select_by_correlation(self, df, threshold=0.95, target_col=None):\n",
    "        \"\"\"\n",
    "        Remove highly correlated features (keeps one from each correlated pair).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            Input data\n",
    "        threshold : float\n",
    "            Correlation threshold (remove if |correlation| > threshold)\n",
    "        target_col : str\n",
    "            Target column (if provided, keeps feature more correlated with target)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with redundant features removed\n",
    "        \"\"\"\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if target_col and target_col in numeric_cols:\n",
    "            numeric_cols = [col for col in numeric_cols if col != target_col]\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = df[numeric_cols].corr().abs()\n",
    "        \n",
    "        # Get upper triangle (avoid duplicate pairs)\n",
    "        upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        \n",
    "        # Find features to remove\n",
    "        to_remove = set()\n",
    "        \n",
    "        for col in upper_tri.columns:\n",
    "            # Get features highly correlated with this feature\n",
    "            high_corr_features = upper_tri.index[upper_tri[col] > threshold].tolist()\n",
    "            \n",
    "            if high_corr_features:\n",
    "                for corr_feature in high_corr_features:\n",
    "                    # If target provided, keep feature more correlated with target\n",
    "                    if target_col and target_col in df.columns:\n",
    "                        corr_with_target_col = abs(df[col].corr(df[target_col]))\n",
    "                        corr_with_target_feat = abs(df[corr_feature].corr(df[target_col]))\n",
    "                        \n",
    "                        if corr_with_target_col >= corr_with_target_feat:\n",
    "                            to_remove.add(corr_feature)\n",
    "                        else:\n",
    "                            to_remove.add(col)\n",
    "                    else:\n",
    "                        # No target, just remove the second feature\n",
    "                        to_remove.add(corr_feature)\n",
    "        \n",
    "        # Select features to keep\n",
    "        self.selected_features = [col for col in numeric_cols if col not in to_remove]\n",
    "        \n",
    "        print(f\"‚úÖ Correlation-Based Selection:\")\n",
    "        print(f\"   Threshold: {threshold}\")\n",
    "        print(f\"   Kept: {len(self.selected_features)} features\")\n",
    "        print(f\"   Removed: {len(to_remove)} features\")\n",
    "        if to_remove:\n",
    "            removed_list = list(to_remove)\n",
    "            print(f\"   Removed features: {removed_list[:5]}...\" if len(removed_list) > 5 else f\"   Removed features: {removed_list}\")\n",
    "        \n",
    "        # Keep selected numeric + all non-numeric + target\n",
    "        non_numeric_cols = [col for col in df.columns if col not in numeric_cols and col != target_col]\n",
    "        keep_cols = non_numeric_cols + self.selected_features\n",
    "        if target_col:\n",
    "            keep_cols.append(target_col)\n",
    "        \n",
    "        return df[keep_cols]\n",
    "    \n",
    "    def select_by_mutual_info(self, df, target_col, k=10, task='classification'):\n",
    "        \"\"\"\n",
    "        Select top k features by mutual information score.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            Input data\n",
    "        target_col : str\n",
    "            Target column name\n",
    "        k : int\n",
    "            Number of top features to select\n",
    "        task : str\n",
    "            'classification' or 'regression'\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with top k features + target\n",
    "        \"\"\"\n",
    "        # Separate features and target\n",
    "        feature_cols = [col for col in df.columns if col != target_col]\n",
    "        X = df[feature_cols].select_dtypes(include=[np.number])\n",
    "        y = df[target_col]\n",
    "        \n",
    "        # Calculate mutual information\n",
    "        if task == 'classification':\n",
    "            mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "        else:\n",
    "            mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "        \n",
    "        # Create DataFrame with scores\n",
    "        mi_df = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'mi_score': mi_scores\n",
    "        }).sort_values('mi_score', ascending=False)\n",
    "        \n",
    "        # Select top k features\n",
    "        self.selected_features = mi_df.head(k)['feature'].tolist()\n",
    "        self.feature_scores = dict(zip(mi_df['feature'], mi_df['mi_score']))\n",
    "        \n",
    "        print(f\"‚úÖ Mutual Information Selection ({task}):\")\n",
    "        print(f\"   Selected top {k} features:\")\n",
    "        for i, row in mi_df.head(k).iterrows():\n",
    "            print(f\"   {row['feature']:30s} MI = {row['mi_score']:.4f}\")\n",
    "        \n",
    "        # Keep selected features + target + non-numeric\n",
    "        non_numeric_cols = [col for col in df.columns if col not in X.columns and col != target_col]\n",
    "        return df[non_numeric_cols + self.selected_features + [target_col]]\n",
    "    \n",
    "    def select_by_rfe(self, df, target_col, n_features=10, task='classification'):\n",
    "        \"\"\"\n",
    "        Recursive Feature Elimination using Random Forest.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            Input data\n",
    "        target_col : str\n",
    "            Target column name\n",
    "        n_features : int\n",
    "            Number of features to select\n",
    "        task : str\n",
    "            'classification' or 'regression'\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with selected features + target\n",
    "        \"\"\"\n",
    "        # Separate features and target\n",
    "        feature_cols = [col for col in df.columns if col != target_col]\n",
    "        X = df[feature_cols].select_dtypes(include=[np.number])\n",
    "        y = df[target_col]\n",
    "        \n",
    "        # Choose estimator based on task\n",
    "        if task == 'classification':\n",
    "            estimator = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "        else:\n",
    "            estimator = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "        \n",
    "        # Perform RFE\n",
    "        rfe = RFE(estimator=estimator, n_features_to_select=n_features, step=1)\n",
    "        rfe.fit(X, y)\n",
    "        \n",
    "        # Get selected features\n",
    "        self.selected_features = X.columns[rfe.support_].tolist()\n",
    "        \n",
    "        # Get feature rankings\n",
    "        rankings = dict(zip(X.columns, rfe.ranking_))\n",
    "        self.feature_scores = rankings\n",
    "        \n",
    "        print(f\"‚úÖ Recursive Feature Elimination ({task}):\")\n",
    "        print(f\"   Selected {n_features} features:\")\n",
    "        for feat in self.selected_features:\n",
    "            print(f\"   {feat}\")\n",
    "        \n",
    "        # Keep selected features + target + non-numeric\n",
    "        non_numeric_cols = [col for col in df.columns if col not in X.columns and col != target_col]\n",
    "        return df[non_numeric_cols + self.selected_features + [target_col]]\n",
    "    \n",
    "    def select_by_importance(self, df, target_col, threshold=0.01, task='classification'):\n",
    "        \"\"\"\n",
    "        Select features by Random Forest feature importance.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            Input data\n",
    "        target_col : str\n",
    "            Target column name\n",
    "        threshold : float\n",
    "            Minimum importance threshold\n",
    "        task : str\n",
    "            'classification' or 'regression'\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with important features + target\n",
    "        \"\"\"\n",
    "        # Separate features and target\n",
    "        feature_cols = [col for col in df.columns if col != target_col]\n",
    "        X = df[feature_cols].select_dtypes(include=[np.number])\n",
    "        y = df[target_col]\n",
    "        \n",
    "        # Train Random Forest\n",
    "        if task == 'classification':\n",
    "            model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        else:\n",
    "            model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        \n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Select features above threshold\n",
    "        self.selected_features = importances[importances['importance'] > threshold]['feature'].tolist()\n",
    "        self.feature_scores = dict(zip(importances['feature'], importances['importance']))\n",
    "        \n",
    "        print(f\"‚úÖ Feature Importance Selection ({task}):\")\n",
    "        print(f\"   Threshold: {threshold}\")\n",
    "        print(f\"   Selected {len(self.selected_features)} features:\")\n",
    "        for i, row in importances[importances['importance'] > threshold].iterrows():\n",
    "            print(f\"   {row['feature']:30s} Importance = {row['importance']:.4f}\")\n",
    "        \n",
    "        # Keep selected features + target + non-numeric\n",
    "        non_numeric_cols = [col for col in df.columns if col not in X.columns and col != target_col]\n",
    "        return df[non_numeric_cols + self.selected_features + [target_col]]\n",
    "    \n",
    "    def visualize_feature_scores(self, top_n=20):\n",
    "        \"\"\"\n",
    "        Visualize feature scores/rankings.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        top_n : int\n",
    "            Number of top features to display\n",
    "        \"\"\"\n",
    "        if not self.feature_scores:\n",
    "            print(\"‚ùå No feature scores available. Run a selection method first.\")\n",
    "            return\n",
    "        \n",
    "        # Sort by score\n",
    "        scores_df = pd.DataFrame(list(self.feature_scores.items()), \n",
    "                                columns=['feature', 'score'])\n",
    "        scores_df = scores_df.sort_values('score', ascending=False).head(top_n)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.barh(range(len(scores_df)), scores_df['score'], color='steelblue')\n",
    "        plt.yticks(range(len(scores_df)), scores_df['feature'])\n",
    "        plt.xlabel('Score')\n",
    "        plt.ylabel('Features')\n",
    "        plt.title(f'Top {top_n} Features by {self.method.capitalize()} Method')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"‚úÖ FeatureSelector class implemented\")\n",
    "print(\"\\nSupported Selection Methods:\")\n",
    "print(\"- select_by_variance(): Remove low-variance features\")\n",
    "print(\"- select_by_correlation(): Remove highly correlated features\")\n",
    "print(\"- select_by_mutual_info(): Select top k by mutual information\")\n",
    "print(\"- select_by_rfe(): Recursive Feature Elimination\")\n",
    "print(\"- select_by_importance(): Select by Random Forest importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff2e3b9",
   "metadata": {},
   "source": [
    "## üî¨ Complete Example: Semiconductor Device Yield Prediction\n",
    "\n",
    "### Scenario\n",
    "\n",
    "**Problem:** Predict device yield (pass/fail) from parametric test data.\n",
    "\n",
    "**Dataset:**\n",
    "- **2000 semiconductor devices** tested across **20 wafers**\n",
    "- **Raw Test Parameters:**\n",
    "  - VDD (voltage): 0.8-1.2V\n",
    "  - IDD (current): 10-100 mA\n",
    "  - Frequency: 1000-3000 MHz\n",
    "  - Power: 0.5-5 W\n",
    "  - Temperature: 25-85¬∞C\n",
    "- **Spatial Information:**\n",
    "  - die_x, die_y: Position on wafer (-10 to +10 mm from center)\n",
    "  - wafer_id: Which wafer (1-20)\n",
    "- **Target:** yield (0 = fail, 1 = pass)\n",
    "\n",
    "**Challenges:**\n",
    "- **Missing Data:** ~10% missing values per feature\n",
    "- **Spatial Effects:** Dies near wafer edge fail more often\n",
    "- **Process Variation:** Different wafers have different characteristics\n",
    "- **Imbalanced:** Only ~15% fail rate\n",
    "\n",
    "**Feature Engineering Strategy:**\n",
    "1. Handle missing data (median imputation)\n",
    "2. Create domain-specific features (efficiency, power density, leakage)\n",
    "3. Create spatial features (radial distance, quadrant, edge indicator)\n",
    "4. Create statistical features (wafer-level aggregations, deviations)\n",
    "5. Scale numeric features (StandardScaler)\n",
    "6. Select important features (correlation + feature importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db90db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Semiconductor Example - Part 1: Data Generation and Initial Engineering\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic semiconductor test data\n",
    "n_samples = 2000\n",
    "n_wafers = 20\n",
    "\n",
    "# Wafer assignment\n",
    "wafer_ids = np.random.randint(1, n_wafers + 1, n_samples)\n",
    "\n",
    "# Die positions on wafer (-10 to +10 mm from center)\n",
    "die_x = np.random.uniform(-10, 10, n_samples)\n",
    "die_y = np.random.uniform(-10, 10, n_samples)\n",
    "\n",
    "# Radial distance from wafer center\n",
    "radial_distance = np.sqrt(die_x**2 + die_y**2)\n",
    "wafer_radius = 10.0\n",
    "\n",
    "# Spatial effect: dies near edge have lower performance\n",
    "spatial_effect = radial_distance / wafer_radius  # 0 at center, 1 at edge\n",
    "\n",
    "# Base test parameters with realistic distributions\n",
    "vdd_base = np.random.normal(1.0, 0.05, n_samples)  # 1.0V ¬± 50mV\n",
    "idd_base = np.random.normal(50, 10, n_samples)      # 50mA ¬± 10mA\n",
    "freq_base = np.random.normal(2000, 200, n_samples)  # 2000MHz ¬± 200MHz\n",
    "temp_base = np.random.normal(55, 15, n_samples)     # 55¬∞C ¬± 15¬∞C\n",
    "\n",
    "# Apply spatial effects (edge dies have worse parameters)\n",
    "vdd = vdd_base - 0.05 * spatial_effect * np.random.randn(n_samples)\n",
    "idd = idd_base * (1 + 0.2 * spatial_effect + 0.1 * np.random.randn(n_samples))\n",
    "freq = freq_base * (1 - 0.1 * spatial_effect + 0.05 * np.random.randn(n_samples))\n",
    "temp = temp_base + 5 * spatial_effect + 2 * np.random.randn(n_samples)\n",
    "\n",
    "# Power calculation (realistic relationship)\n",
    "power = vdd * idd / 1000 + 0.5 * (freq / 1000)**2 / 1000\n",
    "\n",
    "# Wafer-level process variation\n",
    "wafer_means = np.random.normal(0, 0.5, n_wafers)\n",
    "for i in range(n_wafers):\n",
    "    wafer_mask = wafer_ids == (i + 1)\n",
    "    vdd[wafer_mask] += wafer_means[i] * 0.02\n",
    "    idd[wafer_mask] *= (1 + wafer_means[i] * 0.05)\n",
    "\n",
    "# Yield determination (based on multiple factors)\n",
    "# Fail if: edge location + high power + low freq + high temp + wafer variation\n",
    "fail_score = (\n",
    "    0.3 * spatial_effect +                    # Edge effect\n",
    "    0.2 * (power - power.mean()) / power.std() +  # High power bad\n",
    "    0.2 * (temp - temp.mean()) / temp.std() +     # High temp bad\n",
    "    0.15 * (freq.mean() - freq) / freq.std() +    # Low freq bad\n",
    "    0.15 * (idd - idd.mean()) / idd.std()         # High current bad\n",
    ")\n",
    "\n",
    "# Convert to binary with ~15% failure rate\n",
    "fail_threshold = np.percentile(fail_score, 85)\n",
    "yield_binary = (fail_score < fail_threshold).astype(int)\n",
    "\n",
    "# Create DataFrame\n",
    "df_raw = pd.DataFrame({\n",
    "    'wafer_id': wafer_ids,\n",
    "    'die_x': die_x,\n",
    "    'die_y': die_y,\n",
    "    'vdd': vdd,\n",
    "    'idd': idd,\n",
    "    'freq': freq,\n",
    "    'power': power,\n",
    "    'temp': temp,\n",
    "    'yield': yield_binary\n",
    "})\n",
    "\n",
    "# Inject missing values (~10% per feature)\n",
    "missing_rate = 0.10\n",
    "for col in ['vdd', 'idd', 'freq', 'power', 'temp']:\n",
    "    missing_indices = np.random.choice(df_raw.index, size=int(len(df_raw) * missing_rate), replace=False)\n",
    "    df_raw.loc[missing_indices, col] = np.nan\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SEMICONDUCTOR DEVICE YIELD PREDICTION - DATASET\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Dataset Shape: {df_raw.shape}\")\n",
    "print(f\"   Samples: {len(df_raw)}\")\n",
    "print(f\"   Features: {len(df_raw.columns) - 1} (excluding target)\")\n",
    "print(f\"   Target: 'yield' (binary)\")\n",
    "\n",
    "print(f\"\\nüìà Target Distribution:\")\n",
    "print(f\"   Pass (1): {(df_raw['yield'] == 1).sum()} ({(df_raw['yield'] == 1).sum() / len(df_raw) * 100:.1f}%)\")\n",
    "print(f\"   Fail (0): {(df_raw['yield'] == 0).sum()} ({(df_raw['yield'] == 0).sum() / len(df_raw) * 100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚ùì Missing Data:\")\n",
    "for col in ['vdd', 'idd', 'freq', 'power', 'temp']:\n",
    "    missing_count = df_raw[col].isnull().sum()\n",
    "    missing_pct = (missing_count / len(df_raw)) * 100\n",
    "    print(f\"   {col:10s}: {missing_count:4d} missing ({missing_pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìè Feature Statistics (before engineering):\")\n",
    "print(df_raw.describe())\n",
    "\n",
    "# Step 1: Handle Missing Data\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: HANDLE MISSING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "handler = MissingDataHandler(strategy='median')\n",
    "df_imputed = handler.impute_simple(df_raw, strategy='median')\n",
    "\n",
    "print(f\"\\n‚úÖ Imputation Complete:\")\n",
    "print(f\"   Strategy: Median\")\n",
    "print(f\"   Fill Values Used:\")\n",
    "for col, value in handler.fill_values.items():\n",
    "    print(f\"   {col:10s}: {value:.4f}\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(f\"\\n‚úÖ Verification: {df_imputed.isnull().sum().sum()} missing values remain\")\n",
    "\n",
    "# Step 2: Create Domain-Specific Features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: CREATE DOMAIN-SPECIFIC FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "creator = FeatureCreator()\n",
    "\n",
    "# Define domain recipes based on semiconductor physics\n",
    "domain_recipes = {\n",
    "    # Power efficiency: performance per watt\n",
    "    'efficiency': lambda x: x['freq'] / (x['power'] + 1e-10),\n",
    "    \n",
    "    # Power density: normalized by voltage\n",
    "    'power_density': lambda x: x['power'] / (x['vdd'] + 1e-10),\n",
    "    \n",
    "    # Leakage normalized: current per voltage (like resistance)\n",
    "    'leakage_normalized': lambda x: x['idd'] / (x['vdd'] + 1e-10),\n",
    "    \n",
    "    # Temperature coefficient: performance vs temperature\n",
    "    'temp_coefficient': lambda x: x['freq'] / (x['temp'] + 1e-10),\n",
    "    \n",
    "    # Dynamic power factor: VDD^2 * Freq (proportional to dynamic power)\n",
    "    'dynamic_power_factor': lambda x: (x['vdd'] ** 2) * x['freq'],\n",
    "}\n",
    "\n",
    "df_domain = creator.create_domain_features(df_imputed, domain_recipes)\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(domain_recipes)} domain-specific features\")\n",
    "print(f\"   New features: {list(domain_recipes.keys())}\")\n",
    "\n",
    "# Step 3: Create Spatial Features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: CREATE SPATIAL FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Radial distance from wafer center\n",
    "df_domain['radial_distance'] = np.sqrt(df_domain['die_x']**2 + df_domain['die_y']**2)\n",
    "\n",
    "# Quadrant (1-4 based on signs of x and y)\n",
    "df_domain['quadrant'] = 1  # Default\n",
    "df_domain.loc[(df_domain['die_x'] >= 0) & (df_domain['die_y'] >= 0), 'quadrant'] = 1\n",
    "df_domain.loc[(df_domain['die_x'] < 0) & (df_domain['die_y'] >= 0), 'quadrant'] = 2\n",
    "df_domain.loc[(df_domain['die_x'] < 0) & (df_domain['die_y'] < 0), 'quadrant'] = 3\n",
    "df_domain.loc[(df_domain['die_x'] >= 0) & (df_domain['die_y'] < 0), 'quadrant'] = 4\n",
    "\n",
    "# Edge indicator (1 if within 2mm of edge)\n",
    "edge_threshold = 8.0  # mm from center\n",
    "df_domain['is_edge_die'] = (df_domain['radial_distance'] > edge_threshold).astype(int)\n",
    "\n",
    "print(f\"‚úÖ Created 3 spatial features:\")\n",
    "print(f\"   - radial_distance: {df_domain['radial_distance'].min():.2f} to {df_domain['radial_distance'].max():.2f} mm\")\n",
    "print(f\"   - quadrant: {df_domain['quadrant'].nunique()} categories\")\n",
    "print(f\"   - is_edge_die: {df_domain['is_edge_die'].sum()} edge dies ({df_domain['is_edge_die'].sum()/len(df_domain)*100:.1f}%)\")\n",
    "\n",
    "# Step 4: Create Statistical Aggregation Features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: CREATE WAFER-LEVEL AGGREGATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Aggregate by wafer_id\n",
    "agg_cols = ['vdd', 'idd', 'freq', 'power', 'temp']\n",
    "df_agg = creator.create_aggregation_features(df_domain, 'wafer_id', agg_cols, agg_funcs=['mean', 'std'])\n",
    "\n",
    "print(f\"‚úÖ Created wafer-level aggregations:\")\n",
    "print(f\"   Grouped by: wafer_id\")\n",
    "print(f\"   Aggregated columns: {agg_cols}\")\n",
    "print(f\"   Functions: mean, std (+ deviations)\")\n",
    "print(f\"   Total new features: {len(agg_cols) * 2 + len(agg_cols)} per wafer\")\n",
    "\n",
    "print(f\"\\nüìä Current Feature Count: {len(df_agg.columns)} columns\")\n",
    "print(f\"   Raw features: {len(df_raw.columns)}\")\n",
    "print(f\"   Engineered: {len(df_agg.columns) - len(df_raw.columns)}\")\n",
    "\n",
    "# Save for next cell\n",
    "df_engineered = df_agg.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b19f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Semiconductor Example - Part 2: Scaling, Selection, and Model Comparison\n",
    "\n",
    "# Step 5: Encode Categorical Features\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 5: ENCODE CATEGORICAL FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# One-hot encode quadrant\n",
    "encoder = CategoricalEncoder(method='onehot')\n",
    "df_encoded = encoder.fit_transform(df_engineered, categorical_cols=['quadrant'])\n",
    "\n",
    "print(f\"‚úÖ One-hot encoded 'quadrant' feature\")\n",
    "print(f\"   Original: 1 column with 4 categories\")\n",
    "print(f\"   Encoded: {len([c for c in df_encoded.columns if 'quadrant' in c])} binary columns\")\n",
    "\n",
    "# Step 6: Scale Numeric Features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: SCALE NUMERIC FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Separate features and target\n",
    "target_col = 'yield'\n",
    "feature_cols = [col for col in df_encoded.columns if col != target_col]\n",
    "X = df_encoded[feature_cols]\n",
    "y = df_encoded[target_col]\n",
    "\n",
    "# Identify numeric columns to scale\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Scale using StandardScaler\n",
    "scaler = FeatureScaler(method='standard')\n",
    "X_scaled = scaler.fit_transform(X[numeric_cols])\n",
    "\n",
    "# Combine scaled numeric with non-numeric (if any)\n",
    "non_numeric_cols = [col for col in X.columns if col not in numeric_cols]\n",
    "if non_numeric_cols:\n",
    "    X_final = pd.concat([X[non_numeric_cols], X_scaled], axis=1)\n",
    "else:\n",
    "    X_final = X_scaled\n",
    "\n",
    "print(f\"‚úÖ Scaled {len(numeric_cols)} numeric features using StandardScaler\")\n",
    "print(f\"   Mean ‚âà 0, Std ‚âà 1 for all features\")\n",
    "\n",
    "# Step 7: Feature Selection - Remove Highly Correlated Features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 7: FEATURE SELECTION - CORRELATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Combine X_final and y for correlation selection\n",
    "df_for_selection = pd.concat([X_final, y], axis=1)\n",
    "\n",
    "selector_corr = FeatureSelector(method='correlation')\n",
    "df_selected = selector_corr.select_by_correlation(df_for_selection, threshold=0.95, target_col=target_col)\n",
    "\n",
    "# Separate again\n",
    "X_selected = df_selected.drop(columns=[target_col])\n",
    "\n",
    "print(f\"\\nüìä Features after correlation selection: {len(X_selected.columns)}\")\n",
    "\n",
    "# Step 8: Feature Selection - Feature Importance\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 8: FEATURE SELECTION - IMPORTANCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Combine for importance selection\n",
    "df_for_importance = pd.concat([X_selected, y], axis=1)\n",
    "\n",
    "selector_importance = FeatureSelector(method='importance')\n",
    "df_final = selector_importance.select_by_importance(df_for_importance, target_col=target_col, \n",
    "                                                     threshold=0.01, task='classification')\n",
    "\n",
    "# Final feature set\n",
    "X_final_selected = df_final.drop(columns=[target_col])\n",
    "y_final = df_final[target_col]\n",
    "\n",
    "print(f\"\\n‚úÖ Final Feature Set: {len(X_final_selected.columns)} features\")\n",
    "print(f\"   Original: {len(df_raw.columns) - 1} raw features\")\n",
    "print(f\"   Engineered: {len(X_final.columns)} total features\")\n",
    "print(f\"   Selected: {len(X_final_selected.columns)} final features\")\n",
    "print(f\"   Reduction: {((len(X_final.columns) - len(X_final_selected.columns)) / len(X_final.columns) * 100):.1f}%\")\n",
    "\n",
    "# Step 9: Model Comparison - Before and After Feature Engineering\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 9: MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Prepare raw features (just basic scaling, no engineering)\n",
    "X_raw = df_imputed[['vdd', 'idd', 'freq', 'power', 'temp', 'die_x', 'die_y']]\n",
    "y_raw = df_imputed['yield']\n",
    "\n",
    "scaler_raw = FeatureScaler(method='standard')\n",
    "X_raw_scaled = scaler_raw.fit_transform(X_raw)\n",
    "\n",
    "# Split data - raw features\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
    "    X_raw_scaled, y_raw, test_size=0.3, random_state=42, stratify=y_raw\n",
    ")\n",
    "\n",
    "# Split data - engineered features\n",
    "X_train_eng, X_test_eng, y_train_eng, y_test_eng = train_test_split(\n",
    "    X_final_selected, y_final, test_size=0.3, random_state=42, stratify=y_final\n",
    ")\n",
    "\n",
    "print(f\"üìä Train/Test Split:\")\n",
    "print(f\"   Train: {len(X_train_raw)} samples (70%)\")\n",
    "print(f\"   Test: {len(X_test_raw)} samples (30%)\")\n",
    "\n",
    "# Train models on raw features\n",
    "print(f\"\\nüîπ Training on RAW features ({X_raw_scaled.shape[1]} features)...\")\n",
    "\n",
    "lr_raw = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_raw.fit(X_train_raw, y_train_raw)\n",
    "y_pred_lr_raw = lr_raw.predict(X_test_raw)\n",
    "y_prob_lr_raw = lr_raw.predict_proba(X_test_raw)[:, 1]\n",
    "\n",
    "rf_raw = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_raw.fit(X_train_raw, y_train_raw)\n",
    "y_pred_rf_raw = rf_raw.predict(X_test_raw)\n",
    "y_prob_rf_raw = rf_raw.predict_proba(X_test_raw)[:, 1]\n",
    "\n",
    "# Train models on engineered features\n",
    "print(f\"üîπ Training on ENGINEERED features ({X_final_selected.shape[1]} features)...\")\n",
    "\n",
    "lr_eng = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_eng.fit(X_train_eng, y_train_eng)\n",
    "y_pred_lr_eng = lr_eng.predict(X_test_eng)\n",
    "y_prob_lr_eng = lr_eng.predict_proba(X_test_eng)[:, 1]\n",
    "\n",
    "rf_eng = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_eng.fit(X_train_eng, y_train_eng)\n",
    "y_pred_rf_eng = rf_eng.predict(X_test_eng)\n",
    "y_prob_rf_eng = rf_eng.predict_proba(X_test_eng)[:, 1]\n",
    "\n",
    "# Compare Results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTS COMPARISON: RAW vs ENGINEERED FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression (Raw)', 'Random Forest (Raw)', \n",
    "              'Logistic Regression (Engineered)', 'Random Forest (Engineered)'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test_raw, y_pred_lr_raw),\n",
    "        accuracy_score(y_test_raw, y_pred_rf_raw),\n",
    "        accuracy_score(y_test_eng, y_pred_lr_eng),\n",
    "        accuracy_score(y_test_eng, y_pred_rf_eng)\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_test_raw, y_pred_lr_raw),\n",
    "        precision_score(y_test_raw, y_pred_rf_raw),\n",
    "        precision_score(y_test_eng, y_pred_lr_eng),\n",
    "        precision_score(y_test_eng, y_pred_rf_eng)\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_test_raw, y_pred_lr_raw),\n",
    "        recall_score(y_test_raw, y_pred_rf_raw),\n",
    "        recall_score(y_test_eng, y_pred_lr_eng),\n",
    "        recall_score(y_test_eng, y_pred_rf_eng)\n",
    "    ],\n",
    "    'F1': [\n",
    "        f1_score(y_test_raw, y_pred_lr_raw),\n",
    "        f1_score(y_test_raw, y_pred_rf_raw),\n",
    "        f1_score(y_test_eng, y_pred_lr_eng),\n",
    "        f1_score(y_test_eng, y_pred_rf_eng)\n",
    "    ],\n",
    "    'AUC-ROC': [\n",
    "        roc_auc_score(y_test_raw, y_prob_lr_raw),\n",
    "        roc_auc_score(y_test_raw, y_prob_rf_raw),\n",
    "        roc_auc_score(y_test_eng, y_prob_lr_eng),\n",
    "        roc_auc_score(y_test_eng, y_prob_rf_eng)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Calculate improvements\n",
    "lr_improvement = (results.loc[2, 'AUC-ROC'] - results.loc[0, 'AUC-ROC']) / results.loc[0, 'AUC-ROC'] * 100\n",
    "rf_improvement = (results.loc[3, 'AUC-ROC'] - results.loc[1, 'AUC-ROC']) / results.loc[1, 'AUC-ROC'] * 100\n",
    "\n",
    "print(f\"\\nüéØ Feature Engineering Impact:\")\n",
    "print(f\"   Logistic Regression: {lr_improvement:+.1f}% improvement in AUC-ROC\")\n",
    "print(f\"   Random Forest: {rf_improvement:+.1f}% improvement in AUC-ROC\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Feature importance from engineered model\n",
    "importances = pd.DataFrame({\n",
    "    'feature': X_final_selected.columns,\n",
    "    'importance': rf_eng.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(15)\n",
    "\n",
    "axes[0, 0].barh(range(len(importances)), importances['importance'], color='steelblue')\n",
    "axes[0, 0].set_yticks(range(len(importances)))\n",
    "axes[0, 0].set_yticklabels(importances['feature'], fontsize=8)\n",
    "axes[0, 0].set_xlabel('Importance')\n",
    "axes[0, 0].set_title('Top 15 Feature Importances (Engineered Model)')\n",
    "axes[0, 0].invert_yaxis()\n",
    "axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 2. ROC Curves Comparison\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr_lr_raw, tpr_lr_raw, _ = roc_curve(y_test_raw, y_prob_lr_raw)\n",
    "fpr_rf_raw, tpr_rf_raw, _ = roc_curve(y_test_raw, y_prob_rf_raw)\n",
    "fpr_lr_eng, tpr_lr_eng, _ = roc_curve(y_test_eng, y_prob_lr_eng)\n",
    "fpr_rf_eng, tpr_rf_eng, _ = roc_curve(y_test_eng, y_prob_rf_eng)\n",
    "\n",
    "axes[0, 1].plot(fpr_lr_raw, tpr_lr_raw, label=f'LR Raw (AUC={results.loc[0, \"AUC-ROC\"]:.3f})', linestyle='--', color='blue')\n",
    "axes[0, 1].plot(fpr_rf_raw, tpr_rf_raw, label=f'RF Raw (AUC={results.loc[1, \"AUC-ROC\"]:.3f})', linestyle='--', color='green')\n",
    "axes[0, 1].plot(fpr_lr_eng, tpr_lr_eng, label=f'LR Engineered (AUC={results.loc[2, \"AUC-ROC\"]:.3f})', color='blue', linewidth=2)\n",
    "axes[0, 1].plot(fpr_rf_eng, tpr_rf_eng, label=f'RF Engineered (AUC={results.loc[3, \"AUC-ROC\"]:.3f})', color='green', linewidth=2)\n",
    "axes[0, 1].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "axes[0, 1].set_xlabel('False Positive Rate')\n",
    "axes[0, 1].set_ylabel('True Positive Rate')\n",
    "axes[0, 1].set_title('ROC Curves: Raw vs Engineered Features')\n",
    "axes[0, 1].legend(fontsize=8)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Model performance comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC-ROC']\n",
    "raw_lr = results.loc[0, metrics].values\n",
    "eng_lr = results.loc[2, metrics].values\n",
    "raw_rf = results.loc[1, metrics].values\n",
    "eng_rf = results.loc[3, metrics].values\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.2\n",
    "\n",
    "axes[1, 0].bar(x - width*1.5, raw_lr, width, label='LR Raw', color='lightblue')\n",
    "axes[1, 0].bar(x - width/2, eng_lr, width, label='LR Engineered', color='blue')\n",
    "axes[1, 0].bar(x + width/2, raw_rf, width, label='RF Raw', color='lightgreen')\n",
    "axes[1, 0].bar(x + width*1.5, eng_rf, width, label='RF Engineered', color='green')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(metrics, rotation=45)\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].set_title('Metric Comparison: Raw vs Engineered')\n",
    "axes[1, 0].legend(fontsize=8)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "\n",
    "# 4. Feature engineering pipeline summary\n",
    "pipeline_text = f\"\"\"Feature Engineering Pipeline Summary\n",
    "\n",
    "Raw Features: {len(df_raw.columns) - 1}\n",
    "‚îú‚îÄ Parametric Tests: 5 (VDD, IDD, Freq, Power, Temp)\n",
    "‚îî‚îÄ Spatial: 2 (die_x, die_y)\n",
    "\n",
    "After Engineering: {len(X_final.columns)}\n",
    "‚îú‚îÄ Missing Data: Median imputation\n",
    "‚îú‚îÄ Domain Features: +{len(domain_recipes)} (efficiency, power_density, etc.)\n",
    "‚îú‚îÄ Spatial Features: +3 (radial_distance, quadrant, edge)\n",
    "‚îú‚îÄ Aggregations: +{len(agg_cols)*3} (wafer mean/std/deviation)\n",
    "‚îî‚îÄ Encoding: quadrant one-hot\n",
    "\n",
    "After Selection: {len(X_final_selected.columns)}\n",
    "‚îú‚îÄ Correlation Filter: removed {len(X_final.columns) - len(X_selected.columns)} redundant\n",
    "‚îî‚îÄ Importance Filter: kept {len(X_final_selected.columns)} most important\n",
    "\n",
    "Performance Gain:\n",
    "‚îú‚îÄ Logistic Regression: {lr_improvement:+.1f}% AUC improvement\n",
    "‚îî‚îÄ Random Forest: {rf_improvement:+.1f}% AUC improvement\n",
    "\"\"\"\n",
    "\n",
    "axes[1, 1].text(0.05, 0.95, pipeline_text, transform=axes[1, 1].transAxes,\n",
    "                fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Feature Engineering Complete!\")\n",
    "print(f\"   Time saved in production: Feature engineering pipeline automates complex transformations\")\n",
    "print(f\"   Model improvement: {max(lr_improvement, rf_improvement):.1f}% better predictions\")\n",
    "print(f\"   Business impact: Fewer false negatives ‚Üí Reduced yield loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb0864",
   "metadata": {},
   "source": [
    "## üöÄ Real-World Project Ideas\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "#### 1. Automated Feature Discovery Engine ($20M+ Yield Improvement)\n",
    "**Objective:** Build system that automatically discovers optimal features for yield prediction across different device types.\n",
    "\n",
    "**Key Features:**\n",
    "- Automated missing data strategy selection\n",
    "- Domain-agnostic feature generation (polynomial, interactions)\n",
    "- Genetic algorithms for feature selection\n",
    "- Cross-validation to prevent overfitting\n",
    "\n",
    "**Success Metrics:**\n",
    "- 15-25% improvement in yield prediction accuracy\n",
    "- Reduces feature engineering time from weeks to hours\n",
    "- Generalizes across 10+ different product families\n",
    "- Deployed in production for real-time test optimization\n",
    "\n",
    "**Technologies:** Python, sklearn, TPOT (AutoML), Spark for scale\n",
    "\n",
    "**Business Impact:** $20M+ annual savings from reduced scrap and improved test coverage\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Cross-Product Feature Transfer Learning ($15M+ Faster Ramp)\n",
    "**Objective:** Transfer engineered features from mature products to new products to accelerate ramp-up.\n",
    "\n",
    "**Key Features:**\n",
    "- Feature importance transfer between similar devices\n",
    "- Domain adaptation techniques\n",
    "- Physics-based feature templates\n",
    "- Automated feature validation on new product data\n",
    "\n",
    "**Success Metrics:**\n",
    "- 40% faster time-to-market for new products\n",
    "- 60% reduction in feature engineering effort\n",
    "- 80% accuracy with <1000 samples (vs 5000+ normally)\n",
    "\n",
    "**Technologies:** Transfer learning, meta-learning, domain adaptation\n",
    "\n",
    "**Business Impact:** $15M+ revenue from faster product launches\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Spatial Feature Synthesizer for Wafer Analysis ($10M+ Equipment Savings)\n",
    "**Objective:** Create spatial features that reveal equipment-specific failure patterns on wafers.\n",
    "\n",
    "**Key Features:**\n",
    "- Radial, angular, and grid-based spatial encodings\n",
    "- Wafer map clustering and pattern recognition\n",
    "- Equipment signature extraction\n",
    "- Automated root cause correlation\n",
    "\n",
    "**Success Metrics:**\n",
    "- Identify 90%+ of equipment-related failures\n",
    "- Reduce equipment-related yield loss by 30%\n",
    "- Predict equipment maintenance needs 2 weeks in advance\n",
    "\n",
    "**Technologies:** Image processing, CNN for wafer maps, spatial statistics\n",
    "\n",
    "**Business Impact:** $10M+ savings from equipment optimization and reduced downtime\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Test Correlation Feature Engineering ($12M+ Test Time Reduction)\n",
    "**Objective:** Engineer features that capture test-to-test correlations to enable test reduction.\n",
    "\n",
    "**Key Features:**\n",
    "- Association rule mining between test failures\n",
    "- Conditional probability features (P(Test B fails | Test A fails))\n",
    "- Test clustering based on failure patterns\n",
    "- Sequential test ordering optimization\n",
    "\n",
    "**Success Metrics:**\n",
    "- Reduce test time by 30% without yield impact\n",
    "- Identify 50+ redundant tests\n",
    "- 99.5%+ coverage with reduced test set\n",
    "\n",
    "**Technologies:** Apriori algorithm, Bayesian networks, graph analytics\n",
    "\n",
    "**Business Impact:** $12M+ annual savings from reduced test time and equipment utilization\n",
    "\n",
    "---\n",
    "\n",
    "### General Machine Learning Projects\n",
    "\n",
    "#### 5. E-Commerce Customer Feature Pipeline ($50M+ Revenue)\n",
    "**Objective:** Build comprehensive feature engineering pipeline for customer lifetime value prediction.\n",
    "\n",
    "**Key Features:**\n",
    "- RFM features (Recency, Frequency, Monetary)\n",
    "- Behavioral sequences (click patterns, cart abandonment)\n",
    "- Temporal features (seasonality, trend, day-of-week)\n",
    "- Cross-sell propensity scores\n",
    "\n",
    "**Success Metrics:**\n",
    "- 25% improvement in CLV prediction accuracy\n",
    "- 40% increase in marketing campaign ROI\n",
    "- 15% reduction in churn through targeted retention\n",
    "\n",
    "**Technologies:** Spark for scale, Feature Store (Feast), A/B testing framework\n",
    "\n",
    "**Business Impact:** $50M+ revenue from optimized marketing spend\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Financial Fraud Detection Features ($100M+ Fraud Prevention)\n",
    "**Objective:** Engineer features for real-time credit card fraud detection.\n",
    "\n",
    "**Key Features:**\n",
    "- Velocity features (transactions per hour, amount per day)\n",
    "- Geographic anomalies (distance from previous transaction)\n",
    "- Merchant category patterns\n",
    "- Network features (connections between entities)\n",
    "\n",
    "**Success Metrics:**\n",
    "- 95%+ fraud detection rate\n",
    "- <0.1% false positive rate\n",
    "- <100ms prediction latency\n",
    "- Detect novel fraud patterns within 24 hours\n",
    "\n",
    "**Technologies:** Streaming (Kafka), graph databases (Neo4j), online learning\n",
    "\n",
    "**Business Impact:** $100M+ annual fraud prevention\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. Healthcare Risk Prediction Features ($200M+ Patient Safety)\n",
    "**Objective:** Engineer features for hospital readmission and complication prediction.\n",
    "\n",
    "**Key Features:**\n",
    "- Clinical history aggregations (prior admissions, diagnoses)\n",
    "- Medication interaction features\n",
    "- Lab trend features (slope, volatility, anomalies)\n",
    "- Social determinants of health proxies\n",
    "\n",
    "**Success Metrics:**\n",
    "- 30% reduction in preventable readmissions\n",
    "- 40% earlier detection of complications\n",
    "- Reduces false alarms by 50%\n",
    "- Works across 100+ hospitals\n",
    "\n",
    "**Technologies:** FHIR standard, privacy-preserving ML, causal inference\n",
    "\n",
    "**Business Impact:** $200M+ healthcare cost savings, improved patient outcomes\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. IoT Sensor Feature Engineering ($80M+ Maintenance Savings)\n",
    "**Objective:** Engineer features from time-series sensor data for predictive maintenance.\n",
    "\n",
    "**Key Features:**\n",
    "- Statistical features (rolling mean, std, percentiles)\n",
    "- Frequency domain features (FFT components, spectral entropy)\n",
    "- Change point detection features\n",
    "- Multi-sensor correlation features\n",
    "\n",
    "**Success Metrics:**\n",
    "- Predict failures 2 weeks in advance (90% accuracy)\n",
    "- Reduce unplanned downtime by 50%\n",
    "- Extend equipment life by 20%\n",
    "- Optimize maintenance schedules\n",
    "\n",
    "**Technologies:** Time series analysis, wavelets, autoencoders, edge computing\n",
    "\n",
    "**Business Impact:** $80M+ annual savings from optimized maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dda9e8",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways and Best Practices\n",
    "\n",
    "### Critical Insights\n",
    "\n",
    "**1. Feature Engineering Impact:**\n",
    "- Can improve model performance by 20-50% or more\n",
    "- Often more impactful than algorithm selection\n",
    "- Requires domain knowledge AND data science expertise\n",
    "- Good features make simple models outperform complex models with poor features\n",
    "\n",
    "**2. The Feature Engineering Mindset:**\n",
    "```\n",
    "Raw Data ‚Üí Domain Understanding ‚Üí Feature Hypotheses ‚Üí Implementation ‚Üí Validation\n",
    "```\n",
    "Always ask:\n",
    "- What physical/business processes generate this data?\n",
    "- What relationships might exist between features?\n",
    "- What aggregations make sense for this domain?\n",
    "- What temporal patterns are relevant?\n",
    "\n",
    "### When to Use Each Technique\n",
    "\n",
    "| Technique | Use Case | When to Skip |\n",
    "|-----------|----------|-------------|\n",
    "| **Missing Data Imputation** | Always required | If <1% missing, consider dropping rows |\n",
    "| **Scaling** | Linear models, neural nets, distance-based | Tree-based models (optional) |\n",
    "| **One-Hot Encoding** | Nominal categories, <15 levels | High cardinality (>20 categories) |\n",
    "| **Target Encoding** | High cardinality categories | Risk of overfitting with small data |\n",
    "| **Polynomial Features** | Non-linear relationships in linear models | High dimensions already, tree models |\n",
    "| **Domain Features** | Always try if domain knowledge available | Purely exploratory analysis |\n",
    "| **Aggregations** | Group structure exists (wafer, customer) | No meaningful grouping |\n",
    "| **Variance Threshold** | Always as first step | After already doing feature selection |\n",
    "| **Correlation Filter** | Linear models, many features | Tree models (handles correlation) |\n",
    "| **Mutual Information** | Non-linear relationships | Limited compute, use correlation |\n",
    "| **RFE** | Small-medium feature sets | Large datasets (too slow) |\n",
    "\n",
    "### Common Pitfalls and How to Avoid Them\n",
    "\n",
    "**1. Data Leakage**\n",
    "- ‚ùå **Wrong:** Fit scaler/imputer on entire dataset\n",
    "- ‚úÖ **Right:** Fit only on training data, apply to test data\n",
    "- ‚ùå **Wrong:** Use target in feature creation (future information)\n",
    "- ‚úÖ **Right:** Only use information available at prediction time\n",
    "\n",
    "**2. Overfitting Through Feature Engineering**\n",
    "- ‚ùå **Wrong:** Create hundreds of features without validation\n",
    "- ‚úÖ **Right:** Use cross-validation, monitor validation performance\n",
    "- ‚ùå **Wrong:** Target encoding without regularization\n",
    "- ‚úÖ **Right:** Use smoothing or cross-validation for target encoding\n",
    "\n",
    "**3. Forgetting Feature Selection**\n",
    "- ‚ùå **Wrong:** Feed all engineered features to model\n",
    "- ‚úÖ **Right:** Remove redundant and low-importance features\n",
    "- Rule: If features > samples/10, definitely select features\n",
    "\n",
    "**4. Ignoring Computational Cost**\n",
    "- ‚ùå **Wrong:** Complex features that slow down production inference\n",
    "- ‚úÖ **Right:** Balance accuracy gain vs inference speed\n",
    "- Example: Real-time fraud detection needs <100ms, so limit feature complexity\n",
    "\n",
    "**5. Not Documenting Feature Logic**\n",
    "- ‚ùå **Wrong:** Complex lambda functions with no comments\n",
    "- ‚úÖ **Right:** Document domain rationale, units, expected ranges\n",
    "- Include: Feature name, formula, business meaning, valid range\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "**1. Feature Store:**\n",
    "```python\n",
    "# Store feature definitions for reuse\n",
    "feature_store = {\n",
    "    'efficiency': {\n",
    "        'formula': 'freq / power',\n",
    "        'description': 'Performance per watt',\n",
    "        'unit': 'MHz/W',\n",
    "        'valid_range': [200, 5000],\n",
    "        'created_date': '2024-01-15'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**2. Feature Monitoring:**\n",
    "- Track feature distributions over time\n",
    "- Alert if feature values shift (data drift)\n",
    "- Monitor missing value rates\n",
    "- Validate feature ranges\n",
    "\n",
    "**3. Versioning:**\n",
    "- Version your feature engineering code\n",
    "- Track which features were used for each model version\n",
    "- Enable rollback if features break\n",
    "\n",
    "**4. Scalability:**\n",
    "- Test feature engineering on production data volumes\n",
    "- Use vectorized operations (NumPy, Pandas)\n",
    "- Consider Spark/Dask for big data\n",
    "- Cache expensive features\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "**Starting a New Project?**\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[New Dataset] --> B{Understand Data}\n",
    "    B --> C[Handle Missing Data]\n",
    "    C --> D{Domain Knowledge?}\n",
    "    D -->|Yes| E[Create Domain Features]\n",
    "    D -->|No| F[Try Polynomial Features]\n",
    "    E --> G{Many Features?}\n",
    "    F --> G\n",
    "    G -->|Yes >50| H[Feature Selection]\n",
    "    G -->|No <50| I[Try All Features]\n",
    "    H --> J[Train Baseline Model]\n",
    "    I --> J\n",
    "    J --> K{Performance Good?}\n",
    "    K -->|No| L[Iterate: More Features or Better Selection]\n",
    "    K -->|Yes| M[Deploy to Production]\n",
    "    L --> C\n",
    "```\n",
    "\n",
    "### Recommended Workflow\n",
    "\n",
    "**Phase 1: Understand Data (Day 1)**\n",
    "1. Visualize distributions\n",
    "2. Identify missing patterns\n",
    "3. Check correlations\n",
    "4. Document domain knowledge\n",
    "\n",
    "**Phase 2: Basic Engineering (Day 2-3)**\n",
    "1. Handle missing data\n",
    "2. Scale numeric features\n",
    "3. Encode categorical features\n",
    "4. Create 5-10 domain features\n",
    "\n",
    "**Phase 3: Advanced Engineering (Day 4-7)**\n",
    "1. Try polynomial interactions\n",
    "2. Create aggregation features\n",
    "3. Test mathematical transforms\n",
    "4. Generate temporal features (if time series)\n",
    "\n",
    "**Phase 4: Selection & Validation (Day 8-10)**\n",
    "1. Remove low variance\n",
    "2. Remove high correlation\n",
    "3. Select by importance\n",
    "4. Cross-validate thoroughly\n",
    "\n",
    "**Phase 5: Production (Day 11+)**\n",
    "1. Document all features\n",
    "2. Create feature pipeline\n",
    "3. Test on production data\n",
    "4. Set up monitoring\n",
    "\n",
    "### Resources and Next Steps\n",
    "\n",
    "**Further Learning:**\n",
    "- **Books:** \"Feature Engineering for Machine Learning\" (Zheng & Casari)\n",
    "- **Courses:** Fast.ai (practical feature engineering)\n",
    "- **Libraries:** Featuretools (automated feature engineering), TPOT (AutoML)\n",
    "\n",
    "**Next Notebooks:**\n",
    "- **042:** Model Evaluation Metrics (how to measure improvement)\n",
    "- **043:** Cross-Validation Techniques (proper feature validation)\n",
    "- **044:** Hyperparameter Tuning (optimize after feature engineering)\n",
    "\n",
    "**Practice Projects:**\n",
    "- Kaggle competitions (force you to engineer features)\n",
    "- Real-world datasets from work (domain knowledge helps!)\n",
    "- Time series forecasting (temporal feature engineering)\n",
    "\n",
    "---\n",
    "\n",
    "## üèÅ Conclusion\n",
    "\n",
    "**Feature Engineering is Both Art and Science:**\n",
    "- **Art:** Requires intuition, domain knowledge, creativity\n",
    "- **Science:** Needs rigorous validation, statistical testing, documentation\n",
    "\n",
    "**Remember:**\n",
    "> \"Better data beats better algorithms. Better features beat better models.\"\n",
    "\n",
    "The best ML practitioners spend 60-70% of their time on feature engineering and data quality, and only 30-40% on model selection and tuning.\n",
    "\n",
    "**You've now learned:**\n",
    "- ‚úÖ Missing data handling (6 methods)\n",
    "- ‚úÖ Feature scaling (4 methods)\n",
    "- ‚úÖ Categorical encoding (5 methods)\n",
    "- ‚úÖ Feature creation (5 types)\n",
    "- ‚úÖ Feature selection (6 methods)\n",
    "- ‚úÖ End-to-end pipeline (semiconductor example)\n",
    "- ‚úÖ Production best practices\n",
    "\n",
    "**Go engineer some amazing features! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
