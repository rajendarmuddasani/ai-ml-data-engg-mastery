{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd73fa2",
   "metadata": {},
   "source": [
    "# 048: Model Deployment & Serving\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** Production ML system architecture (training, serving, monitoring)\n",
    "- **Implement** REST APIs with FastAPI for real-time model serving\n",
    "- **Build** Docker containers for reproducible deployments\n",
    "- **Deploy** Models to Kubernetes with auto-scaling and load balancing\n",
    "- **Monitor** Model performance, data drift, and system health in production\n",
    "\n",
    "## üìö What is Model Deployment?\n",
    "\n",
    "**Model Deployment** is the process of making trained ML models available for inference in production environments. It's the bridge between research (Jupyter notebooks) and real-world impact (serving 1M predictions/day at <100ms latency with 99.99% uptime).\n",
    "\n",
    "**Production ML Stack:**\n",
    "```\n",
    "Training Pipeline ‚Üí Model Registry ‚Üí Serving Infrastructure ‚Üí Monitoring\n",
    "   (offline)          (versioning)      (online inference)      (alerts)\n",
    "```\n",
    "\n",
    "**Why Model Deployment Matters?**\n",
    "- ‚úÖ **Business Value**: Models only create value when serving predictions (research ‚Üí revenue)\n",
    "- ‚úÖ **Scale**: Serve 1K-1M predictions/sec (Intel: 500K dies/day, <10ms per prediction)\n",
    "- ‚úÖ **Reliability**: 99.99% uptime required (NVIDIA: $100K/hour downtime cost)\n",
    "- ‚úÖ **Latency**: Real-time decisions (<100ms for user-facing, <10ms for embedded)\n",
    "- ‚úÖ **Monitoring**: Detect model degradation before business impact\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**1. Real-Time Defect Detection (Intel)**\n",
    "- **Input**: 512 test parameters per die from test equipment\n",
    "- **Output**: Pass/fail decision + confidence score in <10ms\n",
    "- **Value**: Screen 500K dies/day, 95% defect detection, $15M savings (reduced test escapes)\n",
    "\n",
    "**2. Model Serving Platform (NVIDIA)**\n",
    "- **Input**: Wafer map images + parametric data for quality prediction\n",
    "- **Output**: Yield forecast + failure mode classification\n",
    "- **Value**: Kubernetes deployment with auto-scaling, 100K predictions/day, 99.99% uptime, $8M savings\n",
    "\n",
    "**3. Edge Inference (AMD)**\n",
    "- **Input**: Sensor data from test equipment (temperature, power, timing)\n",
    "- **Output**: Anomaly detection on edge devices (no cloud latency)\n",
    "- **Value**: <1ms inference on FPGA/TPU, real-time monitoring, $5M savings\n",
    "\n",
    "**4. Multi-Model Orchestration (Qualcomm)**\n",
    "- **Input**: Test data requiring 5 different models (yield, bin prediction, outlier detection, time-series forecast, root cause)\n",
    "- **Output**: Unified API serving all models with intelligent routing\n",
    "- **Value**: Centralized platform for 50+ models, 200K predictions/day, $12M savings\n",
    "\n",
    "## üîÑ Model Deployment Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Train Model<br/>Jupyter/Python] --> B[Validate Model<br/>Offline Metrics]\n",
    "    B --> C[Register Model<br/>MLflow/Registry]\n",
    "    C --> D[Package Model<br/>Docker Container]\n",
    "    D --> E[Deploy to K8s<br/>Auto-scaling]\n",
    "    E --> F[Serve Predictions<br/>REST API]\n",
    "    F --> G[Monitor<br/>Metrics/Alerts]\n",
    "    G --> H{Performance OK?}\n",
    "    H -->|No| A\n",
    "    H -->|Yes| F\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style C fill:#fff4e1\n",
    "    style E fill:#e1ffe1\n",
    "    style G fill:#ffe1e1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **010: Linear Regression** - Model training basics\n",
    "- **034: Neural Networks** - Deep learning models\n",
    "- **008: System Design** - Scalability, load balancing, microservices\n",
    "- **009: Git & Version Control** - CI/CD pipelines\n",
    "\n",
    "**Next Steps:**\n",
    "- **111: MLOps Fundamentals** - End-to-end ML pipelines\n",
    "- **131: Cloud Deployment** - AWS SageMaker, GCP Vertex AI, Azure ML\n",
    "- **151: Advanced MLOps** - Feature stores, experiment tracking, A/B testing\n",
    "\n",
    "---\n",
    "\n",
    "Let's deploy production ML systems! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dbd04b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: REST API with FastAPI\n",
    "\n",
    "### Why FastAPI for ML Serving?\n",
    "\n",
    "**FastAPI** is the modern Python framework for building high-performance ML APIs.\n",
    "\n",
    "**Advantages:**\n",
    "- ‚ö° **Performance**: Async I/O, ~3√ó faster than Flask (Intel: 10ms ‚Üí 3ms latency)\n",
    "- üìù **Auto-documentation**: Interactive API docs at `/docs` (Swagger UI)\n",
    "- ‚úÖ **Type Safety**: Pydantic validation catches errors before inference\n",
    "- üîÑ **Async Support**: Handle 1000+ concurrent requests (NVIDIA: 10K req/sec)\n",
    "- üéØ **Production-ready**: Built-in monitoring, health checks, dependency injection\n",
    "\n",
    "**Flask vs FastAPI:**\n",
    "| Feature | Flask | FastAPI |\n",
    "|---------|-------|---------|\n",
    "| **Performance** | Sync (WSGI) | Async (ASGI) 3√ó faster |\n",
    "| **Type Validation** | Manual | Automatic (Pydantic) |\n",
    "| **API Docs** | Manual (Swagger) | Auto-generated |\n",
    "| **Async** | ‚ùå (gevent workaround) | ‚úÖ Native |\n",
    "| **Learning Curve** | Easy | Moderate |\n",
    "\n",
    "---\n",
    "\n",
    "### FastAPI Model Serving Architecture\n",
    "\n",
    "**Intel Defect Detection API:**\n",
    "```\n",
    "Client Request (JSON with 512 test params)\n",
    "    ‚Üì\n",
    "FastAPI Endpoint (/predict)\n",
    "    ‚Üì\n",
    "Input Validation (Pydantic)\n",
    "    ‚Üì\n",
    "Preprocessing (normalize, handle missing)\n",
    "    ‚Üì\n",
    "Model Inference (loaded from disk/cache)\n",
    "    ‚Üì\n",
    "Postprocessing (threshold, confidence)\n",
    "    ‚Üì\n",
    "JSON Response (pass/fail, score, latency)\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "1. **Pydantic Models**: Define input/output schemas\n",
    "2. **Model Loading**: Load once at startup (not per request)\n",
    "3. **Health Check**: `/health` endpoint for K8s liveness/readiness\n",
    "4. **Monitoring**: Log latency, request count, errors\n",
    "5. **Error Handling**: Graceful failures with informative messages\n",
    "\n",
    "---\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "**1. Model Loading Strategy:**\n",
    "- ‚ùå **Bad**: Load model on every request (1s overhead)\n",
    "- ‚úÖ **Good**: Load model at startup, store in memory\n",
    "- ‚úÖ **Better**: Load on-demand with LRU cache (multi-model serving)\n",
    "\n",
    "**2. Batching:**\n",
    "- Single prediction: Simple but inefficient (10ms inference + 5ms overhead)\n",
    "- Dynamic batching: Accumulate requests for 10ms, batch infer (2ms per sample)\n",
    "- Intel: 10√ó throughput with dynamic batching\n",
    "\n",
    "**3. Async vs Sync:**\n",
    "- CPU-bound inference: Sync is fine (blocking operation)\n",
    "- I/O-bound (DB lookup, feature store): Use async (don't block)\n",
    "- NVIDIA: Async feature fetching while model loads\n",
    "\n",
    "**4. Resource Management:**\n",
    "- **CPU**: One worker per core (Intel: 32 cores ‚Üí 32 workers)\n",
    "- **GPU**: One model per GPU, batch requests (NVIDIA: RTX 4090, batch=32)\n",
    "- **Memory**: Monitor model size + request buffers (AMD: 8GB model + 2GB buffer)\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Targets\n",
    "\n",
    "**Latency (P99):**\n",
    "- User-facing: <100ms (recommendation systems)\n",
    "- Internal tools: <500ms (batch processing acceptable)\n",
    "- Real-time: <10ms (Intel wafer test, AMD edge devices)\n",
    "- Embedded: <1ms (FPGA/TPU accelerators)\n",
    "\n",
    "**Throughput:**\n",
    "- Small scale: 10-100 req/sec (single instance)\n",
    "- Medium scale: 1K-10K req/sec (horizontal scaling)\n",
    "- Large scale: 100K+ req/sec (NVIDIA: GPU batching + load balancer)\n",
    "\n",
    "**Availability:**\n",
    "- 99.9% (8.76 hours downtime/year) - Acceptable for internal tools\n",
    "- 99.99% (52 minutes downtime/year) - Production user-facing\n",
    "- 99.999% (5 minutes downtime/year) - Critical systems (Intel fab operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526a6ec",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Build production-ready FastAPI service for Intel defect detection model\n",
    "\n",
    "**Key Points:**\n",
    "- **Pydantic Models**: `TestData` validates 512 input parameters, `PredictionResponse` structures output\n",
    "- **Startup Event**: Load ML model once at startup (not per request for performance)\n",
    "- **Predict Endpoint**: Validates input ‚Üí preprocess ‚Üí model inference ‚Üí postprocess ‚Üí JSON response\n",
    "- **Health Check**: `/health` for Kubernetes liveness/readiness probes\n",
    "\n",
    "**Intel Application**: Test equipment sends 512 parametric measurements via HTTP POST to `/predict`. API returns pass/fail decision + confidence in <10ms. Handles 500K requests/day with 99.99% uptime.\n",
    "\n",
    "**Why This Matters:** FastAPI's async architecture + type safety enables high-throughput, reliable ML serving. $15M savings from catching defects in real-time during wafer test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca53d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI Model Serving Example\n",
    "# Run with: uvicorn main:app --reload --host 0.0.0.0 --port 8000\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Dict, Optional\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Pydantic models for request/response validation\n",
    "class TestData(BaseModel):\n",
    "    \"\"\"Input schema for die test parameters\"\"\"\n",
    "    die_id: str = Field(..., description=\"Unique die identifier\")\n",
    "    test_params: List[float] = Field(..., min_items=512, max_items=512, \n",
    "                                      description=\"512 parametric test measurements\")\n",
    "    \n",
    "    @validator('test_params')\n",
    "    def validate_params(cls, v):\n",
    "        # Check for NaN or infinite values\n",
    "        if any(np.isnan(v)) or any(np.isinf(v)):\n",
    "            raise ValueError(\"Test parameters contain NaN or infinite values\")\n",
    "        return v\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"die_id\": \"wafer123_die456\",\n",
    "                \"test_params\": [0.5] * 512  # Simplified example\n",
    "            }\n",
    "        }\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    \"\"\"Output schema for defect prediction\"\"\"\n",
    "    die_id: str\n",
    "    prediction: str  # \"pass\" or \"fail\"\n",
    "    confidence: float = Field(..., ge=0.0, le=1.0)\n",
    "    anomaly_score: float\n",
    "    inference_time_ms: float\n",
    "    timestamp: str\n",
    "    model_version: str\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    \"\"\"Health check response\"\"\"\n",
    "    status: str\n",
    "    model_loaded: bool\n",
    "    uptime_seconds: float\n",
    "    requests_served: int\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Intel Die Defect Detection API\",\n",
    "    description=\"Real-time defect detection for semiconductor wafer test\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Global state\n",
    "model = None\n",
    "model_version = \"v1.2.3\"\n",
    "start_time = time.time()\n",
    "request_count = 0\n",
    "\n",
    "# Simple mock model for demonstration\n",
    "class MockDefectDetector:\n",
    "    \"\"\"Placeholder for actual trained model (sklearn, PyTorch, etc.)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.threshold = 0.05\n",
    "        self.mean = np.random.randn(512) * 0.1 + 0.5\n",
    "        self.std = np.random.randn(512) * 0.1 + 0.1\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> Dict:\n",
    "        \"\"\"Compute anomaly score (reconstruction error)\"\"\"\n",
    "        # Simulate autoencoder reconstruction error\n",
    "        normalized = (X - self.mean) / (self.std + 1e-8)\n",
    "        anomaly_score = np.mean(normalized ** 2)\n",
    "        \n",
    "        prediction = \"fail\" if anomaly_score > self.threshold else \"pass\"\n",
    "        confidence = 1.0 - min(anomaly_score / (self.threshold * 2), 1.0)\n",
    "        \n",
    "        return {\n",
    "            \"prediction\": prediction,\n",
    "            \"confidence\": float(confidence),\n",
    "            \"anomaly_score\": float(anomaly_score)\n",
    "        }\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_model():\n",
    "    \"\"\"Load model at startup (once, not per request)\"\"\"\n",
    "    global model\n",
    "    logger.info(\"Loading defect detection model...\")\n",
    "    \n",
    "    # In production: load from model registry (MLflow, S3, etc.)\n",
    "    # model = joblib.load(\"model.pkl\")\n",
    "    # or: model = torch.load(\"model.pt\")\n",
    "    \n",
    "    model = MockDefectDetector()\n",
    "    logger.info(f\"Model loaded successfully - version {model_version}\")\n",
    "\n",
    "@app.get(\"/\", tags=[\"Root\"])\n",
    "async def root():\n",
    "    \"\"\"Root endpoint\"\"\"\n",
    "    return {\n",
    "        \"message\": \"Intel Die Defect Detection API\",\n",
    "        \"version\": model_version,\n",
    "        \"docs\": \"/docs\",\n",
    "        \"health\": \"/health\"\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\", response_model=HealthResponse, tags=[\"Health\"])\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint for Kubernetes liveness/readiness probes\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\" if model is not None else \"unhealthy\",\n",
    "        \"model_loaded\": model is not None,\n",
    "        \"uptime_seconds\": time.time() - start_time,\n",
    "        \"requests_served\": request_count\n",
    "    }\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse, tags=[\"Prediction\"])\n",
    "async def predict(data: TestData):\n",
    "    \"\"\"\n",
    "    Predict die defect status from test parameters\n",
    "    \n",
    "    - **die_id**: Unique identifier for the die\n",
    "    - **test_params**: 512 parametric measurements (voltage, current, timing, etc.)\n",
    "    \n",
    "    Returns pass/fail prediction with confidence and anomaly score\n",
    "    \"\"\"\n",
    "    global request_count\n",
    "    request_count += 1\n",
    "    \n",
    "    # Check if model is loaded\n",
    "    if model is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
    "    \n",
    "    # Start timer\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Convert to numpy array\n",
    "        X = np.array(data.test_params).reshape(1, -1)\n",
    "        \n",
    "        # Model inference\n",
    "        result = model.predict(X)\n",
    "        \n",
    "        # Calculate inference time\n",
    "        inference_time = (time.time() - start) * 1000  # Convert to ms\n",
    "        \n",
    "        # Build response\n",
    "        response = PredictionResponse(\n",
    "            die_id=data.die_id,\n",
    "            prediction=result[\"prediction\"],\n",
    "            confidence=result[\"confidence\"],\n",
    "            anomaly_score=result[\"anomaly_score\"],\n",
    "            inference_time_ms=round(inference_time, 2),\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            model_version=model_version\n",
    "        )\n",
    "        \n",
    "        # Log prediction\n",
    "        logger.info(f\"Predicted {data.die_id}: {result['prediction']} \"\n",
    "                   f\"(confidence={result['confidence']:.3f}, latency={inference_time:.2f}ms)\")\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction failed for {data.die_id}: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Prediction error: {str(e)}\")\n",
    "\n",
    "@app.post(\"/predict/batch\", tags=[\"Prediction\"])\n",
    "async def predict_batch(data: List[TestData]):\n",
    "    \"\"\"\n",
    "    Batch prediction for multiple dies (more efficient)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for sample in data:\n",
    "        result = await predict(sample)\n",
    "        results.append(result)\n",
    "    return {\"predictions\": results, \"count\": len(results)}\n",
    "\n",
    "# Demonstration: Simulate API usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"FASTAPI MODEL SERVING DEMONSTRATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Simulate model loading\n",
    "    print(\"\\nüîÑ Loading model...\")\n",
    "    model = MockDefectDetector()\n",
    "    print(\"‚úÖ Model loaded successfully\")\n",
    "    \n",
    "    # Simulate predictions\n",
    "    print(\"\\nüìä Simulating predictions:\")\n",
    "    \n",
    "    # Normal die\n",
    "    normal_die = {\n",
    "        \"die_id\": \"wafer001_die123\",\n",
    "        \"test_params\": (np.random.randn(512) * 0.1 + 0.5).tolist()\n",
    "    }\n",
    "    X_normal = np.array(normal_die[\"test_params\"]).reshape(1, -1)\n",
    "    result_normal = model.predict(X_normal)\n",
    "    print(f\"  Normal die: {result_normal['prediction']} (score={result_normal['anomaly_score']:.4f})\")\n",
    "    \n",
    "    # Defective die (anomalous pattern)\n",
    "    defective_die = {\n",
    "        \"die_id\": \"wafer001_die456\",\n",
    "        \"test_params\": (np.random.randn(512) * 0.5 + 0.8).tolist()\n",
    "    }\n",
    "    X_defective = np.array(defective_die[\"test_params\"]).reshape(1, -1)\n",
    "    result_defective = model.predict(X_defective)\n",
    "    print(f\"  Defective die: {result_defective['prediction']} (score={result_defective['anomaly_score']:.4f})\")\n",
    "    \n",
    "    print(\"\\nüì° API Ready:\")\n",
    "    print(\"  POST /predict - Single prediction\")\n",
    "    print(\"  POST /predict/batch - Batch prediction\")\n",
    "    print(\"  GET /health - Health check\")\n",
    "    print(\"  GET /docs - Interactive API documentation\")\n",
    "    \n",
    "    print(\"\\nüöÄ To run the API server:\")\n",
    "    print(\"  uvicorn main:app --reload --host 0.0.0.0 --port 8000\")\n",
    "    print(\"  Then visit: http://localhost:8000/docs\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Intel Production Stats:\")\n",
    "    print(\"  Throughput: 500K predictions/day (5.8 req/sec)\")\n",
    "    print(\"  Latency: <10ms P99 (target: <10ms)\")\n",
    "    print(\"  Uptime: 99.99% (52 minutes downtime/year)\")\n",
    "    print(\"  Business Value: $15M annual savings\")\n",
    "    \n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bedf140",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Docker Containerization\n",
    "\n",
    "### Why Docker for ML Models?\n",
    "\n",
    "**Docker** packages your model + dependencies + code into a portable container that runs identically anywhere.\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ **Reproducibility**: Works on dev laptop = works in production (no \"works on my machine\")\n",
    "- ‚úÖ **Isolation**: Dependencies don't conflict (TensorFlow 2.x + PyTorch 1.x in separate containers)\n",
    "- ‚úÖ **Portability**: Deploy to AWS, GCP, Azure, on-prem without changes\n",
    "- ‚úÖ **Versioning**: Tag images (`intel-defect-v1.2.3`), rollback in seconds\n",
    "- ‚úÖ **Scaling**: Kubernetes orchestrates thousands of containers\n",
    "\n",
    "---\n",
    "\n",
    "### Dockerfile Best Practices\n",
    "\n",
    "**NVIDIA Model Serving Dockerfile:**\n",
    "\n",
    "```dockerfile\n",
    "# Multi-stage build for smaller images\n",
    "FROM python:3.10-slim as base\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    build-essential \\\n",
    "    curl \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Create non-root user for security\n",
    "RUN useradd -m -u 1000 mluser\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy requirements first (Docker layer caching)\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY app/ ./app/\n",
    "COPY models/ ./models/\n",
    "\n",
    "# Change ownership to non-root user\n",
    "RUN chown -R mluser:mluser /app\n",
    "\n",
    "# Switch to non-root user\n",
    "USER mluser\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\n",
    "  CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Run application\n",
    "CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"4\"]\n",
    "```\n",
    "\n",
    "**Key Practices:**\n",
    "1. **Multi-stage builds**: Separate build dependencies from runtime (smaller image)\n",
    "2. **Layer caching**: Copy requirements.txt before code (faster rebuilds)\n",
    "3. **Non-root user**: Security best practice (mluser, not root)\n",
    "4. **Health check**: Docker knows if container is healthy\n",
    "5. **.dockerignore**: Exclude .git, __pycache__, *.ipynb (smaller context)\n",
    "\n",
    "---\n",
    "\n",
    "### Docker Commands Quick Reference\n",
    "\n",
    "```bash\n",
    "# Build image\n",
    "docker build -t intel-defect-api:v1.2.3 .\n",
    "\n",
    "# Run container locally\n",
    "docker run -d -p 8000:8000 --name defect-api intel-defect-api:v1.2.3\n",
    "\n",
    "# View logs\n",
    "docker logs -f defect-api\n",
    "\n",
    "# Execute command in container\n",
    "docker exec -it defect-api bash\n",
    "\n",
    "# Stop and remove\n",
    "docker stop defect-api && docker rm defect-api\n",
    "\n",
    "# Push to registry\n",
    "docker tag intel-defect-api:v1.2.3 registry.intel.com/ml/defect-api:v1.2.3\n",
    "docker push registry.intel.com/ml/defect-api:v1.2.3\n",
    "\n",
    "# Pull from registry\n",
    "docker pull registry.intel.com/ml/defect-api:v1.2.3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Image Optimization\n",
    "\n",
    "**Before Optimization (NVIDIA):**\n",
    "```\n",
    "Image size: 2.5GB\n",
    "Build time: 10 minutes\n",
    "Layers: 45\n",
    "```\n",
    "\n",
    "**Optimization Strategies:**\n",
    "1. **Use slim base images**: `python:3.10-slim` (200MB) vs `python:3.10` (1GB)\n",
    "2. **Multi-stage builds**: Discard build tools in final image\n",
    "3. **Combine RUN commands**: Each RUN creates a layer\n",
    "4. **Remove cache**: `pip install --no-cache-dir`, `apt-get clean`\n",
    "5. **Minimize layers**: Combine related operations\n",
    "\n",
    "**After Optimization:**\n",
    "```\n",
    "Image size: 800MB (68% reduction)\n",
    "Build time: 3 minutes (70% faster)\n",
    "Layers: 12 (73% fewer)\n",
    "```\n",
    "\n",
    "**NVIDIA Result:** Faster deployments (3 min vs 10 min), lower storage cost ($1K/month ‚Üí $320/month for 500 images).\n",
    "\n",
    "---\n",
    "\n",
    "### AMD Edge Deployment\n",
    "\n",
    "**Challenge:** Deploy model to test equipment with limited resources (4GB RAM, ARM CPU, no GPU).\n",
    "\n",
    "**Solution:** Optimize Docker image for edge devices.\n",
    "\n",
    "**Optimizations:**\n",
    "1. **Quantize model**: FP32 ‚Üí INT8 (4√ó smaller, 3√ó faster on ARM)\n",
    "2. **Model pruning**: Remove 50% of weights (minimal accuracy loss)\n",
    "3. **ARM-specific base image**: `arm64v8/python:3.10-slim`\n",
    "4. **ONNX Runtime**: 5√ó faster inference than PyTorch on CPU\n",
    "5. **Distillation**: Teacher model (large) ‚Üí Student model (small)\n",
    "\n",
    "**Results:**\n",
    "- Model size: 200MB ‚Üí 12MB (95% reduction)\n",
    "- Inference: 50ms ‚Üí 0.8ms (62√ó faster)\n",
    "- Memory: 2GB ‚Üí 150MB (93% reduction)\n",
    "- Fits on edge device with <1ms latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adada65",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Kubernetes Deployment\n",
    "\n",
    "### Why Kubernetes for ML Serving?\n",
    "\n",
    "**Kubernetes (K8s)** is the container orchestration platform for production ML systems.\n",
    "\n",
    "**Key Features:**\n",
    "- ‚ö° **Auto-scaling**: Scale from 2 to 100 pods based on CPU/memory/custom metrics\n",
    "- üîÑ **Load Balancing**: Distribute requests across pods automatically\n",
    "- üíö **Self-healing**: Restart failed pods, replace unhealthy instances\n",
    "- üöÄ **Rolling Updates**: Zero-downtime deployments (gradually replace old pods)\n",
    "- üìä **Resource Management**: CPU/memory requests & limits per pod\n",
    "- üîê **Secrets Management**: Securely store API keys, credentials\n",
    "\n",
    "---\n",
    "\n",
    "### Kubernetes Architecture for ML\n",
    "\n",
    "**NVIDIA Model Serving on K8s:**\n",
    "```\n",
    "                          Ingress (NGINX)\n",
    "                          Load Balancer\n",
    "                                 |\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚Üì            ‚Üì            ‚Üì\n",
    "            Service (ClusterIP)\n",
    "                    |\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚Üì               ‚Üì               ‚Üì\n",
    "  Pod 1           Pod 2           Pod 3\n",
    "  (API + Model)   (API + Model)   (API + Model)\n",
    "  2 CPU, 4GB      2 CPU, 4GB      2 CPU, 4GB\n",
    "  \n",
    "Horizontal Pod Autoscaler (HPA)\n",
    "Scale 2-20 pods based on CPU >70%\n",
    "```\n",
    "\n",
    "**Components:**\n",
    "1. **Deployment**: Defines desired state (3 replicas, resource limits)\n",
    "2. **Service**: Stable endpoint for pods (load balances requests)\n",
    "3. **Ingress**: External access via HTTPS with TLS\n",
    "4. **HPA**: Auto-scaling based on metrics\n",
    "5. **ConfigMap**: Configuration (model paths, thresholds)\n",
    "6. **Secret**: Credentials (model registry, database)\n",
    "\n",
    "---\n",
    "\n",
    "### Kubernetes Manifests\n",
    "\n",
    "**Intel Defect Detection Deployment:**\n",
    "\n",
    "```yaml\n",
    "# deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: defect-detection\n",
    "  namespace: ml-models\n",
    "spec:\n",
    "  replicas: 3  # Start with 3 pods\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: defect-detection\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: defect-detection\n",
    "        version: v1.2.3\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: api\n",
    "        image: registry.intel.com/ml/defect-api:v1.2.3\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"2Gi\"\n",
    "            cpu: \"1000m\"  # 1 CPU\n",
    "          limits:\n",
    "            memory: \"4Gi\"\n",
    "            cpu: \"2000m\"  # 2 CPUs\n",
    "        env:\n",
    "        - name: MODEL_PATH\n",
    "          value: \"/models/defect_v1.2.3.pkl\"\n",
    "        - name: THRESHOLD\n",
    "          value: \"0.05\"\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 10\n",
    "          periodSeconds: 5\n",
    "---\n",
    "# service.yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: defect-detection-svc\n",
    "  namespace: ml-models\n",
    "spec:\n",
    "  selector:\n",
    "    app: defect-detection\n",
    "  ports:\n",
    "  - protocol: TCP\n",
    "    port: 80\n",
    "    targetPort: 8000\n",
    "  type: ClusterIP\n",
    "---\n",
    "# hpa.yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: defect-detection-hpa\n",
    "  namespace: ml-models\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: defect-detection\n",
    "  minReplicas: 3\n",
    "  maxReplicas: 20\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  - type: Pods\n",
    "    pods:\n",
    "      metric:\n",
    "        name: http_requests_per_second\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: \"1000\"\n",
    "```\n",
    "\n",
    "**Deployment Commands:**\n",
    "```bash\n",
    "# Apply manifests\n",
    "kubectl apply -f deployment.yaml\n",
    "kubectl apply -f service.yaml\n",
    "kubectl apply -f hpa.yaml\n",
    "\n",
    "# Check status\n",
    "kubectl get pods -n ml-models\n",
    "kubectl get svc -n ml-models\n",
    "kubectl get hpa -n ml-models\n",
    "\n",
    "# View logs\n",
    "kubectl logs -f deployment/defect-detection -n ml-models\n",
    "\n",
    "# Scale manually\n",
    "kubectl scale deployment defect-detection --replicas=10 -n ml-models\n",
    "\n",
    "# Rolling update (zero downtime)\n",
    "kubectl set image deployment/defect-detection \\\n",
    "  api=registry.intel.com/ml/defect-api:v1.3.0 -n ml-models\n",
    "\n",
    "# Rollback\n",
    "kubectl rollout undo deployment/defect-detection -n ml-models\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Auto-Scaling Strategies\n",
    "\n",
    "**1. CPU-based (Simple):**\n",
    "- Scale when CPU >70% for 30 seconds\n",
    "- Intel: 3 pods ‚Üí 8 pods during peak hours (8am-6pm)\n",
    "\n",
    "**2. Memory-based:**\n",
    "- Scale when memory >80%\n",
    "- NVIDIA: Large models require memory management\n",
    "\n",
    "**3. Custom Metrics (Advanced):**\n",
    "- Request count: >1000 req/sec ‚Üí scale up\n",
    "- Latency: P99 >50ms ‚Üí scale up\n",
    "- Queue depth: >100 requests queued ‚Üí scale up\n",
    "- Qualcomm: Custom Prometheus metrics for queue depth\n",
    "\n",
    "**4. Scheduled Scaling:**\n",
    "- Predictable load patterns\n",
    "- Scale up at 7am (before production shift)\n",
    "- Scale down at 7pm (after hours)\n",
    "\n",
    "---\n",
    "\n",
    "### Qualcomm Multi-Model Serving\n",
    "\n",
    "**Challenge:** Serve 50 different models (yield, binning, outlier, forecast, etc.) efficiently.\n",
    "\n",
    "**Solution:** Multi-model deployment with intelligent routing.\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "API Gateway (single endpoint)\n",
    "    ‚Üì\n",
    "Routing Logic (based on model_id in request)\n",
    "    ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚Üì          ‚Üì          ‚Üì          ‚Üì          ‚Üì\n",
    "Yield      Bin        Outlier    Forecast   RCA\n",
    "Model      Model      Model      Model      Model\n",
    "(10 pods)  (5 pods)   (3 pods)   (8 pods)   (2 pods)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Resource optimization: Allocate pods based on usage\n",
    "- Fault isolation: One model fails, others continue\n",
    "- Independent scaling: Scale yield model without touching others\n",
    "- A/B testing: Route 10% traffic to new model version\n",
    "\n",
    "**Results:**\n",
    "- 50 models serving 200K predictions/day\n",
    "- 99.99% uptime (5 minutes downtime/month)\n",
    "- $12M savings (centralized platform, efficient resource usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975f95b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Monitoring & Observability\n",
    "\n",
    "### Why Monitor ML Models in Production?\n",
    "\n",
    "**Models degrade over time** due to data drift, concept drift, and system changes. Monitoring catches problems before they impact business.\n",
    "\n",
    "**What to Monitor:**\n",
    "1. **System Metrics**: Latency, throughput, error rate, CPU/memory\n",
    "2. **Model Metrics**: Accuracy, precision, recall, F1 (requires labels)\n",
    "3. **Data Drift**: Input distribution changes over time\n",
    "4. **Prediction Drift**: Output distribution changes\n",
    "5. **Business Metrics**: Revenue impact, user engagement\n",
    "\n",
    "---\n",
    "\n",
    "### Three Pillars of Observability\n",
    "\n",
    "**1. Metrics (Quantitative):**\n",
    "- Time-series data (latency, requests/sec, accuracy)\n",
    "- Aggregated: mean, P50, P95, P99\n",
    "- Tools: Prometheus, Grafana, CloudWatch\n",
    "\n",
    "**2. Logs (Qualitative):**\n",
    "- Structured events (prediction logs, errors, warnings)\n",
    "- Searchable, filterable\n",
    "- Tools: ELK stack (Elasticsearch, Logstash, Kibana), Splunk\n",
    "\n",
    "**3. Traces (Causal):**\n",
    "- Request flow through distributed system\n",
    "- Identify bottlenecks (DB query slow? Model inference slow?)\n",
    "- Tools: Jaeger, Zipkin, AWS X-Ray\n",
    "\n",
    "---\n",
    "\n",
    "### Prometheus + Grafana Stack\n",
    "\n",
    "**Intel Monitoring Architecture:**\n",
    "```\n",
    "FastAPI (expose /metrics)\n",
    "    ‚Üì\n",
    "Prometheus (scrape metrics every 15s)\n",
    "    ‚Üì\n",
    "Grafana (visualize dashboards)\n",
    "    ‚Üì\n",
    "AlertManager (send alerts to Slack/PagerDuty)\n",
    "```\n",
    "\n",
    "**Key Metrics to Track:**\n",
    "```python\n",
    "from prometheus_client import Counter, Histogram, Gauge\n",
    "\n",
    "# Request counters\n",
    "predictions_total = Counter(\n",
    "    'predictions_total', \n",
    "    'Total predictions',\n",
    "    ['model_version', 'prediction']\n",
    ")\n",
    "\n",
    "# Latency histogram\n",
    "prediction_latency = Histogram(\n",
    "    'prediction_latency_seconds',\n",
    "    'Prediction latency',\n",
    "    buckets=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    ")\n",
    "\n",
    "# Model accuracy (when labels arrive)\n",
    "model_accuracy = Gauge(\n",
    "    'model_accuracy',\n",
    "    'Model accuracy over last 1000 predictions'\n",
    ")\n",
    "\n",
    "# Anomaly score distribution\n",
    "anomaly_score = Histogram(\n",
    "    'anomaly_score',\n",
    "    'Anomaly scores',\n",
    "    buckets=[0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
    ")\n",
    "```\n",
    "\n",
    "**Intel Dashboard:**\n",
    "- Requests/sec: 5.8 (500K/day avg)\n",
    "- P99 latency: 8.3ms (target: <10ms)\n",
    "- Error rate: 0.02% (target: <0.1%)\n",
    "- Accuracy: 95.2% (baseline: 92%)\n",
    "\n",
    "---\n",
    "\n",
    "### Data Drift Detection\n",
    "\n",
    "**Problem:** Training data (2023) != Production data (2024). Model degrades silently.\n",
    "\n",
    "**AMD Sensor Drift Example:**\n",
    "- **Training**: Temperature sensors calibrated, range [20¬∞C, 80¬∞C]\n",
    "- **Production (6 months later)**: Sensors drift, range [22¬∞C, 85¬∞C]\n",
    "- **Impact**: Model accuracy 92% ‚Üí 87% (5% drop)\n",
    "\n",
    "**Detection Methods:**\n",
    "\n",
    "**1. Statistical Tests:**\n",
    "- **Kolmogorov-Smirnov test**: Compare distributions (p-value <0.05 ‚Üí drift)\n",
    "- **Population Stability Index (PSI)**: PSI >0.1 ‚Üí moderate drift, >0.25 ‚Üí severe drift\n",
    "\n",
    "**2. Domain Classifier:**\n",
    "- Train binary classifier: Training data (class 0) vs Production data (class 1)\n",
    "- Random performance (50% accuracy) ‚Üí no drift\n",
    "- High accuracy (>70%) ‚Üí significant drift\n",
    "\n",
    "**3. Feature-wise Monitoring:**\n",
    "- Track mean, std, min, max, percentiles for each feature\n",
    "- Alert if >2 std deviations from training statistics\n",
    "\n",
    "**NVIDIA Implementation:**\n",
    "```python\n",
    "# Compute PSI for feature\n",
    "def compute_psi(expected, actual, bins=10):\n",
    "    expected_percents = np.histogram(expected, bins=bins)[0] / len(expected)\n",
    "    actual_percents = np.histogram(actual, bins=bins)[0] / len(actual)\n",
    "    \n",
    "    psi = np.sum((actual_percents - expected_percents) * \n",
    "                 np.log(actual_percents / (expected_percents + 1e-10)))\n",
    "    return psi\n",
    "\n",
    "# Monitor daily\n",
    "for feature_idx in range(512):\n",
    "    psi = compute_psi(X_train[:, feature_idx], X_prod_today[:, feature_idx])\n",
    "    if psi > 0.25:\n",
    "        alert(f\"Severe drift detected in feature {feature_idx}: PSI={psi:.3f}\")\n",
    "```\n",
    "\n",
    "**NVIDIA Results:**\n",
    "- Detected drift 2 weeks before accuracy drop\n",
    "- Retrained model proactively\n",
    "- Maintained 99.5% accuracy (no degradation)\n",
    "\n",
    "---\n",
    "\n",
    "### Alert Strategy\n",
    "\n",
    "**Intel Alerting Rules:**\n",
    "\n",
    "**Critical (PagerDuty - immediate response):**\n",
    "- API down (health check fails for 2 minutes)\n",
    "- Error rate >1% for 5 minutes\n",
    "- P99 latency >50ms for 5 minutes\n",
    "- Model accuracy <85% (20% below baseline)\n",
    "\n",
    "**Warning (Slack - investigate within 4 hours):**\n",
    "- Error rate >0.1% for 15 minutes\n",
    "- P99 latency >20ms for 15 minutes\n",
    "- Request rate 2√ó above normal\n",
    "- Data drift PSI >0.25 for any feature\n",
    "\n",
    "**Info (Email - review daily):**\n",
    "- Model accuracy <90%\n",
    "- Request rate drops >50%\n",
    "- New error types appear\n",
    "\n",
    "**Qualcomm Alert Response:**\n",
    "1. **Investigate**: Check Grafana dashboard, read logs\n",
    "2. **Triage**: Determine root cause (data drift? system issue? model bug?)\n",
    "3. **Mitigate**: Rollback to previous version, scale up resources, or retrain\n",
    "4. **Post-mortem**: Document incident, update runbooks, improve monitoring\n",
    "\n",
    "---\n",
    "\n",
    "### Model Performance Tracking\n",
    "\n",
    "**Challenges:**\n",
    "- Ground truth labels arrive late (Intel: die pass/fail known after final test, 2 weeks later)\n",
    "- Can't wait 2 weeks to detect model degradation\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "**1. Proxy Metrics (Real-time):**\n",
    "- Confidence distribution (sudden drop ‚Üí model uncertain)\n",
    "- Anomaly score distribution (shift ‚Üí input pattern change)\n",
    "- Prediction distribution (more failures than usual?)\n",
    "\n",
    "**2. Sampling + Human Labeling:**\n",
    "- Sample 1% of predictions for immediate expert review\n",
    "- Intel: 50 dies/day reviewed by engineer (detect issues in 1 day, not 2 weeks)\n",
    "\n",
    "**3. A/B Testing:**\n",
    "- Route 10% traffic to new model (candidate)\n",
    "- Compare metrics: latency, confidence, anomaly scores\n",
    "- If candidate better, promote to 100%\n",
    "\n",
    "**4. Shadow Deployment:**\n",
    "- New model runs in parallel, doesn't affect production\n",
    "- Compare predictions: if >5% disagreement, investigate\n",
    "- Safe way to validate new models\n",
    "\n",
    "**NVIDIA Shadow Deployment:**\n",
    "- Deployed model v2.0 in shadow mode\n",
    "- Discovered 8% prediction disagreement with v1.5\n",
    "- Investigation: v2.0 overfitted to recent data\n",
    "- Decision: Keep v1.5 in production, retrain v2.0 with more diverse data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7652b90f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Real-World Projects\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "**1. End-to-End ML Platform (Intel)**\n",
    "- **Objective**: Production platform for 20+ ML models serving 1M predictions/day\n",
    "- **Architecture**:\n",
    "  - **Training Pipeline**: Airflow DAG (data prep ‚Üí train ‚Üí validate ‚Üí register)\n",
    "  - **Model Registry**: MLflow (version control, stage transitions, lineage)\n",
    "  - **Serving**: Kubernetes (3-20 pods per model, auto-scaling)\n",
    "  - **API Gateway**: NGINX Ingress with rate limiting, authentication\n",
    "  - **Monitoring**: Prometheus + Grafana + AlertManager\n",
    "  - **Logging**: ELK stack (Elasticsearch, Logstash, Kibana)\n",
    "  - **CI/CD**: GitHub Actions (test ‚Üí build Docker ‚Üí deploy to staging ‚Üí canary ‚Üí production)\n",
    "- **Key Features**:\n",
    "  - Multi-model serving with intelligent routing\n",
    "  - A/B testing framework (10-90 split, gradual rollout)\n",
    "  - Shadow deployment for safe validation\n",
    "  - Automated retraining on data drift (weekly schedule + on-demand)\n",
    "  - Feature store (Feast) for training/serving consistency\n",
    "- **Success Metrics**:\n",
    "  - 20 models deployed, 1M predictions/day\n",
    "  - 99.99% uptime (5 minutes downtime/month)\n",
    "  - <10ms P99 latency (target: <10ms)\n",
    "  - Zero manual deployments (fully automated CI/CD)\n",
    "  - Detect data drift 2 weeks early (proactive retraining)\n",
    "- **Business Value**: $25M annually (20 models √ó $1-2M each, automated operations, early drift detection)\n",
    "- **Implementation**: 12 months (platform design, infrastructure setup, migrate 20 models, train 50 engineers)\n",
    "\n",
    "---\n",
    "\n",
    "**2. Real-Time Edge Inference (AMD)**\n",
    "- **Objective**: Deploy anomaly detection to 500 test equipment units (ARM CPU, 4GB RAM, no cloud)\n",
    "- **Architecture**:\n",
    "  - **Model**: Quantized autoencoder (FP32 ‚Üí INT8, 200MB ‚Üí 12MB)\n",
    "  - **Runtime**: ONNX Runtime (optimized for ARM)\n",
    "  - **Container**: Docker (ARM64 base image, multi-stage build)\n",
    "  - **Orchestration**: K3s (lightweight Kubernetes for edge)\n",
    "  - **Update Mechanism**: GitOps (Fleet pulls updates from Git repo)\n",
    "  - **Monitoring**: Prometheus agent (ship metrics to central server)\n",
    "- **Key Features**:\n",
    "  - Over-the-air updates (deploy to 500 devices in 10 minutes)\n",
    "  - Offline operation (equipment isolated from internet for security)\n",
    "  - Local inference (<1ms latency, no cloud round-trip)\n",
    "  - Fallback model (if primary fails, use simpler rule-based)\n",
    "  - Gradual rollout (canary to 10 devices ‚Üí validate ‚Üí roll out to 500)\n",
    "- **Success Metrics**:\n",
    "  - <1ms inference latency (target: <5ms)\n",
    "  - 150MB memory footprint (fits in 4GB device)\n",
    "  - 99.9% uptime per device (remote monitoring + auto-restart)\n",
    "  - Update 500 devices in 10 minutes (was 2 weeks manual)\n",
    "  - Zero failed updates (atomic updates with rollback)\n",
    "- **Business Value**: $18M annually (real-time anomaly detection, eliminated cloud costs $500K/year, faster updates)\n",
    "- **Implementation**: 8 months (model optimization, K3s setup, GitOps pipeline, fleet management)\n",
    "\n",
    "---\n",
    "\n",
    "**3. Multi-Region Deployment (NVIDIA)**\n",
    "- **Objective**: Serve models globally with <100ms latency from any location\n",
    "- **Architecture**:\n",
    "  - **Regions**: 3 data centers (US-West, US-East, Asia)\n",
    "  - **Load Balancing**: GeoDNS routes to nearest region\n",
    "  - **Kubernetes**: EKS cluster per region (10-50 pods each)\n",
    "  - **Data Replication**: PostgreSQL primary-replica (read from nearest)\n",
    "  - **Model Sync**: S3 cross-region replication (models synced in <5 minutes)\n",
    "  - **Monitoring**: Centralized Grafana (aggregate metrics from all regions)\n",
    "- **Key Features**:\n",
    "  - Geo-routing (US users ‚Üí US cluster, Asia users ‚Üí Asia cluster)\n",
    "  - Failover (US-West down ‚Üí route to US-East automatically)\n",
    "  - Regional model caching (avoid cross-region model fetches)\n",
    "  - Data sovereignty compliance (EU data stays in EU)\n",
    "  - Disaster recovery (backup to different region, RTO <30 minutes)\n",
    "- **Success Metrics**:\n",
    "  - <100ms P99 latency globally (was 300ms single region)\n",
    "  - 99.995% availability (26 seconds downtime/month)\n",
    "  - 10K requests/sec globally (3K-4K per region)\n",
    "  - Zero data loss during region failure (replication lag <5s)\n",
    "  - $2M cost savings (avoid premium tier single-region solution)\n",
    "- **Business Value**: $15M annually (global expansion enabled, improved user experience, reduced latency)\n",
    "- **Implementation**: 6 months (multi-region setup, DR testing, traffic migration)\n",
    "\n",
    "---\n",
    "\n",
    "**4. Continuous Training Pipeline (Qualcomm)**\n",
    "- **Objective**: Automatically retrain models weekly using latest production data\n",
    "- **Architecture**:\n",
    "  - **Data Pipeline**: Kafka ‚Üí Spark Streaming ‚Üí Feature Store (Feast)\n",
    "  - **Training Orchestration**: Kubeflow Pipelines (DAG for train ‚Üí evaluate ‚Üí register ‚Üí deploy)\n",
    "  - **Compute**: Kubernetes with GPU nodes (train 10 models in parallel)\n",
    "  - **Model Registry**: MLflow (track experiments, lineage, staging)\n",
    "  - **Deployment**: Automated promotion (staging ‚Üí canary ‚Üí production)\n",
    "  - **Monitoring**: Track model performance, trigger retraining on drift\n",
    "- **Key Features**:\n",
    "  - Scheduled retraining (every Sunday 2am, low-traffic window)\n",
    "  - Data drift trigger (PSI >0.25 ‚Üí immediate retraining)\n",
    "  - Automated validation (accuracy >90% required for promotion)\n",
    "  - Rollback on failure (if new model worse, revert to previous)\n",
    "  - Experiment tracking (compare 1000+ training runs)\n",
    "- **Success Metrics**:\n",
    "  - Weekly retraining cycle (was monthly manual)\n",
    "  - 92% ‚Üí 95% accuracy (models adapt to recent data)\n",
    "  - Zero manual interventions (fully automated)\n",
    "  - 3 hours training time (parallel GPU training)\n",
    "  - $500K ML engineer time saved (no manual retraining)\n",
    "- **Business Value**: $20M annually (higher accuracy = better decisions, automation saves $500K, faster adaptation to changes)\n",
    "- **Implementation**: 5 months (Kubeflow setup, feature store, automated validation, monitor integration)\n",
    "\n",
    "---\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "**5. High-Traffic Recommendation API**\n",
    "- **Objective**: Serve 100K recommendations/sec for e-commerce platform\n",
    "- **Architecture**: TensorFlow Serving + Kubernetes + Redis caching + CDN\n",
    "- **Key Features**: Model batching (32 samples), feature caching, multi-tier architecture\n",
    "- **Success Metrics**: <50ms P99 latency, 99.99% uptime, 15% CTR increase\n",
    "- **Value**: $50M revenue increase from better recommendations\n",
    "\n",
    "---\n",
    "\n",
    "**6. Medical Imaging API**\n",
    "- **Objective**: Real-time cancer detection from radiology images\n",
    "- **Architecture**: PyTorch + ONNX Runtime + GPU serving + DICOM integration\n",
    "- **Key Features**: High-accuracy model (AUC 0.96), explainable AI (Grad-CAM), HIPAA compliance\n",
    "- **Success Metrics**: <5s inference, 96% sensitivity, 98% specificity, radiologist approval\n",
    "- **Value**: Early cancer detection saves lives, $10M/year revenue\n",
    "\n",
    "---\n",
    "\n",
    "**7. Fraud Detection System**\n",
    "- **Objective**: Real-time fraud scoring for financial transactions\n",
    "- **Architecture**: XGBoost + FastAPI + Redis + Kubernetes + real-time feature pipeline\n",
    "- **Key Features**: <10ms scoring, 1M transactions/day, explainable predictions\n",
    "- **Success Metrics**: 99.5% fraud detection, 0.5% false positives, $100M fraud prevented\n",
    "- **Value**: Protect customers, reduce chargebacks\n",
    "\n",
    "---\n",
    "\n",
    "**8. Chatbot Backend**\n",
    "- **Objective**: Deploy LLM for customer support (1M conversations/day)\n",
    "- **Architecture**: BERT + FastAPI + vLLM (batching) + GPU + prompt caching\n",
    "- **Key Features**: Context management, streaming responses, safety filters\n",
    "- **Success Metrics**: <500ms first token, 90% customer satisfaction, 50% support cost reduction\n",
    "- **Value**: $20M annual savings from automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec62961",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways & Next Steps\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "**1. REST API Serving (FastAPI):**\n",
    "- ‚úÖ **FastAPI**: Async performance, auto-docs, type safety, 3√ó faster than Flask\n",
    "- ‚úÖ **Pydantic**: Input/output validation catches errors before inference\n",
    "- ‚úÖ **Best Practices**: Load model at startup, batch requests, async I/O, health checks\n",
    "- ‚úÖ **Intel**: 500K predictions/day, <10ms P99 latency, 99.99% uptime\n",
    "\n",
    "**2. Docker Containerization:**\n",
    "- ‚úÖ **Reproducibility**: Same environment dev ‚Üí staging ‚Üí production\n",
    "- ‚úÖ **Optimization**: Multi-stage builds, slim images, layer caching (2.5GB ‚Üí 800MB)\n",
    "- ‚úÖ **Security**: Non-root user, health checks, minimal attack surface\n",
    "- ‚úÖ **AMD**: Edge deployment (200MB ‚Üí 12MB), <1ms inference on ARM\n",
    "\n",
    "**3. Kubernetes Deployment:**\n",
    "- ‚úÖ **Auto-scaling**: HPA scales 3-20 pods based on CPU/memory/custom metrics\n",
    "- ‚úÖ **Self-healing**: Restart failed pods, replace unhealthy instances\n",
    "- ‚úÖ **Rolling Updates**: Zero-downtime deployments, gradual rollout, instant rollback\n",
    "- ‚úÖ **NVIDIA**: 100K predictions/day, 99.99% uptime, auto-scale in 30 seconds\n",
    "\n",
    "**4. Monitoring & Observability:**\n",
    "- ‚úÖ **Prometheus + Grafana**: Track latency, throughput, error rate, model metrics\n",
    "- ‚úÖ **Data Drift Detection**: PSI, KS test, domain classifier (detect 2 weeks early)\n",
    "- ‚úÖ **Alerting**: Critical (PagerDuty), Warning (Slack), Info (Email)\n",
    "- ‚úÖ **Qualcomm**: Continuous training, automated retraining on drift, 95% accuracy maintained\n",
    "\n",
    "---\n",
    "\n",
    "### Deployment Architecture Comparison\n",
    "\n",
    "| Aspect | Flask + VM | FastAPI + Docker | FastAPI + K8s |\n",
    "|--------|-----------|------------------|---------------|\n",
    "| **Setup Complexity** | Simple | Moderate | Complex |\n",
    "| **Performance** | 100 req/sec | 300 req/sec | 10K+ req/sec |\n",
    "| **Scaling** | Manual (add VMs) | Manual (add containers) | Auto (HPA) |\n",
    "| **Deployment** | SSH + script | Docker push/pull | `kubectl apply` |\n",
    "| **Downtime** | Yes (5-10 min) | Minimal (1 min) | Zero (rolling) |\n",
    "| **Monitoring** | Basic logs | Docker logs | Prometheus/Grafana |\n",
    "| **Cost (1K req/sec)** | $500/month | $300/month | $200/month |\n",
    "\n",
    "---\n",
    "\n",
    "### Deployment Checklist\n",
    "\n",
    "**Before Production Deployment:**\n",
    "- ‚úÖ **Model Validation**: Accuracy >90% on hold-out test set\n",
    "- ‚úÖ **Load Testing**: Simulate 10√ó expected traffic (Locust, JMeter)\n",
    "- ‚úÖ **Latency Testing**: P99 <100ms (target based on use case)\n",
    "- ‚úÖ **Error Handling**: Graceful failures, informative error messages\n",
    "- ‚úÖ **Security**: API authentication, rate limiting, input sanitization\n",
    "- ‚úÖ **Documentation**: API docs (/docs), runbooks, architecture diagrams\n",
    "- ‚úÖ **Monitoring**: Dashboards, alerts, log aggregation\n",
    "- ‚úÖ **Disaster Recovery**: Backup models, rollback plan, multi-region (optional)\n",
    "\n",
    "**After Deployment:**\n",
    "- ‚úÖ **Canary Deploy**: Route 10% ‚Üí validate ‚Üí 100%\n",
    "- ‚úÖ **Shadow Deploy**: Run new model in parallel, compare predictions\n",
    "- ‚úÖ **Monitor Metrics**: Latency, error rate, model performance, data drift\n",
    "- ‚úÖ **On-call Rotation**: Engineers on-call for critical alerts\n",
    "- ‚úÖ **Post-mortem**: Document incidents, improve processes\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Optimization Guide\n",
    "\n",
    "**Latency Optimization:**\n",
    "1. **Model Level**: Quantization (FP32‚ÜíINT8), pruning, distillation, ONNX Runtime\n",
    "2. **Serving Level**: Batching (dynamic batching for throughput), caching (Redis), async I/O\n",
    "3. **Infrastructure**: GPU (vs CPU), co-location (model + API), CDN (for features)\n",
    "4. **Intel Example**: 10ms ‚Üí 3ms (quantization + batching + GPU)\n",
    "\n",
    "**Throughput Optimization:**\n",
    "1. **Horizontal Scaling**: More pods/containers/VMs\n",
    "2. **Vertical Scaling**: More CPU/memory per instance\n",
    "3. **Batching**: Process 32 samples together (10√ó throughput)\n",
    "4. **Load Balancing**: Distribute requests evenly (NGINX, K8s Service)\n",
    "5. **NVIDIA Example**: 1K ‚Üí 10K req/sec (GPU batching + 20 pods)\n",
    "\n",
    "**Cost Optimization:**\n",
    "1. **Right-sizing**: Don't over-provision (monitor actual usage)\n",
    "2. **Spot Instances**: 70% cheaper for non-critical workloads\n",
    "3. **Auto-scaling**: Scale down during low traffic (nights, weekends)\n",
    "4. **Model Optimization**: Smaller model = less compute = lower cost\n",
    "5. **AMD Example**: $500K/year cloud costs ‚Üí $50K/year edge deployment\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Impact Summary\n",
    "\n",
    "| Company | Solution | Problem Solved | Savings |\n",
    "|---------|----------|----------------|---------|\n",
    "| **Intel** | End-to-end ML platform | 20 models, 1M predictions/day | $25M |\n",
    "| **AMD** | Edge inference | 500 devices, <1ms latency | $18M |\n",
    "| **NVIDIA** | Multi-region deployment | Global <100ms latency | $15M |\n",
    "| **Qualcomm** | Continuous training | Weekly retraining, 95% accuracy | $20M |\n",
    "\n",
    "**Total measurable impact:** $78M across 4 companies\n",
    "\n",
    "---\n",
    "\n",
    "### Common Pitfalls & Solutions\n",
    "\n",
    "**1. Loading Model Per Request:**\n",
    "- ‚ùå Problem: 1s overhead, slow inference\n",
    "- ‚úÖ Solution: Load once at startup, cache in memory\n",
    "\n",
    "**2. No Health Checks:**\n",
    "- ‚ùå Problem: K8s routes traffic to crashed pods\n",
    "- ‚úÖ Solution: /health endpoint for liveness/readiness probes\n",
    "\n",
    "**3. No Monitoring:**\n",
    "- ‚ùå Problem: Model degrades silently, business impact unknown\n",
    "- ‚úÖ Solution: Prometheus + Grafana + alerts on drift/accuracy\n",
    "\n",
    "**4. No Rollback Plan:**\n",
    "- ‚ùå Problem: Bad deployment breaks production, panic\n",
    "- ‚úÖ Solution: Version models, test in staging, canary deploy, instant rollback\n",
    "\n",
    "**5. Ignoring Data Drift:**\n",
    "- ‚ùå Problem: Model trained on 2023 data, serving 2024 data (92% ‚Üí 87% accuracy)\n",
    "- ‚úÖ Solution: Monitor PSI, retrain weekly, alert on drift\n",
    "\n",
    "**6. Single Point of Failure:**\n",
    "- ‚ùå Problem: One server down = entire service down\n",
    "- ‚úÖ Solution: Deploy multiple replicas, load balancing, auto-healing\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Immediate (This Week):**\n",
    "1. Build FastAPI endpoint for personal ML model\n",
    "2. Write Dockerfile and test locally\n",
    "3. Deploy to Docker Hub or local registry\n",
    "\n",
    "**Short-term (This Month):**\n",
    "1. Deploy to Kubernetes (Minikube locally, then cloud)\n",
    "2. Setup Prometheus + Grafana monitoring\n",
    "3. Implement auto-scaling with HPA\n",
    "\n",
    "**Long-term (This Quarter):**\n",
    "1. Build end-to-end ML platform (training ‚Üí registry ‚Üí serving ‚Üí monitoring)\n",
    "2. Implement continuous training pipeline\n",
    "3. Deploy to production with 99.9%+ uptime\n",
    "\n",
    "---\n",
    "\n",
    "### Resources\n",
    "\n",
    "**Books:**\n",
    "1. *Building Machine Learning Powered Applications* by Emmanuel Ameisen\n",
    "2. *Machine Learning Systems* by Chip Huyen\n",
    "3. *Kubernetes Patterns* by Bilgin Ibryam & Roland Hu√ü\n",
    "\n",
    "**Online:**\n",
    "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
    "- [Docker Documentation](https://docs.docker.com/)\n",
    "- [Kubernetes Documentation](https://kubernetes.io/docs/)\n",
    "- [Prometheus + Grafana Tutorials](https://prometheus.io/docs/tutorials/)\n",
    "\n",
    "**Courses:**\n",
    "- [Full Stack Deep Learning](https://fullstackdeeplearning.com/)\n",
    "- [Made With ML](https://madewithml.com/)\n",
    "- [Kubernetes for ML Engineers](https://www.coursera.org/learn/kubernetes)\n",
    "\n",
    "**Practice:**\n",
    "- Deploy simple model (scikit-learn) with FastAPI\n",
    "- Containerize with Docker\n",
    "- Deploy to Kubernetes (Minikube or cloud)\n",
    "- Add monitoring and alerts\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You now master production ML deployment from REST APIs to Kubernetes orchestration to monitoring. You can deploy models serving 1M predictions/day with <10ms latency and 99.99% uptime.\n",
    "\n",
    "**Measurable skills gained:**\n",
    "- Build FastAPI services (3√ó faster than Flask)\n",
    "- Containerize models with Docker (reproducible deployments)\n",
    "- Deploy to Kubernetes with auto-scaling (3-20 pods dynamically)\n",
    "- Monitor production models (Prometheus + Grafana + alerts)\n",
    "- Detect and fix data drift 2 weeks early (proactive retraining)\n",
    "- Achieve 99.99% uptime (5 minutes downtime/month)\n",
    "- Save $15-25M through efficient deployment and monitoring\n",
    "\n",
    "**Ready for end-to-end MLOps?** Proceed to **Notebook 111: MLOps Fundamentals** to learn complete ML pipelines with feature stores, experiment tracking, and CI/CD! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f306769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Create Dockerfile for ML model deployment\n",
    "dockerfile_content = \"\"\"\n",
    "# Multi-stage build for optimized ML model serving\n",
    "FROM python:3.11-slim as builder\n",
    "\n",
    "# Install build dependencies\n",
    "WORKDIR /build\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir --user -r requirements.txt\n",
    "\n",
    "# Production stage\n",
    "FROM python:3.11-slim\n",
    "\n",
    "# Copy installed packages from builder\n",
    "COPY --from=builder /root/.local /root/.local\n",
    "ENV PATH=/root/.local/bin:$PATH\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy application code\n",
    "COPY model_server.py .\n",
    "COPY models/ ./models/\n",
    "COPY config/ ./config/\n",
    "\n",
    "# Create non-root user for security\n",
    "RUN useradd -m -u 1000 mluser && chown -R mluser:mluser /app\n",
    "USER mluser\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8080\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n",
    "  CMD python -c \"import requests; requests.get('http://localhost:8080/health')\"\n",
    "\n",
    "# Run application\n",
    "CMD [\"python\", \"model_server.py\"]\n",
    "\"\"\"\n",
    "\n",
    "# Create requirements.txt\n",
    "requirements_content = \"\"\"\n",
    "fastapi==0.104.1\n",
    "uvicorn[standard]==0.24.0\n",
    "pydantic==2.5.0\n",
    "numpy==1.24.3\n",
    "scikit-learn==1.3.2\n",
    "pandas==2.1.3\n",
    "prometheus-client==0.19.0\n",
    "python-json-logger==2.0.7\n",
    "\"\"\"\n",
    "\n",
    "# Create FastAPI model serving application\n",
    "model_server_content = '''\n",
    "\"\"\"\n",
    "Production ML Model Server with FastAPI\n",
    "Handles yield prediction for semiconductor wafer test data\n",
    "\"\"\"\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel, Field\n",
    "from prometheus_client import Counter, Histogram, generate_latest\n",
    "import numpy as np\n",
    "import pickle\n",
    "import logging\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='{\"timestamp\": \"%(asctime)s\", \"level\": \"%(levelname)s\", \"message\": \"%(message)s\"}'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Prometheus metrics\n",
    "prediction_counter = Counter('model_predictions_total', 'Total predictions made')\n",
    "prediction_latency = Histogram('model_prediction_latency_seconds', 'Prediction latency')\n",
    "error_counter = Counter('model_errors_total', 'Total prediction errors')\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Wafer Yield Prediction API\",\n",
    "    description=\"Production ML model for predicting semiconductor wafer yield\",\n",
    "    version=\"2.0.0\"\n",
    ")\n",
    "\n",
    "# Load model at startup\n",
    "model = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_model():\n",
    "    global model\n",
    "    try:\n",
    "        with open('models/yield_predictor_v2.pkl', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        logger.info(\"Model loaded successfully\", extra={\"model_version\": \"v2.0\"})\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model: {e}\")\n",
    "        raise\n",
    "\n",
    "# Request/Response models\n",
    "class PredictionRequest(BaseModel):\n",
    "    wafer_id: str = Field(..., description=\"Unique wafer identifier\")\n",
    "    voltage: float = Field(..., ge=1.0, le=1.5, description=\"Voltage (V)\")\n",
    "    current: float = Field(..., ge=100, le=1000, description=\"Current (mA)\")\n",
    "    temperature: float = Field(..., ge=20, le=85, description=\"Temperature (¬∞C)\")\n",
    "    test_time: float = Field(..., ge=0, le=300, description=\"Test time (seconds)\")\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"wafer_id\": \"W12345\",\n",
    "                \"voltage\": 1.2,\n",
    "                \"current\": 500,\n",
    "                \"temperature\": 25,\n",
    "                \"test_time\": 45.5\n",
    "            }\n",
    "        }\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    wafer_id: str\n",
    "    predicted_yield: float = Field(..., ge=0, le=100)\n",
    "    confidence: float\n",
    "    model_version: str\n",
    "    latency_ms: float\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint for load balancer\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_loaded\": model is not None,\n",
    "        \"timestamp\": time.time()\n",
    "    }\n",
    "\n",
    "@app.get(\"/metrics\")\n",
    "async def metrics():\n",
    "    \"\"\"Prometheus metrics endpoint\"\"\"\n",
    "    return generate_latest()\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest):\n",
    "    \"\"\"Make yield prediction\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Extract features\n",
    "        features = np.array([[\n",
    "            request.voltage,\n",
    "            request.current,\n",
    "            request.temperature,\n",
    "            request.test_time\n",
    "        ]])\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(features)[0]\n",
    "        confidence = model.predict_proba(features).max()\n",
    "        \n",
    "        # Calculate latency\n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Update metrics\n",
    "        prediction_counter.inc()\n",
    "        prediction_latency.observe(latency_ms / 1000)\n",
    "        \n",
    "        # Log prediction\n",
    "        logger.info(\n",
    "            \"Prediction completed\",\n",
    "            extra={\n",
    "                \"wafer_id\": request.wafer_id,\n",
    "                \"predicted_yield\": float(prediction),\n",
    "                \"latency_ms\": latency_ms\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return PredictionResponse(\n",
    "            wafer_id=request.wafer_id,\n",
    "            predicted_yield=float(prediction),\n",
    "            confidence=float(confidence),\n",
    "            model_version=\"v2.0\",\n",
    "            latency_ms=latency_ms\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_counter.inc()\n",
    "        logger.error(f\"Prediction failed: {e}\", extra={\"wafer_id\": request.wafer_id})\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/batch_predict\")\n",
    "async def batch_predict(requests: List[PredictionRequest]):\n",
    "    \"\"\"Batch prediction for multiple wafers\"\"\"\n",
    "    results = []\n",
    "    for req in requests:\n",
    "        result = await predict(req)\n",
    "        results.append(result)\n",
    "    return {\"predictions\": results, \"count\": len(results)}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8080, log_level=\"info\")\n",
    "'''\n",
    "\n",
    "# Create docker-compose.yml for local testing\n",
    "docker_compose_content = \"\"\"\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  model-server:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "    environment:\n",
    "      - MODEL_VERSION=v2.0\n",
    "      - LOG_LEVEL=INFO\n",
    "    volumes:\n",
    "      - ./models:/app/models:ro\n",
    "    restart: unless-stopped\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          cpus: '2.0'\n",
    "          memory: 4G\n",
    "        reservations:\n",
    "          cpus: '1.0'\n",
    "          memory: 2G\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/health\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "      start_period: 40s\n",
    "\n",
    "  prometheus:\n",
    "    image: prom/prometheus:latest\n",
    "    ports:\n",
    "      - \"9090:9090\"\n",
    "    volumes:\n",
    "      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n",
    "    command:\n",
    "      - '--config.file=/etc/prometheus/prometheus.yml'\n",
    "    restart: unless-stopped\n",
    "\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    environment:\n",
    "      - GF_SECURITY_ADMIN_PASSWORD=admin\n",
    "    restart: unless-stopped\n",
    "\"\"\"\n",
    "\n",
    "# Print deployment files\n",
    "print(\"üê≥ Docker Deployment Configuration\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìÑ Dockerfile (Multi-stage optimized):\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Key features:\")\n",
    "print(\"  ‚úÖ Multi-stage build (reduces image size 60%)\")\n",
    "print(\"  ‚úÖ Non-root user (security best practice)\")\n",
    "print(\"  ‚úÖ Health check (Kubernetes readiness probe)\")\n",
    "print(\"  ‚úÖ Python 3.11 slim base (350MB vs 1GB full image)\")\n",
    "\n",
    "print(\"\\nüìÑ model_server.py (FastAPI Application):\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Capabilities:\")\n",
    "print(\"  ‚úÖ RESTful API with OpenAPI docs\")\n",
    "print(\"  ‚úÖ Pydantic validation (input validation, type safety)\")\n",
    "print(\"  ‚úÖ Prometheus metrics (predictions, latency, errors)\")\n",
    "print(\"  ‚úÖ Structured JSON logging\")\n",
    "print(\"  ‚úÖ Batch prediction endpoint\")\n",
    "print(\"  ‚úÖ Health check for load balancer\")\n",
    "\n",
    "print(\"\\nüìÑ docker-compose.yml (Local Development):\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Services:\")\n",
    "print(\"  üöÄ model-server: ML model API (port 8080)\")\n",
    "print(\"  üìä prometheus: Metrics collection (port 9090)\")\n",
    "print(\"  üìà grafana: Visualization dashboard (port 3000)\")\n",
    "print(\"  \")\n",
    "print(\"Resource limits:\")\n",
    "print(\"  CPU: 1-2 cores, Memory: 2-4 GB\")\n",
    "\n",
    "print(\"\\nüîß Build and Run Commands:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"# Build Docker image\")\n",
    "print(\"docker build -t wafer-yield-model:v2.0 .\")\n",
    "print(\"\")\n",
    "print(\"# Run single container\")\n",
    "print(\"docker run -p 8080:8080 wafer-yield-model:v2.0\")\n",
    "print(\"\")\n",
    "print(\"# Run with docker-compose (includes monitoring)\")\n",
    "print(\"docker-compose up -d\")\n",
    "print(\"\")\n",
    "print(\"# Test API\")\n",
    "print(\"curl -X POST http://localhost:8080/predict \\\\\")\n",
    "print('  -H \"Content-Type: application/json\" \\\\')\n",
    "print('  -d \\'{\"wafer_id\": \"W001\", \"voltage\": 1.2, \"current\": 500, \"temperature\": 25, \"test_time\": 45}\\'')\n",
    "\n",
    "print(\"\\nüìä Expected Performance:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"  Image size: ~450 MB (multi-stage build)\")\n",
    "print(\"  Startup time: 3-5 seconds\")\n",
    "print(\"  Prediction latency: <50ms (p95)\")\n",
    "print(\"  Throughput: 1000 req/sec (single container)\")\n",
    "print(\"  Memory footprint: 1.5-2 GB (loaded model + cache)\")\n",
    "\n",
    "print(\"\\nüè≠ Post-Silicon Validation Deployment:\")\n",
    "print(\"  Use case: Deploy yield predictor to production fab network\")\n",
    "print(\"  Deployment: 3 containers behind load balancer\")\n",
    "print(\"  Monitoring: Prometheus + Grafana dashboards\")\n",
    "print(\"  Integration: REST API called by test equipment controllers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdbbeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kubernetes deployment configuration\n",
    "k8s_deployment = \"\"\"\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: wafer-yield-model\n",
    "  namespace: ml-models\n",
    "  labels:\n",
    "    app: wafer-yield-model\n",
    "    version: v2.0\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: wafer-yield-model\n",
    "  strategy:\n",
    "    type: RollingUpdate\n",
    "    rollingUpdate:\n",
    "      maxSurge: 1        # Max 1 extra pod during update\n",
    "      maxUnavailable: 0  # No downtime during update\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: wafer-yield-model\n",
    "        version: v2.0\n",
    "      annotations:\n",
    "        prometheus.io/scrape: \"true\"\n",
    "        prometheus.io/port: \"8080\"\n",
    "        prometheus.io/path: \"/metrics\"\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: model-server\n",
    "        image: registry.company.com/wafer-yield-model:v2.0\n",
    "        ports:\n",
    "        - containerPort: 8080\n",
    "          name: http\n",
    "        env:\n",
    "        - name: MODEL_VERSION\n",
    "          value: \"v2.0\"\n",
    "        - name: LOG_LEVEL\n",
    "          value: \"INFO\"\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: 500m      # 0.5 CPU core\n",
    "            memory: 1Gi    # 1 GB RAM\n",
    "          limits:\n",
    "            cpu: 2000m     # 2 CPU cores max\n",
    "            memory: 4Gi    # 4 GB RAM max\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8080\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "          timeoutSeconds: 5\n",
    "          failureThreshold: 3\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8080\n",
    "          initialDelaySeconds: 10\n",
    "          periodSeconds: 5\n",
    "          timeoutSeconds: 3\n",
    "          failureThreshold: 2\n",
    "        volumeMounts:\n",
    "        - name: model-storage\n",
    "          mountPath: /app/models\n",
    "          readOnly: true\n",
    "      volumes:\n",
    "      - name: model-storage\n",
    "        persistentVolumeClaim:\n",
    "          claimName: model-pvc\n",
    "      affinity:\n",
    "        podAntiAffinity:\n",
    "          preferredDuringSchedulingIgnoredDuringExecution:\n",
    "          - weight: 100\n",
    "            podAffinityTerm:\n",
    "              labelSelector:\n",
    "                matchExpressions:\n",
    "                - key: app\n",
    "                  operator: In\n",
    "                  values:\n",
    "                  - wafer-yield-model\n",
    "              topologyKey: kubernetes.io/hostname\n",
    "\"\"\"\n",
    "\n",
    "k8s_service = \"\"\"\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: wafer-yield-model-service\n",
    "  namespace: ml-models\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  selector:\n",
    "    app: wafer-yield-model\n",
    "  ports:\n",
    "  - protocol: TCP\n",
    "    port: 80\n",
    "    targetPort: 8080\n",
    "    name: http\n",
    "  sessionAffinity: ClientIP  # Sticky sessions\n",
    "\"\"\"\n",
    "\n",
    "k8s_hpa = \"\"\"\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: wafer-yield-model-hpa\n",
    "  namespace: ml-models\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: wafer-yield-model\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 10\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80\n",
    "  behavior:\n",
    "    scaleDown:\n",
    "      stabilizationWindowSeconds: 300  # Wait 5 min before scaling down\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 50\n",
    "        periodSeconds: 60\n",
    "    scaleUp:\n",
    "      stabilizationWindowSeconds: 60   # Scale up quickly\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 100\n",
    "        periodSeconds: 30\n",
    "\"\"\"\n",
    "\n",
    "k8s_ingress = \"\"\"\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: wafer-yield-model-ingress\n",
    "  namespace: ml-models\n",
    "  annotations:\n",
    "    kubernetes.io/ingress.class: nginx\n",
    "    cert-manager.io/cluster-issuer: letsencrypt-prod\n",
    "    nginx.ingress.kubernetes.io/rate-limit: \"100\"\n",
    "spec:\n",
    "  tls:\n",
    "  - hosts:\n",
    "    - ml-api.company.com\n",
    "    secretName: ml-api-tls\n",
    "  rules:\n",
    "  - host: ml-api.company.com\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /predict\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: wafer-yield-model-service\n",
    "            port:\n",
    "              number: 80\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚ò∏Ô∏è Kubernetes Deployment Configuration\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüì¶ Deployment Manifest:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Configuration:\")\n",
    "print(\"  ‚Ä¢ Replicas: 3 (high availability)\")\n",
    "print(\"  ‚Ä¢ Rolling update: MaxSurge=1, MaxUnavailable=0 (zero downtime)\")\n",
    "print(\"  ‚Ä¢ Resources: 0.5-2 CPU, 1-4 GB memory per pod\")\n",
    "print(\"  ‚Ä¢ Probes: Liveness (detect crashes), Readiness (traffic routing)\")\n",
    "print(\"  ‚Ä¢ Anti-affinity: Spread pods across nodes\")\n",
    "\n",
    "print(\"\\nüîÄ Service (LoadBalancer):\")\n",
    "print(\"-\" * 80)\n",
    "print(\"  ‚Ä¢ Type: LoadBalancer (cloud provider integration)\")\n",
    "print(\"  ‚Ä¢ Port: 80 ‚Üí 8080 (external ‚Üí internal)\")\n",
    "print(\"  ‚Ä¢ Session affinity: ClientIP (sticky sessions)\")\n",
    "\n",
    "print(\"\\nüìà HorizontalPodAutoscaler:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"  ‚Ä¢ Min replicas: 2 (always available)\")\n",
    "print(\"  ‚Ä¢ Max replicas: 10 (handle traffic spikes)\")\n",
    "print(\"  ‚Ä¢ CPU target: 70% utilization\")\n",
    "print(\"  ‚Ä¢ Memory target: 80% utilization\")\n",
    "print(\"  ‚Ä¢ Scale-up: Fast (60s window), Scale-down: Slow (300s window)\")\n",
    "\n",
    "print(\"\\nüåê Ingress (External Access):\")\n",
    "print(\"-\" * 80)\n",
    "print(\"  ‚Ä¢ Domain: ml-api.company.com\")\n",
    "print(\"  ‚Ä¢ TLS: Automatic HTTPS with Let's Encrypt\")\n",
    "print(\"  ‚Ä¢ Rate limiting: 100 req/sec per IP\")\n",
    "\n",
    "print(\"\\nüöÄ Deployment Commands:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"# Apply configurations\")\n",
    "print(\"kubectl apply -f deployment.yaml\")\n",
    "print(\"kubectl apply -f service.yaml\")\n",
    "print(\"kubectl apply -f hpa.yaml\")\n",
    "print(\"kubectl apply -f ingress.yaml\")\n",
    "print(\"\")\n",
    "print(\"# Check status\")\n",
    "print(\"kubectl get pods -n ml-models\")\n",
    "print(\"kubectl get hpa -n ml-models\")\n",
    "print(\"kubectl describe deployment wafer-yield-model -n ml-models\")\n",
    "print(\"\")\n",
    "print(\"# Rolling update (zero downtime)\")\n",
    "print(\"kubectl set image deployment/wafer-yield-model \\\\\")\n",
    "print(\"  model-server=registry.company.com/wafer-yield-model:v2.1 -n ml-models\")\n",
    "print(\"\")\n",
    "print(\"# Rollback if issues\")\n",
    "print(\"kubectl rollout undo deployment/wafer-yield-model -n ml-models\")\n",
    "\n",
    "print(\"\\nüìä Scaling Behavior Simulation:\")\n",
    "print(\"-\" * 80)\n",
    "import numpy as np\n",
    "\n",
    "# Simulate traffic pattern (24 hours)\n",
    "hours = np.arange(24)\n",
    "traffic_pattern = np.array([\n",
    "    20, 15, 10, 10, 15, 30,  # 00:00-05:00 (low)\n",
    "    50, 80, 100, 90, 85, 95,  # 06:00-11:00 (morning peak)\n",
    "    100, 110, 100, 95, 90, 100,  # 12:00-17:00 (afternoon peak)\n",
    "    80, 60, 50, 40, 30, 25   # 18:00-23:00 (evening decline)\n",
    "])\n",
    "\n",
    "# Calculate required pods (assuming 100 req/sec per pod at 70% CPU)\n",
    "cpu_per_pod = 70  # req/sec at 70% CPU target\n",
    "required_pods = np.ceil(traffic_pattern / cpu_per_pod).astype(int)\n",
    "required_pods = np.clip(required_pods, 2, 10)  # Min 2, max 10\n",
    "\n",
    "print(\"Hour | Traffic | Required Pods | Scaling Action\")\n",
    "print(\"-\" * 60)\n",
    "for h in [0, 6, 12, 18, 23]:\n",
    "    action = \"\"\n",
    "    if h > 0:\n",
    "        prev_pods = required_pods[h-1]\n",
    "        curr_pods = required_pods[h]\n",
    "        if curr_pods > prev_pods:\n",
    "            action = f\"‚Üë Scale up (+{curr_pods - prev_pods})\"\n",
    "        elif curr_pods < prev_pods:\n",
    "            action = f\"‚Üì Scale down (-{prev_pods - curr_pods})\"\n",
    "        else:\n",
    "            action = \"‚Üí No change\"\n",
    "    print(f\"{h:02d}:00 | {traffic_pattern[h]:3d} req/s | {required_pods[h]:2d} pods        | {action}\")\n",
    "\n",
    "avg_pods = np.mean(required_pods)\n",
    "fixed_pods = 10  # If no auto-scaling\n",
    "cost_savings = ((fixed_pods * 24) - np.sum(required_pods)) / (fixed_pods * 24) * 100\n",
    "\n",
    "print(f\"\\nüí∞ Cost Analysis:\")\n",
    "print(f\"  Fixed capacity (10 pods √ó 24h): {fixed_pods * 24} pod-hours\")\n",
    "print(f\"  Auto-scaled capacity: {np.sum(required_pods)} pod-hours\")\n",
    "print(f\"  Cost savings: {cost_savings:.1f}%\")\n",
    "\n",
    "print(\"\\nüè≠ Post-Silicon Validation K8s Deployment:\")\n",
    "print(\"  Cluster: 5 nodes (3 control plane, 2 worker nodes)\")\n",
    "print(\"  Pods: 2-10 replicas based on wafer test volume\")\n",
    "print(\"  Storage: NFS-mounted model files (PersistentVolume)\")\n",
    "print(\"  Networking: Internal ClusterIP for test equipment access\")\n",
    "print(\"  Monitoring: Prometheus + Grafana on separate namespace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed14f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Simulate production monitoring data\n",
    "class ProductionMonitor:\n",
    "    \"\"\"Monitor ML model performance in production\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "        self.alerts = []\n",
    "    \n",
    "    def record_prediction(self, model_version: str, latency_ms: float, \n",
    "                         error: bool, actual_yield: float = None, \n",
    "                         predicted_yield: float = None):\n",
    "        \"\"\"Record prediction metrics\"\"\"\n",
    "        self.metrics[f\"{model_version}_latency\"].append(latency_ms)\n",
    "        self.metrics[f\"{model_version}_errors\"].append(1 if error else 0)\n",
    "        \n",
    "        if actual_yield is not None and predicted_yield is not None:\n",
    "            mae = abs(actual_yield - predicted_yield)\n",
    "            self.metrics[f\"{model_version}_mae\"].append(mae)\n",
    "    \n",
    "    def check_alerts(self, model_version: str) -> list:\n",
    "        \"\"\"Check for alerting conditions\"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        # Latency alert (p95 > 500ms)\n",
    "        latencies = self.metrics[f\"{model_version}_latency\"]\n",
    "        if latencies:\n",
    "            p95_latency = np.percentile(latencies, 95)\n",
    "            if p95_latency > 500:\n",
    "                alerts.append(f\"‚ö†Ô∏è High latency: p95={p95_latency:.0f}ms (threshold: 500ms)\")\n",
    "        \n",
    "        # Error rate alert (>1%)\n",
    "        errors = self.metrics[f\"{model_version}_errors\"]\n",
    "        if errors:\n",
    "            error_rate = np.mean(errors) * 100\n",
    "            if error_rate > 1.0:\n",
    "                alerts.append(f\"üö® High error rate: {error_rate:.2f}% (threshold: 1.0%)\")\n",
    "        \n",
    "        # Accuracy drift alert (MAE increase >5%)\n",
    "        mae_values = self.metrics[f\"{model_version}_mae\"]\n",
    "        if len(mae_values) > 100:\n",
    "            recent_mae = np.mean(mae_values[-100:])\n",
    "            baseline_mae = np.mean(mae_values[:100])\n",
    "            drift = ((recent_mae - baseline_mae) / baseline_mae) * 100\n",
    "            if drift > 5.0:\n",
    "                alerts.append(f\"üìâ Accuracy drift: +{drift:.1f}% MAE increase (threshold: 5%)\")\n",
    "        \n",
    "        return alerts\n",
    "\n",
    "# A/B Testing simulation\n",
    "class ABTestManager:\n",
    "    \"\"\"Manage A/B tests for model deployments\"\"\"\n",
    "    \n",
    "    def __init__(self, model_a_version: str, model_b_version: str, \n",
    "                 traffic_split: float = 0.9):\n",
    "        self.model_a = model_a_version\n",
    "        self.model_b = model_b_version\n",
    "        self.traffic_split = traffic_split  # 90% to A, 10% to B\n",
    "        self.results = {model_a_version: [], model_b_version: []}\n",
    "    \n",
    "    def route_request(self) -> str:\n",
    "        \"\"\"Route request to A or B based on traffic split\"\"\"\n",
    "        return self.model_a if random.random() < self.traffic_split else self.model_b\n",
    "    \n",
    "    def record_result(self, model: str, accuracy: float):\n",
    "        \"\"\"Record prediction accuracy\"\"\"\n",
    "        self.results[model].append(accuracy)\n",
    "    \n",
    "    def analyze_test(self) -> dict:\n",
    "        \"\"\"Statistical analysis of A/B test results\"\"\"\n",
    "        results_a = np.array(self.results[self.model_a])\n",
    "        results_b = np.array(self.results[self.model_b])\n",
    "        \n",
    "        mean_a = np.mean(results_a)\n",
    "        mean_b = np.mean(results_b)\n",
    "        std_a = np.std(results_a)\n",
    "        std_b = np.std(results_b)\n",
    "        \n",
    "        # Calculate improvement\n",
    "        improvement = ((mean_b - mean_a) / mean_a) * 100\n",
    "        \n",
    "        # Simple significance test (t-statistic)\n",
    "        n_a, n_b = len(results_a), len(results_b)\n",
    "        pooled_std = np.sqrt((std_a**2 / n_a) + (std_b**2 / n_b))\n",
    "        t_stat = (mean_b - mean_a) / pooled_std if pooled_std > 0 else 0\n",
    "        \n",
    "        # Decision threshold: >2% improvement, t-stat > 2 (roughly p < 0.05)\n",
    "        decision = \"ROLLOUT\" if improvement > 2.0 and abs(t_stat) > 2.0 else \"HOLD\"\n",
    "        \n",
    "        return {\n",
    "            \"model_a\": self.model_a,\n",
    "            \"model_b\": self.model_b,\n",
    "            \"mean_a\": mean_a,\n",
    "            \"mean_b\": mean_b,\n",
    "            \"improvement_pct\": improvement,\n",
    "            \"t_statistic\": t_stat,\n",
    "            \"decision\": decision,\n",
    "            \"confidence\": \"High\" if abs(t_stat) > 2.5 else \"Medium\"\n",
    "        }\n",
    "\n",
    "# Simulation\n",
    "monitor = ProductionMonitor()\n",
    "ab_test = ABTestManager(\"v2.0\", \"v2.1\", traffic_split=0.9)\n",
    "\n",
    "print(\"üìä Production Monitoring & A/B Testing Simulation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simulate 1000 predictions\n",
    "print(\"\\nüîÑ Simulating 1000 production predictions...\")\n",
    "for i in range(1000):\n",
    "    # Route to model version\n",
    "    model_version = ab_test.route_request()\n",
    "    \n",
    "    # Simulate prediction metrics (v2.1 slightly better)\n",
    "    if model_version == \"v2.0\":\n",
    "        latency = np.random.gamma(shape=2, scale=25)  # Mean ~50ms\n",
    "        error = random.random() < 0.005  # 0.5% error rate\n",
    "        mae = np.random.normal(2.5, 0.5)  # MAE ~2.5%\n",
    "    else:  # v2.1\n",
    "        latency = np.random.gamma(shape=2, scale=22)  # Slightly faster\n",
    "        error = random.random() < 0.003  # Lower error rate\n",
    "        mae = np.random.normal(2.0, 0.4)  # Better accuracy\n",
    "    \n",
    "    # Record metrics\n",
    "    monitor.record_prediction(model_version, latency, error, \n",
    "                             actual_yield=95.0, predicted_yield=95.0 - mae)\n",
    "    ab_test.record_result(model_version, 100 - mae)  # Accuracy as %\n",
    "\n",
    "# Check alerts\n",
    "print(\"\\nüö® Alert Check (v2.0):\")\n",
    "alerts_v20 = monitor.check_alerts(\"v2.0\")\n",
    "if alerts_v20:\n",
    "    for alert in alerts_v20:\n",
    "        print(f\"   {alert}\")\n",
    "else:\n",
    "    print(\"   ‚úÖ All metrics within thresholds\")\n",
    "\n",
    "print(\"\\nüö® Alert Check (v2.1):\")\n",
    "alerts_v21 = monitor.check_alerts(\"v2.1\")\n",
    "if alerts_v21:\n",
    "    for alert in alerts_v21:\n",
    "        print(f\"   {alert}\")\n",
    "else:\n",
    "    print(\"   ‚úÖ All metrics within thresholds\")\n",
    "\n",
    "# A/B test analysis\n",
    "print(\"\\nüìà A/B Test Results:\")\n",
    "print(\"-\" * 80)\n",
    "ab_results = ab_test.analyze_test()\n",
    "print(f\"Model A ({ab_results['model_a']}): {ab_results['mean_a']:.2f}% accuracy\")\n",
    "print(f\"Model B ({ab_results['model_b']}): {ab_results['mean_b']:.2f}% accuracy\")\n",
    "print(f\"Improvement: {ab_results['improvement_pct']:+.2f}%\")\n",
    "print(f\"T-statistic: {ab_results['t_statistic']:.2f}\")\n",
    "print(f\"Confidence: {ab_results['confidence']}\")\n",
    "print(f\"\\nüéØ Decision: {ab_results['decision']}\")\n",
    "if ab_results['decision'] == \"ROLLOUT\":\n",
    "    print(\"   ‚úÖ New model shows statistically significant improvement\")\n",
    "    print(\"   ‚Üí Gradually increase traffic: 10% ‚Üí 25% ‚Üí 50% ‚Üí 100%\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Improvement not significant enough\")\n",
    "    print(\"   ‚Üí Keep monitoring, need more data or larger improvement\")\n",
    "\n",
    "# Visualizations\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Latency comparison\n",
    "latencies_v20 = monitor.metrics[\"v2.0_latency\"]\n",
    "latencies_v21 = monitor.metrics[\"v2.1_latency\"]\n",
    "\n",
    "ax1.hist(latencies_v20, bins=30, alpha=0.6, label='v2.0', color='#3498db', edgecolor='black')\n",
    "ax1.hist(latencies_v21, bins=30, alpha=0.6, label='v2.1', color='#2ecc71', edgecolor='black')\n",
    "ax1.axvline(np.percentile(latencies_v20, 95), color='#3498db', linestyle='--', linewidth=2, \n",
    "            label=f'v2.0 p95: {np.percentile(latencies_v20, 95):.0f}ms')\n",
    "ax1.axvline(np.percentile(latencies_v21, 95), color='#2ecc71', linestyle='--', linewidth=2,\n",
    "            label=f'v2.1 p95: {np.percentile(latencies_v21, 95):.0f}ms')\n",
    "ax1.set_xlabel('Latency (ms)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Prediction Latency Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Error rate over time\n",
    "window_size = 50\n",
    "errors_v20 = monitor.metrics[\"v2.0_errors\"]\n",
    "errors_v21 = monitor.metrics[\"v2.1_errors\"]\n",
    "\n",
    "error_rate_v20 = [np.mean(errors_v20[max(0, i-window_size):i+1]) * 100 \n",
    "                  for i in range(len(errors_v20))]\n",
    "error_rate_v21 = [np.mean(errors_v21[max(0, i-window_size):i+1]) * 100 \n",
    "                  for i in range(len(errors_v21))]\n",
    "\n",
    "ax2.plot(error_rate_v20, linewidth=2, label='v2.0', color='#3498db', alpha=0.8)\n",
    "ax2.plot(error_rate_v21, linewidth=2, label='v2.1', color='#2ecc71', alpha=0.8)\n",
    "ax2.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='Alert threshold (1%)')\n",
    "ax2.set_xlabel('Prediction Number', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Error Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Error Rate Over Time (50-request moving average)', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Accuracy comparison (boxplot)\n",
    "accuracy_v20 = ab_test.results[\"v2.0\"]\n",
    "accuracy_v21 = ab_test.results[\"v2.1\"]\n",
    "\n",
    "box_data = [accuracy_v20, accuracy_v21]\n",
    "bp = ax3.boxplot(box_data, labels=['v2.0', 'v2.1'], patch_artist=True,\n",
    "                 boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                 medianprops=dict(color='red', linewidth=2),\n",
    "                 whiskerprops=dict(linewidth=1.5),\n",
    "                 capprops=dict(linewidth=1.5))\n",
    "\n",
    "# Color boxes\n",
    "bp['boxes'][0].set_facecolor('#3498db')\n",
    "bp['boxes'][1].set_facecolor('#2ecc71')\n",
    "\n",
    "ax3.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Model Accuracy Comparison (A/B Test)', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add improvement annotation\n",
    "improvement_pct = ab_results['improvement_pct']\n",
    "ax3.annotate(f'+{improvement_pct:.2f}% improvement', \n",
    "             xy=(1.5, np.mean(accuracy_v21)), xytext=(1.7, np.mean(accuracy_v21) + 0.5),\n",
    "             fontsize=11, color='green', fontweight='bold',\n",
    "             arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "\n",
    "# Plot 4: Traffic split visualization\n",
    "traffic_data = {'v2.0\\n(90%)': 900, 'v2.1\\n(10%)': 100}\n",
    "colors_pie = ['#3498db', '#2ecc71']\n",
    "\n",
    "wedges, texts, autotexts = ax4.pie(traffic_data.values(), labels=traffic_data.keys(), \n",
    "                                     autopct='%1.0f%%', startangle=90, colors=colors_pie,\n",
    "                                     textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontsize(14)\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "ax4.set_title('A/B Test Traffic Split (1000 Predictions)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('production_monitoring_ab_testing.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Monitoring Dashboard Metrics:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"v2.0 Performance:\")\n",
    "print(f\"  ‚Ä¢ Latency: p50={np.percentile(latencies_v20, 50):.0f}ms, p95={np.percentile(latencies_v20, 95):.0f}ms, p99={np.percentile(latencies_v20, 99):.0f}ms\")\n",
    "print(f\"  ‚Ä¢ Error rate: {np.mean(errors_v20) * 100:.3f}%\")\n",
    "print(f\"  ‚Ä¢ Accuracy: {np.mean(accuracy_v20):.2f}%\")\n",
    "print(f\"  ‚Ä¢ Throughput: {len(latencies_v20)} predictions\")\n",
    "\n",
    "print(f\"\\nv2.1 Performance:\")\n",
    "print(f\"  ‚Ä¢ Latency: p50={np.percentile(latencies_v21, 50):.0f}ms, p95={np.percentile(latencies_v21, 95):.0f}ms, p99={np.percentile(latencies_v21, 99):.0f}ms\")\n",
    "print(f\"  ‚Ä¢ Error rate: {np.mean(errors_v21) * 100:.3f}%\")\n",
    "print(f\"  ‚Ä¢ Accuracy: {np.mean(accuracy_v21):.2f}%\")\n",
    "print(f\"  ‚Ä¢ Throughput: {len(latencies_v21)} predictions\")\n",
    "\n",
    "print(\"\\nüè≠ Post-Silicon Validation Monitoring:\")\n",
    "print(\"  Metrics tracked:\")\n",
    "print(\"  ‚Ä¢ Yield prediction MAE (target: <2.5%)\")\n",
    "print(\"  ‚Ä¢ Wafer map rendering time (target: <100ms)\")\n",
    "print(\"  ‚Ä¢ Test equipment API latency (target: <50ms)\")\n",
    "print(\"  ‚Ä¢ Model refresh rate (retrain weekly with new fab data)\")\n",
    "print(\"  \")\n",
    "print(\"  A/B testing strategy:\")\n",
    "print(\"  ‚Ä¢ Shadow mode: Run v2.1 alongside v2.0, compare offline\")\n",
    "print(\"  ‚Ä¢ Canary: Route 10% wafers to v2.1 for 24 hours\")\n",
    "print(\"  ‚Ä¢ Gradual rollout: 10% ‚Üí 25% ‚Üí 50% ‚Üí 100% over 1 week\")\n",
    "print(\"  ‚Ä¢ Rollback plan: Instant rollback if accuracy drops >1%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26a494",
   "metadata": {},
   "source": [
    "## üîë Key Takeaways\n",
    "\n",
    "### Deployment Strategy Decision Matrix\n",
    "\n",
    "| Requirement | Strategy | Tech Stack | Example |\n",
    "|-------------|----------|------------|---------|\n",
    "| **Simple API** (< 100 req/day) | Single VM | Flask + gunicorn | Internal tool |\n",
    "| **Medium Scale** (1K-10K req/sec) | Docker + K8s | FastAPI + Uvicorn | B2B API |\n",
    "| **High Scale** (10K+ req/sec) | Distributed | TensorFlow Serving + Load Balancer | Consumer app |\n",
    "| **Ultra-Low Latency** (<10ms) | Custom C++/Rust | gRPC + Redis cache | Fraud detection |\n",
    "| **Batch Processing** | Scheduled jobs | Spark + Airflow | Nightly retraining |\n",
    "| **Edge Deployment** | Model optimization | TFLite, ONNX | Mobile app |\n",
    "\n",
    "### Model Serving Patterns\n",
    "\n",
    "**1. REST API (Most Common)**\n",
    "- Pros: Language-agnostic, easy integration, HTTP tooling\n",
    "- Cons: Higher latency than gRPC (50-100ms overhead)\n",
    "- Use for: B2B APIs, internal services\n",
    "\n",
    "**2. gRPC**\n",
    "- Pros: 2-5x faster than REST, streaming support\n",
    "- Cons: Requires proto definitions, less tooling\n",
    "- Use for: Microservices, high-throughput systems\n",
    "\n",
    "**3. Batch Inference**\n",
    "- Pros: High throughput (100-1000x), cost-efficient\n",
    "- Cons: Not real-time, latency in hours\n",
    "- Use for: Nightly scoring, recommendations precomputation\n",
    "\n",
    "**4. Streaming**\n",
    "- Pros: Real-time, stateful processing\n",
    "- Cons: Complex infrastructure (Kafka, Flink)\n",
    "- Use for: Fraud detection, real-time analytics\n",
    "\n",
    "### Infrastructure Checklist ‚úÖ\n",
    "\n",
    "**Before Production:**\n",
    "- [ ] Model versioning (MLflow, DVC)\n",
    "- [ ] API documentation (OpenAPI/Swagger)\n",
    "- [ ] Input validation (Pydantic, JSON schema)\n",
    "- [ ] Error handling (graceful degradation)\n",
    "- [ ] Logging (structured JSON logs)\n",
    "- [ ] Monitoring (Prometheus, DataDog)\n",
    "- [ ] Alerting (PagerDuty, Slack)\n",
    "- [ ] Load testing (Locust, JMeter)\n",
    "- [ ] Security (API keys, rate limiting)\n",
    "- [ ] Docker image (<1GB, multi-stage build)\n",
    "- [ ] K8s manifests (deployment, service, HPA)\n",
    "- [ ] CI/CD pipeline (GitHub Actions, Jenkins)\n",
    "- [ ] Rollback plan (blue-green, canary)\n",
    "- [ ] Documentation (runbook, troubleshooting)\n",
    "\n",
    "### Performance Optimization Tips ‚ö°\n",
    "\n",
    "**Model Optimization:**\n",
    "```python\n",
    "# 1. Quantization (4x smaller, 2-3x faster)\n",
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('model/')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# 2. ONNX Runtime (1.5-3x faster inference)\n",
    "import onnxruntime as ort\n",
    "session = ort.InferenceSession('model.onnx')\n",
    "predictions = session.run(None, {'input': features})\n",
    "\n",
    "# 3. Batch inference (10-100x throughput)\n",
    "batch_size = 32  # Process 32 requests together\n",
    "predictions = model.predict(batch_inputs)\n",
    "```\n",
    "\n",
    "**Infrastructure Optimization:**\n",
    "- **Caching**: Redis for hot features (10-100x speedup)\n",
    "- **Load balancing**: Consistent hashing for cache affinity\n",
    "- **Auto-scaling**: HPA based on custom metrics (queue depth)\n",
    "- **GPU acceleration**: 10-100x for deep learning models\n",
    "- **Model pruning**: Remove 30-50% weights with <1% accuracy loss\n",
    "\n",
    "### Common Deployment Pitfalls ‚ö†Ô∏è\n",
    "\n",
    "1. **No versioning**: Can't rollback when issues arise\n",
    "   - Solution: Tag every model (v1.0, v1.1), store in registry\n",
    "\n",
    "2. **Insufficient monitoring**: Can't debug production issues\n",
    "   - Solution: Log every prediction with metadata, track latency/errors\n",
    "\n",
    "3. **No input validation**: Model crashes on unexpected inputs\n",
    "   - Solution: Use Pydantic, validate ranges, handle missing values\n",
    "\n",
    "4. **Tight coupling**: Model server depends on 10 other services\n",
    "   - Solution: Decouple with message queues, implement circuit breakers\n",
    "\n",
    "5. **No A/B testing**: Deploy new model to 100% traffic immediately\n",
    "   - Solution: Shadow mode ‚Üí Canary (10%) ‚Üí Gradual rollout\n",
    "\n",
    "6. **Ignoring latency**: Only optimize for accuracy\n",
    "   - Solution: Balance accuracy vs latency, use simpler models if needed\n",
    "\n",
    "7. **Single point of failure**: One server crash = system down\n",
    "   - Solution: Deploy 3+ replicas, use load balancer, auto-restart pods\n",
    "\n",
    "### Post-Silicon Validation Deployment Best Practices\n",
    "\n",
    "**Model Deployment:**\n",
    "- Deploy yield predictor to fab internal network (isolated from internet)\n",
    "- Use private container registry (Harbor, Artifactory)\n",
    "- Model update frequency: Weekly (trained on latest 1M STDF records)\n",
    "- Rollback capability: Keep last 3 model versions available\n",
    "\n",
    "**Infrastructure:**\n",
    "- Kubernetes cluster on-premises (3 control + 5 worker nodes)\n",
    "- PostgreSQL for feature store (test parameters, historical yield)\n",
    "- Redis for caching hot wafer data (last 1000 wafers)\n",
    "- Prometheus + Grafana for fab-specific dashboards\n",
    "\n",
    "**Monitoring:**\n",
    "- Track prediction MAE per lot, wafer, die location\n",
    "- Alert if MAE >5% for any lot (indicates equipment drift)\n",
    "- Dashboard: Real-time yield predictions, wafer maps, equipment health\n",
    "- Audit trail: Log every prediction with STDF file ID for traceability\n",
    "\n",
    "**Security:**\n",
    "- API key authentication for test equipment access\n",
    "- Rate limiting: 100 req/sec per equipment ID\n",
    "- Network isolation: VPN required for external access\n",
    "- Data retention: 90 days for predictions, GDPR compliant\n",
    "\n",
    "### Next Steps üöÄ\n",
    "\n",
    "**Master Deployment:**\n",
    "1. **Practice**: Deploy simple model to Heroku/AWS Lambda\n",
    "2. **Build**: Create Docker + FastAPI + Kubernetes pipeline\n",
    "3. **Monitor**: Set up Prometheus + Grafana dashboards\n",
    "4. **Optimize**: Load test, profile, optimize latency\n",
    "\n",
    "**Continue Learning:**\n",
    "- **Next**: `082_Production_RAG_Systems.ipynb` - Deploy LLM applications\n",
    "- **Advanced**: MLOps practices, feature stores, model governance\n",
    "- **Read**: \"Building Machine Learning Powered Applications\" by Emmanuel Ameisen\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** üéâ You now understand production ML deployment from Docker containerization to Kubernetes orchestration, monitoring, A/B testing, and optimization. You can confidently deploy ML models that serve millions of predictions reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24715fc",
   "metadata": {},
   "source": [
    "## üéØ Real-World Deployment Projects\n",
    "\n",
    "### Project 1: Wafer Yield Predictor Production Deployment üè≠\n",
    "**Objective:** Deploy real-time yield prediction model to 5 semiconductor fabs\n",
    "\n",
    "**Architecture:**\n",
    "- **Model**: Random Forest trained on 5M+ STDF records\n",
    "- **Infrastructure**: AWS EKS (3-node cluster per fab)\n",
    "- **Deployment**: Docker container, Kubernetes orchestration\n",
    "- **API**: FastAPI serving predictions <50ms latency\n",
    "- **Monitoring**: Prometheus + Grafana dashboards\n",
    "\n",
    "**Deployment Pipeline:**\n",
    "1. Train model on EMR Spark cluster (weekly)\n",
    "2. Package model in Docker image\n",
    "3. Push to ECR (Elastic Container Registry)\n",
    "4. Canary deployment: 10% traffic for 24h\n",
    "5. Full rollout if accuracy stable\n",
    "6. Monitor: MAE, latency, throughput\n",
    "\n",
    "**Success Metrics:**\n",
    "- Prediction accuracy: >95% (MAE <2.5%)\n",
    "- Latency: p95 <100ms\n",
    "- Availability: 99.9% uptime\n",
    "- Cost: <$500/month per fab\n",
    "\n",
    "### Project 2: Customer Churn Prediction API üì±\n",
    "**Objective:** Deploy churn prediction model for 10M+ users\n",
    "\n",
    "**Tech Stack:**\n",
    "- Model: XGBoost (150MB model file)\n",
    "- Serving: TensorFlow Serving + REST API\n",
    "- Infrastructure: GCP GKE, 5-20 pods (auto-scaled)\n",
    "- Caching: Redis for hot user features\n",
    "- Database: BigQuery for feature store\n",
    "\n",
    "**Deployment Strategy:**\n",
    "- Blue-green deployment (zero downtime)\n",
    "- Shadow mode testing (1 week)\n",
    "- A/B test: 20% traffic to new model\n",
    "- Gradual rollout: 20% ‚Üí 50% ‚Üí 100%\n",
    "\n",
    "**Monitoring:**\n",
    "- Prediction volume: 50K/hour peak\n",
    "- False positive rate (alert if >5%)\n",
    "- Model drift detection (monthly retraining)\n",
    "\n",
    "### Project 3: Fraud Detection Real-Time Scoring üí≥\n",
    "**Objective:** Score transactions <100ms for fraud detection\n",
    "\n",
    "**Requirements:**\n",
    "- Ultra-low latency: <100ms p99\n",
    "- High throughput: 10K transactions/sec\n",
    "- Model updates: Daily retraining\n",
    "- Feature freshness: Real-time aggregations\n",
    "\n",
    "**Architecture:**\n",
    "- Model: LightGBM (20MB, fast inference)\n",
    "- Serving: Custom C++ inference server\n",
    "- Load balancing: NGINX (round-robin)\n",
    "- Feature store: Redis with 1-hour TTL\n",
    "- Deployment: Kubernetes with GPU nodes\n",
    "\n",
    "**Scaling:**\n",
    "- 50 pods during business hours\n",
    "- 10 pods overnight\n",
    "- Auto-scale based on queue depth\n",
    "- Circuit breaker: Fallback to rule-based scoring\n",
    "\n",
    "### Project 4: Recommendation System for E-Commerce üõí\n",
    "**Objective:** Serve personalized product recommendations at scale\n",
    "\n",
    "**Challenge:**\n",
    "- 1M+ products, 10M+ users\n",
    "- Model size: 2GB (embeddings)\n",
    "- Latency requirement: <200ms\n",
    "- Personalization: Real-time user context\n",
    "\n",
    "**Solution:**\n",
    "- Model: Two-tower neural network (TensorFlow)\n",
    "- Serving: TorchServe with batch inference\n",
    "- Caching: Multi-layer (CDN ‚Üí Redis ‚Üí Model)\n",
    "- Infrastructure: AWS SageMaker multi-model endpoints\n",
    "\n",
    "**Optimization:**\n",
    "- Quantize model (FP32 ‚Üí INT8, 4x smaller)\n",
    "- Batch predictions (10 users at once)\n",
    "- Cache top-N recommendations (1-hour TTL)\n",
    "- Prefetch for active users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4be4d",
   "metadata": {},
   "source": [
    "### üìä Production Monitoring & A/B Testing\n",
    "\n",
    "**Purpose:** Monitor model performance in production and safely test new model versions\n",
    "\n",
    "**Key Points:**\n",
    "- **Metrics**: Track prediction latency (p50/p95/p99), throughput, error rate, model accuracy\n",
    "- **Logging**: Structured JSON logs for debugging and audit trails\n",
    "- **Alerting**: PagerDuty/Slack alerts for latency >500ms, error rate >1%, accuracy drift >5%\n",
    "- **A/B Testing**: Route 10% traffic to new model (v2.1), compare metrics, gradual rollout\n",
    "\n",
    "**Post-Silicon Use Case:** A/B test new yield prediction model v2.1 vs v2.0 - route 10% wafer data to new model, compare accuracy on 1000 wafers, roll out if accuracy improves >2%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f524c",
   "metadata": {},
   "source": [
    "### ‚ò∏Ô∏è Kubernetes Deployment & Scaling\n",
    "\n",
    "**Purpose:** Deploy ML model on Kubernetes with auto-scaling, rolling updates, and high availability\n",
    "\n",
    "**Key Points:**\n",
    "- **Deployment**: ReplicaSet manages 3+ pods for redundancy\n",
    "- **Service**: ClusterIP for internal access, LoadBalancer for external\n",
    "- **HorizontalPodAutoscaler**: Scale based on CPU (70% target) or custom metrics (request rate)\n",
    "- **Rolling Update**: Zero-downtime deployments with readiness probes\n",
    "\n",
    "**Post-Silicon Use Case:** Deploy yield predictor to 5-node K8s cluster serving 10 fab test stations, auto-scale 2-10 pods based on incoming wafer data volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc5bc4d",
   "metadata": {},
   "source": [
    "## üê≥ Part 4: Production Deployment Implementation\n",
    "\n",
    "### Docker Containerization for ML Models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
