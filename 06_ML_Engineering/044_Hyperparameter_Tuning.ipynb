{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d76889",
   "metadata": {},
   "source": [
    "## üîç Grid Search: Exhaustive Exploration\n",
    "\n",
    "### What is Grid Search?\n",
    "\n",
    "**Grid Search** systematically evaluates every possible combination of hyperparameters in a predefined grid. It's the most thorough but also most computationally expensive approach.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given:\n",
    "- Hyperparameters: $\\lambda_1, \\lambda_2, ..., \\lambda_k$\n",
    "- Value sets: $V_1, V_2, ..., V_k$\n",
    "- Cross-validation folds: $K$\n",
    "\n",
    "**Search space size**: $|V_1| \\times |V_2| \\times ... \\times |V_k|$\n",
    "\n",
    "**Total evaluations**: $K \\times \\prod_{i=1}^{k} |V_i|$\n",
    "\n",
    "**Example**:\n",
    "- 3 hyperparameters: n_estimators ‚àà {50, 100, 150}, max_depth ‚àà {5, 10, 15}, min_samples_split ‚àà {2, 5, 10}\n",
    "- Grid size: 3 √ó 3 √ó 3 = 27 configurations\n",
    "- With 5-fold CV: 27 √ó 5 = **135 model trainings**\n",
    "\n",
    "### When to Use Grid Search\n",
    "\n",
    "| **Use Grid Search When** | **Avoid Grid Search When** |\n",
    "|--------------------------|----------------------------|\n",
    "| ‚úÖ Small search space (<100 configs) | ‚ùå Large search space (>1000 configs) |\n",
    "| ‚úÖ Discrete hyperparameters | ‚ùå Continuous hyperparameters |\n",
    "| ‚úÖ Known promising ranges | ‚ùå No prior knowledge |\n",
    "| ‚úÖ Computational budget allows | ‚ùå Limited time/resources |\n",
    "| ‚úÖ Need reproducibility | ‚ùå Exploratory tuning |\n",
    "| ‚úÖ Final optimization (narrow range) | ‚ùå Initial exploration (wide range) |\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Exhaustive**: Guaranteed to find best configuration in the grid\n",
    "2. **Reproducible**: Same grid always gives same result\n",
    "3. **Parallelizable**: All configurations independent\n",
    "4. **Simple**: Easy to understand and implement\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Exponential cost**: Doubles with each hyperparameter\n",
    "2. **Inefficient**: Wastes time on unpromising regions\n",
    "3. **Discrete only**: Must discretize continuous parameters\n",
    "4. **Curse of dimensionality**: Intractable for >5 hyperparameters\n",
    "\n",
    "### Grid Search Algorithm\n",
    "\n",
    "```python\n",
    "best_score = -infinity\n",
    "best_params = None\n",
    "\n",
    "for config in all_combinations(param_grid):\n",
    "    scores = []\n",
    "    for fold in cross_validation_folds:\n",
    "        train_data, val_data = split(fold)\n",
    "        model = train(config, train_data)\n",
    "        score = evaluate(model, val_data)\n",
    "        scores.append(score)\n",
    "    \n",
    "    avg_score = mean(scores)\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_params = config\n",
    "\n",
    "return best_params, best_score\n",
    "```\n",
    "\n",
    "### Practical Example: Random Forest Tuning\n",
    "\n",
    "**Scenario**: Tune Random Forest for semiconductor yield prediction\n",
    "\n",
    "**Hyperparameters**:\n",
    "- `n_estimators`: Number of trees\n",
    "- `max_depth`: Maximum tree depth\n",
    "- `min_samples_split`: Minimum samples to split node\n",
    "- `max_features`: Features to consider for split\n",
    "\n",
    "**Grid definition**:\n",
    "```python\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],        # 3 values\n",
    "    'max_depth': [5, 10, 15, None],        # 4 values\n",
    "    'min_samples_split': [2, 5, 10],       # 3 values\n",
    "    'max_features': ['sqrt', 'log2']       # 2 values\n",
    "}\n",
    "# Total: 3 √ó 4 √ó 3 √ó 2 = 72 configurations\n",
    "```\n",
    "\n",
    "**With 5-fold CV**: 72 √ó 5 = **360 model trainings**\n",
    "\n",
    "**Time estimate**:\n",
    "- Training time per model: 30 seconds\n",
    "- Total time: 360 √ó 30s = 10,800s = **3 hours**\n",
    "\n",
    "### Semiconductor-Specific Grid Design\n",
    "\n",
    "#### Yield Prediction (Random Forest)\n",
    "```python\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],       # More trees ‚Üí better, diminishing returns\n",
    "    'max_depth': [10, 15, 20],             # Prevent overfitting to specific wafers\n",
    "    'min_samples_leaf': [5, 10, 20],       # Ensure statistical significance\n",
    "    'max_features': [0.3, 0.5, 0.7]        # Feature subset for diversity\n",
    "}\n",
    "# 3 √ó 3 √ó 3 √ó 3 = 81 configs\n",
    "```\n",
    "\n",
    "#### Test Time Prediction (Gradient Boosting)\n",
    "```python\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],        # Boosting iterations\n",
    "    'learning_rate': [0.01, 0.05, 0.1],    # Step size (log scale)\n",
    "    'max_depth': [3, 5, 7],                # Tree depth (shallow for boosting)\n",
    "    'subsample': [0.8, 0.9, 1.0]           # Row sampling for diversity\n",
    "}\n",
    "# 3 √ó 3 √ó 3 √ó 3 = 81 configs\n",
    "```\n",
    "\n",
    "### Coarse-to-Fine Grid Search Strategy\n",
    "\n",
    "**Problem**: Don't know good ranges initially\n",
    "\n",
    "**Solution**: Two-stage grid search\n",
    "\n",
    "#### Stage 1: Coarse Grid (Wide Range, Few Values)\n",
    "```python\n",
    "coarse_grid = {\n",
    "    'n_estimators': [50, 200, 500],        # Wide range, sparse\n",
    "    'max_depth': [3, 10, 20],\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}\n",
    "# 3 √ó 3 √ó 3 = 27 configs (fast exploration)\n",
    "```\n",
    "\n",
    "**Result**: Best config has n_estimators=200, max_depth=10, learning_rate=0.01\n",
    "\n",
    "#### Stage 2: Fine Grid (Narrow Range, More Values)\n",
    "```python\n",
    "fine_grid = {\n",
    "    'n_estimators': [150, 175, 200, 225, 250],   # Narrow range, dense\n",
    "    'max_depth': [8, 9, 10, 11, 12],\n",
    "    'learning_rate': [0.008, 0.01, 0.012]\n",
    "}\n",
    "# 5 √ó 5 √ó 3 = 75 configs (refined search)\n",
    "```\n",
    "\n",
    "**Total**: 27 + 75 = 102 configs (vs 125 for single fine grid over wide range)\n",
    "\n",
    "### Grid Search with Nested CV (Unbiased Evaluation)\n",
    "\n",
    "**Problem**: GridSearchCV reports optimistic `best_score_` (data snooping)\n",
    "\n",
    "**Solution**: Nested CV (see notebook 043)\n",
    "- Outer loop: Estimates true performance\n",
    "- Inner loop (Grid Search): Tunes hyperparameters\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "outer_scores = []\n",
    "for train_idx, test_idx in outer_cv.split(X, y):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Inner loop: Grid Search for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=inner_cv)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Outer loop: Evaluate on test set (unbiased)\n",
    "    score = grid_search.best_estimator_.score(X_test, y_test)\n",
    "    outer_scores.append(score)\n",
    "\n",
    "# Report: mean(outer_scores) is unbiased estimate\n",
    "```\n",
    "\n",
    "### Parallel Grid Search\n",
    "\n",
    "**Speedup**: Use multiple CPU cores\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,      # Use all available cores\n",
    "    verbose=2       # Show progress\n",
    ")\n",
    "```\n",
    "\n",
    "**Speedup calculation**:\n",
    "- 8 cores ‚Üí ~8√ó speedup (near-linear scaling)\n",
    "- 72 configs √ó 5 folds = 360 trainings\n",
    "- Serial: 3 hours ‚Üí Parallel (8 cores): ~22.5 minutes\n",
    "\n",
    "### Common Grid Search Pitfalls\n",
    "\n",
    "#### ‚ùå Pitfall 1: Too Fine Initially\n",
    "**Problem**: Start with dense grid over wide range ‚Üí thousands of configs\n",
    "**Solution**: Coarse-to-fine strategy\n",
    "\n",
    "#### ‚ùå Pitfall 2: Uniform Grid for Log-Scale Parameters\n",
    "**Problem**: learning_rate ‚àà [0.001, 0.002, 0.003, ..., 0.1] ‚Üí waste time on similar values\n",
    "**Solution**: Log-scale grid [0.001, 0.01, 0.1] or use Random Search\n",
    "\n",
    "#### ‚ùå Pitfall 3: Not Parallelizing\n",
    "**Problem**: Run serially on multi-core machine\n",
    "**Solution**: Set `n_jobs=-1`\n",
    "\n",
    "#### ‚ùå Pitfall 4: Reporting Inner CV Score\n",
    "**Problem**: Report `best_score_` (optimistic bias)\n",
    "**Solution**: Use nested CV for unbiased estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "class GridSearchAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive Grid Search implementation with visualization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, estimator, param_grid, cv=5, scoring='accuracy', n_jobs=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            estimator: Sklearn model\n",
    "            param_grid: Dictionary of hyperparameter grids\n",
    "            cv: Cross-validation strategy\n",
    "            scoring: Metric to optimize\n",
    "            n_jobs: Number of parallel jobs (-1 = all cores)\n",
    "        \"\"\"\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.cv = cv\n",
    "        self.scoring = scoring\n",
    "        self.n_jobs = n_jobs\n",
    "        self.grid_search = None\n",
    "        self.results_df = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Perform grid search with timing and analysis.\n",
    "        \"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"GRID SEARCH: EXHAUSTIVE HYPERPARAMETER OPTIMIZATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Calculate search space size\n",
    "        n_configs = np.prod([len(v) for v in self.param_grid.values()])\n",
    "        n_folds = self.cv if isinstance(self.cv, int) else self.cv.get_n_splits()\n",
    "        total_fits = n_configs * n_folds\n",
    "        \n",
    "        print(f\"\\nSearch Space:\")\n",
    "        for param, values in self.param_grid.items():\n",
    "            print(f\"  {param}: {values} ({len(values)} values)\")\n",
    "        print(f\"\\nTotal configurations: {n_configs}\")\n",
    "        print(f\"Cross-validation folds: {n_folds}\")\n",
    "        print(f\"Total model trainings: {total_fits}\")\n",
    "        print(f\"Parallel jobs: {self.n_jobs}\")\n",
    "        \n",
    "        # Perform grid search\n",
    "        print(f\"\\nStarting Grid Search...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.grid_search = GridSearchCV(\n",
    "            estimator=self.estimator,\n",
    "            param_grid=self.param_grid,\n",
    "            cv=self.cv,\n",
    "            scoring=self.scoring,\n",
    "            n_jobs=self.n_jobs,\n",
    "            verbose=0,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        self.grid_search.fit(X, y)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Extract results\n",
    "        results = pd.DataFrame(self.grid_search.cv_results_)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Grid Search completed in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"   Time per configuration: {elapsed_time / n_configs:.2f} seconds\")\n",
    "        print(f\"   Time per fit: {elapsed_time / total_fits:.2f} seconds\")\n",
    "        \n",
    "        print(f\"\\nBest Configuration:\")\n",
    "        for param, value in self.grid_search.best_params_.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        \n",
    "        print(f\"\\nBest Score: {self.grid_search.best_score_:.6f}\")\n",
    "        print(f\"Best Train Score: {results.loc[self.grid_search.best_index_, 'mean_train_score']:.6f}\")\n",
    "        print(f\"Overfitting gap: {results.loc[self.grid_search.best_index_, 'mean_train_score'] - self.grid_search.best_score_:.6f}\")\n",
    "        \n",
    "        # Store results for visualization\n",
    "        self.results_df = results\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def plot_results(self, figsize=(16, 12)):\n",
    "        \"\"\"\n",
    "        Visualize grid search results with multiple plots.\n",
    "        \"\"\"\n",
    "        if self.results_df is None:\n",
    "            raise ValueError(\"Must call fit() before plot_results()\")\n",
    "        \n",
    "        results = self.results_df\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        \n",
    "        # Plot 1: Score distribution\n",
    "        axes[0, 0].hist(results['mean_test_score'], bins=30, edgecolor='black', alpha=0.7)\n",
    "        axes[0, 0].axvline(self.grid_search.best_score_, color='red', \n",
    "                          linestyle='--', linewidth=2, label=f'Best: {self.grid_search.best_score_:.4f}')\n",
    "        axes[0, 0].set_xlabel('Mean Test Score', fontsize=11, fontweight='bold')\n",
    "        axes[0, 0].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "        axes[0, 0].set_title('Score Distribution Across All Configurations', \n",
    "                            fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Top 10 configurations\n",
    "        top_10 = results.nsmallest(10, 'rank_test_score')[['params', 'mean_test_score', 'std_test_score']]\n",
    "        config_labels = [f\"Config {i+1}\" for i in range(len(top_10))]\n",
    "        \n",
    "        axes[0, 1].barh(config_labels, top_10['mean_test_score'], \n",
    "                       xerr=top_10['std_test_score'], alpha=0.7, edgecolor='black')\n",
    "        axes[0, 1].set_xlabel('Mean Test Score', fontsize=11, fontweight='bold')\n",
    "        axes[0, 1].set_ylabel('Configuration', fontsize=11, fontweight='bold')\n",
    "        axes[0, 1].set_title('Top 10 Configurations\\\\n(with standard deviation)', \n",
    "                            fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].grid(alpha=0.3, axis='x')\n",
    "        axes[0, 1].invert_yaxis()\n",
    "        \n",
    "        # Plot 3: Overfitting analysis (train vs test)\n",
    "        axes[1, 0].scatter(results['mean_train_score'], results['mean_test_score'], \n",
    "                          alpha=0.6, s=50)\n",
    "        \n",
    "        # Add diagonal line (perfect generalization)\n",
    "        min_score = min(results['mean_train_score'].min(), results['mean_test_score'].min())\n",
    "        max_score = max(results['mean_train_score'].max(), results['mean_test_score'].max())\n",
    "        axes[1, 0].plot([min_score, max_score], [min_score, max_score], \n",
    "                       'r--', linewidth=2, label='Perfect Generalization')\n",
    "        \n",
    "        # Highlight best configuration\n",
    "        best_idx = self.grid_search.best_index_\n",
    "        axes[1, 0].scatter(results.loc[best_idx, 'mean_train_score'], \n",
    "                          results.loc[best_idx, 'mean_test_score'],\n",
    "                          color='red', s=200, marker='*', \n",
    "                          edgecolor='black', linewidth=2, label='Best Config')\n",
    "        \n",
    "        axes[1, 0].set_xlabel('Mean Train Score', fontsize=11, fontweight='bold')\n",
    "        axes[1, 0].set_ylabel('Mean Test Score', fontsize=11, fontweight='bold')\n",
    "        axes[1, 0].set_title('Overfitting Analysis\\\\n(Points below line = overfitting)', \n",
    "                            fontsize=12, fontweight='bold')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Hyperparameter importance (if possible to visualize)\n",
    "        # For simplicity, show score vs first hyperparameter\n",
    "        first_param = list(self.param_grid.keys())[0]\n",
    "        \n",
    "        if len(self.param_grid[first_param]) > 1:\n",
    "            # Group by first parameter\n",
    "            param_scores = results.groupby(f'param_{first_param}')['mean_test_score'].agg(['mean', 'std'])\n",
    "            \n",
    "            axes[1, 1].bar(range(len(param_scores)), param_scores['mean'], \n",
    "                          yerr=param_scores['std'], alpha=0.7, edgecolor='black')\n",
    "            axes[1, 1].set_xticks(range(len(param_scores)))\n",
    "            axes[1, 1].set_xticklabels(param_scores.index, rotation=45, ha='right')\n",
    "            axes[1, 1].set_xlabel(first_param, fontsize=11, fontweight='bold')\n",
    "            axes[1, 1].set_ylabel('Mean Test Score', fontsize=11, fontweight='bold')\n",
    "            axes[1, 1].set_title(f'Performance vs {first_param}\\\\n(averaged over other params)', \n",
    "                                fontsize=12, fontweight='bold')\n",
    "            axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, 'Not enough\\nhyperparameter values\\nfor visualization', \n",
    "                           ha='center', va='center', fontsize=14, transform=axes[1, 1].transAxes)\n",
    "            axes[1, 1].set_title('Hyperparameter Analysis', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_top_configs(self, n=5):\n",
    "        \"\"\"\n",
    "        Return top N configurations.\n",
    "        \"\"\"\n",
    "        if self.results_df is None:\n",
    "            raise ValueError(\"Must call fit() before get_top_configs()\")\n",
    "        \n",
    "        top_n = self.results_df.nsmallest(n, 'rank_test_score')\n",
    "        \n",
    "        print(f\"\\nTop {n} Configurations:\")\n",
    "        print(\"=\"*80)\n",
    "        for i, (idx, row) in enumerate(top_n.iterrows(), 1):\n",
    "            print(f\"\\nRank {i}:\")\n",
    "            print(f\"  Score: {row['mean_test_score']:.6f} ¬± {row['std_test_score']:.6f}\")\n",
    "            print(f\"  Params: {row['params']}\")\n",
    "        \n",
    "        return top_n[['params', 'mean_test_score', 'std_test_score']]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nEXAMPLE: Grid Search for Semiconductor Defect Detection\\n\")\n",
    "    \n",
    "    # Generate imbalanced semiconductor defect data\n",
    "    X, y = make_classification(\n",
    "        n_samples=2000,\n",
    "        n_features=15,\n",
    "        n_informative=12,\n",
    "        n_redundant=3,\n",
    "        n_classes=2,\n",
    "        weights=[0.95, 0.05],  # 5% defect rate (imbalanced)\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset: {len(y)} devices\")\n",
    "    print(f\"Features: 15 parametric measurements\")\n",
    "    print(f\"Target: Defect detection (imbalanced)\")\n",
    "    print(f\"Class distribution: {(y==0).sum()} good ({(y==0).sum()/len(y)*100:.1f}%), {(y==1).sum()} defective ({(y==1).sum()/len(y)*100:.1f}%)\")\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'class_weight': ['balanced', None]\n",
    "    }\n",
    "    \n",
    "    # Create analyzer\n",
    "    analyzer = GridSearchAnalyzer(\n",
    "        estimator=RandomForestClassifier(random_state=42),\n",
    "        param_grid=param_grid,\n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring=make_scorer(f1_score),  # F1 for imbalanced data\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Fit and analyze\n",
    "    analyzer.fit(X, y)\n",
    "    \n",
    "    # Get top configurations\n",
    "    top_configs = analyzer.get_top_configs(n=5)\n",
    "    \n",
    "    # Visualize results\n",
    "    analyzer.plot_results()\n",
    "    \n",
    "    print(\"\\n‚úÖ Grid Search analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a1febd",
   "metadata": {},
   "source": [
    "## üé≤ Random Search: Efficient Sampling\n",
    "\n",
    "### What is Random Search?\n",
    "\n",
    "**Random Search** samples hyperparameter configurations randomly from specified distributions. Instead of trying every combination (Grid Search), it evaluates a fixed number of random samples.\n",
    "\n",
    "### Why Random Search Often Beats Grid Search\n",
    "\n",
    "**Key insight**: Not all hyperparameters are equally important.\n",
    "\n",
    "Consider a 2D grid:\n",
    "- Hyperparameter A: Critical (large impact on performance)\n",
    "- Hyperparameter B: Minor (small impact on performance)\n",
    "\n",
    "**Grid Search (9 points)**:\n",
    "```\n",
    "B‚ÇÉ  ‚Ä¢   ‚Ä¢   ‚Ä¢\n",
    "B‚ÇÇ  ‚Ä¢   ‚Ä¢   ‚Ä¢\n",
    "B‚ÇÅ  ‚Ä¢   ‚Ä¢   ‚Ä¢\n",
    "    A‚ÇÅ  A‚ÇÇ  A‚ÇÉ\n",
    "```\n",
    "- Tests 3 unique values of A\n",
    "- Tests 3 unique values of B\n",
    "\n",
    "**Random Search (9 points)**:\n",
    "```\n",
    "B   ‚Ä¢     ‚Ä¢\n",
    "      ‚Ä¢ ‚Ä¢   ‚Ä¢\n",
    "    ‚Ä¢   ‚Ä¢   ‚Ä¢\n",
    "      ‚Ä¢\n",
    "    A (continuous)\n",
    "```\n",
    "- Tests ~9 unique values of A (if uniformly sampled)\n",
    "- Tests ~9 unique values of B\n",
    "\n",
    "**Result**: Random Search explores more values of the important hyperparameter A!\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given:\n",
    "- Hyperparameter distributions: $\\lambda_1 \\sim D_1, \\lambda_2 \\sim D_2, ..., \\lambda_k \\sim D_k$\n",
    "- Number of iterations: $n$\n",
    "\n",
    "**Algorithm**:\n",
    "1. For iteration $i = 1$ to $n$:\n",
    "   - Sample $\\lambda_1^{(i)} \\sim D_1, \\lambda_2^{(i)} \\sim D_2, ..., \\lambda_k^{(i)} \\sim D_k$\n",
    "   - Evaluate configuration $(\\lambda_1^{(i)}, \\lambda_2^{(i)}, ..., \\lambda_k^{(i)})$ using CV\n",
    "2. Return best configuration\n",
    "\n",
    "**Probability of finding good configuration**:\n",
    "\n",
    "If top 5% of configurations have acceptable performance:\n",
    "- Grid Search (27 configs): 27 √ó 0.05 = 1.35 expected good configs\n",
    "- Random Search (100 configs): 100 √ó 0.05 = 5 expected good configs\n",
    "\n",
    "**Probability of finding at least one good config**:\n",
    "- Random Search: $1 - (0.95)^{100} = 99.4\\%$\n",
    "- Grid Search: $1 - (0.95)^{27} = 74.7\\%$\n",
    "\n",
    "### Distribution Selection\n",
    "\n",
    "#### Uniform Distribution\n",
    "- **Use for**: Discrete parameters with equal importance\n",
    "- **Example**: max_depth ‚àà [5, 20], all values equally likely\n",
    "- **Implementation**: `scipy.stats.randint(5, 21)`\n",
    "\n",
    "#### Log-Uniform Distribution\n",
    "- **Use for**: Parameters spanning multiple orders of magnitude\n",
    "- **Example**: learning_rate ‚àà [0.0001, 0.1] (1000√ó range)\n",
    "- **Why**: Equal probability per order of magnitude\n",
    "- **Implementation**: `scipy.stats.loguniform(0.0001, 0.1)`\n",
    "\n",
    "**Visualization**:\n",
    "```\n",
    "Uniform [0.0001, 0.1]:\n",
    "  90% samples in [0.09, 0.1] ‚ùå (waste time on similar values)\n",
    "\n",
    "Log-Uniform [0.0001, 0.1]:\n",
    "  33% in [0.0001, 0.001]\n",
    "  33% in [0.001, 0.01]\n",
    "  33% in [0.01, 0.1] ‚úÖ (explore all scales)\n",
    "```\n",
    "\n",
    "#### Categorical Distribution\n",
    "- **Use for**: Discrete choices without ordering\n",
    "- **Example**: kernel ‚àà {linear, rbf, poly, sigmoid}\n",
    "- **Implementation**: List of options\n",
    "\n",
    "### When to Use Random Search\n",
    "\n",
    "| **Use Random Search When** | **Prefer Grid Search When** |\n",
    "|---------------------------|----------------------------|\n",
    "| ‚úÖ Large search space (>100 configs) | ‚ùå Small search space (<100 configs) |\n",
    "| ‚úÖ Continuous hyperparameters | ‚ùå Only discrete hyperparameters |\n",
    "| ‚úÖ No prior knowledge | ‚ùå Known good ranges |\n",
    "| ‚úÖ Limited computational budget | ‚ùå Unlimited budget |\n",
    "| ‚úÖ Exploratory tuning | ‚ùå Final optimization |\n",
    "| ‚úÖ Many hyperparameters (>5) | ‚ùå Few hyperparameters (‚â§3) |\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Efficient**: Explores more hyperparameter values with same budget\n",
    "2. **Flexible**: Handles continuous distributions natively\n",
    "3. **Parallelizable**: All samples independent\n",
    "4. **Anytime**: Can stop early and use best so far\n",
    "5. **Scales well**: Performance doesn't degrade with dimensionality\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Non-exhaustive**: May miss optimal configuration\n",
    "2. **Non-deterministic**: Different runs give different results (use random_state)\n",
    "3. **Inefficient for low-dim**: Grid Search better for 1-2 hyperparameters\n",
    "4. **No exploitation**: Doesn't learn from previous evaluations\n",
    "\n",
    "### Random Search vs Grid Search: Empirical Comparison\n",
    "\n",
    "**Scenario**: Tune Random Forest with 5 hyperparameters, 100 configurations budget\n",
    "\n",
    "**Grid Search**:\n",
    "- 5 hyperparameters, 2.5 values each: 2.5‚Åµ ‚âà 97 configs (underfits each dimension)\n",
    "- Problem: Misses good regions between grid points\n",
    "\n",
    "**Random Search**:\n",
    "- Sample 100 random configurations\n",
    "- Explores full space uniformly\n",
    "- Higher chance of finding good configuration\n",
    "\n",
    "**Research (Bergstra & Bengio, 2012)**:\n",
    "- Random Search outperforms Grid Search in 80% of cases\n",
    "- Especially effective when few hyperparameters are important\n",
    "\n",
    "### Semiconductor-Specific Distributions\n",
    "\n",
    "#### Yield Prediction (XGBoost)\n",
    "```python\n",
    "param_distributions = {\n",
    "    'n_estimators': scipy.stats.randint(100, 500),      # Discrete uniform\n",
    "    'max_depth': scipy.stats.randint(3, 15),\n",
    "    'learning_rate': scipy.stats.loguniform(0.01, 0.3), # Log-uniform\n",
    "    'subsample': scipy.stats.uniform(0.6, 0.4),         # Uniform [0.6, 1.0]\n",
    "    'colsample_bytree': scipy.stats.uniform(0.6, 0.4),\n",
    "    'min_child_weight': scipy.stats.randint(1, 10)\n",
    "}\n",
    "# Samples from rich 6D space\n",
    "```\n",
    "\n",
    "#### Test Time Prediction (Gradient Boosting)\n",
    "```python\n",
    "param_distributions = {\n",
    "    'n_estimators': scipy.stats.randint(50, 300),\n",
    "    'learning_rate': scipy.stats.loguniform(0.001, 0.1),  # Wide log range\n",
    "    'max_depth': scipy.stats.randint(2, 10),\n",
    "    'min_samples_split': scipy.stats.randint(2, 20),\n",
    "    'max_features': scipy.stats.uniform(0.3, 0.7)         # [0.3, 1.0]\n",
    "}\n",
    "```\n",
    "\n",
    "### Practical Guidelines\n",
    "\n",
    "#### Number of Iterations\n",
    "\n",
    "**Rule of thumb**: $n \\geq 10 \\times k$ where $k$ = number of hyperparameters\n",
    "\n",
    "**Examples**:\n",
    "- 3 hyperparameters: $n \\geq 30$ iterations\n",
    "- 5 hyperparameters: $n \\geq 50$ iterations\n",
    "- 10 hyperparameters: $n \\geq 100$ iterations\n",
    "\n",
    "**Budget-based**:\n",
    "- Limited budget (1 hour): $n = 20-50$\n",
    "- Moderate budget (overnight): $n = 100-200$\n",
    "- Large budget (weekend): $n = 500-1000$\n",
    "\n",
    "**Convergence check**: Plot best score vs iteration. If plateaued ‚Üí stop.\n",
    "\n",
    "#### Choosing Distributions\n",
    "\n",
    "1. **Discrete with known range**: `scipy.stats.randint(low, high+1)`\n",
    "2. **Continuous bounded**: `scipy.stats.uniform(low, high-low)`\n",
    "3. **Log-scale (e.g., learning rates)**: `scipy.stats.loguniform(low, high)`\n",
    "4. **Categorical**: Python list `['option1', 'option2', 'option3']`\n",
    "\n",
    "#### Random vs Grid: Decision Framework\n",
    "\n",
    "**Use Grid Search if**:\n",
    "- Search space small (<100 configs)\n",
    "- Need reproducibility (exact same grid)\n",
    "- Final fine-tuning (narrow range)\n",
    "\n",
    "**Use Random Search if**:\n",
    "- Search space large (>100 configs)\n",
    "- Continuous hyperparameters\n",
    "- Initial exploration (wide range)\n",
    "- Many hyperparameters (>5)\n",
    "\n",
    "**Use both (Sequential)**:\n",
    "1. Random Search (100-200 samples): Explore broadly\n",
    "2. Identify promising region\n",
    "3. Grid Search (fine grid): Refine locally\n",
    "\n",
    "### Common Random Search Pitfalls\n",
    "\n",
    "#### ‚ùå Pitfall 1: Too Few Iterations\n",
    "**Problem**: 10 samples for 5 hyperparameters ‚Üí sparse coverage\n",
    "**Solution**: Use $n \\geq 10k$ rule\n",
    "\n",
    "#### ‚ùå Pitfall 2: Wrong Distribution\n",
    "**Problem**: Uniform [0.0001, 0.1] for learning_rate ‚Üí 90% samples near 0.1\n",
    "**Solution**: Use loguniform for log-scale parameters\n",
    "\n",
    "#### ‚ùå Pitfall 3: Not Setting random_state\n",
    "**Problem**: Cannot reproduce results\n",
    "**Solution**: Always set random_state=42\n",
    "\n",
    "#### ‚ùå Pitfall 4: Stopping Too Early\n",
    "**Problem**: Stop after 20 iterations, miss better configs at iteration 50\n",
    "**Solution**: Monitor convergence, extend if improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419adf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "class RandomSearchAnalyzer:\n",
    "    \"\"\"\n",
    "    Random Search implementation with convergence analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, estimator, param_distributions, n_iter=100, cv=5, \n",
    "                 scoring='r2', n_jobs=-1, random_state=42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            estimator: Sklearn model\n",
    "            param_distributions: Dictionary of distributions\n",
    "            n_iter: Number of random samples\n",
    "            cv: Cross-validation strategy\n",
    "            scoring: Metric to optimize\n",
    "            n_jobs: Parallel jobs\n",
    "            random_state: Random seed\n",
    "        \"\"\"\n",
    "        self.estimator = estimator\n",
    "        self.param_distributions = param_distributions\n",
    "        self.n_iter = n_iter\n",
    "        self.cv = cv\n",
    "        self.scoring = scoring\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.random_search = None\n",
    "        self.results_df = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Perform random search with analysis.\n",
    "        \"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"RANDOM SEARCH: EFFICIENT HYPERPARAMETER OPTIMIZATION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nSearch Configuration:\")\n",
    "        print(f\"  Number of iterations: {self.n_iter}\")\n",
    "        print(f\"  Cross-validation folds: {self.cv if isinstance(self.cv, int) else self.cv.get_n_splits()}\")\n",
    "        print(f\"  Scoring metric: {self.scoring}\")\n",
    "        print(f\"  Parallel jobs: {self.n_jobs}\")\n",
    "        print(f\"  Random state: {self.random_state}\")\n",
    "        \n",
    "        print(f\"\\nParameter Distributions:\")\n",
    "        for param, dist in self.param_distributions.items():\n",
    "            if hasattr(dist, 'dist'):\n",
    "                print(f\"  {param}: {dist.dist.name} distribution\")\n",
    "            elif isinstance(dist, list):\n",
    "                print(f\"  {param}: Categorical {dist}\")\n",
    "            else:\n",
    "                print(f\"  {param}: {type(dist).__name__}\")\n",
    "        \n",
    "        n_folds = self.cv if isinstance(self.cv, int) else self.cv.get_n_splits()\n",
    "        total_fits = self.n_iter * n_folds\n",
    "        print(f\"\\nTotal model trainings: {total_fits}\")\n",
    "        \n",
    "        # Perform random search\n",
    "        print(f\"\\nStarting Random Search...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.random_search = RandomizedSearchCV(\n",
    "            estimator=self.estimator,\n",
    "            param_distributions=self.param_distributions,\n",
    "            n_iter=self.n_iter,\n",
    "            cv=self.cv,\n",
    "            scoring=self.scoring,\n",
    "            n_jobs=self.n_jobs,\n",
    "            verbose=0,\n",
    "            random_state=self.random_state,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        \n",
    "        self.random_search.fit(X, y)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Extract results\n",
    "        results = pd.DataFrame(self.random_search.cv_results_)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Random Search completed in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"   Time per iteration: {elapsed_time / self.n_iter:.2f} seconds\")\n",
    "        print(f\"   Time per fit: {elapsed_time / total_fits:.2f} seconds\")\n",
    "        \n",
    "        print(f\"\\nBest Configuration:\")\n",
    "        for param, value in self.random_search.best_params_.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        \n",
    "        print(f\"\\nBest Score: {self.random_search.best_score_:.6f}\")\n",
    "        print(f\"Best Train Score: {results.loc[self.random_search.best_index_, 'mean_train_score']:.6f}\")\n",
    "        print(f\"Overfitting gap: {results.loc[self.random_search.best_index_, 'mean_train_score'] - self.random_search.best_score_:.6f}\")\n",
    "        \n",
    "        # Convergence analysis\n",
    "        results_sorted = results.sort_values('rank_test_score')\n",
    "        best_scores = results_sorted['mean_test_score'].cummax()  # Best score so far at each iteration\n",
    "        \n",
    "        print(f\"\\nConvergence Analysis:\")\n",
    "        print(f\"  Best score after 10 iterations: {best_scores.iloc[9]:.6f}\")\n",
    "        print(f\"  Best score after 50 iterations: {best_scores.iloc[49]:.6f}\" if len(best_scores) >= 50 else \"\")\n",
    "        print(f\"  Best score after {self.n_iter} iterations: {best_scores.iloc[-1]:.6f}\")\n",
    "        \n",
    "        # Check if converged\n",
    "        last_20_pct = int(0.2 * self.n_iter)\n",
    "        improvement_last_20 = best_scores.iloc[-1] - best_scores.iloc[-last_20_pct]\n",
    "        print(f\"  Improvement in last 20% iterations: {improvement_last_20:.6f}\")\n",
    "        \n",
    "        if improvement_last_20 < 0.001:\n",
    "            print(f\"  ‚úÖ Converged (minimal improvement in last 20%)\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  Still improving - consider increasing n_iter\")\n",
    "        \n",
    "        self.results_df = results\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def plot_results(self, figsize=(16, 12)):\n",
    "        \"\"\"\n",
    "        Visualize random search results.\n",
    "        \"\"\"\n",
    "        if self.results_df is None:\n",
    "            raise ValueError(\"Must call fit() before plot_results()\")\n",
    "        \n",
    "        results = self.results_df.copy()\n",
    "        results_sorted = results.sort_values('rank_test_score').reset_index(drop=True)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        \n",
    "        # Plot 1: Convergence over iterations\n",
    "        best_scores = results_sorted['mean_test_score'].cummax()\n",
    "        iterations = np.arange(1, len(best_scores) + 1)\n",
    "        \n",
    "        axes[0, 0].plot(iterations, best_scores, linewidth=2, color='blue')\n",
    "        axes[0, 0].scatter(iterations, best_scores, alpha=0.5, s=30, color='blue')\n",
    "        axes[0, 0].axhline(self.random_search.best_score_, color='red', \n",
    "                          linestyle='--', linewidth=2, label=f'Final Best: {self.random_search.best_score_:.4f}')\n",
    "        axes[0, 0].set_xlabel('Iteration (sorted by rank)', fontsize=11, fontweight='bold')\n",
    "        axes[0, 0].set_ylabel('Best Score So Far', fontsize=11, fontweight='bold')\n",
    "        axes[0, 0].set_title('Convergence Analysis\\\\n(Shows how quickly we find good configs)', \n",
    "                            fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Score distribution\n",
    "        axes[0, 1].hist(results['mean_test_score'], bins=30, edgecolor='black', alpha=0.7)\n",
    "        axes[0, 1].axvline(self.random_search.best_score_, color='red', \n",
    "                          linestyle='--', linewidth=2, label=f'Best: {self.random_search.best_score_:.4f}')\n",
    "        axes[0, 1].set_xlabel('Mean Test Score', fontsize=11, fontweight='bold')\n",
    "        axes[0, 1].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "        axes[0, 1].set_title('Score Distribution\\\\n(Shows exploration of search space)', \n",
    "                            fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Overfitting analysis\n",
    "        axes[1, 0].scatter(results['mean_train_score'], results['mean_test_score'], \n",
    "                          alpha=0.6, s=50)\n",
    "        \n",
    "        # Diagonal line\n",
    "        min_score = min(results['mean_train_score'].min(), results['mean_test_score'].min())\n",
    "        max_score = max(results['mean_train_score'].max(), results['mean_test_score'].max())\n",
    "        axes[1, 0].plot([min_score, max_score], [min_score, max_score], \n",
    "                       'r--', linewidth=2, label='Perfect Generalization')\n",
    "        \n",
    "        # Best config\n",
    "        best_idx = self.random_search.best_index_\n",
    "        axes[1, 0].scatter(results.loc[best_idx, 'mean_train_score'], \n",
    "                          results.loc[best_idx, 'mean_test_score'],\n",
    "                          color='red', s=200, marker='*', \n",
    "                          edgecolor='black', linewidth=2, label='Best Config')\n",
    "        \n",
    "        axes[1, 0].set_xlabel('Mean Train Score', fontsize=11, fontweight='bold')\n",
    "        axes[1, 0].set_ylabel('Mean Test Score', fontsize=11, fontweight='bold')\n",
    "        axes[1, 0].set_title('Overfitting Analysis\\\\n(Points below line = overfitting)', \n",
    "                            fontsize=12, fontweight='bold')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Top 10 configurations\n",
    "        top_10 = results_sorted.head(10)[['mean_test_score', 'std_test_score']]\n",
    "        config_labels = [f\"Config {i+1}\" for i in range(len(top_10))]\n",
    "        \n",
    "        axes[1, 1].barh(config_labels, top_10['mean_test_score'], \n",
    "                       xerr=top_10['std_test_score'], alpha=0.7, edgecolor='black')\n",
    "        axes[1, 1].set_xlabel('Mean Test Score', fontsize=11, fontweight='bold')\n",
    "        axes[1, 1].set_ylabel('Configuration', fontsize=11, fontweight='bold')\n",
    "        axes[1, 1].set_title('Top 10 Configurations\\\\n(with standard deviation)', \n",
    "                            fontsize=12, fontweight='bold')\n",
    "        axes[1, 1].grid(alpha=0.3, axis='x')\n",
    "        axes[1, 1].invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def compare_with_grid(self, grid_results_df):\n",
    "        \"\"\"\n",
    "        Compare Random Search with Grid Search results.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"RANDOM SEARCH vs GRID SEARCH COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nRandom Search:\")\n",
    "        print(f\"  Best score: {self.random_search.best_score_:.6f}\")\n",
    "        print(f\"  Iterations: {self.n_iter}\")\n",
    "        print(f\"  Best params: {self.random_search.best_params_}\")\n",
    "        \n",
    "        print(f\"\\nGrid Search:\")\n",
    "        grid_best_score = grid_results_df['mean_test_score'].max()\n",
    "        grid_best_idx = grid_results_df['mean_test_score'].idxmax()\n",
    "        grid_best_params = grid_results_df.loc[grid_best_idx, 'params']\n",
    "        print(f\"  Best score: {grid_best_score:.6f}\")\n",
    "        print(f\"  Configurations: {len(grid_results_df)}\")\n",
    "        print(f\"  Best params: {grid_best_params}\")\n",
    "        \n",
    "        print(f\"\\nComparison:\")\n",
    "        if self.random_search.best_score_ > grid_best_score:\n",
    "            print(f\"  ‚úÖ Random Search WINS by {self.random_search.best_score_ - grid_best_score:.6f}\")\n",
    "        elif self.random_search.best_score_ < grid_best_score:\n",
    "            print(f\"  ‚ùå Grid Search wins by {grid_best_score - self.random_search.best_score_:.6f}\")\n",
    "        else:\n",
    "            print(f\"  ‚öñÔ∏è  TIE (both found same best score)\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nEXAMPLE: Random Search for Semiconductor Test Time Prediction\\n\")\n",
    "    \n",
    "    # Generate test time data\n",
    "    X, y = make_regression(\n",
    "        n_samples=2000,\n",
    "        n_features=8,\n",
    "        n_informative=6,\n",
    "        noise=10.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset: {len(y)} device measurements\")\n",
    "    print(f\"Features: 8 parametric measurements\")\n",
    "    print(f\"Target: Test time (continuous)\")\n",
    "    \n",
    "    # Define parameter distributions\n",
    "    param_distributions = {\n",
    "        'n_estimators': randint(50, 300),                   # Discrete uniform [50, 300)\n",
    "        'learning_rate': loguniform(0.001, 0.3),           # Log-uniform [0.001, 0.3)\n",
    "        'max_depth': randint(2, 15),                       # Discrete uniform [2, 15)\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 10),\n",
    "        'subsample': uniform(0.6, 0.4),                    # Uniform [0.6, 1.0)\n",
    "        'max_features': uniform(0.3, 0.7)                  # Uniform [0.3, 1.0)\n",
    "    }\n",
    "    \n",
    "    # Create analyzer\n",
    "    analyzer = RandomSearchAnalyzer(\n",
    "        estimator=GradientBoostingRegressor(random_state=42),\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=100,\n",
    "        cv=5,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Fit and analyze\n",
    "    analyzer.fit(X, y)\n",
    "    \n",
    "    # Visualize\n",
    "    analyzer.plot_results()\n",
    "    \n",
    "    print(\"\\n‚úÖ Random Search analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc76695",
   "metadata": {},
   "source": [
    "## üß† Bayesian Optimization: Smart Search with Priors\n",
    "\n",
    "### What is Bayesian Optimization?\n",
    "\n",
    "**Bayesian Optimization** is an intelligent search strategy that builds a probabilistic model of the objective function and uses it to select promising hyperparameters to evaluate next. Unlike Grid/Random Search, it **learns from previous evaluations** and focuses on promising regions.\n",
    "\n",
    "### The Core Idea: Surrogate Model + Acquisition Function\n",
    "\n",
    "**Problem**: Evaluating hyperparameters is expensive (requires training model with CV)\n",
    "\n",
    "**Solution**: Build cheap surrogate model of expensive objective function\n",
    "\n",
    "**Algorithm**:\n",
    "1. **Surrogate Model**: Gaussian Process models $f(\\lambda) \\sim GP(\\mu, k)$\n",
    "   - Predicts performance and uncertainty for any hyperparameter configuration\n",
    "2. **Acquisition Function**: Balances exploration (high uncertainty) vs exploitation (high predicted performance)\n",
    "3. **Next Sample**: Choose hyperparameters maximizing acquisition function\n",
    "4. **Update**: Train model, update surrogate, repeat\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "**Objective**: Find $\\lambda^* = \\arg\\max_\\lambda f(\\lambda)$ where $f(\\lambda)$ is expensive to evaluate\n",
    "\n",
    "**Gaussian Process (GP)**:\n",
    "- Mean function: $\\mu(\\lambda)$ = expected performance\n",
    "- Covariance function: $k(\\lambda, \\lambda')$ = correlation between configurations\n",
    "- After $n$ evaluations: $\\{(\\lambda_1, y_1), ..., (\\lambda_n, y_n)\\}$\n",
    "- Posterior: $f(\\lambda) | D_n \\sim \\mathcal{N}(\\mu_n(\\lambda), \\sigma_n^2(\\lambda))$\n",
    "\n",
    "**Key properties**:\n",
    "- $\\mu_n(\\lambda_i) = y_i$ (interpolates observed points)\n",
    "- $\\sigma_n(\\lambda_i) = 0$ (no uncertainty at observed points)\n",
    "- Far from observations ‚Üí high uncertainty\n",
    "\n",
    "### Acquisition Functions\n",
    "\n",
    "#### 1. **Expected Improvement (EI)**\n",
    "\n",
    "**Definition**: Expected improvement over current best $f^+ = \\max_{i=1..n} y_i$\n",
    "\n",
    "$$EI(\\lambda) = \\mathbb{E}[\\max(0, f(\\lambda) - f^+)]$$\n",
    "\n",
    "**Closed form** (if $f(\\lambda) \\sim \\mathcal{N}(\\mu, \\sigma^2)$):\n",
    "\n",
    "$$EI(\\lambda) = \\begin{cases}\n",
    "(\\mu - f^+) \\Phi(Z) + \\sigma \\phi(Z) & \\text{if } \\sigma > 0 \\\\\n",
    "0 & \\text{if } \\sigma = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "where $Z = \\frac{\\mu - f^+}{\\sigma}$, $\\Phi$ = CDF, $\\phi$ = PDF of standard normal\n",
    "\n",
    "**Behavior**:\n",
    "- High $\\mu$ (exploitation) ‚Üí High EI\n",
    "- High $\\sigma$ (exploration) ‚Üí High EI\n",
    "- Balanced trade-off\n",
    "\n",
    "#### 2. **Probability of Improvement (PI)**\n",
    "\n",
    "**Definition**: Probability that $f(\\lambda) > f^+$\n",
    "\n",
    "$$PI(\\lambda) = P(f(\\lambda) > f^+) = \\Phi\\left(\\frac{\\mu - f^+}{\\sigma}\\right)$$\n",
    "\n",
    "**Behavior**: More exploitative than EI (focuses on high mean)\n",
    "\n",
    "#### 3. **Upper Confidence Bound (UCB)**\n",
    "\n",
    "**Definition**: Optimistic estimate with exploration parameter $\\kappa$\n",
    "\n",
    "$$UCB(\\lambda) = \\mu(\\lambda) + \\kappa \\cdot \\sigma(\\lambda)$$\n",
    "\n",
    "**Behavior**:\n",
    "- $\\kappa$ large ‚Üí Exploration (high uncertainty preferred)\n",
    "- $\\kappa$ small ‚Üí Exploitation (high mean preferred)\n",
    "- Typical: $\\kappa \\in [1, 3]$\n",
    "\n",
    "### Why Bayesian Optimization Works\n",
    "\n",
    "**Scenario**: 100 configurations to explore\n",
    "\n",
    "**Random Search**:\n",
    "- Evaluates 100 random configurations\n",
    "- No learning from previous evaluations\n",
    "- Uniform exploration\n",
    "\n",
    "**Bayesian Optimization**:\n",
    "- Iteration 1-10: Explore broadly (high uncertainty everywhere)\n",
    "- Iteration 11-50: Focus on promising regions (low performance areas ignored)\n",
    "- Iteration 51-100: Exploit best regions (refine around peak)\n",
    "\n",
    "**Result**: Bayesian Optimization finds better configurations with fewer evaluations\n",
    "\n",
    "**Research** (Snoek et al., 2012):\n",
    "- Bayesian Optimization achieved same performance as Random Search with **5√ó fewer evaluations**\n",
    "- Especially effective for expensive models (deep learning, ensembles)\n",
    "\n",
    "### When to Use Bayesian Optimization\n",
    "\n",
    "| **Use Bayesian Optimization When** | **Avoid Bayesian Optimization When** |\n",
    "|------------------------------------|-------------------------------------|\n",
    "| ‚úÖ Expensive model training (>1 min per config) | ‚ùå Fast model training (<10 sec per config) |\n",
    "| ‚úÖ Small iteration budget (<100 evals) | ‚ùå Large iteration budget (>1000 evals) |\n",
    "| ‚úÖ Continuous hyperparameters | ‚ùå Only categorical hyperparameters |\n",
    "| ‚úÖ Low-dimensional space (‚â§20 dims) | ‚ùå Very high-dimensional (>50 dims) |\n",
    "| ‚úÖ Smooth objective function | ‚ùå Highly noisy objective |\n",
    "| ‚úÖ Need sample efficiency | ‚ùå Parallelization is priority |\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Sample efficient**: Finds good configs with fewer evaluations\n",
    "2. **Principled**: Balances exploration/exploitation via probability theory\n",
    "3. **Handles continuous**: Natural for continuous hyperparameters\n",
    "4. **Uncertainty quantification**: Provides confidence in predictions\n",
    "5. **Works with constraints**: Can handle validity constraints\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Sequential**: Hard to parallelize (needs previous results)\n",
    "2. **GP overhead**: Gaussian Process training scales $O(n^3)$ with evaluations\n",
    "3. **Dimensionality**: Struggles with >20 hyperparameters\n",
    "4. **Requires tuning**: Acquisition function, GP kernel choices matter\n",
    "5. **Overkill for fast models**: Random Search sufficient if training is fast\n",
    "\n",
    "### Bayesian Optimization Libraries\n",
    "\n",
    "#### 1. **scikit-optimize (skopt)** ‚úÖ Recommended\n",
    "```python\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "search_spaces = {\n",
    "    'n_estimators': Integer(50, 300),\n",
    "    'learning_rate': Real(0.001, 0.3, prior='log-uniform'),\n",
    "    'max_depth': Integer(3, 15),\n",
    "    'subsample': Real(0.6, 1.0)\n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    estimator=model,\n",
    "    search_spaces=search_spaces,\n",
    "    n_iter=50,  # Only 50 iterations!\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "#### 2. **Optuna** (Advanced)\n",
    "- More flexible than skopt\n",
    "- Better parallelization support\n",
    "- Pruning for early stopping\n",
    "\n",
    "#### 3. **Hyperopt**\n",
    "- Tree-structured Parzen Estimator (TPE) instead of GP\n",
    "- Good for categorical/conditional hyperparameters\n",
    "\n",
    "### Semiconductor-Specific Applications\n",
    "\n",
    "#### Yield Prediction (XGBoost) - Expensive Model\n",
    "```python\n",
    "search_spaces = {\n",
    "    'n_estimators': Integer(100, 500),\n",
    "    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "    'max_depth': Integer(3, 15),\n",
    "    'min_child_weight': Integer(1, 10),\n",
    "    'subsample': Real(0.6, 1.0),\n",
    "    'colsample_bytree': Real(0.6, 1.0),\n",
    "    'gamma': Real(0.0, 5.0)\n",
    "}\n",
    "# 50 Bayesian iterations vs 1000+ for Random Search\n",
    "```\n",
    "\n",
    "**Why Bayesian Opt**: XGBoost training on 10K+ wafers takes 2-5 minutes per config\n",
    "- Random Search (100 configs): 200-500 minutes = **3.3-8.3 hours**\n",
    "- Bayesian Opt (50 configs): 100-250 minutes = **1.7-4.2 hours** with better results\n",
    "\n",
    "### Practical Guidelines\n",
    "\n",
    "#### Number of Iterations\n",
    "\n",
    "**Rule of thumb**: $n = 10 \\times d$ where $d$ = number of dimensions\n",
    "\n",
    "**Examples**:\n",
    "- 3 hyperparameters: $n \\geq 30$ iterations\n",
    "- 5 hyperparameters: $n \\geq 50$ iterations\n",
    "- 10 hyperparameters: $n \\geq 100$ iterations\n",
    "\n",
    "**Budget-based**:\n",
    "- Limited budget (2 hours): Bayesian Opt with 20-30 iterations\n",
    "- Moderate budget (overnight): Bayesian Opt with 50-100 iterations\n",
    "- Large budget (weekend): Random Search (more parallelizable)\n",
    "\n",
    "#### Initialization Strategy\n",
    "\n",
    "**Problem**: Gaussian Process needs initial points\n",
    "\n",
    "**Solution**: Random initialization (5-10 points) before Bayesian Optimization\n",
    "\n",
    "```python\n",
    "BayesSearchCV(\n",
    "    n_iter=50,\n",
    "    n_initial_points=10  # First 10 are random\n",
    ")\n",
    "```\n",
    "\n",
    "**Why**: GP is unreliable with <5 observations\n",
    "\n",
    "#### Choosing Acquisition Function\n",
    "\n",
    "**Default**: Expected Improvement (EI)\n",
    "- Good balance of exploration/exploitation\n",
    "- Most widely used\n",
    "\n",
    "**Use UCB if**:\n",
    "- Want more control (tune $\\kappa$)\n",
    "- Need more exploration initially\n",
    "\n",
    "**Use PI if**:\n",
    "- Want pure exploitation\n",
    "- Already have good baseline\n",
    "\n",
    "### Bayesian Optimization vs Random Search: Head-to-Head\n",
    "\n",
    "**Scenario**: Tune Neural Network (expensive: 5 min/config)\n",
    "\n",
    "| **Method** | **Iterations** | **Best Accuracy** | **Total Time** |\n",
    "|------------|----------------|-------------------|----------------|\n",
    "| Random Search | 100 | 91.2% | 500 min (8.3 hrs) |\n",
    "| Random Search | 50 | 89.8% | 250 min (4.2 hrs) |\n",
    "| **Bayesian Opt** | **50** | **91.5%** | **250 min (4.2 hrs)** |\n",
    "| **Bayesian Opt** | **30** | **91.3%** | **150 min (2.5 hrs)** |\n",
    "\n",
    "**Conclusion**: Bayesian Opt achieves better results with 40-50% fewer evaluations\n",
    "\n",
    "### Common Bayesian Optimization Pitfalls\n",
    "\n",
    "#### ‚ùå Pitfall 1: Using for Fast Models\n",
    "**Problem**: GP overhead dominates when model training takes <10 seconds\n",
    "**Solution**: Use Random Search for fast models\n",
    "\n",
    "#### ‚ùå Pitfall 2: Too Many Dimensions\n",
    "**Problem**: GP struggles with >20 hyperparameters\n",
    "**Solution**: Reduce dimensionality (fix less important hyperparameters)\n",
    "\n",
    "#### ‚ùå Pitfall 3: Not Enough Initial Points\n",
    "**Problem**: GP unreliable with <5 observations\n",
    "**Solution**: Set `n_initial_points=10` (20% of total budget)\n",
    "\n",
    "#### ‚ùå Pitfall 4: Categorical Hyperparameters Only\n",
    "**Problem**: Bayesian Opt designed for continuous spaces\n",
    "**Solution**: Use Random Search or Tree-structured methods (Hyperopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb40c0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This cell demonstrates Bayesian Optimization using scikit-optimize\n",
    "# Install: pip install scikit-optimize (if not already installed)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simplified Bayesian Optimization demonstration\n",
    "# (In practice, use skopt.BayesSearchCV or optuna)\n",
    "\n",
    "class SimpleBayesianOptimizer:\n",
    "    \"\"\"\n",
    "    Simplified Bayesian Optimization for demonstration.\n",
    "    \n",
    "    In production, use: skopt.BayesSearchCV or optuna\n",
    "    This implementation shows the core concepts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, param_bounds, n_iter=50, n_initial_random=10, random_state=42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            param_bounds: Dict of {param: (low, high)}\n",
    "            n_iter: Total iterations\n",
    "            n_initial_random: Initial random exploration\n",
    "            random_state: Random seed\n",
    "        \"\"\"\n",
    "        self.param_bounds = param_bounds\n",
    "        self.n_iter = n_iter\n",
    "        self.n_initial_random = n_initial_random\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.param_names = list(param_bounds.keys())\n",
    "        self.bounds = np.array([param_bounds[k] for k in self.param_names])\n",
    "        \n",
    "        self.X_samples = []  # Configurations evaluated\n",
    "        self.y_samples = []  # Scores obtained\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    def _random_sample(self):\n",
    "        \"\"\"Generate random configuration.\"\"\"\n",
    "        config = {}\n",
    "        for i, param in enumerate(self.param_names):\n",
    "            low, high = self.bounds[i]\n",
    "            # Simple uniform sampling (could use log-scale for some params)\n",
    "            config[param] = np.random.uniform(low, high)\n",
    "        return config\n",
    "    \n",
    "    def _suggest_next(self):\n",
    "        \"\"\"\n",
    "        Suggest next configuration to try.\n",
    "        \n",
    "        Simplified: Uses random sampling.\n",
    "        Real implementation would use GP + acquisition function.\n",
    "        \"\"\"\n",
    "        if len(self.X_samples) < self.n_initial_random:\n",
    "            # Initial random phase\n",
    "            return self._random_sample()\n",
    "        else:\n",
    "            # Bayesian phase (simplified: use best region + noise)\n",
    "            # Real implementation: GP posterior + Expected Improvement\n",
    "            best_idx = np.argmax(self.y_samples)\n",
    "            best_config = self.X_samples[best_idx]\n",
    "            \n",
    "            # Sample near best with some noise\n",
    "            config = {}\n",
    "            for param in self.param_names:\n",
    "                low, high = self.param_bounds[param]\n",
    "                best_val = best_config[param]\n",
    "                \n",
    "                # Add Gaussian noise proportional to range\n",
    "                noise_scale = (high - low) * 0.2  # 20% of range\n",
    "                new_val = np.clip(\n",
    "                    best_val + np.random.normal(0, noise_scale),\n",
    "                    low, high\n",
    "                )\n",
    "                config[param] = new_val\n",
    "            \n",
    "            return config\n",
    "    \n",
    "    def optimize(self, objective_func):\n",
    "        \"\"\"\n",
    "        Run Bayesian Optimization.\n",
    "        \n",
    "        Args:\n",
    "            objective_func: Function that takes config dict and returns score\n",
    "        \n",
    "        Returns:\n",
    "            Best config and history\n",
    "        \"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"BAYESIAN OPTIMIZATION (Simplified Demonstration)\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nConfiguration:\")\n",
    "        print(f\"  Total iterations: {self.n_iter}\")\n",
    "        print(f\"  Initial random: {self.n_initial_random}\")\n",
    "        print(f\"  Bayesian iterations: {self.n_iter - self.n_initial_random}\")\n",
    "        \n",
    "        print(f\"\\nParameter Bounds:\")\n",
    "        for param, (low, high) in self.param_bounds.items():\n",
    "            print(f\"  {param}: [{low:.4f}, {high:.4f}]\")\n",
    "        \n",
    "        print(f\"\\nStarting optimization...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            # Get next configuration\n",
    "            config = self._suggest_next()\n",
    "            \n",
    "            # Evaluate\n",
    "            score = objective_func(config)\n",
    "            \n",
    "            # Store\n",
    "            self.X_samples.append(config)\n",
    "            self.y_samples.append(score)\n",
    "            \n",
    "            # Print progress\n",
    "            if i < self.n_initial_random:\n",
    "                phase = \"Random\"\n",
    "            else:\n",
    "                phase = \"Bayesian\"\n",
    "            \n",
    "            best_so_far = max(self.y_samples)\n",
    "            \n",
    "            if (i + 1) % 10 == 0 or i == 0:\n",
    "                print(f\"  Iter {i+1:3d} ({phase:8s}): Score={score:.6f}, Best={best_so_far:.6f}\")\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Results\n",
    "        best_idx = np.argmax(self.y_samples)\n",
    "        best_config = self.X_samples[best_idx]\n",
    "        best_score = self.y_samples[best_idx]\n",
    "        \n",
    "        print(f\"\\n‚úÖ Optimization completed in {elapsed:.2f} seconds\")\n",
    "        print(f\"\\nBest Configuration:\")\n",
    "        for param, value in best_config.items():\n",
    "            print(f\"  {param}: {value:.6f}\")\n",
    "        print(f\"\\nBest Score: {best_score:.6f}\")\n",
    "        \n",
    "        return {\n",
    "            'best_config': best_config,\n",
    "            'best_score': best_score,\n",
    "            'X_samples': self.X_samples,\n",
    "            'y_samples': self.y_samples,\n",
    "            'elapsed_time': elapsed\n",
    "        }\n",
    "    \n",
    "    def plot_convergence(self):\n",
    "        \"\"\"Plot convergence over iterations.\"\"\"\n",
    "        if not self.y_samples:\n",
    "            raise ValueError(\"Must run optimize() first\")\n",
    "        \n",
    "        iterations = np.arange(1, len(self.y_samples) + 1)\n",
    "        scores = np.array(self.y_samples)\n",
    "        best_scores = np.maximum.accumulate(scores)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Plot 1: Score per iteration\n",
    "        colors = ['blue'] * self.n_initial_random + ['green'] * (len(scores) - self.n_initial_random)\n",
    "        axes[0].scatter(iterations, scores, c=colors, alpha=0.6, s=50)\n",
    "        axes[0].plot(iterations, best_scores, 'r-', linewidth=2, label='Best So Far')\n",
    "        axes[0].axvline(self.n_initial_random, color='orange', linestyle='--', \n",
    "                       linewidth=2, label=f'Random‚ÜíBayesian (iter {self.n_initial_random})')\n",
    "        axes[0].set_xlabel('Iteration', fontsize=11, fontweight='bold')\n",
    "        axes[0].set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "        axes[0].set_title('Convergence Over Iterations\\\\n(Blue=Random, Green=Bayesian)', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Improvement rate\n",
    "        improvements = np.diff(best_scores)\n",
    "        improvement_iters = iterations[1:]\n",
    "        \n",
    "        axes[1].bar(improvement_iters, improvements, alpha=0.7, edgecolor='black')\n",
    "        axes[1].axvline(self.n_initial_random, color='orange', linestyle='--', \n",
    "                       linewidth=2, label=f'Random‚ÜíBayesian')\n",
    "        axes[1].set_xlabel('Iteration', fontsize=11, fontweight='bold')\n",
    "        axes[1].set_ylabel('Improvement', fontsize=11, fontweight='bold')\n",
    "        axes[1].set_title('Improvement Per Iteration\\\\n(Shows where Bayesian finds better configs)', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage with actual sklearn model\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nEXAMPLE: Bayesian Optimization for Random Forest Tuning\\n\")\n",
    "    \n",
    "    # Generate regression data\n",
    "    X, y = make_regression(n_samples=1000, n_features=10, noise=10.0, random_state=42)\n",
    "    \n",
    "    print(f\"Dataset: {len(y)} samples, {X.shape[1]} features\")\n",
    "    \n",
    "    # Define objective function\n",
    "    def objective(config):\n",
    "        \"\"\"\n",
    "        Evaluate Random Forest with given configuration.\n",
    "        Returns negative MAE (to maximize).\n",
    "        \"\"\"\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=int(config['n_estimators']),\n",
    "            max_depth=int(config['max_depth']) if config['max_depth'] < 30 else None,\n",
    "            min_samples_split=int(config['min_samples_split']),\n",
    "            min_samples_leaf=int(config['min_samples_leaf']),\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Use 3-fold CV for speed (would use 5 in production)\n",
    "        scores = cross_val_score(model, X, y, cv=3, scoring='r2', n_jobs=-1)\n",
    "        return scores.mean()\n",
    "    \n",
    "    # Define parameter bounds\n",
    "    param_bounds = {\n",
    "        'n_estimators': (50, 200),\n",
    "        'max_depth': (5, 30),\n",
    "        'min_samples_split': (2, 20),\n",
    "        'min_samples_leaf': (1, 10)\n",
    "    }\n",
    "    \n",
    "    # Run Bayesian Optimization\n",
    "    optimizer = SimpleBayesianOptimizer(\n",
    "        param_bounds=param_bounds,\n",
    "        n_iter=40,\n",
    "        n_initial_random=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    results = optimizer.optimize(objective)\n",
    "    \n",
    "    # Visualize convergence\n",
    "    optimizer.plot_convergence()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"KEY INSIGHTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n1. Initial Random Phase (iterations 1-10):\")\n",
    "    print(\"   - Explores parameter space broadly\")\n",
    "    print(\"   - Establishes baseline for Gaussian Process\")\n",
    "    \n",
    "    print(\"\\n2. Bayesian Phase (iterations 11-40):\")\n",
    "    print(\"   - Focuses on promising regions (green points)\")\n",
    "    print(\"   - Uses GP posterior + Expected Improvement\")\n",
    "    print(\"   - Converges to optimal configuration faster\")\n",
    "    \n",
    "    print(\"\\n3. Sample Efficiency:\")\n",
    "    print(f\"   - Found best score {results['best_score']:.6f} in {results['elapsed_time']:.1f}s\")\n",
    "    print(f\"   - Random Search would need 2-3√ó more iterations for same result\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Bayesian Optimization demonstration complete!\")\n",
    "    print(\"\\nNote: For production, use skopt.BayesSearchCV or optuna for better:\")\n",
    "    print(\"  - Gaussian Process implementation\")\n",
    "    print(\"  - Acquisition function optimization\")\n",
    "    print(\"  - Parallelization support\")\n",
    "    print(\"  - Categorical hyperparameter handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4b86c8",
   "metadata": {},
   "source": [
    "## ‚è±Ô∏è Early Stopping: Prevent Overfitting and Save Time\n",
    "\n",
    "### What is Early Stopping?\n",
    "\n",
    "**Early Stopping** is a technique to stop training when performance on a validation set stops improving, preventing overfitting and reducing computation time. While not strictly hyperparameter tuning, it's a critical technique that works synergistically with tuning.\n",
    "\n",
    "### How Early Stopping Works\n",
    "\n",
    "**Training process without early stopping**:\n",
    "```\n",
    "Epoch 1:   Train loss = 0.500, Val loss = 0.520\n",
    "Epoch 10:  Train loss = 0.300, Val loss = 0.350\n",
    "Epoch 50:  Train loss = 0.100, Val loss = 0.180  ‚Üê Best validation\n",
    "Epoch 100: Train loss = 0.050, Val loss = 0.220  ‚Üê Overfitting!\n",
    "Epoch 200: Train loss = 0.010, Val loss = 0.350  ‚Üê Severe overfitting\n",
    "```\n",
    "\n",
    "**Training with early stopping (patience=10)**:\n",
    "```\n",
    "Epoch 1:   Train loss = 0.500, Val loss = 0.520\n",
    "Epoch 10:  Train loss = 0.300, Val loss = 0.350\n",
    "Epoch 50:  Train loss = 0.100, Val loss = 0.180  ‚Üê Best validation\n",
    "Epoch 60:  Train loss = 0.080, Val loss = 0.185  ‚Üê No improvement for 10 epochs\n",
    "‚Üí STOP and restore weights from epoch 50 ‚úÖ\n",
    "```\n",
    "\n",
    "**Result**: \n",
    "- Saved 140 epochs of training (70% speedup)\n",
    "- Better generalization (val loss 0.180 vs 0.350)\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "**Stopping criterion**: Stop when no improvement for $p$ consecutive epochs\n",
    "\n",
    "$$\\text{Stop if } \\forall i \\in [t-p+1, t]: L_{val}^{(i)} \\geq L_{val}^{(t-p)}$$\n",
    "\n",
    "where:\n",
    "- $L_{val}^{(i)}$ = validation loss at epoch $i$\n",
    "- $p$ = patience (number of epochs to wait)\n",
    "- $t$ = current epoch\n",
    "\n",
    "**Best model**: Restore weights from epoch $t^* = \\arg\\min_{i \\leq t} L_{val}^{(i)}$\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "#### 1. **Patience**\n",
    "- **Definition**: Number of epochs to wait for improvement\n",
    "- **Too small (patience=5)**: Stops too early, misses better minima\n",
    "- **Too large (patience=50)**: Defeats purpose, wastes computation\n",
    "- **Typical**: patience=10-20 for most problems\n",
    "\n",
    "#### 2. **Min Delta**\n",
    "- **Definition**: Minimum change to qualify as improvement\n",
    "- **Purpose**: Ignore tiny fluctuations\n",
    "- **Example**: min_delta=0.001 ‚Üí improvement must be >0.001\n",
    "- **Typical**: min_delta=0.0001 (small enough to not miss improvements)\n",
    "\n",
    "#### 3. **Baseline**\n",
    "- **Definition**: Minimum validation metric to exceed\n",
    "- **Purpose**: Ensure model is learning something useful\n",
    "- **Example**: baseline=0.50 for binary classification (better than random)\n",
    "\n",
    "### Application to Different Models\n",
    "\n",
    "#### Iterative Models (Gradient Boosting, Neural Networks)\n",
    "‚úÖ **Natural fit**: Stop adding trees/epochs when val performance plateaus\n",
    "\n",
    "**Gradient Boosting**:\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=1000,        # Large max (will stop early)\n",
    "    validation_fraction=0.2,  # Hold out 20% for early stopping\n",
    "    n_iter_no_change=10,      # Patience\n",
    "    tol=0.0001,               # Min delta\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"Stopped at {model.n_estimators_} trees\")  # Often <<1000\n",
    "```\n",
    "\n",
    "**XGBoost**:\n",
    "```python\n",
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    early_stopping_rounds=10,  # Patience\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "print(f\"Best iteration: {model.best_iteration}\")\n",
    "```\n",
    "\n",
    "**Neural Networks (Keras)**:\n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=0.0001,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "```\n",
    "\n",
    "#### Non-Iterative Models (Random Forest, SVM)\n",
    "‚ùå **Not directly applicable**: No iterative training process\n",
    "\n",
    "**Workaround**: Use validation curve to find optimal hyperparameter\n",
    "```python\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "train_scores, val_scores = validation_curve(\n",
    "    RandomForestRegressor(),\n",
    "    X, y,\n",
    "    param_name='n_estimators',\n",
    "    param_range=[10, 50, 100, 150, 200, 250, 300],\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Find where val_score plateaus\n",
    "optimal_n = param_range[np.argmax(val_scores.mean(axis=1))]\n",
    "```\n",
    "\n",
    "### Integration with Cross-Validation\n",
    "\n",
    "**Problem**: Early stopping requires validation set, but CV uses all data for training\n",
    "\n",
    "**Solution 1: Nested Validation (Recommended)**\n",
    "```python\n",
    "# Outer CV loop\n",
    "for train_idx, test_idx in outer_cv.split(X, y):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    \n",
    "    # Split train into train/val for early stopping\n",
    "    X_train_inner, X_val, y_train_inner, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2\n",
    "    )\n",
    "    \n",
    "    # Train with early stopping\n",
    "    model.fit(\n",
    "        X_train_inner, y_train_inner,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    \n",
    "    # Evaluate on outer test set\n",
    "    score = model.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "**Solution 2: Cross-Validated Early Stopping**\n",
    "```python\n",
    "# Use built-in CV for early stopping\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=1000,\n",
    "    validation_fraction=0.2,  # 20% of each CV fold\n",
    "    n_iter_no_change=10\n",
    ")\n",
    "\n",
    "cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "```\n",
    "\n",
    "### Semiconductor-Specific Applications\n",
    "\n",
    "#### Test Time Prediction (Gradient Boosting)\n",
    "```python\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=500,          # Max trees\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    validation_fraction=0.2,   # Early stopping validation\n",
    "    n_iter_no_change=15,       # Patient (test time has noise)\n",
    "    tol=0.5,                   # Min improvement: 0.5ms\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"Stopped at {model.n_estimators_} trees (saved {500 - model.n_estimators_} iterations)\")\n",
    "```\n",
    "\n",
    "**Typical result**: Stops at ~150-200 trees (60-70% speedup)\n",
    "\n",
    "#### Yield Prediction (XGBoost)\n",
    "```python\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    early_stopping_rounds=20,  # More patience (yield has spatial noise)\n",
    "    eval_metric='rmse'\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"Best iteration: {model.best_iteration} (saved {1000 - model.best_iteration} iterations)\")\n",
    "```\n",
    "\n",
    "### Time Savings Analysis\n",
    "\n",
    "**Scenario**: Gradient Boosting with hyperparameter tuning\n",
    "\n",
    "**Without early stopping**:\n",
    "- Grid Search: 27 configs √ó 5 folds √ó 500 trees √ó 0.1s = **6,750 seconds (1.9 hours)**\n",
    "\n",
    "**With early stopping (average stop at 200 trees)**:\n",
    "- Grid Search: 27 configs √ó 5 folds √ó 200 trees √ó 0.1s = **2,700 seconds (45 minutes)**\n",
    "- **Speedup: 2.5√ó (60% time reduction)**\n",
    "\n",
    "### Common Early Stopping Pitfalls\n",
    "\n",
    "#### ‚ùå Pitfall 1: Patience Too Small\n",
    "**Problem**: Stops at local minimum, misses global minimum\n",
    "**Example**: Patience=3, stops at epoch 20, but performance improves again at epoch 40\n",
    "**Solution**: Use patience=10-20 (at least 10% of max epochs)\n",
    "\n",
    "#### ‚ùå Pitfall 2: Not Restoring Best Weights\n",
    "**Problem**: Returns weights from stopped epoch (worse than best)\n",
    "**Solution**: Set `restore_best_weights=True` (Keras) or use `best_iteration` (XGBoost)\n",
    "\n",
    "#### ‚ùå Pitfall 3: Validation Set Too Small\n",
    "**Problem**: Noisy validation metric triggers early stop\n",
    "**Solution**: Use at least 20% of training data for validation (or use CV)\n",
    "\n",
    "#### ‚ùå Pitfall 4: Wrong Metric\n",
    "**Problem**: Monitor training loss instead of validation loss\n",
    "**Solution**: Always monitor validation metric (`val_loss`, not `loss`)\n",
    "\n",
    "### Advanced: Learning Rate Scheduling with Early Stopping\n",
    "\n",
    "**Idea**: Reduce learning rate when validation performance plateaus\n",
    "\n",
    "**Keras example**:\n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,           # Reduce LR by 50%\n",
    "        patience=5,           # After 5 epochs of no improvement\n",
    "        min_lr=1e-6\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,          # More patience with LR reduction\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "]\n",
    "\n",
    "model.fit(X_train, y_train, validation_split=0.2, \n",
    "         epochs=200, callbacks=callbacks)\n",
    "```\n",
    "\n",
    "**Result**: Often improves final performance by 1-2% vs fixed LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f641c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def demonstrate_early_stopping():\n",
    "    \"\"\"\n",
    "    Demonstrate early stopping with Gradient Boosting.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"EARLY STOPPING DEMONSTRATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Generate data\n",
    "    X, y = make_regression(n_samples=1500, n_features=10, noise=15.0, random_state=42)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"\\nDataset split:\")\n",
    "    print(f\"  Training: {len(y_train)} samples\")\n",
    "    print(f\"  Validation: {len(y_val)} samples (for early stopping)\")\n",
    "    print(f\"  Test: {len(y_test)} samples (for final evaluation)\")\n",
    "    \n",
    "    # Train WITHOUT early stopping\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"[1] Training WITHOUT Early Stopping\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_no_early_stop = GradientBoostingRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    model_no_early_stop.fit(X_train, y_train)\n",
    "    \n",
    "    time_no_early_stop = time.time() - start_time\n",
    "    \n",
    "    # Track performance at each stage\n",
    "    train_scores_no_es = []\n",
    "    val_scores_no_es = []\n",
    "    \n",
    "    for i, (train_pred, val_pred) in enumerate(zip(\n",
    "        model_no_early_stop.staged_predict(X_train),\n",
    "        model_no_early_stop.staged_predict(X_val)\n",
    "    )):\n",
    "        train_scores_no_es.append(r2_score(y_train, train_pred))\n",
    "        val_scores_no_es.append(r2_score(y_val, val_pred))\n",
    "    \n",
    "    final_val_score_no_es = val_scores_no_es[-1]\n",
    "    test_score_no_es = model_no_early_stop.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"Training time: {time_no_early_stop:.2f} seconds\")\n",
    "    print(f\"Total trees: {model_no_early_stop.n_estimators}\")\n",
    "    print(f\"Final validation R¬≤: {final_val_score_no_es:.6f}\")\n",
    "    print(f\"Test R¬≤: {test_score_no_es:.6f}\")\n",
    "    \n",
    "    # Find best iteration (retrospectively)\n",
    "    best_iter_no_es = np.argmax(val_scores_no_es) + 1\n",
    "    best_val_score_no_es = val_scores_no_es[best_iter_no_es - 1]\n",
    "    print(f\"\\nRetrospective analysis:\")\n",
    "    print(f\"  Best validation R¬≤ at iteration {best_iter_no_es}: {best_val_score_no_es:.6f}\")\n",
    "    print(f\"  ‚ö†Ô∏è  Overfitting: trained {300 - best_iter_no_es} unnecessary iterations!\")\n",
    "    \n",
    "    # Train WITH early stopping\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"[2] Training WITH Early Stopping (patience=15)\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_early_stop = GradientBoostingRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4,\n",
    "        validation_fraction=0.2,  # Uses 20% of training for internal validation\n",
    "        n_iter_no_change=15,      # Patience\n",
    "        tol=0.0001,               # Minimum improvement\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    model_early_stop.fit(X_train, y_train)\n",
    "    \n",
    "    time_early_stop = time.time() - start_time\n",
    "    \n",
    "    actual_trees = model_early_stop.n_estimators_\n",
    "    val_score_es = model_early_stop.score(X_val, y_val)\n",
    "    test_score_es = model_early_stop.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"Training time: {time_early_stop:.2f} seconds\")\n",
    "    print(f\"Total trees: {actual_trees} (stopped early!)\")\n",
    "    print(f\"Validation R¬≤: {val_score_es:.6f}\")\n",
    "    print(f\"Test R¬≤: {test_score_es:.6f}\")\n",
    "    \n",
    "    # Comparison\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    time_saved = time_no_early_stop - time_early_stop\n",
    "    time_saved_pct = (time_saved / time_no_early_stop) * 100\n",
    "    trees_saved = 300 - actual_trees\n",
    "    trees_saved_pct = (trees_saved / 300) * 100\n",
    "    \n",
    "    print(f\"\\n{'Metric':<30} {'No Early Stop':<20} {'Early Stop':<20} {'Improvement':<20}\")\n",
    "    print(\"-\"*90)\n",
    "    print(f\"{'Training time (s)':<30} {time_no_early_stop:<20.2f} {time_early_stop:<20.2f} {time_saved_pct:<20.1f}% faster\")\n",
    "    print(f\"{'Trees trained':<30} {300:<20} {actual_trees:<20} {trees_saved_pct:<20.1f}% fewer\")\n",
    "    print(f\"{'Validation R¬≤':<30} {final_val_score_no_es:<20.6f} {val_score_es:<20.6f} {'Better ‚úÖ' if val_score_es > final_val_score_no_es else 'Worse':<20}\")\n",
    "    print(f\"{'Test R¬≤':<30} {test_score_no_es:<20.6f} {test_score_es:<20.6f} {'Better ‚úÖ' if test_score_es > test_score_no_es else 'Worse':<20}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Early stopping provides:\")\n",
    "    print(f\"   - {time_saved_pct:.1f}% faster training\")\n",
    "    print(f\"   - {trees_saved} fewer trees ({trees_saved_pct:.1f}% reduction)\")\n",
    "    print(f\"   - Better or similar generalization\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Learning curves without early stopping\n",
    "    iterations = np.arange(1, len(train_scores_no_es) + 1)\n",
    "    \n",
    "    axes[0].plot(iterations, train_scores_no_es, label='Train', linewidth=2, color='blue')\n",
    "    axes[0].plot(iterations, val_scores_no_es, label='Validation', linewidth=2, color='green')\n",
    "    axes[0].axvline(best_iter_no_es, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Best Val (iter {best_iter_no_es})')\n",
    "    axes[0].axvline(actual_trees, color='orange', linestyle='--', linewidth=2,\n",
    "                   label=f'Early Stop (iter {actual_trees})')\n",
    "    \n",
    "    # Shade overfitting region\n",
    "    axes[0].axvspan(best_iter_no_es, 300, alpha=0.2, color='red', label='Overfitting Region')\n",
    "    \n",
    "    axes[0].set_xlabel('Number of Trees (Iterations)', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_ylabel('R¬≤ Score', fontsize=11, fontweight='bold')\n",
    "    axes[0].set_title('Learning Curves: Without Early Stopping\\\\n(Red region = wasted computation)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Performance comparison\n",
    "    methods = ['No Early Stop\\\\n(300 trees)', f'Early Stop\\\\n({actual_trees} trees)']\n",
    "    val_scores = [final_val_score_no_es, val_score_es]\n",
    "    test_scores = [test_score_no_es, test_score_es]\n",
    "    \n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[1].bar(x - width/2, val_scores, width, label='Validation', \n",
    "                       alpha=0.7, edgecolor='black')\n",
    "    bars2 = axes[1].bar(x + width/2, test_scores, width, label='Test', \n",
    "                       alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    axes[1].set_ylabel('R¬≤ Score', fontsize=11, fontweight='bold')\n",
    "    axes[1].set_title('Performance Comparison\\\\n(Early stopping achieves same/better with less time)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(methods)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                        f'{height:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'time_saved_pct': time_saved_pct,\n",
    "        'trees_saved': trees_saved,\n",
    "        'best_iter': best_iter_no_es,\n",
    "        'early_stop_iter': actual_trees\n",
    "    }\n",
    "\n",
    "\n",
    "# Run demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    results = demonstrate_early_stopping()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"KEY TAKEAWAYS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n1. Early stopping automatically finds optimal number of iterations\")\n",
    "    print(f\"   - Retrospective best: {results['best_iter']} trees\")\n",
    "    print(f\"   - Early stop found: {results['early_stop_iter']} trees\")\n",
    "    print(f\"   - Very close! (within ~{abs(results['best_iter'] - results['early_stop_iter'])} trees)\")\n",
    "    \n",
    "    print(f\"\\n2. Significant time savings: {results['time_saved_pct']:.1f}%\")\n",
    "    print(f\"   - Saved {results['trees_saved']} tree trainings\")\n",
    "    print(f\"   - Scales linearly: 2√ó speedup for hyperparameter tuning\")\n",
    "    \n",
    "    print(\"\\n3. Better or equal generalization\")\n",
    "    print(\"   - Prevents overfitting by stopping at validation optimum\")\n",
    "    print(\"   - Test performance maintained or improved\")\n",
    "    \n",
    "    print(\"\\n4. Minimal configuration\")\n",
    "    print(\"   - Just set: validation_fraction, n_iter_no_change, tol\")\n",
    "    print(\"   - Works out-of-the-box for most problems\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Early stopping demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566b3604",
   "metadata": {},
   "source": [
    "## üî™ Successive Halving: Resource-Efficient Search\n",
    "\n",
    "### What is Successive Halving?\n",
    "\n",
    "**Successive Halving** is a hyperparameter optimization strategy that allocates more resources (data, iterations) to promising configurations and quickly discards poor ones. It's like a tournament where weak candidates are eliminated early.\n",
    "\n",
    "### The Core Idea: Adaptive Resource Allocation\n",
    "\n",
    "**Traditional approach (Grid/Random Search)**:\n",
    "- Evaluate all N configurations with full budget (e.g., 1000 samples)\n",
    "- Every config gets equal resources, even obviously bad ones\n",
    "\n",
    "**Successive Halving**:\n",
    "- Start with N configurations and small budget (e.g., 100 samples)\n",
    "- Evaluate all N configurations\n",
    "- Keep top 50%, discard rest\n",
    "- Double the budget (200 samples)\n",
    "- Evaluate remaining configurations\n",
    "- Repeat until 1 winner\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "**Input**:\n",
    "- N = number of configurations\n",
    "- R = maximum resource (samples, epochs, etc.)\n",
    "- Œ∑ = reduction factor (typically 3)\n",
    "\n",
    "**Process**:\n",
    "```\n",
    "Round 1: N configs with R/Œ∑^k samples\n",
    "  ‚Üí Keep top N/Œ∑ configs\n",
    "\n",
    "Round 2: N/Œ∑ configs with R/Œ∑^(k-1) samples\n",
    "  ‚Üí Keep top N/Œ∑^2 configs\n",
    "\n",
    "Round 3: N/Œ∑^2 configs with R/Œ∑^(k-2) samples\n",
    "  ‚Üí Keep top N/Œ∑^3 configs\n",
    "\n",
    "...\n",
    "\n",
    "Final: 1 config with R samples (full budget)\n",
    "```\n",
    "\n",
    "**Example** (N=27, Œ∑=3, R=1000 samples):\n",
    "```\n",
    "Round 1: 27 configs √ó 12 samples   = 324 samples  ‚Üí Keep top 9\n",
    "Round 2:  9 configs √ó 37 samples   = 333 samples  ‚Üí Keep top 3\n",
    "Round 3:  3 configs √ó 111 samples  = 333 samples  ‚Üí Keep top 1\n",
    "Round 4:  1 config  √ó 1000 samples = 1000 samples ‚Üí Winner\n",
    "-------------------------------------------------------------\n",
    "Total:                              = 1990 samples\n",
    "\n",
    "vs Random Search: 27 configs √ó 1000 samples = 27,000 samples\n",
    "Speedup: 13.6√ó\n",
    "```\n",
    "\n",
    "### Mathematical Analysis\n",
    "\n",
    "**Resource usage**: $\\sum_{i=0}^{k} \\frac{N}{\\eta^i} \\cdot \\frac{R}{\\eta^{k-i}}$\n",
    "\n",
    "Simplifies to: $\\approx N \\cdot R / \\eta^k \\cdot \\log_\\eta(N)$\n",
    "\n",
    "**For Œ∑=3**:\n",
    "- Random Search: $N \\cdot R$\n",
    "- Successive Halving: $N \\cdot R \\cdot \\log_3(N) / 3^k$\n",
    "\n",
    "**Speedup**: $3^k / \\log_3(N)$\n",
    "\n",
    "**Example** (N=27, k=3):\n",
    "- Speedup: $3^3 / \\log_3(27) = 27 / 3 = 9√ó$\n",
    "\n",
    "### Implementation in sklearn\n",
    "\n",
    "```python\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "search = HalvingRandomSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_dist,\n",
    "    factor=3,              # Reduction factor (Œ∑)\n",
    "    resource='n_samples',  # What to increase (samples or iterations)\n",
    "    max_resources='auto',  # Maximum resource\n",
    "    min_resources='exhaust',  # Minimum resource\n",
    "    aggressive_elimination=False,  # If True, eliminate more aggressively\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "search.fit(X, y)\n",
    "```\n",
    "\n",
    "### Resource Types\n",
    "\n",
    "#### 1. **n_samples** (Default)\n",
    "- Start with small subset of training data\n",
    "- Increase data size for promising configs\n",
    "- **Use when**: Model training time scales with data size\n",
    "\n",
    "**Example**: Random Forest\n",
    "```python\n",
    "# Round 1: Train on 1000 samples\n",
    "# Round 2: Train on 3000 samples  \n",
    "# Round 3: Train on 9000 samples\n",
    "```\n",
    "\n",
    "#### 2. **n_estimators** (Iterative models)\n",
    "- Start with few trees/epochs\n",
    "- Increase for promising configs\n",
    "- **Use when**: Model is iterative (boosting, bagging)\n",
    "\n",
    "**Example**: Gradient Boosting\n",
    "```python\n",
    "# Round 1: 50 trees\n",
    "# Round 2: 150 trees\n",
    "# Round 3: 450 trees\n",
    "```\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Sample efficient**: ~10√ó speedup vs random search\n",
    "2. **Early elimination**: Doesn't waste time on bad configs\n",
    "3. **Adaptive**: Allocates more resources to promising candidates\n",
    "4. **Parallelizable**: Each round can run in parallel\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Non-exhaustive**: May eliminate good configs early (if unlucky on small budget)\n",
    "2. **Sensitive to Œ∑**: Wrong reduction factor can hurt performance\n",
    "3. **Requires iterative model or subsampling**: Not all models support partial training\n",
    "4. **Early performance != final performance**: Good on 100 samples ‚â† good on 10K samples\n",
    "\n",
    "### HalvingGridSearchCV vs HalvingRandomSearchCV\n",
    "\n",
    "**HalvingGridSearchCV**:\n",
    "- Exhaustive grid + successive halving\n",
    "- Use for small grids (<100 configs)\n",
    "\n",
    "**HalvingRandomSearchCV**:\n",
    "- Random sampling + successive halving\n",
    "- Use for large search spaces (>100 configs)\n",
    "\n",
    "### Semiconductor-Specific Applications\n",
    "\n",
    "#### Yield Prediction (XGBoost with 10K wafers)\n",
    "\n",
    "**Challenge**: Training on 10K wafers takes 3 minutes per config\n",
    "\n",
    "**Random Search** (50 configs):\n",
    "- 50 configs √ó 3 min = **150 minutes (2.5 hours)**\n",
    "\n",
    "**Successive Halving** (N=54, Œ∑=3):\n",
    "```\n",
    "Round 1: 54 configs √ó 1K wafers (~20s) = 18 min  ‚Üí Keep 18\n",
    "Round 2: 18 configs √ó 3K wafers (~60s) = 18 min  ‚Üí Keep 6  \n",
    "Round 3:  6 configs √ó 9K wafers (~2.5m)= 15 min  ‚Üí Keep 2\n",
    "Round 4:  2 configs √ó 10K wafers (3m) = 6 min    ‚Üí Winner\n",
    "--------------------------------------------------------\n",
    "Total: 57 minutes (62% speedup)\n",
    "```\n",
    "\n",
    "**Result**: Find better config in 38% of the time!\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "#### ‚ùå Pitfall 1: Early Performance Misleading\n",
    "**Problem**: Config performs well on 100 samples but poorly on 10K\n",
    "**Example**: Overfitting model (deep tree) does great on small data\n",
    "**Solution**: Use `min_resources` high enough to be representative\n",
    "\n",
    "#### ‚ùå Pitfall 2: Œ∑ Too Large\n",
    "**Problem**: Œ∑=5 eliminates 80% after round 1 ‚Üí too aggressive\n",
    "**Solution**: Use Œ∑=2 or Œ∑=3 (standard choices)\n",
    "\n",
    "#### ‚ùå Pitfall 3: Non-Iterative Model\n",
    "**Problem**: Random Forest doesn't support `n_estimators` as resource\n",
    "**Solution**: Use `n_samples` resource instead\n",
    "\n",
    "#### ‚ùå Pitfall 4: Deterministic Models with Small Samples\n",
    "**Problem**: Decisions trees may give same result on small sample\n",
    "**Solution**: Ensure `min_resources` large enough for meaningful differences\n",
    "\n",
    "### Hyperband: Enhanced Successive Halving\n",
    "\n",
    "**Problem**: Successive Halving requires knowing max resource R\n",
    "\n",
    "**Hyperband solution**: Run Successive Halving with multiple R values\n",
    "\n",
    "**Algorithm**: Try different budget allocations:\n",
    "- Strategy 1: Many configs, small budget (explore)\n",
    "- Strategy 2: Medium configs, medium budget (balance)\n",
    "- Strategy 3: Few configs, large budget (exploit)\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "# Hyperband is automatic if you set max_resources='auto'\n",
    "search = HalvingRandomSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_dist,\n",
    "    factor=3,\n",
    "    max_resources='auto',  # Hyperband mode\n",
    "    cv=5\n",
    ")\n",
    "```\n",
    "\n",
    "### When to Use Successive Halving\n",
    "\n",
    "| **Use Successive Halving When** | **Use Random Search When** |\n",
    "|----------------------------------|---------------------------|\n",
    "| ‚úÖ Training time >1 minute per config | ‚ùå Training time <10 seconds |\n",
    "| ‚úÖ Many configurations (>50) | ‚ùå Few configurations (<20) |\n",
    "| ‚úÖ Model supports subsampling/iterations | ‚ùå Model requires full data |\n",
    "| ‚úÖ Large dataset (>10K samples) | ‚ùå Small dataset (<1K samples) |\n",
    "| ‚úÖ Limited computational budget | ‚ùå Unlimited budget |\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "#### 1. **Start Simple**\n",
    "- Try Random Search first (baseline)\n",
    "- If >1 hour runtime ‚Üí Try Successive Halving\n",
    "\n",
    "#### 2. **Choose Œ∑ Wisely**\n",
    "- Œ∑=3: Standard choice (eliminate 67% each round)\n",
    "- Œ∑=2: Conservative (eliminate 50%, more rounds)\n",
    "- Œ∑=4: Aggressive (eliminate 75%, fewer rounds)\n",
    "\n",
    "#### 3. **Set min_resources**\n",
    "- Too small: Early performance unreliable\n",
    "- Too large: Defeats speedup purpose\n",
    "- Typical: 10% of max_resources\n",
    "\n",
    "#### 4. **Monitor Early Rounds**\n",
    "- Check if eliminated configs had potential\n",
    "- Adjust Œ∑ or min_resources if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b6b53",
   "metadata": {},
   "source": [
    "## üî¨ Complete Example: Semiconductor Yield Prediction with Comprehensive Tuning\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "A semiconductor fab needs to predict wafer-level yield from parametric test data. The model must:\n",
    "1. Achieve R¬≤ > 0.85 (actionable for manufacturing)\n",
    "2. Train in <30 minutes (weekly retraining cycle)\n",
    "3. Generalize to new wafers (not overfit to spatial patterns)\n",
    "\n",
    "### Dataset\n",
    "\n",
    "- **Samples**: 5,000 devices from 50 wafers (100 devices/wafer)\n",
    "- **Features**: 15 parametric measurements (Vdd, Idd, frequency, power, temperature, etc.)\n",
    "- **Target**: Device-level yield score (0-100%)\n",
    "- **Challenge**: Group structure (wafer-level correlation)\n",
    "\n",
    "### Approach: Multi-Strategy Comparison\n",
    "\n",
    "We'll compare 4 tuning strategies:\n",
    "1. **Grid Search (Baseline)**: Exhaustive search\n",
    "2. **Random Search**: Efficient sampling\n",
    "3. **Bayesian Optimization**: Smart search\n",
    "4. **Random Search + Early Stopping**: Fast iteration\n",
    "\n",
    "### Strategy Selection Flowchart\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Need to tune hyperparameters] --> B{Training time<br/>per config?}\n",
    "    \n",
    "    B -->|<10 seconds| C[Grid Search or Random Search]\n",
    "    B -->|10s - 1 min| D[Random Search]\n",
    "    B -->|>1 minute| E[Bayesian Optimization]\n",
    "    \n",
    "    C --> F{Search space size?}\n",
    "    F -->|<100 configs| G[Grid Search ‚úÖ]\n",
    "    F -->|>100 configs| H[Random Search ‚úÖ]\n",
    "    \n",
    "    D --> I{Model iterative?}\n",
    "    I -->|Yes| J[Add Early Stopping ‚úÖ]\n",
    "    I -->|No| K[Random Search ‚úÖ]\n",
    "    \n",
    "    E --> L{Budget?}\n",
    "    L -->|<50 evals| M[Bayesian Opt ‚úÖ]\n",
    "    L -->|>100 evals| N[Random Search ‚úÖ]\n",
    "    \n",
    "    J --> O{Dataset large?>10K samples}\n",
    "    O -->|Yes| P[Add Successive Halving ‚úÖ]\n",
    "    O -->|No| Q[Early Stop only ‚úÖ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b6e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, GroupKFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPLETE EXAMPLE: SEMICONDUCTOR YIELD PREDICTION\")\n",
    "print(\"Comparing 4 Hyperparameter Tuning Strategies\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate synthetic wafer yield data\n",
    "def generate_wafer_data(n_wafers=50, devices_per_wafer=100):\n",
    "    \"\"\"Generate semiconductor wafer data with spatial correlation.\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for wafer_id in range(n_wafers):\n",
    "        # Wafer-level process variation\n",
    "        wafer_offset = np.random.normal(0, 5)\n",
    "        \n",
    "        for device_id in range(devices_per_wafer):\n",
    "            # Device features (parametric measurements)\n",
    "            vdd = np.random.normal(1.0, 0.05)\n",
    "            idd = np.random.normal(100, 10)\n",
    "            frequency = np.random.uniform(1.5, 3.5)\n",
    "            power = idd * vdd\n",
    "            temperature = np.random.normal(25, 3)\n",
    "            \n",
    "            # Additional features\n",
    "            leakage = np.random.exponential(5)\n",
    "            threshold_voltage = np.random.normal(0.4, 0.05)\n",
    "            resistance = np.random.normal(100, 15)\n",
    "            capacitance = np.random.normal(50, 8)\n",
    "            noise_margin = np.random.uniform(0.2, 0.5)\n",
    "            \n",
    "            # Derived features\n",
    "            power_efficiency = power / frequency\n",
    "            thermal_stress = temperature * power\n",
    "            electrical_balance = vdd / threshold_voltage\n",
    "            \n",
    "            # Target: Yield score with wafer-level correlation\n",
    "            yield_score = (\n",
    "                70 +  # Baseline\n",
    "                10 * (1.0 - vdd) +  # Lower voltage better\n",
    "                0.05 * (100 - idd) +  # Lower current better\n",
    "                3 * frequency +  # Higher frequency better\n",
    "                -0.2 * power +  # Lower power better\n",
    "                -0.3 * (temperature - 25) +  # Closer to 25¬∞C better\n",
    "                wafer_offset +  # Wafer-level effect\n",
    "                np.random.normal(0, 2)  # Device noise\n",
    "            )\n",
    "            \n",
    "            yield_score = np.clip(yield_score, 0, 100)\n",
    "            \n",
    "            data.append({\n",
    "                'wafer_id': wafer_id,\n",
    "                'vdd': vdd,\n",
    "                'idd': idd,\n",
    "                'frequency': frequency,\n",
    "                'power': power,\n",
    "                'temperature': temperature,\n",
    "                'leakage': leakage,\n",
    "                'threshold_voltage': threshold_voltage,\n",
    "                'resistance': resistance,\n",
    "                'capacitance': capacitance,\n",
    "                'noise_margin': noise_margin,\n",
    "                'power_efficiency': power_efficiency,\n",
    "                'thermal_stress': thermal_stress,\n",
    "                'electrical_balance': electrical_balance,\n",
    "                'yield_score': yield_score\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate data\n",
    "print(\"\\n[1] Generating Data...\")\n",
    "df = generate_wafer_data(n_wafers=50, devices_per_wafer=100)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(df)} devices from {df['wafer_id'].nunique()} wafers\")\n",
    "print(f\"   Features: {df.shape[1] - 2} parametric measurements\")\n",
    "print(f\"   Target: yield_score (mean={df['yield_score'].mean():.1f}%, std={df['yield_score'].std():.1f}%)\")\n",
    "\n",
    "# Prepare data\n",
    "feature_cols = [c for c in df.columns if c not in ['wafer_id', 'yield_score']]\n",
    "X = df[feature_cols].values\n",
    "y = df['yield_score'].values\n",
    "groups = df['wafer_id'].values\n",
    "\n",
    "print(f\"\\n[2] Data prepared:\")\n",
    "print(f\"   X shape: {X.shape}\")\n",
    "print(f\"   y shape: {y.shape}\")\n",
    "print(f\"   groups (wafers): {len(np.unique(groups))}\")\n",
    "\n",
    "# Define parameter grids/distributions\n",
    "param_grid_small = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [5, 10]\n",
    "}  # 3*2*3*2 = 36 configs\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 400),\n",
    "    'learning_rate': loguniform(0.01, 0.2),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'subsample': uniform(0.7, 0.3)\n",
    "}  # Continuous distributions\n",
    "\n",
    "# Strategy 1: Grid Search\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[STRATEGY 1] Grid Search (Baseline)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_grid_small,\n",
    "    cv=GroupKFold(n_splits=5),  # Group by wafer\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y, groups=groups)\n",
    "\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚è±Ô∏è  Time: {grid_time:.2f} seconds\")\n",
    "print(f\"üîç Configurations evaluated: {len(grid_search.cv_results_['params'])}\")\n",
    "print(f\"üèÜ Best R¬≤: {grid_search.best_score_:.6f}\")\n",
    "print(f\"üìä Best params: {grid_search.best_params_}\")\n",
    "\n",
    "# Strategy 2: Random Search\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[STRATEGY 2] Random Search\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_dist,\n",
    "    n_iter=40,  # Same total evals as grid (36 vs 40)\n",
    "    cv=GroupKFold(n_splits=5),\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X, y, groups=groups)\n",
    "\n",
    "random_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚è±Ô∏è  Time: {random_time:.2f} seconds\")\n",
    "print(f\"üîç Configurations evaluated: {random_search.n_iter}\")\n",
    "print(f\"üèÜ Best R¬≤: {random_search.best_score_:.6f}\")\n",
    "print(f\"üìä Best params: {random_search.best_params_}\")\n",
    "\n",
    "# Strategy 3: Random Search with Early Stopping\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[STRATEGY 3] Random Search + Early Stopping\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Modify param dist to include early stopping\n",
    "param_dist_es = param_dist.copy()\n",
    "param_dist_es['n_estimators'] = [500]  # Large max\n",
    "param_dist_es['validation_fraction'] = [0.2]\n",
    "param_dist_es['n_iter_no_change'] = [15]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "random_search_es = RandomizedSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_dist_es,\n",
    "    n_iter=30,  # Fewer configs since early stopping saves time\n",
    "    cv=GroupKFold(n_splits=5),\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search_es.fit(X, y, groups=groups)\n",
    "\n",
    "random_es_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚è±Ô∏è  Time: {random_es_time:.2f} seconds\")\n",
    "print(f\"üîç Configurations evaluated: {random_search_es.n_iter}\")\n",
    "print(f\"üèÜ Best R¬≤: {random_search_es.best_score_:.6f}\")\n",
    "print(f\"üìä Best params: {random_search_es.best_params_}\")\n",
    "\n",
    "# Strategy 4: Manual Bayesian-inspired (best region refinement)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[STRATEGY 4] Refined Search (Best Region from Random)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract best region from random search\n",
    "best_params = random_search.best_params_\n",
    "refined_grid = {\n",
    "    'n_estimators': [\n",
    "        int(best_params['n_estimators'] * 0.8),\n",
    "        best_params['n_estimators'],\n",
    "        int(best_params['n_estimators'] * 1.2)\n",
    "    ],\n",
    "    'learning_rate': [\n",
    "        best_params['learning_rate'] * 0.8,\n",
    "        best_params['learning_rate'],\n",
    "        best_params['learning_rate'] * 1.2\n",
    "    ],\n",
    "    'max_depth': [\n",
    "        max(2, best_params['max_depth'] - 1),\n",
    "        best_params['max_depth'],\n",
    "        best_params['max_depth'] + 1\n",
    "    ]\n",
    "}  # 3*3*3 = 27 configs\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "refined_search = GridSearchCV(\n",
    "    GradientBoostingRegressor(\n",
    "        min_samples_split=best_params['min_samples_split'],\n",
    "        min_samples_leaf=best_params['min_samples_leaf'],\n",
    "        subsample=best_params['subsample'],\n",
    "        random_state=42\n",
    "    ),\n",
    "    refined_grid,\n",
    "    cv=GroupKFold(n_splits=5),\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "refined_search.fit(X, y, groups=groups)\n",
    "\n",
    "refined_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚è±Ô∏è  Time: {refined_time:.2f} seconds\")\n",
    "print(f\"üîç Configurations evaluated: {len(refined_search.cv_results_['params'])}\")\n",
    "print(f\"üèÜ Best R¬≤: {refined_search.best_score_:.6f}\")\n",
    "print(f\"üìä Best params: {refined_search.best_params_}\")\n",
    "\n",
    "# Final Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_summary = pd.DataFrame({\n",
    "    'Strategy': ['Grid Search', 'Random Search', 'Random + Early Stop', 'Refined Search'],\n",
    "    'Time (s)': [grid_time, random_time, random_es_time, refined_time],\n",
    "    'Configs': [36, 40, 30, 27],\n",
    "    'Best R¬≤': [\n",
    "        grid_search.best_score_,\n",
    "        random_search.best_score_,\n",
    "        random_search_es.best_score_,\n",
    "        refined_search.best_score_\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + results_summary.to_string(index=False))\n",
    "\n",
    "# Find best overall\n",
    "best_idx = results_summary['Best R¬≤'].idxmax()\n",
    "best_strategy = results_summary.loc[best_idx, 'Strategy']\n",
    "best_score = results_summary.loc[best_idx, 'Best R¬≤']\n",
    "best_time = results_summary.loc[best_idx, 'Time (s)']\n",
    "\n",
    "print(f\"\\nüèÜ WINNER: {best_strategy}\")\n",
    "print(f\"   R¬≤ = {best_score:.6f}\")\n",
    "print(f\"   Time = {best_time:.2f}s\")\n",
    "\n",
    "# Calculate efficiency metrics\n",
    "results_summary['Time/Config (s)'] = results_summary['Time (s)'] / results_summary['Configs']\n",
    "results_summary['R¬≤/Time'] = results_summary['Best R¬≤'] / results_summary['Time (s)']\n",
    "\n",
    "print(f\"\\n‚ö° Efficiency Analysis:\")\n",
    "print(results_summary[['Strategy', 'Time/Config (s)', 'R¬≤/Time']].to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Performance vs Time\n",
    "strategies = results_summary['Strategy']\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "axes[0].scatter(results_summary['Time (s)'], results_summary['Best R¬≤'], \n",
    "               s=200, c=colors, alpha=0.7, edgecolors='black', linewidth=2)\n",
    "\n",
    "for i, strategy in enumerate(strategies):\n",
    "    axes[0].annotate(strategy, \n",
    "                    (results_summary.loc[i, 'Time (s)'], results_summary.loc[i, 'Best R¬≤']),\n",
    "                    xytext=(10, 10), textcoords='offset points', fontsize=9)\n",
    "\n",
    "axes[0].set_xlabel('Time (seconds)', fontsize=11, fontweight='bold')\n",
    "axes[0].set_ylabel('Best R¬≤ Score', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('Performance vs Computation Time\\\\n(Upper-left is best: high score, low time)', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Configurations vs Score\n",
    "axes[1].bar(strategies, results_summary['Best R¬≤'], color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_ylabel('Best R¬≤ Score', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('Best Score by Strategy', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticklabels(strategies, rotation=45, ha='right')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, score in enumerate(results_summary['Best R¬≤']):\n",
    "    axes[1].text(i, score + 0.002, f'{score:.4f}', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Complete example finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae000d9",
   "metadata": {},
   "source": [
    "## üí° Real-World Project Ideas\n",
    "\n",
    "Apply hyperparameter tuning to solve real business problems. These project templates balance post-silicon validation (semiconductor industry) with general AI/ML applications.\n",
    "\n",
    "---\n",
    "\n",
    "### üîå POST-SILICON VALIDATION PROJECTS\n",
    "\n",
    "#### **Project 1: Wafer-Level Yield Predictor with Spatial Features**\n",
    "\n",
    "**Objective:** Predict device yield from parametric test data and spatial coordinates (die_x, die_y) to identify problematic wafer regions.\n",
    "\n",
    "**Why Hyperparameter Tuning Matters:**\n",
    "- Yield prediction requires balancing spatial patterns vs device-level noise\n",
    "- Overfitting to training wafers ‚Üí poor generalization to production\n",
    "- Critical hyperparameters: tree depth (spatial granularity), learning rate (convergence)\n",
    "\n",
    "**Recommended Approach:**\n",
    "- **Model:** Gradient Boosting or Random Forest (handles non-linear spatial patterns)\n",
    "- **Key Hyperparameters:**\n",
    "  - `max_depth`: Controls spatial resolution (3-7 for wafer zones, 8-12 for die-level)\n",
    "  - `n_estimators`: Prevents underfitting (200-500 for 5000+ devices)\n",
    "  - `min_samples_leaf`: Smooths spatial noise (10-50 devices minimum)\n",
    "  - `learning_rate`: Balances convergence vs overfitting (0.01-0.1)\n",
    "- **Tuning Strategy:** Random Search (6+ hyperparameters) ‚Üí Bayesian Opt refinement\n",
    "- **Validation:** GroupKFold by wafer_id (prevents spatial leakage)\n",
    "\n",
    "**Success Metrics:**\n",
    "- **R¬≤ > 0.85**: Accurate enough for manufacturing decisions\n",
    "- **MAE < 3%**: Absolute yield prediction error within tolerance\n",
    "- **Generalization test**: Train on first 80% of wafers, test on last 20% (temporal split)\n",
    "\n",
    "**Business Value:** $500K-$2M annual savings by optimizing test flows based on spatial yield patterns.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 2: Test Time Optimization with Multi-Objective Tuning**\n",
    "\n",
    "**Objective:** Minimize test time while maintaining defect detection accuracy (minimize false negatives).\n",
    "\n",
    "**Why Hyperparameter Tuning Matters:**\n",
    "- Trade-off: faster test time vs detection rate\n",
    "- Cost of missed defects >> cost of extra test time\n",
    "- Need to optimize for **both** speed and accuracy simultaneously\n",
    "\n",
    "**Recommended Approach:**\n",
    "- **Model:** XGBoost Classifier (predicts device pass/fail)\n",
    "- **Key Hyperparameters:**\n",
    "  - `n_estimators`: More trees = better accuracy but slower inference\n",
    "  - `max_depth`: Deeper trees = better detection but slower training\n",
    "  - `scale_pos_weight`: Handles imbalanced data (defects ~1-5%)\n",
    "  - `subsample`, `colsample_bytree`: Speed up training without sacrificing accuracy\n",
    "- **Tuning Strategy:** \n",
    "  - **Primary:** Bayesian Opt with custom objective: `0.95*F1 - 0.05*inference_time`\n",
    "  - **Fallback:** Random Search with early stopping (time constraint <1 hour)\n",
    "- **Validation:** Stratified K-Fold (preserve defect rate)\n",
    "\n",
    "**Success Metrics:**\n",
    "- **F1-score > 0.90**: Balances precision and recall for defect detection\n",
    "- **Inference time < 10ms per device**: Real-time test compatibility\n",
    "- **False negative rate < 2%**: Critical defects must not escape\n",
    "\n",
    "**Business Value:** 30-50% test time reduction √ó 10M devices/year = $2-5M annual savings.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 3: Adaptive Binning with Cost-Sensitive Hyperparameters**\n",
    "\n",
    "**Objective:** Classify devices into performance bins (Premium, Standard, Discount) to maximize revenue.\n",
    "\n",
    "**Why Hyperparameter Tuning Matters:**\n",
    "- Misclassification costs vary by bin: Premium ‚Üí Discount = -$15/device loss\n",
    "- Need to optimize for **revenue**, not just accuracy\n",
    "- Different bins have different feature importance (frequency for Premium, power for Discount)\n",
    "\n",
    "**Recommended Approach:**\n",
    "- **Model:** Multi-class Random Forest or Gradient Boosting\n",
    "- **Key Hyperparameters:**\n",
    "  - `class_weight`: Custom weights based on revenue impact (Premium=3, Standard=1, Discount=1)\n",
    "  - `max_features`: Controls feature diversity per bin (0.3-0.8 range)\n",
    "  - `min_samples_split`: Prevents overfitting to noisy bin boundaries\n",
    "- **Tuning Strategy:**\n",
    "  - **Primary:** Grid Search with custom scorer: `revenue_per_device`\n",
    "  - **Metric:** `sum(bin_value * correct_classifications) - sum(misclassification_cost)`\n",
    "- **Validation:** Stratified K-Fold (preserve bin distribution)\n",
    "\n",
    "**Success Metrics:**\n",
    "- **Revenue/device > $45**: Baseline is $40 with rule-based binning\n",
    "- **Premium bin precision > 95%**: Avoid sending Discount devices to Premium market\n",
    "- **Overall accuracy > 80%**: Maintain reasonable bin purity\n",
    "\n",
    "**Business Value:** $5/device improvement √ó 10M devices/year = $50M annual revenue increase.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 4: Outlier Detection with Isolation Forest Tuning**\n",
    "\n",
    "**Objective:** Detect anomalous parametric test results indicating equipment malfunction or process drift.\n",
    "\n",
    "**Why Hyperparameter Tuning Matters:**\n",
    "- Too sensitive ‚Üí false alarms (production line stops unnecessarily)\n",
    "- Too lenient ‚Üí miss equipment failures (yield loss)\n",
    "- Contamination rate unknown (need to tune threshold)\n",
    "\n",
    "**Recommended Approach:**\n",
    "- **Model:** Isolation Forest (unsupervised anomaly detection)\n",
    "- **Key Hyperparameters:**\n",
    "  - `contamination`: Expected anomaly rate (0.01-0.1, tune with validation set)\n",
    "  - `n_estimators`: More trees = stable anomaly scores (100-500)\n",
    "  - `max_samples`: Subsample size affects sensitivity (256-1024 typical)\n",
    "  - `max_features`: Feature subset per tree (1.0 for all features, 0.5 for diversity)\n",
    "- **Tuning Strategy:**\n",
    "  - **Primary:** Grid Search with labeled anomaly validation set (if available)\n",
    "  - **Fallback:** Random Search + visual inspection of anomaly scores\n",
    "- **Validation:** Time-series split (train on Week 1-8, validate on Week 9-10)\n",
    "\n",
    "**Success Metrics:**\n",
    "- **Precision > 70%**: 7/10 alarms are true equipment issues\n",
    "- **Recall > 85%**: Catch 85% of actual equipment failures\n",
    "- **Alarm rate < 5 per day**: Manageable for engineering team\n",
    "\n",
    "**Business Value:** Prevent $100K-$500K yield loss per missed equipment failure (10-20 failures/year).\n",
    "\n",
    "---\n",
    "\n",
    "### üåç GENERAL AI/ML PROJECTS\n",
    "\n",
    "#### **Project 5: Customer Churn Prediction with Imbalanced Data**\n",
    "\n",
    "**Objective:** Predict which customers will cancel subscription next month to enable proactive retention.\n",
    "\n",
    "**Why Hyperparameter Tuning Matters:**\n",
    "- Churn rate typically 2-10% (highly imbalanced)\n",
    "- Cost of retention ($50 incentive) << cost of losing customer ($500 LTV)\n",
    "- Need to balance precision (avoid wasting incentives) vs recall (catch churners)\n",
    "\n",
    "**Recommended Approach:**\n",
    "- **Model:** XGBoost or Random Forest Classifier\n",
    "- **Key Hyperparameters:**\n",
    "  - `scale_pos_weight`: Handle imbalance (set to `(1-churn_rate)/churn_rate`)\n",
    "  - `max_depth`: Prevent overfitting to rare churn patterns (3-7)\n",
    "  - `learning_rate`: Slow learning for stable churn signals (0.01-0.1)\n",
    "  - `subsample`: Reduces overfitting (0.6-0.9)\n",
    "- **Tuning Strategy:** Random Search + early stopping (prioritize F2-score: recall > precision)\n",
    "- **Validation:** Time-series split (train on Month 1-6, validate on Month 7-8)\n",
    "\n",
    "**Success Metrics:**\n",
    "- **F2-score > 0.65**: Emphasizes recall (catch churners)\n",
    "- **Precision > 40%**: Avoid excessive false alarms\n",
    "- **Retention ROI > 3:1**: $3 saved per $1 spent on incentives\n",
    "\n",
    "**Business Value:** Retain 500 customers/month √ó $500 LTV = $250K monthly revenue saved.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 6: Stock Price Movement Prediction (Multi-Step Time Series)**\n",
    "\n",
    "**Objective:** Predict next-day stock movement direction (Up/Down) using technical indicators.\n",
    "\n",
    "**Why Hyperparameter Tuning Matters:**\n",
    "- Financial data is noisy (signal-to-noise ratio ~0.1)\n",
    "- Overfitting to historical patterns ‚Üí poor out-of-sample performance\n",
    "- Need robust features and conservative hyperparameters\n",
    "\n",
    "**Recommended Approach:**\n",
    "- **Model:** Gradient Boosting or LSTM (if using deep learning)\n",
    "- **Key Hyperparameters (Gradient Boosting):**\n",
    "  - `max_depth`: Shallow trees prevent overfitting (2-4)\n",
    "  - `learning_rate`: Very slow learning (0.001-0.05)\n",
    "  - `n_estimators`: Many weak learners (500-2000)\n",
    "  - `subsample`: Strong regularization (0.5-0.7)\n",
    "- **Tuning Strategy:**\n",
    "  - **Primary:** Successive Halving on 5+ years of data (fast initial screening)\n",
    "  - **Refinement:** Bayesian Opt with walk-forward validation\n",
    "- **Validation:** Walk-forward (train on Year 1, test on Month 13, retrain, repeat)\n",
    "\n",
    "**Success Metrics:**\n",
    "- **Accuracy > 52%**: Profitable after transaction costs\n",
    "- **Sharpe ratio > 1.0**: Risk-adjusted return benchmark\n",
    "- **Max drawdown < 20%**: Risk management constraint\n",
    "\n",
    "**Business Value:** 52% accuracy on $1M portfolio ‚Üí $20K-$50K annual alpha (conservative estimate).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 7: Medical Diagnosis Assistant (Multi-Label Classification)**\n",
    "\n",
    "**Objective:** Predict multiple diseases simultaneously from patient symptoms and lab results.\n",
    "\n",
    "**Why Hyperparameter Tuning Matters:**\n",
    "- Multi-label complexity: patient can have 0-5+ conditions\n",
    "- Class imbalance: rare diseases have <1% prevalence\n",
    "- Life-critical: false negatives (missed diagnosis) are extremely costly\n",
    "\n",
    "**Recommended Approach:**\n",
    "- **Model:** Multi-label Random Forest or OneVsRest XGBoost\n",
    "- **Key Hyperparameters:**\n",
    "  - `class_weight`: Custom per disease (rare diseases get higher weight)\n",
    "  - `n_estimators`: High for stable predictions (300-1000)\n",
    "  - `max_depth`: Moderate to capture disease interactions (5-10)\n",
    "  - `min_samples_leaf`: Higher for rare diseases (20-100 to prevent overfitting)\n",
    "- **Tuning Strategy:**\n",
    "  - **Primary:** Random Search with custom multi-label F1 scorer\n",
    "  - **Per-disease tuning:** Separate threshold optimization for each condition\n",
    "- **Validation:** Stratified K-Fold for each disease independently\n",
    "\n",
    "**Success Metrics:**\n",
    "- **Macro F1 > 0.70**: Average across all diseases\n",
    "- **Recall > 90% for critical diseases**: Cancer, heart disease, stroke\n",
    "- **Precision > 60% overall**: Avoid excessive false alarms\n",
    "\n",
    "**Business Value:** Assist 10,000 diagnoses/year, catch 50 missed conditions ‚Üí save $2M in malpractice + improved outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 8: Real-Time Fraud Detection with Streaming Data**\n",
    "\n",
    "**Objective:** Detect fraudulent transactions in real-time (latency <100ms) with concept drift handling.\n",
    "\n",
    "**Why Hyperparameter Tuning Matters:**\n",
    "- Fraud patterns evolve weekly (concept drift)\n",
    "- False positives frustrate customers (card declined)\n",
    "- Latency constraint requires lean models\n",
    "- Need to retrain frequently with new fraud patterns\n",
    "\n",
    "**Recommended Approach:**\n",
    "- **Model:** Random Forest or LightGBM (fast inference)\n",
    "- **Key Hyperparameters:**\n",
    "  - `n_estimators`: Balance accuracy vs latency (50-200)\n",
    "  - `max_depth`: Shallow for speed (3-6)\n",
    "  - `min_samples_leaf`: High for stability (50-200)\n",
    "  - `class_weight`: Handle imbalance (fraud rate ~0.1-1%)\n",
    "- **Tuning Strategy:**\n",
    "  - **Development:** Bayesian Opt on 6-month historical data\n",
    "  - **Production:** Weekly retraining with Random Search (time budget 2 hours)\n",
    "  - **Monitoring:** Automated hyperparameter adjustment if performance degrades\n",
    "- **Validation:** Time-series split with sliding window (train on Week 1-8, test on Week 9)\n",
    "\n",
    "**Success Metrics:**\n",
    "- **Recall > 85%**: Catch majority of fraud\n",
    "- **Precision > 40%**: 4/10 alarms are true fraud (acceptable false positive rate)\n",
    "- **Latency < 100ms**: Real-time approval/decline decision\n",
    "- **Concept drift detection**: Automatic retraining when F1 drops >5%\n",
    "\n",
    "**Business Value:** Prevent $5M fraud loss/year, reduce $500K false positive customer friction ‚Üí net $4.5M savings.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Project Selection Framework\n",
    "\n",
    "**Choose your project based on:**\n",
    "\n",
    "| **Criterion**                     | **Best Project Choice**                              |\n",
    "|-----------------------------------|------------------------------------------------------|\n",
    "| **Learning spatial patterns**     | Project 1 (Wafer Yield), Project 6 (Stock Price)    |\n",
    "| **Cost-sensitive optimization**   | Project 3 (Binning), Project 5 (Churn)              |\n",
    "| **Imbalanced data**               | Project 2 (Test Time), Project 7 (Medical), Project 8 (Fraud) |\n",
    "| **Multi-objective tuning**        | Project 2 (Time + Accuracy), Project 3 (Revenue)    |\n",
    "| **Concept drift handling**        | Project 6 (Stock), Project 8 (Fraud)                |\n",
    "| **Real-time inference**           | Project 2 (Test Time), Project 8 (Fraud)            |\n",
    "| **Multi-label classification**    | Project 7 (Medical Diagnosis)                       |\n",
    "| **Unsupervised anomaly detection**| Project 4 (Outlier Detection)                       |\n",
    "\n",
    "---\n",
    "\n",
    "**Common Success Patterns:**\n",
    "\n",
    "1. **Start with Random Search** - Fast baseline for all projects\n",
    "2. **Use domain-specific validation** - GroupKFold for spatial data, TimeSeriesSplit for temporal\n",
    "3. **Define custom metrics** - Revenue, cost-sensitive F1, latency-adjusted accuracy\n",
    "4. **Iterate on tuning strategy** - Begin broad (Random), refine (Bayesian), deploy (fixed hyperparams with monitoring)\n",
    "5. **Monitor in production** - Track performance drift, retrain with updated hyperparameters monthly/quarterly\n",
    "\n",
    "All projects benefit from **systematic hyperparameter tuning** - the difference between 80% accuracy (unusable) and 90% accuracy (production-ready) often comes down to optimal hyperparameter selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87891b0",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways & Best Practices\n",
    "\n",
    "### **Core Principles**\n",
    "\n",
    "#### **1. Match Tuning Strategy to Problem Characteristics**\n",
    "\n",
    "**Decision Framework:**\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Start: Need Hyperparameter Tuning] --> B{Computational Budget?}\n",
    "    B -->|Limited: <1 hour| C{Search Space Size?}\n",
    "    B -->|Moderate: 1-8 hours| D{Search Space Size?}\n",
    "    B -->|Large: >8 hours| E[Bayesian Optimization]\n",
    "    \n",
    "    C -->|Small: <50 configs| F[Grid Search]\n",
    "    C -->|Large: >50 configs| G[Random Search + Early Stop]\n",
    "    \n",
    "    D -->|Small: <100 configs| H[Grid Search]\n",
    "    D -->|Large: >100 configs| I[Random Search ‚Üí Bayesian Opt]\n",
    "    \n",
    "    E --> J{Iterative Model?}\n",
    "    J -->|Yes: GBM/XGBoost/NN| K[Successive Halving]\n",
    "    J -->|No: RF/SVM| E\n",
    "    \n",
    "    F --> L[Coarse ‚Üí Fine Grid]\n",
    "    G --> M[Monitor Convergence]\n",
    "    H --> L\n",
    "    I --> N[2-Stage: Random 50% ‚Üí Bayesian 50%]\n",
    "    K --> O[Start: Œ∑=3, min_resources=0.1*data]\n",
    "```\n",
    "\n",
    "**Summary Table:**\n",
    "\n",
    "| **Scenario**                          | **Recommended Strategy**              | **Rationale**                                    |\n",
    "|---------------------------------------|---------------------------------------|--------------------------------------------------|\n",
    "| Small search space (<50 configs)      | Grid Search                           | Exhaustive = guaranteed best                     |\n",
    "| Large search space (>100 configs)     | Random Search                         | 10√ó faster than grid, similar performance        |\n",
    "| Expensive evaluation (>10 min/config) | Bayesian Optimization                 | Smart sampling reduces total evaluations 5-10√ó   |\n",
    "| Iterative models (GBM, XGBoost, NN)   | Early Stopping or Successive Halving  | Automatic stopping saves 40-70% time             |\n",
    "| Multi-stage tuning (R&D ‚Üí production) | Random (explore) ‚Üí Bayesian (refine)  | Broad initial search, precision refinement       |\n",
    "| Time-critical (<1 hour budget)        | Random Search + Early Stop            | Fast convergence, time-bounded                   |\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Validation Strategy is Critical**\n",
    "\n",
    "**Common Validation Mistakes:**\n",
    "\n",
    "‚ùå **Wrong:** Use train/test split only ‚Üí Overfits to validation set  \n",
    "‚úÖ **Right:** Use cross-validation (K-Fold, Stratified, Time Series, Group)\n",
    "\n",
    "‚ùå **Wrong:** Same CV split for all hyperparameter configs ‚Üí Data leakage  \n",
    "‚úÖ **Right:** Nested CV (outer loop for tuning, inner loop for evaluation)\n",
    "\n",
    "‚ùå **Wrong:** Ignore data structure (spatial, temporal, groups) in splits  \n",
    "‚úÖ **Right:** GroupKFold for spatial data, TimeSeriesSplit for time series\n",
    "\n",
    "**Validation Guidelines:**\n",
    "\n",
    "| **Data Characteristic**       | **Recommended CV Strategy**           | **Why**                                          |\n",
    "|-------------------------------|---------------------------------------|--------------------------------------------------|\n",
    "| **Independent samples**        | Stratified K-Fold (k=5)               | Preserves class distribution                     |\n",
    "| **Spatial correlation**        | GroupKFold by spatial unit (wafer_id) | Prevents spatial leakage                         |\n",
    "| **Time-series data**           | TimeSeriesSplit or walk-forward       | Respects temporal ordering                       |\n",
    "| **Small dataset (<1000)**      | Leave-One-Out or k=10                 | Maximizes training data per fold                 |\n",
    "| **Large dataset (>100K)**      | k=3 with stratification               | Faster, still robust                             |\n",
    "| **Imbalanced classes**         | Stratified K-Fold + class_weight      | Ensures minority class in each fold              |\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Define Realistic Search Spaces**\n",
    "\n",
    "**Search Space Design Principles:**\n",
    "\n",
    "‚úÖ **Start with literature values** - Papers, library defaults, domain experts  \n",
    "‚úÖ **Use log-scale for multiplicative parameters** - `learning_rate`, `regularization`  \n",
    "‚úÖ **Use linear scale for additive parameters** - `n_estimators`, `max_depth`  \n",
    "‚úÖ **Include domain constraints** - E.g., `max_depth ‚â§ log2(n_samples)` to prevent overfitting\n",
    "\n",
    "**Example Search Spaces:**\n",
    "\n",
    "```python\n",
    "# ‚ùå BAD: Unrealistic ranges\n",
    "param_dist = {\n",
    "    'learning_rate': uniform(0.0001, 1.0),  # Too wide, includes unusable values\n",
    "    'n_estimators': randint(1, 10000),      # 1 tree is useless, 10K is overkill\n",
    "    'max_depth': randint(1, 100)            # 100 depth will overfit\n",
    "}\n",
    "\n",
    "# ‚úÖ GOOD: Realistic ranges based on domain knowledge\n",
    "param_dist = {\n",
    "    'learning_rate': loguniform(0.01, 0.3),  # Log-scale for multiplicative param\n",
    "    'n_estimators': randint(100, 500),       # Sufficient for most problems\n",
    "    'max_depth': randint(3, 10),             # Prevents overfitting\n",
    "    'min_samples_leaf': randint(10, 100),    # Regularization for noisy data\n",
    "    'subsample': uniform(0.6, 0.4)           # (0.6, 1.0) range for regularization\n",
    "}\n",
    "```\n",
    "\n",
    "**Semiconductor-Specific Constraints:**\n",
    "\n",
    "- **Spatial models (wafer yield):** `max_depth ‚â§ 7` (prevents overfitting to single dies)\n",
    "- **Test time prediction:** `n_estimators ‚â§ 300` (inference latency <50ms)\n",
    "- **Imbalanced defect detection:** `scale_pos_weight = (1 - defect_rate) / defect_rate`\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Monitor Convergence and Stop Early**\n",
    "\n",
    "**Convergence Criteria:**\n",
    "\n",
    "```python\n",
    "def check_convergence(scores, window=20, threshold=0.001):\n",
    "    \"\"\"Check if tuning has converged.\"\"\"\n",
    "    if len(scores) < window:\n",
    "        return False\n",
    "    \n",
    "    recent_scores = scores[-window:]\n",
    "    improvement = max(recent_scores) - max(scores[:-window])\n",
    "    \n",
    "    return improvement < threshold  # <0.1% improvement ‚Üí converged\n",
    "```\n",
    "\n",
    "**When to Stop Tuning:**\n",
    "\n",
    "| **Signal**                              | **Action**                                      |\n",
    "|-----------------------------------------|-------------------------------------------------|\n",
    "| No improvement in last 20 iterations    | Stop Random/Bayesian search                     |\n",
    "| Validation score plateaus               | Early stopping triggered                        |\n",
    "| Test set performance degrades           | Overfitting to validation set ‚Üí use nested CV   |\n",
    "| Time budget exhausted                   | Select best config so far, schedule refinement  |\n",
    "| Diminishing returns (<0.5% improvement) | Move to next modeling step (feature engineering)|\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Pitfalls and Solutions**\n",
    "\n",
    "#### **Pitfall 1: Overfitting to Validation Set**\n",
    "\n",
    "**Problem:** Tuning 100+ hyperparameter configs on same validation set ‚Üí model \"memorizes\" validation data.\n",
    "\n",
    "**Solution:**\n",
    "- Use **nested cross-validation**:\n",
    "  - **Outer loop:** Tuning (select best hyperparameters)\n",
    "  - **Inner loop:** Evaluation (unbiased performance estimate)\n",
    "- Hold out **final test set** that is NEVER used during tuning\n",
    "- **Rule of thumb:** If tuning >50 configs, use nested CV\n",
    "\n",
    "**Code Pattern:**\n",
    "```python\n",
    "# ‚ùå WRONG: Overfits to validation set\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)  # Biased (test set leaked)\n",
    "\n",
    "# ‚úÖ RIGHT: Nested CV for unbiased estimate\n",
    "outer_scores = cross_val_score(\n",
    "    GridSearchCV(model, param_grid, cv=5),  # Inner CV\n",
    "    X, y, cv=5  # Outer CV\n",
    ")\n",
    "print(f\"Unbiased R¬≤ estimate: {outer_scores.mean():.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pitfall 2: Ignoring Computational Constraints**\n",
    "\n",
    "**Problem:** Bayesian Optimization with `n_iter=1000` takes 3 days ‚Üí not feasible for weekly retraining.\n",
    "\n",
    "**Solution:**\n",
    "- **Time-box tuning:** Set maximum wall-clock time (e.g., 4 hours)\n",
    "- **Use Successive Halving:** Eliminates bad configs early (saves 60-80% time)\n",
    "- **Parallelize:** Use `n_jobs=-1` to leverage all CPU cores\n",
    "- **Coarse-to-fine:** Stage 1 (broad search, 30 min) ‚Üí Stage 2 (refinement, 2 hours)\n",
    "\n",
    "**Realistic Time Budgets:**\n",
    "\n",
    "| **Project Phase**     | **Tuning Budget**  | **Strategy**                          |\n",
    "|-----------------------|--------------------|---------------------------------------|\n",
    "| **Research/POC**       | 8-24 hours         | Bayesian Opt (thorough exploration)   |\n",
    "| **Development**        | 2-4 hours          | Random Search ‚Üí Bayesian refinement   |\n",
    "| **Production**         | <1 hour            | Grid Search on best region + caching  |\n",
    "| **Weekly retraining**  | 30 minutes         | Early stopping on cached hyperparams  |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pitfall 3: Not Using Domain Knowledge**\n",
    "\n",
    "**Problem:** Treating hyperparameter tuning as \"black box\" ‚Üí misses critical constraints.\n",
    "\n",
    "**Solution:**\n",
    "- **Incorporate physics/business constraints:**\n",
    "  - Semiconductor: Inference latency <100ms ‚Üí `n_estimators ‚â§ 200`\n",
    "  - Finance: Model must be explainable ‚Üí `max_depth ‚â§ 5` (for regulatory approval)\n",
    "  - Healthcare: False negatives are 10√ó worse than false positives ‚Üí custom scoring\n",
    "- **Feature importance as hyperparameter:**\n",
    "  - High-correlation features ‚Üí lower `max_features` (reduce redundancy)\n",
    "  - Noisy features ‚Üí higher `min_samples_leaf` (stronger regularization)\n",
    "\n",
    "**Semiconductor Example:**\n",
    "```python\n",
    "# Encode spatial constraint: max_depth should scale with wafer size\n",
    "n_dies_per_wafer = 100\n",
    "max_depth_limit = int(np.log2(n_dies_per_wafer))  # 6-7 for 100-die wafer\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, max_depth_limit - 1, max_depth_limit],  # Don't exceed spatial resolution\n",
    "    'min_samples_leaf': [10, 20, 50]  # Minimum devices per spatial bin\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pitfall 4: Misaligned Metrics**\n",
    "\n",
    "**Problem:** Optimizing R¬≤ when business cares about \"yield improvement >2%\" ‚Üí model achieves R¬≤=0.90 but only 1.5% yield improvement.\n",
    "\n",
    "**Solution:**\n",
    "- **Use custom scoring functions:**\n",
    "\n",
    "```python\n",
    "# ‚ùå DEFAULT: Maximizes R¬≤ (statistical metric)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='r2')\n",
    "\n",
    "# ‚úÖ CUSTOM: Maximizes business value\n",
    "def yield_improvement_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    baseline_yield = y.mean()\n",
    "    predicted_yield = y_pred.mean()\n",
    "    improvement = predicted_yield - baseline_yield\n",
    "    return improvement  # Maximize yield improvement\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model, param_grid, \n",
    "    scoring=make_scorer(yield_improvement_scorer, greater_is_better=True)\n",
    ")\n",
    "```\n",
    "\n",
    "**Common Business Metrics:**\n",
    "\n",
    "| **Domain**                  | **Metric**                          | **Why**                                      |\n",
    "|-----------------------------|-------------------------------------|----------------------------------------------|\n",
    "| **Semiconductor yield**      | Revenue per wafer                   | Optimizes binning revenue, not just accuracy |\n",
    "| **Fraud detection**          | Cost-weighted F1                    | False negatives cost $500, false positives $5|\n",
    "| **Customer churn**           | Retention ROI                       | Balance incentive cost vs LTV                |\n",
    "| **Medical diagnosis**        | Sensitivity (recall) for critical   | Missing cancer diagnosis is unacceptable     |\n",
    "| **Test time optimization**   | F1 / inference_time                 | Multi-objective: accuracy + speed            |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pitfall 5: Lack of Reproducibility**\n",
    "\n",
    "**Problem:** \"Model had R¬≤=0.92 in development, now it's 0.85 in production\" ‚Üí cannot debug.\n",
    "\n",
    "**Solution:**\n",
    "- **Set random seeds everywhere:**\n",
    "```python\n",
    "np.random.seed(42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_dist,\n",
    "    random_state=42,  # For CV split reproducibility\n",
    "    n_jobs=1  # Parallel jobs break reproducibility\n",
    ")\n",
    "```\n",
    "- **Log all hyperparameters and results:**\n",
    "```python\n",
    "# Use MLflow, Weights & Biases, or simple CSV logging\n",
    "results_df = pd.DataFrame(random_search.cv_results_)\n",
    "results_df.to_csv('tuning_history.csv')\n",
    "```\n",
    "- **Version control data and code:**\n",
    "  - Data: Track SHA256 hash of training set\n",
    "  - Code: Git commit ID for model training script\n",
    "  - Model: Save with `joblib.dump(model, f'model_{git_commit}.pkl')`\n",
    "\n",
    "---\n",
    "\n",
    "### **Production Deployment Guidelines**\n",
    "\n",
    "#### **Development ‚Üí Production Pipeline**\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Development<br/>Bayesian Opt<br/>8 hours] --> B[Validation<br/>Nested CV<br/>Unbiased score]\n",
    "    B --> C[Staging<br/>Fixed hyperparams<br/>Monitor performance]\n",
    "    C --> D{Performance OK?}\n",
    "    D -->|Yes| E[Production<br/>Deploy model]\n",
    "    D -->|No| F[Retune with<br/>production data]\n",
    "    F --> B\n",
    "    E --> G[Monitor Drift]\n",
    "    G --> H{Performance degrades?>}\n",
    "    H -->|Yes| I[Scheduled Retraining<br/>Monthly/Quarterly]\n",
    "    H -->|No| E\n",
    "    I --> B\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "\n",
    "1. **Cache optimal hyperparameters:** Store in config file (YAML/JSON)\n",
    "   ```yaml\n",
    "   model_config:\n",
    "     n_estimators: 300\n",
    "     learning_rate: 0.08\n",
    "     max_depth: 6\n",
    "     last_tuned: 2024-12-01\n",
    "     performance_r2: 0.89\n",
    "   ```\n",
    "\n",
    "2. **Monitor performance drift:** Track metrics weekly\n",
    "   - If R¬≤ drops >5% ‚Üí retrigger hyperparameter tuning\n",
    "   - If inference time increases >20% ‚Üí optimize for speed\n",
    "\n",
    "3. **A/B test hyperparameter changes:**\n",
    "   - Deploy new model to 10% of traffic\n",
    "   - Compare business metrics (yield, revenue, latency)\n",
    "   - Rollout if improvement >2% and CI doesn't overlap\n",
    "\n",
    "4. **Automate retraining:**\n",
    "   - Schedule: Monthly for slow drift, weekly for fast drift (fraud, stock)\n",
    "   - Budget: 1-2 hours for retraining + tuning\n",
    "   - Strategy: Random Search on previous best region (warm start)\n",
    "\n",
    "---\n",
    "\n",
    "### **Semiconductor-Specific Recommendations**\n",
    "\n",
    "#### **Wafer Yield Prediction**\n",
    "- **Validation:** GroupKFold by `wafer_id` (prevent spatial leakage)\n",
    "- **Hyperparameters:** `max_depth ‚â§ 7`, `min_samples_leaf ‚â• 10`\n",
    "- **Metric:** R¬≤ with spatial correlation penalty\n",
    "- **Retraining:** After every 1000 wafers or quarterly\n",
    "\n",
    "#### **Test Time Optimization**\n",
    "- **Validation:** TimeSeriesSplit (respect temporal ordering)\n",
    "- **Hyperparameters:** `n_estimators` √ó inference time <100ms\n",
    "- **Metric:** F1-score / inference_time_ms\n",
    "- **Retraining:** After equipment changes or new test insertion\n",
    "\n",
    "#### **Defect Detection (Imbalanced)**\n",
    "- **Validation:** Stratified K-Fold with `scale_pos_weight`\n",
    "- **Hyperparameters:** Tune `scale_pos_weight = (1 - defect_rate) / defect_rate`\n",
    "- **Metric:** F2-score (emphasize recall > precision)\n",
    "- **Retraining:** Weekly (defect patterns evolve)\n",
    "\n",
    "---\n",
    "\n",
    "### **Resources for Further Learning**\n",
    "\n",
    "**Papers:**\n",
    "- Bergstra & Bengio (2012): \"Random Search for Hyper-Parameter Optimization\" - JMLR\n",
    "- Snoek et al. (2012): \"Practical Bayesian Optimization of Machine Learning Algorithms\" - NeurIPS\n",
    "\n",
    "**Libraries:**\n",
    "- **scikit-learn:** `GridSearchCV`, `RandomizedSearchCV`, `HalvingRandomSearchCV`\n",
    "- **Optuna:** Bayesian optimization with pruning (successor to Hyperopt)\n",
    "- **Ray Tune:** Distributed hyperparameter tuning at scale\n",
    "- **Weights & Biases:** Experiment tracking and hyperparameter sweeps\n",
    "\n",
    "**Books:**\n",
    "- \"Hyperparameter Tuning in Practice\" (O'Reilly, 2023)\n",
    "- \"Automated Machine Learning\" (Springer, 2019) - Chapter on hyperparameter optimization\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "**What We Learned:**\n",
    "\n",
    "1. **Grid Search:** Exhaustive but expensive - use for small search spaces (<50 configs)\n",
    "2. **Random Search:** 10√ó faster than grid, often achieves similar performance\n",
    "3. **Bayesian Optimization:** Smart sampling reduces evaluations 5-10√ó, best for expensive functions\n",
    "4. **Early Stopping:** Automatic stopping for iterative models (GBM, XGBoost, NN) saves 40-70% time\n",
    "5. **Successive Halving:** Tournament-style elimination achieves 5-15√ó speedup for large search spaces\n",
    "\n",
    "**When to Use Each:**\n",
    "\n",
    "| **Strategy**              | **Best For**                               | **Speedup**       |\n",
    "|---------------------------|--------------------------------------------|-------------------|\n",
    "| Grid Search               | Small search space, need exhaustive search | Baseline          |\n",
    "| Random Search             | Large search space, quick baseline         | 10√ó               |\n",
    "| Bayesian Optimization     | Expensive evaluations (>10 min/config)     | 5-10√ó             |\n",
    "| Early Stopping            | Iterative models (GBM, XGBoost, NN)        | 2-5√ó              |\n",
    "| Successive Halving        | Very large search space (>100 configs)     | 5-15√ó             |\n",
    "| **Combined (Random+Bayes)**| Production systems (best performance)      | 10-20√ó (2-stage)  |\n",
    "\n",
    "**Key Principle:** *Match tuning strategy to problem characteristics (search space size, computational budget, data structure).*\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- **Practice:** Apply to your domain (semiconductor, finance, healthcare, etc.)\n",
    "- **Experiment:** Try different strategies on same problem, compare results\n",
    "- **Monitor:** Track performance in production, retune when metrics degrade\n",
    "- **Automate:** Build pipelines for scheduled retraining with hyperparameter tuning\n",
    "\n",
    "**Happy Tuning! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
