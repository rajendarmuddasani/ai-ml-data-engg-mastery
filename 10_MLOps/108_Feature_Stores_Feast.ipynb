{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7ab074",
   "metadata": {},
   "source": [
    "# 108: Feature Stores with Feast\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** the feature store architecture and why it solves training-serving skew\n",
    "- **Implement** Feast feature store with both batch and real-time features\n",
    "- **Build** feature versioning and lineage tracking for reproducibility\n",
    "- **Apply** feature stores to semiconductor test data pipelines\n",
    "- **Evaluate** online vs offline feature retrieval performance and consistency\n",
    "\n",
    "## üìö What is a Feature Store?\n",
    "\n",
    "A feature store is a centralized repository that manages, versions, and serves ML features consistently across training and production. Without a feature store, teams duplicate feature engineering logic (SQL in training notebooks, Python in production APIs), causing subtle bugs where training features ‚â† serving features. Feature stores solve this by computing features once, storing them with metadata (version, timestamp, lineage), and serving them through unified APIs.\n",
    "\n",
    "Feast (Feature Store) is an open-source solution providing **online store** (Redis/DynamoDB for <10ms serving) and **offline store** (BigQuery/Snowflake for batch training). Features are defined declaratively‚Äîyou specify transformations once, and Feast handles materialization to both stores. Point-in-time correctness ensures no data leakage: when retrieving historical features for training at timestamp T, you only get data available before T.\n",
    "\n",
    "In semiconductor manufacturing, feature stores standardize parametric test features across yield prediction, binning, and test time optimization models. Instead of each team extracting \"average Vdd over last 10 wafers\" differently, the feature is computed once from STDF files, versioned (v1.2.3), and reused. When a new tester is added, update the feature definition centrally and all models get consistent data.\n",
    "\n",
    "**Why Feature Stores?**\n",
    "- ‚úÖ **Consistency**: Training features = serving features (eliminate training-serving skew)\n",
    "- ‚úÖ **Reusability**: Define \"wafer_avg_vdd\" once, use in 10 models\n",
    "- ‚úÖ **Versioning**: Reproduce model trained 6 months ago with exact feature definitions\n",
    "- ‚úÖ **Speed**: <10ms online serving, pre-computed aggregations\n",
    "- ‚úÖ **Governance**: Track feature lineage, who uses which features, data quality\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Use Case 1: Parametric Feature Standardization**\n",
    "- **Problem**: 5 teams compute \"device power\" differently (Vdd√óIdd vs max instantaneous vs average)\n",
    "- **Solution**: Define \"device_power_avg\" feature in Feast, all teams use same calculation\n",
    "- **Feature**: `vdd_mean * idd_mean` computed from STDF, updated daily\n",
    "- **Impact**: Eliminated 3 bugs where models disagreed on device classification\n",
    "- **Value**: 100% feature consistency across 12 ML models\n",
    "\n",
    "**Use Case 2: Real-Time Wafer Context**\n",
    "- **Problem**: Yield model needs last 50 wafers' statistics, slow to compute on-the-fly\n",
    "- **Solution**: Feast online store with streaming aggregations (Kafka ‚Üí Redis)\n",
    "- **Feature**: Rolling 50-wafer mean/std for Vdd, Idd, frequency (updated every wafer)\n",
    "- **Impact**: Reduced serving latency from 800ms to 12ms\n",
    "- **Value**: Enabled real-time yield prediction during test (catch bad wafers early)\n",
    "\n",
    "**Use Case 3: Feature Versioning for Reproducibility**\n",
    "- **Problem**: Model trained 3 months ago, need to reproduce results for audit\n",
    "- **Solution**: Feast feature versioning with point-in-time correctness\n",
    "- **Feature**: `wafer_spatial_features_v2` (center vs edge die statistics)\n",
    "- **Impact**: Reproduced training data exactly, passed compliance audit\n",
    "- **Value**: Regulatory compliance for automotive qualification\n",
    "\n",
    "**Use Case 4: Multi-Fab Feature Harmonization**\n",
    "- **Problem**: 3 fabs measure temperature differently (¬∞C, ¬∞F, different sensors)\n",
    "- **Solution**: Feast feature transformations normalize to standard units\n",
    "- **Feature**: `test_temp_normalized` (all fabs ‚Üí Celsius, calibrated)\n",
    "- **Impact**: Single model works across all fabs (was 3 separate models)\n",
    "- **Value**: 60% reduction in model maintenance overhead\n",
    "\n",
    "## üîÑ Feature Store Architecture\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Raw Data Sources] --> B[Feature Definitions]\n",
    "    B --> C[Feast SDK]\n",
    "    \n",
    "    A1[STDF Files] --> A\n",
    "    A2[SQL Database] --> A\n",
    "    A3[Kafka Streams] --> A\n",
    "    \n",
    "    C --> D[Offline Store]\n",
    "    C --> E[Online Store]\n",
    "    \n",
    "    D --> D1[Parquet/BigQuery]\n",
    "    D1 --> F[Training Pipeline]\n",
    "    \n",
    "    E --> E1[Redis/DynamoDB]\n",
    "    E1 --> G[Real-Time Serving]\n",
    "    \n",
    "    F --> H[Model Training]\n",
    "    H --> I[Model Registry]\n",
    "    \n",
    "    G --> J[Prediction API]\n",
    "    J --> K[Production Traffic]\n",
    "    \n",
    "    B --> L[Feature Registry]\n",
    "    L --> M[Metadata]\n",
    "    M --> M1[Version]\n",
    "    M --> M2[Owner]\n",
    "    M --> M3[Lineage]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style D1 fill:#e1ffe1\n",
    "    style E1 fill:#fff5e1\n",
    "    style K fill:#ffe1f5\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **041**: Model Evaluation - Understanding feature importance\n",
    "- **091**: SQL Advanced - Feature extraction from databases\n",
    "- **103**: Advanced Feature Engineering - Complex feature transformations\n",
    "\n",
    "**This Notebook (108):**\n",
    "- Feast installation and configuration\n",
    "- Feature definition and registration\n",
    "- Offline retrieval for training\n",
    "- Online serving for production\n",
    "- Point-in-time correctness\n",
    "\n",
    "**Next Steps:**\n",
    "- **109**: ML Pipelines with Airflow - Automated feature materialization\n",
    "- **131**: Cloud Deployment - Scalable feature store infrastructure\n",
    "\n",
    "---\n",
    "\n",
    "Let's build centralized feature infrastructure! üóÑÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86214875",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "**Note:** This notebook demonstrates feature store concepts. Full Feast requires infrastructure (Redis, file storage). We'll simulate the core workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4636c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Feature store environment ready!\")\n",
    "print(\"\\nüìù Note: Full Feast setup requires:\")\n",
    "print(\"   - pip install feast\")\n",
    "print(\"   - Redis for online store\")\n",
    "print(\"   - Parquet/BigQuery for offline store\")\n",
    "print(\"\\n   This notebook simulates core concepts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb912c06",
   "metadata": {},
   "source": [
    "## 2. Simulate Raw STDF Data Stream\n",
    "\n",
    "**Purpose:** Generate time-series semiconductor test data as source for features.\n",
    "\n",
    "**Key Points:**\n",
    "- **Entity**: `device_id` (primary key for feature lookups)\n",
    "- **Timestamp**: Event time for point-in-time correctness\n",
    "- **Raw measurements**: Vdd, Idd, freq, temp (will transform into features)\n",
    "- **Why this matters**: Features must handle streaming data with timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 30 days of test data (streaming simulation)\n",
    "start_date = datetime(2025, 11, 1)\n",
    "n_devices = 5000\n",
    "dates = [start_date + timedelta(days=i//200) for i in range(n_devices)]\n",
    "\n",
    "# Raw STDF measurements\n",
    "raw_data = pd.DataFrame({\n",
    "    'device_id': [f'DEV_{i:06d}' for i in range(n_devices)],\n",
    "    'event_timestamp': dates,\n",
    "    'wafer_id': [f'W{i//100:04d}' for i in range(n_devices)],\n",
    "    'die_x': np.random.randint(0, 30, n_devices),\n",
    "    'die_y': np.random.randint(0, 30, n_devices),\n",
    "    'vdd_raw': np.random.normal(1.2, 0.08, n_devices),\n",
    "    'idd_raw': np.random.normal(50, 8, n_devices),\n",
    "    'freq_raw': np.random.normal(2000, 150, n_devices),\n",
    "    'temp_raw': np.random.normal(85, 12, n_devices),\n",
    "    'vth_raw': np.random.normal(0.4, 0.03, n_devices),\n",
    "    'test_time_ms': np.random.uniform(50, 500, n_devices)\n",
    "})\n",
    "\n",
    "print(f\"Raw STDF data: {len(raw_data)} devices over 30 days\")\n",
    "print(f\"Date range: {raw_data['event_timestamp'].min()} to {raw_data['event_timestamp'].max()}\")\n",
    "print(f\"\\nSample raw data:\")\n",
    "print(raw_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae13629c",
   "metadata": {},
   "source": [
    "## 3. Define Feature Transformations\n",
    "\n",
    "**Purpose:** Transform raw measurements into ML-ready features (the core of feature stores).\n",
    "\n",
    "**Key Points:**\n",
    "- **Feature 1**: Device-level aggregations (power, normalized metrics)\n",
    "- **Feature 2**: Wafer-level rolling statistics (last 50 devices)\n",
    "- **Feature 3**: Spatial features (distance from wafer center)\n",
    "- **Why this matters**: Centralized definitions prevent inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce09582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Set 1: Device-level features (fast, no aggregation)\n",
    "def compute_device_features(df):\n",
    "    \"\"\"Transform raw device measurements into ML features.\"\"\"\n",
    "    features = df.copy()\n",
    "    \n",
    "    # Power consumption\n",
    "    features['power_watts'] = features['vdd_raw'] * features['idd_raw'] / 1000\n",
    "    \n",
    "    # Normalized metrics (Z-score normalization)\n",
    "    features['vdd_normalized'] = (features['vdd_raw'] - 1.2) / 0.08\n",
    "    features['idd_normalized'] = (features['idd_raw'] - 50) / 8\n",
    "    \n",
    "    # Performance ratio\n",
    "    features['freq_per_watt'] = features['freq_raw'] / features['power_watts']\n",
    "    \n",
    "    # Spatial position\n",
    "    features['distance_from_center'] = np.sqrt(\n",
    "        (features['die_x'] - 15)**2 + (features['die_y'] - 15)**2\n",
    "    )\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Feature Set 2: Wafer-level rolling aggregations (requires historical context)\n",
    "def compute_wafer_rolling_features(df, window=50):\n",
    "    \"\"\"Compute rolling statistics per wafer (simulates online store aggregations).\"\"\"\n",
    "    df_sorted = df.sort_values('event_timestamp')\n",
    "    \n",
    "    # Rolling mean/std for last N devices on same wafer\n",
    "    wafer_features = []\n",
    "    for wafer_id in df_sorted['wafer_id'].unique():\n",
    "        wafer_data = df_sorted[df_sorted['wafer_id'] == wafer_id].copy()\n",
    "        \n",
    "        wafer_data['wafer_vdd_rolling_mean'] = (\n",
    "            wafer_data['vdd_raw'].rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        wafer_data['wafer_vdd_rolling_std'] = (\n",
    "            wafer_data['vdd_raw'].rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "        wafer_data['wafer_power_rolling_mean'] = (\n",
    "            (wafer_data['vdd_raw'] * wafer_data['idd_raw'] / 1000)\n",
    "            .rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        wafer_features.append(wafer_data)\n",
    "    \n",
    "    return pd.concat(wafer_features, ignore_index=True)\n",
    "\n",
    "# Apply transformations\n",
    "print(\"Computing device-level features...\")\n",
    "device_features = compute_device_features(raw_data)\n",
    "\n",
    "print(\"Computing wafer-level rolling features...\")\n",
    "wafer_rolling_features = compute_wafer_rolling_features(device_features)\n",
    "\n",
    "print(f\"\\n‚úÖ Features computed for {len(wafer_rolling_features)} devices\")\n",
    "print(f\"\\nFeature columns: {wafer_rolling_features.columns.tolist()}\")\n",
    "print(f\"\\nSample features:\")\n",
    "print(wafer_rolling_features[[\n",
    "    'device_id', 'power_watts', 'freq_per_watt', \n",
    "    'wafer_vdd_rolling_mean', 'distance_from_center'\n",
    "]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef49e91",
   "metadata": {},
   "source": [
    "## 3. Feast Feature Registry Setup\n",
    "\n",
    "**Purpose:** Define feature schemas and metadata for centralized feature management.\n",
    "\n",
    "**Key Points:**\n",
    "- **Entity**: Primary key for feature lookup (e.g., `device_id`, `user_id`)\n",
    "- **Feature View**: Logical grouping of features from same data source\n",
    "- **TTL (Time-To-Live)**: How long features stay in online store before refresh\n",
    "- **Batch Source**: Historical data (Parquet/BigQuery) for training\n",
    "\n",
    "**Why This Matters:** Feature registry prevents duplicate feature engineering across teams and ensures version consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67807b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Feast feature definitions (in practice, these are defined in feature_store.yaml)\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class FeatureView:\n",
    "    name: str\n",
    "    entities: List[str]\n",
    "    features: List[str]\n",
    "    ttl_hours: int\n",
    "    description: str\n",
    "\n",
    "# Define feature views\n",
    "device_features_view = FeatureView(\n",
    "    name=\"device_parametric_features\",\n",
    "    entities=[\"device_id\"],\n",
    "    features=[\"Vdd\", \"Idd\", \"Frequency\", \"Power\", \"Temperature\"],\n",
    "    ttl_hours=168,  # 1 week\n",
    "    description=\"Real-time parametric test measurements\"\n",
    ")\n",
    "\n",
    "wafer_agg_features_view = FeatureView(\n",
    "    name=\"wafer_aggregate_features\",\n",
    "    entities=[\"wafer_id\"],\n",
    "    features=[\"wafer_avg_yield\", \"wafer_test_time_p95\", \"defect_density\"],\n",
    "    ttl_hours=720,  # 30 days\n",
    "    description=\"Wafer-level aggregated statistics\"\n",
    ")\n",
    "\n",
    "print(\"Feature Registry Initialized:\")\n",
    "print(f\"  - {device_features_view.name}: {len(device_features_view.features)} features\")\n",
    "print(f\"  - {wafer_agg_features_view.name}: {len(wafer_agg_features_view.features)} features\")\n",
    "print(f\"\\nFeature freshness guaranteed for {device_features_view.ttl_hours} hours (device-level)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463bf5f0",
   "metadata": {},
   "source": [
    "## 4. Offline Store: Historical Feature Retrieval\n",
    "\n",
    "**Purpose:** Retrieve point-in-time correct features for training data generation.\n",
    "\n",
    "**Key Points:**\n",
    "- **Point-in-Time Correctness**: Features as they existed at training time (no data leakage)\n",
    "- **Entity Join**: Match features to training labels by entity ID + timestamp\n",
    "- **Batch Retrieval**: Efficient loading of 100K+ samples for model training\n",
    "- **Temporal Consistency**: Training data reflects real-world deployment constraints\n",
    "\n",
    "**Why This Matters:** Prevents train-serve skew by ensuring training features match production feature computation exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e02d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate offline feature retrieval (point-in-time correct)\n",
    "# In real Feast: store.get_historical_features(entity_df, features)\n",
    "\n",
    "# Create training entity dataframe (device IDs + timestamps)\n",
    "training_entities = pd.DataFrame({\n",
    "    'device_id': raw_data['device_id'].values[:600],\n",
    "    'event_timestamp': pd.to_datetime('2024-01-15') + pd.to_timedelta(np.arange(600), unit='h')\n",
    "})\n",
    "\n",
    "# Simulate feature join (in practice, Feast does point-in-time join automatically)\n",
    "offline_features = training_entities.merge(\n",
    "    device_features[['device_id', 'Vdd', 'Idd', 'Frequency', 'Power', 'Temperature']],\n",
    "    on='device_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Add wafer-level features\n",
    "wafer_mapping = raw_data[['device_id', 'wafer_id']].drop_duplicates()\n",
    "offline_features = offline_features.merge(wafer_mapping, on='device_id', how='left')\n",
    "offline_features = offline_features.merge(\n",
    "    wafer_features[['wafer_id', 'wafer_avg_yield', 'wafer_test_time_p95', 'defect_density']],\n",
    "    on='wafer_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Offline Features Retrieved:\")\n",
    "print(f\"  Samples: {len(offline_features)}\")\n",
    "print(f\"  Device-level features: 5 (Vdd, Idd, Frequency, Power, Temperature)\")\n",
    "print(f\"  Wafer-level features: 3 (avg_yield, test_time_p95, defect_density)\")\n",
    "print(f\"\\nSample (first 3 rows):\")\n",
    "print(offline_features.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1412564",
   "metadata": {},
   "source": [
    "## 5. Online Store: Real-Time Feature Serving\n",
    "\n",
    "**Purpose:** Low-latency feature retrieval for production inference (< 10ms).\n",
    "\n",
    "**Key Points:**\n",
    "- **Redis/DynamoDB Backend**: In-memory key-value stores for sub-millisecond lookups\n",
    "- **Materialization**: Pre-compute features from batch sources and cache in online store\n",
    "- **Freshness**: TTL-based expiration ensures stale features are refreshed\n",
    "- **Read Pattern**: `get_online_features(entity_ids, feature_names)` ‚Üí Dict\n",
    "\n",
    "**Why This Matters:** Production models need features in milliseconds, not minutes. Online stores bridge batch computation and real-time serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2079a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate online feature store (Redis-like key-value store)\n",
    "online_store = {}\n",
    "\n",
    "# Materialize features to online store (in practice: feast materialize-incremental)\n",
    "for _, row in device_features.iterrows():\n",
    "    key = f\"device:{row['device_id']}\"\n",
    "    online_store[key] = {\n",
    "        'Vdd': row['Vdd'],\n",
    "        'Idd': row['Idd'],\n",
    "        'Frequency': row['Frequency'],\n",
    "        'Power': row['Power'],\n",
    "        'Temperature': row['Temperature']\n",
    "    }\n",
    "\n",
    "# Simulate real-time feature retrieval\n",
    "def get_online_features(device_id):\n",
    "    \"\"\"Simulate low-latency feature lookup (< 10ms in production).\"\"\"\n",
    "    key = f\"device:{device_id}\"\n",
    "    return online_store.get(key, {})\n",
    "\n",
    "# Example: Model inference at runtime\n",
    "test_device_id = 'D001'\n",
    "features = get_online_features(test_device_id)\n",
    "\n",
    "print(f\"Online Feature Retrieval (Simulated):\")\n",
    "print(f\"  Device ID: {test_device_id}\")\n",
    "print(f\"  Features Retrieved: {features}\")\n",
    "print(f\"\\n‚úÖ Latency: ~2ms (Redis lookup in production)\")\n",
    "print(f\"Total devices in online store: {len(online_store)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216dde63",
   "metadata": {},
   "source": [
    "## 6. Feature Versioning & Lineage\n",
    "\n",
    "**Purpose:** Track feature transformations and schema changes over time for reproducibility.\n",
    "\n",
    "**Key Points:**\n",
    "- **Version Control**: Git-like tracking for feature definitions (v1, v2, v3)\n",
    "- **Lineage**: Trace feature back to raw data sources and transformations\n",
    "- **Backward Compatibility**: Old models can still query old feature versions\n",
    "- **Audit Trail**: Compliance requirement for regulated industries (finance, healthcare)\n",
    "\n",
    "**Why This Matters:** When model performance changes, knowing which feature version was used is critical for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate feature versioning\n",
    "feature_versions = {\n",
    "    'v1.0': {\n",
    "        'name': 'device_parametric_features_v1',\n",
    "        'features': ['Vdd', 'Idd', 'Frequency'],  # Original 3 features\n",
    "        'created_at': '2023-06-01',\n",
    "        'description': 'Initial parametric features'\n",
    "    },\n",
    "    'v1.1': {\n",
    "        'name': 'device_parametric_features_v1.1',\n",
    "        'features': ['Vdd', 'Idd', 'Frequency', 'Power'],  # Added Power\n",
    "        'created_at': '2023-09-15',\n",
    "        'description': 'Added computed Power feature'\n",
    "    },\n",
    "    'v2.0': {\n",
    "        'name': 'device_parametric_features_v2',\n",
    "        'features': ['Vdd', 'Idd', 'Frequency', 'Power', 'Temperature'],  # Added Temperature\n",
    "        'created_at': '2024-01-10',\n",
    "        'description': 'Added Temperature sensor data (breaking change: requires new data pipeline)'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Lineage tracking (simplified)\n",
    "feature_lineage = {\n",
    "    'Power': {\n",
    "        'source': 'Computed from Vdd * Idd / 1000',\n",
    "        'dependencies': ['Vdd', 'Idd'],\n",
    "        'transformation': 'real-time computation'\n",
    "    },\n",
    "    'wafer_avg_yield': {\n",
    "        'source': 'Aggregation of device Pass/Fail',\n",
    "        'dependencies': ['Pass (per device)'],\n",
    "        'transformation': 'batch aggregation (daily)'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Feature Version History:\")\n",
    "for version, metadata in feature_versions.items():\n",
    "    print(f\"\\n  {version} ({metadata['created_at']}):\")\n",
    "    print(f\"    Features: {metadata['features']}\")\n",
    "    print(f\"    Description: {metadata['description']}\")\n",
    "\n",
    "print(\"\\n\\nFeature Lineage Example:\")\n",
    "print(f\"  Feature: Power\")\n",
    "print(f\"    Source: {feature_lineage['Power']['source']}\")\n",
    "print(f\"    Dependencies: {feature_lineage['Power']['dependencies']}\")\n",
    "print(f\"\\nüí° When model v1.5 trained on 2023-10-01, it used feature version v1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11937ab9",
   "metadata": {},
   "source": [
    "## 7. Visualization: Feature Store Architecture\n",
    "\n",
    "**Purpose:** Understand data flow from raw sources to model serving.\n",
    "\n",
    "**Key Points:**\n",
    "- **Dual-Path**: Batch (offline) for training, streaming (online) for inference\n",
    "- **Materialization**: Background job syncs offline ‚Üí online store\n",
    "- **Consistency**: Same feature definitions used in training and serving\n",
    "\n",
    "**Why This Matters:** Visualizing the architecture prevents common mistakes like using different feature logic in training vs production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b998cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Feature Store Metrics & Architecture', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Feature freshness (simulated staleness distribution)\n",
    "freshness_hours = np.random.exponential(12, 1000)  # Exponential distribution\n",
    "axes[0, 0].hist(freshness_hours, bins=30, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].axvline(x=24, color='red', linestyle='--', label='SLA: 24 hours')\n",
    "axes[0, 0].set_title('Feature Freshness Distribution')\n",
    "axes[0, 0].set_xlabel('Staleness (hours)')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Feature coverage (% of entities with complete features)\n",
    "feature_coverage = {\n",
    "    'Vdd': 98.5,\n",
    "    'Idd': 97.2,\n",
    "    'Frequency': 99.1,\n",
    "    'Power': 96.8,\n",
    "    'Temperature': 94.3\n",
    "}\n",
    "axes[0, 1].barh(list(feature_coverage.keys()), list(feature_coverage.values()), color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].axvline(x=95, color='red', linestyle='--', label='Target: 95%')\n",
    "axes[0, 1].set_title('Feature Coverage (%)')\n",
    "axes[0, 1].set_xlabel('Coverage (%)')\n",
    "axes[0, 1].set_xlim(90, 100)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Online store latency (simulated p50, p95, p99)\n",
    "latencies = {\n",
    "    'p50': 2.3,\n",
    "    'p95': 8.1,\n",
    "    'p99': 15.4\n",
    "}\n",
    "axes[1, 0].bar(latencies.keys(), latencies.values(), color=['green', 'orange', 'red'], edgecolor='black')\n",
    "axes[1, 0].axhline(y=10, color='red', linestyle='--', label='SLA: 10ms')\n",
    "axes[1, 0].set_title('Online Store Latency')\n",
    "axes[1, 0].set_ylabel('Latency (ms)')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Feature usage (how many models use each feature)\n",
    "feature_usage = {\n",
    "    'Vdd': 5,\n",
    "    'Idd': 4,\n",
    "    'Frequency': 6,\n",
    "    'Power': 3,\n",
    "    'Temperature': 2\n",
    "}\n",
    "axes[1, 1].bar(feature_usage.keys(), feature_usage.values(), color='coral', edgecolor='black')\n",
    "axes[1, 1].set_title('Feature Reuse Across Models')\n",
    "axes[1, 1].set_ylabel('# of Models Using Feature')\n",
    "axes[1, 1].set_xlabel('Feature Name')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Feature Store Health Summary:\")\n",
    "print(f\"  Average Freshness: {np.mean(freshness_hours):.1f} hours\")\n",
    "print(f\"  Features Below Coverage Target: {sum([1 for v in feature_coverage.values() if v < 95])}\")\n",
    "print(f\"  p99 Latency: {latencies['p99']:.1f}ms (SLA: < 10ms ‚ö†Ô∏è)\")\n",
    "print(f\"  Most Reused Feature: Frequency (used by {feature_usage['Frequency']} models)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa447a",
   "metadata": {},
   "source": [
    "## üöÄ Real-World Project Templates\n",
    "\n",
    "Build production feature stores using these architectures:\n",
    "\n",
    "### 1Ô∏è‚É£ **Post-Silicon Test Feature Platform**\n",
    "- **Objective**: Centralize parametric test features for yield prediction across 20+ models  \n",
    "- **Data**: STDF files (10M devices/month), wafer maps, test program versions  \n",
    "- **Success Metric**: Reduce feature engineering time from 2 weeks ‚Üí 2 days per model  \n",
    "- **Features**: Device-level (Vdd, Idd, freq), wafer-level (spatial stats), lot-level (trends)  \n",
    "- **Tech Stack**: Feast, S3 (offline), Redis (online), Spark for aggregations, Airflow orchestration\n",
    "\n",
    "### 2Ô∏è‚É£ **E-Commerce Personalization Feature Store**\n",
    "- **Objective**: Real-time user/product features for recommendation engine  \n",
    "- **Data**: Clickstream (5M events/hour), purchase history, inventory (100K SKUs)  \n",
    "- **Success Metric**: < 5ms p99 latency for feature retrieval, 99.9% uptime  \n",
    "- **Features**: User (lifetime value, category preferences), Product (trending score, inventory)  \n",
    "- **Tech Stack**: Tecton, DynamoDB, Kinesis, Lambda for real-time transformations\n",
    "\n",
    "### 3Ô∏è‚É£ **Fraud Detection Feature Mesh**\n",
    "- **Objective**: Share fraud indicators across credit card, account takeover, and loan fraud models  \n",
    "- **Data**: Transactions (1M/day), device fingerprints, IP geolocation, social graph  \n",
    "- **Success Metric**: Block $5M fraudulent transactions/month with <0.5% false positives  \n",
    "- **Features**: Transaction velocity (1hr/24hr), account age, geolocation risk score  \n",
    "- **Tech Stack**: Hopsworks, Kafka, Flink for streaming aggregations, PostgreSQL\n",
    "\n",
    "### 4Ô∏è‚É£ **Autonomous Driving Perception Features**\n",
    "- **Objective**: Sensor fusion features for object detection models in real-time  \n",
    "- **Data**: Camera frames (30 FPS), LiDAR point clouds, GPS/IMU (100Hz)  \n",
    "- **Success Metric**: < 50ms end-to-end latency from sensor ‚Üí prediction  \n",
    "- **Features**: Object bounding boxes, lane curvature, relative velocity, weather conditions  \n",
    "- **Tech Stack**: Custom C++ feature store, ROS topics, Redis, CUDA for GPU transforms\n",
    "\n",
    "### 5Ô∏è‚É£ **Healthcare Patient Risk Scoring**\n",
    "- **Objective**: HIPAA-compliant feature store for readmission/mortality prediction  \n",
    "- **Data**: EHR records (vitals, labs, medications), 500K patients, 10 years history  \n",
    "- **Success Metric**: Predict 30-day readmission with AUROC > 0.82  \n",
    "- **Features**: Lab trends (creatinine slope), medication adherence, comorbidity index  \n",
    "- **Tech Stack**: Feast on-prem, HIPAA-compliant S3, Snowflake, encrypted Redis\n",
    "\n",
    "### 6Ô∏è‚É£ **Financial Trading Signal Features**\n",
    "- **Objective**: Sub-millisecond feature serving for algorithmic trading  \n",
    "- **Data**: Market tick data (1M ticks/sec), order book depth, news sentiment  \n",
    "- **Success Metric**: < 1ms p50 latency, 99.999% availability  \n",
    "- **Features**: Price momentum (5s/1min/5min), volatility, bid-ask spread, correlation matrix  \n",
    "- **Tech Stack**: Custom in-memory C++ store, kdb+/q, FPGA acceleration, co-location\n",
    "\n",
    "### 7Ô∏è‚É£ **Supply Chain Demand Forecasting**\n",
    "- **Objective**: Centralized feature platform for 100+ SKU forecast models  \n",
    "- **Data**: POS sales (10K stores), inventory, promotions, weather, holidays  \n",
    "- **Success Metric**: Reduce stockouts by 30%, overstock by 25%  \n",
    "- **Features**: Sales trends (7d/30d/1y), promotional lift, weather impact scores  \n",
    "- **Tech Stack**: Databricks Feature Store, Delta Lake, Spark, Redshift\n",
    "\n",
    "### 8Ô∏è‚É£ **Smart Grid Load Forecasting**\n",
    "- **Objective**: Regional energy demand features for grid balancing  \n",
    "- **Data**: Smart meter readings (1M households, 15min intervals), weather, events  \n",
    "- **Success Metric**: < 3% MAPE for 24-hour ahead forecast  \n",
    "- **Features**: Historical load patterns, temperature lags, day-of-week/holiday indicators  \n",
    "- **Tech Stack**: Feast, TimescaleDB, Kafka, Prophet for trend features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75266d0b",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### What is a Feature Store?\n",
    "A centralized platform for storing, managing, and serving ML features across offline (training) and online (inference) environments with consistency guarantees.\n",
    "\n",
    "### Why Feature Stores?\n",
    "- **Eliminate Train-Serve Skew**: Same feature logic in training and production\n",
    "- **Feature Reuse**: Teams share features instead of duplicating engineering effort\n",
    "- **Versioning**: Track feature changes and ensure reproducibility\n",
    "- **Performance**: Low-latency serving (< 10ms) for real-time models\n",
    "\n",
    "### Core Components\n",
    "\n",
    "| **Component** | **Purpose** | **Technology Examples** |\n",
    "|--------------|------------|------------------------|\n",
    "| **Offline Store** | Historical features for training | S3, BigQuery, Snowflake, Delta Lake |\n",
    "| **Online Store** | Real-time feature serving | Redis, DynamoDB, Cassandra |\n",
    "| **Feature Registry** | Metadata (schemas, owners, versions) | PostgreSQL, MySQL, SQLite |\n",
    "| **Transformation Engine** | Feature computation logic | Spark, Flink, Pandas, SQL |\n",
    "| **Materialization** | Sync offline ‚Üí online store | Airflow, Prefect, Kubernetes CronJobs |\n",
    "\n",
    "### Feast Architecture Patterns\n",
    "\n",
    "**Batch Features (Offline Store):**\n",
    "```python\n",
    "# Define feature view\n",
    "@feast.feature_view(\n",
    "    entities=[\"device_id\"],\n",
    "    ttl=timedelta(weeks=1),\n",
    "    source=ParquetSource(path=\"s3://features/device_parametric.parquet\")\n",
    ")\n",
    "def device_features(df: DataFrame):\n",
    "    return df[[\"Vdd\", \"Idd\", \"Frequency\", \"Power\", \"Temperature\"]]\n",
    "\n",
    "# Training: Point-in-time correct retrieval\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\"device_features:Vdd\", \"device_features:Frequency\"]\n",
    ").to_df()\n",
    "```\n",
    "\n",
    "**Real-Time Features (Online Store):**\n",
    "```python\n",
    "# Materialize to Redis\n",
    "store.materialize_incremental(end_date=datetime.now())\n",
    "\n",
    "# Inference: Low-latency lookup\n",
    "features = store.get_online_features(\n",
    "    entity_rows=[{\"device_id\": \"D001\"}],\n",
    "    features=[\"device_features:Vdd\", \"device_features:Temperature\"]\n",
    ").to_dict()\n",
    "```\n",
    "\n",
    "### Point-in-Time Correctness\n",
    "Ensures no data leakage by joining features as they existed at historical timestamps:\n",
    "- Training on 2023-09-01 uses features computed ‚â§ 2023-09-01\n",
    "- Prevents \"future peeking\" (using 2023-10-01 data to predict 2023-09-01 labels)\n",
    "\n",
    "### Common Pitfalls\n",
    "- ‚ùå **Inconsistent Transformations**: Training uses pandas, serving uses SQL ‚Üí different results\n",
    "- ‚ùå **Stale Online Features**: Forgetting to materialize ‚Üí serving outdated features\n",
    "- ‚ùå **No Feature Monitoring**: Drift goes unnoticed until model degrades\n",
    "- ‚ùå **Over-Engineering**: Small projects don't need full feature store (start simple)\n",
    "\n",
    "### When to Use Feature Stores\n",
    "- ‚úÖ **Multiple models** share overlapping features (e.g., 5+ models using same user features)\n",
    "- ‚úÖ **Real-time inference** required (< 100ms latency SLA)\n",
    "- ‚úÖ **Frequent retraining** (weekly/monthly) ‚Üí need reproducible feature pipelines\n",
    "- ‚úÖ **Large teams** (10+ ML engineers) ‚Üí prevent duplicate work\n",
    "- ‚ùå **Single model**, batch predictions only ‚Üí simpler alternatives suffice\n",
    "\n",
    "### Post-Silicon Use Cases\n",
    "- **Wafer-Level Features**: Spatial statistics (die neighbors, edge vs center)\n",
    "- **Temporal Features**: Test parameter trends across lots (process drift detection)\n",
    "- **Multi-Site Features**: Correlation between wafer test and final test results\n",
    "- **Equipment Features**: Tester calibration metadata joined with parametric data\n",
    "\n",
    "### Tool Comparison\n",
    "\n",
    "| **Tool** | **Best For** | **Strengths** | **Limitations** |\n",
    "|---------|-------------|--------------|----------------|\n",
    "| **Feast** | Open-source, Kubernetes-native | Free, active community, flexible | Requires DevOps expertise |\n",
    "| **Tecton** | Enterprise, real-time streaming | Managed service, great UI | Expensive, vendor lock-in |\n",
    "| **Hopsworks** | On-prem, regulated industries | Full control, feature monitoring | Complex setup |\n",
    "| **AWS SageMaker** | AWS-native workloads | Seamless AWS integration | AWS lock-in, limited customization |\n",
    "| **Databricks** | Spark-heavy pipelines | Unity Catalog integration | Spark overhead for simple features |\n",
    "\n",
    "### Performance Benchmarks (Typical)\n",
    "- **Offline Store**: 100K samples in 2-10 seconds (Parquet/BigQuery)\n",
    "- **Online Store**: p99 latency < 10ms (Redis), < 50ms (DynamoDB)\n",
    "- **Materialization**: 1M features in 5-30 minutes (Spark-based)\n",
    "\n",
    "### Next Steps\n",
    "- **Notebook 109**: ML Pipelines (orchestrate feature generation + model training)\n",
    "- **Advanced**: Streaming features with Kafka/Flink, feature encryption, multi-region replication\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: *Feature stores are infrastructure. Build when complexity justifies the investment!* üèóÔ∏è"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
