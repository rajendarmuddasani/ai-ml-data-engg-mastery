{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0432935b",
   "metadata": {},
   "source": [
    "# 125: ML Testing & Validation\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** testing strategies specific to ML systems (beyond traditional software)\n",
    "- **Implement** unit tests for data pipelines, feature engineering, and model components\n",
    "- **Build** integration tests for end-to-end ML workflows\n",
    "- **Apply** model validation frameworks to post-silicon yield prediction systems\n",
    "- **Evaluate** test coverage and quality metrics for ML code\n",
    "- **Design** property-based testing for ML invariants\n",
    "\n",
    "## ðŸ“š What is ML Testing & Validation?\n",
    "\n",
    "**ML testing** extends traditional software testing to address the unique challenges of machine learning systems: non-deterministic behavior, data dependencies, gradual performance degradation, and the interplay between code, data, and models. Unlike traditional software where bugs are binary (works/doesn't work), ML bugs are often subtle performance regressions that require statistical testing and continuous monitoring.\n",
    "\n",
    "**Why ML Testing is Different:**\n",
    "- âœ… **Code + Data + Model**: Must test all three components and their interactions\n",
    "- âœ… **Non-deterministic**: Same input can produce different outputs (stochastic algorithms)\n",
    "- âœ… **Gradual degradation**: Performance slowly declines (data drift, concept drift)\n",
    "- âœ… **No ground truth in production**: Can't verify correctness of predictions directly\n",
    "- âœ… **Statistical testing**: Assertions use distributions, not exact values\n",
    "- âœ… **Infrastructure dependencies**: Models depend on feature stores, data pipelines, serving systems\n",
    "\n",
    "**Testing Pyramid for ML:**\n",
    "1. **Unit Tests (70%)**: Data validation, feature engineering, model components\n",
    "2. **Integration Tests (20%)**: End-to-end pipelines, model training, serving\n",
    "3. **System Tests (10%)**: Performance, load testing, monitoring\n",
    "\n",
    "## ðŸ­ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Yield Prediction Model Testing**\n",
    "- Input: Test unit tests for feature engineering (Vdd normalization, power calculations)\n",
    "- Tests: Assert feature ranges, null handling, statistical properties\n",
    "- Output: Confidence that features are computed correctly before model training\n",
    "- Value: Catch data quality issues before they corrupt model training (save 2-3 days retraining)\n",
    "\n",
    "**Test Time Prediction Integration Tests**\n",
    "- Input: End-to-end test from STDF data â†’ features â†’ model â†’ prediction â†’ API response\n",
    "- Tests: Validate entire pipeline produces correct predictions within latency SLA\n",
    "- Output: Ensure deployment doesn't break existing functionality\n",
    "- Value: Prevent production incidents (99.9% uptime SLA for inline test binning)\n",
    "\n",
    "**Binning Model Validation Framework**\n",
    "- Input: New binning model candidate (v2.0) vs production model (v1.5)\n",
    "- Tests: A/B comparison on holdout set, fairness checks, edge case testing\n",
    "- Output: Statistical evidence v2.0 is better (or reject deployment)\n",
    "- Value: Safe model updates, no accuracy regressions, compliance with test specifications\n",
    "\n",
    "**Wafer Map Analysis Property Tests**\n",
    "- Input: Spatial feature engineering functions (neighbor calculations, radial distance)\n",
    "- Tests: Property-based tests (symmetry, boundary conditions, invariants)\n",
    "- Output: Verify spatial logic correct for all wafer geometries\n",
    "- Value: Catch edge cases (wafer edge, incomplete dies, coordinate systems)\n",
    "\n",
    "## ðŸ”„ ML Testing Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    subgraph \"Development\"\n",
    "        A[Write Code] --> B[Unit Tests]\n",
    "        B --> C[Local Validation]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Data Pipeline Testing\"\n",
    "        D[Data Ingestion] --> E[Schema Validation]\n",
    "        E --> F[Data Quality Tests]\n",
    "        F --> G[Feature Engineering Tests]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Model Testing\"\n",
    "        H[Model Training] --> I[Training Tests]\n",
    "        I --> J[Model Validation]\n",
    "        J --> K[Performance Tests]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Integration Testing\"\n",
    "        L[End-to-End Pipeline] --> M[Integration Tests]\n",
    "        M --> N[Load Tests]\n",
    "        N --> O[Monitoring Tests]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Production Validation\"\n",
    "        P[Deployment] --> Q[Smoke Tests]\n",
    "        Q --> R[A/B Tests]\n",
    "        R --> S[Shadow Mode]\n",
    "    end\n",
    "    \n",
    "    C --> D\n",
    "    G --> H\n",
    "    K --> L\n",
    "    O --> P\n",
    "    S --> T[Production]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style T fill:#e1ffe1\n",
    "    style J fill:#fff5e1\n",
    "    style M fill:#ffe1e1\n",
    "```\n",
    "\n",
    "## ðŸ“Š Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **121_MLOps_Fundamentals.ipynb** - MLOps lifecycle, experiment tracking\n",
    "- **122_MLflow_Complete_Guide.ipynb** - Model versioning\n",
    "- **124_Feature_Store_Implementation.ipynb** - Feature engineering patterns\n",
    "\n",
    "**Next Steps:**\n",
    "- **126_Continuous_Training_Pipelines.ipynb** - Automated retraining with tests\n",
    "- **127_Model_Serving_Patterns.ipynb** - Deployment with validation gates\n",
    "- **128_ML_Monitoring.ipynb** - Production monitoring and alerting\n",
    "\n",
    "---\n",
    "\n",
    "Let's build robust ML testing frameworks! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12dd7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ML testing libraries\n",
    "# !pip install pytest pytest-cov hypothesis great-expectations scikit-learn pandas numpy\n",
    "\n",
    "import pytest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ML testing libraries loaded\")\n",
    "print(\"Focus: Unit tests, integration tests, model validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf51ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data validation tests using pytest\n",
    "\n",
    "class TestSTDFDataSchema:\n",
    "    \"\"\"Unit tests for STDF data schema validation.\"\"\"\n",
    "    \n",
    "    def test_required_columns_present(self):\n",
    "        \"\"\"Test that all required columns exist.\"\"\"\n",
    "        data = pd.DataFrame({\n",
    "            'device_id': ['DEV001'],\n",
    "            'vdd': [1.2],\n",
    "            'idd': [250.0],\n",
    "            'freq': [2.5],\n",
    "            'temp': [85.0],\n",
    "            'yield': [1]\n",
    "        })\n",
    "        \n",
    "        required_cols = ['device_id', 'vdd', 'idd', 'freq', 'temp', 'yield']\n",
    "        \n",
    "        for col in required_cols:\n",
    "            assert col in data.columns, f\"Missing required column: {col}\"\n",
    "        \n",
    "        print(\"âœ… test_required_columns_present PASSED\")\n",
    "    \n",
    "    def test_data_types_correct(self):\n",
    "        \"\"\"Test that columns have correct data types.\"\"\"\n",
    "        data = pd.DataFrame({\n",
    "            'device_id': ['DEV001', 'DEV002'],\n",
    "            'vdd': [1.2, 1.21],\n",
    "            'idd': [250.0, 255.0],\n",
    "            'freq': [2.5, 2.48],\n",
    "            'temp': [85.0, 86.0],\n",
    "            'yield': [1, 0]\n",
    "        })\n",
    "        \n",
    "        assert data['device_id'].dtype == object, \"device_id should be string\"\n",
    "        assert np.issubdtype(data['vdd'].dtype, np.floating), \"vdd should be float\"\n",
    "        assert np.issubdtype(data['idd'].dtype, np.floating), \"idd should be float\"\n",
    "        assert np.issubdtype(data['freq'].dtype, np.floating), \"freq should be float\"\n",
    "        assert np.issubdtype(data['temp'].dtype, np.floating), \"temp should be float\"\n",
    "        assert np.issubdtype(data['yield'].dtype, np.integer), \"yield should be int\"\n",
    "        \n",
    "        print(\"âœ… test_data_types_correct PASSED\")\n",
    "    \n",
    "    def test_parametric_ranges_valid(self):\n",
    "        \"\"\"Test that parametric values are within valid ranges.\"\"\"\n",
    "        data = pd.DataFrame({\n",
    "            'device_id': ['DEV001'],\n",
    "            'vdd': [1.2],\n",
    "            'idd': [250.0],\n",
    "            'freq': [2.5],\n",
    "            'temp': [85.0],\n",
    "            'yield': [1]\n",
    "        })\n",
    "        \n",
    "        # Parametric limits (from test specifications)\n",
    "        assert data['vdd'].between(0.9, 1.5).all(), \"Vdd out of range (0.9-1.5V)\"\n",
    "        assert data['idd'].between(0, 500).all(), \"Idd out of range (0-500mA)\"\n",
    "        assert data['freq'].between(1.0, 5.0).all(), \"Freq out of range (1-5GHz)\"\n",
    "        assert data['temp'].between(-40, 125).all(), \"Temp out of range (-40-125Â°C)\"\n",
    "        assert data['yield'].isin([0, 1]).all(), \"Yield must be 0 or 1\"\n",
    "        \n",
    "        print(\"âœ… test_parametric_ranges_valid PASSED\")\n",
    "    \n",
    "    def test_no_nulls_in_critical_columns(self):\n",
    "        \"\"\"Test that critical columns have no null values.\"\"\"\n",
    "        data = pd.DataFrame({\n",
    "            'device_id': ['DEV001', 'DEV002'],\n",
    "            'vdd': [1.2, 1.21],\n",
    "            'idd': [250.0, 255.0],\n",
    "            'freq': [2.5, 2.48],\n",
    "            'temp': [85.0, 86.0],\n",
    "            'yield': [1, 0]\n",
    "        })\n",
    "        \n",
    "        critical_cols = ['device_id', 'vdd', 'idd', 'freq', 'yield']\n",
    "        \n",
    "        for col in critical_cols:\n",
    "            null_count = data[col].isnull().sum()\n",
    "            assert null_count == 0, f\"Column {col} has {null_count} null values\"\n",
    "        \n",
    "        print(\"âœ… test_no_nulls_in_critical_columns PASSED\")\n",
    "    \n",
    "    def test_device_id_unique(self):\n",
    "        \"\"\"Test that device IDs are unique (no duplicates).\"\"\"\n",
    "        data = pd.DataFrame({\n",
    "            'device_id': ['DEV001', 'DEV002', 'DEV003'],\n",
    "            'vdd': [1.2, 1.21, 1.19],\n",
    "            'idd': [250.0, 255.0, 248.0],\n",
    "            'freq': [2.5, 2.48, 2.51],\n",
    "            'temp': [85.0, 86.0, 84.0],\n",
    "            'yield': [1, 0, 1]\n",
    "        })\n",
    "        \n",
    "        assert data['device_id'].is_unique, \"device_id column has duplicates\"\n",
    "        \n",
    "        print(\"âœ… test_device_id_unique PASSED\")\n",
    "\n",
    "# Run tests\n",
    "tester = TestSTDFDataSchema()\n",
    "tester.test_required_columns_present()\n",
    "tester.test_data_types_correct()\n",
    "tester.test_parametric_ranges_valid()\n",
    "tester.test_no_nulls_in_critical_columns()\n",
    "tester.test_device_id_unique()\n",
    "\n",
    "print(\"\\nâœ… All data schema tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bde9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering functions (to be tested)\n",
    "\n",
    "def compute_power(vdd, idd):\n",
    "    \"\"\"Calculate power consumption in milliwatts.\"\"\"\n",
    "    return vdd * idd\n",
    "\n",
    "def compute_power_efficiency(freq, power):\n",
    "    \"\"\"Calculate frequency per unit power (GHz/W).\"\"\"\n",
    "    if power == 0:\n",
    "        return np.nan  # Avoid division by zero\n",
    "    return freq / power\n",
    "\n",
    "def normalize_zscore(values):\n",
    "    \"\"\"Normalize values to Z-scores.\"\"\"\n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values)\n",
    "    if std == 0:\n",
    "        return np.zeros_like(values)  # Constant values â†’ all zeros\n",
    "    return (values - mean) / std\n",
    "\n",
    "def compute_rolling_mean(values, window=7):\n",
    "    \"\"\"Compute rolling mean with specified window.\"\"\"\n",
    "    if len(values) < window:\n",
    "        return np.full_like(values, np.nan, dtype=float)\n",
    "    return pd.Series(values).rolling(window=window, min_periods=1).mean().values\n",
    "\n",
    "# Unit tests for feature engineering\n",
    "\n",
    "class TestFeatureEngineering:\n",
    "    \"\"\"Unit tests for feature engineering functions.\"\"\"\n",
    "    \n",
    "    def test_compute_power_normal_case(self):\n",
    "        \"\"\"Test power calculation with normal values.\"\"\"\n",
    "        vdd = 1.2  # Volts\n",
    "        idd = 250.0  # mA\n",
    "        expected_power = 300.0  # mW\n",
    "        \n",
    "        actual_power = compute_power(vdd, idd)\n",
    "        \n",
    "        assert np.isclose(actual_power, expected_power, rtol=1e-6), \\\n",
    "            f\"Power calculation incorrect: expected {expected_power}, got {actual_power}\"\n",
    "        \n",
    "        print(\"âœ… test_compute_power_normal_case PASSED\")\n",
    "    \n",
    "    def test_compute_power_edge_cases(self):\n",
    "        \"\"\"Test power calculation with edge cases.\"\"\"\n",
    "        # Zero current\n",
    "        assert compute_power(1.2, 0) == 0, \"Power should be 0 when current is 0\"\n",
    "        \n",
    "        # Zero voltage\n",
    "        assert compute_power(0, 250.0) == 0, \"Power should be 0 when voltage is 0\"\n",
    "        \n",
    "        # Both zero\n",
    "        assert compute_power(0, 0) == 0, \"Power should be 0 when both are 0\"\n",
    "        \n",
    "        print(\"âœ… test_compute_power_edge_cases PASSED\")\n",
    "    \n",
    "    def test_power_efficiency_normal_case(self):\n",
    "        \"\"\"Test power efficiency calculation.\"\"\"\n",
    "        freq = 2.5  # GHz\n",
    "        power = 300.0  # mW\n",
    "        expected_efficiency = 2.5 / 300.0  # GHz/mW\n",
    "        \n",
    "        actual_efficiency = compute_power_efficiency(freq, power)\n",
    "        \n",
    "        assert np.isclose(actual_efficiency, expected_efficiency, rtol=1e-6), \\\n",
    "            f\"Efficiency calculation incorrect\"\n",
    "        \n",
    "        print(\"âœ… test_power_efficiency_normal_case PASSED\")\n",
    "    \n",
    "    def test_power_efficiency_zero_power(self):\n",
    "        \"\"\"Test power efficiency with zero power (division by zero).\"\"\"\n",
    "        freq = 2.5\n",
    "        power = 0\n",
    "        \n",
    "        result = compute_power_efficiency(freq, power)\n",
    "        \n",
    "        assert np.isnan(result), \"Should return NaN when power is 0\"\n",
    "        \n",
    "        print(\"âœ… test_power_efficiency_zero_power PASSED\")\n",
    "    \n",
    "    def test_normalize_zscore_normal_case(self):\n",
    "        \"\"\"Test Z-score normalization.\"\"\"\n",
    "        values = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "        \n",
    "        zscores = normalize_zscore(values)\n",
    "        \n",
    "        # Z-scores should have mean â‰ˆ 0 and std â‰ˆ 1\n",
    "        assert np.isclose(np.mean(zscores), 0, atol=1e-6), \"Z-scores mean should be 0\"\n",
    "        assert np.isclose(np.std(zscores), 1, atol=1e-6), \"Z-scores std should be 1\"\n",
    "        \n",
    "        print(\"âœ… test_normalize_zscore_normal_case PASSED\")\n",
    "    \n",
    "    def test_normalize_zscore_constant_values(self):\n",
    "        \"\"\"Test Z-score with constant values (zero std).\"\"\"\n",
    "        values = np.array([5.0, 5.0, 5.0, 5.0])\n",
    "        \n",
    "        zscores = normalize_zscore(values)\n",
    "        \n",
    "        # All values same â†’ all Z-scores should be 0\n",
    "        assert np.all(zscores == 0), \"Constant values should have Z-scores of 0\"\n",
    "        \n",
    "        print(\"âœ… test_normalize_zscore_constant_values PASSED\")\n",
    "    \n",
    "    def test_rolling_mean_normal_case(self):\n",
    "        \"\"\"Test rolling mean calculation.\"\"\"\n",
    "        values = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "        window = 3\n",
    "        \n",
    "        rolling = compute_rolling_mean(values, window=window)\n",
    "        \n",
    "        # Check specific values\n",
    "        assert np.isclose(rolling[2], 2.0, atol=1e-6), \"Rolling mean at index 2 should be 2.0\"\n",
    "        assert np.isclose(rolling[9], 9.0, atol=1e-6), \"Rolling mean at index 9 should be 9.0\"\n",
    "        \n",
    "        print(\"âœ… test_rolling_mean_normal_case PASSED\")\n",
    "    \n",
    "    def test_rolling_mean_insufficient_data(self):\n",
    "        \"\"\"Test rolling mean with fewer values than window.\"\"\"\n",
    "        values = np.array([1, 2])\n",
    "        window = 7\n",
    "        \n",
    "        rolling = compute_rolling_mean(values, window=window)\n",
    "        \n",
    "        # Should return NaN when data < window\n",
    "        assert np.all(np.isnan(rolling)), \"Should return NaN when data < window\"\n",
    "        \n",
    "        print(\"âœ… test_rolling_mean_insufficient_data PASSED\")\n",
    "    \n",
    "    def test_deterministic_output(self):\n",
    "        \"\"\"Test that functions produce deterministic outputs.\"\"\"\n",
    "        vdd = 1.2\n",
    "        idd = 250.0\n",
    "        \n",
    "        # Call multiple times\n",
    "        result1 = compute_power(vdd, idd)\n",
    "        result2 = compute_power(vdd, idd)\n",
    "        result3 = compute_power(vdd, idd)\n",
    "        \n",
    "        assert result1 == result2 == result3, \"Function should be deterministic\"\n",
    "        \n",
    "        print(\"âœ… test_deterministic_output PASSED\")\n",
    "\n",
    "# Run tests\n",
    "fe_tester = TestFeatureEngineering()\n",
    "fe_tester.test_compute_power_normal_case()\n",
    "fe_tester.test_compute_power_edge_cases()\n",
    "fe_tester.test_power_efficiency_normal_case()\n",
    "fe_tester.test_power_efficiency_zero_power()\n",
    "fe_tester.test_normalize_zscore_normal_case()\n",
    "fe_tester.test_normalize_zscore_constant_values()\n",
    "fe_tester.test_rolling_mean_normal_case()\n",
    "fe_tester.test_rolling_mean_insufficient_data()\n",
    "fe_tester.test_deterministic_output()\n",
    "\n",
    "print(\"\\nâœ… All feature engineering tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386544b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property-based testing with Hypothesis\n",
    "from hypothesis import given, strategies as st, assume\n",
    "\n",
    "class TestFeatureProperties:\n",
    "    \"\"\"Property-based tests for feature engineering.\"\"\"\n",
    "    \n",
    "    @given(\n",
    "        vdd=st.floats(min_value=0.1, max_value=2.0, allow_nan=False, allow_infinity=False),\n",
    "        idd=st.floats(min_value=0, max_value=1000, allow_nan=False, allow_infinity=False)\n",
    "    )\n",
    "    def test_power_always_non_negative(self, vdd, idd):\n",
    "        \"\"\"Property: Power should always be non-negative.\"\"\"\n",
    "        power = compute_power(vdd, idd)\n",
    "        assert power >= 0, f\"Power should be non-negative, got {power}\"\n",
    "    \n",
    "    @given(\n",
    "        vdd=st.floats(min_value=0.1, max_value=2.0, allow_nan=False, allow_infinity=False),\n",
    "        idd=st.floats(min_value=0.1, max_value=1000, allow_nan=False, allow_infinity=False)\n",
    "    )\n",
    "    def test_power_increases_with_voltage(self, vdd, idd):\n",
    "        \"\"\"Property: Increasing voltage should increase power (current constant).\"\"\"\n",
    "        power1 = compute_power(vdd, idd)\n",
    "        power2 = compute_power(vdd * 1.1, idd)  # 10% voltage increase\n",
    "        \n",
    "        assert power2 > power1, \"Power should increase with voltage\"\n",
    "    \n",
    "    @given(\n",
    "        values=st.lists(\n",
    "            st.floats(min_value=-100, max_value=100, allow_nan=False, allow_infinity=False),\n",
    "            min_size=10,\n",
    "            max_size=100\n",
    "        )\n",
    "    )\n",
    "    def test_zscore_mean_approximately_zero(self, values):\n",
    "        \"\"\"Property: Z-scores should have mean â‰ˆ 0.\"\"\"\n",
    "        values_array = np.array(values)\n",
    "        \n",
    "        # Skip if all values are the same (would result in division by zero)\n",
    "        if np.std(values_array) < 1e-10:\n",
    "            return\n",
    "        \n",
    "        zscores = normalize_zscore(values_array)\n",
    "        mean_zscore = np.mean(zscores)\n",
    "        \n",
    "        assert abs(mean_zscore) < 1e-6, f\"Z-score mean should be ~0, got {mean_zscore}\"\n",
    "    \n",
    "    @given(\n",
    "        values=st.lists(\n",
    "            st.floats(min_value=-100, max_value=100, allow_nan=False, allow_infinity=False),\n",
    "            min_size=10,\n",
    "            max_size=100\n",
    "        )\n",
    "    )\n",
    "    def test_zscore_std_approximately_one(self, values):\n",
    "        \"\"\"Property: Z-scores should have std â‰ˆ 1.\"\"\"\n",
    "        values_array = np.array(values)\n",
    "        \n",
    "        # Skip if all values are the same\n",
    "        if np.std(values_array) < 1e-10:\n",
    "            return\n",
    "        \n",
    "        zscores = normalize_zscore(values_array)\n",
    "        std_zscore = np.std(zscores)\n",
    "        \n",
    "        assert abs(std_zscore - 1.0) < 1e-6, f\"Z-score std should be ~1, got {std_zscore}\"\n",
    "    \n",
    "    @given(\n",
    "        freq=st.floats(min_value=0.1, max_value=5.0, allow_nan=False, allow_infinity=False),\n",
    "        power=st.floats(min_value=0.1, max_value=1000, allow_nan=False, allow_infinity=False)\n",
    "    )\n",
    "    def test_efficiency_inverse_relationship(self, freq, power):\n",
    "        \"\"\"Property: Doubling power should halve efficiency (freq constant).\"\"\"\n",
    "        eff1 = compute_power_efficiency(freq, power)\n",
    "        eff2 = compute_power_efficiency(freq, power * 2)\n",
    "        \n",
    "        assert np.isclose(eff2, eff1 / 2, rtol=1e-6), \\\n",
    "            \"Doubling power should halve efficiency\"\n",
    "\n",
    "# Run property-based tests\n",
    "print(\"Running property-based tests (100 examples per test)...\")\n",
    "print(\"(Hypothesis will generate random inputs to find edge cases)\\n\")\n",
    "\n",
    "prop_tester = TestFeatureProperties()\n",
    "\n",
    "# Note: In real pytest, these would run automatically with @given decorator\n",
    "# For notebook demonstration, we'll run them manually with specific examples\n",
    "\n",
    "print(\"âœ… test_power_always_non_negative:\")\n",
    "prop_tester.test_power_always_non_negative(1.2, 250.0)\n",
    "prop_tester.test_power_always_non_negative(0.5, 100.0)\n",
    "prop_tester.test_power_always_non_negative(1.8, 400.0)\n",
    "print(\"   Verified for multiple random inputs\")\n",
    "\n",
    "print(\"\\nâœ… test_power_increases_with_voltage:\")\n",
    "prop_tester.test_power_increases_with_voltage(1.0, 200.0)\n",
    "prop_tester.test_power_increases_with_voltage(1.5, 300.0)\n",
    "print(\"   Verified voltage-power relationship\")\n",
    "\n",
    "print(\"\\nâœ… test_zscore_mean_approximately_zero:\")\n",
    "prop_tester.test_zscore_mean_approximately_zero([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "prop_tester.test_zscore_mean_approximately_zero([10.5, 11.2, 9.8, 10.1, 10.9])\n",
    "print(\"   Verified Z-score mean property\")\n",
    "\n",
    "print(\"\\nâœ… test_zscore_std_approximately_one:\")\n",
    "prop_tester.test_zscore_std_approximately_one([1, 2, 3, 4, 5])\n",
    "prop_tester.test_zscore_std_approximately_one([100, 110, 90, 95, 105])\n",
    "print(\"   Verified Z-score std property\")\n",
    "\n",
    "print(\"\\nâœ… test_efficiency_inverse_relationship:\")\n",
    "prop_tester.test_efficiency_inverse_relationship(2.5, 300.0)\n",
    "prop_tester.test_efficiency_inverse_relationship(3.0, 400.0)\n",
    "print(\"   Verified efficiency-power relationship\")\n",
    "\n",
    "print(\"\\nâœ… All property-based tests passed!\")\n",
    "print(\"In production, Hypothesis would test 100+ random inputs automatically\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75852a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic training data\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_yield_data(n_samples=1000):\n",
    "    \"\"\"Generate synthetic device yield data for testing.\"\"\"\n",
    "    vdd = np.random.normal(1.2, 0.02, n_samples)\n",
    "    idd = np.random.normal(250, 15, n_samples)\n",
    "    freq = np.random.normal(2.5, 0.1, n_samples)\n",
    "    temp = np.random.normal(85, 5, n_samples)\n",
    "    \n",
    "    # Yield based on parametric limits\n",
    "    yield_label = (\n",
    "        (vdd >= 1.15) & (vdd <= 1.25) &\n",
    "        (idd <= 280) &\n",
    "        (freq >= 2.3) &\n",
    "        (temp <= 100)\n",
    "    ).astype(int)\n",
    "    \n",
    "    X = pd.DataFrame({\n",
    "        'vdd': vdd,\n",
    "        'idd': idd,\n",
    "        'freq': freq,\n",
    "        'temp': temp\n",
    "    })\n",
    "    y = yield_label\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Model testing class\n",
    "\n",
    "class TestYieldPredictionModel:\n",
    "    \"\"\"Tests for yield prediction model.\"\"\"\n",
    "    \n",
    "    def test_model_trains_without_error(self):\n",
    "        \"\"\"Test that model trains successfully.\"\"\"\n",
    "        X, y = generate_yield_data(n_samples=500)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=10, random_state=42, max_depth=5)\n",
    "        \n",
    "        # Should not raise any exception\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"âœ… test_model_trains_without_error PASSED\")\n",
    "    \n",
    "    def test_model_makes_predictions(self):\n",
    "        \"\"\"Test that model produces predictions.\"\"\"\n",
    "        X, y = generate_yield_data(n_samples=500)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Check predictions have correct shape\n",
    "        assert len(predictions) == len(X_test), \"Prediction count mismatch\"\n",
    "        \n",
    "        # Check predictions are valid labels (0 or 1)\n",
    "        assert set(predictions).issubset({0, 1}), \"Invalid prediction labels\"\n",
    "        \n",
    "        print(\"âœ… test_model_makes_predictions PASSED\")\n",
    "    \n",
    "    def test_model_predicts_probabilities(self):\n",
    "        \"\"\"Test that model produces valid probabilities.\"\"\"\n",
    "        X, y = generate_yield_data(n_samples=500)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        probabilities = model.predict_proba(X_test)\n",
    "        \n",
    "        # Check shape (n_samples, n_classes)\n",
    "        assert probabilities.shape == (len(X_test), 2), \"Probability shape incorrect\"\n",
    "        \n",
    "        # Check probabilities sum to 1\n",
    "        prob_sums = probabilities.sum(axis=1)\n",
    "        assert np.allclose(prob_sums, 1.0), \"Probabilities don't sum to 1\"\n",
    "        \n",
    "        # Check probabilities in [0, 1]\n",
    "        assert np.all(probabilities >= 0) and np.all(probabilities <= 1), \\\n",
    "            \"Probabilities out of [0, 1] range\"\n",
    "        \n",
    "        print(\"âœ… test_model_predicts_probabilities PASSED\")\n",
    "    \n",
    "    def test_model_beats_baseline(self):\n",
    "        \"\"\"Test that model performs better than random guessing.\"\"\"\n",
    "        X, y = generate_yield_data(n_samples=500)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        accuracy = model.score(X_test, y_test)\n",
    "        \n",
    "        # Model should beat random guessing (50% for binary)\n",
    "        baseline_accuracy = 0.5\n",
    "        assert accuracy > baseline_accuracy + 0.1, \\\n",
    "            f\"Model accuracy ({accuracy:.3f}) not significantly better than baseline ({baseline_accuracy})\"\n",
    "        \n",
    "        print(f\"âœ… test_model_beats_baseline PASSED (accuracy: {accuracy:.3f} > {baseline_accuracy + 0.1})\")\n",
    "    \n",
    "    def test_model_reproducibility(self):\n",
    "        \"\"\"Test that model produces same results with same seed.\"\"\"\n",
    "        X, y = generate_yield_data(n_samples=500)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Train two models with same seed\n",
    "        model1 = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "        model1.fit(X_train, y_train)\n",
    "        pred1 = model1.predict(X_test)\n",
    "        \n",
    "        model2 = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "        model2.fit(X_train, y_train)\n",
    "        pred2 = model2.predict(X_test)\n",
    "        \n",
    "        # Predictions should be identical\n",
    "        assert np.array_equal(pred1, pred2), \"Model not reproducible with same seed\"\n",
    "        \n",
    "        print(\"âœ… test_model_reproducibility PASSED\")\n",
    "    \n",
    "    def test_model_not_predicting_all_same_class(self):\n",
    "        \"\"\"Test that model doesn't predict all zeros or all ones.\"\"\"\n",
    "        X, y = generate_yield_data(n_samples=500)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Check both classes are predicted\n",
    "        unique_predictions = set(predictions)\n",
    "        assert len(unique_predictions) > 1, \"Model predicting only one class (broken model)\"\n",
    "        \n",
    "        print(f\"âœ… test_model_not_predicting_all_same_class PASSED (predicts {unique_predictions})\")\n",
    "    \n",
    "    def test_model_feature_importance_not_zero(self):\n",
    "        \"\"\"Test that model uses features (importance > 0).\"\"\"\n",
    "        X, y = generate_yield_data(n_samples=500)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # At least one feature should have non-zero importance\n",
    "        assert np.any(importances > 0), \"All feature importances are zero (model not learning)\"\n",
    "        \n",
    "        # Sum of importances should be 1.0\n",
    "        assert np.isclose(importances.sum(), 1.0), \"Feature importances don't sum to 1\"\n",
    "        \n",
    "        print(f\"âœ… test_model_feature_importance_not_zero PASSED\")\n",
    "\n",
    "# Run model tests\n",
    "model_tester = TestYieldPredictionModel()\n",
    "model_tester.test_model_trains_without_error()\n",
    "model_tester.test_model_makes_predictions()\n",
    "model_tester.test_model_predicts_probabilities()\n",
    "model_tester.test_model_beats_baseline()\n",
    "model_tester.test_model_reproducibility()\n",
    "model_tester.test_model_not_predicting_all_same_class()\n",
    "model_tester.test_model_feature_importance_not_zero()\n",
    "\n",
    "print(\"\\nâœ… All model tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da14099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration tests for ML pipeline\n",
    "\n",
    "class YieldPredictionPipeline:\n",
    "    \"\"\"Complete yield prediction pipeline for testing.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.feature_columns = ['vdd', 'idd', 'freq', 'temp', 'power']\n",
    "    \n",
    "    def train(self, data):\n",
    "        \"\"\"Train pipeline on data.\"\"\"\n",
    "        # Extract features\n",
    "        X = data[['vdd', 'idd', 'freq', 'temp']].copy()\n",
    "        X['power'] = compute_power(X['vdd'], X['idd'])\n",
    "        y = data['yield']\n",
    "        \n",
    "        # Train model\n",
    "        self.model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "        self.model.fit(X, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"Make predictions on new data.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Pipeline not trained\")\n",
    "        \n",
    "        # Extract and engineer features\n",
    "        X = data[['vdd', 'idd', 'freq', 'temp']].copy()\n",
    "        X['power'] = compute_power(X['vdd'], X['idd'])\n",
    "        \n",
    "        # Predict\n",
    "        predictions = self.model.predict(X)\n",
    "        probabilities = self.model.predict_proba(X)\n",
    "        \n",
    "        return predictions, probabilities\n",
    "\n",
    "class TestMLPipelineIntegration:\n",
    "    \"\"\"Integration tests for complete ML pipeline.\"\"\"\n",
    "    \n",
    "    def test_end_to_end_pipeline(self):\n",
    "        \"\"\"Test complete pipeline: data â†’ features â†’ train â†’ predict.\"\"\"\n",
    "        # Generate training data\n",
    "        X, y = generate_yield_data(n_samples=1000)\n",
    "        train_data = X.copy()\n",
    "        train_data['yield'] = y\n",
    "        \n",
    "        # Train pipeline\n",
    "        pipeline = YieldPredictionPipeline()\n",
    "        pipeline.train(train_data)\n",
    "        \n",
    "        # Generate test data\n",
    "        X_test, y_test = generate_yield_data(n_samples=200)\n",
    "        test_data = X_test.copy()\n",
    "        \n",
    "        # Predict\n",
    "        predictions, probabilities = pipeline.predict(test_data)\n",
    "        \n",
    "        # Verify outputs\n",
    "        assert len(predictions) == 200, \"Wrong number of predictions\"\n",
    "        assert probabilities.shape == (200, 2), \"Wrong probability shape\"\n",
    "        assert set(predictions).issubset({0, 1}), \"Invalid predictions\"\n",
    "        \n",
    "        # Verify accuracy\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        assert accuracy > 0.6, f\"Pipeline accuracy too low: {accuracy:.3f}\"\n",
    "        \n",
    "        print(f\"âœ… test_end_to_end_pipeline PASSED (accuracy: {accuracy:.3f})\")\n",
    "    \n",
    "    def test_pipeline_handles_single_sample(self):\n",
    "        \"\"\"Test pipeline can process single device.\"\"\"\n",
    "        # Train pipeline\n",
    "        X, y = generate_yield_data(n_samples=1000)\n",
    "        train_data = X.copy()\n",
    "        train_data['yield'] = y\n",
    "        \n",
    "        pipeline = YieldPredictionPipeline()\n",
    "        pipeline.train(train_data)\n",
    "        \n",
    "        # Single device\n",
    "        single_device = pd.DataFrame({\n",
    "            'vdd': [1.2],\n",
    "            'idd': [250.0],\n",
    "            'freq': [2.5],\n",
    "            'temp': [85.0]\n",
    "        })\n",
    "        \n",
    "        predictions, probabilities = pipeline.predict(single_device)\n",
    "        \n",
    "        assert len(predictions) == 1, \"Should predict for single device\"\n",
    "        assert predictions[0] in [0, 1], \"Invalid prediction\"\n",
    "        \n",
    "        print(\"âœ… test_pipeline_handles_single_sample PASSED\")\n",
    "    \n",
    "    def test_pipeline_handles_batch(self):\n",
    "        \"\"\"Test pipeline can process large batches efficiently.\"\"\"\n",
    "        import time\n",
    "        \n",
    "        # Train pipeline\n",
    "        X, y = generate_yield_data(n_samples=1000)\n",
    "        train_data = X.copy()\n",
    "        train_data['yield'] = y\n",
    "        \n",
    "        pipeline = YieldPredictionPipeline()\n",
    "        pipeline.train(train_data)\n",
    "        \n",
    "        # Large batch\n",
    "        X_batch, _ = generate_yield_data(n_samples=10000)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        predictions, _ = pipeline.predict(X_batch)\n",
    "        elapsed_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Check latency (should be <500ms for 10K predictions)\n",
    "        assert elapsed_ms < 500, f\"Batch prediction too slow: {elapsed_ms:.1f}ms\"\n",
    "        assert len(predictions) == 10000, \"Wrong prediction count\"\n",
    "        \n",
    "        throughput = 10000 / (elapsed_ms / 1000)\n",
    "        print(f\"âœ… test_pipeline_handles_batch PASSED (latency: {elapsed_ms:.1f}ms, throughput: {throughput:.0f} pred/sec)\")\n",
    "    \n",
    "    def test_pipeline_error_handling(self):\n",
    "        \"\"\"Test pipeline handles invalid inputs gracefully.\"\"\"\n",
    "        # Train pipeline\n",
    "        X, y = generate_yield_data(n_samples=1000)\n",
    "        train_data = X.copy()\n",
    "        train_data['yield'] = y\n",
    "        \n",
    "        pipeline = YieldPredictionPipeline()\n",
    "        pipeline.train(train_data)\n",
    "        \n",
    "        # Missing column\n",
    "        invalid_data = pd.DataFrame({\n",
    "            'vdd': [1.2],\n",
    "            'idd': [250.0]\n",
    "            # Missing freq, temp\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            pipeline.predict(invalid_data)\n",
    "            assert False, \"Should raise error on missing columns\"\n",
    "        except KeyError:\n",
    "            print(\"âœ… test_pipeline_error_handling PASSED (correctly raises error)\")\n",
    "    \n",
    "    def test_pipeline_reproducibility(self):\n",
    "        \"\"\"Test pipeline produces same results on same data.\"\"\"\n",
    "        # Train pipeline twice\n",
    "        X, y = generate_yield_data(n_samples=1000)\n",
    "        train_data = X.copy()\n",
    "        train_data['yield'] = y\n",
    "        \n",
    "        pipeline1 = YieldPredictionPipeline()\n",
    "        pipeline1.train(train_data)\n",
    "        \n",
    "        pipeline2 = YieldPredictionPipeline()\n",
    "        pipeline2.train(train_data)\n",
    "        \n",
    "        # Test data\n",
    "        X_test, _ = generate_yield_data(n_samples=100)\n",
    "        \n",
    "        pred1, _ = pipeline1.predict(X_test)\n",
    "        pred2, _ = pipeline2.predict(X_test)\n",
    "        \n",
    "        # Predictions should be identical (same random seed)\n",
    "        assert np.array_equal(pred1, pred2), \"Pipeline not reproducible\"\n",
    "        \n",
    "        print(\"âœ… test_pipeline_reproducibility PASSED\")\n",
    "\n",
    "# Run integration tests\n",
    "integration_tester = TestMLPipelineIntegration()\n",
    "integration_tester.test_end_to_end_pipeline()\n",
    "integration_tester.test_pipeline_handles_single_sample()\n",
    "integration_tester.test_pipeline_handles_batch()\n",
    "integration_tester.test_pipeline_error_handling()\n",
    "integration_tester.test_pipeline_reproducibility()\n",
    "\n",
    "print(\"\\nâœ… All integration tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ce9c0",
   "metadata": {},
   "source": [
    "## 7. Real-World Project Templates\n",
    "\n",
    "**8 production-ready ML testing projects** (4 post-silicon + 4 general AI/ML).\n",
    "\n",
    "Each project includes:\n",
    "- Complete test strategy\n",
    "- Test coverage targets\n",
    "- Critical tests to implement\n",
    "- Business value and impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = \"\"\"\n",
    "================================================================================\n",
    "REAL-WORLD ML TESTING PROJECTS\n",
    "================================================================================\n",
    "\n",
    "POST-SILICON VALIDATION PROJECTS\n",
    "---------------------------------\n",
    "\n",
    "1. YIELD PREDICTION MODEL TEST SUITE (COMPREHENSIVE)\n",
    "   \n",
    "   Objective: 90% test coverage for yield prediction pipeline, catch bugs\n",
    "              before production deployment\n",
    "   \n",
    "   Success Metrics:\n",
    "   - Code coverage: >90% (pytest-cov)\n",
    "   - Test count: 100+ unit tests, 20+ integration tests\n",
    "   - CI/CD: All tests pass before merge\n",
    "   - Bug detection: Catch 95% of bugs pre-production\n",
    "   \n",
    "   Test Strategy:\n",
    "   \n",
    "   **Unit Tests (70 tests):**\n",
    "   - Data validation: Schema, ranges, nulls (15 tests)\n",
    "   - Feature engineering: Power, efficiency, Z-scores (20 tests)\n",
    "   - Model components: Training, prediction, probabilities (15 tests)\n",
    "   - Utilities: Data loading, preprocessing, metrics (20 tests)\n",
    "   \n",
    "   **Integration Tests (15 tests):**\n",
    "   - End-to-end pipeline: STDF â†’ features â†’ model â†’ prediction\n",
    "   - Performance: Latency <100ms p99, throughput >1000 pred/sec\n",
    "   - Error handling: Graceful degradation on bad inputs\n",
    "   - Reproducibility: Same seed â†’ same results\n",
    "   \n",
    "   **Property-Based Tests (10 tests):**\n",
    "   - Feature invariants: Power â‰¥ 0, Z-score mean â‰ˆ 0\n",
    "   - Model properties: Predictions in [0,1], probabilities sum to 1\n",
    "   - Edge cases: Extreme values, boundary conditions\n",
    "   \n",
    "   **System Tests (5 tests):**\n",
    "   - Load testing: 10K concurrent predictions\n",
    "   - Monitoring: Alerts trigger on drift\n",
    "   - A/B testing: New model vs production model\n",
    "   \n",
    "   Critical Tests:\n",
    "   ```python\n",
    "   def test_yield_prediction_accuracy_above_threshold():\n",
    "       \\\"\\\"\\\"CRITICAL: Model must achieve >85% accuracy.\\\"\\\"\\\"\n",
    "       model = train_yield_model(data)\n",
    "       accuracy = evaluate_model(model, test_data)\n",
    "       assert accuracy > 0.85, f\"Accuracy {accuracy:.3f} below threshold\"\n",
    "   \n",
    "   def test_no_data_leakage():\n",
    "       \\\"\\\"\\\"CRITICAL: Features use only past data (point-in-time).\\\"\\\"\\\"\n",
    "       features = compute_features(data, timestamp=T)\n",
    "       future_data = data[data['timestamp'] > T]\n",
    "       assert not feature_uses_future_data(features, future_data)\n",
    "   \n",
    "   def test_prediction_latency_sla():\n",
    "       \\\"\\\"\\\"CRITICAL: Predictions must complete <100ms p99.\\\"\\\"\\\"\n",
    "       latencies = benchmark_prediction_latency(model, n=10000)\n",
    "       p99_latency = np.percentile(latencies, 99)\n",
    "       assert p99_latency < 100, f\"p99 latency {p99_latency}ms exceeds 100ms SLA\"\n",
    "   ```\n",
    "   \n",
    "   Business Value: Prevent production incidents ($500K/incident), faster\n",
    "                   development (CI catches bugs early), compliance (audit trail)\n",
    "\n",
    "\n",
    "2. TEST TIME PREDICTION MODEL VALIDATION FRAMEWORK\n",
    "   \n",
    "   Objective: Validate test time prediction model before deployment, ensure\n",
    "              <5% MAPE and no catastrophic failures\n",
    "   \n",
    "   Success Metrics:\n",
    "   - MAPE: <5% on holdout set\n",
    "   - No predictions >10x actual test time (catastrophic errors)\n",
    "   - Passes 30+ validation checks\n",
    "   - Fairness: No bias across device types\n",
    "   \n",
    "   Validation Framework:\n",
    "   \n",
    "   **Performance Validation:**\n",
    "   - Accuracy: MAPE, RMSE, RÂ² score\n",
    "   - Error distribution: Check for systematic bias\n",
    "   - Percentile accuracy: P10, P50, P90 predictions\n",
    "   \n",
    "   **Robustness Validation:**\n",
    "   - Edge cases: Fast devices (<1 sec), slow devices (>60 sec)\n",
    "   - Outliers: Handle 3-sigma extreme test times\n",
    "   - Missing features: Graceful degradation with imputation\n",
    "   \n",
    "   **Fairness Validation:**\n",
    "   - Device type parity: Equal accuracy across types\n",
    "   - Lot-to-lot consistency: No bias toward specific lots\n",
    "   - Wafer position: No edge vs center bias\n",
    "   \n",
    "   **Safety Validation:**\n",
    "   - No catastrophic errors: Predictions within 10x actual\n",
    "   - Conservative estimates: Better to overestimate than underestimate\n",
    "   - Alarm on outliers: Flag predictions >3 std from mean\n",
    "   \n",
    "   Implementation:\n",
    "   ```python\n",
    "   class TestTimeModelValidator:\n",
    "       def __init__(self, model, test_data):\n",
    "           self.model = model\n",
    "           self.test_data = test_data\n",
    "       \n",
    "       def validate_accuracy(self):\n",
    "           predictions = self.model.predict(self.test_data.X)\n",
    "           actual = self.test_data.y\n",
    "           \n",
    "           mape = mean_absolute_percentage_error(actual, predictions)\n",
    "           assert mape < 0.05, f\"MAPE {mape:.3f} exceeds 5% threshold\"\n",
    "       \n",
    "       def validate_no_catastrophic_errors(self):\n",
    "           predictions = self.model.predict(self.test_data.X)\n",
    "           actual = self.test_data.y\n",
    "           \n",
    "           max_error = (predictions / actual).max()\n",
    "           assert max_error < 10, f\"Catastrophic error: {max_error}x actual\"\n",
    "       \n",
    "       def validate_fairness(self):\n",
    "           for device_type in self.test_data['device_type'].unique():\n",
    "               subset = self.test_data[self.test_data['device_type'] == device_type]\n",
    "               accuracy = self.model.score(subset.X, subset.y)\n",
    "               assert accuracy > 0.80, f\"{device_type} accuracy too low\"\n",
    "   ```\n",
    "   \n",
    "   Business Value: $10M annual savings (optimize test flow), prevent\n",
    "                   underestimating test time (capacity planning), compliance\n",
    "\n",
    "\n",
    "3. WAFER MAP SPATIAL FEATURE TESTING (EDGE CASES)\n",
    "   \n",
    "   Objective: Exhaustive testing of spatial feature engineering for wafer maps,\n",
    "              catch coordinate system bugs, edge effects, incomplete wafers\n",
    "   \n",
    "   Success Metrics:\n",
    "   - Property tests: 1000+ random wafer geometries tested\n",
    "   - Edge coverage: All wafer edge positions tested\n",
    "   - Symmetry verification: Radial features symmetric\n",
    "   - Coordinate systems: Cartesian and polar both correct\n",
    "   \n",
    "   Spatial Property Tests:\n",
    "   \n",
    "   **Neighbor Invariants:**\n",
    "   ```python\n",
    "   @given(die_x=st.integers(0, 50), die_y=st.integers(0, 50))\n",
    "   def test_neighbor_count_max_eight(die_x, die_y):\n",
    "       \\\"\\\"\\\"Property: Die can have at most 8 neighbors.\\\"\\\"\\\"\n",
    "       wafer = generate_test_wafer()\n",
    "       neighbors = get_neighbors(wafer, die_x, die_y)\n",
    "       assert len(neighbors) <= 8\n",
    "   \n",
    "   @given(die_x=st.integers(0, 50), die_y=st.integers(0, 50))\n",
    "   def test_neighbor_symmetry(die_x, die_y):\n",
    "       \\\"\\\"\\\"Property: If A is neighbor of B, B is neighbor of A.\\\"\\\"\\\"\n",
    "       wafer = generate_test_wafer()\n",
    "       neighbors_of_A = get_neighbors(wafer, die_x, die_y)\n",
    "       for neighbor in neighbors_of_A:\n",
    "           neighbors_of_neighbor = get_neighbors(wafer, neighbor.x, neighbor.y)\n",
    "           assert (die_x, die_y) in [(n.x, n.y) for n in neighbors_of_neighbor]\n",
    "   ```\n",
    "   \n",
    "   **Radial Distance Invariants:**\n",
    "   ```python\n",
    "   def test_radial_distance_center_zero():\n",
    "       \\\"\\\"\\\"Center die has radial distance 0.\\\"\\\"\\\"\n",
    "       wafer = generate_test_wafer(radius=50)\n",
    "       center_distance = compute_radial_distance(25, 25, wafer.center)\n",
    "       assert center_distance == 0\n",
    "   \n",
    "   def test_radial_distance_increases_outward():\n",
    "       \\\"\\\"\\\"Distance increases moving away from center.\\\"\\\"\\\"\n",
    "       wafer = generate_test_wafer()\n",
    "       for r in range(0, 50):\n",
    "           distance = compute_radial_distance(25 + r, 25, wafer.center)\n",
    "           next_distance = compute_radial_distance(25 + r + 1, 25, wafer.center)\n",
    "           assert next_distance > distance\n",
    "   ```\n",
    "   \n",
    "   **Edge Effect Tests:**\n",
    "   ```python\n",
    "   def test_edge_dies_flagged():\n",
    "       \\\"\\\"\\\"Dies near wafer edge should be flagged.\\\"\\\"\\\"\n",
    "       wafer = generate_test_wafer(radius=50)\n",
    "       edge_distance_threshold = 5  # mm from edge\n",
    "       \n",
    "       for die in wafer.dies:\n",
    "           dist_to_edge = compute_distance_to_edge(die, wafer.radius)\n",
    "           is_edge = is_edge_die(die, wafer.radius, threshold=edge_distance_threshold)\n",
    "           \n",
    "           if dist_to_edge < edge_distance_threshold:\n",
    "               assert is_edge, f\"Die at {die.x},{die.y} should be flagged as edge\"\n",
    "           else:\n",
    "               assert not is_edge\n",
    "   ```\n",
    "   \n",
    "   Business Value: Prevent spatial analysis errors (5% yield improvement = $10M),\n",
    "                   correct wafer map visualizations, accurate defect clustering\n",
    "\n",
    "\n",
    "4. BINNING MODEL A/B TEST FRAMEWORK\n",
    "   \n",
    "   Objective: Statistical testing framework to validate new binning model (v2.0)\n",
    "              vs production (v1.5) before deployment\n",
    "   \n",
    "   Success Metrics:\n",
    "   - Statistical significance: p-value <0.05 for accuracy improvement\n",
    "   - Practical significance: +2% F1 score improvement\n",
    "   - No regression: All bins maintain or improve accuracy\n",
    "   - Consistency: Results stable across 5 different random seeds\n",
    "   \n",
    "   A/B Test Framework:\n",
    "   \n",
    "   **Statistical Tests:**\n",
    "   ```python\n",
    "   class BinningModelABTest:\n",
    "       def __init__(self, model_v1, model_v2, test_data):\n",
    "           self.model_v1 = model_v1\n",
    "           self.model_v2 = model_v2\n",
    "           self.test_data = test_data\n",
    "       \n",
    "       def test_accuracy_improvement_significant(self):\n",
    "           \\\"\\\"\\\"Test if v2.0 accuracy is statistically better than v1.5.\\\"\\\"\\\"\n",
    "           from scipy.stats import wilcoxon\n",
    "           \n",
    "           # Bootstrap 100 times\n",
    "           v1_accuracies = []\n",
    "           v2_accuracies = []\n",
    "           \n",
    "           for seed in range(100):\n",
    "               sample = self.test_data.sample(frac=0.8, random_state=seed)\n",
    "               v1_acc = accuracy_score(sample.y, self.model_v1.predict(sample.X))\n",
    "               v2_acc = accuracy_score(sample.y, self.model_v2.predict(sample.X))\n",
    "               v1_accuracies.append(v1_acc)\n",
    "               v2_accuracies.append(v2_acc)\n",
    "           \n",
    "           # Wilcoxon signed-rank test (paired samples)\n",
    "           statistic, p_value = wilcoxon(v2_accuracies, v1_accuracies, alternative='greater')\n",
    "           \n",
    "           assert p_value < 0.05, f\"v2.0 not significantly better (p={p_value:.4f})\"\n",
    "           \n",
    "           mean_improvement = np.mean(v2_accuracies) - np.mean(v1_accuracies)\n",
    "           assert mean_improvement > 0.02, f\"Improvement too small: {mean_improvement:.3f}\"\n",
    "       \n",
    "       def test_no_bin_regression(self):\n",
    "           \\\"\\\"\\\"Test that no bin category has worse accuracy in v2.0.\\\"\\\"\\\"\n",
    "           for bin_category in self.test_data['bin'].unique():\n",
    "               subset = self.test_data[self.test_data['bin'] == bin_category]\n",
    "               \n",
    "               v1_f1 = f1_score(subset.y, self.model_v1.predict(subset.X))\n",
    "               v2_f1 = f1_score(subset.y, self.model_v2.predict(subset.X))\n",
    "               \n",
    "               assert v2_f1 >= v1_f1 - 0.01, \\\n",
    "                   f\"Bin {bin_category} regressed: v1={v1_f1:.3f} â†’ v2={v2_f1:.3f}\"\n",
    "   ```\n",
    "   \n",
    "   **Shadow Mode Testing:**\n",
    "   - Run v2.0 in shadow (doesn't affect production)\n",
    "   - Compare predictions to v1.5 on live traffic\n",
    "   - Collect 7 days of data before go-live\n",
    "   \n",
    "   Business Value: Safe model updates (no production regressions), data-driven\n",
    "                   decisions (statistical evidence), compliance (validation docs)\n",
    "\n",
    "\n",
    "GENERAL AI/ML PROJECTS\n",
    "----------------------\n",
    "\n",
    "5. FRAUD DETECTION MODEL TEST SUITE\n",
    "   \n",
    "   Objective: Comprehensive testing for fraud detection model with focus on\n",
    "              precision (minimize false positives) and recall (catch fraud)\n",
    "   \n",
    "   Success Metrics:\n",
    "   - Precision: >95% (low false positive rate)\n",
    "   - Recall: >90% (catch most fraud)\n",
    "   - Test coverage: >85% code coverage\n",
    "   - Response time: <50ms p99 (real-time blocking)\n",
    "   \n",
    "   Critical Tests:\n",
    "   - Known fraud patterns: Model catches 100% of known fraud types\n",
    "   - False positive rate: <5% on legitimate transactions\n",
    "   - Edge cases: High-value transactions, cross-border, new merchants\n",
    "   - Adversarial robustness: Resistant to evasion techniques\n",
    "   - Fairness: No demographic bias\n",
    "   \n",
    "   Business Value: $50M annual fraud prevented, <2% customer friction\n",
    "\n",
    "\n",
    "6. CHURN PREDICTION MODEL VALIDATION\n",
    "   \n",
    "   Objective: Validate churn prediction model with emphasis on early detection\n",
    "              (30 days before churn) and actionable predictions\n",
    "   \n",
    "   Success Metrics:\n",
    "   - AUC-ROC: >0.85 (discrimination)\n",
    "   - Early detection: Predict 30 days ahead with >80% recall\n",
    "   - Calibration: Predicted probabilities match actual churn rates\n",
    "   - Actionable: Top 10% highest risk = 60%+ actual churn\n",
    "   \n",
    "   Validation Tests:\n",
    "   - Temporal validation: Train on 2023, test on 2024 data\n",
    "   - Cohort analysis: Consistent performance across customer segments\n",
    "   - Calibration plots: Predicted vs actual churn rates aligned\n",
    "   - Feature drift monitoring: Alert if distributions change\n",
    "   \n",
    "   Business Value: $100M customer retention, targeted interventions\n",
    "\n",
    "\n",
    "7. RECOMMENDATION SYSTEM TESTING (E-COMMERCE)\n",
    "   \n",
    "   Objective: Test recommendation quality, diversity, and business metrics\n",
    "              before A/B testing with live traffic\n",
    "   \n",
    "   Success Metrics:\n",
    "   - Offline metrics: NDCG@10 >0.7, Precision@10 >0.4\n",
    "   - Diversity: >80% recommendations cover >20 different categories\n",
    "   - Novelty: >30% recommendations are new items (not previously purchased)\n",
    "   - Fairness: All merchants get proportional exposure\n",
    "   \n",
    "   Test Types:\n",
    "   - Offline replay: Simulate recommendations on historical data\n",
    "   - User journey testing: Multi-step sessions, cart additions\n",
    "   - Cold start testing: New users, new products\n",
    "   - Business rule validation: Never recommend out-of-stock items\n",
    "   \n",
    "   Business Value: +15% CTR ($20M revenue), improved customer experience\n",
    "\n",
    "\n",
    "8. DEMAND FORECASTING MODEL VALIDATION (RETAIL)\n",
    "   \n",
    "   Objective: Validate SKU-level demand forecasts with emphasis on inventory\n",
    "              optimization and stockout prevention\n",
    "   \n",
    "   Success Metrics:\n",
    "   - MAPE: <15% across all SKUs\n",
    "   - Stockout prevention: >95% in-stock for high-demand items\n",
    "   - Overstock minimization: <10% excess inventory\n",
    "   - Forecast horizon: 28-day accuracy within 20%\n",
    "   \n",
    "   Validation Framework:\n",
    "   - Backtesting: Rolling window validation on 2 years historical data\n",
    "   - Promotion testing: Accuracy during promotional periods\n",
    "   - Seasonality: Correct holiday and seasonal patterns\n",
    "   - External factors: Weather, economic indicators correlation\n",
    "   \n",
    "   Business Value: $30M inventory reduction, 10% revenue increase\n",
    "\n",
    "\n",
    "================================================================================\n",
    "IMPLEMENTATION CHECKLIST (FOR ALL PROJECTS)\n",
    "================================================================================\n",
    "\n",
    "Phase 1: Test Infrastructure Setup (Week 1)\n",
    "â–¡ pytest framework configured\n",
    "â–¡ Code coverage reporting (pytest-cov)\n",
    "â–¡ CI/CD integration (GitHub Actions, Jenkins)\n",
    "â–¡ Test data fixtures (synthetic + real samples)\n",
    "â–¡ Mocking frameworks for external dependencies\n",
    "\n",
    "Phase 2: Unit Test Implementation (Week 2-3)\n",
    "â–¡ Data validation tests (schema, ranges, nulls)\n",
    "â–¡ Feature engineering tests (correctness, edge cases)\n",
    "â–¡ Model component tests (training, prediction)\n",
    "â–¡ Utility function tests (helpers, metrics)\n",
    "â–¡ Target: >80% code coverage\n",
    "\n",
    "Phase 3: Integration Test Implementation (Week 4)\n",
    "â–¡ End-to-end pipeline tests\n",
    "â–¡ Performance/latency tests\n",
    "â–¡ Error handling tests\n",
    "â–¡ Reproducibility tests\n",
    "â–¡ Load testing (concurrent requests)\n",
    "\n",
    "Phase 4: Property-Based Testing (Week 5)\n",
    "â–¡ Hypothesis setup and configuration\n",
    "â–¡ Feature invariants (mathematical properties)\n",
    "â–¡ Model properties (output constraints)\n",
    "â–¡ Edge case generation (boundary conditions)\n",
    "â–¡ Run 1000+ random test cases per property\n",
    "\n",
    "Phase 5: Model Validation (Week 6)\n",
    "â–¡ Accuracy/performance benchmarks\n",
    "â–¡ Fairness and bias testing\n",
    "â–¡ Robustness testing (adversarial, noise)\n",
    "â–¡ A/B testing framework\n",
    "â–¡ Shadow mode deployment\n",
    "\n",
    "Phase 6: Production Monitoring Tests (Week 7)\n",
    "â–¡ Data drift detection tests\n",
    "â–¡ Model performance monitoring\n",
    "â–¡ Alert verification tests\n",
    "â–¡ Dashboard functionality tests\n",
    "â–¡ Incident response testing\n",
    "\n",
    "Best Practices:\n",
    "- Write tests BEFORE implementing features (TDD)\n",
    "- Aim for 90% code coverage, 100% critical path coverage\n",
    "- Run fast tests in CI (<5 min), slow tests nightly\n",
    "- Mock external dependencies (databases, APIs)\n",
    "- Use fixtures for common test data\n",
    "- Property tests for mathematical invariants\n",
    "- Integration tests for critical workflows\n",
    "- Version test data with code\n",
    "\"\"\"\n",
    "\n",
    "print(projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939a95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "takeaways = \"\"\"\n",
    "================================================================================\n",
    "KEY TAKEAWAYS: ML TESTING & VALIDATION\n",
    "================================================================================\n",
    "\n",
    "1. WHY ML TESTING IS DIFFERENT\n",
    "   ---------------------------\n",
    "   \n",
    "   Traditional Software Testing:\n",
    "   - Binary outcomes: Works or doesn't work\n",
    "   - Deterministic: Same input â†’ same output\n",
    "   - Code only: Test logic and algorithms\n",
    "   - Bugs are obvious: Crashes, wrong outputs\n",
    "   \n",
    "   ML System Testing:\n",
    "   - Continuous outcomes: Accuracy 85% vs 90%\n",
    "   - Non-deterministic: Randomness, stochastic algorithms\n",
    "   - Code + Data + Model: Test all three components\n",
    "   - Bugs are subtle: Gradual performance degradation\n",
    "   \n",
    "   Unique ML Challenges:\n",
    "   1. **Data Dependencies**: Model performance depends on data quality\n",
    "   2. **No Ground Truth**: Can't verify correctness in production\n",
    "   3. **Gradual Degradation**: Data drift slowly reduces accuracy\n",
    "   4. **Statistical Testing**: Assertions use distributions not exact values\n",
    "   5. **Training-Serving Skew**: Features computed differently\n",
    "   \n",
    "   Testing Philosophy:\n",
    "   - Unit tests: Test components in isolation\n",
    "   - Integration tests: Test components together\n",
    "   - System tests: Test entire ML system\n",
    "   - Monitoring tests: Test production behavior\n",
    "   \n",
    "   ML Testing Pyramid:\n",
    "   70% Unit Tests (fast, many)\n",
    "   20% Integration Tests (medium speed)\n",
    "   10% System Tests (slow, few)\n",
    "\n",
    "\n",
    "2. UNIT TESTING DATA PIPELINES\n",
    "   ----------------------------\n",
    "   \n",
    "   What to Test:\n",
    "   \n",
    "   **Schema Validation:**\n",
    "   - Column names and data types correct\n",
    "   - Required columns present\n",
    "   - No unexpected columns\n",
    "   \n",
    "   ```python\n",
    "   def test_schema():\n",
    "       data = load_data()\n",
    "       assert 'device_id' in data.columns\n",
    "       assert data['vdd'].dtype == float\n",
    "       assert len(data.columns) == 6  # Expected column count\n",
    "   ```\n",
    "   \n",
    "   **Data Quality:**\n",
    "   - Valid ranges (Vdd: 0.9-1.5V, Temp: -40 to 125Â°C)\n",
    "   - Null handling (critical columns have no nulls)\n",
    "   - Uniqueness constraints (device_id is unique)\n",
    "   - Referential integrity (foreign keys exist)\n",
    "   \n",
    "   ```python\n",
    "   def test_parametric_ranges():\n",
    "       data = load_data()\n",
    "       assert data['vdd'].between(0.9, 1.5).all()\n",
    "       assert data['temp'].between(-40, 125).all()\n",
    "       assert data['device_id'].is_unique\n",
    "   ```\n",
    "   \n",
    "   **Statistical Properties:**\n",
    "   - Distributions match expected (mean, std within bounds)\n",
    "   - No extreme outliers (>5 sigma)\n",
    "   - Correlations as expected (Vdd vs Idd positive)\n",
    "   \n",
    "   ```python\n",
    "   def test_vdd_distribution():\n",
    "       data = load_data()\n",
    "       mean = data['vdd'].mean()\n",
    "       std = data['vdd'].std()\n",
    "       assert 1.15 < mean < 1.25, \"Vdd mean out of expected range\"\n",
    "       assert std < 0.05, \"Vdd std too high\"\n",
    "   ```\n",
    "   \n",
    "   **Data Freshness:**\n",
    "   - Data timestamp within acceptable window\n",
    "   - No stale data (>24 hours old)\n",
    "   - Complete data (all expected batches present)\n",
    "   \n",
    "   Tools:\n",
    "   - pytest: Unit testing framework\n",
    "   - Great Expectations: Data validation library\n",
    "   - Pandera: DataFrame schema validation\n",
    "   - Hypothesis: Property-based testing\n",
    "\n",
    "\n",
    "3. UNIT TESTING FEATURE ENGINEERING\n",
    "   ---------------------------------\n",
    "   \n",
    "   Critical Tests:\n",
    "   \n",
    "   **Mathematical Correctness:**\n",
    "   - Formulas implemented correctly (power = vdd * idd)\n",
    "   - Units consistent (voltage in V, current in mA)\n",
    "   - Precision adequate (floating point errors acceptable)\n",
    "   \n",
    "   ```python\n",
    "   def test_power_calculation():\n",
    "       vdd = 1.2  # V\n",
    "       idd = 250.0  # mA\n",
    "       power = compute_power(vdd, idd)\n",
    "       assert np.isclose(power, 300.0, rtol=1e-6)  # 300 mW\n",
    "   ```\n",
    "   \n",
    "   **Edge Cases:**\n",
    "   - Zero values (power when current=0)\n",
    "   - Negative values (handle or reject)\n",
    "   - Infinities and NaNs (division by zero)\n",
    "   - Empty arrays (min size checks)\n",
    "   \n",
    "   ```python\n",
    "   def test_power_efficiency_zero_power():\n",
    "       freq = 2.5\n",
    "       power = 0\n",
    "       efficiency = compute_power_efficiency(freq, power)\n",
    "       assert np.isnan(efficiency), \"Should return NaN for zero power\"\n",
    "   ```\n",
    "   \n",
    "   **Determinism:**\n",
    "   - Same input â†’ same output (reproducibility)\n",
    "   - No random seeds in feature engineering\n",
    "   - No current_time() calls (breaks reproducibility)\n",
    "   \n",
    "   ```python\n",
    "   def test_deterministic_features():\n",
    "       data = load_test_data()\n",
    "       features1 = compute_features(data)\n",
    "       features2 = compute_features(data)\n",
    "       assert features1.equals(features2), \"Features not deterministic\"\n",
    "   ```\n",
    "   \n",
    "   **Null Handling:**\n",
    "   - Consistent strategy (impute with mean, median, or forward fill)\n",
    "   - No silent NaN propagation\n",
    "   - Document null handling policy\n",
    "   \n",
    "   ```python\n",
    "   def test_null_handling():\n",
    "       data = pd.DataFrame({'vdd': [1.2, np.nan, 1.21]})\n",
    "       features = compute_features(data)\n",
    "       assert features['vdd'].isnull().sum() == 0, \"Nulls not handled\"\n",
    "   ```\n",
    "   \n",
    "   Best Practices:\n",
    "   - Test happy path AND edge cases\n",
    "   - Use parametrized tests for multiple inputs\n",
    "   - Test both valid and invalid inputs\n",
    "   - Verify error messages are clear\n",
    "\n",
    "\n",
    "4. PROPERTY-BASED TESTING WITH HYPOTHESIS\n",
    "   ----------------------------------------\n",
    "   \n",
    "   What is Property-Based Testing?\n",
    "   Instead of writing specific test cases, define properties that should\n",
    "   ALWAYS hold, then let Hypothesis generate 100+ random test cases.\n",
    "   \n",
    "   Example:\n",
    "   ```python\n",
    "   from hypothesis import given, strategies as st\n",
    "   \n",
    "   @given(\n",
    "       vdd=st.floats(min_value=0.1, max_value=2.0),\n",
    "       idd=st.floats(min_value=0, max_value=1000)\n",
    "   )\n",
    "   def test_power_always_non_negative(vdd, idd):\n",
    "       power = compute_power(vdd, idd)\n",
    "       assert power >= 0  # Property: Power is always non-negative\n",
    "   ```\n",
    "   \n",
    "   Benefits:\n",
    "   - Finds edge cases you didn't think of\n",
    "   - Tests 100+ random inputs automatically\n",
    "   - Minimal code (define property, not test cases)\n",
    "   - Reproduces failures (shrinks to minimal failing case)\n",
    "   \n",
    "   Common Properties to Test:\n",
    "   \n",
    "   **Mathematical Invariants:**\n",
    "   - Power â‰¥ 0 (always non-negative)\n",
    "   - Probabilities sum to 1\n",
    "   - Z-scores have mean â‰ˆ 0, std â‰ˆ 1\n",
    "   \n",
    "   **Relationships:**\n",
    "   - Increasing voltage â†’ increasing power (current constant)\n",
    "   - Doubling power â†’ halving efficiency (freq constant)\n",
    "   - Symmetry: distance(A, B) = distance(B, A)\n",
    "   \n",
    "   **Boundary Conditions:**\n",
    "   - Empty input â†’ empty output (or error)\n",
    "   - Single element â†’ valid result\n",
    "   - Large input (10K elements) â†’ no crash\n",
    "   \n",
    "   **Inverse Functions:**\n",
    "   - encode(decode(x)) = x\n",
    "   - normalize â†’ denormalize returns original\n",
    "   \n",
    "   When to Use:\n",
    "   - Mathematical functions (easy to define properties)\n",
    "   - Data transformations (invariants should hold)\n",
    "   - Parsers/serializers (round-trip property)\n",
    "   - NOT for complex ML models (hard to define properties)\n",
    "\n",
    "\n",
    "5. MODEL TESTING\n",
    "   --------------\n",
    "   \n",
    "   Training Tests:\n",
    "   \n",
    "   **Smoke Test:**\n",
    "   - Model trains without errors\n",
    "   - Training completes in reasonable time (<5 min for small dataset)\n",
    "   - Model converges (loss decreases)\n",
    "   \n",
    "   ```python\n",
    "   def test_model_trains():\n",
    "       X, y = load_train_data()\n",
    "       model = RandomForestClassifier()\n",
    "       model.fit(X, y)  # Should not raise exception\n",
    "   ```\n",
    "   \n",
    "   **Learning Test:**\n",
    "   - Model performs better than random guessing\n",
    "   - Model performs better than simple baseline (mean predictor)\n",
    "   - Training accuracy > test accuracy (some overfitting expected)\n",
    "   \n",
    "   ```python\n",
    "   def test_model_beats_baseline():\n",
    "       X_train, X_test, y_train, y_test = split_data()\n",
    "       model.fit(X_train, y_train)\n",
    "       accuracy = model.score(X_test, y_test)\n",
    "       baseline = 0.5  # Random guessing for binary classification\n",
    "       assert accuracy > baseline + 0.1\n",
    "   ```\n",
    "   \n",
    "   Prediction Tests:\n",
    "   \n",
    "   **Output Shape:**\n",
    "   - Correct number of predictions (matches input size)\n",
    "   - Correct dimensionality (1D array for binary, 2D for multi-class)\n",
    "   \n",
    "   **Output Validity:**\n",
    "   - Predictions are valid labels (0, 1 for binary)\n",
    "   - Probabilities in [0, 1] range\n",
    "   - Probabilities sum to 1 across classes\n",
    "   \n",
    "   ```python\n",
    "   def test_prediction_probabilities_valid():\n",
    "       model.fit(X_train, y_train)\n",
    "       probs = model.predict_proba(X_test)\n",
    "       assert (probs >= 0).all() and (probs <= 1).all()\n",
    "       assert np.allclose(probs.sum(axis=1), 1.0)\n",
    "   ```\n",
    "   \n",
    "   **Prediction Diversity:**\n",
    "   - Model doesn't predict all zeros or all ones\n",
    "   - Predicted classes match actual class distribution (roughly)\n",
    "   \n",
    "   ```python\n",
    "   def test_model_predicts_both_classes():\n",
    "       predictions = model.predict(X_test)\n",
    "       unique_preds = set(predictions)\n",
    "       assert len(unique_preds) > 1, \"Model predicts only one class\"\n",
    "   ```\n",
    "   \n",
    "   **Reproducibility:**\n",
    "   - Same random seed â†’ same results\n",
    "   - Model serialization â†’ deserialization preserves behavior\n",
    "   \n",
    "   ```python\n",
    "   def test_model_reproducible():\n",
    "       model1 = RandomForestClassifier(random_state=42)\n",
    "       model1.fit(X_train, y_train)\n",
    "       pred1 = model1.predict(X_test)\n",
    "       \n",
    "       model2 = RandomForestClassifier(random_state=42)\n",
    "       model2.fit(X_train, y_train)\n",
    "       pred2 = model2.predict(X_test)\n",
    "       \n",
    "       assert np.array_equal(pred1, pred2)\n",
    "   ```\n",
    "\n",
    "\n",
    "6. INTEGRATION TESTING\n",
    "   --------------------\n",
    "   \n",
    "   What to Test:\n",
    "   \n",
    "   **End-to-End Pipeline:**\n",
    "   - Data â†’ Features â†’ Model â†’ Predictions â†’ Output\n",
    "   - Verify all components work together\n",
    "   - Check data flows correctly through pipeline\n",
    "   \n",
    "   ```python\n",
    "   def test_full_pipeline():\n",
    "       raw_data = load_raw_stdf_data()\n",
    "       features = feature_engineering_pipeline(raw_data)\n",
    "       predictions = model_pipeline.predict(features)\n",
    "       api_response = format_response(predictions)\n",
    "       \n",
    "       assert api_response['status'] == 'success'\n",
    "       assert len(api_response['predictions']) == len(raw_data)\n",
    "   ```\n",
    "   \n",
    "   **Performance/Latency:**\n",
    "   - Single prediction latency <100ms\n",
    "   - Batch prediction throughput >1000 pred/sec\n",
    "   - Memory usage within limits (<2GB)\n",
    "   \n",
    "   ```python\n",
    "   def test_prediction_latency():\n",
    "       import time\n",
    "       data = generate_test_data(n=10000)\n",
    "       \n",
    "       start = time.time()\n",
    "       predictions = pipeline.predict(data)\n",
    "       elapsed_ms = (time.time() - start) * 1000\n",
    "       \n",
    "       throughput = 10000 / (elapsed_ms / 1000)\n",
    "       assert throughput > 1000, f\"Throughput too low: {throughput:.0f} pred/sec\"\n",
    "   ```\n",
    "   \n",
    "   **Error Handling:**\n",
    "   - Graceful degradation on missing features\n",
    "   - Informative error messages\n",
    "   - Fallback to defaults when appropriate\n",
    "   \n",
    "   ```python\n",
    "   def test_pipeline_handles_missing_features():\n",
    "       data_with_missing = pd.DataFrame({'vdd': [1.2]})  # Missing idd, freq, temp\n",
    "       \n",
    "       try:\n",
    "           pipeline.predict(data_with_missing)\n",
    "           assert False, \"Should raise error on missing features\"\n",
    "       except ValueError as e:\n",
    "           assert 'missing columns' in str(e).lower()\n",
    "   ```\n",
    "   \n",
    "   **Data Flow:**\n",
    "   - Features match model's expected input\n",
    "   - No feature leakage from training to test\n",
    "   - Point-in-time correctness maintained\n",
    "   \n",
    "   **Monitoring Integration:**\n",
    "   - Metrics logged correctly\n",
    "   - Alerts trigger on anomalies\n",
    "   - Dashboards update in real-time\n",
    "\n",
    "\n",
    "7. MODEL VALIDATION STRATEGIES\n",
    "   ----------------------------\n",
    "   \n",
    "   Offline Validation:\n",
    "   \n",
    "   **Holdout Set:**\n",
    "   - Reserve 20% data for final validation\n",
    "   - Never used for training or hyperparameter tuning\n",
    "   - Simulates production performance\n",
    "   \n",
    "   **Cross-Validation:**\n",
    "   - K-fold (k=5 or 10)\n",
    "   - Time-series split (for temporal data)\n",
    "   - Stratified (preserve class distribution)\n",
    "   \n",
    "   **Temporal Validation:**\n",
    "   - Train on past data, test on future data\n",
    "   - Critical for time-series, churn prediction\n",
    "   - Example: Train on 2023, validate on 2024\n",
    "   \n",
    "   Online Validation (Production):\n",
    "   \n",
    "   **A/B Testing:**\n",
    "   - New model (B) vs production model (A)\n",
    "   - Split traffic 50/50 or 90/10 (conservative)\n",
    "   - Measure business metrics (revenue, engagement)\n",
    "   - Statistical significance test (t-test, chi-square)\n",
    "   \n",
    "   **Shadow Mode:**\n",
    "   - New model runs alongside production\n",
    "   - Predictions logged but not used\n",
    "   - Compare to production model\n",
    "   - Safe way to validate before go-live\n",
    "   \n",
    "   **Canary Deployment:**\n",
    "   - Deploy to 5% of traffic first\n",
    "   - Monitor closely for errors, degradation\n",
    "   - Gradual rollout: 5% â†’ 20% â†’ 50% â†’ 100%\n",
    "   - Rollback if issues detected\n",
    "   \n",
    "   Validation Metrics:\n",
    "   \n",
    "   **Classification:**\n",
    "   - Accuracy, Precision, Recall, F1\n",
    "   - AUC-ROC, AUC-PR\n",
    "   - Confusion matrix\n",
    "   - Calibration (predicted probs match actual rates)\n",
    "   \n",
    "   **Regression:**\n",
    "   - MAE, RMSE, MAPE\n",
    "   - RÂ² score\n",
    "   - Residual plots (check for patterns)\n",
    "   \n",
    "   **Business Metrics:**\n",
    "   - Revenue impact ($)\n",
    "   - Customer satisfaction (NPS)\n",
    "   - Cost reduction (%)\n",
    "   - Time savings (hours)\n",
    "\n",
    "\n",
    "8. TEST COVERAGE AND CI/CD\n",
    "   ------------------------\n",
    "   \n",
    "   Code Coverage:\n",
    "   - Target: >80% line coverage (pytest-cov)\n",
    "   - Critical paths: 100% coverage\n",
    "   - Generate reports: coverage html, coverage xml\n",
    "   - Track over time (coverage should increase)\n",
    "   \n",
    "   ```bash\n",
    "   pytest --cov=src --cov-report=html --cov-report=term\n",
    "   ```\n",
    "   \n",
    "   Continuous Integration:\n",
    "   - Run all tests on every commit\n",
    "   - Fast tests (<5 min) in pre-commit\n",
    "   - Full test suite in CI pipeline\n",
    "   - Block merge if tests fail\n",
    "   \n",
    "   CI Pipeline Example:\n",
    "   ```yaml\n",
    "   # .github/workflows/test.yml\n",
    "   name: ML Tests\n",
    "   on: [push, pull_request]\n",
    "   \n",
    "   jobs:\n",
    "     test:\n",
    "       runs-on: ubuntu-latest\n",
    "       steps:\n",
    "         - uses: actions/checkout@v2\n",
    "         - name: Set up Python\n",
    "           uses: actions/setup-python@v2\n",
    "           with:\n",
    "             python-version: 3.9\n",
    "         - name: Install dependencies\n",
    "           run: pip install -r requirements.txt\n",
    "         - name: Run unit tests\n",
    "           run: pytest tests/unit --cov=src\n",
    "         - name: Run integration tests\n",
    "           run: pytest tests/integration\n",
    "         - name: Upload coverage\n",
    "           uses: codecov/codecov-action@v2\n",
    "   ```\n",
    "   \n",
    "   Test Organization:\n",
    "   ```\n",
    "   tests/\n",
    "   â”œâ”€â”€ unit/                  # Fast, isolated tests\n",
    "   â”‚   â”œâ”€â”€ test_data.py       # Data validation\n",
    "   â”‚   â”œâ”€â”€ test_features.py   # Feature engineering\n",
    "   â”‚   â””â”€â”€ test_models.py     # Model components\n",
    "   â”œâ”€â”€ integration/           # Slower, end-to-end tests\n",
    "   â”‚   â”œâ”€â”€ test_pipeline.py   # Full ML pipeline\n",
    "   â”‚   â””â”€â”€ test_api.py        # API integration\n",
    "   â”œâ”€â”€ performance/           # Load and stress tests\n",
    "   â”‚   â””â”€â”€ test_latency.py\n",
    "   â””â”€â”€ fixtures/              # Shared test data\n",
    "       â””â”€â”€ sample_data.py\n",
    "   ```\n",
    "\n",
    "\n",
    "9. COMMON PITFALLS AND SOLUTIONS\n",
    "   ------------------------------\n",
    "   \n",
    "   Pitfall 1: Data Leakage in Tests\n",
    "   Problem: Test data contains information from training data\n",
    "   Example: Using future data in features (violates point-in-time)\n",
    "   Solution: Strict train/test split, temporal validation\n",
    "   \n",
    "   Pitfall 2: Overfitting to Test Set\n",
    "   Problem: Tuning hyperparameters based on test set performance\n",
    "   Example: Running 100 experiments, reporting best test accuracy\n",
    "   Solution: Use validation set for tuning, holdout for final eval\n",
    "   \n",
    "   Pitfall 3: Ignoring Training-Serving Skew\n",
    "   Problem: Features computed differently in training vs serving\n",
    "   Example: Training uses SQL, serving uses Python (different results)\n",
    "   Solution: Feature store ensures consistency\n",
    "   \n",
    "   Pitfall 4: Brittle Tests (Flaky)\n",
    "   Problem: Tests pass/fail randomly\n",
    "   Example: Tests depend on random initialization without seed\n",
    "   Solution: Set random seeds, use deterministic algorithms\n",
    "   \n",
    "   Pitfall 5: Testing Implementation Not Interface\n",
    "   Problem: Tests tied to internal implementation details\n",
    "   Example: Testing internal variables instead of outputs\n",
    "   Solution: Test public API and outputs, not internals\n",
    "   \n",
    "   Pitfall 6: Insufficient Edge Case Coverage\n",
    "   Problem: Tests only cover happy path\n",
    "   Example: Not testing nulls, zeros, empty inputs\n",
    "   Solution: Property-based testing, explicit edge case tests\n",
    "   \n",
    "   Pitfall 7: No Performance Testing\n",
    "   Problem: Model works but too slow for production\n",
    "   Example: Prediction latency 5 seconds (SLA is 100ms)\n",
    "   Solution: Integration tests include latency benchmarks\n",
    "   \n",
    "   Pitfall 8: Ignoring Model Degradation\n",
    "   Problem: Model accuracy slowly decreases over time\n",
    "   Example: Data drift causes 90% â†’ 80% accuracy over 6 months\n",
    "   Solution: Continuous monitoring, automated retraining\n",
    "\n",
    "\n",
    "10. BEST PRACTICES\n",
    "    ---------------\n",
    "    \n",
    "    Test-Driven Development (TDD):\n",
    "    1. Write test first (it fails)\n",
    "    2. Implement minimal code to pass test\n",
    "    3. Refactor while keeping tests passing\n",
    "    4. Repeat\n",
    "    \n",
    "    Benefits:\n",
    "    - Forces thinking about requirements\n",
    "    - Ensures testable code\n",
    "    - High test coverage by design\n",
    "    - Confidence in refactoring\n",
    "    \n",
    "    Testing Checklist:\n",
    "    â–¡ Data validation tests (schema, quality)\n",
    "    â–¡ Feature engineering tests (correctness, edge cases)\n",
    "    â–¡ Model tests (training, prediction, performance)\n",
    "    â–¡ Integration tests (end-to-end pipeline)\n",
    "    â–¡ Performance tests (latency, throughput)\n",
    "    â–¡ Property-based tests (invariants)\n",
    "    â–¡ Error handling tests (graceful degradation)\n",
    "    â–¡ Reproducibility tests (same seed â†’ same results)\n",
    "    â–¡ CI/CD integration (auto-run on commit)\n",
    "    â–¡ Coverage tracking (>80% target)\n",
    "    \n",
    "    Test Quality:\n",
    "    - Fast: Unit tests <10ms, integration tests <1s\n",
    "    - Isolated: No dependencies on external services\n",
    "    - Repeatable: Deterministic, same results every run\n",
    "    - Self-checking: Automatic assertions (no manual inspection)\n",
    "    - Timely: Written alongside code, not after\n",
    "    \n",
    "    Documentation:\n",
    "    - Docstrings explain what test validates\n",
    "    - Comments explain WHY, not what\n",
    "    - README with how to run tests\n",
    "    - Examples of good/bad inputs\n",
    "\n",
    "\n",
    "11. PRODUCTION CHECKLIST\n",
    "    ---------------------\n",
    "    \n",
    "    Before Deploying ML Model:\n",
    "    \n",
    "    Data Validation:\n",
    "    â–¡ Schema tests pass\n",
    "    â–¡ Data quality tests pass\n",
    "    â–¡ No data leakage detected\n",
    "    â–¡ Point-in-time correctness verified\n",
    "    \n",
    "    Model Validation:\n",
    "    â–¡ Accuracy meets threshold (>85%)\n",
    "    â–¡ No catastrophic errors (predictions <10x reality)\n",
    "    â–¡ Fairness tests pass (no demographic bias)\n",
    "    â–¡ Robustness verified (adversarial examples)\n",
    "    â–¡ Calibration acceptable (predicted probs match actual)\n",
    "    \n",
    "    Integration:\n",
    "    â–¡ End-to-end tests pass\n",
    "    â–¡ Latency within SLA (<100ms p99)\n",
    "    â–¡ Throughput adequate (>1000 pred/sec)\n",
    "    â–¡ Error handling graceful\n",
    "    â–¡ Monitoring integrated\n",
    "    \n",
    "    Deployment:\n",
    "    â–¡ A/B test configured\n",
    "    â–¡ Shadow mode validated\n",
    "    â–¡ Rollback plan documented\n",
    "    â–¡ Alerts configured\n",
    "    â–¡ Runbooks updated\n",
    "    \n",
    "    Post-Deployment:\n",
    "    â–¡ Monitor accuracy daily\n",
    "    â–¡ Track data drift\n",
    "    â–¡ Alert on performance degradation\n",
    "    â–¡ Automated retraining pipeline ready\n",
    "\n",
    "\n",
    "12. NEXT STEPS\n",
    "    -----------\n",
    "    \n",
    "    After ML Testing, continue with:\n",
    "    \n",
    "    126_Continuous_Training_Pipelines.ipynb:\n",
    "    - Automated retraining when drift detected\n",
    "    - Testing in retraining pipeline\n",
    "    - Validation gates before deployment\n",
    "    \n",
    "    127_Model_Serving_Patterns.ipynb:\n",
    "    - Testing serving infrastructure\n",
    "    - Load testing API endpoints\n",
    "    - Integration with monitoring\n",
    "    \n",
    "    128_ML_Monitoring.ipynb:\n",
    "    - Testing monitoring systems\n",
    "    - Validating alert logic\n",
    "    - Dashboard functionality tests\n",
    "\n",
    "\n",
    "================================================================================\n",
    "FINAL SUMMARY\n",
    "================================================================================\n",
    "\n",
    "ML testing goes beyond traditional software testing to address unique\n",
    "challenges: non-determinism, data dependencies, gradual degradation, and\n",
    "the interplay between code, data, and models.\n",
    "\n",
    "Key Principles:\n",
    "1. Test data, features, and models (not just code)\n",
    "2. Use statistical assertions (distributions, not exact values)\n",
    "3. Property-based testing finds edge cases\n",
    "4. Integration tests validate entire pipeline\n",
    "5. Continuous validation in production (A/B, shadow mode)\n",
    "6. Test coverage >80%, critical paths 100%\n",
    "7. CI/CD integration prevents regressions\n",
    "\n",
    "Critical Tests:\n",
    "- Data schema and quality\n",
    "- Feature engineering correctness\n",
    "- Model performance baselines\n",
    "- End-to-end pipeline integration\n",
    "- Latency and throughput SLAs\n",
    "- Reproducibility and determinism\n",
    "\n",
    "Testing Pyramid: 70% unit, 20% integration, 10% system tests.\n",
    "\n",
    "Remember: In ML, no test failure is often WORSE than a loud failure.\n",
    "Silent accuracy degradation can cost millions before detection.\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739ce5ce",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways - ML Testing Mastery\n",
    "\n",
    "Comprehensive reference guide for production ML testing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b51a2",
   "metadata": {},
   "source": [
    "## 6. Integration Testing - End-to-End ML Pipeline\n",
    "\n",
    "**What's happening:** Testing complete ML workflow from data â†’ features â†’ model â†’ predictions.\n",
    "\n",
    "**Key points:**\n",
    "- **Pipeline tests**: Verify all components work together\n",
    "- **Performance SLA**: Check latency, throughput under load\n",
    "- **Data flow**: Input data â†’ transformed correctly â†’ valid predictions\n",
    "- **Error handling**: System degrades gracefully on bad inputs\n",
    "\n",
    "**Production scenario:** Before deploying, run full integration test simulating production traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea02ce2",
   "metadata": {},
   "source": [
    "## 5. Model Testing - Training and Prediction Validation\n",
    "\n",
    "**What's happening:** Testing model behavior, training stability, and prediction quality.\n",
    "\n",
    "**Key points:**\n",
    "- **Training tests**: Model trains without errors, converges, learns patterns\n",
    "- **Prediction tests**: Output shapes, probability ranges, label consistency\n",
    "- **Performance tests**: Accuracy baselines, no random guessing\n",
    "- **Reproducibility**: Same seed â†’ same results\n",
    "\n",
    "**Why critical:** Models can train successfully but still be fundamentally broken (predicting all zeros, not learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff31cd4",
   "metadata": {},
   "source": [
    "## 4. Property-Based Testing - Invariants and Edge Cases\n",
    "\n",
    "**What's happening:** Using Hypothesis library to generate random test cases and verify properties.\n",
    "\n",
    "**Key points:**\n",
    "- **Property testing**: Define invariants that should always hold\n",
    "- **Automatic test generation**: Hypothesis generates 100+ test cases\n",
    "- **Edge case discovery**: Find bugs you didn't think to test\n",
    "- **Statistical properties**: Verify distributions, ranges, relationships\n",
    "\n",
    "**Production value:** Catch obscure bugs that manual test cases miss (wafer edge cases, numerical precision)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945ff968",
   "metadata": {},
   "source": [
    "## 3. Unit Testing Feature Engineering - Transformation Logic\n",
    "\n",
    "**What's happening:** Testing feature engineering functions for correctness and edge cases.\n",
    "\n",
    "**Key points:**\n",
    "- **Deterministic outputs**: Same input â†’ same output (reproducibility)\n",
    "- **Edge cases**: Handle zeros, negatives, infinities, nulls\n",
    "- **Mathematical correctness**: Verify formulas (power = vdd * idd)\n",
    "- **Performance**: Test computation time for large datasets\n",
    "\n",
    "**Why this matters:** Incorrect feature engineering is the #1 cause of training-serving skew."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf64da9",
   "metadata": {},
   "source": [
    "## 2. Unit Testing Data Pipelines - Schema Validation\n",
    "\n",
    "**What's happening:** Testing data ingestion and schema validation for STDF test data.\n",
    "\n",
    "**Key points:**\n",
    "- **Schema tests**: Data types, column names, required fields\n",
    "- **Boundary tests**: Valid ranges for parametric values\n",
    "- **Null handling**: Missing data detection and policies\n",
    "- **Fail fast**: Catch data issues early (before model training)\n",
    "\n",
    "**Post-silicon context:** STDF files must conform to IEEE 1505 standard with strict parametric ranges."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
