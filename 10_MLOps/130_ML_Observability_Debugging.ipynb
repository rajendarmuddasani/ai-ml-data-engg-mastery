{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf2c7dd",
   "metadata": {},
   "source": [
    "# 130: ML Observability & Debugging\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** distributed tracing for ML pipelines (track data ‚Üí feature ‚Üí model ‚Üí prediction flow)\n",
    "- **Implement** model explainability with SHAP and LIME (debug individual predictions)\n",
    "- **Build** performance profiling systems (identify latency bottlenecks in inference pipelines)\n",
    "- **Apply** error analysis techniques to post-silicon validation (root cause detection for test failures)\n",
    "- **Master** debugging strategies for production ML systems (systematic troubleshooting)\n",
    "- **Deploy** comprehensive observability dashboards (unified view of ML system health)\n",
    "\n",
    "## üìö What is ML Observability?\n",
    "\n",
    "**ML Observability** is the practice of monitoring, understanding, and debugging machine learning systems in production through comprehensive instrumentation and analysis.\n",
    "\n",
    "Unlike traditional software observability (logs, metrics, traces), ML observability adds:\n",
    "- **Model performance tracking:** Accuracy, latency, drift over time\n",
    "- **Prediction explainability:** Why did the model make this specific prediction?\n",
    "- **Feature attribution:** Which features contributed most to the prediction?\n",
    "- **Error analysis:** What patterns exist in model failures?\n",
    "- **Data quality monitoring:** Is input data within expected distributions?\n",
    "\n",
    "**Traditional Observability:**\n",
    "```\n",
    "Request ‚Üí Server ‚Üí Database ‚Üí Response\n",
    "   ‚Üì         ‚Üì         ‚Üì          ‚Üì\n",
    " Trace    Logs    Metrics    Status\n",
    "```\n",
    "\n",
    "**ML Observability:**\n",
    "```\n",
    "Request ‚Üí Feature Engineering ‚Üí Model Inference ‚Üí Prediction ‚Üí Response\n",
    "   ‚Üì              ‚Üì                    ‚Üì              ‚Üì           ‚Üì\n",
    " Trace      Feature Values      SHAP Values    Confidence    Status\n",
    "   ‚Üì              ‚Üì                    ‚Üì              ‚Üì           ‚Üì\n",
    "Input Data   Missing Features   Latency Breakdown  Accuracy   Errors\n",
    "```\n",
    "\n",
    "**Why ML Observability?**\n",
    "- ‚úÖ **Debug faster:** Identify root cause of prediction errors in minutes (not days)\n",
    "- ‚úÖ **Prevent incidents:** Detect anomalies before they impact business (early warning)\n",
    "- ‚úÖ **Explain decisions:** Provide transparency for stakeholders (regulatory compliance)\n",
    "- ‚úÖ **Optimize performance:** Identify bottlenecks, reduce latency by 50-80%\n",
    "- ‚úÖ **Improve models:** Learn from failures, prioritize retraining efforts\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Use Case 1: Wafer Test Failure Root Cause Analysis**\n",
    "- **Input:** 1000 failed devices (binning model predicted fail, actual fail)\n",
    "- **Output:** Top 3 root causes (Vdd out of range ‚Üí 45%, spatial correlation ‚Üí 30%, temperature ‚Üí 25%)\n",
    "- **Value:** Debug time reduced from 8 hours (manual inspection) ‚Üí 15 minutes (automated analysis)\n",
    "\n",
    "**Use Case 2: Test Time Optimization Debugging**\n",
    "- **Input:** Test time prediction model (predicting 45ms, actual 120ms for specific devices)\n",
    "- **Output:** Bottleneck identified (feature: device_complexity underestimated by 60%)\n",
    "- **Value:** Fix feature engineering bug ‚Üí improve prediction RMSE from 30ms ‚Üí 8ms\n",
    "\n",
    "**Use Case 3: Binning Model Explainability**\n",
    "- **Input:** Device binned as \"Fail\" (customer disputes, demands explanation)\n",
    "- **Output:** SHAP waterfall plot (Vdd contribution: -0.3, Idd: -0.2, frequency: -0.1 ‚Üí total: -0.6 fail score)\n",
    "- **Value:** Regulatory compliance (IEEE 1505 audit trail), customer transparency\n",
    "\n",
    "**Use Case 4: Spatial Correlation Model Performance Profiling**\n",
    "- **Input:** Wafer map inference taking 500ms (SLA: <100ms)\n",
    "- **Output:** Latency breakdown (neighbor search: 400ms, feature compute: 80ms, model: 20ms)\n",
    "- **Value:** Optimize neighbor search (spatial index) ‚Üí reduce latency 500ms ‚Üí 60ms\n",
    "\n",
    "## üîÑ ML Observability Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Production ML System] --> B[Distributed Tracing]\n",
    "    A --> C[Model Explainability]\n",
    "    A --> D[Performance Profiling]\n",
    "    A --> E[Error Analysis]\n",
    "    \n",
    "    B --> F[Feature ‚Üí Model ‚Üí Prediction Flow]\n",
    "    C --> G[SHAP/LIME Analysis]\n",
    "    D --> H[Latency Breakdown]\n",
    "    E --> I[Root Cause Detection]\n",
    "    \n",
    "    F --> J[Observability Dashboard]\n",
    "    G --> J\n",
    "    H --> J\n",
    "    I --> J\n",
    "    \n",
    "    J --> K[Alerts & Insights]\n",
    "    K --> L[Debug & Fix]\n",
    "    L --> A\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style J fill:#ffe1e1\n",
    "    style L fill:#e1ffe1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **Notebook 129:** Advanced MLOps - Feature Stores & Real-Time Monitoring (drift detection, data quality)\n",
    "- **Notebook 128:** Shadow Mode Deployment (A/B testing, canary deployment)\n",
    "- **Notebook 127:** ML Governance & Compliance (audit trails, lineage tracking)\n",
    "\n",
    "**Next Steps:**\n",
    "- **Notebook 131:** Containerization for ML (Docker, Kubernetes, model serving)\n",
    "- **Notebook 132:** Service Mesh for ML (Istio, traffic management, observability)\n",
    "- **Notebook 133:** CI/CD for ML (automated testing, deployment pipelines)\n",
    "\n",
    "---\n",
    "\n",
    "Let's build production-grade ML observability and debugging systems! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3569aabc",
   "metadata": {},
   "source": [
    "## 2. üîç Distributed Tracing for ML Pipelines\n",
    "\n",
    "### üìù What's Happening in This Section?\n",
    "\n",
    "**Purpose:** Implement distributed tracing to track the complete flow of ML predictions from request ‚Üí feature engineering ‚Üí model inference ‚Üí response, capturing timing, metadata, and errors at each stage.\n",
    "\n",
    "**Key Points:**\n",
    "- **Trace ID propagation**: Single ID follows request through entire pipeline (correlate all operations)\n",
    "- **Span hierarchy**: Parent-child relationships (request ‚Üí feature_fetch ‚Üí model_predict ‚Üí response)\n",
    "- **Timing instrumentation**: Capture start/end timestamps for each operation (identify bottlenecks)\n",
    "- **Context enrichment**: Attach metadata (feature values, model version, input size, cache hits)\n",
    "- **Error tracking**: Capture exceptions with full context (which stage failed, why)\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Debug production issues:** \"Why did this specific prediction take 2 seconds?\" (answer in trace)\n",
    "- **Optimize latency:** Identify slowest operation (feature fetch: 1.5s ‚Üí optimize caching)\n",
    "- **Root cause analysis:** Trace errors back to source (missing feature ‚Üí upstream pipeline failure)\n",
    "\n",
    "**Post-Silicon Application:** Trace wafer binning prediction: STDF ingestion ‚Üí feature engineering (spatial correlation) ‚Üí model inference ‚Üí binning decision (track end-to-end flow, identify delays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Span:\n",
    "    \"\"\"Individual operation in distributed trace\"\"\"\n",
    "    span_id: str\n",
    "    parent_span_id: Optional[str]\n",
    "    operation_name: str\n",
    "    start_time: float\n",
    "    end_time: Optional[float] = None\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    error: Optional[str] = None\n",
    "    \n",
    "    @property\n",
    "    def duration_ms(self) -> float:\n",
    "        \"\"\"Calculate span duration in milliseconds\"\"\"\n",
    "        if self.end_time:\n",
    "            return (self.end_time - self.start_time) * 1000\n",
    "        return 0.0\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert span to dictionary for logging\"\"\"\n",
    "        return {\n",
    "            'span_id': self.span_id,\n",
    "            'parent_span_id': self.parent_span_id,\n",
    "            'operation': self.operation_name,\n",
    "            'duration_ms': round(self.duration_ms, 2),\n",
    "            'metadata': self.metadata,\n",
    "            'error': self.error\n",
    "        }\n",
    "\n",
    "\n",
    "class MLTracer:\n",
    "    \"\"\"Distributed tracing system for ML pipelines\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.traces = {}  # trace_id ‚Üí list of spans\n",
    "        self.active_spans = {}  # span_id ‚Üí Span\n",
    "    \n",
    "    def start_trace(self, trace_id: str, operation_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Start new trace (root span)\n",
    "        \n",
    "        Args:\n",
    "            trace_id: Unique identifier for this request\n",
    "            operation_name: Name of root operation (e.g., \"predict_request\")\n",
    "        \n",
    "        Returns:\n",
    "            span_id: ID of root span\n",
    "        \"\"\"\n",
    "        span_id = f\"{trace_id}_span_0\"\n",
    "        span = Span(\n",
    "            span_id=span_id,\n",
    "            parent_span_id=None,\n",
    "            operation_name=operation_name,\n",
    "            start_time=time.time()\n",
    "        )\n",
    "        \n",
    "        self.traces[trace_id] = [span]\n",
    "        self.active_spans[span_id] = span\n",
    "        \n",
    "        return span_id\n",
    "    \n",
    "    def start_span(self, trace_id: str, parent_span_id: str, operation_name: str, \n",
    "                   metadata: Optional[Dict] = None) -> str:\n",
    "        \"\"\"\n",
    "        Start child span within trace\n",
    "        \n",
    "        Args:\n",
    "            trace_id: ID of parent trace\n",
    "            parent_span_id: ID of parent span\n",
    "            operation_name: Name of this operation\n",
    "            metadata: Optional metadata to attach\n",
    "        \n",
    "        Returns:\n",
    "            span_id: ID of new span\n",
    "        \"\"\"\n",
    "        span_count = len(self.traces.get(trace_id, []))\n",
    "        span_id = f\"{trace_id}_span_{span_count}\"\n",
    "        \n",
    "        span = Span(\n",
    "            span_id=span_id,\n",
    "            parent_span_id=parent_span_id,\n",
    "            operation_name=operation_name,\n",
    "            start_time=time.time(),\n",
    "            metadata=metadata or {}\n",
    "        )\n",
    "        \n",
    "        if trace_id not in self.traces:\n",
    "            self.traces[trace_id] = []\n",
    "        \n",
    "        self.traces[trace_id].append(span)\n",
    "        self.active_spans[span_id] = span\n",
    "        \n",
    "        return span_id\n",
    "    \n",
    "    def end_span(self, span_id: str, metadata: Optional[Dict] = None, error: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        End span, record duration and optional metadata/error\n",
    "        \n",
    "        Args:\n",
    "            span_id: ID of span to end\n",
    "            metadata: Additional metadata to attach\n",
    "            error: Error message if operation failed\n",
    "        \"\"\"\n",
    "        if span_id in self.active_spans:\n",
    "            span = self.active_spans[span_id]\n",
    "            span.end_time = time.time()\n",
    "            \n",
    "            if metadata:\n",
    "                span.metadata.update(metadata)\n",
    "            \n",
    "            if error:\n",
    "                span.error = error\n",
    "            \n",
    "            del self.active_spans[span_id]\n",
    "    \n",
    "    def get_trace(self, trace_id: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get all spans for a trace\"\"\"\n",
    "        if trace_id in self.traces:\n",
    "            return [span.to_dict() for span in self.traces[trace_id]]\n",
    "        return []\n",
    "    \n",
    "    def get_trace_summary(self, trace_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary statistics for trace\"\"\"\n",
    "        spans = self.traces.get(trace_id, [])\n",
    "        \n",
    "        if not spans:\n",
    "            return {}\n",
    "        \n",
    "        total_duration = max(span.duration_ms for span in spans)\n",
    "        operation_durations = {}\n",
    "        \n",
    "        for span in spans:\n",
    "            op = span.operation_name\n",
    "            if op not in operation_durations:\n",
    "                operation_durations[op] = []\n",
    "            operation_durations[op].append(span.duration_ms)\n",
    "        \n",
    "        # Calculate percentage breakdown\n",
    "        operation_breakdown = {\n",
    "            op: {\n",
    "                'total_ms': sum(durations),\n",
    "                'percentage': (sum(durations) / total_duration * 100) if total_duration > 0 else 0,\n",
    "                'count': len(durations)\n",
    "            }\n",
    "            for op, durations in operation_durations.items()\n",
    "        }\n",
    "        \n",
    "        # Check for errors\n",
    "        errors = [span.error for span in spans if span.error]\n",
    "        \n",
    "        return {\n",
    "            'trace_id': trace_id,\n",
    "            'total_duration_ms': round(total_duration, 2),\n",
    "            'span_count': len(spans),\n",
    "            'operation_breakdown': operation_breakdown,\n",
    "            'errors': errors\n",
    "        }\n",
    "\n",
    "\n",
    "# Example: Trace wafer binning prediction pipeline\n",
    "print(\"=\" * 60)\n",
    "print(\"Distributed Tracing for Wafer Binning Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tracer = MLTracer()\n",
    "\n",
    "# Simulate 5 predictions with tracing\n",
    "for i in range(5):\n",
    "    trace_id = f\"wafer_predict_{i:03d}\"\n",
    "    \n",
    "    # Root span: prediction request\n",
    "    root_span = tracer.start_trace(trace_id, \"predict_wafer_binning\")\n",
    "    \n",
    "    # Span 1: Fetch features from feature store\n",
    "    fetch_span = tracer.start_span(\n",
    "        trace_id, root_span, \"fetch_features\",\n",
    "        metadata={'wafer_id': f'W{i:04d}', 'feature_count': 15}\n",
    "    )\n",
    "    time.sleep(0.01 + np.random.uniform(0, 0.02))  # Simulate variable latency\n",
    "    tracer.end_span(fetch_span, metadata={'cache_hit': i % 2 == 0})\n",
    "    \n",
    "    # Span 2: Compute spatial correlation features\n",
    "    spatial_span = tracer.start_span(\n",
    "        trace_id, root_span, \"compute_spatial_features\",\n",
    "        metadata={'neighbor_count': 24, 'radius_mm': 3}\n",
    "    )\n",
    "    time.sleep(0.005 + np.random.uniform(0, 0.015))\n",
    "    tracer.end_span(spatial_span, metadata={'neighbors_found': 24})\n",
    "    \n",
    "    # Span 3: Model inference\n",
    "    model_span = tracer.start_span(\n",
    "        trace_id, root_span, \"model_inference\",\n",
    "        metadata={'model_version': 'v2.3', 'model_type': 'RandomForest'}\n",
    "    )\n",
    "    time.sleep(0.003 + np.random.uniform(0, 0.007))\n",
    "    tracer.end_span(model_span, metadata={'prediction': 'Pass' if i % 3 != 0 else 'Fail'})\n",
    "    \n",
    "    # End root span\n",
    "    tracer.end_span(root_span, metadata={'response_status': 200})\n",
    "\n",
    "# Analyze traces\n",
    "print(\"\\nüìä Trace Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(5):\n",
    "    trace_id = f\"wafer_predict_{i:03d}\"\n",
    "    summary = tracer.get_trace_summary(trace_id)\n",
    "    \n",
    "    print(f\"\\nTrace ID: {summary['trace_id']}\")\n",
    "    print(f\"  Total Duration: {summary['total_duration_ms']:.2f} ms\")\n",
    "    print(f\"  Operations:\")\n",
    "    \n",
    "    for op, stats in sorted(summary['operation_breakdown'].items(), \n",
    "                           key=lambda x: x[1]['percentage'], reverse=True):\n",
    "        print(f\"    - {op}: {stats['total_ms']:.2f} ms ({stats['percentage']:.1f}%)\")\n",
    "\n",
    "# Identify bottleneck\n",
    "print(\"\\nüéØ Performance Bottleneck Analysis:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "all_summaries = [tracer.get_trace_summary(f\"wafer_predict_{i:03d}\") for i in range(5)]\n",
    "avg_breakdown = {}\n",
    "\n",
    "for summary in all_summaries:\n",
    "    for op, stats in summary['operation_breakdown'].items():\n",
    "        if op not in avg_breakdown:\n",
    "            avg_breakdown[op] = []\n",
    "        avg_breakdown[op].append(stats['percentage'])\n",
    "\n",
    "print(\"\\nAverage Time Breakdown:\")\n",
    "for op, percentages in sorted(avg_breakdown.items(), \n",
    "                             key=lambda x: np.mean(x[1]), reverse=True):\n",
    "    avg_pct = np.mean(percentages)\n",
    "    print(f\"  {op}: {avg_pct:.1f}% of total time\")\n",
    "\n",
    "print(\"\\n‚úÖ Bottleneck identified: Optimize 'fetch_features' operation (caching, batching)\")\n",
    "print(\"‚úÖ Trace IDs enable correlating errors across distributed services\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3472ff6",
   "metadata": {},
   "source": [
    "## 3. üß† Model Explainability and Debugging with SHAP\n",
    "\n",
    "### üìù What's Happening in This Section?\n",
    "\n",
    "**Purpose:** Implement SHAP (SHapley Additive exPlanations) values to explain individual predictions, debug model behavior, and identify which features contributed most to specific decisions.\n",
    "\n",
    "**Key Points:**\n",
    "- **SHAP values**: Unified measure of feature importance based on game theory (Shapley values)\n",
    "- **Additive feature attribution**: Prediction = base_value + Œ£(SHAP_values) (exact decomposition)\n",
    "- **Local explanations**: Why this specific prediction? (waterfall plot shows feature contributions)\n",
    "- **Global explanations**: Which features matter most overall? (summary plot aggregates across dataset)\n",
    "- **Model-agnostic**: Works for any model (tree-based, neural networks, linear models)\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Debug predictions:** \"Why did model predict Fail for device X?\" (Vdd=-0.3, Idd=-0.2 ‚Üí -0.5 total)\n",
    "- **Build trust:** Stakeholders understand model reasoning (not black box)\n",
    "- **Feature engineering:** Identify uninformative features (remove noise, improve performance)\n",
    "- **Regulatory compliance:** Provide audit trail for decisions (IEEE 1505, FDA requirements)\n",
    "\n",
    "**Post-Silicon Application:** Explain wafer binning decisions: Device binned as \"Fail\" - SHAP shows Vdd contributed -0.3 (primary driver), spatial correlation -0.15 (neighboring devices also failed), temperature -0.05 (minor factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da54e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSHAPExplainer:\n",
    "    \"\"\"\n",
    "    Simplified SHAP-like explainer for tree-based models\n",
    "    \n",
    "    Note: This is an educational implementation. For production, use the `shap` library:\n",
    "    import shap\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, X_background: np.ndarray, feature_names: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize explainer\n",
    "        \n",
    "        Args:\n",
    "            model: Trained model with predict_proba method\n",
    "            X_background: Background dataset for computing baseline\n",
    "            feature_names: Names of features\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.X_background = X_background\n",
    "        self.feature_names = feature_names\n",
    "        self.base_value = model.predict_proba(X_background)[:, 1].mean()\n",
    "    \n",
    "    def explain_instance(self, X_instance: np.ndarray, num_samples: int = 100) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute approximate SHAP values for single instance using permutation\n",
    "        \n",
    "        Args:\n",
    "            X_instance: Single data point to explain (1D array)\n",
    "            num_samples: Number of permutations for approximation\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping feature names to SHAP values\n",
    "        \"\"\"\n",
    "        n_features = len(X_instance)\n",
    "        shap_values = np.zeros(n_features)\n",
    "        \n",
    "        # Get prediction for instance\n",
    "        pred = self.model.predict_proba(X_instance.reshape(1, -1))[0, 1]\n",
    "        \n",
    "        # Approximate SHAP values using permutation importance\n",
    "        for i in range(n_features):\n",
    "            # Create modified instances with feature i from background\n",
    "            marginal_contrib = []\n",
    "            \n",
    "            for _ in range(num_samples):\n",
    "                # Random background sample\n",
    "                bg_idx = np.random.randint(0, len(self.X_background))\n",
    "                X_modified = X_instance.copy()\n",
    "                X_modified[i] = self.X_background[bg_idx, i]\n",
    "                \n",
    "                # Prediction difference\n",
    "                pred_modified = self.model.predict_proba(X_modified.reshape(1, -1))[0, 1]\n",
    "                marginal_contrib.append(pred - pred_modified)\n",
    "            \n",
    "            shap_values[i] = np.mean(marginal_contrib)\n",
    "        \n",
    "        # Normalize to sum to (prediction - base_value)\n",
    "        total_shap = shap_values.sum()\n",
    "        target_sum = pred - self.base_value\n",
    "        \n",
    "        if abs(total_shap) > 1e-6:\n",
    "            shap_values = shap_values * (target_sum / total_shap)\n",
    "        \n",
    "        return dict(zip(self.feature_names, shap_values))\n",
    "    \n",
    "    def plot_waterfall(self, shap_values: Dict[str, float], instance_prediction: float, \n",
    "                      instance_data: Dict[str, Any], title: str = \"SHAP Waterfall Plot\"):\n",
    "        \"\"\"\n",
    "        Create waterfall plot showing feature contributions\n",
    "        \n",
    "        Args:\n",
    "            shap_values: Dictionary of feature SHAP values\n",
    "            instance_prediction: Model prediction for this instance\n",
    "            instance_data: Feature values for this instance\n",
    "            title: Plot title\n",
    "        \"\"\"\n",
    "        # Sort features by absolute SHAP value\n",
    "        sorted_features = sorted(shap_values.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        # Take top 10 features\n",
    "        top_features = sorted_features[:10]\n",
    "        \n",
    "        # Create waterfall data\n",
    "        features = ['Base Value'] + [f[0] for f in top_features] + ['Prediction']\n",
    "        values = [self.base_value] + [f[1] for f in top_features] + [instance_prediction]\n",
    "        \n",
    "        # Calculate cumulative sum for plotting\n",
    "        cumulative = [self.base_value]\n",
    "        for _, shap_val in top_features:\n",
    "            cumulative.append(cumulative[-1] + shap_val)\n",
    "        cumulative.append(instance_prediction)\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        colors = ['blue' if v >= 0 else 'red' for v in [0] + [f[1] for f in top_features] + [0]]\n",
    "        \n",
    "        for i in range(len(features) - 1):\n",
    "            if i == 0:\n",
    "                ax.barh(i, cumulative[i], color='gray', alpha=0.3)\n",
    "            else:\n",
    "                start = cumulative[i-1]\n",
    "                width = values[i]\n",
    "                ax.barh(i, width, left=start, color=colors[i], alpha=0.7)\n",
    "                \n",
    "                # Add connecting line\n",
    "                if i < len(features) - 1:\n",
    "                    ax.plot([cumulative[i], cumulative[i]], [i-0.4, i+0.4], \n",
    "                           'k--', linewidth=0.5, alpha=0.3)\n",
    "        \n",
    "        # Final prediction bar\n",
    "        ax.barh(len(features)-1, cumulative[-1], color='green', alpha=0.3)\n",
    "        \n",
    "        # Labels\n",
    "        ax.set_yticks(range(len(features)))\n",
    "        feature_labels = ['Base Value']\n",
    "        for fname, shap_val in top_features:\n",
    "            fval = instance_data.get(fname, 'N/A')\n",
    "            feature_labels.append(f\"{fname}={fval:.3f}\\n({shap_val:+.3f})\")\n",
    "        feature_labels.append(f\"Prediction\\n{instance_prediction:.3f}\")\n",
    "        ax.set_yticklabels(feature_labels, fontsize=9)\n",
    "        \n",
    "        ax.set_xlabel('Model Output (Probability)', fontsize=11)\n",
    "        ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example: Explain wafer binning predictions\n",
    "print(\"=\" * 60)\n",
    "print(\"Model Explainability: Wafer Device Binning\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate synthetic STDF-like data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Features: Vdd, Idd, frequency, temperature, test_time, spatial_correlation\n",
    "data = {\n",
    "    'vdd': np.random.normal(1.2, 0.02, n_samples),\n",
    "    'idd': np.random.normal(100, 10, n_samples),\n",
    "    'frequency': np.random.normal(2000, 100, n_samples),\n",
    "    'temperature': np.random.normal(25, 5, n_samples),\n",
    "    'test_time_ms': np.random.normal(50, 10, n_samples),\n",
    "    'neighbor_yield_avg': np.random.uniform(0.7, 1.0, n_samples)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create target: Fail if Vdd too low OR neighbor yield low\n",
    "df['binning'] = (\n",
    "    ((df['vdd'] < 1.18) | (df['neighbor_yield_avg'] < 0.75))\n",
    ").astype(int)\n",
    "\n",
    "# Train model\n",
    "X = df[['vdd', 'idd', 'frequency', 'temperature', 'test_time_ms', 'neighbor_yield_avg']].values\n",
    "y = df['binning'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "print(f\"\\n‚úÖ Model trained: {accuracy:.2%} accuracy on test set\")\n",
    "\n",
    "# Create explainer\n",
    "feature_names = ['vdd', 'idd', 'frequency', 'temperature', 'test_time_ms', 'neighbor_yield_avg']\n",
    "explainer = SimpleSHAPExplainer(model, X_train, feature_names)\n",
    "\n",
    "print(f\"üìä Base prediction (average): {explainer.base_value:.3f}\")\n",
    "\n",
    "# Explain specific failed device\n",
    "failed_indices = np.where(y_test == 1)[0]\n",
    "if len(failed_indices) > 0:\n",
    "    fail_idx = failed_indices[0]\n",
    "    X_fail = X_test[fail_idx]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Explaining Failed Device Prediction\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get prediction\n",
    "    pred_prob = model.predict_proba(X_fail.reshape(1, -1))[0, 1]\n",
    "    pred_class = model.predict(X_fail.reshape(1, -1))[0]\n",
    "    \n",
    "    print(f\"\\nDevice Features:\")\n",
    "    instance_data = {}\n",
    "    for i, fname in enumerate(feature_names):\n",
    "        print(f\"  {fname}: {X_fail[i]:.3f}\")\n",
    "        instance_data[fname] = X_fail[i]\n",
    "    \n",
    "    print(f\"\\nPrediction: {'Fail' if pred_class == 1 else 'Pass'} (probability: {pred_prob:.3f})\")\n",
    "    \n",
    "    # Compute SHAP values\n",
    "    print(\"\\nüîç Computing SHAP values (feature attributions)...\")\n",
    "    shap_vals = explainer.explain_instance(X_fail, num_samples=50)\n",
    "    \n",
    "    print(\"\\nFeature Contributions (SHAP values):\")\n",
    "    for fname, shap_val in sorted(shap_vals.items(), key=lambda x: abs(x[1]), reverse=True):\n",
    "        direction = \"‚Üí FAIL\" if shap_val > 0 else \"‚Üí PASS\"\n",
    "        print(f\"  {fname}: {shap_val:+.4f} {direction}\")\n",
    "    \n",
    "    # Verify: base_value + sum(SHAP) ‚âà prediction\n",
    "    total_shap = sum(shap_vals.values())\n",
    "    reconstructed = explainer.base_value + total_shap\n",
    "    print(f\"\\n‚úÖ Verification:\")\n",
    "    print(f\"  Base value: {explainer.base_value:.4f}\")\n",
    "    print(f\"  Sum of SHAP values: {total_shap:+.4f}\")\n",
    "    print(f\"  Predicted probability: {pred_prob:.4f}\")\n",
    "    print(f\"  Reconstructed: {reconstructed:.4f} (difference: {abs(pred_prob - reconstructed):.6f})\")\n",
    "    \n",
    "    # Visualize\n",
    "    explainer.plot_waterfall(shap_vals, pred_prob, instance_data, \n",
    "                            title=\"SHAP Waterfall: Why Device Failed Binning\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Key Insights:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"‚Ä¢ SHAP values show exact contribution of each feature to prediction\")\n",
    "print(\"‚Ä¢ Negative SHAP values push toward Pass (class 0)\")\n",
    "print(\"‚Ä¢ Positive SHAP values push toward Fail (class 1)\")\n",
    "print(\"‚Ä¢ Use for debugging: Identify which features drove wrong predictions\")\n",
    "print(\"‚Ä¢ Use for transparency: Explain decisions to stakeholders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62587c1",
   "metadata": {},
   "source": [
    "## 4. ‚ö° Performance Profiling and Latency Optimization\n",
    "\n",
    "### üìù What's Happening in This Section?\n",
    "\n",
    "**Purpose:** Build performance profiling system to measure latency breakdown of ML inference pipelines, identify bottlenecks, and optimize for production SLAs (<100ms p99 latency).\n",
    "\n",
    "**Key Points:**\n",
    "- **Latency breakdown**: Measure time spent in each stage (data preprocessing: 40ms, model inference: 30ms, post-processing: 20ms)\n",
    "- **Percentile analysis**: Track p50, p95, p99 latency (catch tail latency issues that impact UX)\n",
    "- **Batch vs single inference**: Compare throughput (single: 50 QPS, batch=32: 800 QPS)\n",
    "- **Memory profiling**: Track peak memory usage (detect memory leaks, optimize batch size)\n",
    "- **CPU/GPU utilization**: Identify underutilized resources (GPU at 30% ‚Üí increase batch size)\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Meet SLAs:** Production requires <100ms p99 latency (profiling reveals 150ms ‚Üí optimize to 80ms)\n",
    "- **Cost optimization:** Increase throughput 16x (50 QPS ‚Üí 800 QPS) by batching (fewer servers needed)\n",
    "- **User experience:** Reduce tail latency (p99: 500ms ‚Üí 120ms) improves customer satisfaction\n",
    "- **Capacity planning:** Understand resource limits (max throughput before latency degrades)\n",
    "\n",
    "**Post-Silicon Application:** Profile wafer map inference: Spatial neighbor search takes 400ms (80% of latency) ‚Üí optimize with KD-tree ‚Üí reduce to 50ms ‚Üí total latency 500ms ‚Üí 100ms (5x speedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceProfiler:\n",
    "    \"\"\"Performance profiling system for ML inference pipelines\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stage_timings = {}  # stage_name ‚Üí list of durations\n",
    "        self.memory_usage = []\n",
    "    \n",
    "    def profile_stage(self, stage_name: str):\n",
    "        \"\"\"Context manager for profiling a stage\"\"\"\n",
    "        return StageProfiler(self, stage_name)\n",
    "    \n",
    "    def record_timing(self, stage_name: str, duration_ms: float):\n",
    "        \"\"\"Record timing for a stage\"\"\"\n",
    "        if stage_name not in self.stage_timings:\n",
    "            self.stage_timings[stage_name] = []\n",
    "        self.stage_timings[stage_name].append(duration_ms)\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Compute summary statistics for all stages\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        for stage, timings in self.stage_timings.items():\n",
    "            if timings:\n",
    "                stats[stage] = {\n",
    "                    'count': len(timings),\n",
    "                    'mean_ms': np.mean(timings),\n",
    "                    'std_ms': np.std(timings),\n",
    "                    'p50_ms': np.percentile(timings, 50),\n",
    "                    'p95_ms': np.percentile(timings, 95),\n",
    "                    'p99_ms': np.percentile(timings, 99),\n",
    "                    'min_ms': np.min(timings),\n",
    "                    'max_ms': np.max(timings)\n",
    "                }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def print_summary(self, sla_ms: Optional[float] = None):\n",
    "        \"\"\"Print performance summary\"\"\"\n",
    "        stats = self.get_statistics()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"Performance Profile Summary\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(f\"\\n{'Stage':<30} {'Count':>8} {'Mean':>10} {'P50':>10} {'P95':>10} {'P99':>10}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        total_mean = 0\n",
    "        for stage, s in sorted(stats.items(), key=lambda x: x[1]['mean_ms'], reverse=True):\n",
    "            print(f\"{stage:<30} {s['count']:>8} {s['mean_ms']:>9.2f}ms {s['p50_ms']:>9.2f}ms \"\n",
    "                  f\"{s['p95_ms']:>9.2f}ms {s['p99_ms']:>9.2f}ms\")\n",
    "            total_mean += s['mean_ms']\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'TOTAL':<30} {'':<8} {total_mean:>9.2f}ms\")\n",
    "        \n",
    "        if sla_ms:\n",
    "            # Check p99 against SLA\n",
    "            max_p99 = max(s['p99_ms'] for s in stats.values())\n",
    "            sla_status = \"‚úÖ PASS\" if max_p99 < sla_ms else \"‚ùå FAIL\"\n",
    "            print(f\"\\nSLA Check (p99 < {sla_ms}ms): {sla_status} (actual p99: {max_p99:.2f}ms)\")\n",
    "    \n",
    "    def plot_latency_distribution(self):\n",
    "        \"\"\"Plot latency distribution for each stage\"\"\"\n",
    "        stats = self.get_statistics()\n",
    "        \n",
    "        if not stats:\n",
    "            print(\"No timing data to plot\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Plot 1: Box plot of latencies\n",
    "        ax1 = axes[0]\n",
    "        stage_names = list(self.stage_timings.keys())\n",
    "        data = [self.stage_timings[stage] for stage in stage_names]\n",
    "        \n",
    "        bp = ax1.boxplot(data, labels=stage_names, patch_artist=True)\n",
    "        for patch in bp['boxes']:\n",
    "            patch.set_facecolor('skyblue')\n",
    "        \n",
    "        ax1.set_ylabel('Latency (ms)', fontsize=11)\n",
    "        ax1.set_title('Latency Distribution by Stage', fontsize=12, fontweight='bold')\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Plot 2: Cumulative percentage breakdown\n",
    "        ax2 = axes[1]\n",
    "        total_times = {stage: np.sum(timings) for stage, timings in self.stage_timings.items()}\n",
    "        total = sum(total_times.values())\n",
    "        \n",
    "        sorted_stages = sorted(total_times.items(), key=lambda x: x[1], reverse=True)\n",
    "        stages = [s[0] for s in sorted_stages]\n",
    "        percentages = [(s[1] / total * 100) for s in sorted_stages]\n",
    "        \n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(stages)))\n",
    "        ax2.bar(stages, percentages, color=colors, alpha=0.8)\n",
    "        \n",
    "        ax2.set_ylabel('% of Total Time', fontsize=11)\n",
    "        ax2.set_title('Time Breakdown by Stage', fontsize=12, fontweight='bold')\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class StageProfiler:\n",
    "    \"\"\"Context manager for profiling individual stages\"\"\"\n",
    "    \n",
    "    def __init__(self, profiler: PerformanceProfiler, stage_name: str):\n",
    "        self.profiler = profiler\n",
    "        self.stage_name = stage_name\n",
    "        self.start_time = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        duration_ms = (time.time() - self.start_time) * 1000\n",
    "        self.profiler.record_timing(self.stage_name, duration_ms)\n",
    "\n",
    "\n",
    "# Example: Profile wafer map inference pipeline\n",
    "print(\"=\" * 60)\n",
    "print(\"Performance Profiling: Wafer Map Inference\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "profiler = PerformanceProfiler()\n",
    "\n",
    "# Train model on synthetic wafer data\n",
    "np.random.seed(42)\n",
    "n_devices = 5000\n",
    "\n",
    "X_wafer = np.random.randn(n_devices, 10)\n",
    "y_wafer = (X_wafer[:, 0] + X_wafer[:, 1] > 0).astype(int)\n",
    "\n",
    "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(\n",
    "    X_wafer, y_wafer, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "wafer_model = RandomForestClassifier(n_estimators=50, max_depth=8, random_state=42)\n",
    "wafer_model.fit(X_train_w, y_train_w)\n",
    "\n",
    "print(\"‚úÖ Model trained for wafer map inference\")\n",
    "\n",
    "# Simulate 100 inference requests with profiling\n",
    "print(\"\\nüîç Profiling 100 inference requests...\")\n",
    "\n",
    "for i in range(100):\n",
    "    # Stage 1: Data preprocessing\n",
    "    with profiler.profile_stage(\"data_preprocessing\"):\n",
    "        # Simulate feature normalization, validation\n",
    "        X_request = X_test_w[i:i+1].copy()\n",
    "        X_normalized = (X_request - X_request.mean()) / (X_request.std() + 1e-8)\n",
    "        time.sleep(0.002 + np.random.uniform(0, 0.003))\n",
    "    \n",
    "    # Stage 2: Feature engineering (spatial correlation)\n",
    "    with profiler.profile_stage(\"feature_engineering\"):\n",
    "        # Simulate expensive spatial neighbor search\n",
    "        time.sleep(0.015 + np.random.uniform(0, 0.010))\n",
    "    \n",
    "    # Stage 3: Model inference\n",
    "    with profiler.profile_stage(\"model_inference\"):\n",
    "        prediction = wafer_model.predict_proba(X_normalized)\n",
    "        time.sleep(0.001 + np.random.uniform(0, 0.002))\n",
    "    \n",
    "    # Stage 4: Post-processing\n",
    "    with profiler.profile_stage(\"post_processing\"):\n",
    "        # Simulate result formatting, logging\n",
    "        result = {\n",
    "            'prediction': int(prediction.argmax()),\n",
    "            'confidence': float(prediction.max()),\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        time.sleep(0.001 + np.random.uniform(0, 0.001))\n",
    "\n",
    "# Print summary\n",
    "profiler.print_summary(sla_ms=25.0)\n",
    "\n",
    "# Visualize\n",
    "profiler.plot_latency_distribution()\n",
    "\n",
    "# Optimization recommendations\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ Optimization Recommendations:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "stats = profiler.get_statistics()\n",
    "bottleneck = max(stats.items(), key=lambda x: x[1]['mean_ms'])\n",
    "\n",
    "print(f\"\\n1. BOTTLENECK IDENTIFIED: '{bottleneck[0]}' ({bottleneck[1]['mean_ms']:.2f}ms mean)\")\n",
    "print(f\"   ‚Üí This stage consumes {bottleneck[1]['mean_ms'] / sum(s['mean_ms'] for s in stats.values()) * 100:.1f}% of total time\")\n",
    "\n",
    "if 'feature_engineering' in stats:\n",
    "    print(f\"\\n2. FEATURE ENGINEERING OPTIMIZATION:\")\n",
    "    print(f\"   Current: {stats['feature_engineering']['mean_ms']:.2f}ms (spatial neighbor search)\")\n",
    "    print(f\"   Recommended: Use KD-tree or spatial indexing ‚Üí reduce to <5ms (3x speedup)\")\n",
    "\n",
    "print(f\"\\n3. BATCHING OPPORTUNITY:\")\n",
    "print(f\"   Current: Single request = {sum(s['mean_ms'] for s in stats.values()):.2f}ms\")\n",
    "print(f\"   Recommended: Batch 32 requests ‚Üí amortize overhead ‚Üí 5-10x throughput increase\")\n",
    "\n",
    "print(f\"\\n4. CACHING OPPORTUNITY:\")\n",
    "print(f\"   Cache preprocessed features for repeated requests\")\n",
    "print(f\"   Expected: 40-60% latency reduction for cache hits\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d27d0",
   "metadata": {},
   "source": [
    "## 5. üîé Error Analysis and Root Cause Detection\n",
    "\n",
    "### üìù What's Happening in This Section?\n",
    "\n",
    "**Purpose:** Build systematic error analysis framework to identify patterns in model failures, prioritize debugging efforts, and detect root causes (data quality issues, feature engineering bugs, model limitations).\n",
    "\n",
    "**Key Points:**\n",
    "- **Error clustering**: Group similar failures (spatial patterns, feature value ranges, temporal trends)\n",
    "- **Confusion matrix analysis**: Identify which classes are confused (Fail predicted as Pass vs Pass as Fail)\n",
    "- **Feature correlation with errors**: Which feature values predict mistakes? (errors when Vdd < 1.18V)\n",
    "- **Temporal error patterns**: Are errors increasing over time? (concept drift detection)\n",
    "- **Severity-based prioritization**: Focus on high-impact errors (false negatives in safety-critical systems)\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Faster debugging:** Identify root cause in 15 minutes (not 8 hours of manual inspection)\n",
    "- **Prioritize fixes:** Focus on errors that impact business (false negatives cost $10K each)\n",
    "- **Prevent recurrence:** Fix root cause (data quality issue), not symptoms (retrain model)\n",
    "- **Improve model:** Learn from failures, add features to address systematic errors\n",
    "\n",
    "**Post-Silicon Application:** Analyze 100 test failures: 45% have Vdd < 1.18V (out-of-spec), 30% have high spatial correlation (neighboring devices failed), 25% have temperature extremes ‚Üí prioritize Vdd validation in data pipeline (prevents 45% of errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e7ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorAnalyzer:\n",
    "    \"\"\"Systematic error analysis framework for ML models\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_names: List[str]):\n",
    "        self.feature_names = feature_names\n",
    "        self.errors = []  # List of error records\n",
    "    \n",
    "    def log_error(self, X: np.ndarray, y_true: int, y_pred: int, \n",
    "                  prediction_prob: float, metadata: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Log a prediction error\n",
    "        \n",
    "        Args:\n",
    "            X: Feature values\n",
    "            y_true: True label\n",
    "            y_pred: Predicted label\n",
    "            prediction_prob: Prediction probability/confidence\n",
    "            metadata: Optional metadata (timestamp, device_id, etc.)\n",
    "        \"\"\"\n",
    "        error_record = {\n",
    "            'features': dict(zip(self.feature_names, X)),\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "            'prediction_prob': prediction_prob,\n",
    "            'error_type': self._classify_error(y_true, y_pred),\n",
    "            'metadata': metadata or {}\n",
    "        }\n",
    "        self.errors.append(error_record)\n",
    "    \n",
    "    def _classify_error(self, y_true: int, y_pred: int) -> str:\n",
    "        \"\"\"Classify error type\"\"\"\n",
    "        if y_true == 0 and y_pred == 1:\n",
    "            return 'false_positive'\n",
    "        elif y_true == 1 and y_pred == 0:\n",
    "            return 'false_negative'\n",
    "        else:\n",
    "            return 'correct'\n",
    "    \n",
    "    def analyze_error_patterns(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze patterns in errors\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with error analysis results\n",
    "        \"\"\"\n",
    "        if not self.errors:\n",
    "            return {'message': 'No errors logged'}\n",
    "        \n",
    "        # Error type distribution\n",
    "        error_types = {}\n",
    "        for err in self.errors:\n",
    "            et = err['error_type']\n",
    "            error_types[et] = error_types.get(et, 0) + 1\n",
    "        \n",
    "        # Feature statistics for errors\n",
    "        feature_stats = {}\n",
    "        for fname in self.feature_names:\n",
    "            values = [err['features'][fname] for err in self.errors]\n",
    "            feature_stats[fname] = {\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values),\n",
    "                'min': np.min(values),\n",
    "                'max': np.max(values),\n",
    "                'p25': np.percentile(values, 25),\n",
    "                'p75': np.percentile(values, 75)\n",
    "            }\n",
    "        \n",
    "        # Confidence distribution for errors\n",
    "        confidences = [err['prediction_prob'] for err in self.errors]\n",
    "        \n",
    "        return {\n",
    "            'total_errors': len(self.errors),\n",
    "            'error_type_distribution': error_types,\n",
    "            'feature_statistics': feature_stats,\n",
    "            'confidence_stats': {\n",
    "                'mean': np.mean(confidences),\n",
    "                'std': np.std(confidences),\n",
    "                'p50': np.percentile(confidences, 50)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def find_root_causes(self, X_train: np.ndarray, threshold_std: float = 2.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Identify potential root causes by comparing error features to training distribution\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training data for baseline distribution\n",
    "            threshold_std: Number of std devs for outlier detection\n",
    "        \n",
    "        Returns:\n",
    "            List of potential root causes\n",
    "        \"\"\"\n",
    "        root_causes = []\n",
    "        \n",
    "        # Compute training statistics\n",
    "        train_stats = {}\n",
    "        for i, fname in enumerate(self.feature_names):\n",
    "            train_stats[fname] = {\n",
    "                'mean': X_train[:, i].mean(),\n",
    "                'std': X_train[:, i].std()\n",
    "            }\n",
    "        \n",
    "        # Check each feature for distribution shift in errors\n",
    "        for fname in self.feature_names:\n",
    "            error_values = [err['features'][fname] for err in self.errors]\n",
    "            error_mean = np.mean(error_values)\n",
    "            \n",
    "            # Calculate shift in standard deviations\n",
    "            train_mean = train_stats[fname]['mean']\n",
    "            train_std = train_stats[fname]['std']\n",
    "            \n",
    "            shift_stds = abs(error_mean - train_mean) / (train_std + 1e-8)\n",
    "            \n",
    "            if shift_stds > threshold_std:\n",
    "                root_causes.append({\n",
    "                    'feature': fname,\n",
    "                    'shift_std_devs': shift_stds,\n",
    "                    'train_mean': train_mean,\n",
    "                    'error_mean': error_mean,\n",
    "                    'recommendation': f\"Errors occur when {fname} deviates from training distribution\"\n",
    "                })\n",
    "        \n",
    "        # Sort by magnitude of shift\n",
    "        root_causes.sort(key=lambda x: x['shift_std_devs'], reverse=True)\n",
    "        \n",
    "        return root_causes\n",
    "    \n",
    "    def plot_error_analysis(self):\n",
    "        \"\"\"Visualize error patterns\"\"\"\n",
    "        if not self.errors:\n",
    "            print(\"No errors to analyze\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Plot 1: Error type distribution\n",
    "        ax1 = axes[0, 0]\n",
    "        analysis = self.analyze_error_patterns()\n",
    "        error_types = analysis['error_type_distribution']\n",
    "        \n",
    "        ax1.bar(error_types.keys(), error_types.values(), color=['red', 'orange'], alpha=0.7)\n",
    "        ax1.set_ylabel('Count', fontsize=11)\n",
    "        ax1.set_title('Error Type Distribution', fontsize=12, fontweight='bold')\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Feature value distributions (errors vs overall)\n",
    "        ax2 = axes[0, 1]\n",
    "        \n",
    "        # Select most important feature (highest variance in errors)\n",
    "        variances = {fname: np.var([err['features'][fname] for err in self.errors]) \n",
    "                    for fname in self.feature_names}\n",
    "        top_feature = max(variances.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        error_vals = [err['features'][top_feature] for err in self.errors]\n",
    "        ax2.hist(error_vals, bins=20, alpha=0.7, color='red', label='Errors')\n",
    "        ax2.set_xlabel(top_feature, fontsize=11)\n",
    "        ax2.set_ylabel('Frequency', fontsize=11)\n",
    "        ax2.set_title(f'Feature Distribution: {top_feature}', fontsize=12, fontweight='bold')\n",
    "        ax2.legend()\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Confidence distribution for errors\n",
    "        ax3 = axes[1, 0]\n",
    "        confidences = [err['prediction_prob'] for err in self.errors]\n",
    "        \n",
    "        ax3.hist(confidences, bins=20, alpha=0.7, color='orange', edgecolor='black')\n",
    "        ax3.axvline(np.mean(confidences), color='red', linestyle='--', \n",
    "                   label=f'Mean: {np.mean(confidences):.3f}')\n",
    "        ax3.set_xlabel('Prediction Confidence', fontsize=11)\n",
    "        ax3.set_ylabel('Frequency', fontsize=11)\n",
    "        ax3.set_title('Confidence Distribution for Errors', fontsize=12, fontweight='bold')\n",
    "        ax3.legend()\n",
    "        ax3.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Feature importance for errors (variance-based)\n",
    "        ax4 = axes[1, 1]\n",
    "        \n",
    "        sorted_features = sorted(variances.items(), key=lambda x: x[1], reverse=True)[:8]\n",
    "        features = [f[0] for f in sorted_features]\n",
    "        vars = [f[1] for f in sorted_features]\n",
    "        \n",
    "        ax4.barh(features, vars, color='skyblue', alpha=0.8)\n",
    "        ax4.set_xlabel('Variance in Error Cases', fontsize=11)\n",
    "        ax4.set_title('Feature Variance in Errors (High = Discriminative)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax4.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example: Error analysis for wafer binning model\n",
    "print(\"=\" * 60)\n",
    "print(\"Error Analysis: Wafer Device Binning Failures\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate predictions and log errors\n",
    "analyzer = ErrorAnalyzer(feature_names)\n",
    "\n",
    "print(\"\\nüîç Analyzing model predictions and logging errors...\")\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Log errors\n",
    "error_count = 0\n",
    "for i in range(len(X_test)):\n",
    "    if y_pred_test[i] != y_test[i]:\n",
    "        analyzer.log_error(\n",
    "            X=X_test[i],\n",
    "            y_true=y_test[i],\n",
    "            y_pred=y_pred_test[i],\n",
    "            prediction_prob=y_pred_proba[i],\n",
    "            metadata={'sample_index': i}\n",
    "        )\n",
    "        error_count += 1\n",
    "\n",
    "print(f\"‚úÖ Logged {error_count} errors out of {len(X_test)} predictions ({error_count/len(X_test)*100:.1f}% error rate)\")\n",
    "\n",
    "# Analyze patterns\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Error Pattern Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "analysis = analyzer.analyze_error_patterns()\n",
    "\n",
    "print(f\"\\nTotal Errors: {analysis['total_errors']}\")\n",
    "print(\"\\nError Type Distribution:\")\n",
    "for etype, count in analysis['error_type_distribution'].items():\n",
    "    pct = count / analysis['total_errors'] * 100\n",
    "    print(f\"  {etype}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nFeature Statistics in Error Cases:\")\n",
    "for fname, stats in list(analysis['feature_statistics'].items())[:5]:\n",
    "    print(f\"  {fname}:\")\n",
    "    print(f\"    Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}\")\n",
    "    print(f\"    Range: [{stats['min']:.4f}, {stats['max']:.4f}]\")\n",
    "\n",
    "# Find root causes\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Root Cause Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "root_causes = analyzer.find_root_causes(X_train, threshold_std=1.5)\n",
    "\n",
    "if root_causes:\n",
    "    print(f\"\\nüéØ Identified {len(root_causes)} potential root causes:\\n\")\n",
    "    \n",
    "    for i, cause in enumerate(root_causes[:5], 1):\n",
    "        print(f\"{i}. Feature: {cause['feature']}\")\n",
    "        print(f\"   Shift: {cause['shift_std_devs']:.2f} standard deviations\")\n",
    "        print(f\"   Training mean: {cause['train_mean']:.4f}\")\n",
    "        print(f\"   Error mean: {cause['error_mean']:.4f}\")\n",
    "        print(f\"   üí° {cause['recommendation']}\\n\")\n",
    "else:\n",
    "    print(\"No significant root causes detected (errors appear random)\")\n",
    "\n",
    "# Visualize\n",
    "analyzer.plot_error_analysis()\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîß Debugging Recommendations:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if root_causes:\n",
    "    top_cause = root_causes[0]\n",
    "    print(f\"\\n1. PRIORITY: Investigate '{top_cause['feature']}' feature\")\n",
    "    print(f\"   ‚Üí Errors have {top_cause['shift_std_devs']:.1f}x higher deviation than training\")\n",
    "    print(f\"   ‚Üí Check data pipeline for '{top_cause['feature']}' (quality issues?)\")\n",
    "    \n",
    "print(f\"\\n2. ERROR TYPE FOCUS:\")\n",
    "fp_count = analysis['error_type_distribution'].get('false_positive', 0)\n",
    "fn_count = analysis['error_type_distribution'].get('false_negative', 0)\n",
    "\n",
    "if fp_count > fn_count:\n",
    "    print(f\"   ‚Üí More false positives ({fp_count}) than false negatives ({fn_count})\")\n",
    "    print(f\"   ‚Üí Model is too conservative (predicts Fail when actually Pass)\")\n",
    "    print(f\"   ‚Üí Recommendation: Adjust decision threshold upward (0.5 ‚Üí 0.6)\")\n",
    "else:\n",
    "    print(f\"   ‚Üí More false negatives ({fn_count}) than false positives ({fp_count})\")\n",
    "    print(f\"   ‚Üí Model is too aggressive (predicts Pass when actually Fail)\")\n",
    "    print(f\"   ‚Üí Recommendation: Adjust decision threshold downward (0.5 ‚Üí 0.4)\")\n",
    "\n",
    "print(f\"\\n3. CONFIDENCE ANALYSIS:\")\n",
    "conf_mean = analysis['confidence_stats']['mean']\n",
    "if conf_mean < 0.7:\n",
    "    print(f\"   ‚Üí Low average confidence ({conf_mean:.3f}) in error cases\")\n",
    "    print(f\"   ‚Üí Model is uncertain ‚Üí add more features or collect more training data\")\n",
    "else:\n",
    "    print(f\"   ‚Üí High average confidence ({conf_mean:.3f}) despite errors\")\n",
    "    print(f\"   ‚Üí Model is overconfident ‚Üí check for overfitting or feature leakage\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aca7355",
   "metadata": {},
   "source": [
    "## 6. üöÄ Real-World Project Templates\n",
    "\n",
    "---\n",
    "\n",
    "### Project 1: Distributed Tracing for Wafer Test Pipeline\n",
    "\n",
    "**Objective:** Implement end-to-end distributed tracing for STDF data ingestion ‚Üí feature engineering ‚Üí binning prediction ‚Üí result storage pipeline\n",
    "\n",
    "**Business Value:**\n",
    "- **Debug time reduction:** 8 hours (manual log inspection) ‚Üí 15 minutes (trace analysis)\n",
    "- **SLA compliance:** Identify 80% bottleneck (spatial neighbor search) ‚Üí optimize ‚Üí meet <100ms p99 latency\n",
    "- **Root cause visibility:** Trace errors back to source (STDF parsing failure ‚Üí which file, which device)\n",
    "\n",
    "**Features to Implement:**\n",
    "- Trace ID propagation across distributed services (ingestion ‚Üí feature store ‚Üí model service ‚Üí storage)\n",
    "- Span instrumentation for each pipeline stage (capture timing, metadata, errors)\n",
    "- Trace aggregation dashboard (visualize end-to-end flow, identify slowest spans)\n",
    "- Error correlation (trace ID ‚Üí all related logs/metrics across services)\n",
    "- Latency percentile tracking (p50, p95, p99 per stage)\n",
    "- Context enrichment (wafer_id, device_count, cache hits, model version)\n",
    "\n",
    "**Success Criteria:**\n",
    "- ‚úÖ 100% of requests have trace IDs (enable full request tracking)\n",
    "- ‚úÖ Latency breakdown accurate to ¬±5ms (identify true bottlenecks)\n",
    "- ‚úÖ Error traces available within 1 second (real-time debugging)\n",
    "- ‚úÖ Trace retention: 7 days (support historical analysis)\n",
    "- ‚úÖ Dashboard visualizes critical path (Gantt chart of spans)\n",
    "\n",
    "**STDF Data Application:**\n",
    "- Trace: STDF file upload ‚Üí parsing ‚Üí device extraction ‚Üí feature computation (spatial correlation, parametric aggregations) ‚Üí model inference ‚Üí binning decision ‚Üí DynamoDB write\n",
    "- Insight: \"Why did wafer W0042 take 2 seconds?\" ‚Üí Trace shows spatial correlation took 1.5s ‚Üí optimize neighbor search\n",
    "\n",
    "---\n",
    "\n",
    "### Project 2: SHAP-Based Model Explainability for Fraud Detection\n",
    "\n",
    "**Objective:** Build production explainability system for fraud detection model, providing real-time SHAP explanations for flagged transactions\n",
    "\n",
    "**Business Value:**\n",
    "- **Customer transparency:** Explain why transaction flagged (dispute resolution, regulatory compliance)\n",
    "- **Model debugging:** Identify when model relies on spurious features (debug before production deployment)\n",
    "- **Feature engineering:** Remove uninformative features (reduce model size, improve latency)\n",
    "\n",
    "**Features to Implement:**\n",
    "- Real-time SHAP value computation (<50ms p99 latency for online explanations)\n",
    "- Waterfall plot generation (visualize feature contributions for customer service reps)\n",
    "- Global feature importance aggregation (which features matter most across all predictions)\n",
    "- Explanation caching (cache SHAP values for repeated requests)\n",
    "- Explainability API (REST endpoint: POST /explain with transaction_id)\n",
    "\n",
    "**Success Criteria:**\n",
    "- ‚úÖ SHAP values computed in <50ms p99 (production SLA)\n",
    "- ‚úÖ Explanation accuracy: base_value + Œ£(SHAP) = prediction (within 0.01)\n",
    "- ‚úÖ Cover 100% of flagged transactions (regulatory requirement)\n",
    "- ‚úÖ Customer satisfaction: 80% of disputes resolved with explanation\n",
    "- ‚úÖ Feature audit: Identify top 10 drivers (focus feature engineering efforts)\n",
    "\n",
    "**Data Application:**\n",
    "- Features: transaction_amount, user_account_age, recent_transactions_30d, merchant_category, device_fingerprint\n",
    "- Explain: \"Why flagged?\" ‚Üí SHAP shows transaction_amount (+0.25), recent_transactions_30d (+0.15), device_fingerprint (+0.10) ‚Üí total +0.50 ‚Üí flagged\n",
    "\n",
    "---\n",
    "\n",
    "### Project 3: Performance Profiling for Recommendation Engine\n",
    "\n",
    "**Objective:** Profile recommendation engine to identify latency bottlenecks, optimize to meet <100ms p99 latency SLA for 10K QPS\n",
    "\n",
    "**Business Value:**\n",
    "- **Revenue impact:** Reduce latency 150ms ‚Üí 80ms ‚Üí improve user engagement (click-through rate +5%)\n",
    "- **Cost optimization:** Identify 10x throughput opportunity (batching) ‚Üí reduce server count 50% ‚Üí save $200K/year\n",
    "- **SLA compliance:** Meet <100ms p99 requirement (avoid penalties, customer churn)\n",
    "\n",
    "**Features to Implement:**\n",
    "- Stage-level profiling (user feature fetch, item feature fetch, model scoring, ranking, personalization)\n",
    "- Percentile latency tracking (p50, p95, p99, p99.9 per stage)\n",
    "- Batch vs single inference comparison (measure throughput vs latency tradeoff)\n",
    "- Memory profiling (detect memory leaks, optimize batch size for GPU)\n",
    "- Continuous profiling (sample 1% of requests, low overhead)\n",
    "- Bottleneck alerting (alert when single stage >40% of total time)\n",
    "\n",
    "**Success Criteria:**\n",
    "- ‚úÖ p99 latency <100ms (meet SLA, currently 150ms)\n",
    "- ‚úÖ Throughput 10K QPS (support peak traffic)\n",
    "- ‚úÖ Memory usage <2GB per replica (enable efficient scaling)\n",
    "- ‚úÖ Identify bottleneck within 5 minutes of deployment (rapid debugging)\n",
    "- ‚úÖ Optimization targets: 3 highest-impact stages (prioritize engineering effort)\n",
    "\n",
    "**Data Application:**\n",
    "- Profile: User features (5ms) ‚Üí Item features (40ms) ‚Üí Model scoring (30ms) ‚Üí Ranking (60ms) ‚Üí Total 135ms\n",
    "- Bottleneck: Ranking stage (60ms, 44% of total) ‚Üí optimize with approximate nearest neighbor ‚Üí reduce to 15ms ‚Üí total 90ms (‚úÖ meet SLA)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 4: Error Analysis for Autonomous Driving Perception\n",
    "\n",
    "**Objective:** Build error analysis system for object detection model (safety-critical), identify root causes of false negatives (missed pedestrians)\n",
    "\n",
    "**Business Value:**\n",
    "- **Safety:** Reduce false negatives 100 ‚Üí 10 (prevent accidents, save lives)\n",
    "- **Regulatory compliance:** Demonstrate systematic error analysis for NHTSA review\n",
    "- **Model improvement:** Prioritize data collection (focus on challenging scenarios: night, rain, occlusion)\n",
    "\n",
    "**Features to Implement:**\n",
    "- Error logging (log every false negative with full context: image, features, metadata)\n",
    "- Error clustering (group similar errors: nighttime, occlusions, small objects, far distance)\n",
    "- Feature correlation analysis (which sensor features predict errors?)\n",
    "- Temporal error tracking (are errors increasing? concept drift?)\n",
    "- Severity-based prioritization (focus on high-risk errors: highway > parking lot)\n",
    "- Automated root cause detection (compare error distribution vs training distribution)\n",
    "\n",
    "**Success Criteria:**\n",
    "- ‚úÖ 100% false negative logging (capture every missed pedestrian)\n",
    "- ‚úÖ Root cause identified in <1 hour (rapid debugging for safety issues)\n",
    "- ‚úÖ Error rate reduction: 2% ‚Üí 0.5% (systematic improvement)\n",
    "- ‚úÖ Cluster quality: 80% of errors explained by top 3 clusters (actionable insights)\n",
    "- ‚úÖ Data collection priorities: top 5 scenarios for new data (efficient improvement)\n",
    "\n",
    "**Data Application:**\n",
    "- Analyze: 100 false negatives (missed pedestrians)\n",
    "- Findings: 40% nighttime (low visibility), 35% occluded (behind cars), 25% small/far (sensor resolution)\n",
    "- Action: Collect 10K nighttime images, train occlusion-robust model, improve small object detection\n",
    "\n",
    "---\n",
    "\n",
    "### Project 5: Observability Dashboard for Yield Prediction Model\n",
    "\n",
    "**Objective:** Build comprehensive observability dashboard for wafer yield prediction, unifying tracing, explainability, profiling, and error analysis\n",
    "\n",
    "**Business Value:**\n",
    "- **Single pane of glass:** Debug any issue from one dashboard (reduce MTTR 2 hours ‚Üí 20 minutes)\n",
    "- **Proactive monitoring:** Detect issues before business impact (drift, latency spikes, error patterns)\n",
    "- **Stakeholder transparency:** Explain model behavior to non-technical fab managers\n",
    "\n",
    "**Features to Implement:**\n",
    "- Real-time metrics (QPS, latency percentiles, error rate, model accuracy)\n",
    "- Distributed traces (visualize end-to-end request flow)\n",
    "- SHAP waterfall plots (explain specific predictions on-demand)\n",
    "- Error analysis (cluster errors, identify root causes)\n",
    "- Performance breakdown (latency by stage, bottleneck identification)\n",
    "- Drift detection (feature drift, concept drift, performance degradation)\n",
    "- Alerting (integrate with PagerDuty, Slack for critical issues)\n",
    "\n",
    "**Success Criteria:**\n",
    "- ‚úÖ Dashboard load time <2 seconds (real-time debugging)\n",
    "- ‚úÖ Cover all observability dimensions (metrics, traces, logs, explanations)\n",
    "- ‚úÖ Drill-down capability (dashboard ‚Üí trace ‚Üí error ‚Üí SHAP explanation in <10 clicks)\n",
    "- ‚úÖ Adoption: 100% of engineers use for debugging (replace manual log inspection)\n",
    "- ‚úÖ MTTR reduction: 2 hours ‚Üí 20 minutes (10x faster debugging)\n",
    "\n",
    "**STDF Application:**\n",
    "- Dashboard shows: \n",
    "  - Metrics: 500 QPS, 45ms p99 latency, 0.1% error rate, 94% accuracy\n",
    "  - Trace: STDF ingestion (10ms) ‚Üí features (25ms) ‚Üí model (8ms) ‚Üí storage (2ms)\n",
    "  - SHAP: Top feature = neighbor_yield_avg (-0.3 ‚Üí predict low yield)\n",
    "  - Errors: 10 false positives (predicted low yield, actual high) ‚Üí cluster shows all have recent process change\n",
    "\n",
    "---\n",
    "\n",
    "### Project 6: Latency Optimization for Real-Time Bidding (RTB)\n",
    "\n",
    "**Objective:** Optimize RTB model inference to meet <10ms p99 latency (bid requests timeout at 100ms, model must be <10% of budget)\n",
    "\n",
    "**Business Value:**\n",
    "- **Revenue:** Reduce timeouts 20% ‚Üí 2% ‚Üí capture $5M additional revenue/year\n",
    "- **Competitive advantage:** Faster bids win auctions (improve win rate 30% ‚Üí 35%)\n",
    "- **Cost efficiency:** Meet SLA with 50% fewer servers (save $300K/year)\n",
    "\n",
    "**Features to Implement:**\n",
    "- Micro-profiling (nanosecond-level timing for critical path)\n",
    "- Model optimization (quantization, pruning, distillation)\n",
    "- Feature caching (pre-compute expensive aggregations)\n",
    "- Batch inference (process multiple bids in parallel)\n",
    "- GPU vs CPU comparison (measure cost/performance tradeoff)\n",
    "- Load testing (measure latency degradation under high QPS)\n",
    "\n",
    "**Success Criteria:**\n",
    "- ‚úÖ p99 latency <10ms (currently 25ms, 60% reduction needed)\n",
    "- ‚úÖ Throughput 100K QPS (support peak traffic)\n",
    "- ‚úÖ Timeout rate <2% (currently 20%, 10x reduction)\n",
    "- ‚úÖ Cost per inference <$0.0001 (enable profitability at scale)\n",
    "- ‚úÖ Model accuracy degradation <1% after optimization (maintain performance)\n",
    "\n",
    "**Data Application:**\n",
    "- Current: Feature fetch (15ms) + model (8ms) + post-process (2ms) = 25ms p99\n",
    "- Optimized: Cache features (3ms) + quantized model (4ms) + batch post-process (1ms) = 8ms p99 (‚úÖ meet SLA)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 7: Root Cause Detection for Test Failure Clustering (STDF)\n",
    "\n",
    "**Objective:** Automatically cluster test failures from STDF data, identify root causes (equipment drift, process variation, spatial patterns)\n",
    "\n",
    "**Business Value:**\n",
    "- **Debug time:** 8 hours (manual inspection) ‚Üí 30 minutes (automated clustering)\n",
    "- **Yield improvement:** Identify systematic issues (equipment calibration) ‚Üí fix ‚Üí improve yield 92% ‚Üí 95% (+$2M/year)\n",
    "- **Preventive maintenance:** Detect equipment drift early ‚Üí schedule maintenance ‚Üí prevent catastrophic failures\n",
    "\n",
    "**Features to Implement:**\n",
    "- Failure clustering (group devices by failure signature: which tests failed, values)\n",
    "- Spatial correlation analysis (wafer map visualization, identify spatial patterns)\n",
    "- Temporal trend detection (are failures increasing over time?)\n",
    "- Equipment correlation (which test equipment associated with failures?)\n",
    "- Parametric outlier detection (which parametric values out-of-spec?)\n",
    "- Automated root cause ranking (top 5 most likely causes with confidence scores)\n",
    "\n",
    "**Success Criteria:**\n",
    "- ‚úÖ Cluster 95% of failures into <10 clusters (actionable categories)\n",
    "- ‚úÖ Root cause accuracy 80% (validated by fab engineers)\n",
    "- ‚úÖ Analysis time <30 minutes for 10K device failures (scalable)\n",
    "- ‚úÖ Spatial pattern detection 90% accurate (identify die location issues)\n",
    "- ‚úÖ Prevent 50% of future failures (proactive equipment maintenance)\n",
    "\n",
    "**STDF Application:**\n",
    "- Input: 1000 failed devices from wafer W0100\n",
    "- Clustering: Cluster 1 (450 devices, Vdd < 1.18V, spatial: edge dies), Cluster 2 (300 devices, Idd > 180mA, equipment: tester #3), Cluster 3 (250 devices, temperature > 120¬∞C, temporal: last 2 hours)\n",
    "- Root causes: Edge die yield issue (process), Tester #3 calibration drift (equipment), Thermal chamber malfunction (equipment)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 8: Explainability for Medical Diagnosis Model (Regulatory Compliance)\n",
    "\n",
    "**Objective:** Provide full explainability for medical diagnosis model to meet FDA requirements (510(k) submission)\n",
    "\n",
    "**Business Value:**\n",
    "- **Regulatory approval:** FDA requires explainability for AI/ML medical devices (enable $50M market)\n",
    "- **Clinical trust:** Doctors understand model reasoning ‚Üí increase adoption 30% ‚Üí 80%\n",
    "- **Legal protection:** Audit trail for decisions (defend against malpractice claims)\n",
    "\n",
    "**Features to Implement:**\n",
    "- SHAP/LIME explanations for every prediction (waterfall plots, feature attributions)\n",
    "- Confidence intervals (uncertainty quantification for risk assessment)\n",
    "- Counterfactual explanations (\"What would need to change for different diagnosis?\")\n",
    "- Explanation versioning (link explanation to model version for audit trail)\n",
    "- Clinical validation (compare SHAP attributions vs clinician reasoning)\n",
    "- Regulatory report generation (automated PDF with explanation, confidence, model version)\n",
    "\n",
    "**Success Criteria:**\n",
    "- ‚úÖ 100% prediction coverage (explainability for every diagnosis)\n",
    "- ‚úÖ FDA submission includes explainability documentation (510(k) approval)\n",
    "- ‚úÖ Clinical validation: 85% agreement (SHAP matches clinician reasoning)\n",
    "- ‚úÖ Explanation generation <1 second (real-time during consultation)\n",
    "- ‚úÖ Audit trail: 5-year retention (compliance requirement)\n",
    "\n",
    "**Data Application:**\n",
    "- Diagnosis: Diabetic retinopathy (grade 3, proliferative)\n",
    "- SHAP explanation: Microaneurysms (+0.35), neovascularization (+0.28), hemorrhages (+0.15) ‚Üí total +0.78 ‚Üí grade 3\n",
    "- Counterfactual: \"If microaneurysms reduced by 50%, prediction would be grade 2 (moderate)\"\n",
    "- Regulatory: PDF report with retinal image, SHAP waterfall, confidence intervals, model version v2.3, timestamp, patient consent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b80051d",
   "metadata": {},
   "source": [
    "## 7. üéØ Comprehensive Takeaways: Mastering ML Observability & Debugging\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **The Three Pillars of ML Observability**\n",
    "\n",
    "**Traditional Software Observability:**\n",
    "- **Metrics:** CPU, memory, QPS, latency, error rate\n",
    "- **Logs:** Application logs, system logs, access logs\n",
    "- **Traces:** Distributed request flow across services\n",
    "\n",
    "**ML Observability Additions:**\n",
    "- **Model Performance:** Accuracy, precision, recall, drift detection\n",
    "- **Prediction Explainability:** Feature attributions, SHAP values, confidence\n",
    "- **Data Quality:** Schema validation, distribution shifts, anomalies\n",
    "\n",
    "**Unified Approach:**\n",
    "```\n",
    "Every ML prediction should generate:\n",
    "1. Metrics: latency, throughput, cache hit rate\n",
    "2. Logs: prediction, features, model version, timestamp\n",
    "3. Traces: request ID, span IDs for each pipeline stage\n",
    "4. Model telemetry: SHAP values, confidence, drift metrics\n",
    "5. Data quality: feature validation results, outlier flags\n",
    "```\n",
    "\n",
    "**Why All Five:**\n",
    "- **Metrics alone:** \"Latency is 150ms\" (but why? which stage? optimization target?)\n",
    "- **Metrics + Traces:** \"Feature fetch is 100ms\" (but why slow? cache miss? data size?)\n",
    "- **Metrics + Traces + Logs:** \"Cache miss for user_123\" (but why predict wrong? features OK?)\n",
    "- **Full observability:** \"Cache miss ‚Üí stale features ‚Üí wrong prediction (SHAP shows outdated purchase_30d)\"\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Distributed Tracing for ML Pipelines**\n",
    "\n",
    "**Trace Anatomy:**\n",
    "```\n",
    "Trace ID: req_12345 (single request, end-to-end)\n",
    "‚îú‚îÄ Span 1: request_handler (parent, total: 120ms)\n",
    "‚îÇ  ‚îú‚îÄ Span 2: feature_fetch (child of 1, 60ms)\n",
    "‚îÇ  ‚îÇ  ‚îú‚îÄ Span 3: cache_lookup (child of 2, 5ms, HIT)\n",
    "‚îÇ  ‚îÇ  ‚îî‚îÄ Span 4: db_query (child of 2, 50ms, fallback for misses)\n",
    "‚îÇ  ‚îú‚îÄ Span 5: feature_transform (child of 1, 20ms)\n",
    "‚îÇ  ‚îú‚îÄ Span 6: model_inference (child of 1, 30ms)\n",
    "‚îÇ  ‚îî‚îÄ Span 7: response_format (child of 1, 10ms)\n",
    "```\n",
    "\n",
    "**Critical Metadata to Attach:**\n",
    "- **Input size:** `{\"num_features\": 100, \"batch_size\": 32}`\n",
    "- **Cache status:** `{\"cache_hit\": true, \"cache_ttl_seconds\": 3600}`\n",
    "- **Model metadata:** `{\"model_version\": \"v2.3\", \"model_type\": \"RandomForest\"}`\n",
    "- **Resource usage:** `{\"memory_mb\": 250, \"cpu_cores\": 2}`\n",
    "- **Errors:** `{\"error\": \"NullPointerException\", \"stack_trace\": \"...\"}`\n",
    "\n",
    "**Trace Sampling:**\n",
    "- **Production:** Sample 1-5% (low overhead, sufficient for debugging)\n",
    "- **Debugging:** Sample 100% temporarily (full visibility, diagnose rare issues)\n",
    "- **Head-based sampling:** Decide at request start (consistent for entire trace)\n",
    "- **Tail-based sampling:** Decide at request end (keep only slow/error traces)\n",
    "\n",
    "**Post-Silicon Example:**\n",
    "```\n",
    "Trace: Wafer W0042 binning prediction (total: 2.1 seconds, SLA: <100ms)\n",
    "‚îú‚îÄ STDF parsing: 50ms (‚úÖ fast)\n",
    "‚îú‚îÄ Feature engineering: 1850ms (‚ùå bottleneck!)\n",
    "‚îÇ  ‚îú‚îÄ Device aggregation: 100ms\n",
    "‚îÇ  ‚îî‚îÄ Spatial correlation: 1750ms (‚ö†Ô∏è optimize neighbor search!)\n",
    "‚îú‚îÄ Model inference: 150ms\n",
    "‚îî‚îÄ Storage write: 50ms\n",
    "\n",
    "Action: Optimize spatial correlation (KD-tree index) ‚Üí reduce 1750ms ‚Üí 80ms ‚Üí total 330ms\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **SHAP Values: The Gold Standard for Explainability**\n",
    "\n",
    "**What are SHAP Values?**\n",
    "- **Game theory:** Based on Shapley values (cooperative game theory, fair contribution)\n",
    "- **Additive:** prediction = base_value + Œ£(SHAP_values) (exact decomposition)\n",
    "- **Model-agnostic:** Works for any model (trees, neural nets, linear models)\n",
    "- **Consistent:** If feature contribution increases, SHAP value increases (monotonic)\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "```\n",
    "For feature i:\n",
    "SHAP_i = Œ£ [|S|! (M - |S| - 1)! / M!] √ó [f(S ‚à™ {i}) - f(S)]\n",
    "        S‚äÜF\\{i}\n",
    "\n",
    "Where:\n",
    "- F: all features\n",
    "- S: subset of features (excluding i)\n",
    "- f(S): model prediction using only features in S\n",
    "- M: total number of features\n",
    "\n",
    "Interpretation: Average marginal contribution of feature i across all possible feature subsets\n",
    "```\n",
    "\n",
    "**Practical Computation:**\n",
    "- **Exact (small models):** Enumerate all 2^M subsets (exponential, only feasible for M<15)\n",
    "- **Approximate (large models):** Sample subsets, estimate SHAP values (fast, slight error)\n",
    "- **TreeSHAP (tree models):** Polynomial time algorithm for tree-based models (fast + exact)\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "**1. Debug Individual Predictions:**\n",
    "```\n",
    "Model predicted: Fail (prob = 0.85)\n",
    "SHAP waterfall:\n",
    "  Base value: 0.20 (average prediction on training data)\n",
    "  + vdd = 1.15V: +0.30 (low voltage ‚Üí fail)\n",
    "  + neighbor_yield = 0.65: +0.20 (neighbors failed ‚Üí fail)\n",
    "  + idd = 105mA: +0.10 (high current ‚Üí fail)\n",
    "  + temperature = 28¬∞C: +0.05 (normal temp, slight contribution)\n",
    "  = Prediction: 0.85 (‚úÖ decomposition exact)\n",
    "\n",
    "Insight: Primary driver is low Vdd (1.15V vs spec 1.20V ¬± 0.02V)\n",
    "```\n",
    "\n",
    "**2. Global Feature Importance:**\n",
    "```\n",
    "Aggregate |SHAP| across all predictions:\n",
    "1. vdd: 0.25 (most important, check voltage regulation)\n",
    "2. neighbor_yield: 0.18 (spatial correlation matters)\n",
    "3. idd: 0.12 (current matters)\n",
    "4. frequency: 0.02 (least important, consider removing)\n",
    "```\n",
    "\n",
    "**3. Feature Engineering Validation:**\n",
    "```\n",
    "Added new feature: neighbor_yield_avg (spatial correlation)\n",
    "SHAP importance: 0.18 (2nd most important feature!)\n",
    "Validation: Feature is informative, keep in production\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Performance Profiling Best Practices**\n",
    "\n",
    "**Latency Breakdown Strategy:**\n",
    "\n",
    "**Stage-Level Profiling:**\n",
    "```python\n",
    "with profiler.profile_stage(\"feature_engineering\"):\n",
    "    features = compute_features(input_data)\n",
    "    # Profiler automatically records duration\n",
    "```\n",
    "\n",
    "**Function-Level Profiling (Granular):**\n",
    "```python\n",
    "@profile_function\n",
    "def compute_spatial_correlation(x, y, radius):\n",
    "    # Detailed profiling of expensive function\n",
    "    neighbors = find_neighbors(x, y, radius)  # Profile this separately\n",
    "    return np.mean([n.yield_pct for n in neighbors])\n",
    "```\n",
    "\n",
    "**Line-Level Profiling (Debugging):**\n",
    "```python\n",
    "# Use line_profiler for hotspot identification\n",
    "@profile\n",
    "def expensive_function():\n",
    "    data = load_data()        # Line 1: 50ms\n",
    "    features = transform(data)  # Line 2: 200ms ‚Üê bottleneck!\n",
    "    return features\n",
    "```\n",
    "\n",
    "**Percentile Analysis:**\n",
    "```\n",
    "Latency distribution:\n",
    "- Mean: 45ms (misleading, doesn't show tail)\n",
    "- P50: 40ms (median, half of requests faster)\n",
    "- P95: 80ms (5% of requests slower)\n",
    "- P99: 150ms (1% of requests, tail latency)\n",
    "- P99.9: 500ms (rare, but impacts user experience)\n",
    "\n",
    "SLA: p99 <100ms\n",
    "Status: ‚ùå FAIL (150ms p99)\n",
    "Action: Investigate p95-p99 range (80-150ms), identify outliers\n",
    "```\n",
    "\n",
    "**Batch vs Single Inference:**\n",
    "```\n",
    "Single inference:\n",
    "- Latency: 30ms per request\n",
    "- Throughput: 33 QPS (1000ms / 30ms)\n",
    "- Overhead: High (model load, context switch per request)\n",
    "\n",
    "Batch inference (batch_size=32):\n",
    "- Latency: 80ms for 32 requests (2.5ms per request amortized)\n",
    "- Throughput: 400 QPS (32 requests / 80ms)\n",
    "- Speedup: 12x throughput (GPU parallelism, reduced overhead)\n",
    "- Tradeoff: Higher per-request latency (80ms vs 30ms), but much higher total throughput\n",
    "```\n",
    "\n",
    "**Memory Profiling:**\n",
    "```python\n",
    "import tracemalloc\n",
    "\n",
    "tracemalloc.start()\n",
    "\n",
    "# Run inference\n",
    "predictions = model.predict(large_batch)\n",
    "\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "print(f\"Peak memory: {peak / 1024**2:.1f} MB\")\n",
    "\n",
    "tracemalloc.stop()\n",
    "\n",
    "# Detect memory leaks:\n",
    "# If peak memory increases over time ‚Üí memory leak (investigate object retention)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Error Analysis Frameworks**\n",
    "\n",
    "**Error Classification:**\n",
    "\n",
    "**Type 1: False Positive (FP)**\n",
    "- **Definition:** Predicted positive, actually negative\n",
    "- **Example:** Predicted Fail, device passed\n",
    "- **Impact:** Unnecessary rejection (yield loss, revenue loss)\n",
    "- **Cost:** Low-moderate (can re-test, but wastes time/money)\n",
    "\n",
    "**Type 2: False Negative (FN)**\n",
    "- **Definition:** Predicted negative, actually positive\n",
    "- **Example:** Predicted Pass, device failed in customer hands\n",
    "- **Impact:** Escape to customer (warranty claims, reputation damage)\n",
    "- **Cost:** High-critical (safety issues, customer trust loss)\n",
    "\n",
    "**Asymmetric Cost:**\n",
    "```\n",
    "Fraud detection:\n",
    "- FP: Block legitimate transaction (customer annoyed)\n",
    "- FN: Miss fraud (lose $1000 per transaction)\n",
    "- Strategy: Minimize FN (lower threshold, accept more FP)\n",
    "\n",
    "Post-silicon validation:\n",
    "- FP: Reject good device (yield loss: $50)\n",
    "- FN: Ship bad device (warranty claim: $500, reputation damage: priceless)\n",
    "- Strategy: Minimize FN (strict binning, accept some FP)\n",
    "```\n",
    "\n",
    "**Root Cause Detection:**\n",
    "\n",
    "**Method 1: Feature Distribution Comparison**\n",
    "```python\n",
    "# Compare error features vs training features\n",
    "for feature in features:\n",
    "    error_mean = np.mean([e[feature] for e in errors])\n",
    "    train_mean = np.mean(X_train[:, feature])\n",
    "    shift = abs(error_mean - train_mean) / train_std\n",
    "    \n",
    "    if shift > 2.0:\n",
    "        print(f\"Root cause: {feature} shifted {shift:.1f} std devs in errors\")\n",
    "```\n",
    "\n",
    "**Method 2: Error Clustering**\n",
    "```python\n",
    "# Group errors by similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "error_features = np.array([e['features'] for e in errors])\n",
    "clustering = DBSCAN(eps=0.3, min_samples=5).fit(error_features)\n",
    "\n",
    "# Largest cluster = most common error pattern\n",
    "largest_cluster_id = Counter(clustering.labels_).most_common(1)[0][0]\n",
    "cluster_errors = error_features[clustering.labels_ == largest_cluster_id]\n",
    "\n",
    "print(f\"Cluster {largest_cluster_id}: {len(cluster_errors)} errors\")\n",
    "print(f\"Common pattern: {np.mean(cluster_errors, axis=0)}\")\n",
    "```\n",
    "\n",
    "**Method 3: Decision Tree Error Explainer**\n",
    "```python\n",
    "# Train decision tree to predict errors (interpretable)\n",
    "X_all = np.vstack([X_correct, X_errors])\n",
    "y_all = [0] * len(X_correct) + [1] * len(X_errors)\n",
    "\n",
    "error_tree = DecisionTreeClassifier(max_depth=3)\n",
    "error_tree.fit(X_all, y_all)\n",
    "\n",
    "# Interpret tree rules\n",
    "# Example: \"Errors occur when vdd < 1.18 AND neighbor_yield < 0.75\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Observability Dashboard Design**\n",
    "\n",
    "**Dashboard Hierarchy:**\n",
    "\n",
    "**Level 1: Executive Summary (1 screen)**\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  ML Model Health Overview                   ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  QPS: 5,420          Latency p99: 85ms      ‚îÇ\n",
    "‚îÇ  Accuracy: 94.2%     Error Rate: 0.3%       ‚îÇ\n",
    "‚îÇ  ‚úÖ All systems operational                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Level 2: Component Breakdown (drill-down)**\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Latency Breakdown                          ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Feature Fetch:      30ms (35%)             ‚îÇ\n",
    "‚îÇ  Model Inference:    40ms (47%)             ‚îÇ\n",
    "‚îÇ  Post-Processing:    15ms (18%)             ‚îÇ\n",
    "‚îÇ  ‚ö†Ô∏è Model Inference above target (30ms)     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Level 3: Trace/Error Drilldown (debug specific issues)**\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Recent Errors (last hour)                  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  12:34:56 - Trace ID: abc123                ‚îÇ\n",
    "‚îÇ    Error: NullPointerException              ‚îÇ\n",
    "‚îÇ    Feature: neighbor_yield_avg = None       ‚îÇ\n",
    "‚îÇ    ‚ö†Ô∏è Click for full trace                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Key Metrics to Display:**\n",
    "\n",
    "**Real-Time (1-minute window):**\n",
    "- QPS (queries per second)\n",
    "- Latency (p50, p95, p99)\n",
    "- Error rate (% of requests)\n",
    "- Cache hit rate\n",
    "\n",
    "**Hourly Aggregates:**\n",
    "- Accuracy (% correct predictions)\n",
    "- Prediction distribution (% Pass vs Fail)\n",
    "- Feature drift (KS test p-value)\n",
    "- Resource usage (CPU, memory, GPU)\n",
    "\n",
    "**Daily Aggregates:**\n",
    "- Model performance trend (accuracy over time)\n",
    "- Error analysis (top error patterns)\n",
    "- Cost metrics ($ per 1M predictions)\n",
    "\n",
    "**Integration with Existing Tools:**\n",
    "- **Grafana:** Time-series metrics, alerting\n",
    "- **Jaeger/Zipkin:** Distributed tracing visualization\n",
    "- **ELK Stack:** Log aggregation, search\n",
    "- **Custom dashboard:** ML-specific (SHAP, error analysis)\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Debugging Workflow**\n",
    "\n",
    "**Systematic Debugging Process:**\n",
    "\n",
    "**Step 1: Reproduce Issue**\n",
    "```\n",
    "‚ùå \"Model sometimes predicts wrong\"\n",
    "‚úÖ \"Model predicts wrong for wafer W0042, device D123 (trace ID: abc123)\"\n",
    "\n",
    "Gather:\n",
    "- Trace ID (full request flow)\n",
    "- Input data (features, raw values)\n",
    "- Expected output vs actual output\n",
    "- Timestamp (when did it occur?)\n",
    "```\n",
    "\n",
    "**Step 2: Isolate Stage**\n",
    "```\n",
    "Use trace to identify which stage failed:\n",
    "- Feature fetch: OK (30ms, cache hit)\n",
    "- Feature transform: OK (15ms, no nulls)\n",
    "- Model inference: ‚ö†Ô∏è ANOMALY (prediction=0.95, expected=0.30)\n",
    "- Post-processing: OK\n",
    "\n",
    "Conclusion: Issue in model inference stage\n",
    "```\n",
    "\n",
    "**Step 3: Analyze Inputs**\n",
    "```\n",
    "Check feature values:\n",
    "- vdd: 1.15V (‚ö†Ô∏è low, spec is 1.20V ¬± 0.02V)\n",
    "- idd: 105mA (OK)\n",
    "- neighbor_yield: 0.65 (‚ö†Ô∏è low, neighbors failed)\n",
    "\n",
    "Hypothesis: Low Vdd + low neighbor yield ‚Üí high fail prediction\n",
    "```\n",
    "\n",
    "**Step 4: Explain Prediction (SHAP)**\n",
    "```\n",
    "Compute SHAP values:\n",
    "- vdd: +0.40 (major contributor to fail prediction)\n",
    "- neighbor_yield: +0.25 (secondary contributor)\n",
    "- Other features: +0.10 (minor)\n",
    "\n",
    "Validation: SHAP confirms hypothesis (low Vdd drives prediction)\n",
    "```\n",
    "\n",
    "**Step 5: Root Cause**\n",
    "```\n",
    "Why is Vdd low?\n",
    "- Check upstream: STDF file has Vdd=1.15V (data is correct)\n",
    "- Check equipment: Tester #3 voltage regulator drifted\n",
    "- Check process: Recent fab change lowered target voltage\n",
    "\n",
    "Root cause: Equipment calibration drift (Tester #3)\n",
    "```\n",
    "\n",
    "**Step 6: Fix + Validate**\n",
    "```\n",
    "Fix: Recalibrate Tester #3 voltage regulator\n",
    "Validate:\n",
    "- Re-test device D123: Vdd now 1.20V ‚Üí prediction=0.30 (correct!)\n",
    "- Monitor: Check all devices tested by Tester #3 (batch revalidation)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Explainability Beyond SHAP**\n",
    "\n",
    "**LIME (Local Interpretable Model-agnostic Explanations):**\n",
    "- **Approach:** Fit linear model locally around prediction (interpretable approximation)\n",
    "- **Use case:** Explain complex models (neural networks) with simple linear features\n",
    "- **Tradeoff:** Faster than SHAP (approximate), but less theoretically sound\n",
    "\n",
    "**Counterfactual Explanations:**\n",
    "- **Question:** \"What would need to change for different prediction?\"\n",
    "- **Example:** \"If Vdd increased from 1.15V to 1.19V, prediction would change from Fail to Pass\"\n",
    "- **Use case:** Actionable feedback (what to fix to change outcome)\n",
    "\n",
    "**Anchor Explanations:**\n",
    "- **Question:** \"What features guarantee this prediction (regardless of others)?\"\n",
    "- **Example:** \"If Vdd < 1.18V, prediction is Fail with 95% confidence (even if other features change)\"\n",
    "- **Use case:** Robust rules (confidence in explanation)\n",
    "\n",
    "**Attention Mechanisms (Neural Networks):**\n",
    "- **Approach:** Visualize which input tokens model focuses on\n",
    "- **Use case:** Text/image models (which words/pixels most important)\n",
    "- **Example:** Sentiment analysis: \"The food was [great] but service was terrible\" (attention on \"great\" ‚Üí positive)\n",
    "\n",
    "**Feature Importance vs Feature Attribution:**\n",
    "- **Feature Importance:** Global (which features matter overall?)\n",
    "- **Feature Attribution:** Local (which features matter for this prediction?)\n",
    "- **Example:** \n",
    "  - Importance: Vdd is most important feature globally (25% of model decisions)\n",
    "  - Attribution: For device D123, neighbor_yield contributed most (+0.30 SHAP) even though globally less important\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Latency Optimization Strategies**\n",
    "\n",
    "**Low-Hanging Fruit (Quick Wins):**\n",
    "\n",
    "**1. Caching:**\n",
    "```python\n",
    "# Before: Query database every request (50ms)\n",
    "features = db.query(f\"SELECT * FROM features WHERE user_id={user_id}\")\n",
    "\n",
    "# After: Cache in Redis (1ms)\n",
    "features = cache.get(f\"features:{user_id}\")\n",
    "if not features:\n",
    "    features = db.query(...)\n",
    "    cache.set(f\"features:{user_id}\", features, ttl=3600)\n",
    "\n",
    "# Speedup: 50x (50ms ‚Üí 1ms for cache hits)\n",
    "```\n",
    "\n",
    "**2. Batching:**\n",
    "```python\n",
    "# Before: Process 1 request at a time (30ms each)\n",
    "for request in requests:\n",
    "    prediction = model.predict(request)  # 30ms √ó 100 = 3000ms\n",
    "\n",
    "# After: Batch 32 requests (100ms for 32)\n",
    "for batch in batched(requests, batch_size=32):\n",
    "    predictions = model.predict(np.array(batch))  # 100ms for 32 requests\n",
    "\n",
    "# Speedup: 10x (3000ms ‚Üí 300ms for 100 requests)\n",
    "```\n",
    "\n",
    "**3. Feature Selection:**\n",
    "```python\n",
    "# Before: Use all 100 features (40ms preprocessing)\n",
    "X = preprocess_all_features(input_data)\n",
    "\n",
    "# After: Use top 20 features by importance (8ms preprocessing)\n",
    "X = preprocess_selected_features(input_data, top_k=20)\n",
    "\n",
    "# Speedup: 5x (40ms ‚Üí 8ms), minimal accuracy loss (<1%)\n",
    "```\n",
    "\n",
    "**Advanced Optimizations:**\n",
    "\n",
    "**4. Model Quantization:**\n",
    "```python\n",
    "# Before: FP32 model (200MB, 30ms inference)\n",
    "model_fp32 = load_model(\"model_fp32.pkl\")\n",
    "\n",
    "# After: INT8 model (50MB, 10ms inference)\n",
    "model_int8 = quantize_model(model_fp32, dtype=\"int8\")\n",
    "\n",
    "# Speedup: 3x (30ms ‚Üí 10ms), <1% accuracy loss\n",
    "# Storage: 4x smaller (200MB ‚Üí 50MB)\n",
    "```\n",
    "\n",
    "**5. Model Pruning:**\n",
    "```python\n",
    "# Before: 100-tree Random Forest (30ms inference)\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# After: 20-tree pruned forest (8ms inference)\n",
    "model = prune_trees(model, target_trees=20, metric=\"importance\")\n",
    "\n",
    "# Speedup: 3.75x (30ms ‚Üí 8ms), <2% accuracy loss\n",
    "```\n",
    "\n",
    "**6. Approximate Algorithms:**\n",
    "```python\n",
    "# Before: Exact nearest neighbor search (O(n), 200ms)\n",
    "neighbors = [d for d in devices if distance(d, target) < radius]\n",
    "\n",
    "# After: Approximate KD-tree (O(log n), 20ms)\n",
    "neighbors = kd_tree.query_radius(target, radius)\n",
    "\n",
    "# Speedup: 10x (200ms ‚Üí 20ms), 99% recall\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10. **Error Budget and SLA Management**\n",
    "\n",
    "**Error Budget Concept:**\n",
    "```\n",
    "SLA: 99.9% uptime (3 nines)\n",
    "Error budget: 0.1% downtime per month\n",
    "= 43 minutes downtime allowed per month\n",
    "\n",
    "Current status:\n",
    "- Incidents this month: 2 outages (10 min + 15 min = 25 min)\n",
    "- Budget remaining: 43 - 25 = 18 minutes\n",
    "- ‚úÖ Still within budget (but monitor closely)\n",
    "```\n",
    "\n",
    "**Latency SLA:**\n",
    "```\n",
    "SLA: p99 latency <100ms\n",
    "Measurement window: 7 days rolling\n",
    "\n",
    "Current status:\n",
    "- Day 1-6: p99 = 85ms (‚úÖ within SLA)\n",
    "- Day 7: p99 = 120ms (‚ùå violation)\n",
    "- Action: Trigger investigation, identify root cause\n",
    "```\n",
    "\n",
    "**Accuracy SLA:**\n",
    "```\n",
    "SLA: Model accuracy >95% (measured daily)\n",
    "\n",
    "Current status:\n",
    "- Baseline accuracy: 96%\n",
    "- Current accuracy: 94% (2% degradation)\n",
    "- Alert threshold: 5% degradation\n",
    "- Status: ‚ö†Ô∏è Warning (monitor, not critical yet)\n",
    "```\n",
    "\n",
    "**Burn Rate:**\n",
    "```\n",
    "Error budget burn rate = (current error rate) / (allowed error rate)\n",
    "\n",
    "Example:\n",
    "- Allowed error rate: 0.1% (from 99.9% SLA)\n",
    "- Current error rate: 0.5%\n",
    "- Burn rate: 0.5% / 0.1% = 5x\n",
    "\n",
    "Interpretation: Burning error budget 5x faster than sustainable\n",
    "Action: Fix errors immediately (or exhaust budget in 6 days instead of 30)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 11. **ML-Specific Monitoring Metrics**\n",
    "\n",
    "**Beyond Traditional Metrics:**\n",
    "\n",
    "**Prediction Distribution:**\n",
    "```python\n",
    "# Monitor: Are predictions shifting over time?\n",
    "pred_distribution_week1 = [0.8, 0.2]  # [Pass: 80%, Fail: 20%]\n",
    "pred_distribution_week2 = [0.6, 0.4]  # [Pass: 60%, Fail: 40%]\n",
    "\n",
    "# Shift: Fail predictions doubled (20% ‚Üí 40%)\n",
    "# Possible causes:\n",
    "# 1. Concept drift (real change in data)\n",
    "# 2. Data quality issue (bad features)\n",
    "# 3. Upstream pipeline bug\n",
    "```\n",
    "\n",
    "**Confidence Distribution:**\n",
    "```python\n",
    "# Monitor: Is model becoming less confident?\n",
    "confidence_week1 = [0.95, 0.92, 0.88, ...]  # High confidence\n",
    "confidence_week2 = [0.65, 0.58, 0.72, ...]  # Low confidence\n",
    "\n",
    "# Shift: Average confidence dropped (0.92 ‚Üí 0.65)\n",
    "# Interpretation: Model uncertain (new data patterns)\n",
    "# Action: Collect labels, retrain on recent data\n",
    "```\n",
    "\n",
    "**Feature Null Rate:**\n",
    "```python\n",
    "# Monitor: Are features missing more often?\n",
    "null_rate_week1 = {'vdd': 0.1%, 'idd': 0.2%, ...}\n",
    "null_rate_week2 = {'vdd': 5.0%, 'idd': 0.2%, ...}\n",
    "\n",
    "# Shift: Vdd null rate spiked (0.1% ‚Üí 5.0%)\n",
    "# Root cause: Upstream STDF pipeline issue\n",
    "# Action: Alert data engineering team\n",
    "```\n",
    "\n",
    "**Inference Volume:**\n",
    "```python\n",
    "# Monitor: Are we getting expected traffic?\n",
    "expected_qps = 1000\n",
    "actual_qps_week1 = 980  # ‚úÖ Normal variance\n",
    "actual_qps_week2 = 200  # ‚ùå 80% drop!\n",
    "\n",
    "# Shift: Traffic dropped 80%\n",
    "# Possible causes:\n",
    "# 1. Upstream service down (requests not reaching model)\n",
    "# 2. Client bug (not calling API)\n",
    "# 3. Business change (fab production paused)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 12. **Integration with Existing Tools**\n",
    "\n",
    "**OpenTelemetry (Unified Observability):**\n",
    "```python\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.exporter.jaeger import JaegerExporter\n",
    "\n",
    "# Configure tracing\n",
    "trace.set_tracer_provider(TracerProvider())\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "# Instrument ML pipeline\n",
    "with tracer.start_as_current_span(\"ml_inference\") as span:\n",
    "    span.set_attribute(\"model.version\", \"v2.3\")\n",
    "    span.set_attribute(\"input.size\", len(features))\n",
    "    \n",
    "    prediction = model.predict(features)\n",
    "    \n",
    "    span.set_attribute(\"prediction.value\", float(prediction))\n",
    "    span.set_attribute(\"prediction.confidence\", float(confidence))\n",
    "```\n",
    "\n",
    "**Prometheus (Metrics):**\n",
    "```python\n",
    "from prometheus_client import Counter, Histogram\n",
    "\n",
    "# Define metrics\n",
    "prediction_counter = Counter('ml_predictions_total', 'Total predictions', ['model', 'outcome'])\n",
    "latency_histogram = Histogram('ml_inference_latency_seconds', 'Inference latency')\n",
    "\n",
    "# Instrument code\n",
    "with latency_histogram.time():\n",
    "    prediction = model.predict(features)\n",
    "\n",
    "prediction_counter.labels(model='yield_model', outcome='pass').inc()\n",
    "```\n",
    "\n",
    "**MLflow (Experiment Tracking + Model Registry):**\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "# Log experiment\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_metric(\"accuracy\", 0.95)\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    # Log SHAP explanations\n",
    "    mlflow.log_artifact(\"shap_summary.png\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 13. **Production Debugging War Stories**\n",
    "\n",
    "**Case Study 1: The Mysterious Latency Spike**\n",
    "```\n",
    "Symptom: p99 latency spiked from 50ms ‚Üí 500ms\n",
    "Initial hypothesis: Database slow (checked: DB fine)\n",
    "Trace analysis: Feature fetch stage 5ms ‚Üí 400ms\n",
    "Root cause: Cache eviction (Redis memory full)\n",
    "Fix: Increase Redis memory + add TTL monitoring\n",
    "Lesson: Monitor cache metrics (hit rate, memory usage)\n",
    "```\n",
    "\n",
    "**Case Study 2: The Silent Model Degradation**\n",
    "```\n",
    "Symptom: Customer complaints (more false positives)\n",
    "Initial hypothesis: Model bug (checked: model unchanged)\n",
    "Error analysis: FP rate increased 2% ‚Üí 8% (4x!)\n",
    "Feature analysis: avg_transaction_30d shifted (distribution drift)\n",
    "Root cause: Upstream feature pipeline bug (wrong date range)\n",
    "Fix: Fix feature computation, retrain model\n",
    "Lesson: Monitor feature distributions, not just accuracy\n",
    "```\n",
    "\n",
    "**Case Study 3: The Misleading SHAP Values**\n",
    "```\n",
    "Symptom: SHAP shows feature X important, but removing it improves accuracy\n",
    "Initial hypothesis: SHAP bug (checked: SHAP correct)\n",
    "Root cause: Feature X correlated with label during training (data leakage)\n",
    "Fix: Remove feature X, retrain without leakage\n",
    "Lesson: SHAP shows what model uses, not what's causal (correlation ‚â† causation)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 14. **Testing Observability Systems**\n",
    "\n",
    "**Unit Tests:**\n",
    "```python\n",
    "def test_tracer_span_creation():\n",
    "    tracer = MLTracer()\n",
    "    trace_id = \"test_001\"\n",
    "    \n",
    "    span_id = tracer.start_trace(trace_id, \"test_op\")\n",
    "    assert span_id in tracer.active_spans\n",
    "    \n",
    "    tracer.end_span(span_id)\n",
    "    assert span_id not in tracer.active_spans\n",
    "\n",
    "def test_shap_explainer_decomposition():\n",
    "    explainer = SHAPExplainer(model, X_background)\n",
    "    shap_values = explainer.explain_instance(X_test[0])\n",
    "    \n",
    "    # Verify: base + sum(SHAP) ‚âà prediction\n",
    "    reconstructed = explainer.base_value + sum(shap_values.values())\n",
    "    actual_pred = model.predict_proba(X_test[0])[0, 1]\n",
    "    \n",
    "    assert abs(reconstructed - actual_pred) < 0.01\n",
    "```\n",
    "\n",
    "**Integration Tests:**\n",
    "```python\n",
    "def test_end_to_end_observability():\n",
    "    # Simulate full prediction with observability\n",
    "    tracer = MLTracer()\n",
    "    profiler = PerformanceProfiler()\n",
    "    explainer = SHAPExplainer(model, X_train)\n",
    "    \n",
    "    trace_id = \"integration_test\"\n",
    "    tracer.start_trace(trace_id, \"prediction\")\n",
    "    \n",
    "    with profiler.profile_stage(\"inference\"):\n",
    "        prediction = model.predict(X_test[0])\n",
    "    \n",
    "    shap_values = explainer.explain_instance(X_test[0])\n",
    "    \n",
    "    # Verify all components captured data\n",
    "    assert len(tracer.get_trace(trace_id)) > 0\n",
    "    assert \"inference\" in profiler.stage_timings\n",
    "    assert len(shap_values) == X_test.shape[1]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 15. **Key Takeaways Summary**\n",
    "\n",
    "‚úÖ **Distributed tracing enables end-to-end debugging** - trace request from input ‚Üí feature ‚Üí model ‚Üí output (correlate errors across services)\n",
    "\n",
    "‚úÖ **SHAP values provide exact feature attribution** - prediction = base + Œ£(SHAP) (explain individual predictions, debug model behavior)\n",
    "\n",
    "‚úÖ **Performance profiling identifies bottlenecks** - measure latency breakdown by stage (optimize highest-impact operations first)\n",
    "\n",
    "‚úÖ **Error analysis reveals systematic patterns** - cluster errors, compare distributions (prioritize fixes, prevent recurrence)\n",
    "\n",
    "‚úÖ **Percentile metrics matter more than averages** - p99 latency captures tail behavior (impacts user experience, SLA compliance)\n",
    "\n",
    "‚úÖ **Observability requires instrumentation** - logs, metrics, traces, SHAP, drift detection (unified view of ML system health)\n",
    "\n",
    "‚úÖ **Root cause detection requires data** - compare error features vs training distribution (identify shifts, data quality issues)\n",
    "\n",
    "‚úÖ **Explainability builds trust** - SHAP waterfall plots, confidence intervals (regulatory compliance, stakeholder transparency)\n",
    "\n",
    "‚úÖ **Post-silicon observability is critical** - STDF pipeline tracing, spatial error clustering, equipment correlation (faster debug, higher yield)\n",
    "\n",
    "‚úÖ **Production checklist:** Distributed tracing, SHAP on-demand, latency profiling, error logging, drift monitoring, dashboards, alerts\n",
    "\n",
    "---\n",
    "\n",
    "### 16. **Production Readiness Checklist**\n",
    "\n",
    "**Distributed Tracing:**\n",
    "- [ ] Trace ID generation (UUID per request)\n",
    "- [ ] Trace ID propagation (HTTP headers, message queues)\n",
    "- [ ] Span instrumentation (all pipeline stages)\n",
    "- [ ] Metadata attachment (model version, features, cache status)\n",
    "- [ ] Trace export (Jaeger, Zipkin, or OpenTelemetry)\n",
    "- [ ] Trace sampling (1-5% in production, 100% for debugging)\n",
    "\n",
    "**Model Explainability:**\n",
    "- [ ] SHAP explainer deployed (on-demand or pre-computed)\n",
    "- [ ] Explanation API (<50ms p99 latency for online queries)\n",
    "- [ ] Waterfall plot generation (visualization for stakeholders)\n",
    "- [ ] Global feature importance (aggregated SHAP across dataset)\n",
    "- [ ] Explanation versioning (link to model version)\n",
    "- [ ] Explanation storage (7-day retention for audit)\n",
    "\n",
    "**Performance Profiling:**\n",
    "- [ ] Stage-level timing (feature fetch, inference, post-processing)\n",
    "- [ ] Percentile tracking (p50, p95, p99 per stage)\n",
    "- [ ] Continuous profiling (1% sampling, low overhead)\n",
    "- [ ] Profiling dashboard (real-time latency breakdown)\n",
    "- [ ] Bottleneck alerting (single stage >40% of total time)\n",
    "- [ ] Resource monitoring (CPU, memory, GPU utilization)\n",
    "\n",
    "**Error Analysis:**\n",
    "- [ ] Error logging (100% of prediction errors)\n",
    "- [ ] Error clustering (group similar failures)\n",
    "- [ ] Root cause detection (feature distribution comparison)\n",
    "- [ ] Error analysis dashboard (visualize patterns)\n",
    "- [ ] Severity classification (FP vs FN, business impact)\n",
    "- [ ] Error trend monitoring (are errors increasing?)\n",
    "\n",
    "**Observability Dashboard:**\n",
    "- [ ] Real-time metrics (QPS, latency, error rate, accuracy)\n",
    "- [ ] Trace visualization (Gantt chart, span hierarchy)\n",
    "- [ ] SHAP explanation viewer (on-demand queries)\n",
    "- [ ] Error analysis view (clusters, root causes)\n",
    "- [ ] Alert integration (PagerDuty, Slack, email)\n",
    "- [ ] Mobile access (debug from anywhere)\n",
    "\n",
    "---\n",
    "\n",
    "### 17. **Next Steps in Learning**\n",
    "\n",
    "**Notebook 131: Containerization for ML**\n",
    "- Docker for model serving (reproducible environments, version control)\n",
    "- Multi-stage builds (optimize image size)\n",
    "- Container orchestration basics (Kubernetes, ECS)\n",
    "\n",
    "**Notebook 132: Service Mesh for ML**\n",
    "- Istio for traffic management (A/B testing, canary deployment)\n",
    "- Distributed tracing integration (automatic span generation)\n",
    "- Observability out-of-the-box (metrics, logs, traces)\n",
    "\n",
    "**Notebook 133: CI/CD for ML**\n",
    "- Automated testing (unit tests, integration tests, model validation)\n",
    "- Continuous training pipelines (trigger retraining on drift)\n",
    "- Deployment automation (blue-green, canary, rollback)\n",
    "\n",
    "**Beyond MLOps:**\n",
    "- **AIOps:** AI for observability (anomaly detection, root cause analysis, auto-remediation)\n",
    "- **Chaos Engineering:** Test observability under failure conditions (random pod kills, network delays)\n",
    "- **Edge ML:** Observability for edge devices (limited bandwidth, offline operation)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've mastered ML observability and debugging systems.** üéâ\n",
    "\n",
    "You now understand:\n",
    "- ‚úÖ Distributed tracing (track request flow end-to-end)\n",
    "- ‚úÖ Model explainability (SHAP values, waterfall plots)\n",
    "- ‚úÖ Performance profiling (latency breakdown, bottleneck identification)\n",
    "- ‚úÖ Error analysis (clustering, root cause detection)\n",
    "- ‚úÖ Production observability (dashboards, alerts, debugging workflows)\n",
    "\n",
    "**You're now equipped to debug production ML systems with confidence and speed.** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff2abbc",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### When to Use ML Observability\n",
    "- **Production models**: Any model serving predictions in production (100+ requests/day)\n",
    "- **Critical decisions**: High-cost errors (yield prediction, binning decisions worth $M annually)\n",
    "- **Data drift monitoring**: Input distributions change over time (new product introductions, process changes)\n",
    "- **Model performance tracking**: Detect accuracy degradation before business impact\n",
    "- **Debugging**: Investigate prediction errors, outliers, unexpected behavior\n",
    "\n",
    "### Limitations\n",
    "- **Metric selection**: Too many metrics = alert fatigue, too few = miss issues (balance coverage vs. noise)\n",
    "- **Lag in ground truth**: Can't compute accuracy until labels available (weeks/months for field failures)\n",
    "- **Computational overhead**: Logging all predictions + features adds latency (5-10ms typical)\n",
    "- **Storage costs**: Retaining prediction logs for analysis (GB-TB scale for high-volume services)\n",
    "\n",
    "### Alternatives\n",
    "- **Manual spot checks**: Periodic manual review of predictions (doesn't scale, misses systemic issues)\n",
    "- **A/B testing**: Continuous comparison to baseline model (good for improvement validation)\n",
    "- **Offline evaluation**: Batch model testing on held-out sets (misses production-specific issues)\n",
    "- **Unit tests**: Test model code correctness (doesn't catch data distribution shifts)\n",
    "\n",
    "### Best Practices\n",
    "- **Multi-layer monitoring**: Infrastructure (latency, errors) + model (accuracy, drift) + business (revenue impact)\n",
    "- **Statistical alerts**: 3-sigma rules, sequential testing (avoid false alarms from noise)\n",
    "- **Logging strategy**: Sample 1-10% of predictions for detailed analysis, aggregate metrics for all\n",
    "- **Alerting hierarchy**: P0 (immediate page) for >10% accuracy drop, P1 (ticket) for drift warnings\n",
    "- **Runbooks**: Document response procedures for each alert type (who investigates? escalation path?)\n",
    "- **Feedback loops**: Route alerts to data scientists, enable quick model retraining/rollback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f11b2b1",
   "metadata": {},
   "source": [
    "## üìä Diagnostic Checks Summary\n",
    "\n",
    "### Implementation Checklist\n",
    "‚úÖ **Logging Infrastructure**\n",
    "- Structured logging: JSON format with timestamp, model_version, input_features, prediction, latency\n",
    "- Sampling strategy: 100% for errors, 10% for normal predictions, 1% for detailed feature logging\n",
    "- Log aggregation: ELK stack (Elasticsearch, Logstash, Kibana) or CloudWatch Logs\n",
    "- Retention policy: 30 days for detailed logs, 1 year for aggregated metrics\n",
    "\n",
    "‚úÖ **Metrics Tracking**\n",
    "- Prediction metrics: Distribution (mean, p50, p95, p99), outlier rates (>3œÉ)\n",
    "- Performance metrics: Latency (p50, p95, p99), throughput (requests/sec), error rate\n",
    "- Data drift: KL divergence, KS test p-values for feature distributions\n",
    "- Model accuracy: Online metrics (when labels available), proxy metrics (confidence scores)\n",
    "\n",
    "‚úÖ **Alerting System**\n",
    "- Statistical alerts: 3-sigma rules for metric deviations, sequential probability ratio test (SPRT)\n",
    "- Thresholds: P0 (>10% accuracy drop, >50% latency increase), P1 (drift p<0.01, error rate >5%)\n",
    "- Alert routing: PagerDuty for P0, Slack/email for P1, dashboard for P2\n",
    "- Deduplication: Suppress duplicate alerts within 1hr window\n",
    "\n",
    "‚úÖ **Debugging Tools**\n",
    "- Prediction explainability: SHAP values logged for sampled predictions\n",
    "- Error analysis: Cluster errors by input characteristics, identify failure modes\n",
    "- A/B testing: Shadow mode for canary deployments, traffic splitting 90/10\n",
    "- Rollback mechanism: Automated rollback if accuracy drops >15% for 1hr\n",
    "\n",
    "### Quality Metrics\n",
    "- **Logging overhead**: <5ms p95 latency increase, <5% CPU overhead\n",
    "- **Alert accuracy**: <10% false positive rate, <1% false negative rate\n",
    "- **Mean time to detect (MTTD)**: <15min for critical issues\n",
    "- **Mean time to recover (MTTR)**: <2hr from detection to resolution\n",
    "\n",
    "### Post-Silicon Validation Applications\n",
    "**1. Yield Prediction Model Observability**\n",
    "- Metrics: Prediction distribution (yield% mean, std), feature drift (test parameter distributions)\n",
    "- Alerts: P0 if predicted yield <70% for >100 wafers (investigate immediately), P1 if KS test p<0.01 for Vdd distribution\n",
    "- Debugging: SHAP values identify which test parameters driving low yield predictions\n",
    "- Business value: Detect model degradation 12-24hr before manual review, $4M-$8M/year faster response\n",
    "\n",
    "**2. Binning Model Monitoring (Device Classification)**\n",
    "- Metrics: Bin distribution (% premium, standard, low-power), confidence scores, misclassification rate (when ground truth available)\n",
    "- Alerts: P0 if >30% bins with confidence <80% (model uncertainty spike), P1 if bin mix shifts >15% vs. historical\n",
    "- Root cause: Correlate bin shifts with lot attributes (fab, product, test site)\n",
    "- Business value: Prevent revenue loss from incorrect binning ($10M-$25M/year at high volumes)\n",
    "\n",
    "**3. Test Time Prediction Model Debugging**\n",
    "- Metrics: Prediction error (MAE, RMSE), residual distribution, outlier rate (>2x predicted time)\n",
    "- Alerts: P1 if MAE increases >20% vs. baseline (model drift or test program changes)\n",
    "- Debugging: Stratify errors by product/site, identify systematic biases\n",
    "- Business value: Accurate test time forecasts enable capacity planning, $3M-$8M/year optimized scheduling\n",
    "\n",
    "### Business ROI Estimation\n",
    "\n",
    "**Scenario 1: Medium-Volume Semiconductor Fab (100K wafers/year, 5 production models)**\n",
    "- Model degradation detection: Catch issues 24hr earlier = **$2.5M/year** reduced scrap/rework\n",
    "- Data drift alerts: Identify test program changes breaking assumptions = **$1.5M/year** avoided bad predictions\n",
    "- Automated rollback: 2hr MTTR vs. 24hr manual = **$3M/year** reduced downtime impact\n",
    "- **Total ROI: $7M/year** (cost: $200K observability platform + $150K team = $6.65M net)\n",
    "\n",
    "**Scenario 2: High-Volume Automotive Semiconductor (500K wafers/year, 20+ models)**\n",
    "- Comprehensive monitoring: 15min MTTD for all critical models = **$15M/year** faster incident response\n",
    "- A/B testing infrastructure: Safe canary deployments = **$8M/year** avoided bad model releases\n",
    "- Explainability logging: SHAP values for error debugging = **$5M/year** faster root cause (4hr ‚Üí 30min)\n",
    "- **Total ROI: $28M/year** (cost: $1M enterprise observability + $500K team = $26.5M net)\n",
    "\n",
    "**Scenario 3: Advanced Node R&D Fab (<10K wafers/year)**\n",
    "- Experimental model monitoring: Track A/B tests across process experiments = **$3M/year** research velocity\n",
    "- Feature importance tracking: Identify which process parameters drive predictions = **$2.5M/year** physics insights\n",
    "- Model versioning + rollback: Quick recovery from bad model updates = **$1.5M/year** avoided experiment delays\n",
    "- **Total ROI: $7M/year** (cost: $150K observability tools + $100K setup = $6.75M net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc199b5",
   "metadata": {},
   "source": [
    "## üìà Progress Update\n",
    "\n",
    "**Notebook 130: ML Observability & Debugging** expanded from 11 ‚Üí 15 cells ‚úÖ\n",
    "\n",
    "**Session progress: 10 notebooks completed**\n",
    "- 12-cell: 129, 133, 162, 163, 164  \n",
    "- 11-cell: 111, 112, 116, 130\n",
    "\n",
    "Next: 138, 151 (11-cell notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dc65b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Mastery Achievement\n",
    "\n",
    "**You now have production-grade expertise in:**\n",
    "- ‚úÖ Implementing structured logging with sampling strategies for production ML models\n",
    "- ‚úÖ Tracking prediction, performance, and data drift metrics with statistical alerting\n",
    "- ‚úÖ Building observability dashboards (ELK stack, CloudWatch) with P0/P1/P2 alert hierarchies\n",
    "- ‚úÖ Debugging model errors with SHAP explainability and error clustering analysis\n",
    "- ‚úÖ Applying ML observability to yield prediction, binning models, and test time forecasting\n",
    "\n",
    "**Next Steps:**\n",
    "- **Advanced Drift Detection**: Multivariate drift (multiple features jointly), context-aware alerting\n",
    "- **Causal Debugging**: Root cause analysis linking input changes to prediction degradation\n",
    "- **Automated Remediation**: Self-healing models that retrain on drift detection"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
