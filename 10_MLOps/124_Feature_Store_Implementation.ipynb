{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99245baf",
   "metadata": {},
   "source": [
    "# 124: Feature Store Implementation\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** feature stores and their role in ML production systems\n",
    "- **Implement** offline and online feature serving patterns\n",
    "- **Build** a feature store using Feast framework\n",
    "- **Apply** feature stores to post-silicon validation workflows\n",
    "- **Evaluate** feature consistency across training and serving\n",
    "- **Design** feature engineering pipelines for production scale\n",
    "\n",
    "## üìö What is a Feature Store?\n",
    "\n",
    "A **feature store** is a centralized repository for storing, managing, and serving ML features. It solves the critical problem of **training-serving skew** by ensuring features computed during training are identical to features used during inference. Feature stores provide versioning, point-in-time correctness, and scalable serving for both batch and real-time predictions.\n",
    "\n",
    "**Why Feature Stores?**\n",
    "- ‚úÖ **Eliminates training-serving skew** (features computed identically)\n",
    "- ‚úÖ **Reduces feature engineering duplication** (define once, use everywhere)\n",
    "- ‚úÖ **Ensures point-in-time correctness** (no data leakage from future)\n",
    "- ‚úÖ **Enables feature reuse** (share features across models and teams)\n",
    "- ‚úÖ **Provides feature versioning** (reproducible experiments)\n",
    "- ‚úÖ **Supports low-latency serving** (pre-computed features for real-time)\n",
    "\n",
    "**Core Components:**\n",
    "1. **Offline Store**: Historical features for model training (data warehouse, data lake)\n",
    "2. **Online Store**: Low-latency features for real-time inference (Redis, DynamoDB)\n",
    "3. **Feature Registry**: Metadata catalog (definitions, schemas, lineage)\n",
    "4. **Feature Computation Engine**: Transforms raw data into features\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Centralized Device Feature Store**\n",
    "- Input: Raw STDF test data (voltage, current, frequency, temperature)\n",
    "- Features: Aggregated statistics (mean/std/percentile per device, wafer, lot)\n",
    "- Output: Consistent features for yield prediction, binning, anomaly detection\n",
    "- Value: Single source of truth for device characterization, eliminates duplicate feature engineering across 10+ models\n",
    "\n",
    "**Real-Time Test Binning**\n",
    "- Input: Device test parameters (Vdd=1.23V, Idd=245mA, Freq=2.4GHz)\n",
    "- Features: Pre-computed percentile rankings, Z-scores vs lot distribution\n",
    "- Output: Bin assignment in <50ms (PASS/FAIL/BIN1/BIN2)\n",
    "- Value: Low-latency online serving enables inline binning during test, reduces retest cycles\n",
    "\n",
    "**Wafer-Level Feature Engineering**\n",
    "- Input: Die-level test results (die_x, die_y, parametric values)\n",
    "- Features: Spatial aggregations (neighboring die statistics, radial patterns, edge effects)\n",
    "- Output: Wafer-aware features for spatial correlation models\n",
    "- Value: Point-in-time correctness ensures training features match production wafer maps\n",
    "\n",
    "**Feature Versioning for Experiments**\n",
    "- Input: Multiple feature engineering approaches (raw params, polynomial, interactions)\n",
    "- Features: Version-controlled feature sets (v1: raw, v2: +polynomials, v3: +interactions)\n",
    "- Output: Reproducible experiments with consistent feature definitions\n",
    "- Value: Track which feature set version achieved best yield prediction accuracy\n",
    "\n",
    "## üîÑ Feature Store Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    subgraph \"Feature Definition\"\n",
    "        A[Raw Data Sources] --> B[Feature Engineering Logic]\n",
    "        B --> C[Feature Registry]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Offline Flow - Training\"\n",
    "        C --> D[Historical Feature Store]\n",
    "        D --> E[Point-in-Time Join]\n",
    "        E --> F[Training Dataset]\n",
    "        F --> G[Model Training]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Online Flow - Serving\"\n",
    "        C --> H[Online Feature Store]\n",
    "        I[Real-Time Request] --> H\n",
    "        H --> J[Feature Vector]\n",
    "        J --> K[Model Inference]\n",
    "        K --> L[Prediction]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Feature Computation\"\n",
    "        M[Batch Pipeline] --> D\n",
    "        N[Stream Pipeline] --> H\n",
    "    end\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style D fill:#fff5e1\n",
    "    style H fill:#ffe1e1\n",
    "    style G fill:#e1ffe1\n",
    "    style L fill:#e1ffe1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **121_MLOps_Fundamentals.ipynb** - MLOps lifecycle, experiment tracking\n",
    "- **122_MLflow_Complete_Guide.ipynb** - Model versioning and deployment\n",
    "- **123_Model_Monitoring_Drift_Detection.ipynb** - Feature drift detection\n",
    "\n",
    "**Next Steps:**\n",
    "- **125_ML_Testing_Validation.ipynb** - Unit/integration testing for ML\n",
    "- **126_CI_CD_for_ML.ipynb** - Automated ML pipelines\n",
    "- **127_Model_Serving_Patterns.ipynb** - Deployment architectures\n",
    "\n",
    "---\n",
    "\n",
    "Let's build production feature stores! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc51c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install feature store libraries\n",
    "# !pip install feast pandas numpy scikit-learn pyyaml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Feature store libraries loaded\")\n",
    "print(\"Focus: Centralized feature management, training/serving consistency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a93ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple feature store implementation (conceptual)\n",
    "class SimpleFeatureStore:\n",
    "    \"\"\"\n",
    "    Minimal feature store demonstrating core concepts.\n",
    "    \n",
    "    Components:\n",
    "    - Feature registry (metadata)\n",
    "    - Offline store (historical features)\n",
    "    - Online store (real-time features)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_registry = {}  # Metadata catalog\n",
    "        self.offline_store = {}     # Historical features (pandas DataFrames)\n",
    "        self.online_store = {}      # Latest features (key-value)\n",
    "    \n",
    "    def register_feature(self, name, description, feature_type, computation_fn):\n",
    "        \"\"\"Register feature definition in catalog.\"\"\"\n",
    "        self.feature_registry[name] = {\n",
    "            'description': description,\n",
    "            'type': feature_type,\n",
    "            'computation': computation_fn,\n",
    "            'created_at': datetime.now()\n",
    "        }\n",
    "        print(f\"‚úÖ Registered feature: {name}\")\n",
    "    \n",
    "    def materialize_offline(self, feature_name, data):\n",
    "        \"\"\"\n",
    "        Compute and store historical features for training.\n",
    "        \n",
    "        Args:\n",
    "            feature_name: Feature to materialize\n",
    "            data: Raw data (DataFrame with 'timestamp' column)\n",
    "        \"\"\"\n",
    "        if feature_name not in self.feature_registry:\n",
    "            raise ValueError(f\"Feature {feature_name} not registered\")\n",
    "        \n",
    "        computation_fn = self.feature_registry[feature_name]['computation']\n",
    "        features = computation_fn(data)\n",
    "        \n",
    "        # Store with timestamp for point-in-time correctness\n",
    "        self.offline_store[feature_name] = features\n",
    "        print(f\"‚úÖ Materialized {len(features)} offline records for {feature_name}\")\n",
    "    \n",
    "    def materialize_online(self, feature_name, entity_key, features):\n",
    "        \"\"\"\n",
    "        Update online store for low-latency serving.\n",
    "        \n",
    "        Args:\n",
    "            feature_name: Feature to update\n",
    "            entity_key: Entity identifier (device_id, customer_id, etc.)\n",
    "            features: Feature values\n",
    "        \"\"\"\n",
    "        if feature_name not in self.online_store:\n",
    "            self.online_store[feature_name] = {}\n",
    "        \n",
    "        self.online_store[feature_name][entity_key] = features\n",
    "        print(f\"‚úÖ Updated online features for {entity_key}\")\n",
    "    \n",
    "    def get_historical_features(self, feature_names, entity_df, event_timestamp_col='timestamp'):\n",
    "        \"\"\"\n",
    "        Get point-in-time correct features for training.\n",
    "        \n",
    "        Args:\n",
    "            feature_names: List of features to retrieve\n",
    "            entity_df: DataFrame with entity IDs and timestamps\n",
    "            event_timestamp_col: Timestamp column name\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with features joined to entities\n",
    "        \"\"\"\n",
    "        result = entity_df.copy()\n",
    "        \n",
    "        for feature_name in feature_names:\n",
    "            if feature_name not in self.offline_store:\n",
    "                print(f\"‚ö†Ô∏è Feature {feature_name} not found in offline store\")\n",
    "                continue\n",
    "            \n",
    "            # Point-in-time join: only use features <= event timestamp\n",
    "            feature_data = self.offline_store[feature_name]\n",
    "            \n",
    "            # Simplified join (production would use more sophisticated logic)\n",
    "            result = result.merge(\n",
    "                feature_data, \n",
    "                on='device_id', \n",
    "                how='left', \n",
    "                suffixes=('', f'_{feature_name}')\n",
    "            )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_online_features(self, feature_names, entity_keys):\n",
    "        \"\"\"\n",
    "        Get latest features for real-time inference.\n",
    "        \n",
    "        Args:\n",
    "            feature_names: List of features to retrieve\n",
    "            entity_keys: List of entity IDs\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of features per entity\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for entity_key in entity_keys:\n",
    "            entity_features = {}\n",
    "            \n",
    "            for feature_name in feature_names:\n",
    "                if feature_name in self.online_store and entity_key in self.online_store[feature_name]:\n",
    "                    entity_features[feature_name] = self.online_store[feature_name][entity_key]\n",
    "                else:\n",
    "                    entity_features[feature_name] = None\n",
    "            \n",
    "            results[entity_key] = entity_features\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize feature store\n",
    "fs = SimpleFeatureStore()\n",
    "print(\"Simple feature store initialized\")\n",
    "print(f\"Components: Registry, Offline Store, Online Store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87450c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic STDF test data\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_stdf_data(n_devices=1000):\n",
    "    \"\"\"Generate synthetic device test data.\"\"\"\n",
    "    \n",
    "    # Base timestamp\n",
    "    base_time = datetime.now() - timedelta(days=30)\n",
    "    \n",
    "    # Generate device IDs and timestamps\n",
    "    device_ids = [f\"DEV{i:05d}\" for i in range(n_devices)]\n",
    "    timestamps = [base_time + timedelta(hours=i*0.5) for i in range(n_devices)]\n",
    "    \n",
    "    # Parametric test measurements\n",
    "    vdd = np.random.normal(1.2, 0.02, n_devices)      # Voltage (V)\n",
    "    idd = np.random.normal(250, 15, n_devices)         # Current (mA)\n",
    "    freq = np.random.normal(2.5, 0.1, n_devices)       # Frequency (GHz)\n",
    "    temp = np.random.normal(85, 5, n_devices)          # Temperature (¬∞C)\n",
    "    \n",
    "    # Yield labels (based on parametric limits)\n",
    "    yield_pass = (\n",
    "        (vdd >= 1.15) & (vdd <= 1.25) &\n",
    "        (idd <= 280) &\n",
    "        (freq >= 2.3) &\n",
    "        (temp <= 100)\n",
    "    ).astype(int)\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'device_id': device_ids,\n",
    "        'timestamp': timestamps,\n",
    "        'vdd': vdd,\n",
    "        'idd': idd,\n",
    "        'freq': freq,\n",
    "        'temp': temp,\n",
    "        'yield': yield_pass\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate data\n",
    "raw_data = generate_stdf_data(n_devices=1000)\n",
    "\n",
    "print(\"‚úÖ Generated synthetic STDF data\")\n",
    "print(f\"\\nDataset shape: {raw_data.shape}\")\n",
    "print(f\"Date range: {raw_data['timestamp'].min()} to {raw_data['timestamp'].max()}\")\n",
    "print(f\"Yield rate: {raw_data['yield'].mean():.1%}\")\n",
    "print(\"\\nFirst 5 records:\")\n",
    "print(raw_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669f8623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature transformations\n",
    "\n",
    "def compute_basic_features(data):\n",
    "    \"\"\"Basic parametric features (raw values).\"\"\"\n",
    "    return data[['device_id', 'timestamp', 'vdd', 'idd', 'freq', 'temp']].copy()\n",
    "\n",
    "def compute_power_features(data):\n",
    "    \"\"\"Derived power features.\"\"\"\n",
    "    result = data[['device_id', 'timestamp']].copy()\n",
    "    result['power'] = data['vdd'] * data['idd']  # Power (mW)\n",
    "    result['power_efficiency'] = data['freq'] / result['power']  # GHz/mW\n",
    "    return result\n",
    "\n",
    "def compute_zscore_features(data):\n",
    "    \"\"\"Normalized Z-scores vs population.\"\"\"\n",
    "    result = data[['device_id', 'timestamp']].copy()\n",
    "    result['vdd_zscore'] = (data['vdd'] - data['vdd'].mean()) / data['vdd'].std()\n",
    "    result['idd_zscore'] = (data['idd'] - data['idd'].mean()) / data['idd'].std()\n",
    "    result['freq_zscore'] = (data['freq'] - data['freq'].mean()) / data['freq'].std()\n",
    "    return result\n",
    "\n",
    "def compute_aggregate_features(data):\n",
    "    \"\"\"Rolling aggregate features (last 100 devices).\"\"\"\n",
    "    result = data[['device_id', 'timestamp']].copy()\n",
    "    result['vdd_rolling_mean'] = data['vdd'].rolling(window=100, min_periods=1).mean()\n",
    "    result['idd_rolling_std'] = data['idd'].rolling(window=100, min_periods=1).std()\n",
    "    result['freq_rolling_median'] = data['freq'].rolling(window=100, min_periods=1).median()\n",
    "    return result\n",
    "\n",
    "# Register features in store\n",
    "fs.register_feature(\n",
    "    name='basic_params',\n",
    "    description='Raw parametric measurements from STDF',\n",
    "    feature_type='batch',\n",
    "    computation_fn=compute_basic_features\n",
    ")\n",
    "\n",
    "fs.register_feature(\n",
    "    name='power_metrics',\n",
    "    description='Derived power consumption features',\n",
    "    feature_type='batch',\n",
    "    computation_fn=compute_power_features\n",
    ")\n",
    "\n",
    "fs.register_feature(\n",
    "    name='zscore_normalized',\n",
    "    description='Population-normalized Z-scores',\n",
    "    feature_type='batch',\n",
    "    computation_fn=compute_zscore_features\n",
    ")\n",
    "\n",
    "fs.register_feature(\n",
    "    name='rolling_aggregates',\n",
    "    description='Rolling window statistics',\n",
    "    feature_type='streaming',\n",
    "    computation_fn=compute_aggregate_features\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Registered 4 feature definitions in feature store\")\n",
    "print(f\"Feature registry size: {len(fs.feature_registry)}\")\n",
    "print(\"\\nRegistered features:\")\n",
    "for name, meta in fs.feature_registry.items():\n",
    "    print(f\"  - {name}: {meta['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9bc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize features for offline training\n",
    "\n",
    "# Compute all features\n",
    "fs.materialize_offline('basic_params', raw_data)\n",
    "fs.materialize_offline('power_metrics', raw_data)\n",
    "fs.materialize_offline('zscore_normalized', raw_data)\n",
    "fs.materialize_offline('rolling_aggregates', raw_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OFFLINE FEATURE STORE STATUS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for feature_name, feature_data in fs.offline_store.items():\n",
    "    print(f\"\\n{feature_name}:\")\n",
    "    print(f\"  Records: {len(feature_data)}\")\n",
    "    print(f\"  Columns: {list(feature_data.columns)}\")\n",
    "    print(f\"  Memory: {feature_data.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "# Create training dataset by joining features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING DATASET CREATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Entity DataFrame (devices we want features for)\n",
    "entity_df = raw_data[['device_id', 'timestamp', 'yield']].copy()\n",
    "\n",
    "# Get historical features (point-in-time join)\n",
    "training_features = fs.get_historical_features(\n",
    "    feature_names=['basic_params', 'power_metrics', 'zscore_normalized'],\n",
    "    entity_df=entity_df,\n",
    "    event_timestamp_col='timestamp'\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Training dataset created\")\n",
    "print(f\"Shape: {training_features.shape}\")\n",
    "print(f\"Columns: {list(training_features.columns)}\")\n",
    "print(f\"\\nSample records:\")\n",
    "print(training_features.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dfd9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using feature store features\n",
    "\n",
    "# Select feature columns (exclude metadata)\n",
    "feature_cols = ['vdd', 'idd', 'freq', 'temp', 'power', 'power_efficiency', \n",
    "                'vdd_zscore', 'idd_zscore', 'freq_zscore']\n",
    "\n",
    "# Prepare training data\n",
    "X = training_features[feature_cols].fillna(0)  # Handle any missing values\n",
    "y = training_features['yield']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL TRAINING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Test samples: {len(X_test)}\")\n",
    "print(f\"  Features used: {len(feature_cols)}\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "print(f\"  F1 Score: {f1:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 5 Most Important Features:\")\n",
    "for idx, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"  {row['feature']:20s}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Model trained successfully using feature store features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac1848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate online feature store population\n",
    "\n",
    "# Select latest features for a subset of devices (production devices)\n",
    "production_devices = raw_data.tail(20)  # Last 20 devices represent production\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ONLINE FEATURE STORE POPULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for idx, device in production_devices.iterrows():\n",
    "    device_id = device['device_id']\n",
    "    \n",
    "    # Compute features for this device\n",
    "    basic = compute_basic_features(pd.DataFrame([device]))\n",
    "    power = compute_power_features(pd.DataFrame([device]))\n",
    "    zscore = compute_zscore_features(raw_data)  # Use full population for Z-scores\n",
    "    \n",
    "    # Extract single device features\n",
    "    device_features = {\n",
    "        'vdd': device['vdd'],\n",
    "        'idd': device['idd'],\n",
    "        'freq': device['freq'],\n",
    "        'temp': device['temp'],\n",
    "        'power': device['vdd'] * device['idd'],\n",
    "        'power_efficiency': device['freq'] / (device['vdd'] * device['idd']),\n",
    "        'vdd_zscore': zscore.loc[idx, 'vdd_zscore'],\n",
    "        'idd_zscore': zscore.loc[idx, 'idd_zscore'],\n",
    "        'freq_zscore': zscore.loc[idx, 'freq_zscore']\n",
    "    }\n",
    "    \n",
    "    # Update online store\n",
    "    fs.materialize_online('device_features', device_id, device_features)\n",
    "\n",
    "print(f\"\\n‚úÖ Populated online store for {len(production_devices)} production devices\")\n",
    "print(f\"Online store size: {len(fs.online_store['device_features'])} entities\")\n",
    "\n",
    "# Test online feature retrieval (low-latency lookup)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ONLINE FEATURE RETRIEVAL TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simulate production inference request\n",
    "test_device_ids = ['DEV00980', 'DEV00985', 'DEV00990']\n",
    "\n",
    "# Retrieve features (this would be <10ms in production with Redis)\n",
    "online_features = fs.get_online_features(\n",
    "    feature_names=['device_features'],\n",
    "    entity_keys=test_device_ids\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Retrieved online features for {len(test_device_ids)} devices\")\n",
    "\n",
    "for device_id, features in online_features.items():\n",
    "    print(f\"\\n{device_id}:\")\n",
    "    if features['device_features']:\n",
    "        for key, value in features['device_features'].items():\n",
    "            print(f\"  {key:20s}: {value:.4f}\" if isinstance(value, float) else f\"  {key:20s}: {value}\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è Features not found in online store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3025c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production inference simulation\n",
    "\n",
    "def predict_with_online_features(device_ids, feature_store, model):\n",
    "    \"\"\"\n",
    "    Simulate production inference using online feature store.\n",
    "    \n",
    "    Args:\n",
    "        device_ids: List of device IDs to predict\n",
    "        feature_store: Initialized feature store\n",
    "        model: Trained ML model\n",
    "    \n",
    "    Returns:\n",
    "        Predictions and latency statistics\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    predictions = []\n",
    "    latencies = []\n",
    "    \n",
    "    for device_id in device_ids:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 1. Retrieve features from online store (Redis in production)\n",
    "        features = feature_store.get_online_features(\n",
    "            feature_names=['device_features'],\n",
    "            entity_keys=[device_id]\n",
    "        )\n",
    "        \n",
    "        # 2. Prepare feature vector\n",
    "        device_features = features[device_id]['device_features']\n",
    "        \n",
    "        if device_features is None:\n",
    "            predictions.append({'device_id': device_id, 'prediction': None, 'error': 'Features not found'})\n",
    "            continue\n",
    "        \n",
    "        feature_vector = np.array([\n",
    "            device_features['vdd'],\n",
    "            device_features['idd'],\n",
    "            device_features['freq'],\n",
    "            device_features['temp'],\n",
    "            device_features['power'],\n",
    "            device_features['power_efficiency'],\n",
    "            device_features['vdd_zscore'],\n",
    "            device_features['idd_zscore'],\n",
    "            device_features['freq_zscore']\n",
    "        ]).reshape(1, -1)\n",
    "        \n",
    "        # 3. Model inference\n",
    "        prediction = model.predict(feature_vector)[0]\n",
    "        probability = model.predict_proba(feature_vector)[0]\n",
    "        \n",
    "        # 4. Record latency\n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        latencies.append(latency_ms)\n",
    "        \n",
    "        predictions.append({\n",
    "            'device_id': device_id,\n",
    "            'prediction': 'PASS' if prediction == 1 else 'FAIL',\n",
    "            'confidence': probability[prediction],\n",
    "            'latency_ms': latency_ms\n",
    "        })\n",
    "    \n",
    "    return predictions, latencies\n",
    "\n",
    "# Test production inference\n",
    "test_devices = ['DEV00980', 'DEV00985', 'DEV00990', 'DEV00995']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PRODUCTION INFERENCE SIMULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "predictions, latencies = predict_with_online_features(test_devices, fs, model)\n",
    "\n",
    "print(f\"\\n‚úÖ Completed {len(predictions)} predictions\")\n",
    "print(f\"\\nLatency Statistics:\")\n",
    "print(f\"  Mean: {np.mean(latencies):.2f} ms\")\n",
    "print(f\"  P50:  {np.percentile(latencies, 50):.2f} ms\")\n",
    "print(f\"  P95:  {np.percentile(latencies, 95):.2f} ms\")\n",
    "print(f\"  P99:  {np.percentile(latencies, 99):.2f} ms\")\n",
    "\n",
    "print(f\"\\nPrediction Results:\")\n",
    "print(\"-\" * 60)\n",
    "for pred in predictions:\n",
    "    if 'error' in pred:\n",
    "        print(f\"{pred['device_id']}: ERROR - {pred['error']}\")\n",
    "    else:\n",
    "        print(f\"{pred['device_id']}: {pred['prediction']:5s} (confidence: {pred['confidence']:.3f}, latency: {pred['latency_ms']:.2f}ms)\")\n",
    "\n",
    "print(\"\\n‚úÖ Real-time inference using online feature store successful\")\n",
    "print(\"Note: Production with Redis would achieve <10ms feature retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca68316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feast feature store conceptual structure\n",
    "# (Would require feast installation and configuration)\n",
    "\n",
    "feast_structure = \"\"\"\n",
    "FEAST FEATURE STORE STRUCTURE\n",
    "==============================\n",
    "\n",
    "1. Project Structure:\n",
    "   feature_repo/\n",
    "   ‚îú‚îÄ‚îÄ feature_store.yaml        # Configuration (offline/online stores)\n",
    "   ‚îú‚îÄ‚îÄ entities.py                # Entity definitions (device, customer, etc.)\n",
    "   ‚îú‚îÄ‚îÄ features.py                # Feature view definitions\n",
    "   ‚îî‚îÄ‚îÄ data_sources.py            # Data source connections\n",
    "\n",
    "2. Entity Definition (entities.py):\n",
    "   ```python\n",
    "   from feast import Entity, ValueType\n",
    "   \n",
    "   device = Entity(\n",
    "       name=\"device\",\n",
    "       value_type=ValueType.STRING,\n",
    "       description=\"Semiconductor device entity\"\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. Feature View (features.py):\n",
    "   ```python\n",
    "   from feast import FeatureView, Field\n",
    "   from feast.types import Float32, String\n",
    "   from datetime import timedelta\n",
    "   \n",
    "   device_features = FeatureView(\n",
    "       name=\"device_parametrics\",\n",
    "       entities=[\"device\"],\n",
    "       ttl=timedelta(days=30),\n",
    "       schema=[\n",
    "           Field(name=\"vdd\", dtype=Float32),\n",
    "           Field(name=\"idd\", dtype=Float32),\n",
    "           Field(name=\"freq\", dtype=Float32),\n",
    "           Field(name=\"temp\", dtype=Float32),\n",
    "           Field(name=\"power\", dtype=Float32),\n",
    "       ],\n",
    "       source=device_data_source,  # From data_sources.py\n",
    "       online=True\n",
    "   )\n",
    "   ```\n",
    "\n",
    "4. Configuration (feature_store.yaml):\n",
    "   ```yaml\n",
    "   project: semiconductor_features\n",
    "   registry: data/registry.db\n",
    "   provider: local\n",
    "   online_store:\n",
    "       type: redis\n",
    "       connection_string: localhost:6379\n",
    "   offline_store:\n",
    "       type: file\n",
    "   ```\n",
    "\n",
    "5. CLI Commands:\n",
    "   ```bash\n",
    "   # Initialize project\n",
    "   feast init feature_repo\n",
    "   \n",
    "   # Register features\n",
    "   feast -c feature_repo apply\n",
    "   \n",
    "   # Materialize historical features to offline store\n",
    "   feast -c feature_repo materialize-incremental 2024-01-01T00:00:00\n",
    "   \n",
    "   # Materialize latest features to online store\n",
    "   feast -c feature_repo materialize 2024-01-01T00:00:00 2024-12-31T23:59:59\n",
    "   ```\n",
    "\n",
    "6. Feature Retrieval (Python SDK):\n",
    "   ```python\n",
    "   from feast import FeatureStore\n",
    "   \n",
    "   store = FeatureStore(repo_path=\"feature_repo/\")\n",
    "   \n",
    "   # Offline (training)\n",
    "   training_df = store.get_historical_features(\n",
    "       entity_df=entity_df,\n",
    "       features=[\"device_parametrics:vdd\", \"device_parametrics:idd\"]\n",
    "   ).to_df()\n",
    "   \n",
    "   # Online (inference)\n",
    "   online_features = store.get_online_features(\n",
    "       features=[\"device_parametrics:vdd\", \"device_parametrics:idd\"],\n",
    "       entity_rows=[{\"device\": \"DEV00123\"}]\n",
    "   ).to_dict()\n",
    "   ```\n",
    "\n",
    "7. Key Benefits:\n",
    "   - Point-in-time correctness (no data leakage)\n",
    "   - Automatic online/offline consistency\n",
    "   - Feature versioning and lineage\n",
    "   - Scalable to billions of features\n",
    "   - Production-ready (used by Uber, Shopify, etc.)\n",
    "\"\"\"\n",
    "\n",
    "print(feast_structure)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEAST VS CUSTOM IMPLEMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Aspect': [\n",
    "        'Point-in-time correctness',\n",
    "        'Online/offline consistency',\n",
    "        'Feature versioning',\n",
    "        'Scalability',\n",
    "        'Setup complexity',\n",
    "        'Production readiness'\n",
    "    ],\n",
    "    'Custom Implementation': [\n",
    "        'Manual implementation required',\n",
    "        'Requires careful design',\n",
    "        'Manual versioning',\n",
    "        'Limited (single machine)',\n",
    "        'Low (quick start)',\n",
    "        'Requires significant work'\n",
    "    ],\n",
    "    'Feast': [\n",
    "        'Built-in (automatic)',\n",
    "        'Automatic guarantee',\n",
    "        'Built-in registry',\n",
    "        'High (distributed)',\n",
    "        'Medium (config required)',\n",
    "        'Production-ready'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"\\n‚úÖ Feast recommended for production feature stores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc0fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature store monitoring and validation\n",
    "\n",
    "class FeatureStoreMonitor:\n",
    "    \"\"\"Monitor feature store health and quality.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_store):\n",
    "        self.fs = feature_store\n",
    "        self.baseline_stats = {}\n",
    "    \n",
    "    def compute_feature_statistics(self, feature_name):\n",
    "        \"\"\"Compute statistics for a feature.\"\"\"\n",
    "        if feature_name not in self.fs.offline_store:\n",
    "            return None\n",
    "        \n",
    "        data = self.fs.offline_store[feature_name]\n",
    "        \n",
    "        stats = {}\n",
    "        for col in data.select_dtypes(include=[np.number]).columns:\n",
    "            stats[col] = {\n",
    "                'mean': data[col].mean(),\n",
    "                'std': data[col].std(),\n",
    "                'min': data[col].min(),\n",
    "                'max': data[col].max(),\n",
    "                'nulls': data[col].isnull().sum(),\n",
    "                'count': len(data[col])\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def set_baseline(self, feature_name):\n",
    "        \"\"\"Set baseline statistics for drift detection.\"\"\"\n",
    "        stats = self.compute_feature_statistics(feature_name)\n",
    "        if stats:\n",
    "            self.baseline_stats[feature_name] = stats\n",
    "            print(f\"‚úÖ Baseline set for {feature_name}\")\n",
    "    \n",
    "    def check_feature_drift(self, feature_name, threshold=0.2):\n",
    "        \"\"\"\n",
    "        Check if features have drifted from baseline.\n",
    "        \n",
    "        Args:\n",
    "            feature_name: Feature to check\n",
    "            threshold: Maximum allowed relative change (20%)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with drift alerts\n",
    "        \"\"\"\n",
    "        if feature_name not in self.baseline_stats:\n",
    "            return {'error': 'Baseline not set'}\n",
    "        \n",
    "        current_stats = self.compute_feature_statistics(feature_name)\n",
    "        baseline = self.baseline_stats[feature_name]\n",
    "        \n",
    "        drift_alerts = {}\n",
    "        \n",
    "        for col in current_stats:\n",
    "            current = current_stats[col]\n",
    "            base = baseline[col]\n",
    "            \n",
    "            # Check mean drift\n",
    "            mean_change = abs(current['mean'] - base['mean']) / (base['mean'] + 1e-10)\n",
    "            \n",
    "            # Check std drift\n",
    "            std_change = abs(current['std'] - base['std']) / (base['std'] + 1e-10)\n",
    "            \n",
    "            # Check null increase\n",
    "            null_increase = current['nulls'] - base['nulls']\n",
    "            \n",
    "            if mean_change > threshold or std_change > threshold or null_increase > 0:\n",
    "                drift_alerts[col] = {\n",
    "                    'mean_drift': mean_change,\n",
    "                    'std_drift': std_change,\n",
    "                    'null_increase': null_increase,\n",
    "                    'status': 'üö® DRIFT DETECTED' if mean_change > threshold else '‚ö†Ô∏è WARNING'\n",
    "                }\n",
    "        \n",
    "        return drift_alerts\n",
    "    \n",
    "    def validate_feature_quality(self, feature_name):\n",
    "        \"\"\"Validate feature data quality.\"\"\"\n",
    "        stats = self.compute_feature_statistics(feature_name)\n",
    "        \n",
    "        if not stats:\n",
    "            return {'error': 'Feature not found'}\n",
    "        \n",
    "        issues = []\n",
    "        \n",
    "        for col, col_stats in stats.items():\n",
    "            # Check for excessive nulls\n",
    "            null_rate = col_stats['nulls'] / col_stats['count']\n",
    "            if null_rate > 0.1:\n",
    "                issues.append(f\"‚ö†Ô∏è {col}: {null_rate:.1%} null values (threshold: 10%)\")\n",
    "            \n",
    "            # Check for zero variance (constant features)\n",
    "            if col_stats['std'] < 1e-6:\n",
    "                issues.append(f\"‚ö†Ô∏è {col}: Nearly constant (std={col_stats['std']:.6f})\")\n",
    "            \n",
    "            # Check for extreme values (basic outlier detection)\n",
    "            mean = col_stats['mean']\n",
    "            std = col_stats['std']\n",
    "            range_width = col_stats['max'] - col_stats['min']\n",
    "            \n",
    "            if range_width > 10 * std:\n",
    "                issues.append(f\"‚ö†Ô∏è {col}: Potential outliers (range >> std)\")\n",
    "        \n",
    "        return issues if issues else ['‚úÖ All quality checks passed']\n",
    "\n",
    "# Initialize monitor\n",
    "monitor = FeatureStoreMonitor(fs)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE STORE MONITORING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Set baseline for features\n",
    "monitor.set_baseline('basic_params')\n",
    "monitor.set_baseline('power_metrics')\n",
    "\n",
    "# Check feature quality\n",
    "print(\"\\nFeature Quality Validation:\")\n",
    "print(\"-\" * 60)\n",
    "for feature_name in ['basic_params', 'power_metrics']:\n",
    "    print(f\"\\n{feature_name}:\")\n",
    "    quality_issues = monitor.validate_feature_quality(feature_name)\n",
    "    for issue in quality_issues:\n",
    "        print(f\"  {issue}\")\n",
    "\n",
    "# Simulate feature drift (create new data with shifted distribution)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DRIFT DETECTION SIMULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create drifted data\n",
    "drifted_data = raw_data.copy()\n",
    "drifted_data['vdd'] = drifted_data['vdd'] + 0.05  # Shift voltage by 50mV\n",
    "drifted_data['idd'] = drifted_data['idd'] * 1.15  # Increase current by 15%\n",
    "\n",
    "# Materialize drifted features\n",
    "fs.materialize_offline('basic_params', drifted_data)\n",
    "\n",
    "# Check for drift\n",
    "drift_alerts = monitor.check_feature_drift('basic_params', threshold=0.1)\n",
    "\n",
    "print(\"\\nDrift Detection Results:\")\n",
    "print(\"-\" * 60)\n",
    "if drift_alerts:\n",
    "    for col, alert in drift_alerts.items():\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Status: {alert['status']}\")\n",
    "        print(f\"  Mean drift: {alert['mean_drift']:.3f} ({alert['mean_drift']*100:.1f}%)\")\n",
    "        print(f\"  Std drift: {alert['std_drift']:.3f} ({alert['std_drift']*100:.1f}%)\")\n",
    "        print(f\"  Null increase: {alert['null_increase']}\")\n",
    "else:\n",
    "    print(\"‚úÖ No drift detected\")\n",
    "\n",
    "print(\"\\n‚úÖ Feature monitoring and validation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = \"\"\"\n",
    "================================================================================\n",
    "REAL-WORLD FEATURE STORE PROJECTS\n",
    "================================================================================\n",
    "\n",
    "POST-SILICON VALIDATION PROJECTS\n",
    "---------------------------------\n",
    "\n",
    "1. CENTRALIZED DEVICE FEATURE STORE FOR MULTI-MODEL ECOSYSTEM\n",
    "   \n",
    "   Objective: Build single feature store serving 10+ ML models (yield, binning, \n",
    "              test time, anomaly detection, wafer correlation)\n",
    "   \n",
    "   Success Metrics:\n",
    "   - Feature reuse: >70% features shared across models\n",
    "   - Training-serving skew: <1% accuracy difference\n",
    "   - Feature compute reduction: 80% (eliminate duplication)\n",
    "   - Model development time: 50% faster (pre-built features)\n",
    "   \n",
    "   Architecture:\n",
    "   - Offline: Snowflake data warehouse (historical STDF data)\n",
    "   - Online: Redis cluster (real-time binning)\n",
    "   - Computation: Airflow DAGs (daily feature materialization)\n",
    "   - Registry: Feast feature definitions\n",
    "   \n",
    "   Features (50+ total):\n",
    "   - Basic: Vdd, Idd, Freq, Temp (raw parametrics)\n",
    "   - Derived: Power, power_efficiency, thermal_margin\n",
    "   - Statistical: Z-scores, percentiles vs lot/wafer\n",
    "   - Temporal: Rolling means, test sequence effects\n",
    "   - Spatial: Wafer map neighbors, radial position\n",
    "   \n",
    "   Implementation:\n",
    "   ```python\n",
    "   # Entity: Device\n",
    "   device = Entity(name=\"device\", value_type=ValueType.STRING)\n",
    "   \n",
    "   # Feature views\n",
    "   basic_params = FeatureView(\n",
    "       name=\"device_basic_params\",\n",
    "       entities=[\"device\"],\n",
    "       ttl=timedelta(days=90),  # 90-day retention\n",
    "       features=[...],\n",
    "       source=stdf_data_source\n",
    "   )\n",
    "   \n",
    "   spatial_features = FeatureView(\n",
    "       name=\"wafer_spatial\",\n",
    "       entities=[\"device\"],\n",
    "       features=[...],  # Neighbor statistics\n",
    "       source=wafer_map_source\n",
    "   )\n",
    "   ```\n",
    "   \n",
    "   Business Value: $2M annual savings (eliminate duplicate feature pipelines,\n",
    "                   faster model development, consistent features reduce errors)\n",
    "\n",
    "2. REAL-TIME BINNING FEATURE STORE (INLINE TEST)\n",
    "   \n",
    "   Objective: Enable <50ms binning decisions during production test using \n",
    "              pre-computed features from online store\n",
    "   \n",
    "   Success Metrics:\n",
    "   - Latency: <50ms p99 (feature retrieval + inference)\n",
    "   - Throughput: 10,000 predictions/sec\n",
    "   - Availability: 99.9% uptime\n",
    "   - Accuracy: Match offline binning model exactly\n",
    "   \n",
    "   Architecture:\n",
    "   - Online Store: Redis Cluster (6 nodes, 100K devices cached)\n",
    "   - Feature Pipeline: Kafka Streams (real-time feature computation)\n",
    "   - Model Serving: TensorFlow Serving (binning model)\n",
    "   - Monitoring: Prometheus + Grafana (latency, drift, errors)\n",
    "   \n",
    "   Features (30 features):\n",
    "   - Device params: Vdd, Idd, Freq, Temp, Power\n",
    "   - Lot context: Lot mean/std for each param (updated hourly)\n",
    "   - Percentile ranks: Device ranking vs lot distribution\n",
    "   - Derived: power_efficiency, freq_per_watt, thermal_headroom\n",
    "   \n",
    "   Workflow:\n",
    "   1. Device tested ‚Üí parametric values sent to Kafka\n",
    "   2. Kafka Streams: Compute features ‚Üí update Redis\n",
    "   3. Binning service: GET device features from Redis\n",
    "   4. Model inference: Predict bin (PASS/BIN1/BIN2/FAIL)\n",
    "   5. Return to tester: <50ms total latency\n",
    "   \n",
    "   Implementation:\n",
    "   ```python\n",
    "   # Online-only features (low latency)\n",
    "   online_features = FeatureView(\n",
    "       name=\"binning_features\",\n",
    "       entities=[\"device\"],\n",
    "       ttl=timedelta(hours=24),  # Fresh features only\n",
    "       features=[...],\n",
    "       online=True,\n",
    "       offline=False  # Not needed for training\n",
    "   )\n",
    "   \n",
    "   # Real-time serving\n",
    "   features = store.get_online_features(\n",
    "       features=[\"binning_features:vdd\", \"binning_features:lot_vdd_zscore\"],\n",
    "       entity_rows=[{\"device\": device_id}]\n",
    "   ).to_dict()\n",
    "   ```\n",
    "   \n",
    "   Business Value: 30% test time reduction (inline binning eliminates retest),\n",
    "                   $5M annual savings, improved yield (faster feedback)\n",
    "\n",
    "3. WAFER-LEVEL SPATIAL FEATURE ENGINEERING\n",
    "   \n",
    "   Objective: Build feature store with spatial correlation features for wafer\n",
    "              map analysis and die-level yield prediction\n",
    "   \n",
    "   Success Metrics:\n",
    "   - Spatial features: 20+ (neighbors, radial, edge effects)\n",
    "   - Yield prediction improvement: +5% accuracy vs baseline\n",
    "   - Defect pattern detection: Identify systematic failures\n",
    "   - Coverage: All wafer test sites (500K+ dies/day)\n",
    "   \n",
    "   Architecture:\n",
    "   - Offline: BigQuery (petabyte-scale wafer test history)\n",
    "   - Computation: Apache Spark (distributed spatial joins)\n",
    "   - Storage: Parquet files (partitioned by wafer_id, date)\n",
    "   - Lineage: Feast + Great Expectations (data validation)\n",
    "   \n",
    "   Spatial Features:\n",
    "   - Neighbor statistics: mean/std of 8 neighbors for each param\n",
    "   - Radial position: Distance from wafer center, angle\n",
    "   - Edge effects: Distance from wafer edge, edge bin flags\n",
    "   - Cluster features: Local density, nearest failure distance\n",
    "   - Wafer-level: Wafer mean/std, gradient vectors\n",
    "   \n",
    "   Point-in-Time Correctness:\n",
    "   - Critical: Don't leak future die results into neighbor features\n",
    "   - Solution: Compute neighbors only from previously tested dies\n",
    "   \n",
    "   Implementation:\n",
    "   ```python\n",
    "   def compute_spatial_features(wafer_df):\n",
    "       \\\"\\\"\\\"Compute spatial features with point-in-time correctness.\\\"\\\"\\\"\n",
    "       results = []\n",
    "       \n",
    "       for idx, die in wafer_df.sort_values('test_timestamp').iterrows():\n",
    "           # Only use dies tested BEFORE current die\n",
    "           previous_dies = wafer_df[wafer_df['test_timestamp'] < die['test_timestamp']]\n",
    "           \n",
    "           # Find neighbors\n",
    "           neighbors = previous_dies[\n",
    "               (abs(previous_dies['die_x'] - die['die_x']) <= 1) &\n",
    "               (abs(previous_dies['die_y'] - die['die_y']) <= 1)\n",
    "           ]\n",
    "           \n",
    "           features = {\n",
    "               'neighbor_mean_vdd': neighbors['vdd'].mean(),\n",
    "               'neighbor_std_vdd': neighbors['vdd'].std(),\n",
    "               'radial_distance': np.sqrt(die['die_x']**2 + die['die_y']**2),\n",
    "               # ... more spatial features\n",
    "           }\n",
    "           results.append(features)\n",
    "       \n",
    "       return pd.DataFrame(results)\n",
    "   ```\n",
    "   \n",
    "   Business Value: 5% yield improvement = $10M+ annual (better defect detection,\n",
    "                   root cause analysis, proactive process adjustments)\n",
    "\n",
    "4. FEATURE VERSIONING FOR A/B EXPERIMENTS\n",
    "   \n",
    "   Objective: Version-control feature definitions to enable reproducible\n",
    "              experiments and rollback when new features hurt performance\n",
    "   \n",
    "   Success Metrics:\n",
    "   - Reproducibility: 100% (same features ‚Üí same results)\n",
    "   - Experiment velocity: 3x faster (pre-built feature versions)\n",
    "   - Rollback time: <1 hour (revert to previous feature version)\n",
    "   - Feature lineage: Full tracking from raw data to model\n",
    "   \n",
    "   Versioning Strategy:\n",
    "   - v1.0: Baseline (raw parametrics: Vdd, Idd, Freq, Temp)\n",
    "   - v1.1: +Derived features (power, efficiency)\n",
    "   - v1.2: +Polynomial features (Vdd^2, Freq*Vdd interactions)\n",
    "   - v2.0: +Spatial features (wafer map neighbors)\n",
    "   - v2.1: +Temporal features (rolling statistics)\n",
    "   \n",
    "   Implementation:\n",
    "   ```python\n",
    "   # Feature definitions with versioning\n",
    "   device_features_v1 = FeatureView(\n",
    "       name=\"device_features\",\n",
    "       version=\"1.0\",\n",
    "       entities=[\"device\"],\n",
    "       features=[Field(name=\"vdd\"), Field(name=\"idd\"), ...],\n",
    "       tags={\"version\": \"baseline\"}\n",
    "   )\n",
    "   \n",
    "   device_features_v2 = FeatureView(\n",
    "       name=\"device_features\",\n",
    "       version=\"2.0\",\n",
    "       entities=[\"device\"],\n",
    "       features=[...],  # + spatial features\n",
    "       tags={\"version\": \"with_spatial\"}\n",
    "   )\n",
    "   \n",
    "   # Retrieve specific version\n",
    "   training_df = store.get_historical_features(\n",
    "       entity_df=entity_df,\n",
    "       features=[\"device_features__v1.0:vdd\", ...]  # Explicit version\n",
    "   ).to_df()\n",
    "   ```\n",
    "   \n",
    "   Experiment Tracking:\n",
    "   - MLflow: Log feature version with each experiment\n",
    "   - Comparison: A/B test v1.0 vs v2.0 on same validation set\n",
    "   - Decision: Promote v2.0 if +2% accuracy improvement\n",
    "   \n",
    "   Business Value: Faster experimentation, reproducible results, safe rollback,\n",
    "                   regulatory compliance (feature lineage for audits)\n",
    "\n",
    "\n",
    "GENERAL AI/ML PROJECTS\n",
    "----------------------\n",
    "\n",
    "5. E-COMMERCE RECOMMENDATION FEATURE STORE\n",
    "   \n",
    "   Objective: Unified feature store for recommendation models (product, user,\n",
    "              context features) serving 1M+ users\n",
    "   \n",
    "   Success Metrics:\n",
    "   - CTR improvement: +15% (better features)\n",
    "   - Feature freshness: <5 min latency (real-time user activity)\n",
    "   - Model variety: 5+ models sharing same features\n",
    "   - Infrastructure cost: -40% (eliminate duplicate pipelines)\n",
    "   \n",
    "   Architecture:\n",
    "   - Offline: Snowflake (historical clickstream, transactions)\n",
    "   - Online: DynamoDB (user profiles, product features)\n",
    "   - Streaming: Kafka + Flink (real-time activity features)\n",
    "   - Registry: Feast (300+ features)\n",
    "   \n",
    "   Features:\n",
    "   - User: demographics, purchase history, browsing patterns\n",
    "   - Product: category, price, popularity, reviews\n",
    "   - Context: time_of_day, device_type, location\n",
    "   - Derived: user_product_affinity, price_sensitivity\n",
    "   - Real-time: last_5_views, cart_value, session_duration\n",
    "   \n",
    "   Implementation:\n",
    "   ```python\n",
    "   # User features (updated hourly)\n",
    "   user_features = FeatureView(\n",
    "       name=\"user_profile\",\n",
    "       entities=[\"user\"],\n",
    "       ttl=timedelta(hours=24),\n",
    "       features=[...],\n",
    "       source=user_data_warehouse\n",
    "   )\n",
    "   \n",
    "   # Real-time session features (updated on every event)\n",
    "   session_features = FeatureView(\n",
    "       name=\"user_session\",\n",
    "       entities=[\"user\"],\n",
    "       ttl=timedelta(minutes=30),\n",
    "       features=[...],\n",
    "       source=kafka_stream\n",
    "   )\n",
    "   ```\n",
    "   \n",
    "   Business Value: $20M annual revenue increase (CTR improvement),\n",
    "                   $2M infrastructure savings (eliminate duplication)\n",
    "\n",
    "6. FRAUD DETECTION FEATURE STORE WITH GRAPH FEATURES\n",
    "   \n",
    "   Objective: Feature store combining transactional, behavioral, and graph\n",
    "              features for real-time fraud detection (<100ms)\n",
    "   \n",
    "   Success Metrics:\n",
    "   - Fraud detection rate: 95% (vs 80% baseline)\n",
    "   - False positive rate: <2% (minimize customer friction)\n",
    "   - Latency: <100ms p99 (real-time blocking)\n",
    "   - Feature types: 100+ (traditional + graph embeddings)\n",
    "   \n",
    "   Features:\n",
    "   - Transactional: amount, merchant, category, time\n",
    "   - Behavioral: velocity (transactions/hour), amount patterns\n",
    "   - Historical: user lifetime value, fraud history\n",
    "   - Graph: PageRank, community detection, suspicious connections\n",
    "   - Derived: amount_zscore_vs_user_history, merchant_risk_score\n",
    "   \n",
    "   Graph Feature Engineering:\n",
    "   ```python\n",
    "   # Build transaction graph\n",
    "   G = nx.Graph()\n",
    "   for txn in transactions:\n",
    "       G.add_edge(txn['user_id'], txn['merchant_id'], weight=txn['amount'])\n",
    "   \n",
    "   # Compute graph features\n",
    "   pagerank = nx.pagerank(G)\n",
    "   communities = nx.community.louvain_communities(G)\n",
    "   \n",
    "   # Store as features\n",
    "   graph_features = pd.DataFrame({\n",
    "       'user_id': list(pagerank.keys()),\n",
    "       'pagerank_score': list(pagerank.values()),\n",
    "       'community_id': [get_community(u, communities) for u in pagerank.keys()]\n",
    "   })\n",
    "   ```\n",
    "   \n",
    "   Business Value: $50M annual fraud prevented, $10M false positive reduction,\n",
    "                   customer trust (fewer false declines)\n",
    "\n",
    "7. CHURN PREDICTION FEATURE STORE (TELECOM)\n",
    "   \n",
    "   Objective: Feature store for customer churn prediction with 200+ features\n",
    "              from billing, usage, support, and network data\n",
    "   \n",
    "   Success Metrics:\n",
    "   - Churn prediction AUC: >0.85\n",
    "   - Early detection: 30 days before churn\n",
    "   - Feature coverage: 100% of customers\n",
    "   - Refresh frequency: Daily (offline), hourly (online)\n",
    "   \n",
    "   Features (200+ total):\n",
    "   - Billing: monthly_charge, payment_delays, plan_changes\n",
    "   - Usage: voice_minutes, data_GB, roaming_frequency\n",
    "   - Support: tickets_count, avg_resolution_time, satisfaction\n",
    "   - Network: dropped_calls, avg_speed, coverage_quality\n",
    "   - Derived: usage_trend, payment_reliability, support_burden\n",
    "   - Temporal: 3mo/6mo/12mo rolling averages\n",
    "   \n",
    "   Point-in-Time Challenge:\n",
    "   - Problem: Churn label is future event (30 days ahead)\n",
    "   - Solution: Features must be from 30+ days before churn date\n",
    "   \n",
    "   Implementation:\n",
    "   ```python\n",
    "   # Create training dataset with 30-day offset\n",
    "   entity_df = pd.DataFrame({\n",
    "       'customer_id': customer_ids,\n",
    "       'event_timestamp': churn_dates - timedelta(days=30),  # 30 days before\n",
    "       'churned': churn_labels\n",
    "   })\n",
    "   \n",
    "   # Get features as they existed 30 days before churn\n",
    "   training_df = store.get_historical_features(\n",
    "       entity_df=entity_df,\n",
    "       features=[...],  # Point-in-time correct\n",
    "   ).to_df()\n",
    "   ```\n",
    "   \n",
    "   Business Value: $100M annual retention (save 20% of at-risk customers),\n",
    "                   proactive interventions, targeted offers\n",
    "\n",
    "8. DEMAND FORECASTING FEATURE STORE (RETAIL)\n",
    "   \n",
    "   Objective: Multi-SKU demand forecasting with features from sales, weather,\n",
    "              promotions, holidays, and economic indicators\n",
    "   \n",
    "   Success Metrics:\n",
    "   - Forecast accuracy: MAPE <15%\n",
    "   - SKU coverage: 10,000+ products\n",
    "   - Forecast horizon: 28 days\n",
    "   - Feature update: Daily (automated pipeline)\n",
    "   \n",
    "   Architecture:\n",
    "   - Offline: BigQuery (5 years sales history)\n",
    "   - External: Weather API, economic data API\n",
    "   - Computation: Airflow DAGs (daily feature materialization)\n",
    "   - Models: LightGBM per product category\n",
    "   \n",
    "   Features:\n",
    "   - Historical sales: 7d/14d/28d/365d rolling means\n",
    "   - Trend: Linear regression slope over 90 days\n",
    "   - Seasonality: Day of week, month, quarter effects\n",
    "   - Promotions: Active promotions, discount %, ad spend\n",
    "   - Weather: Temperature, precipitation, forecast\n",
    "   - Calendar: Holidays, paydays, special events\n",
    "   - Economic: Unemployment, consumer confidence, gas prices\n",
    "   - Derived: price_elasticity, promotion_effectiveness\n",
    "   \n",
    "   Implementation:\n",
    "   ```python\n",
    "   # External data source (weather)\n",
    "   weather_source = PushSource(\n",
    "       name=\"weather_data\",\n",
    "       batch_source=weather_api_connector\n",
    "   )\n",
    "   \n",
    "   weather_features = FeatureView(\n",
    "       name=\"weather\",\n",
    "       entities=[\"location\", \"date\"],\n",
    "       features=[...],\n",
    "       source=weather_source\n",
    "   )\n",
    "   \n",
    "   # Join sales + weather + promotions\n",
    "   training_df = store.get_historical_features(\n",
    "       entity_df=sku_dates_df,\n",
    "       features=[\n",
    "           \"sales_history:rolling_7d_mean\",\n",
    "           \"weather:temperature\",\n",
    "           \"promotions:active_discount\"\n",
    "       ]\n",
    "   ).to_df()\n",
    "   ```\n",
    "   \n",
    "   Business Value: $30M inventory reduction (better forecasting),\n",
    "                   10% revenue increase (fewer stockouts),\n",
    "                   optimized promotions\n",
    "\n",
    "\n",
    "================================================================================\n",
    "IMPLEMENTATION CHECKLIST (FOR ALL PROJECTS)\n",
    "================================================================================\n",
    "\n",
    "Phase 1: Design & Planning (Week 1-2)\n",
    "‚ñ° Define entities (device, customer, product, etc.)\n",
    "‚ñ° Identify features (raw, derived, aggregated)\n",
    "‚ñ° Choose offline store (Snowflake, BigQuery, S3)\n",
    "‚ñ° Choose online store (Redis, DynamoDB, Cassandra)\n",
    "‚ñ° Design feature computation pipeline (Airflow, Spark)\n",
    "‚ñ° Plan monitoring (drift detection, quality checks)\n",
    "\n",
    "Phase 2: Implementation (Week 3-6)\n",
    "‚ñ° Set up Feast (feast init, configure stores)\n",
    "‚ñ° Define feature views (entities.py, features.py)\n",
    "‚ñ° Implement feature transformations\n",
    "‚ñ° Build offline materialization pipeline\n",
    "‚ñ° Build online serving pipeline\n",
    "‚ñ° Implement monitoring dashboards\n",
    "\n",
    "Phase 3: Integration (Week 7-8)\n",
    "‚ñ° Integrate with model training pipeline\n",
    "‚ñ° Integrate with model serving (online features)\n",
    "‚ñ° Set up feature drift alerts\n",
    "‚ñ° Configure backup and disaster recovery\n",
    "‚ñ° Load testing (latency, throughput)\n",
    "\n",
    "Phase 4: Production (Week 9-10)\n",
    "‚ñ° Deploy to production environment\n",
    "‚ñ° Monitor feature freshness and quality\n",
    "‚ñ° Track training-serving skew\n",
    "‚ñ° Optimize performance (caching, indexing)\n",
    "‚ñ° Document features (registry, wiki)\n",
    "\n",
    "Phase 5: Iteration (Ongoing)\n",
    "‚ñ° Add new features based on model experiments\n",
    "‚ñ° Version features (v1, v2, etc.)\n",
    "‚ñ° Deprecate unused features\n",
    "‚ñ° Optimize costs (storage, compute)\n",
    "‚ñ° Share features across teams\n",
    "\"\"\"\n",
    "\n",
    "print(projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839df2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "takeaways = \"\"\"\n",
    "================================================================================\n",
    "KEY TAKEAWAYS: FEATURE STORE IMPLEMENTATION\n",
    "================================================================================\n",
    "\n",
    "1. CORE CONCEPTS\n",
    "   -------------\n",
    "   \n",
    "   What is a Feature Store?\n",
    "   - Centralized repository for ML features\n",
    "   - Solves training-serving skew (features computed identically)\n",
    "   - Ensures point-in-time correctness (no data leakage)\n",
    "   - Enables feature reuse across models and teams\n",
    "   \n",
    "   Critical Problem Solved:\n",
    "   Problem: Training features ‚â† Serving features\n",
    "   Example: Training uses SQL query, serving uses Python function\n",
    "   Result: Model accuracy drops in production (skew)\n",
    "   Solution: Feature store computes features once, serves to both\n",
    "   \n",
    "   Components:\n",
    "   1. Feature Registry: Metadata catalog (definitions, schemas, lineage)\n",
    "   2. Offline Store: Historical features for training (data warehouse)\n",
    "   3. Online Store: Low-latency features for serving (key-value store)\n",
    "   4. Feature Computation: Transforms raw data ‚Üí features\n",
    "\n",
    "\n",
    "2. OFFLINE VS ONLINE STORES\n",
    "   -------------------------\n",
    "   \n",
    "   Offline Store (Training):\n",
    "   - Purpose: Historical features for model training\n",
    "   - Storage: Data warehouse (Snowflake, BigQuery, Redshift)\n",
    "   - Query pattern: Large batch queries (millions of rows)\n",
    "   - Latency: Seconds to minutes (not time-critical)\n",
    "   - Point-in-time joins: Features as they existed at event timestamp\n",
    "   - Use cases: Training datasets, feature exploration, backtesting\n",
    "   \n",
    "   Online Store (Serving):\n",
    "   - Purpose: Real-time features for production inference\n",
    "   - Storage: Key-value store (Redis, DynamoDB, Cassandra)\n",
    "   - Query pattern: Single entity lookup (GET device:12345)\n",
    "   - Latency: <10ms p99 (critical for real-time)\n",
    "   - Freshness: Continuously updated (streaming pipeline)\n",
    "   - Use cases: REST API serving, mobile apps, real-time decisions\n",
    "   \n",
    "   Example:\n",
    "   # Offline: Training yield prediction model\n",
    "   training_df = store.get_historical_features(\n",
    "       entity_df=devices_df,  # 100K devices\n",
    "       features=[\"device_features:vdd\", \"device_features:idd\"]\n",
    "   ).to_df()  # Takes 30 seconds, returns 100K rows\n",
    "   \n",
    "   # Online: Production binning (real-time)\n",
    "   features = store.get_online_features(\n",
    "       features=[\"device_features:vdd\", \"device_features:idd\"],\n",
    "       entity_rows=[{\"device\": \"DEV00123\"}]\n",
    "   ).to_dict()  # Takes <10ms, returns 1 device\n",
    "\n",
    "\n",
    "3. POINT-IN-TIME CORRECTNESS\n",
    "   --------------------------\n",
    "   \n",
    "   Critical Concept:\n",
    "   Features must only use data available at the event timestamp.\n",
    "   Prevents data leakage from the future.\n",
    "   \n",
    "   Example Problem (Without Point-in-Time):\n",
    "   Training: Predict if device will fail at time T\n",
    "   Feature: Lot average voltage (computed from entire lot)\n",
    "   Issue: Lot average includes devices tested AFTER time T\n",
    "   Result: Model sees future data during training (leakage)\n",
    "   Production: Lot average only includes past devices\n",
    "   Outcome: Model accuracy drops (training-serving skew)\n",
    "   \n",
    "   Solution (With Point-in-Time):\n",
    "   For each training example at time T:\n",
    "   - Only use features computed from data BEFORE time T\n",
    "   - Feature store automatically filters data by timestamp\n",
    "   - Training features match production features exactly\n",
    "   \n",
    "   Implementation:\n",
    "   entity_df = pd.DataFrame({\n",
    "       'device_id': ['DEV001', 'DEV002'],\n",
    "       'event_timestamp': [datetime(2024,1,1,10,0), datetime(2024,1,1,11,0)]\n",
    "   })\n",
    "   \n",
    "   # Feature store joins features as they existed at event_timestamp\n",
    "   training_df = store.get_historical_features(\n",
    "       entity_df=entity_df,\n",
    "       features=[\"lot_statistics:mean_vdd\"]  # Only uses data <= event_timestamp\n",
    "   ).to_df()\n",
    "   \n",
    "   Critical for:\n",
    "   - Time series forecasting (don't leak future values)\n",
    "   - Churn prediction (features from 30 days before churn)\n",
    "   - Fraud detection (features from before transaction)\n",
    "   - Spatial features (only previously tested dies)\n",
    "\n",
    "\n",
    "4. FEATURE ENGINEERING PATTERNS\n",
    "   -----------------------------\n",
    "   \n",
    "   Raw Features:\n",
    "   - Direct measurements from data sources\n",
    "   - Example: vdd, idd, freq, temp (STDF parameters)\n",
    "   - Minimal transformation, high interpretability\n",
    "   \n",
    "   Derived Features:\n",
    "   - Computed from raw features\n",
    "   - Example: power = vdd * idd, efficiency = freq / power\n",
    "   - Domain knowledge embedded, engineered insights\n",
    "   \n",
    "   Aggregate Features:\n",
    "   - Statistical summaries over groups\n",
    "   - Example: lot_mean_vdd = AVG(vdd) GROUP BY lot_id\n",
    "   - Contextual information, relative comparisons\n",
    "   \n",
    "   Temporal Features:\n",
    "   - Time-based patterns and trends\n",
    "   - Example: rolling_7d_mean, day_of_week, trend_slope\n",
    "   - Capture seasonality, trends, time-of-day effects\n",
    "   \n",
    "   Spatial Features:\n",
    "   - Location-based relationships\n",
    "   - Example: neighbor_mean (wafer maps), distance_from_center\n",
    "   - Capture spatial correlations, clustering\n",
    "   \n",
    "   Categorical Encodings:\n",
    "   - Transform categories to numeric\n",
    "   - Example: bin_category ‚Üí one-hot encoding\n",
    "   - Enable ML algorithms to use categorical data\n",
    "   \n",
    "   Interaction Features:\n",
    "   - Combinations of features\n",
    "   - Example: vdd * freq, temp_high AND idd_high\n",
    "   - Capture non-linear relationships\n",
    "   \n",
    "   Feature Hierarchy:\n",
    "   Level 1 (Raw): vdd=1.2V, idd=250mA\n",
    "   Level 2 (Derived): power=300mW, efficiency=0.83 GHz/W\n",
    "   Level 3 (Normalized): vdd_zscore=0.5, power_percentile=85\n",
    "   Level 4 (Aggregate): lot_mean_power=320mW, wafer_std_vdd=0.02\n",
    "   Level 5 (Spatial): neighbor_mean_power=310mW, radial_distance=45mm\n",
    "\n",
    "\n",
    "5. FEAST FRAMEWORK - PRODUCTION STANDARD\n",
    "   --------------------------------------\n",
    "   \n",
    "   Why Feast?\n",
    "   - Open source, industry standard (Uber, Shopify, etc.)\n",
    "   - Automatic point-in-time correctness\n",
    "   - Online/offline consistency guaranteed\n",
    "   - Feature versioning and lineage built-in\n",
    "   - Scales to billions of features\n",
    "   \n",
    "   Core Abstractions:\n",
    "   \n",
    "   Entity: Thing being modeled (device, customer, product)\n",
    "   device = Entity(name=\"device\", value_type=ValueType.STRING)\n",
    "   \n",
    "   Data Source: Raw data location\n",
    "   device_source = FileSource(\n",
    "       path=\"s3://bucket/stdf_data.parquet\",\n",
    "       timestamp_field=\"test_timestamp\"\n",
    "   )\n",
    "   \n",
    "   Feature View: Group of related features\n",
    "   device_features = FeatureView(\n",
    "       name=\"device_parametrics\",\n",
    "       entities=[\"device\"],\n",
    "       ttl=timedelta(days=90),  # Feature retention\n",
    "       schema=[Field(name=\"vdd\", dtype=Float32), ...],\n",
    "       source=device_source,\n",
    "       online=True  # Materialize to online store\n",
    "   )\n",
    "   \n",
    "   Feature Service: Named set of features for a model\n",
    "   binning_service = FeatureService(\n",
    "       name=\"binning_v1\",\n",
    "       features=[\n",
    "           device_features[[\"vdd\", \"idd\", \"freq\"]],\n",
    "           lot_features[[\"lot_mean_vdd\"]]\n",
    "       ]\n",
    "   )\n",
    "   \n",
    "   CLI Workflow:\n",
    "   # 1. Initialize project\n",
    "   feast init feature_repo\n",
    "   \n",
    "   # 2. Define features (entities.py, features.py)\n",
    "   \n",
    "   # 3. Register features\n",
    "   feast -c feature_repo apply\n",
    "   \n",
    "   # 4. Materialize to offline store (historical)\n",
    "   feast materialize-incremental $(date -u +\"%Y-%m-%dT%H:%M:%S\")\n",
    "   \n",
    "   # 5. Materialize to online store (latest)\n",
    "   feast materialize 2024-01-01T00:00:00 2024-12-31T23:59:59\n",
    "   \n",
    "   Python SDK:\n",
    "   from feast import FeatureStore\n",
    "   \n",
    "   store = FeatureStore(repo_path=\"feature_repo/\")\n",
    "   \n",
    "   # Training (offline)\n",
    "   training_df = store.get_historical_features(\n",
    "       entity_df=entity_df,\n",
    "       features=[\"device_parametrics:vdd\", \"device_parametrics:idd\"]\n",
    "   ).to_df()\n",
    "   \n",
    "   # Inference (online)\n",
    "   online_features = store.get_online_features(\n",
    "       features=[\"device_parametrics:vdd\"],\n",
    "       entity_rows=[{\"device\": \"DEV123\"}]\n",
    "   ).to_dict()\n",
    "\n",
    "\n",
    "6. FEATURE VERSIONING & REPRODUCIBILITY\n",
    "   -------------------------------------\n",
    "   \n",
    "   Why Version Features?\n",
    "   - Reproduce experiments (same features ‚Üí same results)\n",
    "   - A/B test feature sets (v1 vs v2)\n",
    "   - Rollback when new features hurt performance\n",
    "   - Regulatory compliance (audit trail)\n",
    "   \n",
    "   Versioning Strategies:\n",
    "   \n",
    "   1. Explicit Version in Name:\n",
    "   device_features_v1 = FeatureView(name=\"device_features_v1\", ...)\n",
    "   device_features_v2 = FeatureView(name=\"device_features_v2\", ...)\n",
    "   \n",
    "   2. Git-Based Versioning:\n",
    "   - Feature definitions in Git repository\n",
    "   - Tag releases: git tag v1.0.0\n",
    "   - Checkout specific version for reproduction\n",
    "   \n",
    "   3. Feature Registry Versioning (Feast):\n",
    "   - Feast tracks feature view versions automatically\n",
    "   - Retrieve specific version: features__v1.0:vdd\n",
    "   \n",
    "   4. Schema Evolution:\n",
    "   v1.0: Basic features (vdd, idd, freq, temp)\n",
    "   v1.1: +Derived (power, efficiency) [backward compatible]\n",
    "   v2.0: +Spatial (neighbors) [new feature view]\n",
    "   v2.1: vdd ‚Üí vdd_mv (unit change) [breaking change]\n",
    "   \n",
    "   Experiment Tracking Integration:\n",
    "   with mlflow.start_run():\n",
    "       mlflow.log_param(\"feature_version\", \"v2.0\")\n",
    "       mlflow.log_param(\"features\", [\"vdd\", \"idd\", \"power\", \"neighbors\"])\n",
    "       \n",
    "       # Train with specific feature version\n",
    "       features = store.get_historical_features(\n",
    "           features=[\"device_features__v2.0:vdd\", ...]\n",
    "       )\n",
    "       \n",
    "   Rollback Procedure:\n",
    "   1. Detect performance degradation (model monitoring)\n",
    "   2. Identify feature version causing issue (v2.1)\n",
    "   3. Rollback to previous version (v2.0)\n",
    "   4. Redeploy model trained on v2.0 features\n",
    "   5. Investigate v2.1 issue, fix, re-release as v2.2\n",
    "\n",
    "\n",
    "7. ONLINE SERVING ARCHITECTURE\n",
    "   ----------------------------\n",
    "   \n",
    "   Requirements:\n",
    "   - Latency: <10ms p99 (feature retrieval only)\n",
    "   - Throughput: 10K-100K requests/sec\n",
    "   - Availability: 99.9%+ uptime\n",
    "   - Consistency: Match offline features exactly\n",
    "   \n",
    "   Technology Stack:\n",
    "   \n",
    "   Redis (Most Common):\n",
    "   - In-memory key-value store\n",
    "   - 1-5ms latency at p99\n",
    "   - 100K+ ops/sec per node\n",
    "   - Clustering for horizontal scaling\n",
    "   - Persistence (RDB, AOF)\n",
    "   \n",
    "   DynamoDB (AWS):\n",
    "   - Managed NoSQL database\n",
    "   - Single-digit millisecond latency\n",
    "   - Auto-scaling (pay per request)\n",
    "   - 99.99% availability SLA\n",
    "   - Global tables for multi-region\n",
    "   \n",
    "   Cassandra (High Scale):\n",
    "   - Distributed NoSQL database\n",
    "   - Linear scalability (add nodes)\n",
    "   - Multi-datacenter replication\n",
    "   - Tunable consistency\n",
    "   - Ideal for >1M writes/sec\n",
    "   \n",
    "   Data Model:\n",
    "   Key: entity_type:entity_id (e.g., device:DEV00123)\n",
    "   Value: JSON with all features {vdd: 1.2, idd: 250, ...}\n",
    "   \n",
    "   Example (Redis):\n",
    "   # Write (streaming pipeline)\n",
    "   redis.set(\n",
    "       \"device:DEV00123\",\n",
    "       json.dumps({\"vdd\": 1.2, \"idd\": 250, \"freq\": 2.5}),\n",
    "       ex=86400  # 24-hour TTL\n",
    "   )\n",
    "   \n",
    "   # Read (inference service)\n",
    "   features_json = redis.get(\"device:DEV00123\")\n",
    "   features = json.loads(features_json)\n",
    "   \n",
    "   Caching Strategy:\n",
    "   - Hot entities: Cache in application memory (Redis + local cache)\n",
    "   - Cold entities: Fetch from Redis on-demand\n",
    "   - TTL: Match feature update frequency (hourly, daily)\n",
    "   \n",
    "   Deployment Pattern:\n",
    "   Internet ‚Üí Load Balancer ‚Üí Inference Service ‚Üí Redis Cluster\n",
    "                                     ‚Üì\n",
    "                                 ML Model\n",
    "   \n",
    "   Monitoring:\n",
    "   - Latency (p50, p95, p99)\n",
    "   - Cache hit rate\n",
    "   - Redis memory usage\n",
    "   - Feature staleness (time since last update)\n",
    "\n",
    "\n",
    "8. FEATURE MONITORING & DRIFT DETECTION\n",
    "   -------------------------------------\n",
    "   \n",
    "   What to Monitor:\n",
    "   \n",
    "   1. Feature Freshness:\n",
    "   - Time since last feature update\n",
    "   - Alert if >2x expected update interval\n",
    "   - Example: Daily features not updated in 36 hours\n",
    "   \n",
    "   2. Feature Quality:\n",
    "   - Null rate (<10% threshold)\n",
    "   - Zero variance (constant features)\n",
    "   - Outliers (>3 std from mean)\n",
    "   - Data type violations\n",
    "   \n",
    "   3. Feature Drift:\n",
    "   - Statistical tests: KS test, PSI, Chi-square\n",
    "   - Distribution shift (mean, std, percentiles)\n",
    "   - Alert if PSI >0.2 (major drift)\n",
    "   \n",
    "   4. Feature Importance Drift:\n",
    "   - Track feature importance over time\n",
    "   - Alert if top 5 features change significantly\n",
    "   - Indicates concept drift or data shift\n",
    "   \n",
    "   Monitoring Implementation:\n",
    "   \n",
    "   class FeatureMonitor:\n",
    "       def __init__(self, baseline_stats):\n",
    "           self.baseline = baseline_stats\n",
    "       \n",
    "       def check_drift(self, current_features):\n",
    "           alerts = []\n",
    "           \n",
    "           # 1. Freshness\n",
    "           if current_features['last_update'] > threshold:\n",
    "               alerts.append(\"STALE: Features not updated\")\n",
    "           \n",
    "           # 2. Quality\n",
    "           null_rate = current_features.isnull().mean()\n",
    "           if null_rate > 0.1:\n",
    "               alerts.append(f\"QUALITY: {null_rate:.1%} nulls\")\n",
    "           \n",
    "           # 3. Statistical drift\n",
    "           psi = calculate_psi(self.baseline['vdd'], current_features['vdd'])\n",
    "           if psi > 0.2:\n",
    "               alerts.append(f\"DRIFT: PSI={psi:.3f} for vdd\")\n",
    "           \n",
    "           return alerts\n",
    "   \n",
    "   Dashboard Metrics:\n",
    "   - Feature availability (% entities with features)\n",
    "   - Feature completeness (% non-null values)\n",
    "   - Feature drift score (PSI, KS statistic)\n",
    "   - Feature correlation (to detect redundancy)\n",
    "   - Feature usage (which models use which features)\n",
    "   \n",
    "   Alerting Rules:\n",
    "   - Severity 1 (Page): Online store down, >50% null rate\n",
    "   - Severity 2 (Alert): Major drift (PSI >0.2), stale features\n",
    "   - Severity 3 (Warning): Minor drift (PSI 0.1-0.2), quality issues\n",
    "   \n",
    "   Automated Retraining:\n",
    "   if feature_drift_detected() or performance_degraded():\n",
    "       trigger_retraining_pipeline()\n",
    "       wait_for_new_model()\n",
    "       if new_model_better():\n",
    "           deploy_new_model()\n",
    "       else:\n",
    "           rollback_features()\n",
    "\n",
    "\n",
    "9. FEATURE STORE BEST PRACTICES\n",
    "   -----------------------------\n",
    "   \n",
    "   Design Principles:\n",
    "   \n",
    "   1. Feature Reusability:\n",
    "   - Define features once, use across all models\n",
    "   - Avoid model-specific feature definitions\n",
    "   - Create feature libraries by domain (device, wafer, lot)\n",
    "   \n",
    "   2. Clear Naming Conventions:\n",
    "   - Prefix with entity: device_vdd, lot_mean_idd\n",
    "   - Suffix with aggregation: vdd_rolling_7d_mean\n",
    "   - Version suffix: device_features_v2\n",
    "   \n",
    "   3. Feature Documentation:\n",
    "   - Description: What the feature represents\n",
    "   - Computation: How it's calculated (SQL, Python)\n",
    "   - Business meaning: Why it's useful\n",
    "   - Owner: Team responsible for feature\n",
    "   - SLA: Update frequency, freshness requirements\n",
    "   \n",
    "   4. Separation of Concerns:\n",
    "   - Raw features: Minimal transformation\n",
    "   - Derived features: Domain-specific logic\n",
    "   - Model features: Model-specific transformations\n",
    "   - Keep raw ‚Üí derived ‚Üí model hierarchy\n",
    "   \n",
    "   5. Idempotency:\n",
    "   - Feature computation should be deterministic\n",
    "   - Same input ‚Üí same output (reproducibility)\n",
    "   - No random seeds, no current_time() in features\n",
    "   \n",
    "   6. Incremental Computation:\n",
    "   - Only recompute changed features (not all)\n",
    "   - Use watermarks to track processed data\n",
    "   - Saves compute costs, reduces latency\n",
    "   \n",
    "   7. Cost Optimization:\n",
    "   - TTL: Delete old features (save storage)\n",
    "   - Compression: Use Parquet, columnar formats\n",
    "   - Materialization schedule: Daily vs hourly vs real-time\n",
    "   - Caching: Hot features in memory, cold on disk\n",
    "   \n",
    "   Code Quality:\n",
    "   \n",
    "   # Good: Clear, reusable, documented\n",
    "   @feature(name=\"device_power\", description=\"Power consumption in mW\")\n",
    "   def compute_power(vdd, idd):\n",
    "       \\\"\\\"\\\"\n",
    "       Calculate device power.\n",
    "       \n",
    "       Args:\n",
    "           vdd: Voltage (V)\n",
    "           idd: Current (mA)\n",
    "       \n",
    "       Returns:\n",
    "           Power in milliwatts\n",
    "       \\\"\\\"\\\"\n",
    "       return vdd * idd\n",
    "   \n",
    "   # Bad: Unclear, hardcoded, undocumented\n",
    "   def f(x, y):\n",
    "       return x * y * 1000  # Why 1000?\n",
    "\n",
    "\n",
    "10. COMMON PITFALLS & SOLUTIONS\n",
    "    ----------------------------\n",
    "    \n",
    "    Pitfall 1: Training-Serving Skew\n",
    "    Problem: Features computed differently in training vs serving\n",
    "    Example: Training uses SQL AVG(), serving uses Python np.mean()\n",
    "    Solution: Use feature store for both (guaranteed consistency)\n",
    "    \n",
    "    Pitfall 2: Data Leakage (Future Data)\n",
    "    Problem: Training uses features from after event timestamp\n",
    "    Example: Lot average includes devices tested later\n",
    "    Solution: Use point-in-time joins (feature store automatic)\n",
    "    \n",
    "    Pitfall 3: Null Handling Inconsistency\n",
    "    Problem: Training fills nulls with 0, serving fills with mean\n",
    "    Solution: Document null strategy, enforce in feature definition\n",
    "    \n",
    "    Pitfall 4: Feature Staleness (Online Store)\n",
    "    Problem: Online features not updated, model uses old data\n",
    "    Example: User features from 2 weeks ago\n",
    "    Solution: Monitor freshness, set TTL, alert on stale features\n",
    "    \n",
    "    Pitfall 5: Over-Engineering Features\n",
    "    Problem: 1000+ features, most unused, high maintenance cost\n",
    "    Solution: Start simple, add features based on experiments\n",
    "    \n",
    "    Pitfall 6: Ignoring Feature Drift\n",
    "    Problem: Features change distribution, model accuracy drops\n",
    "    Solution: Monitor drift (PSI, KS test), retrain when detected\n",
    "    \n",
    "    Pitfall 7: Poor Performance (Slow Queries)\n",
    "    Problem: Offline queries take hours, blocking training\n",
    "    Solution: Partition data, use columnar formats, pre-aggregate\n",
    "    \n",
    "    Pitfall 8: No Feature Versioning\n",
    "    Problem: Can't reproduce past experiments, rollback impossible\n",
    "    Solution: Version feature definitions in Git, track in MLflow\n",
    "    \n",
    "    Pitfall 9: Single Point of Failure\n",
    "    Problem: Online store down = all predictions fail\n",
    "    Solution: Redis clustering, fallback to default features\n",
    "    \n",
    "    Pitfall 10: Ignoring Costs\n",
    "    Problem: Feature store costs $50K/month, mostly unused features\n",
    "    Solution: Set TTL, delete unused features, optimize storage\n",
    "\n",
    "\n",
    "11. WHEN TO USE A FEATURE STORE\n",
    "    ----------------------------\n",
    "    \n",
    "    Strong Indicators (Use Feature Store):\n",
    "    ‚úÖ Multiple models using same features (reuse)\n",
    "    ‚úÖ Both training and real-time serving (consistency critical)\n",
    "    ‚úÖ Large team (>5 data scientists, need collaboration)\n",
    "    ‚úÖ High model count (>10 models in production)\n",
    "    ‚úÖ Complex features (aggregations, temporal, spatial)\n",
    "    ‚úÖ Compliance requirements (lineage, reproducibility)\n",
    "    ‚úÖ Point-in-time correctness critical (finance, healthcare)\n",
    "    \n",
    "    Weak Indicators (Maybe Skip):\n",
    "    ‚ö†Ô∏è Single model, simple features (overhead not worth it)\n",
    "    ‚ö†Ô∏è Batch-only inference (no real-time serving)\n",
    "    ‚ö†Ô∏è Small team (<3 people, coordination overhead)\n",
    "    ‚ö†Ô∏è Proof-of-concept, short-lived project\n",
    "    ‚ö†Ô∏è Features change frequently (high churn)\n",
    "    \n",
    "    Alternatives to Feature Store:\n",
    "    \n",
    "    Simple Pipeline (Low Complexity):\n",
    "    - Jupyter notebook: Feature engineering\n",
    "    - Pickle file: Save training features\n",
    "    - Same notebook: Load for inference\n",
    "    - Works for: Single model, batch inference, small team\n",
    "    \n",
    "    Data Warehouse Only (Medium Complexity):\n",
    "    - BigQuery/Snowflake: Store features as tables\n",
    "    - dbt: Transform raw ‚Üí features (SQL)\n",
    "    - Cache in Redis: For serving (manual setup)\n",
    "    - Works for: Multiple models, but only batch inference\n",
    "    \n",
    "    Custom Feature Store (High Complexity):\n",
    "    - Build your own (like we did in this notebook)\n",
    "    - Works for: Specific constraints, learning purposes\n",
    "    - Risk: Maintenance burden, missing features\n",
    "    \n",
    "    When to Adopt Feast:\n",
    "    - 3+ models in production\n",
    "    - Both batch and real-time serving\n",
    "    - Team size >5 data scientists\n",
    "    - Need point-in-time correctness\n",
    "    - Want to avoid building/maintaining custom solution\n",
    "\n",
    "\n",
    "12. PRODUCTION READINESS CHECKLIST\n",
    "    --------------------------------\n",
    "    \n",
    "    Infrastructure:\n",
    "    ‚ñ° Offline store configured (Snowflake, BigQuery, S3)\n",
    "    ‚ñ° Online store deployed (Redis cluster, DynamoDB)\n",
    "    ‚ñ° Feature computation pipeline (Airflow, Spark)\n",
    "    ‚ñ° Monitoring dashboards (Grafana, Datadog)\n",
    "    ‚ñ° Alerting configured (PagerDuty, Slack)\n",
    "    \n",
    "    Features:\n",
    "    ‚ñ° Feature definitions documented\n",
    "    ‚ñ° Feature versioning strategy defined\n",
    "    ‚ñ° Feature ownership assigned (team/person)\n",
    "    ‚ñ° Feature SLAs documented (freshness, quality)\n",
    "    ‚ñ° Feature tests written (unit, integration)\n",
    "    \n",
    "    Data Quality:\n",
    "    ‚ñ° Null handling strategy defined\n",
    "    ‚ñ° Outlier detection configured\n",
    "    ‚ñ° Schema validation (Great Expectations)\n",
    "    ‚ñ° Data lineage tracked\n",
    "    ‚ñ° Backup and disaster recovery\n",
    "    \n",
    "    Performance:\n",
    "    ‚ñ° Online store latency <10ms p99\n",
    "    ‚ñ° Offline queries optimized (<5 min)\n",
    "    ‚ñ° Load testing completed (10K+ QPS)\n",
    "    ‚ñ° Autoscaling configured\n",
    "    ‚ñ° Cost monitoring enabled\n",
    "    \n",
    "    Security:\n",
    "    ‚ñ° Access control (IAM, RBAC)\n",
    "    ‚ñ° Encryption at rest and in transit\n",
    "    ‚ñ° Audit logging enabled\n",
    "    ‚ñ° PII handling compliant (GDPR, etc.)\n",
    "    ‚ñ° Network isolation (VPC, firewall)\n",
    "    \n",
    "    Operations:\n",
    "    ‚ñ° Runbooks documented\n",
    "    ‚ñ° On-call rotation defined\n",
    "    ‚ñ° Incident response plan\n",
    "    ‚ñ° Rollback procedures tested\n",
    "    ‚ñ° Feature deprecation policy\n",
    "\n",
    "\n",
    "13. NEXT STEPS IN LEARNING PATH\n",
    "    -----------------------------\n",
    "    \n",
    "    After Feature Stores, continue with:\n",
    "    \n",
    "    125_ML_Testing_Validation.ipynb:\n",
    "    - Unit tests for feature transformations\n",
    "    - Integration tests for feature pipelines\n",
    "    - Validate feature store outputs\n",
    "    \n",
    "    126_CI_CD_for_ML.ipynb:\n",
    "    - Automate feature materialization\n",
    "    - Feature store deployment pipeline\n",
    "    - Feature registry updates in CI/CD\n",
    "    \n",
    "    127_Model_Serving_Patterns.ipynb:\n",
    "    - Integrate feature store with serving\n",
    "    - Online feature retrieval in REST API\n",
    "    - Caching strategies for features\n",
    "    \n",
    "    Advanced Topics:\n",
    "    - Stream processing (Kafka, Flink) for real-time features\n",
    "    - Feature embeddings (represent categories as vectors)\n",
    "    - Graph features (relationships, network analysis)\n",
    "    - AutoML for feature selection\n",
    "    - Feature engineering at scale (Spark, Ray)\n",
    "\n",
    "\n",
    "================================================================================\n",
    "FINAL SUMMARY\n",
    "================================================================================\n",
    "\n",
    "Feature Stores solve the critical problem of training-serving skew by ensuring\n",
    "features are computed identically for both model training and production\n",
    "inference. They provide:\n",
    "\n",
    "1. Consistency: Same features in training and serving\n",
    "2. Reusability: Define once, use across all models\n",
    "3. Point-in-Time Correctness: No data leakage from future\n",
    "4. Low-Latency Serving: <10ms feature retrieval\n",
    "5. Versioning: Reproducible experiments, safe rollback\n",
    "6. Monitoring: Drift detection, quality checks\n",
    "\n",
    "Feast is the production standard (open source, battle-tested at Uber/Shopify).\n",
    "\n",
    "Start simple, add complexity as needed. Not every project needs a feature\n",
    "store, but for multi-model production systems with real-time serving, it's\n",
    "essential infrastructure.\n",
    "\n",
    "Key metric: Training-serving consistency. If model accuracy is 95% in training\n",
    "but 85% in production, investigate training-serving skew first (likely feature\n",
    "computation differences).\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(takeaways)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ea808",
   "metadata": {},
   "source": [
    "## 12. Key Takeaways - Feature Store Mastery\n",
    "\n",
    "Comprehensive reference guide for production feature store implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9bc0e0",
   "metadata": {},
   "source": [
    "## 11. Real-World Project Templates\n",
    "\n",
    "**8 production-ready feature store projects** (4 post-silicon + 4 general AI/ML).\n",
    "\n",
    "Each project includes:\n",
    "- Clear objective and success criteria\n",
    "- Feature store architecture\n",
    "- Feature engineering strategy\n",
    "- Business value and impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aeb255",
   "metadata": {},
   "source": [
    "## 10. Feature Store Monitoring & Validation\n",
    "\n",
    "**What's happening:** Ensuring feature quality and detecting feature drift.\n",
    "\n",
    "**Key points:**\n",
    "- **Feature freshness**: Alert if features haven't been updated within SLA\n",
    "- **Feature drift**: Detect distribution shifts in feature values\n",
    "- **Feature quality**: Validate nulls, outliers, data types\n",
    "- **Performance impact**: Monitor how feature changes affect model accuracy\n",
    "\n",
    "**Production monitoring:** Daily jobs check feature statistics, alert on anomalies, trigger retraining if drift detected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb00af9",
   "metadata": {},
   "source": [
    "## 9. Feast Framework - Production Feature Store\n",
    "\n",
    "**What's happening:** Introduction to Feast, the leading open-source feature store.\n",
    "\n",
    "**Key points:**\n",
    "- **Feast components**: Feature registry, offline store (BigQuery/Snowflake), online store (Redis/DynamoDB)\n",
    "- **Feature definitions**: Entity, data source, feature view (Python files)\n",
    "- **CLI commands**: feast init, feast apply, feast materialize\n",
    "- **SDK**: Python API for feature retrieval\n",
    "\n",
    "**Production setup:** Feast manages feature versioning, point-in-time joins, and online/offline consistency automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fe5d9a",
   "metadata": {},
   "source": [
    "## 8. Real-Time Inference with Online Features\n",
    "\n",
    "**What's happening:** Production inference using features from online store.\n",
    "\n",
    "**Key points:**\n",
    "- **Low latency**: Feature retrieval <10ms, total inference <50ms\n",
    "- **Feature consistency**: Identical to training features (no skew)\n",
    "- **Entity lookup**: Retrieve pre-computed features by device_id\n",
    "- **Prediction**: Apply trained model to online features\n",
    "\n",
    "**Production workflow:** Test handler ‚Üí online feature store ‚Üí model inference ‚Üí bin decision ‚Üí tester."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ac5dee",
   "metadata": {},
   "source": [
    "## 7. Online Feature Serving - Real-Time Inference\n",
    "\n",
    "**What's happening:** Populating online store for low-latency feature retrieval during production inference.\n",
    "\n",
    "**Key points:**\n",
    "- **Online store**: Redis, DynamoDB, Cassandra (key-value, <10ms latency)\n",
    "- **Feature freshness**: Continuously updated from streaming pipeline\n",
    "- **Entity-based lookup**: Retrieve features by device_id, customer_id, etc.\n",
    "- **Precomputed features**: No computation at inference time\n",
    "\n",
    "**Production scenario:** Production tester sends device_id ‚Üí feature store returns features ‚Üí model predicts bin in <50ms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9310c6d",
   "metadata": {},
   "source": [
    "## 6. Model Training with Feature Store Features\n",
    "\n",
    "**What's happening:** Training yield prediction model using features from feature store.\n",
    "\n",
    "**Key points:**\n",
    "- **Feature consistency**: Same features used for training and serving\n",
    "- **Feature selection**: Choose relevant features from store\n",
    "- **Model performance**: Baseline for comparison with production\n",
    "- **Feature importance**: Identify most predictive features\n",
    "\n",
    "**Why this matters:** Model trained on feature store features will match production performance (no training-serving skew)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650225a1",
   "metadata": {},
   "source": [
    "## 5. Offline Feature Materialization - Training Dataset\n",
    "\n",
    "**What's happening:** Computing and storing historical features for model training.\n",
    "\n",
    "**Key points:**\n",
    "- **Batch computation**: Process all historical data at once\n",
    "- **Point-in-time correctness**: Features valid at specific timestamps\n",
    "- **Storage**: Data warehouse, data lake, or file storage\n",
    "- **Training dataset**: Join features to labels for supervised learning\n",
    "\n",
    "**Production workflow:** Airflow/Kubeflow schedules daily batch jobs to materialize features from STDF data warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f14a0d",
   "metadata": {},
   "source": [
    "## 4. Feature Definitions - Register Features in Store\n",
    "\n",
    "**What's happening:** Defining feature transformations and registering them in the feature store.\n",
    "\n",
    "**Key points:**\n",
    "- **Feature function**: Transform raw data ‚Üí feature values\n",
    "- **Metadata**: Name, description, type, computation logic\n",
    "- **Registry**: Central catalog of all available features\n",
    "\n",
    "**Production note:** In Feast, features are defined in Python feature definitions files with entity, data source, and feature view specifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7231b",
   "metadata": {},
   "source": [
    "## 3. Post-Silicon Feature Engineering - Device Characterization\n",
    "\n",
    "**What's happening:** Creating device features from STDF test data for yield prediction.\n",
    "\n",
    "**Key points:**\n",
    "- **Raw parameters**: Vdd, Idd, Freq, Temp (direct measurements)\n",
    "- **Aggregate features**: Statistical summaries (mean, std, percentiles)\n",
    "- **Derived features**: Ratios, power calculations, Z-scores\n",
    "- **Temporal features**: Test time, sequence effects\n",
    "\n",
    "**Why this matters:** Consistent feature definitions ensure yield model trained on historical data performs accurately on production devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d1165",
   "metadata": {},
   "source": [
    "## 2. Feature Store Fundamentals - Building Blocks\n",
    "\n",
    "**What's happening:** Understanding the core concepts before implementation.\n",
    "\n",
    "**Key points:**\n",
    "- **Feature Definition**: Schema, data types, transformation logic\n",
    "- **Offline vs Online**: Historical data (training) vs low-latency (serving)\n",
    "- **Point-in-Time Correctness**: Prevents data leakage from future\n",
    "- **Feature Versioning**: Track changes, enable reproducibility\n",
    "\n",
    "**Post-silicon context:** Device features must be identical between model training and production test binning to avoid skew."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
