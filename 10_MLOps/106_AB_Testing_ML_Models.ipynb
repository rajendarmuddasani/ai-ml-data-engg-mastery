{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1aa9aae",
   "metadata": {},
   "source": [
    "# 106: A/B Testing for ML Models\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** statistical foundations of A/B testing: hypothesis testing, p-values, statistical power\n",
    "- **Implement** online A/B tests comparing model variants in production\n",
    "- **Build** multi-armed bandit strategies for adaptive experimentation\n",
    "- **Apply** A/B testing to semiconductor yield prediction model deployments\n",
    "- **Evaluate** test duration, sample size requirements, and early stopping criteria\n",
    "\n",
    "## üìö What is A/B Testing for ML Models?\n",
    "\n",
    "A/B testing for machine learning validates whether a new model actually performs better than the current production model under real-world conditions. Unlike offline evaluation on test sets, A/B testing exposes both models to live data simultaneously, randomly routing traffic between them while measuring business metrics. This reveals issues invisible in offline testing: data distribution shifts, user behavior changes, system integration bugs, and actual business impact.\n",
    "\n",
    "Traditional A/B testing compares static variants (e.g., blue button vs red button). ML model A/B testing is more complex because models are non-deterministic, predictions interact with downstream systems, and metrics may have high variance or delayed feedback. A rigorous A/B test requires proper randomization, sufficient statistical power, guardrail metrics (to catch regressions), and clear success criteria agreed upon before deployment.\n",
    "\n",
    "In semiconductor manufacturing, A/B testing validates whether new yield prediction models, test time optimizations, or binning algorithms actually improve KPIs (yield, cost, quality) without introducing unexpected failures. For example, a model may show 95% accuracy offline but cause 10% more false rejects in production due to calibration drift‚ÄîA/B testing catches this before full rollout.\n",
    "\n",
    "**Why A/B Testing for ML Models?**\n",
    "- ‚úÖ **Validation**: Offline metrics (R¬≤, AUC) don't guarantee real-world improvement‚ÄîA/B tests measure actual impact\n",
    "- ‚úÖ **Risk Mitigation**: Gradual rollout (5% ‚Üí 50% ‚Üí 100%) limits blast radius if new model fails\n",
    "- ‚úÖ **Causal Inference**: Randomization ensures performance differences are due to model, not confounders\n",
    "- ‚úÖ **Business Metrics**: Test what matters (revenue, cost, yield) not just ML metrics (RMSE, accuracy)\n",
    "- ‚úÖ **Continuous Improvement**: Culture of experimentation enables rapid model iteration\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Use Case 1: Yield Prediction Model Upgrade**\n",
    "- **Setup**: A = Current Random Forest (R¬≤=0.88), B = New XGBoost (R¬≤=0.92 offline)\n",
    "- **Metric**: False reject rate (devices incorrectly predicted to fail)\n",
    "- **Test**: Route 50% of lots to each model for 2 weeks (n=200 lots)\n",
    "- **Result**: Model B reduces false rejects 15% (p=0.003), saves $500K/month in unnecessary scrapping\n",
    "- **Decision**: Roll out Model B to 100% of production\n",
    "\n",
    "**Use Case 2: Adaptive Test Insertion Algorithm**\n",
    "- **Setup**: A = Fixed test sequence, B = ML-driven adaptive testing (skip low-risk tests)\n",
    "- **Metrics**: Primary = Test time, Guardrail = Defect escape rate\n",
    "- **Test**: Multi-armed bandit with Thompson sampling, 10K devices\n",
    "- **Result**: Model B reduces test time 28% BUT defect escapes increase 2% ‚Üí REJECT Model B\n",
    "- **Decision**: Retrain Model B with stricter safety constraints, re-test\n",
    "\n",
    "**Use Case 3: Wafer Map Defect Classifier**\n",
    "- **Setup**: A = Rule-based classifier, B = CNN-based AutoML model\n",
    "- **Metric**: Correct defect type identification (validated by engineers)\n",
    "- **Test**: Parallel deployment, engineers label 500 wafer maps for ground truth\n",
    "- **Result**: Model B achieves 94% accuracy vs 78% for Model A (p<0.001)\n",
    "- **Decision**: Deploy Model B, decommission rule-based system\n",
    "\n",
    "**Use Case 4: Binning Algorithm Optimization**\n",
    "- **Setup**: A = Manual binning rules, B = Data-driven ML binning\n",
    "- **Metrics**: Primary = BIN1 yield (premium), Guardrail = Customer returns <0.1%\n",
    "- **Test**: A/A test first (validate infrastructure), then A/B for 4 weeks\n",
    "- **Result**: Model B increases BIN1 yield 6% with zero return rate increase (p=0.012)\n",
    "- **Value**: $3M additional quarterly revenue from premium bin optimization\n",
    "\n",
    "## üîÑ A/B Testing Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[New Model Candidate] --> B[Offline Evaluation]\n",
    "    B --> C{Passes Threshold?}\n",
    "    \n",
    "    C -->|No| D[Reject Model]\n",
    "    C -->|Yes| E[Define Success Metrics]\n",
    "    \n",
    "    E --> F[Power Analysis]\n",
    "    F --> G[Calculate Sample Size]\n",
    "    G --> H[Design Experiment]\n",
    "    \n",
    "    H --> I[A/A Test]\n",
    "    I --> J{Infrastructure OK?}\n",
    "    J -->|No| K[Fix Bias]\n",
    "    K --> I\n",
    "    \n",
    "    J -->|Yes| L[A/B Test Launch]\n",
    "    L --> M[Traffic Splitting]\n",
    "    \n",
    "    M --> N[Control: Model A]\n",
    "    M --> O[Treatment: Model B]\n",
    "    \n",
    "    N --> P[Monitor Metrics]\n",
    "    O --> P\n",
    "    \n",
    "    P --> Q{Guardrails OK?}\n",
    "    Q -->|No| R[Emergency Stop]\n",
    "    R --> D\n",
    "    \n",
    "    Q -->|Yes| S{Significant Result?}\n",
    "    S -->|Not Yet| T{Budget Exhausted?}\n",
    "    T -->|No| P\n",
    "    T -->|Yes| U[Inconclusive]\n",
    "    \n",
    "    S -->|Yes, B Better| V[Gradual Rollout]\n",
    "    V --> W[100% Traffic to B]\n",
    "    \n",
    "    S -->|Yes, A Better| D\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style W fill:#e1ffe1\n",
    "    style R fill:#ffe1e1\n",
    "    style D fill:#ffe1e1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **041**: Model Evaluation - Understanding offline metrics\n",
    "- **104**: Model Interpretability - Debugging model differences\n",
    "- **105**: AutoML - Generating candidate models to test\n",
    "\n",
    "**This Notebook (106):**\n",
    "- Hypothesis testing fundamentals (t-tests, chi-square)\n",
    "- Sample size and statistical power calculations\n",
    "- A/B test implementation (traffic splitting, metric collection)\n",
    "- Multi-armed bandits (Thompson sampling, UCB)\n",
    "- Sequential testing and early stopping\n",
    "\n",
    "**Next Steps:**\n",
    "- **107**: Model Monitoring - Continuous performance tracking post-deployment\n",
    "- **131**: Cloud Deployment - Production infrastructure for A/B testing at scale\n",
    "\n",
    "---\n",
    "\n",
    "Let's test models the right way‚Äîwith real data and real impact! üìä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938605b",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind, chi2_contingency, norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Environment ready for A/B testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966af78e",
   "metadata": {},
   "source": [
    "## 2. Generate Semiconductor Production Data\n",
    "\n",
    "**Purpose:** Simulate production environment for A/B testing.\n",
    "\n",
    "**Key Points:**\n",
    "- **Realistic variance**: Production data has more noise than offline test sets\n",
    "- **Time dependency**: Sequential lots with temporal patterns\n",
    "- **Business metrics**: False rejects (cost), false accepts (quality risk)\n",
    "- **Why this matters**: A/B tests must handle real-world variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate 500 production lots tested over time\n",
    "n_lots = 500\n",
    "devices_per_lot = 100\n",
    "\n",
    "# Time-based patterns (production drift)\n",
    "time = np.arange(n_lots)\n",
    "drift = 0.02 * np.sin(2 * np.pi * time / 100)  # Seasonal drift\n",
    "\n",
    "# Generate lot-level features\n",
    "lot_data = []\n",
    "for lot_id in range(n_lots):\n",
    "    # Parametric measurements with drift\n",
    "    vdd = np.random.normal(1.2 + drift[lot_id], 0.08, devices_per_lot)\n",
    "    idd = np.random.normal(50 + drift[lot_id] * 10, 8, devices_per_lot)\n",
    "    freq = np.random.normal(2000, 150, devices_per_lot)\n",
    "    temp = np.random.normal(85, 12, devices_per_lot)\n",
    "    vth = np.random.normal(0.4 + drift[lot_id], 0.03, devices_per_lot)\n",
    "    \n",
    "    # True yield (unknown in production)\n",
    "    power = vdd * idd\n",
    "    true_yield = (\n",
    "        100 - 0.35 * power + 12 * vth - 0.01 * temp * freq / 1000\n",
    "        + np.random.normal(0, 3, devices_per_lot)\n",
    "    )\n",
    "    true_yield = np.clip(true_yield, 60, 100)\n",
    "    \n",
    "    # Pass/fail labels (yield > 85 = pass)\n",
    "    pass_fail = (true_yield > 85).astype(int)\n",
    "    \n",
    "    # Store lot-level aggregates\n",
    "    lot_data.append({\n",
    "        'lot_id': lot_id,\n",
    "        'time': lot_id,\n",
    "        'avg_vdd': vdd.mean(),\n",
    "        'avg_idd': idd.mean(),\n",
    "        'avg_freq': freq.mean(),\n",
    "        'avg_temp': temp.mean(),\n",
    "        'avg_vth': vth.mean(),\n",
    "        'true_yield_pct': pass_fail.mean() * 100,\n",
    "        'devices': devices_per_lot\n",
    "    })\n",
    "\n",
    "df_production = pd.DataFrame(lot_data)\n",
    "\n",
    "print(f\"Production dataset: {len(df_production)} lots, {devices_per_lot} devices/lot\")\n",
    "print(f\"\\nYield statistics:\")\n",
    "print(df_production['true_yield_pct'].describe())\n",
    "print(f\"\\nTemporal drift range: {drift.min():.4f} to {drift.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4522592b",
   "metadata": {},
   "source": [
    "## 3. Train Two Model Variants\n",
    "\n",
    "**Purpose:** Create Model A (baseline) and Model B (new candidate) for comparison.\n",
    "\n",
    "**Key Points:**\n",
    "- **Model A**: Current production model (Random Forest)\n",
    "- **Model B**: New candidate (Gradient Boosting)\n",
    "- **Offline metrics**: B appears better, but does it hold in production?\n",
    "- **Why this matters**: Offline superiority ‚â† production superiority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0595aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data (first 300 lots)\n",
    "train_df = df_production.iloc[:300].copy()\n",
    "test_df = df_production.iloc[300:].copy()\n",
    "\n",
    "feature_cols = ['avg_vdd', 'avg_idd', 'avg_freq', 'avg_temp', 'avg_vth']\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['true_yield_pct']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['true_yield_pct']\n",
    "\n",
    "# Model A: Random Forest (current production)\n",
    "model_a = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "model_a.fit(X_train, y_train)\n",
    "y_pred_a = model_a.predict(X_test)\n",
    "rmse_a = np.sqrt(mean_squared_error(y_test, y_pred_a))\n",
    "r2_a = r2_score(y_test, y_pred_a)\n",
    "\n",
    "# Model B: Gradient Boosting (new candidate)\n",
    "model_b = GradientBoostingRegressor(n_estimators=150, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "model_b.fit(X_train, y_train)\n",
    "y_pred_b = model_b.predict(X_test)\n",
    "rmse_b = np.sqrt(mean_squared_error(y_test, y_pred_b))\n",
    "r2_b = r2_score(y_test, y_pred_b)\n",
    "\n",
    "print(\"Offline Evaluation (on test set):\")\n",
    "print(f\"\\nModel A (Random Forest):\")\n",
    "print(f\"  RMSE: {rmse_a:.3f}%\")\n",
    "print(f\"  R¬≤: {r2_a:.4f}\")\n",
    "\n",
    "print(f\"\\nModel B (Gradient Boosting):\")\n",
    "print(f\"  RMSE: {rmse_b:.3f}%\")\n",
    "print(f\"  R¬≤: {r2_b:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä Offline Comparison:\")\n",
    "print(f\"  Model B RMSE improvement: {((rmse_a - rmse_b) / rmse_a * 100):.1f}%\")\n",
    "print(f\"  Model B R¬≤ improvement: {(r2_b - r2_a):.4f}\")\n",
    "print(f\"\\n‚ùì Question: Will this offline improvement translate to production?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88717fc4",
   "metadata": {},
   "source": [
    "## 4. Statistical Power Analysis\n",
    "\n",
    "**Concept:** Calculate required sample size for detecting meaningful differences.\n",
    "\n",
    "**Mathematics:**\n",
    "$$n = \\frac{2(Z_{\\alpha/2} + Z_{\\beta})^2 \\sigma^2}{\\delta^2}$$\n",
    "\n",
    "Where:\n",
    "- $n$ = sample size per group\n",
    "- $Z_{\\alpha/2}$ = critical value for significance level (1.96 for Œ±=0.05)\n",
    "- $Z_{\\beta}$ = critical value for power (0.84 for 80% power)\n",
    "- $\\sigma$ = standard deviation\n",
    "- $\\delta$ = minimum detectable effect\n",
    "\n",
    "**Why critical:** Underpowered tests waste time, overpowered tests waste resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e364c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(baseline_std, min_effect, alpha=0.05, power=0.80):\n",
    "    \"\"\"\n",
    "    Calculate required sample size for two-sample t-test.\n",
    "    \n",
    "    Parameters:\n",
    "    - baseline_std: Standard deviation of metric\n",
    "    - min_effect: Minimum effect size to detect (absolute units)\n",
    "    - alpha: Significance level (Type I error rate)\n",
    "    - power: Statistical power (1 - Type II error rate)\n",
    "    \"\"\"\n",
    "    z_alpha = norm.ppf(1 - alpha/2)\n",
    "    z_beta = norm.ppf(power)\n",
    "    \n",
    "    n = 2 * ((z_alpha + z_beta) ** 2) * (baseline_std ** 2) / (min_effect ** 2)\n",
    "    \n",
    "    return int(np.ceil(n))\n",
    "\n",
    "# Power analysis for our A/B test\n",
    "baseline_std = y_test.std()  # Variance in yield\n",
    "min_effect = 2.0  # Want to detect 2% yield difference\n",
    "\n",
    "sample_size = calculate_sample_size(baseline_std, min_effect)\n",
    "\n",
    "print(\"Statistical Power Analysis:\")\n",
    "print(f\"\\nParameters:\")\n",
    "print(f\"  Baseline std: {baseline_std:.2f}%\")\n",
    "print(f\"  Minimum detectable effect: {min_effect:.1f}%\")\n",
    "print(f\"  Significance level (Œ±): 0.05\")\n",
    "print(f\"  Statistical power (1-Œ≤): 0.80\")\n",
    "\n",
    "print(f\"\\nRequired sample size: {sample_size} lots per group\")\n",
    "print(f\"Total lots needed: {sample_size * 2}\")\n",
    "\n",
    "# Test duration estimate\n",
    "lots_per_day = 10\n",
    "test_days = (sample_size * 2) / lots_per_day\n",
    "print(f\"\\nEstimated test duration: {test_days:.1f} days (at {lots_per_day} lots/day)\")\n",
    "\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "print(f\"  Need {sample_size} lots in each group to detect {min_effect}% yield difference\")\n",
    "print(f\"  With 80% probability (power) and 5% false positive rate (Œ±)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181267b9",
   "metadata": {},
   "source": [
    "## 5. Simulate A/B Test Execution\n",
    "\n",
    "**Purpose:** Randomly assign production lots to Model A vs Model B.\n",
    "\n",
    "**Key Points:**\n",
    "- **Random assignment**: Coin flip for each lot ensures unbiased comparison\n",
    "- **Business metric**: False reject rate (predicted fail, actually pass)\n",
    "- **Guardrail metric**: False accept rate (predicted pass, actually fail)\n",
    "- **Why this matters**: Real A/B tests track multiple metrics simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e66aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use production lots 300-500 for A/B test (200 lots available)\n",
    "ab_test_df = df_production.iloc[300:].copy()\n",
    "n_ab_lots = len(ab_test_df)\n",
    "\n",
    "# Random assignment (50/50 split)\n",
    "np.random.seed(42)\n",
    "ab_test_df['variant'] = np.random.choice(['A', 'B'], size=n_ab_lots)\n",
    "\n",
    "# Make predictions for each lot based on assignment\n",
    "predictions_a = model_a.predict(ab_test_df[feature_cols])\n",
    "predictions_b = model_b.predict(ab_test_df[feature_cols])\n",
    "\n",
    "ab_test_df['predicted_yield'] = np.where(\n",
    "    ab_test_df['variant'] == 'A',\n",
    "    predictions_a,\n",
    "    predictions_b\n",
    ")\n",
    "\n",
    "# Business metrics (using 85% threshold)\n",
    "ab_test_df['predicted_pass'] = (ab_test_df['predicted_yield'] > 85).astype(int)\n",
    "ab_test_df['actual_pass'] = (ab_test_df['true_yield_pct'] > 85).astype(int)\n",
    "\n",
    "# False rejects: Predicted fail, actually pass (COSTLY - we reject good devices)\n",
    "ab_test_df['false_reject'] = (\n",
    "    (ab_test_df['predicted_pass'] == 0) & (ab_test_df['actual_pass'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "# False accepts: Predicted pass, actually fail (RISKY - quality escapes)\n",
    "ab_test_df['false_accept'] = (\n",
    "    (ab_test_df['predicted_pass'] == 1) & (ab_test_df['actual_pass'] == 0)\n",
    ").astype(int)\n",
    "\n",
    "print(\"A/B Test Setup:\")\n",
    "print(f\"\\nTotal lots: {n_ab_lots}\")\n",
    "print(f\"  Variant A: {(ab_test_df['variant'] == 'A').sum()} lots\")\n",
    "print(f\"  Variant B: {(ab_test_df['variant'] == 'B').sum()} lots\")\n",
    "\n",
    "print(f\"\\nRandomization check (should be ~50/50):\")\n",
    "print(f\"  A: {(ab_test_df['variant'] == 'A').sum() / n_ab_lots * 100:.1f}%\")\n",
    "print(f\"  B: {(ab_test_df['variant'] == 'B').sum() / n_ab_lots * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398963b7",
   "metadata": {},
   "source": [
    "## 6. Analyze A/B Test Results\n",
    "\n",
    "**Purpose:** Statistical comparison of Model A vs Model B performance.\n",
    "\n",
    "**Key Points:**\n",
    "- **Primary metric**: False reject rate (cost reduction)\n",
    "- **Guardrail metric**: False accept rate (quality protection)\n",
    "- **Statistical test**: Two-proportion z-test\n",
    "- **Why this matters**: Need statistically significant improvement to justify deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics by variant\n",
    "results_a = ab_test_df[ab_test_df['variant'] == 'A']\n",
    "results_b = ab_test_df[ab_test_df['variant'] == 'B']\n",
    "\n",
    "# Primary metric: False reject rate\n",
    "fr_rate_a = results_a['false_reject'].mean() * 100\n",
    "fr_rate_b = results_b['false_reject'].mean() * 100\n",
    "\n",
    "# Guardrail metric: False accept rate\n",
    "fa_rate_a = results_a['false_accept'].mean() * 100\n",
    "fa_rate_b = results_b['false_accept'].mean() * 100\n",
    "\n",
    "# Prediction error (RMSE)\n",
    "rmse_prod_a = np.sqrt(mean_squared_error(\n",
    "    results_a['true_yield_pct'],\n",
    "    results_a['predicted_yield']\n",
    "))\n",
    "rmse_prod_b = np.sqrt(mean_squared_error(\n",
    "    results_b['true_yield_pct'],\n",
    "    results_b['predicted_yield']\n",
    "))\n",
    "\n",
    "print(\"A/B Test Results:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä PRIMARY METRIC: False Reject Rate\")\n",
    "print(f\"  Model A: {fr_rate_a:.2f}% ({results_a['false_reject'].sum()} / {len(results_a)} lots)\")\n",
    "print(f\"  Model B: {fr_rate_b:.2f}% ({results_b['false_reject'].sum()} / {len(results_b)} lots)\")\n",
    "print(f\"  Improvement: {fr_rate_a - fr_rate_b:.2f} percentage points\")\n",
    "print(f\"  Relative improvement: {((fr_rate_a - fr_rate_b) / fr_rate_a * 100):.1f}%\")\n",
    "\n",
    "print(f\"\\nüõ°Ô∏è GUARDRAIL METRIC: False Accept Rate\")\n",
    "print(f\"  Model A: {fa_rate_a:.2f}%\")\n",
    "print(f\"  Model B: {fa_rate_b:.2f}%\")\n",
    "print(f\"  Change: {fa_rate_b - fa_rate_a:+.2f} percentage points\")\n",
    "\n",
    "print(f\"\\nüìà RMSE (Prediction Accuracy)\")\n",
    "print(f\"  Model A: {rmse_prod_a:.3f}%\")\n",
    "print(f\"  Model B: {rmse_prod_b:.3f}%\")\n",
    "print(f\"  Improvement: {rmse_prod_a - rmse_prod_b:.3f}%\")\n",
    "\n",
    "# Statistical significance test for false reject rate\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "successes = np.array([results_a['false_reject'].sum(), results_b['false_reject'].sum()])\n",
    "samples = np.array([len(results_a), len(results_b)])\n",
    "\n",
    "z_stat, p_value = proportions_ztest(successes, samples)\n",
    "\n",
    "print(f\"\\nüìä Statistical Significance Test (False Reject Rate):\")\n",
    "print(f\"  Z-statistic: {z_stat:.3f}\")\n",
    "print(f\"  P-value: {p_value:.4f}\")\n",
    "print(f\"  Significance level: Œ± = 0.05\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\n  ‚úÖ SIGNIFICANT: Difference is statistically significant (p < 0.05)\")\n",
    "    print(f\"     Model B is reliably better than Model A\")\n",
    "else:\n",
    "    print(f\"\\n  ‚ùå NOT SIGNIFICANT: Difference could be due to chance (p >= 0.05)\")\n",
    "    print(f\"     Need more data or larger effect size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0424fcc",
   "metadata": {},
   "source": [
    "## 7. Visualize A/B Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: False reject rate comparison\n",
    "metrics = ['False Reject Rate (%)', 'False Accept Rate (%)']\n",
    "a_values = [fr_rate_a, fa_rate_a]\n",
    "b_values = [fr_rate_b, fa_rate_b]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(x - width/2, a_values, width, label='Model A', alpha=0.8, color='skyblue')\n",
    "axes[0, 0].bar(x + width/2, b_values, width, label='Model B', alpha=0.8, color='lightcoral')\n",
    "axes[0, 0].set_ylabel('Rate (%)')\n",
    "axes[0, 0].set_title('Business Metrics Comparison')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(metrics)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Prediction error distribution\n",
    "errors_a = results_a['true_yield_pct'] - results_a['predicted_yield']\n",
    "errors_b = results_b['true_yield_pct'] - results_b['predicted_yield']\n",
    "\n",
    "axes[0, 1].hist(errors_a, bins=20, alpha=0.6, label=f'Model A (œÉ={errors_a.std():.2f})', color='skyblue')\n",
    "axes[0, 1].hist(errors_b, bins=20, alpha=0.6, label=f'Model B (œÉ={errors_b.std():.2f})', color='lightcoral')\n",
    "axes[0, 1].axvline(0, color='black', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Prediction Error (%)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Error Distribution')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Cumulative false reject rate over time\n",
    "results_a_sorted = results_a.sort_values('time')\n",
    "results_b_sorted = results_b.sort_values('time')\n",
    "\n",
    "cumulative_fr_a = results_a_sorted['false_reject'].cumsum() / np.arange(1, len(results_a_sorted) + 1) * 100\n",
    "cumulative_fr_b = results_b_sorted['false_reject'].cumsum() / np.arange(1, len(results_b_sorted) + 1) * 100\n",
    "\n",
    "axes[1, 0].plot(cumulative_fr_a.values, label='Model A', linewidth=2, color='skyblue')\n",
    "axes[1, 0].plot(cumulative_fr_b.values, label='Model B', linewidth=2, color='lightcoral')\n",
    "axes[1, 0].set_xlabel('Lots Tested')\n",
    "axes[1, 0].set_ylabel('Cumulative False Reject Rate (%)')\n",
    "axes[1, 0].set_title('Sequential Test Monitoring')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Prediction scatter\n",
    "axes[1, 1].scatter(results_a['true_yield_pct'], results_a['predicted_yield'],\n",
    "                   alpha=0.5, s=30, label='Model A', color='skyblue')\n",
    "axes[1, 1].scatter(results_b['true_yield_pct'], results_b['predicted_yield'],\n",
    "                   alpha=0.5, s=30, label='Model B', color='lightcoral')\n",
    "axes[1, 1].plot([60, 100], [60, 100], 'k--', lw=2, label='Perfect prediction')\n",
    "axes[1, 1].axhline(85, color='red', linestyle=':', linewidth=1, label='Pass/Fail threshold')\n",
    "axes[1, 1].axvline(85, color='red', linestyle=':', linewidth=1)\n",
    "axes[1, 1].set_xlabel('Actual Yield (%)')\n",
    "axes[1, 1].set_ylabel('Predicted Yield (%)')\n",
    "axes[1, 1].set_title('Prediction Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0fcc44",
   "metadata": {},
   "source": [
    "## 8. Multi-Armed Bandit (Thompson Sampling)\n",
    "\n",
    "**Concept:** Adaptively allocate traffic to better-performing variant during test.\n",
    "\n",
    "**Mathematics (Beta-Bernoulli):**\n",
    "$$P(\\theta_A | \\text{data}) \\sim \\text{Beta}(\\alpha_A + s_A, \\beta_A + f_A)$$\n",
    "\n",
    "Where:\n",
    "- $s_A$ = successes (correct predictions)\n",
    "- $f_A$ = failures (incorrect predictions)\n",
    "- Sample from both distributions, route to higher sample\n",
    "\n",
    "**Advantage:** Minimizes regret (cost of testing inferior variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f02ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling:\n",
    "    def __init__(self, n_variants=2):\n",
    "        # Beta distribution parameters (prior: uniform)\n",
    "        self.alpha = np.ones(n_variants)  # Successes + 1\n",
    "        self.beta = np.ones(n_variants)   # Failures + 1\n",
    "        self.n_variants = n_variants\n",
    "        self.history = []\n",
    "        \n",
    "    def select_variant(self):\n",
    "        \"\"\"Sample from Beta distributions and select best.\"\"\"\n",
    "        samples = np.random.beta(self.alpha, self.beta)\n",
    "        selected = np.argmax(samples)\n",
    "        return selected, samples\n",
    "    \n",
    "    def update(self, variant, reward):\n",
    "        \"\"\"Update Beta parameters based on outcome.\"\"\"\n",
    "        if reward == 1:  # Success (correct prediction)\n",
    "            self.alpha[variant] += 1\n",
    "        else:  # Failure (incorrect prediction)\n",
    "            self.beta[variant] += 1\n",
    "            \n",
    "        self.history.append({\n",
    "            'variant': variant,\n",
    "            'reward': reward,\n",
    "            'alpha': self.alpha.copy(),\n",
    "            'beta': self.beta.copy()\n",
    "        })\n",
    "\n",
    "# Run Thompson Sampling on production data\n",
    "ts = ThompsonSampling(n_variants=2)\n",
    "selections = []\n",
    "rewards = []\n",
    "\n",
    "for idx, row in ab_test_df.iterrows():\n",
    "    # Select variant\n",
    "    variant, _ = ts.select_variant()\n",
    "    \n",
    "    # Make prediction\n",
    "    if variant == 0:  # Model A\n",
    "        pred = model_a.predict(row[feature_cols].values.reshape(1, -1))[0]\n",
    "    else:  # Model B\n",
    "        pred = model_b.predict(row[feature_cols].values.reshape(1, -1))[0]\n",
    "    \n",
    "    # Evaluate (reward = 1 if prediction correct, 0 otherwise)\n",
    "    pred_class = 1 if pred > 85 else 0\n",
    "    actual_class = 1 if row['true_yield_pct'] > 85 else 0\n",
    "    reward = 1 if pred_class == actual_class else 0\n",
    "    \n",
    "    # Update\n",
    "    ts.update(variant, reward)\n",
    "    selections.append(variant)\n",
    "    rewards.append(reward)\n",
    "\n",
    "# Analyze results\n",
    "selections = np.array(selections)\n",
    "rewards = np.array(rewards)\n",
    "\n",
    "print(\"Thompson Sampling Results:\")\n",
    "print(f\"\\nVariant selection:\")\n",
    "print(f\"  Model A: {(selections == 0).sum()} times ({(selections == 0).mean() * 100:.1f}%)\")\n",
    "print(f\"  Model B: {(selections == 1).sum()} times ({(selections == 1).mean() * 100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nPerformance by variant:\")\n",
    "print(f\"  Model A accuracy: {rewards[selections == 0].mean() * 100:.1f}%\")\n",
    "print(f\"  Model B accuracy: {rewards[selections == 1].mean() * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nOverall accuracy: {rewards.mean() * 100:.1f}%\")\n",
    "\n",
    "# Plot selection over time\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cumulative selection proportion\n",
    "cumulative_b = np.cumsum(selections == 1) / np.arange(1, len(selections) + 1)\n",
    "axes[0].plot(cumulative_b * 100, linewidth=2)\n",
    "axes[0].axhline(50, color='red', linestyle='--', label='Equal split')\n",
    "axes[0].set_xlabel('Trial Number')\n",
    "axes[0].set_ylabel('Model B Selection Rate (%)')\n",
    "axes[0].set_title('Thompson Sampling: Adaptive Allocation')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Beta distributions at end of test\n",
    "x = np.linspace(0, 1, 1000)\n",
    "dist_a = stats.beta.pdf(x, ts.alpha[0], ts.beta[0])\n",
    "dist_b = stats.beta.pdf(x, ts.alpha[1], ts.beta[1])\n",
    "\n",
    "axes[1].plot(x, dist_a, label=f'Model A (Œ±={ts.alpha[0]:.0f}, Œ≤={ts.beta[0]:.0f})', linewidth=2)\n",
    "axes[1].plot(x, dist_b, label=f'Model B (Œ±={ts.alpha[1]:.0f}, Œ≤={ts.beta[1]:.0f})', linewidth=2)\n",
    "axes[1].set_xlabel('Success Rate')\n",
    "axes[1].set_ylabel('Probability Density')\n",
    "axes[1].set_title('Posterior Distributions (End of Test)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Thompson Sampling automatically favored the better model!\")\n",
    "print(f\"   Reduced regret by testing inferior variant less frequently\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127d02b",
   "metadata": {},
   "source": [
    "## 9. Decision Framework\n",
    "\n",
    "**Purpose:** Structured decision criteria for A/B test outcomes.\n",
    "\n",
    "**Key Points:**\n",
    "- **Statistical significance**: p < 0.05\n",
    "- **Practical significance**: Effect size > minimum threshold\n",
    "- **Guardrail checks**: No degradation in quality metrics\n",
    "- **Business value**: ROI justifies deployment cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f428927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ab_decision(p_value, effect_size, guardrail_ok, min_effect=1.0, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Decision framework for A/B test outcomes.\n",
    "    \n",
    "    Parameters:\n",
    "    - p_value: Statistical significance\n",
    "    - effect_size: Magnitude of improvement (percentage points)\n",
    "    - guardrail_ok: Boolean, True if guardrails passed\n",
    "    - min_effect: Minimum practical effect size\n",
    "    - alpha: Significance threshold\n",
    "    \n",
    "    Returns:\n",
    "    - Decision: 'SHIP', 'ITERATE', or 'STOP'\n",
    "    \"\"\"\n",
    "    statistically_significant = p_value < alpha\n",
    "    practically_significant = abs(effect_size) >= min_effect\n",
    "    \n",
    "    print(\"A/B Test Decision Framework\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n1. Statistical Significance:\")\n",
    "    print(f\"   p-value: {p_value:.4f}\")\n",
    "    print(f\"   Threshold: {alpha}\")\n",
    "    print(f\"   Result: {'‚úÖ PASS' if statistically_significant else '‚ùå FAIL'}\")\n",
    "    \n",
    "    print(f\"\\n2. Practical Significance:\")\n",
    "    print(f\"   Effect size: {effect_size:.2f} percentage points\")\n",
    "    print(f\"   Minimum threshold: {min_effect:.2f}\")\n",
    "    print(f\"   Result: {'‚úÖ PASS' if practically_significant else '‚ùå FAIL'}\")\n",
    "    \n",
    "    print(f\"\\n3. Guardrail Metrics:\")\n",
    "    print(f\"   False accept rate: {fa_rate_b:.2f}% (Model B)\")\n",
    "    print(f\"   Baseline: {fa_rate_a:.2f}% (Model A)\")\n",
    "    print(f\"   Result: {'‚úÖ PASS' if guardrail_ok else '‚ùå FAIL (Quality degradation!)'}\")\n",
    "    \n",
    "    # Decision logic\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    if statistically_significant and practically_significant and guardrail_ok:\n",
    "        decision = \"SHIP IT! üöÄ\"\n",
    "        print(f\"\\n‚úÖ {decision}\")\n",
    "        print(f\"   Model B is significantly better and guardrails passed\")\n",
    "        print(f\"   Recommended rollout: 10% ‚Üí 50% ‚Üí 100% over 2 weeks\")\n",
    "    elif statistically_significant and practically_significant and not guardrail_ok:\n",
    "        decision = \"ITERATE ‚öôÔ∏è\"\n",
    "        print(f\"\\n‚ö†Ô∏è  {decision}\")\n",
    "        print(f\"   Model B improves primary metric but degrades guardrail\")\n",
    "        print(f\"   Recommended: Retrain with guardrail constraints, re-test\")\n",
    "    elif statistically_significant and not practically_significant:\n",
    "        decision = \"STOP üõë\"\n",
    "        print(f\"\\n‚ùå {decision}\")\n",
    "        print(f\"   Effect size too small to justify deployment cost\")\n",
    "        print(f\"   Recommended: Keep Model A, focus on larger improvements\")\n",
    "    else:\n",
    "        decision = \"INCONCLUSIVE ü§∑\"\n",
    "        print(f\"\\n‚ùå {decision}\")\n",
    "        print(f\"   Not statistically significant - could be noise\")\n",
    "        print(f\"   Recommended: Extend test duration or increase sample size\")\n",
    "    \n",
    "    return decision\n",
    "\n",
    "# Apply decision framework to our test\n",
    "effect_size = fr_rate_a - fr_rate_b  # Improvement in false reject rate\n",
    "guardrail_ok = fa_rate_b <= fa_rate_a * 1.05  # Allow 5% guardrail degradation\n",
    "\n",
    "decision = make_ab_decision(\n",
    "    p_value=p_value,\n",
    "    effect_size=effect_size,\n",
    "    guardrail_ok=guardrail_ok,\n",
    "    min_effect=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434ecf3e",
   "metadata": {},
   "source": [
    "## 10. Project Templates\n",
    "\n",
    "### Project 1: Production A/B Testing Infrastructure\n",
    "**Objective:** Build reusable A/B testing platform for all ML models\n",
    "- Create traffic splitter routing production requests to variant A or B\n",
    "- Implement metric logging (latency, accuracy, business KPIs)\n",
    "- Build real-time dashboard showing test progress\n",
    "- Auto-stop feature if guardrails violated\n",
    "- **Success Metric:** Deploy 5+ A/B tests in 6 months, <2% infrastructure overhead\n",
    "\n",
    "### Project 2: Sequential A/B Testing with Early Stopping\n",
    "**Objective:** Implement SPRT (Sequential Probability Ratio Test) for faster decisions\n",
    "- Calculate upper/lower boundaries for cumulative test statistic\n",
    "- Stop test as soon as crossing boundary (don't wait for fixed duration)\n",
    "- Reduce average test time 40% while maintaining Type I/II error rates\n",
    "- Validate with simulations before production deployment\n",
    "- **Success Metric:** Reduce test duration from 4 weeks to 10 days on average\n",
    "\n",
    "### Project 3: Multi-Metric A/B Testing\n",
    "**Objective:** Optimize for multiple objectives simultaneously\n",
    "- Primary: False reject rate, Secondary: Test time, Guardrail: False accept rate\n",
    "- Use Bonferroni correction for multiple comparisons\n",
    "- Build Pareto frontier of non-dominated solutions\n",
    "- Let stakeholders choose preferred trade-off point\n",
    "- **Success Metric:** Deployed model balances 3 metrics vs optimizing single metric\n",
    "\n",
    "### Project 4: Contextual Bandits for Personalized Models\n",
    "**Objective:** Route to best model based on lot characteristics\n",
    "- Features: Product family, wafer fab, test program version\n",
    "- LinUCB algorithm for exploration-exploitation\n",
    "- Learn which model works best for which context\n",
    "- Deploy hybrid system using multiple specialized models\n",
    "- **Success Metric:** 10% better performance than single global model\n",
    "\n",
    "### Project 5: A/B Testing ROI Calculator\n",
    "**Objective:** Business case tool for justifying test investments\n",
    "- Inputs: False reject cost, test duration, deployment effort\n",
    "- Calculate NPV of deploying Model B vs staying with Model A\n",
    "- Sensitivity analysis on key assumptions\n",
    "- Automated report generation for management\n",
    "- **Success Metric:** 100% of A/B tests have pre-approved ROI threshold\n",
    "\n",
    "### Project 6: Bayesian A/B Testing\n",
    "**Objective:** Replace frequentist tests with Bayesian credible intervals\n",
    "- Implement Bayesian t-test with informative priors\n",
    "- Report probability that Model B is better (not just p-values)\n",
    "- Incorporate domain knowledge (\"Model B shouldn't be 50% better\")\n",
    "- Continuous monitoring with posterior updates\n",
    "- **Success Metric:** More intuitive results for stakeholders, faster convergence\n",
    "\n",
    "### Project 7: Automated A/B Test Analysis\n",
    "**Objective:** Auto-generate insights from completed tests\n",
    "- Segment analysis: Which product families benefit most?\n",
    "- Temporal analysis: Does performance vary by time-of-day/week?\n",
    "- Novelty detection: Flag unusual patterns in test data\n",
    "- Natural language summary: \"Model B reduces costs 12% for Product X\"\n",
    "- **Success Metric:** Zero manual analysis, insights delivered within 1 hour of test completion\n",
    "\n",
    "### Project 8: Long-Term Holdout Validation\n",
    "**Objective:** Catch slow degradation missed by short A/B tests\n",
    "- Keep 5% traffic on Model A permanently (even after B wins)\n",
    "- Monitor for concept drift over 3-6 months\n",
    "- Detect if Model B advantage disappears over time\n",
    "- Alert if Model A becomes better (trigger rollback)\n",
    "- **Success Metric:** Catch 2+ drift-related failures before impacting 100% of traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98ceae",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "**When to Use A/B Testing:**\n",
    "- ‚úÖ **Production deployment**: Always test new models in production before 100% rollout\n",
    "- ‚úÖ **High-stakes decisions**: When mistakes cost >$100K (yield, quality, safety)\n",
    "- ‚úÖ **Uncertain offline-online correlation**: When offline metrics poorly predict real impact\n",
    "- ‚úÖ **Multiple candidates**: Compare 2-5 model variants simultaneously\n",
    "- ‚úÖ **Iterative improvement**: Culture of continuous experimentation\n",
    "\n",
    "**When NOT to Use A/B Testing:**\n",
    "- ‚ùå **Low traffic**: <1000 samples takes too long to reach significance\n",
    "- ‚ùå **Immediate need**: Can't wait weeks for statistical significance\n",
    "- ‚ùå **Unsafe testing**: Can't expose customers/devices to potentially worse variant\n",
    "- ‚ùå **Identical offline performance**: If offline metrics are identical, A/B test won't help\n",
    "\n",
    "**Critical Success Factors:**\n",
    "1. **Randomization**: Truly random assignment (coin flip, hash function)\n",
    "2. **Sample size**: Power analysis upfront, don't start underpowered tests\n",
    "3. **Guardrails**: Monitor quality/safety metrics, auto-stop if violated\n",
    "4. **Pre-registration**: Define success criteria before test starts (avoid p-hacking)\n",
    "5. **Multiple metrics**: Track primary + secondary + guardrail metrics\n",
    "6. **Iteration**: Failed test = learning, iterate on Model B and re-test\n",
    "\n",
    "**Common Pitfalls:**\n",
    "- ‚ö†Ô∏è **Peeking**: Checking results early and stopping when \"significant\" (inflates false positives)\n",
    "- ‚ö†Ô∏è **Multiple testing**: Running many A/B tests without correction (Bonferroni, FDR)\n",
    "- ‚ö†Ô∏è **Sample ratio mismatch**: 51/49 split instead of 50/50 indicates bias\n",
    "- ‚ö†Ô∏è **Ignoring guardrails**: Optimizing primary metric at expense of quality\n",
    "- ‚ö†Ô∏è **Novelty effects**: Initial improvement fades after 2-4 weeks\n",
    "- ‚ö†Ô∏è **Insufficient power**: Starting test that's mathematically unlikely to detect real effects\n",
    "\n",
    "**Best Practices:**\n",
    "1. **A/A test first**: Validate infrastructure has no bias (should see no difference)\n",
    "2. **Pre-compute sample size**: Don't guess‚Äîuse power analysis\n",
    "3. **Define MDE**: Minimum detectable effect‚Äîsmallest improvement worth deploying\n",
    "4. **Segment analysis**: Does Model B help all customer segments equally?\n",
    "5. **Long-term holdout**: Keep 5% traffic on Model A to catch drift\n",
    "6. **Document everything**: Test plan, results, decision rationale\n",
    "7. **Automate analysis**: Reduce human error in statistical tests\n",
    "8. **Gradual rollout**: 10% ‚Üí 50% ‚Üí 100% over days/weeks\n",
    "\n",
    "**Statistical Checklist:**\n",
    "- [ ] Null/alternative hypotheses defined\n",
    "- [ ] Significance level (Œ±) and power (1-Œ≤) set\n",
    "- [ ] Sample size calculated and achievable\n",
    "- [ ] Randomization mechanism validated (A/A test)\n",
    "- [ ] Primary metric defined and measurable\n",
    "- [ ] Guardrail metrics defined with thresholds\n",
    "- [ ] Test duration estimated (time to N samples)\n",
    "- [ ] Early stopping rules defined (if using sequential testing)\n",
    "- [ ] Multiple testing correction planned (if >1 metric)\n",
    "\n",
    "**Next Steps:**\n",
    "- Study **107: Model Monitoring** for post-deployment tracking\n",
    "- Explore Bayesian A/B testing (more intuitive for stakeholders)\n",
    "- Learn multi-armed bandits for faster convergence\n",
    "- Read \"Trustworthy Online Controlled Experiments\" (Kohavi et al.)\n",
    "- Practice with real production traffic (start with low-stakes models)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
