{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87fd95fa",
   "metadata": {},
   "source": [
    "# 129: Advanced MLOps - Feature Stores & Real-Time Monitoring\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** feature store architecture and the offline/online serving dichotomy\n",
    "- **Implement** production-grade feature stores with versioning and lineage tracking\n",
    "- **Build** real-time feature serving pipelines with <10ms latency requirements\n",
    "- **Apply** data quality monitoring with schema validation and distribution drift detection\n",
    "- **Master** model performance monitoring to detect accuracy degradation and concept drift\n",
    "- **Deploy** comprehensive observability systems for production ML pipelines\n",
    "\n",
    "## üìö What is a Feature Store?\n",
    "\n",
    "A **feature store** is a centralized repository for storing, managing, and serving ML features for both training and inference. It solves the **training-serving skew** problem by ensuring features computed during training are identical to features served during inference.\n",
    "\n",
    "**The Training-Serving Skew Problem:**\n",
    "- **Training time:** Features computed in batch (Spark/pandas), aggregated over historical data\n",
    "- **Inference time:** Features must be computed in real-time (<10ms latency) with live data\n",
    "- **Skew:** Different feature computation logic leads to accuracy degradation in production\n",
    "\n",
    "**Feature Store Solution:**\n",
    "- **Single source of truth:** Same feature definitions for training and serving\n",
    "- **Offline store:** Batch features for training (Parquet, Delta Lake, S3)\n",
    "- **Online store:** Low-latency features for inference (Redis, DynamoDB, Cassandra)\n",
    "- **Feature versioning:** Track feature changes, enable reproducibility\n",
    "- **Point-in-time correctness:** No data leakage from future into past\n",
    "\n",
    "**Why Feature Stores Matter:**\n",
    "- ‚úÖ **Eliminate training-serving skew** - Same code for batch and real-time features\n",
    "- ‚úÖ **Feature reuse** - Share features across teams (customer_lifetime_value used by 5 models)\n",
    "- ‚úÖ **Faster experimentation** - Pre-computed features ready for model training\n",
    "- ‚úÖ **Reproducibility** - Feature versioning enables exact training reproduction\n",
    "- ‚úÖ **Governance** - Track feature lineage, ownership, SLA compliance\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Feature Store for Wafer Test Data:**\n",
    "- **Input:** Raw STDF files (10,000 parameters per device), test_time, die coordinates\n",
    "- **Features:** Aggregated statistics (mean/std/quantiles per wafer), spatial correlations (neighbor yield)\n",
    "- **Offline:** Compute 30-day rolling statistics for training yield models\n",
    "- **Online:** Serve real-time features for binning decisions (<5ms latency)\n",
    "- **Value:** Consistent features between training (batch) and production (real-time binning)\n",
    "\n",
    "**Real-Time Monitoring for Yield Prediction:**\n",
    "- **Input:** Live yield predictions (1,000 devices/minute)\n",
    "- **Monitoring:** Track prediction distribution, accuracy vs actual yield, latency p99\n",
    "- **Drift detection:** Alert when input features shift (Vdd distribution changes)\n",
    "- **Concept drift:** Alert when accuracy drops (model degrading, process change)\n",
    "- **Value:** Catch model degradation within minutes (not weeks)\n",
    "\n",
    "**Data Quality for Parametric Tests:**\n",
    "- **Input:** STDF parametric test results (voltage, current, frequency, power)\n",
    "- **Schema validation:** Ensure test_name, test_value, test_limits present\n",
    "- **Distribution checks:** Flag when Vdd values outside expected range (1.0-1.4V)\n",
    "- **Null detection:** Alert on missing critical parameters (would break model)\n",
    "- **Value:** Prevent bad data from reaching models (garbage in ‚Üí garbage out)\n",
    "\n",
    "**Feature Lineage for Compliance:**\n",
    "- **Requirement:** FDA/automotive require traceability of all features used in models\n",
    "- **Solution:** Track which raw STDF fields ‚Üí derived features ‚Üí model predictions\n",
    "- **Example:** final_yield ‚Üê neighbor_yield_avg ‚Üê (die_x, die_y, pass_fail) from STDF\n",
    "- **Value:** Audit trail for regulatory compliance, root cause analysis\n",
    "\n",
    "## üîÑ Feature Store Architecture\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    subgraph \"Data Sources\"\n",
    "        A1[STDF Files] --> B[Feature Engineering]\n",
    "        A2[Test Logs] --> B\n",
    "        A3[Manufacturing DB] --> B\n",
    "    end\n",
    "    \n",
    "    subgraph \"Feature Store\"\n",
    "        B --> C1[Offline Store<br/>Parquet/Delta Lake]\n",
    "        B --> C2[Online Store<br/>Redis/DynamoDB]\n",
    "        C1 --> D1[Training Pipeline]\n",
    "        C2 --> D2[Real-Time Inference]\n",
    "    end\n",
    "    \n",
    "    subgraph \"ML Lifecycle\"\n",
    "        D1 --> E[Model Training]\n",
    "        E --> F[Model Registry]\n",
    "        F --> D2\n",
    "        D2 --> G[Predictions]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Monitoring\"\n",
    "        G --> H1[Performance Monitor]\n",
    "        G --> H2[Drift Detector]\n",
    "        B --> H3[Data Quality Check]\n",
    "        H1 --> I[Alerts]\n",
    "        H2 --> I\n",
    "        H3 --> I\n",
    "    end\n",
    "    \n",
    "    style C1 fill:#e1f5ff\n",
    "    style C2 fill:#ffe1e1\n",
    "    style H1 fill:#e1ffe1\n",
    "    style H2 fill:#ffe1f5\n",
    "    style H3 fill:#fff5e1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **124: Feature Store Implementation** - Basic feature store concepts\n",
    "- **127: Model Governance & Compliance** - Lineage tracking, audit trails\n",
    "- **128: Shadow Mode Deployment** - Safe deployment strategies\n",
    "\n",
    "**Next Steps:**\n",
    "- **130: ML Observability & Debugging** - Distributed tracing, model debugging\n",
    "- **131: Container Orchestration** - Kubernetes for ML, horizontal scaling\n",
    "\n",
    "---\n",
    "\n",
    "Let's build production-grade feature stores and monitoring systems! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0c00bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import json\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Setup complete - Ready for feature stores and monitoring!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2500071",
   "metadata": {},
   "source": [
    "## 2. Production-Grade Feature Store Implementation\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement enterprise feature store with offline/online serving, versioning, point-in-time correctness, and lineage tracking.\n",
    "\n",
    "**Key Points:**\n",
    "- **Offline store:** Parquet-based storage for training (batch feature computation)\n",
    "- **Online store:** In-memory cache for inference (<5ms latency)\n",
    "- **Point-in-time joins:** Prevent data leakage (no future features in past training)\n",
    "- **Feature versioning:** Track feature definition changes over time\n",
    "- **Lineage tracking:** Record which raw data ‚Üí features ‚Üí models\n",
    "\n",
    "**Why This Matters:** Training-serving skew causes 10-30% accuracy drop in production. Feature stores ensure identical feature computation for training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d758c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureStore:\n",
    "    \"\"\"\n",
    "    Production-grade feature store with offline/online serving.\n",
    "    \n",
    "    Supports:\n",
    "    - Offline store: Batch features for training (Parquet/Delta Lake simulation)\n",
    "    - Online store: Real-time features for inference (in-memory cache)\n",
    "    - Point-in-time correctness: No data leakage from future\n",
    "    - Feature versioning: Track definition changes\n",
    "    - Lineage tracking: Raw data ‚Üí features ‚Üí models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, store_name):\n",
    "        self.store_name = store_name\n",
    "        self.offline_store = {}  # {feature_group: DataFrame}\n",
    "        self.online_store = {}   # {entity_id: {feature_name: value}}\n",
    "        self.feature_definitions = {}  # {feature_name: definition_metadata}\n",
    "        self.feature_lineage = defaultdict(list)  # {feature_name: [source_tables]}\n",
    "        self.feature_versions = defaultdict(list)  # {feature_name: [versions]}\n",
    "        \n",
    "    def register_feature_group(self, name, entity_key, features, description, version=\"v1.0\"):\n",
    "        \"\"\"\n",
    "        Register feature group (collection of related features).\n",
    "        \n",
    "        Args:\n",
    "            name: Feature group name (e.g., \"wafer_aggregates\")\n",
    "            entity_key: Join key (e.g., \"wafer_id\", \"device_id\")\n",
    "            features: List of feature names\n",
    "            description: Human-readable description\n",
    "            version: Feature version\n",
    "        \"\"\"\n",
    "        self.feature_definitions[name] = {\n",
    "            'entity_key': entity_key,\n",
    "            'features': features,\n",
    "            'description': description,\n",
    "            'version': version,\n",
    "            'registered_at': datetime.now()\n",
    "        }\n",
    "        \n",
    "        self.feature_versions[name].append({\n",
    "            'version': version,\n",
    "            'registered_at': datetime.now(),\n",
    "            'features': features\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Registered feature group: {name} ({version})\")\n",
    "        print(f\"   Entity key: {entity_key}\")\n",
    "        print(f\"   Features: {', '.join(features)}\")\n",
    "        \n",
    "        return name\n",
    "    \n",
    "    def write_to_offline_store(self, feature_group_name, df, timestamp_col=None):\n",
    "        \"\"\"\n",
    "        Write features to offline store (training data).\n",
    "        \n",
    "        Offline store simulates Parquet/Delta Lake storage.\n",
    "        Includes timestamp for point-in-time correctness.\n",
    "        \"\"\"\n",
    "        if timestamp_col and timestamp_col not in df.columns:\n",
    "            # Add timestamp if not present\n",
    "            df[timestamp_col] = datetime.now()\n",
    "        \n",
    "        # Store in offline store (simulating partitioned storage)\n",
    "        if feature_group_name not in self.offline_store:\n",
    "            self.offline_store[feature_group_name] = []\n",
    "        \n",
    "        self.offline_store[feature_group_name].append(df.copy())\n",
    "        \n",
    "        print(f\"üì¶ Wrote {len(df)} rows to offline store: {feature_group_name}\")\n",
    "        \n",
    "        return len(df)\n",
    "    \n",
    "    def write_to_online_store(self, feature_group_name, df, entity_key):\n",
    "        \"\"\"\n",
    "        Write features to online store (real-time inference).\n",
    "        \n",
    "        Online store is in-memory cache (simulates Redis/DynamoDB).\n",
    "        Only latest values stored per entity.\n",
    "        \"\"\"\n",
    "        definition = self.feature_definitions.get(feature_group_name)\n",
    "        if not definition:\n",
    "            raise ValueError(f\"Feature group {feature_group_name} not registered\")\n",
    "        \n",
    "        features = definition['features']\n",
    "        \n",
    "        # Write to online store (entity_id ‚Üí feature dict)\n",
    "        for _, row in df.iterrows():\n",
    "            entity_id = row[entity_key]\n",
    "            \n",
    "            if entity_id not in self.online_store:\n",
    "                self.online_store[entity_id] = {}\n",
    "            \n",
    "            for feature in features:\n",
    "                if feature in row:\n",
    "                    self.online_store[entity_id][feature] = row[feature]\n",
    "        \n",
    "        print(f\"‚ö° Wrote {len(df)} entities to online store: {feature_group_name}\")\n",
    "        print(f\"   Latency target: <5ms per entity lookup\")\n",
    "        \n",
    "        return len(df)\n",
    "    \n",
    "    def get_offline_features(self, feature_group_name, entity_ids=None, start_date=None, end_date=None):\n",
    "        \"\"\"\n",
    "        Retrieve features from offline store (for training).\n",
    "        \n",
    "        Supports:\n",
    "        - Entity filtering (specific wafer IDs, device IDs)\n",
    "        - Time range filtering (point-in-time correctness)\n",
    "        - Batch retrieval (1000s of entities)\n",
    "        \"\"\"\n",
    "        if feature_group_name not in self.offline_store:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Combine all batches\n",
    "        dfs = self.offline_store[feature_group_name]\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # Filter by entity IDs\n",
    "        if entity_ids is not None:\n",
    "            definition = self.feature_definitions[feature_group_name]\n",
    "            entity_key = definition['entity_key']\n",
    "            df = df[df[entity_key].isin(entity_ids)]\n",
    "        \n",
    "        # Filter by date range (point-in-time correctness)\n",
    "        if 'timestamp' in df.columns:\n",
    "            if start_date:\n",
    "                df = df[df['timestamp'] >= start_date]\n",
    "            if end_date:\n",
    "                df = df[df['timestamp'] <= end_date]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_online_features(self, entity_ids, feature_names):\n",
    "        \"\"\"\n",
    "        Retrieve features from online store (for inference).\n",
    "        \n",
    "        Low-latency lookup (<5ms) for real-time predictions.\n",
    "        Returns only requested features for specified entities.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        results = []\n",
    "        for entity_id in entity_ids:\n",
    "            if entity_id in self.online_store:\n",
    "                feature_dict = {'entity_id': entity_id}\n",
    "                for feature_name in feature_names:\n",
    "                    feature_dict[feature_name] = self.online_store[entity_id].get(feature_name)\n",
    "                results.append(feature_dict)\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        result_df = pd.DataFrame(results)\n",
    "        \n",
    "        print(f\"‚ö° Retrieved {len(results)} entities from online store\")\n",
    "        print(f\"   Latency: {latency_ms:.2f}ms ({latency_ms/len(entity_ids):.2f}ms per entity)\")\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def track_lineage(self, feature_name, source_tables, transformation_logic):\n",
    "        \"\"\"\n",
    "        Track feature lineage for governance and debugging.\n",
    "        \n",
    "        Records:\n",
    "        - Source tables/features used\n",
    "        - Transformation logic applied\n",
    "        - Created timestamp\n",
    "        \"\"\"\n",
    "        self.feature_lineage[feature_name].append({\n",
    "            'source_tables': source_tables,\n",
    "            'transformation': transformation_logic,\n",
    "            'created_at': datetime.now()\n",
    "        })\n",
    "        \n",
    "        return feature_name\n",
    "    \n",
    "    def get_feature_lineage(self, feature_name):\n",
    "        \"\"\"Get full lineage for a feature (for audit, debugging).\"\"\"\n",
    "        return self.feature_lineage.get(feature_name, [])\n",
    "    \n",
    "    def get_feature_metadata(self, feature_group_name):\n",
    "        \"\"\"Get feature group metadata (definition, version, registered time).\"\"\"\n",
    "        return self.feature_definitions.get(feature_group_name)\n",
    "\n",
    "# Example: Feature store for wafer test data\n",
    "print(\"üè™ Feature Store: Wafer Test Features\\\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize feature store\n",
    "fs = FeatureStore(\"wafer_test_feature_store\")\n",
    "\n",
    "# Register feature group: wafer-level aggregates\n",
    "wafer_features = [\n",
    "    'yield_pct',\n",
    "    'avg_vdd',\n",
    "    'std_vdd',\n",
    "    'avg_test_time_ms',\n",
    "    'device_count',\n",
    "    'neighbor_yield_avg'\n",
    "]\n",
    "\n",
    "fs.register_feature_group(\n",
    "    name='wafer_aggregates',\n",
    "    entity_key='wafer_id',\n",
    "    features=wafer_features,\n",
    "    description='Wafer-level aggregate features from STDF test data',\n",
    "    version='v1.0'\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "# Generate synthetic wafer test data\n",
    "n_wafers = 100\n",
    "n_devices_per_wafer = 500\n",
    "\n",
    "wafer_data = []\n",
    "\n",
    "for wafer_id in range(1, n_wafers + 1):\n",
    "    # Simulate wafer-level aggregates\n",
    "    yield_pct = np.random.uniform(85, 99)\n",
    "    avg_vdd = np.random.normal(1.2, 0.02)\n",
    "    std_vdd = np.random.uniform(0.01, 0.05)\n",
    "    avg_test_time = np.random.normal(100, 10)\n",
    "    device_count = n_devices_per_wafer\n",
    "    \n",
    "    # Simulate spatial correlation (neighbor yield)\n",
    "    neighbor_yield = yield_pct + np.random.normal(0, 2)\n",
    "    \n",
    "    wafer_data.append({\n",
    "        'wafer_id': f'W{wafer_id:04d}',\n",
    "        'yield_pct': yield_pct,\n",
    "        'avg_vdd': avg_vdd,\n",
    "        'std_vdd': std_vdd,\n",
    "        'avg_test_time_ms': avg_test_time,\n",
    "        'device_count': device_count,\n",
    "        'neighbor_yield_avg': neighbor_yield,\n",
    "        'timestamp': datetime.now() - timedelta(days=np.random.randint(0, 30))\n",
    "    })\n",
    "\n",
    "wafer_df = pd.DataFrame(wafer_data)\n",
    "\n",
    "print(f\"üìä Generated {len(wafer_df)} wafer records\")\n",
    "print(f\"\\\\nSample data:\")\n",
    "print(wafer_df.head(3))\n",
    "print()\n",
    "\n",
    "# Write to offline store (for training)\n",
    "print(\"=\"*80)\n",
    "print(\"OFFLINE STORE (Training Data)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fs.write_to_offline_store(\n",
    "    feature_group_name='wafer_aggregates',\n",
    "    df=wafer_df,\n",
    "    timestamp_col='timestamp'\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "# Write to online store (for real-time inference)\n",
    "print(\"=\"*80)\n",
    "print(\"ONLINE STORE (Real-Time Inference)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Latest 20 wafers to online store\n",
    "latest_wafers = wafer_df.nlargest(20, 'timestamp')\n",
    "\n",
    "fs.write_to_online_store(\n",
    "    feature_group_name='wafer_aggregates',\n",
    "    df=latest_wafers,\n",
    "    entity_key='wafer_id'\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "# Track feature lineage\n",
    "fs.track_lineage(\n",
    "    feature_name='neighbor_yield_avg',\n",
    "    source_tables=['stdf_wafer_test.parametric_results'],\n",
    "    transformation_logic='AVG(yield_pct) WHERE die_x IN (x-1, x, x+1) AND die_y IN (y-1, y, y+1)'\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE RETRIEVAL - OFFLINE (Training)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Retrieve offline features for training (past 15 days)\n",
    "training_features = fs.get_offline_features(\n",
    "    feature_group_name='wafer_aggregates',\n",
    "    start_date=datetime.now() - timedelta(days=15),\n",
    "    end_date=datetime.now()\n",
    ")\n",
    "\n",
    "print(f\"Retrieved {len(training_features)} wafer records for training\")\n",
    "print(f\"Date range: {training_features['timestamp'].min()} to {training_features['timestamp'].max()}\")\n",
    "print()\n",
    "\n",
    "# Train simple model on offline features\n",
    "X_train = training_features[['avg_vdd', 'std_vdd', 'avg_test_time_ms', 'neighbor_yield_avg']]\n",
    "y_train = training_features['yield_pct']\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"‚úÖ Model trained on {len(X_train)} offline samples\")\n",
    "print()\n",
    "\n",
    "# Retrieve online features for inference\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE RETRIEVAL - ONLINE (Real-Time Inference)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Simulate real-time inference request (5 wafers)\n",
    "inference_wafer_ids = latest_wafers['wafer_id'].head(5).tolist()\n",
    "\n",
    "online_features = fs.get_online_features(\n",
    "    entity_ids=inference_wafer_ids,\n",
    "    feature_names=['avg_vdd', 'std_vdd', 'avg_test_time_ms', 'neighbor_yield_avg']\n",
    ")\n",
    "\n",
    "print(f\"\\\\nOnline features:\")\n",
    "print(online_features)\n",
    "print()\n",
    "\n",
    "# Make predictions with online features\n",
    "X_inference = online_features[['avg_vdd', 'std_vdd', 'avg_test_time_ms', 'neighbor_yield_avg']]\n",
    "predictions = model.predict(X_inference)\n",
    "\n",
    "print(f\"‚úÖ Predictions for {len(predictions)} wafers:\")\n",
    "for i, (wafer_id, pred) in enumerate(zip(inference_wafer_ids, predictions)):\n",
    "    print(f\"   {wafer_id}: Predicted yield = {pred:.2f}%\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Get feature lineage\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE LINEAGE (Governance & Audit)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "lineage = fs.get_feature_lineage('neighbor_yield_avg')\n",
    "print(f\"\\\\nFeature: neighbor_yield_avg\")\n",
    "print(f\"Source tables: {lineage[0]['source_tables']}\")\n",
    "print(f\"Transformation: {lineage[0]['transformation']}\")\n",
    "print(f\"Created at: {lineage[0]['created_at']}\")\n",
    "print()\n",
    "\n",
    "# Get feature metadata\n",
    "metadata = fs.get_feature_metadata('wafer_aggregates')\n",
    "print(f\"Feature Group Metadata:\")\n",
    "print(f\"  Name: wafer_aggregates\")\n",
    "print(f\"  Version: {metadata['version']}\")\n",
    "print(f\"  Entity key: {metadata['entity_key']}\")\n",
    "print(f\"  Features: {', '.join(metadata['features'])}\")\n",
    "print(f\"  Registered: {metadata['registered_at']}\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ FEATURE STORE DEMONSTRATION COMPLETE\")\n",
    "print(f\"   Offline store: {len(wafer_df)} records (training)\")\n",
    "print(f\"   Online store: {len(fs.online_store)} entities (inference)\")\n",
    "print(f\"   Latency: <5ms per entity (production-ready)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd13dc06",
   "metadata": {},
   "source": [
    "## 3. Real-Time Feature Serving Pipeline\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Build low-latency feature serving pipeline for production inference with <10ms p99 latency.\n",
    "\n",
    "**Key Points:**\n",
    "- **Feature caching:** Pre-compute expensive aggregations, cache in Redis/memory\n",
    "- **Batch retrieval:** Fetch multiple entities in single call (reduce network overhead)\n",
    "- **Feature transformation:** Real-time computations (ratios, differences) from cached base features\n",
    "- **Monitoring:** Track latency p50/p95/p99, cache hit rate, error rate\n",
    "\n",
    "**Why This Matters:** Real-time predictions require <10ms feature retrieval. Slow features block production inference, violate SLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359cf287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealTimeFeatureServer:\n",
    "    \"\"\"\n",
    "    Low-latency feature serving for production inference.\n",
    "    \n",
    "    Optimizations:\n",
    "    - In-memory caching (Redis simulation)\n",
    "    - Batch retrieval (fetch multiple entities at once)\n",
    "    - Feature transformation pipeline (derived features from base features)\n",
    "    - Latency monitoring (p50, p95, p99)\n",
    "    - Cache hit rate tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_store):\n",
    "        self.feature_store = feature_store\n",
    "        self.cache = {}  # Simulates Redis in-memory cache\n",
    "        self.latency_log = []\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "        \n",
    "    def get_features_batch(self, entity_ids, feature_names, use_cache=True):\n",
    "        \"\"\"\n",
    "        Batch feature retrieval with caching.\n",
    "        \n",
    "        Fetches multiple entities in single call (reduces network overhead).\n",
    "        Uses cache for frequently accessed features.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for entity_id in entity_ids:\n",
    "            cache_key = f\"{entity_id}::{','.join(sorted(feature_names))}\"\n",
    "            \n",
    "            if use_cache and cache_key in self.cache:\n",
    "                # Cache hit\n",
    "                self.cache_hits += 1\n",
    "                features = self.cache[cache_key]\n",
    "            else:\n",
    "                # Cache miss - retrieve from feature store\n",
    "                self.cache_misses += 1\n",
    "                \n",
    "                feature_dict = {'entity_id': entity_id}\n",
    "                \n",
    "                # Get from online store\n",
    "                if entity_id in self.feature_store.online_store:\n",
    "                    for feature_name in feature_names:\n",
    "                        feature_dict[feature_name] = self.feature_store.online_store[entity_id].get(feature_name)\n",
    "                \n",
    "                features = feature_dict\n",
    "                \n",
    "                # Update cache\n",
    "                if use_cache:\n",
    "                    self.cache[cache_key] = features\n",
    "            \n",
    "            results.append(features)\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        self.latency_log.append(latency_ms)\n",
    "        \n",
    "        result_df = pd.DataFrame(results)\n",
    "        \n",
    "        return result_df, latency_ms\n",
    "    \n",
    "    def compute_derived_features(self, base_features_df):\n",
    "        \"\"\"\n",
    "        Compute derived features from base features.\n",
    "        \n",
    "        Examples:\n",
    "        - Ratios: vdd_idd_ratio = vdd / idd\n",
    "        - Differences: vdd_delta = vdd - vdd_nominal\n",
    "        - Aggregations: already cached in base features\n",
    "        \"\"\"\n",
    "        df = base_features_df.copy()\n",
    "        \n",
    "        # Example derived features for wafer test\n",
    "        if 'avg_vdd' in df.columns and 'std_vdd' in df.columns:\n",
    "            df['vdd_coefficient_of_variation'] = df['std_vdd'] / df['avg_vdd']\n",
    "        \n",
    "        if 'yield_pct' in df.columns and 'neighbor_yield_avg' in df.columns:\n",
    "            df['yield_vs_neighbor_delta'] = df['yield_pct'] - df['neighbor_yield_avg']\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_features_with_transformations(self, entity_ids, base_features, derived_features=None):\n",
    "        \"\"\"\n",
    "        Retrieve base features and compute derived features.\n",
    "        \n",
    "        Workflow:\n",
    "        1. Fetch base features from cache/store (fast)\n",
    "        2. Compute derived features (simple math, <1ms)\n",
    "        3. Return combined feature set\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get base features\n",
    "        base_df, base_latency = self.get_features_batch(entity_ids, base_features)\n",
    "        \n",
    "        # Compute derived features\n",
    "        if derived_features:\n",
    "            base_df = self.compute_derived_features(base_df)\n",
    "        \n",
    "        total_latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return base_df, total_latency_ms\n",
    "    \n",
    "    def get_latency_stats(self):\n",
    "        \"\"\"Calculate latency percentiles.\"\"\"\n",
    "        if not self.latency_log:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            'p50_ms': np.percentile(self.latency_log, 50),\n",
    "            'p95_ms': np.percentile(self.latency_log, 95),\n",
    "            'p99_ms': np.percentile(self.latency_log, 99),\n",
    "            'mean_ms': np.mean(self.latency_log),\n",
    "            'max_ms': np.max(self.latency_log),\n",
    "            'total_requests': len(self.latency_log)\n",
    "        }\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        \"\"\"Calculate cache hit rate.\"\"\"\n",
    "        total = self.cache_hits + self.cache_misses\n",
    "        \n",
    "        if total == 0:\n",
    "            return {'hit_rate': 0, 'cache_size': 0}\n",
    "        \n",
    "        return {\n",
    "            'hit_rate': self.cache_hits / total,\n",
    "            'cache_hits': self.cache_hits,\n",
    "            'cache_misses': self.cache_misses,\n",
    "            'cache_size': len(self.cache)\n",
    "        }\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear cache (for testing, debugging).\"\"\"\n",
    "        self.cache = {}\n",
    "        print(\"üóëÔ∏è  Cache cleared\")\n",
    "\n",
    "# Example: Real-time feature serving for binning model\n",
    "print(\"‚ö° Real-Time Feature Serving: Device Binning Model\\\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize feature server\n",
    "feature_server = RealTimeFeatureServer(fs)\n",
    "\n",
    "# Simulate production inference workload\n",
    "print(\"SIMULATION: Production inference workload (1000 requests)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate 1000 inference requests (random wafer IDs)\n",
    "inference_requests = []\n",
    "available_wafer_ids = list(fs.online_store.keys())\n",
    "\n",
    "for i in range(1000):\n",
    "    # Randomly select 1-5 wafers per request\n",
    "    batch_size = np.random.randint(1, 6)\n",
    "    entity_ids = np.random.choice(available_wafer_ids, batch_size, replace=False).tolist()\n",
    "    inference_requests.append(entity_ids)\n",
    "\n",
    "print(f\"Generated {len(inference_requests)} inference requests\")\n",
    "print(f\"Batch sizes: 1-5 wafers per request\")\n",
    "print()\n",
    "\n",
    "# Process requests with caching\n",
    "base_features = ['avg_vdd', 'std_vdd', 'avg_test_time_ms', 'neighbor_yield_avg']\n",
    "\n",
    "print(\"Processing requests...\")\n",
    "for i, entity_ids in enumerate(inference_requests):\n",
    "    features_df, latency = feature_server.get_features_with_transformations(\n",
    "        entity_ids=entity_ids,\n",
    "        base_features=base_features,\n",
    "        derived_features=True\n",
    "    )\n",
    "    \n",
    "    if (i + 1) % 200 == 0:\n",
    "        print(f\"  Processed {i + 1}/1000 requests...\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"LATENCY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "latency_stats = feature_server.get_latency_stats()\n",
    "\n",
    "print(f\"\\\\nLatency Statistics (1000 requests):\")\n",
    "print(f\"  Mean latency:   {latency_stats['mean_ms']:.2f}ms\")\n",
    "print(f\"  P50 latency:    {latency_stats['p50_ms']:.2f}ms\")\n",
    "print(f\"  P95 latency:    {latency_stats['p95_ms']:.2f}ms\")\n",
    "print(f\"  P99 latency:    {latency_stats['p99_ms']:.2f}ms\")\n",
    "print(f\"  Max latency:    {latency_stats['max_ms']:.2f}ms\")\n",
    "\n",
    "# Check SLA compliance (p99 < 10ms)\n",
    "sla_target_p99 = 10.0\n",
    "sla_met = latency_stats['p99_ms'] < sla_target_p99\n",
    "\n",
    "print(f\"\\\\nSLA Compliance:\")\n",
    "print(f\"  Target p99: <{sla_target_p99}ms\")\n",
    "print(f\"  Actual p99: {latency_stats['p99_ms']:.2f}ms\")\n",
    "print(f\"  Status: {'‚úÖ MET' if sla_met else '‚ùå VIOLATED'}\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"CACHE PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cache_stats = feature_server.get_cache_stats()\n",
    "\n",
    "print(f\"\\\\nCache Statistics:\")\n",
    "print(f\"  Cache hit rate: {cache_stats['hit_rate']:.1%}\")\n",
    "print(f\"  Cache hits:     {cache_stats['cache_hits']}\")\n",
    "print(f\"  Cache misses:   {cache_stats['cache_misses']}\")\n",
    "print(f\"  Cache size:     {cache_stats['cache_size']} entries\")\n",
    "\n",
    "print(f\"\\\\nCache Effectiveness:\")\n",
    "if cache_stats['hit_rate'] > 0.8:\n",
    "    print(f\"  ‚úÖ Excellent (>{80}% hit rate) - Cache is highly effective\")\n",
    "elif cache_stats['hit_rate'] > 0.5:\n",
    "    print(f\"  ‚ö†Ô∏è  Good ({cache_stats['hit_rate']:.0%} hit rate) - Consider cache warming\")\n",
    "else:\n",
    "    print(f\"  ‚ùå Poor (<50% hit rate) - Increase cache size or TTL\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Demonstrate cache impact\n",
    "print(\"=\"*80)\n",
    "print(\"CACHE IMPACT DEMONSTRATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Clear cache and measure latency without caching\n",
    "feature_server.clear_cache()\n",
    "feature_server.cache_hits = 0\n",
    "feature_server.cache_misses = 0\n",
    "feature_server.latency_log = []\n",
    "\n",
    "print(\"\\\\nRun 1: Without cache (cold start)\")\n",
    "test_entity_ids = available_wafer_ids[:10]\n",
    "\n",
    "for _ in range(100):\n",
    "    _, _ = feature_server.get_features_batch(test_entity_ids, base_features, use_cache=False)\n",
    "\n",
    "no_cache_stats = feature_server.get_latency_stats()\n",
    "print(f\"  Mean latency (no cache): {no_cache_stats['mean_ms']:.2f}ms\")\n",
    "\n",
    "# Now with cache\n",
    "feature_server.cache_hits = 0\n",
    "feature_server.cache_misses = 0\n",
    "feature_server.latency_log = []\n",
    "\n",
    "print(\"\\\\nRun 2: With cache (warm cache)\")\n",
    "\n",
    "for _ in range(100):\n",
    "    _, _ = feature_server.get_features_batch(test_entity_ids, base_features, use_cache=True)\n",
    "\n",
    "cache_stats_run2 = feature_server.get_latency_stats()\n",
    "cache_hit_stats = feature_server.get_cache_stats()\n",
    "\n",
    "print(f\"  Mean latency (with cache): {cache_stats_run2['mean_ms']:.2f}ms\")\n",
    "print(f\"  Cache hit rate: {cache_hit_stats['hit_rate']:.1%}\")\n",
    "print(f\"  Speedup: {no_cache_stats['mean_ms'] / cache_stats_run2['mean_ms']:.1f}x faster\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ REAL-TIME FEATURE SERVING COMPLETE\")\n",
    "print(f\"   P99 latency: {latency_stats['p99_ms']:.2f}ms (target: <10ms)\")\n",
    "print(f\"   Cache hit rate: {cache_stats['hit_rate']:.1%}\")\n",
    "print(f\"   Production-ready: {'‚úÖ YES' if sla_met and cache_stats['hit_rate'] > 0.5 else '‚ùå NO'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d8eb8",
   "metadata": {},
   "source": [
    "## 4. Model Performance Monitoring & Drift Detection\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Monitor production model performance in real-time, detect accuracy degradation and concept drift before business impact.\n",
    "\n",
    "**Key Points:**\n",
    "- **Accuracy tracking:** Monitor prediction accuracy over time (sliding window)\n",
    "- **Concept drift detection:** Statistical tests for distribution changes (KS test, PSI)\n",
    "- **Performance alerts:** Trigger when accuracy drops >5% or drift detected\n",
    "- **Root cause analysis:** Identify which features shifted (feature-level drift)\n",
    "\n",
    "**Why This Matters:** Models degrade in production (data drift, concept drift, seasonality). Early detection prevents business losses (bad predictions costing $$ before manual discovery)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d226a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPerformanceMonitor:\n",
    "    \"\"\"\n",
    "    Real-time model performance monitoring and drift detection.\n",
    "    \n",
    "    Tracks:\n",
    "    - Prediction accuracy over time (sliding window)\n",
    "    - Concept drift (target distribution changes)\n",
    "    - Data drift (feature distribution changes)\n",
    "    - Performance degradation alerts\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, window_size=1000):\n",
    "        self.model_name = model_name\n",
    "        self.window_size = window_size\n",
    "        self.predictions = []\n",
    "        self.actuals = []\n",
    "        self.timestamps = []\n",
    "        self.feature_distributions = defaultdict(list)\n",
    "        self.baseline_distribution = {}\n",
    "        \n",
    "    def log_prediction(self, prediction, actual, features, timestamp=None):\n",
    "        \"\"\"\n",
    "        Log prediction for monitoring.\n",
    "        \n",
    "        Args:\n",
    "            prediction: Model prediction\n",
    "            actual: Ground truth (if available)\n",
    "            features: Input features dict\n",
    "            timestamp: Prediction time\n",
    "        \"\"\"\n",
    "        self.predictions.append(prediction)\n",
    "        self.actuals.append(actual)\n",
    "        self.timestamps.append(timestamp or datetime.now())\n",
    "        \n",
    "        # Log feature values for drift detection\n",
    "        for feature_name, feature_value in features.items():\n",
    "            if isinstance(feature_value, (int, float)):\n",
    "                self.feature_distributions[feature_name].append(feature_value)\n",
    "    \n",
    "    def set_baseline_distribution(self, baseline_features):\n",
    "        \"\"\"\n",
    "        Set baseline feature distribution (training data).\n",
    "        \n",
    "        Used for drift detection - compare production features vs training.\n",
    "        \"\"\"\n",
    "        for feature_name, values in baseline_features.items():\n",
    "            self.baseline_distribution[feature_name] = np.array(values)\n",
    "        \n",
    "        print(f\"‚úÖ Baseline distribution set for {len(baseline_features)} features\")\n",
    "    \n",
    "    def calculate_accuracy(self, window='all'):\n",
    "        \"\"\"\n",
    "        Calculate accuracy over specified window.\n",
    "        \n",
    "        Args:\n",
    "            window: 'all', 'recent' (last N samples), or integer (exact window size)\n",
    "        \"\"\"\n",
    "        if not self.predictions or not self.actuals:\n",
    "            return None\n",
    "        \n",
    "        preds = np.array(self.predictions)\n",
    "        acts = np.array(self.actuals)\n",
    "        \n",
    "        if window == 'recent':\n",
    "            preds = preds[-self.window_size:]\n",
    "            acts = acts[-self.window_size:]\n",
    "        elif isinstance(window, int):\n",
    "            preds = preds[-window:]\n",
    "            acts = acts[-window:]\n",
    "        \n",
    "        # Calculate accuracy (for classification) or MAE (for regression)\n",
    "        if preds.dtype == acts.dtype and preds.dtype in [np.int32, np.int64, object]:\n",
    "            # Classification\n",
    "            accuracy = np.mean(preds == acts)\n",
    "            return {'type': 'classification', 'accuracy': accuracy}\n",
    "        else:\n",
    "            # Regression\n",
    "            mae = np.mean(np.abs(preds - acts))\n",
    "            rmse = np.sqrt(np.mean((preds - acts) ** 2))\n",
    "            return {'type': 'regression', 'mae': mae, 'rmse': rmse}\n",
    "    \n",
    "    def detect_concept_drift(self, reference_window=1000, current_window=500):\n",
    "        \"\"\"\n",
    "        Detect concept drift using Kolmogorov-Smirnov test.\n",
    "        \n",
    "        Compares prediction distribution between reference and current windows.\n",
    "        \n",
    "        Returns:\n",
    "            drift_detected: Boolean\n",
    "            p_value: Statistical significance\n",
    "            ks_statistic: K-S test statistic\n",
    "        \"\"\"\n",
    "        if len(self.predictions) < reference_window + current_window:\n",
    "            return None\n",
    "        \n",
    "        # Reference window (older predictions)\n",
    "        ref_preds = np.array(self.predictions[-reference_window - current_window:-current_window])\n",
    "        \n",
    "        # Current window (recent predictions)\n",
    "        cur_preds = np.array(self.predictions[-current_window:])\n",
    "        \n",
    "        # Kolmogorov-Smirnov test\n",
    "        ks_statistic, p_value = stats.ks_2samp(ref_preds, cur_preds)\n",
    "        \n",
    "        # Drift detected if p < 0.05 (distributions are different)\n",
    "        drift_detected = p_value < 0.05\n",
    "        \n",
    "        return {\n",
    "            'drift_detected': drift_detected,\n",
    "            'p_value': p_value,\n",
    "            'ks_statistic': ks_statistic,\n",
    "            'reference_window': reference_window,\n",
    "            'current_window': current_window\n",
    "        }\n",
    "    \n",
    "    def detect_feature_drift(self, feature_name, method='ks_test'):\n",
    "        \"\"\"\n",
    "        Detect drift for specific feature.\n",
    "        \n",
    "        Compares production feature distribution vs baseline (training).\n",
    "        \n",
    "        Methods:\n",
    "        - ks_test: Kolmogorov-Smirnov test (distribution comparison)\n",
    "        - psi: Population Stability Index (binned comparison)\n",
    "        \"\"\"\n",
    "        if feature_name not in self.baseline_distribution:\n",
    "            return None\n",
    "        \n",
    "        if feature_name not in self.feature_distributions:\n",
    "            return None\n",
    "        \n",
    "        baseline = self.baseline_distribution[feature_name]\n",
    "        current = np.array(self.feature_distributions[feature_name][-self.window_size:])\n",
    "        \n",
    "        if method == 'ks_test':\n",
    "            ks_statistic, p_value = stats.ks_2samp(baseline, current)\n",
    "            drift_detected = p_value < 0.05\n",
    "            \n",
    "            return {\n",
    "                'feature': feature_name,\n",
    "                'method': 'ks_test',\n",
    "                'drift_detected': drift_detected,\n",
    "                'p_value': p_value,\n",
    "                'ks_statistic': ks_statistic\n",
    "            }\n",
    "        \n",
    "        elif method == 'psi':\n",
    "            # Population Stability Index\n",
    "            psi = self._calculate_psi(baseline, current)\n",
    "            \n",
    "            # PSI thresholds:\n",
    "            # < 0.1: No significant change\n",
    "            # 0.1 - 0.25: Moderate change\n",
    "            # > 0.25: Significant change\n",
    "            drift_detected = psi > 0.25\n",
    "            \n",
    "            return {\n",
    "                'feature': feature_name,\n",
    "                'method': 'psi',\n",
    "                'drift_detected': drift_detected,\n",
    "                'psi': psi,\n",
    "                'threshold': 0.25\n",
    "            }\n",
    "    \n",
    "    def _calculate_psi(self, baseline, current, bins=10):\n",
    "        \"\"\"\n",
    "        Calculate Population Stability Index.\n",
    "        \n",
    "        PSI = Œ£ (current_pct - baseline_pct) * ln(current_pct / baseline_pct)\n",
    "        \"\"\"\n",
    "        # Create bins based on baseline distribution\n",
    "        bin_edges = np.percentile(baseline, np.linspace(0, 100, bins + 1))\n",
    "        \n",
    "        # Histogram for baseline and current\n",
    "        baseline_hist, _ = np.histogram(baseline, bins=bin_edges)\n",
    "        current_hist, _ = np.histogram(current, bins=bin_edges)\n",
    "        \n",
    "        # Normalize to percentages\n",
    "        baseline_pct = baseline_hist / len(baseline)\n",
    "        current_pct = current_hist / len(current)\n",
    "        \n",
    "        # Avoid log(0)\n",
    "        baseline_pct = np.where(baseline_pct == 0, 0.0001, baseline_pct)\n",
    "        current_pct = np.where(current_pct == 0, 0.0001, current_pct)\n",
    "        \n",
    "        # Calculate PSI\n",
    "        psi = np.sum((current_pct - baseline_pct) * np.log(current_pct / baseline_pct))\n",
    "        \n",
    "        return psi\n",
    "    \n",
    "    def check_performance_degradation(self, baseline_accuracy, threshold=0.05):\n",
    "        \"\"\"\n",
    "        Check if performance degraded compared to baseline.\n",
    "        \n",
    "        Args:\n",
    "            baseline_accuracy: Expected accuracy (from validation set)\n",
    "            threshold: Alert if accuracy drops by this amount (e.g., 0.05 = 5%)\n",
    "        \"\"\"\n",
    "        current_metrics = self.calculate_accuracy(window='recent')\n",
    "        \n",
    "        if current_metrics is None:\n",
    "            return None\n",
    "        \n",
    "        if current_metrics['type'] == 'classification':\n",
    "            current_acc = current_metrics['accuracy']\n",
    "            degradation = baseline_accuracy - current_acc\n",
    "            degraded = degradation > threshold\n",
    "            \n",
    "            return {\n",
    "                'degraded': degraded,\n",
    "                'baseline_accuracy': baseline_accuracy,\n",
    "                'current_accuracy': current_acc,\n",
    "                'degradation': degradation,\n",
    "                'threshold': threshold\n",
    "            }\n",
    "        else:\n",
    "            # For regression, use relative MAE increase\n",
    "            # (more complex, baseline MAE needed)\n",
    "            return {\n",
    "                'degraded': False,\n",
    "                'note': 'Regression degradation requires baseline MAE'\n",
    "            }\n",
    "    \n",
    "    def generate_monitoring_report(self, baseline_accuracy=None):\n",
    "        \"\"\"Generate comprehensive monitoring report.\"\"\"\n",
    "        report = {\n",
    "            'model_name': self.model_name,\n",
    "            'total_predictions': len(self.predictions),\n",
    "            'monitoring_period': {\n",
    "                'start': min(self.timestamps) if self.timestamps else None,\n",
    "                'end': max(self.timestamps) if self.timestamps else None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Accuracy metrics\n",
    "        overall_metrics = self.calculate_accuracy(window='all')\n",
    "        recent_metrics = self.calculate_accuracy(window='recent')\n",
    "        \n",
    "        report['accuracy'] = {\n",
    "            'overall': overall_metrics,\n",
    "            'recent': recent_metrics\n",
    "        }\n",
    "        \n",
    "        # Concept drift\n",
    "        concept_drift = self.detect_concept_drift()\n",
    "        report['concept_drift'] = concept_drift\n",
    "        \n",
    "        # Feature drift (for all features)\n",
    "        feature_drifts = []\n",
    "        for feature_name in self.baseline_distribution.keys():\n",
    "            drift = self.detect_feature_drift(feature_name, method='psi')\n",
    "            if drift:\n",
    "                feature_drifts.append(drift)\n",
    "        \n",
    "        report['feature_drift'] = feature_drifts\n",
    "        \n",
    "        # Performance degradation\n",
    "        if baseline_accuracy:\n",
    "            degradation = self.check_performance_degradation(baseline_accuracy)\n",
    "            report['degradation'] = degradation\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Example: Model performance monitoring for yield prediction\n",
    "print(\"üìä Model Performance Monitoring: Yield Prediction\\\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train baseline model\n",
    "np.random.seed(42)\n",
    "\n",
    "n_train = 1000\n",
    "X_train_monitor = pd.DataFrame({\n",
    "    'avg_vdd': np.random.normal(1.2, 0.02, n_train),\n",
    "    'std_vdd': np.random.uniform(0.01, 0.05, n_train),\n",
    "    'avg_test_time_ms': np.random.normal(100, 10, n_train),\n",
    "    'neighbor_yield_avg': np.random.uniform(85, 99, n_train)\n",
    "})\n",
    "y_train_monitor = 0.9 * X_train_monitor['neighbor_yield_avg'] + np.random.normal(0, 2, n_train)\n",
    "\n",
    "model_monitor = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "model_monitor.fit(X_train_monitor, y_train_monitor)\n",
    "\n",
    "# Calculate baseline accuracy\n",
    "y_pred_baseline = model_monitor.predict(X_train_monitor)\n",
    "baseline_mae = mean_absolute_error(y_train_monitor, y_pred_baseline)\n",
    "\n",
    "print(f\"‚úÖ Model trained\")\n",
    "print(f\"   Baseline MAE: {baseline_mae:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Initialize performance monitor\n",
    "monitor = ModelPerformanceMonitor(model_name='yield_prediction_v2.0', window_size=500)\n",
    "\n",
    "# Set baseline distribution (training data)\n",
    "baseline_features = {\n",
    "    'avg_vdd': X_train_monitor['avg_vdd'].values,\n",
    "    'std_vdd': X_train_monitor['std_vdd'].values,\n",
    "    'avg_test_time_ms': X_train_monitor['avg_test_time_ms'].values,\n",
    "    'neighbor_yield_avg': X_train_monitor['neighbor_yield_avg'].values\n",
    "}\n",
    "\n",
    "monitor.set_baseline_distribution(baseline_features)\n",
    "print()\n",
    "\n",
    "# Simulate production predictions (1500 samples)\n",
    "print(\"=\"*80)\n",
    "print(\"SIMULATING PRODUCTION PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Normal predictions (first 1000 samples - similar to training)\n",
    "print(\"\\\\nPhase 1: Normal operation (1000 predictions)...\")\n",
    "\n",
    "for i in range(1000):\n",
    "    features = {\n",
    "        'avg_vdd': np.random.normal(1.2, 0.02),\n",
    "        'std_vdd': np.random.uniform(0.01, 0.05),\n",
    "        'avg_test_time_ms': np.random.normal(100, 10),\n",
    "        'neighbor_yield_avg': np.random.uniform(85, 99)\n",
    "    }\n",
    "    \n",
    "    X = pd.DataFrame([features])\n",
    "    prediction = model_monitor.predict(X)[0]\n",
    "    \n",
    "    # Simulate ground truth\n",
    "    actual = 0.9 * features['neighbor_yield_avg'] + np.random.normal(0, 2)\n",
    "    \n",
    "    monitor.log_prediction(\n",
    "        prediction=prediction,\n",
    "        actual=actual,\n",
    "        features=features,\n",
    "        timestamp=datetime.now() - timedelta(hours=1000-i)\n",
    "    )\n",
    "\n",
    "print(\"  ‚úÖ Logged 1000 predictions\")\n",
    "\n",
    "# Drifted predictions (next 500 samples - distribution shift)\n",
    "print(\"\\\\nPhase 2: Data drift (500 predictions)...\")\n",
    "print(\"  ‚ö†Ô∏è  avg_vdd shifted from 1.2V to 1.25V (process change)\")\n",
    "\n",
    "for i in range(500):\n",
    "    features = {\n",
    "        'avg_vdd': np.random.normal(1.25, 0.02),  # SHIFTED!\n",
    "        'std_vdd': np.random.uniform(0.01, 0.05),\n",
    "        'avg_test_time_ms': np.random.normal(100, 10),\n",
    "        'neighbor_yield_avg': np.random.uniform(85, 99)\n",
    "    }\n",
    "    \n",
    "    X = pd.DataFrame([features])\n",
    "    prediction = model_monitor.predict(X)[0]\n",
    "    \n",
    "    # Ground truth also shifts (concept drift)\n",
    "    actual = 0.85 * features['neighbor_yield_avg'] + np.random.normal(0, 3)\n",
    "    \n",
    "    monitor.log_prediction(\n",
    "        prediction=prediction,\n",
    "        actual=actual,\n",
    "        features=features,\n",
    "        timestamp=datetime.now() - timedelta(hours=500-i)\n",
    "    )\n",
    "\n",
    "print(\"  ‚úÖ Logged 500 predictions (with drift)\")\n",
    "print()\n",
    "\n",
    "# Generate monitoring report\n",
    "print(\"=\"*80)\n",
    "print(\"MONITORING REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "report = monitor.generate_monitoring_report(baseline_accuracy=0.95)\n",
    "\n",
    "print(f\"\\\\nModel: {report['model_name']}\")\n",
    "print(f\"Total predictions: {report['total_predictions']}\")\n",
    "print(f\"Monitoring period: {report['monitoring_period']['start']} to {report['monitoring_period']['end']}\")\n",
    "\n",
    "print(f\"\\\\nACCURACY METRICS:\")\n",
    "print(f\"  Recent window MAE: {report['accuracy']['recent']['mae']:.2f}%\")\n",
    "print(f\"  Recent window RMSE: {report['accuracy']['recent']['rmse']:.2f}%\")\n",
    "\n",
    "print(f\"\\\\nCONCEPT DRIFT DETECTION:\")\n",
    "if report['concept_drift']:\n",
    "    cd = report['concept_drift']\n",
    "    print(f\"  Drift detected: {'‚ö†Ô∏è  YES' if cd['drift_detected'] else '‚úÖ NO'}\")\n",
    "    print(f\"  KS statistic: {cd['ks_statistic']:.4f}\")\n",
    "    print(f\"  P-value: {cd['p_value']:.4f}\")\n",
    "    print(f\"  Reference window: {cd['reference_window']} predictions\")\n",
    "    print(f\"  Current window: {cd['current_window']} predictions\")\n",
    "\n",
    "print(f\"\\\\nFEATURE DRIFT DETECTION (PSI):\")\n",
    "drifted_features = [fd for fd in report['feature_drift'] if fd['drift_detected']]\n",
    "\n",
    "for fd in report['feature_drift']:\n",
    "    status = \"‚ö†Ô∏è  DRIFT\" if fd['drift_detected'] else \"‚úÖ OK\"\n",
    "    print(f\"  {fd['feature']}: {status} (PSI={fd['psi']:.3f})\")\n",
    "\n",
    "if drifted_features:\n",
    "    print(f\"\\\\n‚ö†Ô∏è  WARNING: {len(drifted_features)} feature(s) showing drift!\")\n",
    "    for fd in drifted_features:\n",
    "        print(f\"     - {fd['feature']}: PSI={fd['psi']:.3f} (threshold={fd['threshold']})\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if report['concept_drift'] and report['concept_drift']['drift_detected']:\n",
    "    print(\"\\\\nüö® CONCEPT DRIFT DETECTED\")\n",
    "    print(\"   Action: Retrain model with recent data\")\n",
    "    print(\"   Timeline: Immediate (accuracy may be degrading)\")\n",
    "\n",
    "if drifted_features:\n",
    "    print(\"\\\\n‚ö†Ô∏è  FEATURE DRIFT DETECTED\")\n",
    "    print(f\"   Affected features: {', '.join([fd['feature'] for fd in drifted_features])}\")\n",
    "    print(\"   Action: Investigate root cause (process change, sensor drift, data pipeline issue)\")\n",
    "    print(\"   Timeline: Within 24 hours\")\n",
    "\n",
    "if not (report['concept_drift'] and report['concept_drift']['drift_detected']) and not drifted_features:\n",
    "    print(\"\\\\n‚úÖ NO DRIFT DETECTED\")\n",
    "    print(\"   Model performance stable\")\n",
    "    print(\"   Continue monitoring\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ MODEL PERFORMANCE MONITORING COMPLETE\")\n",
    "print(f\"   Total predictions monitored: {report['total_predictions']}\")\n",
    "print(f\"   Drift detection: {'‚ö†Ô∏è  ACTIVE' if drifted_features or (report['concept_drift'] and report['concept_drift']['drift_detected']) else '‚úÖ NORMAL'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a27b469",
   "metadata": {},
   "source": [
    "## 5. Data Quality Monitoring for ML Pipelines\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement comprehensive data quality checks to prevent bad data from reaching models (garbage in ‚Üí garbage out).\n",
    "\n",
    "**Key Points:**\n",
    "- **Schema validation:** Ensure required fields present, correct data types\n",
    "- **Range checks:** Flag values outside expected bounds (Vdd: 1.0-1.4V)\n",
    "- **Null detection:** Alert on missing critical parameters\n",
    "- **Distribution monitoring:** Compare current vs historical data distributions\n",
    "\n",
    "**Why This Matters:** Bad data causes model failures (exceptions, incorrect predictions, degraded accuracy). Data quality checks catch issues upstream before they impact production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e67a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityMonitor:\n",
    "    \"\"\"\n",
    "    Data quality monitoring for ML pipelines.\n",
    "    \n",
    "    Checks:\n",
    "    - Schema validation (required fields, data types)\n",
    "    - Range validation (min/max bounds)\n",
    "    - Null detection (missing values)\n",
    "    - Distribution validation (statistical comparison)\n",
    "    - Anomaly detection (outliers)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.schema = {}\n",
    "        self.validation_rules = {}\n",
    "        self.baseline_stats = {}\n",
    "        self.quality_issues = []\n",
    "        \n",
    "    def define_schema(self, schema):\n",
    "        \"\"\"\n",
    "        Define expected schema.\n",
    "        \n",
    "        Schema format:\n",
    "        {\n",
    "            'field_name': {\n",
    "                'type': 'float'/'int'/'string',\n",
    "                'required': True/False,\n",
    "                'nullable': True/False\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.schema = schema\n",
    "        print(f\"‚úÖ Schema defined for {self.dataset_name}\")\n",
    "        print(f\"   Fields: {len(schema)}\")\n",
    "        \n",
    "        return schema\n",
    "    \n",
    "    def add_validation_rule(self, field, rule_type, **kwargs):\n",
    "        \"\"\"\n",
    "        Add validation rule for field.\n",
    "        \n",
    "        Rule types:\n",
    "        - range: min, max\n",
    "        - categorical: allowed_values\n",
    "        - regex: pattern\n",
    "        - custom: function\n",
    "        \"\"\"\n",
    "        if field not in self.validation_rules:\n",
    "            self.validation_rules[field] = []\n",
    "        \n",
    "        rule = {'type': rule_type, **kwargs}\n",
    "        self.validation_rules[field].append(rule)\n",
    "        \n",
    "        return rule\n",
    "    \n",
    "    def validate_schema(self, df):\n",
    "        \"\"\"\n",
    "        Validate DataFrame against schema.\n",
    "        \n",
    "        Checks:\n",
    "        - Required fields present\n",
    "        - Correct data types\n",
    "        - Nullable constraints\n",
    "        \"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Check required fields\n",
    "        for field, spec in self.schema.items():\n",
    "            if spec.get('required', False):\n",
    "                if field not in df.columns:\n",
    "                    issues.append({\n",
    "                        'type': 'missing_field',\n",
    "                        'field': field,\n",
    "                        'severity': 'critical',\n",
    "                        'message': f\"Required field '{field}' missing\"\n",
    "                    })\n",
    "        \n",
    "        # Check data types\n",
    "        for field in df.columns:\n",
    "            if field in self.schema:\n",
    "                expected_type = self.schema[field]['type']\n",
    "                actual_dtype = str(df[field].dtype)\n",
    "                \n",
    "                # Type validation (simplified)\n",
    "                type_mismatch = False\n",
    "                if expected_type == 'float' and not ('float' in actual_dtype or 'int' in actual_dtype):\n",
    "                    type_mismatch = True\n",
    "                elif expected_type == 'int' and 'int' not in actual_dtype:\n",
    "                    type_mismatch = True\n",
    "                elif expected_type == 'string' and 'object' not in actual_dtype:\n",
    "                    type_mismatch = True\n",
    "                \n",
    "                if type_mismatch:\n",
    "                    issues.append({\n",
    "                        'type': 'type_mismatch',\n",
    "                        'field': field,\n",
    "                        'severity': 'high',\n",
    "                        'expected': expected_type,\n",
    "                        'actual': actual_dtype,\n",
    "                        'message': f\"Field '{field}' type mismatch: expected {expected_type}, got {actual_dtype}\"\n",
    "                    })\n",
    "        \n",
    "        # Check nullability\n",
    "        for field, spec in self.schema.items():\n",
    "            if field in df.columns and not spec.get('nullable', True):\n",
    "                null_count = df[field].isnull().sum()\n",
    "                if null_count > 0:\n",
    "                    issues.append({\n",
    "                        'type': 'null_constraint_violation',\n",
    "                        'field': field,\n",
    "                        'severity': 'high',\n",
    "                        'null_count': null_count,\n",
    "                        'message': f\"Field '{field}' has {null_count} nulls (not nullable)\"\n",
    "                    })\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def validate_ranges(self, df):\n",
    "        \"\"\"Validate field values against range rules.\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        for field, rules in self.validation_rules.items():\n",
    "            if field not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            for rule in rules:\n",
    "                if rule['type'] == 'range':\n",
    "                    min_val = rule.get('min')\n",
    "                    max_val = rule.get('max')\n",
    "                    \n",
    "                    if min_val is not None:\n",
    "                        violations = df[df[field] < min_val]\n",
    "                        if len(violations) > 0:\n",
    "                            issues.append({\n",
    "                                'type': 'range_violation',\n",
    "                                'field': field,\n",
    "                                'severity': 'medium',\n",
    "                                'rule': f'min={min_val}',\n",
    "                                'violation_count': len(violations),\n",
    "                                'message': f\"Field '{field}' has {len(violations)} values < {min_val}\"\n",
    "                            })\n",
    "                    \n",
    "                    if max_val is not None:\n",
    "                        violations = df[df[field] > max_val]\n",
    "                        if len(violations) > 0:\n",
    "                            issues.append({\n",
    "                                'type': 'range_violation',\n",
    "                                'field': field,\n",
    "                                'severity': 'medium',\n",
    "                                'rule': f'max={max_val}',\n",
    "                                'violation_count': len(violations),\n",
    "                                'message': f\"Field '{field}' has {len(violations)} values > {max_val}\"\n",
    "                            })\n",
    "                \n",
    "                elif rule['type'] == 'categorical':\n",
    "                    allowed_values = rule.get('allowed_values', [])\n",
    "                    violations = df[~df[field].isin(allowed_values)]\n",
    "                    \n",
    "                    if len(violations) > 0:\n",
    "                        issues.append({\n",
    "                            'type': 'categorical_violation',\n",
    "                            'field': field,\n",
    "                            'severity': 'medium',\n",
    "                            'violation_count': len(violations),\n",
    "                            'message': f\"Field '{field}' has {len(violations)} values not in allowed set\"\n",
    "                        })\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def set_baseline_statistics(self, df):\n",
    "        \"\"\"\n",
    "        Calculate baseline statistics for distribution monitoring.\n",
    "        \n",
    "        Stores mean, std, min, max, percentiles for numerical fields.\n",
    "        \"\"\"\n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            self.baseline_stats[col] = {\n",
    "                'mean': df[col].mean(),\n",
    "                'std': df[col].std(),\n",
    "                'min': df[col].min(),\n",
    "                'max': df[col].max(),\n",
    "                'p25': df[col].quantile(0.25),\n",
    "                'p50': df[col].quantile(0.50),\n",
    "                'p75': df[col].quantile(0.75),\n",
    "                'p95': df[col].quantile(0.95)\n",
    "            }\n",
    "        \n",
    "        print(f\"‚úÖ Baseline statistics set for {len(self.baseline_stats)} numerical fields\")\n",
    "        \n",
    "        return self.baseline_stats\n",
    "    \n",
    "    def validate_distribution(self, df, threshold_std_shift=2.0):\n",
    "        \"\"\"\n",
    "        Validate current data distribution vs baseline.\n",
    "        \n",
    "        Flags if mean shifts by >2 standard deviations.\n",
    "        \"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        for col, baseline in self.baseline_stats.items():\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            current_mean = df[col].mean()\n",
    "            baseline_mean = baseline['mean']\n",
    "            baseline_std = baseline['std']\n",
    "            \n",
    "            # Check if mean shifted significantly\n",
    "            shift_in_stds = abs(current_mean - baseline_mean) / baseline_std if baseline_std > 0 else 0\n",
    "            \n",
    "            if shift_in_stds > threshold_std_shift:\n",
    "                issues.append({\n",
    "                    'type': 'distribution_shift',\n",
    "                    'field': col,\n",
    "                    'severity': 'high',\n",
    "                    'baseline_mean': baseline_mean,\n",
    "                    'current_mean': current_mean,\n",
    "                    'shift_stds': shift_in_stds,\n",
    "                    'message': f\"Field '{col}' mean shifted by {shift_in_stds:.1f} std devs\"\n",
    "                })\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def detect_anomalies(self, df, method='iqr', threshold=3.0):\n",
    "        \"\"\"\n",
    "        Detect anomalies/outliers in numerical fields.\n",
    "        \n",
    "        Methods:\n",
    "        - iqr: Interquartile range (values outside Q1 - 1.5*IQR or Q3 + 1.5*IQR)\n",
    "        - zscore: Z-score (values with |z| > threshold)\n",
    "        \"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            if method == 'iqr':\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                \n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                anomalies = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "                \n",
    "                if len(anomalies) > 0:\n",
    "                    issues.append({\n",
    "                        'type': 'anomaly',\n",
    "                        'field': col,\n",
    "                        'severity': 'low',\n",
    "                        'method': 'iqr',\n",
    "                        'anomaly_count': len(anomalies),\n",
    "                        'anomaly_pct': len(anomalies) / len(df) * 100,\n",
    "                        'message': f\"Field '{col}' has {len(anomalies)} anomalies ({len(anomalies)/len(df)*100:.1f}%)\"\n",
    "                    })\n",
    "            \n",
    "            elif method == 'zscore':\n",
    "                mean = df[col].mean()\n",
    "                std = df[col].std()\n",
    "                \n",
    "                if std > 0:\n",
    "                    z_scores = np.abs((df[col] - mean) / std)\n",
    "                    anomalies = df[z_scores > threshold]\n",
    "                    \n",
    "                    if len(anomalies) > 0:\n",
    "                        issues.append({\n",
    "                            'type': 'anomaly',\n",
    "                            'field': col,\n",
    "                            'severity': 'low',\n",
    "                            'method': 'zscore',\n",
    "                            'anomaly_count': len(anomalies),\n",
    "                            'anomaly_pct': len(anomalies) / len(df) * 100,\n",
    "                            'message': f\"Field '{col}' has {len(anomalies)} anomalies (|z| > {threshold})\"\n",
    "                        })\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def run_quality_checks(self, df):\n",
    "        \"\"\"\n",
    "        Run all data quality checks.\n",
    "        \n",
    "        Returns comprehensive quality report.\n",
    "        \"\"\"\n",
    "        all_issues = []\n",
    "        \n",
    "        # Schema validation\n",
    "        schema_issues = self.validate_schema(df)\n",
    "        all_issues.extend(schema_issues)\n",
    "        \n",
    "        # Range validation\n",
    "        range_issues = self.validate_ranges(df)\n",
    "        all_issues.extend(range_issues)\n",
    "        \n",
    "        # Distribution validation\n",
    "        if self.baseline_stats:\n",
    "            dist_issues = self.validate_distribution(df)\n",
    "            all_issues.extend(dist_issues)\n",
    "        \n",
    "        # Anomaly detection\n",
    "        anomaly_issues = self.detect_anomalies(df)\n",
    "        all_issues.extend(anomaly_issues)\n",
    "        \n",
    "        # Store issues\n",
    "        self.quality_issues.extend(all_issues)\n",
    "        \n",
    "        # Categorize by severity\n",
    "        critical = [i for i in all_issues if i['severity'] == 'critical']\n",
    "        high = [i for i in all_issues if i['severity'] == 'high']\n",
    "        medium = [i for i in all_issues if i['severity'] == 'medium']\n",
    "        low = [i for i in all_issues if i['severity'] == 'low']\n",
    "        \n",
    "        report = {\n",
    "            'dataset': self.dataset_name,\n",
    "            'row_count': len(df),\n",
    "            'column_count': len(df.columns),\n",
    "            'total_issues': len(all_issues),\n",
    "            'issues_by_severity': {\n",
    "                'critical': len(critical),\n",
    "                'high': len(high),\n",
    "                'medium': len(medium),\n",
    "                'low': len(low)\n",
    "            },\n",
    "            'issues': all_issues,\n",
    "            'passed': len(critical) == 0 and len(high) == 0\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Example: Data quality monitoring for STDF wafer test data\n",
    "print(\"üîç Data Quality Monitoring: STDF Wafer Test Data\\\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize data quality monitor\n",
    "dq = DataQualityMonitor(\"stdf_wafer_test\")\n",
    "\n",
    "# Define schema for wafer test data\n",
    "schema = {\n",
    "    'wafer_id': {'type': 'string', 'required': True, 'nullable': False},\n",
    "    'device_id': {'type': 'string', 'required': True, 'nullable': False},\n",
    "    'vdd': {'type': 'float', 'required': True, 'nullable': False},\n",
    "    'idd': {'type': 'float', 'required': True, 'nullable': False},\n",
    "    'frequency': {'type': 'float', 'required': True, 'nullable': False},\n",
    "    'temperature': {'type': 'float', 'required': False, 'nullable': True},\n",
    "    'test_time_ms': {'type': 'float', 'required': True, 'nullable': False},\n",
    "    'pass_fail': {'type': 'int', 'required': True, 'nullable': False}\n",
    "}\n",
    "\n",
    "dq.define_schema(schema)\n",
    "print()\n",
    "\n",
    "# Add validation rules\n",
    "dq.add_validation_rule('vdd', 'range', min=1.0, max=1.4)\n",
    "dq.add_validation_rule('idd', 'range', min=0, max=200)\n",
    "dq.add_validation_rule('frequency', 'range', min=1000, max=3000)\n",
    "dq.add_validation_rule('temperature', 'range', min=-40, max=125)\n",
    "dq.add_validation_rule('test_time_ms', 'range', min=10, max=500)\n",
    "dq.add_validation_rule('pass_fail', 'categorical', allowed_values=[0, 1])\n",
    "\n",
    "print(\"‚úÖ Validation rules added\")\n",
    "print(\"   vdd: 1.0-1.4V\")\n",
    "print(\"   idd: 0-200mA\")\n",
    "print(\"   frequency: 1000-3000 MHz\")\n",
    "print(\"   temperature: -40 to 125¬∞C\")\n",
    "print(\"   test_time_ms: 10-500ms\")\n",
    "print(\"   pass_fail: 0 or 1\")\n",
    "print()\n",
    "\n",
    "# Generate baseline dataset (good quality)\n",
    "n_baseline = 1000\n",
    "\n",
    "baseline_data = pd.DataFrame({\n",
    "    'wafer_id': [f'W{i//100:04d}' for i in range(n_baseline)],\n",
    "    'device_id': [f'D{i:06d}' for i in range(n_baseline)],\n",
    "    'vdd': np.random.normal(1.2, 0.02, n_baseline),\n",
    "    'idd': np.random.normal(50, 5, n_baseline),\n",
    "    'frequency': np.random.normal(2400, 100, n_baseline),\n",
    "    'temperature': np.random.normal(25, 10, n_baseline),\n",
    "    'test_time_ms': np.random.normal(100, 10, n_baseline),\n",
    "    'pass_fail': np.random.choice([0, 1], n_baseline, p=[0.05, 0.95])\n",
    "})\n",
    "\n",
    "# Set baseline statistics\n",
    "dq.set_baseline_statistics(baseline_data)\n",
    "print()\n",
    "\n",
    "# Test dataset with quality issues\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING DATA QUALITY CHECKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate test dataset with intentional issues\n",
    "n_test = 200\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    'wafer_id': [f'W{i//100:04d}' for i in range(n_test)],\n",
    "    'device_id': [f'D{i:06d}' for i in range(n_test)],\n",
    "    'vdd': np.concatenate([\n",
    "        np.random.normal(1.2, 0.02, 180),\n",
    "        np.random.normal(1.5, 0.05, 20)  # 20 out-of-range values\n",
    "    ]),\n",
    "    'idd': np.random.normal(50, 5, n_test),\n",
    "    'frequency': np.random.normal(2600, 100, n_test),  # Mean shifted from 2400\n",
    "    'temperature': np.random.normal(25, 10, n_test),\n",
    "    'test_time_ms': np.random.normal(100, 10, n_test),\n",
    "    'pass_fail': np.random.choice([0, 1], n_test, p=[0.05, 0.95])\n",
    "})\n",
    "\n",
    "# Introduce some nulls\n",
    "test_data.loc[0:5, 'temperature'] = None\n",
    "\n",
    "print(f\"\\\\nTest dataset: {len(test_data)} rows\")\n",
    "print(f\"Intentional issues:\")\n",
    "print(f\"  - 20 vdd values > 1.4V (range violation)\")\n",
    "print(f\"  - 6 null temperature values\")\n",
    "print(f\"  - frequency mean shifted from 2400 to 2600 (distribution shift)\")\n",
    "print()\n",
    "\n",
    "# Run quality checks\n",
    "report = dq.run_quality_checks(test_data)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\\\nDataset: {report['dataset']}\")\n",
    "print(f\"Rows: {report['row_count']}\")\n",
    "print(f\"Columns: {report['column_count']}\")\n",
    "\n",
    "print(f\"\\\\nISSUES BY SEVERITY:\")\n",
    "print(f\"  Critical: {report['issues_by_severity']['critical']}\")\n",
    "print(f\"  High: {report['issues_by_severity']['high']}\")\n",
    "print(f\"  Medium: {report['issues_by_severity']['medium']}\")\n",
    "print(f\"  Low: {report['issues_by_severity']['low']}\")\n",
    "print(f\"  Total: {report['total_issues']}\")\n",
    "\n",
    "print(f\"\\\\nQUALITY CHECK: {'‚úÖ PASSED' if report['passed'] else '‚ùå FAILED'}\")\n",
    "\n",
    "print(f\"\\\\nDETAILED ISSUES:\")\n",
    "for issue in report['issues']:\n",
    "    severity_icon = {'critical': 'üî¥', 'high': 'üü†', 'medium': 'üü°', 'low': 'üîµ'}\n",
    "    icon = severity_icon.get(issue['severity'], '‚ö™')\n",
    "    print(f\"  {icon} [{issue['severity'].upper()}] {issue['message']}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Action recommendations\n",
    "print(\"=\"*80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if report['issues_by_severity']['critical'] > 0:\n",
    "    print(\"\\\\nüî¥ CRITICAL ISSUES - BLOCK DATA PIPELINE\")\n",
    "    print(\"   Action: Do not process this data batch\")\n",
    "    print(\"   Investigate root cause immediately\")\n",
    "\n",
    "if report['issues_by_severity']['high'] > 0:\n",
    "    print(\"\\\\nüü† HIGH SEVERITY ISSUES - ALERT ON-CALL\")\n",
    "    print(\"   Action: Review issues, may indicate data pipeline problem\")\n",
    "    print(\"   Examples: Missing required fields, type mismatches, excessive nulls\")\n",
    "\n",
    "if report['issues_by_severity']['medium'] > 0:\n",
    "    print(\"\\\\nüü° MEDIUM SEVERITY ISSUES - LOG FOR REVIEW\")\n",
    "    print(\"   Action: Process data but flag for investigation\")\n",
    "    print(\"   Examples: Range violations, unexpected categorical values\")\n",
    "\n",
    "if report['issues_by_severity']['low'] > 0:\n",
    "    print(\"\\\\nüîµ LOW SEVERITY ISSUES - INFORMATIONAL\")\n",
    "    print(\"   Action: Monitor trends over time\")\n",
    "    print(\"   Examples: Anomalies/outliers (may be valid extreme cases)\")\n",
    "\n",
    "if report['passed']:\n",
    "    print(\"\\\\n‚úÖ ALL CRITICAL & HIGH CHECKS PASSED\")\n",
    "    print(\"   Data quality acceptable for production\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ DATA QUALITY MONITORING COMPLETE\")\n",
    "print(f\"   Total issues found: {report['total_issues']}\")\n",
    "print(f\"   Data quality: {'‚úÖ ACCEPTABLE' if report['passed'] else '‚ùå UNACCEPTABLE'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba9f17",
   "metadata": {},
   "source": [
    "## 6. üöÄ Real-World Project Templates\n",
    "\n",
    "### Project 1: Enterprise Feature Store for Wafer Test Data\n",
    "\n",
    "**Objective:** Build production feature store serving 10,000+ wafer features with <5ms p99 latency for real-time binning decisions.\n",
    "\n",
    "**Business Value:** Consistent features between training and inference eliminate 10-30% accuracy drop from training-serving skew. Feature reuse across 5 models saves 200 engineering hours/year.\n",
    "\n",
    "**Features to Implement:**\n",
    "- Offline store: Parquet-partitioned storage on S3 (30-day rolling window, 1TB data)\n",
    "- Online store: Redis cluster (10K QPS, <5ms p99 latency)\n",
    "- Feature groups: wafer_aggregates, device_parametrics, spatial_correlations, temporal_trends\n",
    "- Point-in-time joins: Prevent data leakage (no future features in past training)\n",
    "- Feature versioning: Track definition changes (v1.0 ‚Üí v1.1 migration)\n",
    "- Lineage tracking: STDF fields ‚Üí derived features ‚Üí model predictions\n",
    "\n",
    "**Success Criteria:**\n",
    "- Online store p99 latency <5ms for 10K concurrent requests\n",
    "- Offline batch feature generation processes 1TB data in <30 minutes\n",
    "- Zero training-serving skew (offline/online feature agreement >99.9%)\n",
    "- Feature versioning supports rollback within 15 minutes\n",
    "- Comprehensive lineage for audit compliance (FDA, automotive)\n",
    "\n",
    "**STDF Data Application:**\n",
    "- Raw data: 10K parametric tests per device (voltage, current, frequency, power)\n",
    "- Feature groups: Wafer-level aggregates (mean/std Vdd, yield%, spatial patterns)\n",
    "- Real-time serving: Binning model fetches features for 1,000 devices/sec\n",
    "- Training: 30-day historical features for yield prediction model retraining\n",
    "\n",
    "---\n",
    "\n",
    "### Project 2: Real-Time Feature Serving for Fraud Detection\n",
    "\n",
    "**Objective:** Build low-latency feature pipeline serving user transaction features with <10ms p99 latency for real-time fraud scoring.\n",
    "\n",
    "**Business Value:** Real-time fraud detection requires <100ms total inference time (feature retrieval + model prediction). Slow features block transactions, violate SLA, lose revenue.\n",
    "\n",
    "**Features to Implement:**\n",
    "- Feature caching: Pre-compute expensive aggregations (user 30-day stats cached in Redis)\n",
    "- Batch retrieval: Fetch 100 users in single call (reduce network overhead from 100x 1ms ‚Üí 1x 5ms)\n",
    "- Feature transformation: Real-time ratios (transaction_amt / user_avg_30day) computed on-the-fly\n",
    "- Cache warming: Pre-fetch features for high-value users during low-traffic hours\n",
    "- Latency monitoring: Track p50/p95/p99, alert if >10ms\n",
    "\n",
    "**Success Criteria:**\n",
    "- P99 latency <10ms for feature retrieval (100 users batch)\n",
    "- Cache hit rate >85% (warm cache reduces latency 5-10x)\n",
    "- Support 10,000 QPS (peak traffic, Black Friday)\n",
    "- Graceful degradation: Fallback to defaults if cache miss (don't block transactions)\n",
    "- Monitoring dashboard: Real-time latency, cache hit rate, error rate\n",
    "\n",
    "**Data Application:**\n",
    "- Base features: user_id, transaction_amount, merchant_id, location, time\n",
    "- Cached aggregates: user_30day_avg, user_30day_std, merchant_fraud_rate\n",
    "- Derived features: amount_vs_avg_ratio, time_since_last_txn, location_distance_km\n",
    "- Model: XGBoost fraud classifier (requires 20 features, <100ms end-to-end)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 3: Model Performance Monitoring for Yield Prediction\n",
    "\n",
    "**Objective:** Monitor production yield prediction model 24/7, detect accuracy degradation within 1 hour (not weeks).\n",
    "\n",
    "**Business Value:** Yield prediction drives fab decisions ($M impact). Degraded model causes incorrect estimates, bad capacity planning. Early detection saves $500K+/incident.\n",
    "\n",
    "**Features to Implement:**\n",
    "- Accuracy tracking: Sliding window (1000 predictions), compare actual vs predicted yield\n",
    "- Concept drift: KS test comparing prediction distributions (1-week ago vs today)\n",
    "- Feature drift: PSI (Population Stability Index) for all input features\n",
    "- Automated alerts: Slack/PagerDuty if accuracy drops >5% or drift p-value <0.05\n",
    "- Root cause analysis: Identify which feature drifted (guides investigation)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Detect accuracy degradation within 1 hour (1000 predictions at 1000/day rate)\n",
    "- False positive rate <5% (alerts only for real issues, not noise)\n",
    "- Root cause identification: Flag specific drifted features (e.g., \"avg_vdd shifted 1.2V ‚Üí 1.25V\")\n",
    "- Automated retraining trigger: If drift sustained for 24 hours\n",
    "- Dashboard: Real-time accuracy, drift status, alert history\n",
    "\n",
    "**STDF Data Application:**\n",
    "- Model: Yield prediction Random Forest (accuracy baseline: 95%)\n",
    "- Monitor inputs: avg_vdd, std_vdd, test_time_ms, neighbor_yield_avg\n",
    "- Drift scenarios: Process change (Vdd shift), equipment drift, seasonal patterns\n",
    "- Action: Alert FAB engineer if drift detected, investigate root cause (sensor, process)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 4: Data Quality Monitoring for STDF Ingestion Pipeline\n",
    "\n",
    "**Objective:** Validate STDF data quality before reaching ML models, prevent garbage-in-garbage-out (99.9% clean data target).\n",
    "\n",
    "**Business Value:** Bad STDF data causes model failures (exceptions), incorrect predictions, degraded accuracy. Data quality gates catch issues upstream, save debugging time (8 hours ‚Üí 15 minutes).\n",
    "\n",
    "**Features to Implement:**\n",
    "- Schema validation: Required fields (wafer_id, test_name, test_value, test_limits, pass_fail)\n",
    "- Range checks: Vdd 1.0-1.4V, Idd 0-200mA, frequency 1000-3000MHz, temperature -40 to 125¬∞C\n",
    "- Null detection: Flag missing critical parameters (would crash model)\n",
    "- Distribution monitoring: Alert if mean shifts >2 std devs (indicates sensor drift)\n",
    "- Anomaly detection: IQR method for outliers (may be valid extreme cases or bad data)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Block data batch if critical issues (missing required fields, 100% nulls)\n",
    "- Alert if high severity issues (>5% range violations, distribution shift)\n",
    "- Log medium/low issues for review (anomalies, minor violations)\n",
    "- Process 10,000 STDF files/day with <1 minute validation latency per file\n",
    "- Comprehensive report: Issue counts, severity, affected fields, recommendations\n",
    "\n",
    "**STDF Data Application:**\n",
    "- Input: 10,000 devices per wafer, 100 parametric tests per device\n",
    "- Validation: Ensure test_name, test_value, test_limits present and within bounds\n",
    "- Distribution: Compare current wafer vs 30-day historical (mean/std Vdd, Idd)\n",
    "- Action: Reject wafer if >10% devices have out-of-range parameters\n",
    "\n",
    "---\n",
    "\n",
    "### Project 5: Feature Store for Customer Lifetime Value (CLV) Model\n",
    "\n",
    "**Objective:** Build feature store serving customer behavioral features for CLV prediction with daily retraining and real-time inference.\n",
    "\n",
    "**Business Value:** CLV model drives marketing spend decisions ($10M annual budget). Feature consistency ensures training accuracy translates to production (no 20% degradation from skew).\n",
    "\n",
    "**Features to Implement:**\n",
    "- Offline store: Daily batch features (customer_30day_purchases, avg_order_value, category_preferences)\n",
    "- Online store: Real-time features (days_since_last_purchase, current_cart_value)\n",
    "- Point-in-time correctness: Training features from date T use only data available at T\n",
    "- Feature versioning: Track changes (added recency features in v2.0)\n",
    "- Scheduled updates: Daily offline feature refresh (2AM ET), online cache warming (6AM ET)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Offline features updated daily (process 10M customer records in <1 hour)\n",
    "- Online features served with <10ms p99 latency\n",
    "- Point-in-time correctness: Zero data leakage (validated with historical backtests)\n",
    "- Feature version tracking: Can reproduce training data from 6 months ago\n",
    "- A/B test validation: New features increase CLV prediction accuracy by >3%\n",
    "\n",
    "**Data Application:**\n",
    "- Historical features: 30-day purchase count, avg order value, category counts\n",
    "- Real-time features: cart_value, time_on_site, page_views\n",
    "- Derived features: recency (days since last), frequency (purchases/30days), monetary (total spend)\n",
    "- Model: Gradient boosting regressor (predicts CLV over next 12 months)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 6: Drift Detection for Recommendation Engine\n",
    "\n",
    "**Objective:** Monitor recommendation model performance and detect user behavior shifts (concept drift) within 24 hours.\n",
    "\n",
    "**Business Value:** Recommendation CTR drives revenue ($500K/day). Concept drift (e.g., holiday season, trending products) degrades CTR 10-20%. Early detection enables rapid retraining, saves $50K/day.\n",
    "\n",
    "**Features to Implement:**\n",
    "- CTR monitoring: Track click-through rate hourly (baseline: 3.5%, alert if <3.2%)\n",
    "- Distribution monitoring: Item popularity distribution (detect trending products)\n",
    "- Cohort analysis: CTR by user segment (new users, power users, inactive)\n",
    "- Seasonality detection: Compare current vs same week last year (expected patterns)\n",
    "- Automated retraining: Trigger if CTR drops >5% for 24 hours\n",
    "\n",
    "**Success Criteria:**\n",
    "- Detect CTR degradation within 24 hours (hourly monitoring)\n",
    "- Distinguish concept drift from temporary noise (24-hour sustained drop triggers alert)\n",
    "- Cohort-level analysis: Identify which user segments affected\n",
    "- Automated retraining: New model trained and deployed within 12 hours\n",
    "- ROI tracking: CTR recovery after retraining (3.2% ‚Üí 3.6%)\n",
    "\n",
    "**Data Application:**\n",
    "- Metrics: CTR, conversion rate, revenue per impression\n",
    "- Drift scenarios: Holiday season, viral products, competitor promotions\n",
    "- Action: Retrain model with recent 7 days (capture current trends)\n",
    "- Validation: A/B test new model (ensure CTR improvement >5%)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 7: Real-Time Feature Validation for Autonomous Driving\n",
    "\n",
    "**Objective:** Validate sensor features in real-time (<1ms latency) before feeding to perception models (safety-critical).\n",
    "\n",
    "**Business Value:** Invalid features cause perception failures (miss pedestrians, wrong lane detection). Real-time validation prevents catastrophic failures, ensures safety.\n",
    "\n",
    "**Features to Implement:**\n",
    "- Schema validation: Required fields (lidar_points, camera_rgb, radar_detections)\n",
    "- Range checks: Lidar distance 0-100m, camera resolution 1920x1080, radar velocity -50 to 50 m/s\n",
    "- Sensor fusion validation: Timestamp alignment (<10ms skew between sensors)\n",
    "- Null detection: Missing sensor data triggers safe mode (slow down, alert driver)\n",
    "- Anomaly detection: Sudden sensor spikes (likely sensor failure, not real objects)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Validation latency <1ms (perception pipeline requires <50ms end-to-end)\n",
    "- Zero false negatives: Never allow invalid data to reach model (safety-critical)\n",
    "- False positive rate <0.1%: Minimize safe mode triggers (user experience)\n",
    "- Sensor fusion validation: Ensure multi-sensor timestamps aligned within 10ms\n",
    "- Comprehensive logging: All validation failures logged for post-incident analysis\n",
    "\n",
    "**Data Application:**\n",
    "- Sensors: Lidar (100K points/frame), camera (1920x1080 RGB), radar (detections with velocity)\n",
    "- Validation: Check timestamps aligned, ranges valid, no missing data\n",
    "- Action: If validation fails, trigger safe mode (reduce speed, alert driver)\n",
    "- Logging: Record failures for debugging (sensor malfunction, software bug)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 8: Feature Lineage Tracking for Regulatory Compliance\n",
    "\n",
    "**Objective:** Build complete feature lineage system for ML models in regulated industries (FDA medical devices, financial services).\n",
    "\n",
    "**Business Value:** Regulatory audits require traceability: raw data ‚Üí features ‚Üí predictions. Manual documentation costs 40 hours/model. Automated lineage saves 95% effort.\n",
    "\n",
    "**Features to Implement:**\n",
    "- Source tracking: Map each feature to source tables/columns (e.g., neighbor_yield_avg ‚Üê stdf.wafer_test.die_yield)\n",
    "- Transformation logging: Record transformation logic (SQL, Python code)\n",
    "- Versioning: Track feature definition changes over time (v1.0 used simple avg, v2.0 added spatial weighting)\n",
    "- Model linkage: Which features used by which models\n",
    "- Audit report generation: One-click report for regulators (complete lineage graph)\n",
    "\n",
    "**Success Criteria:**\n",
    "- 100% feature coverage: Every feature has documented lineage\n",
    "- Automated tracking: No manual documentation (lineage captured during feature engineering)\n",
    "- Audit report generation: <5 minutes for complete model lineage\n",
    "- Historical reconstruction: Can reproduce feature values from 2 years ago\n",
    "- Compliance validation: Passes FDA/financial regulatory audits\n",
    "\n",
    "**STDF Data Application:**\n",
    "- Raw data: stdf.parametric_tests table (test_name, test_value, wafer_id, die_x, die_y)\n",
    "- Feature: neighbor_yield_avg ‚Üê AVG(yield) WHERE die within 3mm radius\n",
    "- Transformation: Spatial join + aggregation (documented in lineage)\n",
    "- Model: Uses neighbor_yield_avg for binning decisions\n",
    "- Audit trail: Raw STDF ‚Üí derived feature ‚Üí model prediction ‚Üí binning decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5e6df",
   "metadata": {},
   "source": [
    "## 7. üéØ Comprehensive Takeaways: Mastering Feature Stores & Monitoring\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **The Training-Serving Skew Problem**\n",
    "\n",
    "**What is Training-Serving Skew:**\n",
    "- **Training time:** Features computed in batch (Spark, pandas) using historical data\n",
    "- **Serving time:** Features must be computed in real-time (<10ms) with live data\n",
    "- **Skew:** Different code paths ‚Üí different feature values ‚Üí accuracy degradation\n",
    "\n",
    "**Real-World Impact:**\n",
    "```\n",
    "Training accuracy: 95%\n",
    "Production accuracy: 75% (20% degradation!)\n",
    "Root cause: Training used 30-day rolling average (batch),\n",
    "           Production used 7-day average (different logic)\n",
    "```\n",
    "\n",
    "**Solution: Feature Store:**\n",
    "- **Single source of truth:** Same feature definitions for training and serving\n",
    "- **Offline store:** Batch features for training (Parquet, Delta Lake)\n",
    "- **Online store:** Real-time features for inference (Redis, DynamoDB)\n",
    "- **Consistent computation:** Same code generates offline and online features\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Feature definition (used for both offline and online)\n",
    "def compute_neighbor_yield_avg(wafer_id, die_x, die_y, data_source):\n",
    "    neighbors = data_source.get_neighbors(die_x, die_y, radius=3mm)\n",
    "    return np.mean([d.yield_pct for d in neighbors])\n",
    "\n",
    "# Offline: data_source = Spark DataFrame (historical STDF)\n",
    "# Online: data_source = Redis cache (current wafer)\n",
    "# Result: Identical features, zero skew\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Feature Store Architecture Patterns**\n",
    "\n",
    "**Offline Store (Training):**\n",
    "- **Purpose:** Batch feature generation for model training\n",
    "- **Storage:** Parquet (S3), Delta Lake, Hive, BigQuery\n",
    "- **Latency:** Minutes to hours (not time-critical)\n",
    "- **Volume:** Terabytes (years of historical data)\n",
    "- **Access pattern:** Large scans (millions of rows for training)\n",
    "\n",
    "**Online Store (Inference):**\n",
    "- **Purpose:** Low-latency feature retrieval for predictions\n",
    "- **Storage:** Redis, DynamoDB, Cassandra, Aerospike\n",
    "- **Latency:** <5-10ms p99 (strict SLA)\n",
    "- **Volume:** Gigabytes (recent data only, cache-friendly)\n",
    "- **Access pattern:** Point lookups (single entity per request)\n",
    "\n",
    "**Hybrid Pattern:**\n",
    "```\n",
    "Training pipeline:\n",
    "1. Read offline store (30 days of historical features)\n",
    "2. Train model on batch features\n",
    "3. Validate model accuracy\n",
    "\n",
    "Inference pipeline:\n",
    "1. Read online store (latest features for entity_id)\n",
    "2. Model.predict(online_features)\n",
    "3. Serve prediction in <100ms\n",
    "```\n",
    "\n",
    "**Key Decision: When to use which store:**\n",
    "- **Offline only:** Batch predictions (daily reports, email campaigns)\n",
    "- **Online only:** Real-time predictions (fraud detection, recommendations)\n",
    "- **Hybrid (most common):** Train on offline, serve from online\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Point-in-Time Correctness**\n",
    "\n",
    "**The Data Leakage Problem:**\n",
    "```python\n",
    "# WRONG: Data leakage from future into past\n",
    "training_data = []\n",
    "for date in training_dates:\n",
    "    features = compute_features(data_up_to=today)  # BUG: Using future data!\n",
    "    label = get_label(date)\n",
    "    training_data.append((features, label))\n",
    "\n",
    "# Result: Model sees future information during training,\n",
    "#         overestimates accuracy, fails in production\n",
    "```\n",
    "\n",
    "**Correct Point-in-Time Join:**\n",
    "```python\n",
    "# CORRECT: Only use data available at prediction time\n",
    "training_data = []\n",
    "for date in training_dates:\n",
    "    features = compute_features(data_up_to=date)  # Only past data\n",
    "    label = get_label(date)\n",
    "    training_data.append((features, label))\n",
    "\n",
    "# Result: Model trained on realistic features,\n",
    "#         accuracy matches production\n",
    "```\n",
    "\n",
    "**Implementation:**\n",
    "- **Timestamp columns:** Every feature row has `feature_timestamp`\n",
    "- **Join logic:** `SELECT * FROM features WHERE feature_timestamp <= prediction_time`\n",
    "- **Validation:** Historical backtests (predict 2023-01-01 using only 2022 data)\n",
    "\n",
    "**Post-Silicon Example:**\n",
    "```python\n",
    "# Train yield model for date 2024-01-01\n",
    "# Feature: neighbor_yield_avg (spatial correlation)\n",
    "\n",
    "# WRONG:\n",
    "neighbor_yield = avg(yield_pct for all devices)  # Includes future devices!\n",
    "\n",
    "# CORRECT:\n",
    "neighbor_yield = avg(yield_pct for devices WHERE test_timestamp < 2024-01-01)\n",
    "\n",
    "# Validation:\n",
    "# Backtest on 2023 data, verify predictions match actual (no lookahead)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Feature Versioning and Evolution**\n",
    "\n",
    "**Why Feature Versioning:**\n",
    "- Features evolve over time (new sources, improved logic, bug fixes)\n",
    "- Models depend on specific feature versions\n",
    "- Need to reproduce training from 6 months ago (regulatory, debugging)\n",
    "\n",
    "**Versioning Strategy:**\n",
    "```python\n",
    "# Feature group version history\n",
    "wafer_aggregates:\n",
    "  v1.0 (2024-01-01): Initial version (simple mean/std)\n",
    "  v1.1 (2024-03-15): Added spatial correlation features\n",
    "  v2.0 (2024-06-01): Breaking change (neighbor radius 3mm ‚Üí 5mm)\n",
    "  v2.1 (2024-08-01): Bug fix (null handling)\n",
    "```\n",
    "\n",
    "**Model-Feature Compatibility:**\n",
    "```\n",
    "Model: yield_prediction_v3.0\n",
    "Required features: wafer_aggregates >= v1.1, device_parametrics >= v2.0\n",
    "\n",
    "# Feature store validates compatibility\n",
    "if wafer_aggregates.version < v1.1:\n",
    "    raise IncompatibleFeatureVersion(\"Need wafer_aggregates v1.1+ for spatial features\")\n",
    "```\n",
    "\n",
    "**Migration Strategy:**\n",
    "- **Backward compatible (v1.0 ‚Üí v1.1):** Add new features, keep old ones\n",
    "- **Breaking change (v1.1 ‚Üí v2.0):** Retrain all models, coordinated deployment\n",
    "- **Rollback:** Keep v1.1 online for 30 days (models can rollback if v2.0 fails)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Real-Time Feature Serving Optimizations**\n",
    "\n",
    "**Latency Budget:**\n",
    "```\n",
    "Total inference SLA: <100ms\n",
    "- Feature retrieval: 10ms (online store)\n",
    "- Model prediction: 50ms (forward pass)\n",
    "- Post-processing: 10ms (formatting, logging)\n",
    "- Network overhead: 30ms (API latency)\n",
    "```\n",
    "\n",
    "**Optimization Techniques:**\n",
    "\n",
    "**1. Feature Caching:**\n",
    "```python\n",
    "# Without cache: Query database for every request (50ms)\n",
    "features = db.query(\"SELECT * FROM features WHERE user_id = ?\", user_id)\n",
    "\n",
    "# With cache: In-memory lookup (1ms)\n",
    "features = cache.get(f\"features:user:{user_id}\")\n",
    "if not features:\n",
    "    features = db.query(...)\n",
    "    cache.set(f\"features:user:{user_id}\", features, ttl=3600)\n",
    "\n",
    "# Speedup: 50ms ‚Üí 1ms (50x faster)\n",
    "```\n",
    "\n",
    "**2. Batch Retrieval:**\n",
    "```python\n",
    "# Without batching: N separate calls (N * 5ms)\n",
    "for user_id in user_ids:\n",
    "    features = redis.get(f\"user:{user_id}\")  # 100 calls √ó 5ms = 500ms\n",
    "\n",
    "# With batching: Single pipelined call\n",
    "features = redis.mget([f\"user:{uid}\" for uid in user_ids])  # 1 call √ó 10ms = 10ms\n",
    "\n",
    "# Speedup: 500ms ‚Üí 10ms (50x faster)\n",
    "```\n",
    "\n",
    "**3. Derived Feature Computation:**\n",
    "```python\n",
    "# Base features (cached): user_30day_avg, user_30day_std\n",
    "# Derived features (computed on-the-fly): \n",
    "#   - coefficient_of_variation = std / mean\n",
    "#   - z_score = (current_value - mean) / std\n",
    "\n",
    "# Tradeoff: Store 10 base features (small cache), compute 30 derived (fast math)\n",
    "# vs Store 40 total features (large cache, slower lookups)\n",
    "```\n",
    "\n",
    "**4. Cache Warming:**\n",
    "```\n",
    "# Pre-fetch features for high-probability users (machine learning!)\n",
    "# Example: Users likely to visit site in next hour\n",
    "\n",
    "During low-traffic (2AM-6AM):\n",
    "  for user_id in predicted_active_users:\n",
    "      cache.set(f\"user:{user_id}\", compute_features(user_id))\n",
    "\n",
    "During peak traffic (12PM-6PM):\n",
    "  Cache hit rate: 90% (most users pre-cached)\n",
    "  P99 latency: 5ms (cache hits) instead of 50ms (database queries)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Drift Detection Methods**\n",
    "\n",
    "**Types of Drift:**\n",
    "\n",
    "**1. Data Drift (Input Distribution Change):**\n",
    "- **Definition:** Feature distributions shift over time\n",
    "- **Example:** avg_vdd changes from 1.2V to 1.25V (process change)\n",
    "- **Impact:** Model still predicts, but accuracy degrades\n",
    "- **Detection:** KS test, PSI, distribution comparison\n",
    "\n",
    "**2. Concept Drift (Input-Output Relationship Change):**\n",
    "- **Definition:** Relationship between features and target changes\n",
    "- **Example:** User behavior changes (holiday season, pandemic)\n",
    "- **Impact:** Same features ‚Üí different predictions needed\n",
    "- **Detection:** Accuracy degradation, prediction distribution shift\n",
    "\n",
    "**3. Label Drift (Output Distribution Change):**\n",
    "- **Definition:** Target variable distribution changes\n",
    "- **Example:** Fraud rate increases from 0.1% to 0.5%\n",
    "- **Impact:** Class imbalance worsens, model predictions biased\n",
    "- **Detection:** Target distribution comparison\n",
    "\n",
    "**Statistical Tests:**\n",
    "\n",
    "**Kolmogorov-Smirnov (KS) Test:**\n",
    "```python\n",
    "# Compare two distributions (reference vs current)\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "reference_vdd = [1.18, 1.19, 1.20, 1.21, ...]  # Training data\n",
    "current_vdd = [1.23, 1.24, 1.25, 1.26, ...]    # Production data\n",
    "\n",
    "ks_statistic, p_value = ks_2samp(reference_vdd, current_vdd)\n",
    "\n",
    "# Interpretation:\n",
    "# p_value < 0.05: Distributions are significantly different (drift detected)\n",
    "# p_value >= 0.05: No significant difference (no drift)\n",
    "\n",
    "if p_value < 0.05:\n",
    "    alert(\"Data drift detected in avg_vdd\")\n",
    "```\n",
    "\n",
    "**Population Stability Index (PSI):**\n",
    "```python\n",
    "# Binned comparison (useful for categorical/discrete features)\n",
    "\n",
    "def calculate_psi(baseline, current, bins=10):\n",
    "    # Create bins\n",
    "    bin_edges = np.percentile(baseline, np.linspace(0, 100, bins+1))\n",
    "    \n",
    "    # Histogram\n",
    "    baseline_hist, _ = np.histogram(baseline, bins=bin_edges)\n",
    "    current_hist, _ = np.histogram(current, bins=bin_edges)\n",
    "    \n",
    "    # Normalize\n",
    "    baseline_pct = baseline_hist / len(baseline)\n",
    "    current_pct = current_hist / len(current)\n",
    "    \n",
    "    # PSI formula\n",
    "    psi = np.sum((current_pct - baseline_pct) * np.log(current_pct / baseline_pct))\n",
    "    \n",
    "    return psi\n",
    "\n",
    "# Interpretation:\n",
    "# PSI < 0.1: No significant change\n",
    "# PSI 0.1-0.25: Moderate change (investigate)\n",
    "# PSI > 0.25: Significant change (retrain model)\n",
    "```\n",
    "\n",
    "**When to Use Each:**\n",
    "- **KS Test:** Continuous features (voltage, current, price)\n",
    "- **PSI:** Any feature (handles categorical), easier to interpret\n",
    "- **Chi-Square:** Categorical features (binning category, user segment)\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Data Quality Validation Rules**\n",
    "\n",
    "**Schema Validation:**\n",
    "```python\n",
    "# Define expected schema\n",
    "schema = {\n",
    "    'wafer_id': {'type': 'string', 'required': True, 'nullable': False},\n",
    "    'vdd': {'type': 'float', 'required': True, 'nullable': False},\n",
    "    'test_time_ms': {'type': 'float', 'required': True, 'nullable': False}\n",
    "}\n",
    "\n",
    "# Validate data\n",
    "def validate_schema(df, schema):\n",
    "    issues = []\n",
    "    \n",
    "    for field, spec in schema.items():\n",
    "        # Check if required field present\n",
    "        if spec['required'] and field not in df.columns:\n",
    "            issues.append(f\"Missing required field: {field}\")\n",
    "        \n",
    "        # Check nullability\n",
    "        if not spec['nullable'] and df[field].isnull().any():\n",
    "            issues.append(f\"Field {field} has nulls (not allowed)\")\n",
    "    \n",
    "    return issues\n",
    "```\n",
    "\n",
    "**Range Validation:**\n",
    "```python\n",
    "# Define ranges\n",
    "ranges = {\n",
    "    'vdd': {'min': 1.0, 'max': 1.4},\n",
    "    'idd': {'min': 0, 'max': 200},\n",
    "    'temperature': {'min': -40, 'max': 125}\n",
    "}\n",
    "\n",
    "# Validate ranges\n",
    "def validate_ranges(df, ranges):\n",
    "    issues = []\n",
    "    \n",
    "    for field, bounds in ranges.items():\n",
    "        if field in df.columns:\n",
    "            violations = df[(df[field] < bounds['min']) | (df[field] > bounds['max'])]\n",
    "            \n",
    "            if len(violations) > 0:\n",
    "                issues.append(f\"{field}: {len(violations)} values out of range [{bounds['min']}, {bounds['max']}]\")\n",
    "    \n",
    "    return issues\n",
    "```\n",
    "\n",
    "**Distribution Validation:**\n",
    "```python\n",
    "# Compare current vs historical\n",
    "def validate_distribution(current_df, baseline_stats, threshold_std=2.0):\n",
    "    issues = []\n",
    "    \n",
    "    for col, baseline in baseline_stats.items():\n",
    "        current_mean = current_df[col].mean()\n",
    "        shift_stds = abs(current_mean - baseline['mean']) / baseline['std']\n",
    "        \n",
    "        if shift_stds > threshold_std:\n",
    "            issues.append(f\"{col}: Mean shifted {shift_stds:.1f} std devs\")\n",
    "    \n",
    "    return issues\n",
    "```\n",
    "\n",
    "**Severity Levels:**\n",
    "- **Critical:** Block data pipeline (missing required fields, 100% nulls)\n",
    "- **High:** Alert on-call engineer (type mismatches, excessive nulls)\n",
    "- **Medium:** Log for review (range violations, unexpected values)\n",
    "- **Low:** Informational (anomalies, minor issues)\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Monitoring Metrics and Dashboards**\n",
    "\n",
    "**Feature Store Metrics:**\n",
    "```\n",
    "Offline Store:\n",
    "- Batch job duration (target: <30 min for 1TB)\n",
    "- Feature freshness (age of latest features)\n",
    "- Storage size (GB, growth rate)\n",
    "- Failed batch jobs (count, error types)\n",
    "\n",
    "Online Store:\n",
    "- Latency (p50, p95, p99) - target: <5ms p99\n",
    "- Throughput (QPS) - target: 10K QPS\n",
    "- Cache hit rate - target: >85%\n",
    "- Error rate - target: <0.1%\n",
    "- Storage size (GB, eviction rate)\n",
    "```\n",
    "\n",
    "**Model Performance Metrics:**\n",
    "```\n",
    "Accuracy:\n",
    "- Overall accuracy (classification) or MAE/RMSE (regression)\n",
    "- Accuracy by cohort (new users, power users, etc.)\n",
    "- Accuracy trend (7-day rolling average)\n",
    "\n",
    "Drift:\n",
    "- Feature drift count (how many features drifted)\n",
    "- Concept drift p-value (KS test)\n",
    "- Distribution shift magnitude (PSI, std devs)\n",
    "\n",
    "Alerts:\n",
    "- Total alerts fired (count per day)\n",
    "- False positive rate (alerts without real issues)\n",
    "- Time to resolution (alert ‚Üí fix deployed)\n",
    "```\n",
    "\n",
    "**Data Quality Metrics:**\n",
    "```\n",
    "Validation:\n",
    "- Total records validated (count per day)\n",
    "- Issues by severity (critical, high, medium, low)\n",
    "- Pass rate (% batches with no critical issues)\n",
    "\n",
    "Specific Checks:\n",
    "- Schema validation pass rate\n",
    "- Range violations (count, percentage)\n",
    "- Null rate (% records with nulls)\n",
    "- Distribution shift count (fields drifted)\n",
    "- Anomaly rate (% outliers)\n",
    "```\n",
    "\n",
    "**Dashboard Layout:**\n",
    "```\n",
    "Real-Time Monitoring Dashboard:\n",
    "\n",
    "Row 1: KPIs\n",
    "- P99 latency (gauge, target: <10ms)\n",
    "- Cache hit rate (gauge, target: >85%)\n",
    "- Error rate (gauge, target: <0.1%)\n",
    "- Throughput (line chart, 5-minute window)\n",
    "\n",
    "Row 2: Drift Detection\n",
    "- Feature drift count (bar chart, by feature)\n",
    "- Concept drift p-value (line chart, 24-hour window)\n",
    "- Accuracy trend (line chart, 7-day rolling)\n",
    "\n",
    "Row 3: Data Quality\n",
    "- Issues by severity (stacked bar chart)\n",
    "- Pass rate (line chart, daily)\n",
    "- Top violated fields (table)\n",
    "\n",
    "Row 4: Alerts\n",
    "- Recent alerts (table, last 24 hours)\n",
    "- Alert history (timeline)\n",
    "- On-call status (who's responding)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Feature Store Best Practices**\n",
    "\n",
    "**1. Feature Naming Conventions:**\n",
    "```python\n",
    "# Good: Descriptive, unambiguous\n",
    "user_30day_purchase_count\n",
    "device_avg_vdd_7day\n",
    "wafer_neighbor_yield_avg_3mm\n",
    "\n",
    "# Bad: Ambiguous, unclear time window\n",
    "user_purchases  # 30 days? All time?\n",
    "device_voltage  # Average? Max? Current?\n",
    "wafer_yield  # Individual die? Wafer average?\n",
    "```\n",
    "\n",
    "**2. Feature Documentation:**\n",
    "```python\n",
    "# Feature metadata\n",
    "{\n",
    "    'name': 'wafer_neighbor_yield_avg_3mm',\n",
    "    'description': 'Average yield of neighboring dies within 3mm radius',\n",
    "    'type': 'float',\n",
    "    'range': [0, 100],\n",
    "    'source': 'stdf.wafer_test.parametric_results',\n",
    "    'transformation': 'AVG(yield_pct) WHERE DISTANCE(die_x, die_y, x, y) < 3mm',\n",
    "    'owner': 'fab_analytics_team',\n",
    "    'created': '2024-01-15',\n",
    "    'version': 'v1.1',\n",
    "    'dependencies': ['die_x', 'die_y', 'yield_pct'],\n",
    "    'sla': 'Updated daily 2AM ET'\n",
    "}\n",
    "```\n",
    "\n",
    "**3. Feature Granularity:**\n",
    "```python\n",
    "# Coarse granularity: User-level\n",
    "user_lifetime_purchases = 450\n",
    "\n",
    "# Fine granularity: User-product-level\n",
    "user_product_purchases = {\n",
    "    'product_A': 100,\n",
    "    'product_B': 250,\n",
    "    'product_C': 100\n",
    "}\n",
    "\n",
    "# Tradeoff:\n",
    "# Coarse: Smaller cache, faster lookups, less expressive\n",
    "# Fine: Larger cache, slower lookups, more expressive\n",
    "\n",
    "# Rule: Start coarse, add fine-grained when models need it\n",
    "```\n",
    "\n",
    "**4. Feature Store as Platform:**\n",
    "```\n",
    "Team A (Binning Model):\n",
    "- Uses: wafer_aggregates, device_parametrics\n",
    "- Contributes: binning_predictions (for downstream models)\n",
    "\n",
    "Team B (Yield Model):\n",
    "- Uses: wafer_aggregates, binning_predictions\n",
    "- Contributes: yield_forecasts\n",
    "\n",
    "Team C (Test Optimization):\n",
    "- Uses: wafer_aggregates, device_parametrics\n",
    "- Contributes: test_coverage_metrics\n",
    "\n",
    "# Result: Feature reuse, faster experimentation, consistent features\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10. **Monitoring Alert Strategy**\n",
    "\n",
    "**Alert Levels:**\n",
    "\n",
    "**P0 (Critical):**\n",
    "- Production model serving errors >1% (blocking predictions)\n",
    "- Feature store online store down (inference impossible)\n",
    "- Data quality: Critical issues (missing required fields)\n",
    "- **Response:** Page on-call immediately, all hands on deck\n",
    "\n",
    "**P1 (High):**\n",
    "- Accuracy drop >10% (sustained for 1 hour)\n",
    "- Feature drift detected (multiple features)\n",
    "- Latency p99 >50ms (violating SLA)\n",
    "- **Response:** Alert team within 15 minutes, investigate immediately\n",
    "\n",
    "**P2 (Medium):**\n",
    "- Accuracy drop 5-10% (monitor for 4 hours)\n",
    "- Single feature drift (investigate root cause)\n",
    "- Cache hit rate <70% (degraded performance)\n",
    "- **Response:** Alert team within 1 hour, investigate during business hours\n",
    "\n",
    "**P3 (Low):**\n",
    "- Data quality: Low severity issues (anomalies)\n",
    "- Feature store batch job delayed (not blocking production)\n",
    "- Minor range violations (<1% of records)\n",
    "- **Response:** Log for review, no immediate action\n",
    "\n",
    "**Alert Fatigue Prevention:**\n",
    "```python\n",
    "# Bad: Alert on every single issue\n",
    "if accuracy < baseline_accuracy:\n",
    "    alert(\"Accuracy drop!\")  # Fires 100 times/day (noise)\n",
    "\n",
    "# Good: Alert only on sustained issues\n",
    "accuracy_window = last_1000_predictions()\n",
    "if accuracy_window.mean() < baseline_accuracy - 0.05:\n",
    "    if sustained_for_hours >= 4:\n",
    "        alert(\"Sustained accuracy drop >5% for 4 hours\")\n",
    "```\n",
    "\n",
    "**Alert Context:**\n",
    "```python\n",
    "# Bad: Vague alert\n",
    "\"Drift detected\"\n",
    "\n",
    "# Good: Actionable alert\n",
    "Alert: Feature Drift Detected\n",
    "  Feature: avg_vdd\n",
    "  Baseline: 1.20V ¬± 0.02V\n",
    "  Current: 1.25V ¬± 0.03V\n",
    "  Shift: +2.5 std devs\n",
    "  Possible causes:\n",
    "    - Process change (new fab settings?)\n",
    "    - Equipment drift (sensor calibration?)\n",
    "    - Data pipeline issue (unit conversion?)\n",
    "  Recommended action:\n",
    "    - Investigate with FAB team\n",
    "    - Check equipment calibration logs\n",
    "    - Validate data pipeline\n",
    "  Runbook: https://wiki.company.com/drift-investigation\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 11. **Feature Store Scaling Strategies**\n",
    "\n",
    "**Offline Store Scaling:**\n",
    "\n",
    "**1. Partitioning:**\n",
    "```python\n",
    "# Partition by date (enables incremental updates)\n",
    "/features/wafer_aggregates/date=2024-01-01/part-00000.parquet\n",
    "/features/wafer_aggregates/date=2024-01-02/part-00000.parquet\n",
    "\n",
    "# Partition by entity (enables parallel processing)\n",
    "/features/user_features/user_id_range=0-999999/part-00000.parquet\n",
    "/features/user_features/user_id_range=1000000-1999999/part-00000.parquet\n",
    "\n",
    "# Query optimization:\n",
    "SELECT * FROM wafer_aggregates WHERE date >= '2024-01-01'\n",
    "# Only scans relevant partitions (10GB instead of 1TB)\n",
    "```\n",
    "\n",
    "**2. Incremental Computation:**\n",
    "```python\n",
    "# Naive: Recompute all features daily (expensive)\n",
    "user_30day_purchases = df.groupby('user_id').agg({'purchase': 'count'})\n",
    "\n",
    "# Incremental: Only compute for new data\n",
    "yesterday_purchases = new_data.groupby('user_id').agg({'purchase': 'count'})\n",
    "user_30day_purchases = (\n",
    "    old_features.join(yesterday_purchases, 'user_id')\n",
    "    .select('user_id', (old_count - oldest_day_count + yesterday_count).alias('count'))\n",
    ")\n",
    "\n",
    "# Speedup: 10 hours ‚Üí 30 minutes (20x faster)\n",
    "```\n",
    "\n",
    "**Online Store Scaling:**\n",
    "\n",
    "**1. Sharding:**\n",
    "```python\n",
    "# Single Redis instance: 10K QPS limit\n",
    "# Sharded Redis cluster: 100K QPS (10 shards)\n",
    "\n",
    "shard = hash(user_id) % num_shards\n",
    "redis_shard = redis_cluster[shard]\n",
    "features = redis_shard.get(f\"user:{user_id}\")\n",
    "```\n",
    "\n",
    "**2. Read Replicas:**\n",
    "```python\n",
    "# Primary: Writes only (feature updates)\n",
    "# Replicas: Reads only (inference requests)\n",
    "\n",
    "if operation == 'write':\n",
    "    redis_primary.set(key, value)\n",
    "else:\n",
    "    redis_replica = random.choice(redis_replicas)  # Load balance\n",
    "    redis_replica.get(key)\n",
    "\n",
    "# Read capacity: 10K QPS √ó 5 replicas = 50K QPS\n",
    "```\n",
    "\n",
    "**3. TTL-based Eviction:**\n",
    "```python\n",
    "# Problem: Cache grows unbounded (memory exhaustion)\n",
    "\n",
    "# Solution: TTL-based eviction\n",
    "redis.setex(\n",
    "    key=f\"user:{user_id}\",\n",
    "    value=features,\n",
    "    time=3600  # Expire after 1 hour\n",
    ")\n",
    "\n",
    "# Result: Only active users in cache (10M users ‚Üí 1M cached)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 12. **Real-Time vs Batch Feature Pipelines**\n",
    "\n",
    "**Batch Features (Offline Store):**\n",
    "```python\n",
    "# Scheduled nightly job (2AM ET)\n",
    "spark.read.parquet(\"s3://raw-data/stdf/\")\n",
    "    .groupBy(\"wafer_id\")\n",
    "    .agg(\n",
    "        avg(\"vdd\").alias(\"avg_vdd\"),\n",
    "        stddev(\"vdd\").alias(\"std_vdd\"),\n",
    "        count(\"*\").alias(\"device_count\")\n",
    "    )\n",
    "    .write.parquet(\"s3://features/wafer_aggregates/date=2024-01-01/\")\n",
    "\n",
    "# Characteristics:\n",
    "# - Latency: 30 minutes (not time-critical)\n",
    "# - Volume: 1TB (millions of wafers)\n",
    "# - Schedule: Daily (updated once per day)\n",
    "# - Use case: Training data generation\n",
    "```\n",
    "\n",
    "**Real-Time Features (Online Store):**\n",
    "```python\n",
    "# Streaming pipeline (Kafka ‚Üí Flink ‚Üí Redis)\n",
    "wafer_stream\n",
    "    .keyBy(\"wafer_id\")\n",
    "    .window(TumblingEventTimeWindows.of(Time.hours(1)))\n",
    "    .aggregate(new WaferAggregator())\n",
    "    .addSink(new RedisSink())\n",
    "\n",
    "# Characteristics:\n",
    "# - Latency: <1 second (event-time to feature-available)\n",
    "# - Volume: 1K wafers/sec (streaming rate)\n",
    "# - Schedule: Continuous (updated on every event)\n",
    "# - Use case: Real-time inference\n",
    "```\n",
    "\n",
    "**Hybrid Pattern:**\n",
    "```python\n",
    "# Base features: Batch (expensive aggregations)\n",
    "user_1year_purchases = batch_compute()  # Runs daily\n",
    "\n",
    "# Delta features: Real-time (incremental updates)\n",
    "user_today_purchases = stream_compute()  # Runs continuously\n",
    "\n",
    "# Combined feature:\n",
    "user_total_purchases = user_1year_purchases + user_today_purchases\n",
    "\n",
    "# Tradeoff:\n",
    "# - Batch: Accurate but stale (updated daily)\n",
    "# - Real-time: Fresh but approximate (only recent data)\n",
    "# - Hybrid: Best of both (accurate base + fresh delta)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 13. **Feature Store Integration with ML Lifecycle**\n",
    "\n",
    "**Training Pipeline:**\n",
    "```python\n",
    "# 1. Fetch offline features\n",
    "features_df = feature_store.get_offline_features(\n",
    "    feature_groups=['wafer_aggregates', 'device_parametrics'],\n",
    "    start_date='2023-01-01',\n",
    "    end_date='2023-12-31'\n",
    ")\n",
    "\n",
    "# 2. Train model\n",
    "model = train_model(features_df)\n",
    "\n",
    "# 3. Log feature metadata\n",
    "mlflow.log_param(\"feature_groups\", ['wafer_aggregates', 'device_parametrics'])\n",
    "mlflow.log_param(\"feature_versions\", {'wafer_aggregates': 'v2.0', 'device_parametrics': 'v1.5'})\n",
    "\n",
    "# 4. Register model with feature requirements\n",
    "model_registry.register(\n",
    "    model=model,\n",
    "    required_features=['avg_vdd', 'std_vdd', 'neighbor_yield_avg'],\n",
    "    feature_versions={'wafer_aggregates': 'v2.0'}\n",
    ")\n",
    "```\n",
    "\n",
    "**Inference Pipeline:**\n",
    "```python\n",
    "# 1. Fetch online features\n",
    "features = feature_store.get_online_features(\n",
    "    entity_ids=['W0001'],\n",
    "    feature_names=['avg_vdd', 'std_vdd', 'neighbor_yield_avg']\n",
    ")\n",
    "\n",
    "# 2. Validate feature compatibility\n",
    "model_metadata = model_registry.get_metadata(model_id)\n",
    "if features.version < model_metadata.required_version:\n",
    "    raise IncompatibleFeatureVersion()\n",
    "\n",
    "# 3. Make prediction\n",
    "prediction = model.predict(features)\n",
    "\n",
    "# 4. Log prediction for monitoring\n",
    "monitor.log_prediction(\n",
    "    prediction=prediction,\n",
    "    features=features,\n",
    "    model_version=model_metadata.version\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 14. **Cost Optimization Strategies**\n",
    "\n",
    "**Offline Store:**\n",
    "```python\n",
    "# Cost: $100/TB/month (S3 storage)\n",
    "\n",
    "# Optimization 1: Compression\n",
    "# Uncompressed: 1TB\n",
    "# Snappy compression: 300GB (3x smaller, $33/month)\n",
    "df.write.option(\"compression\", \"snappy\").parquet(\"s3://features/\")\n",
    "\n",
    "# Optimization 2: Columnar storage\n",
    "# Row-based (CSV): 1TB\n",
    "# Columnar (Parquet): 200GB (5x smaller, $20/month)\n",
    "\n",
    "# Optimization 3: Retention policy\n",
    "# Keep 30 days: 30TB √ó $100 = $3,000/month\n",
    "# Keep 7 days: 7TB √ó $100 = $700/month (4.3x cheaper)\n",
    "```\n",
    "\n",
    "**Online Store:**\n",
    "```python\n",
    "# Cost: $1,000/month (Redis cluster, 100GB memory)\n",
    "\n",
    "# Optimization 1: TTL-based eviction\n",
    "# All users: 100GB (10M users √ó 10KB)\n",
    "# Active users only: 20GB (2M active √ó 10KB, 5x cheaper)\n",
    "redis.setex(key, features, ttl=3600)\n",
    "\n",
    "# Optimization 2: Feature compression\n",
    "# Full feature set: 10KB per user\n",
    "# Compressed: 2KB per user (5x smaller, 5x cheaper)\n",
    "features = compress(features)\n",
    "\n",
    "# Optimization 3: Derived features on-the-fly\n",
    "# Store 10 base features: 1KB\n",
    "# Compute 20 derived features: 0KB (free)\n",
    "# Instead of storing 30 features: 3KB\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 15. **Testing and Validation**\n",
    "\n",
    "**Feature Store Testing:**\n",
    "```python\n",
    "# Test 1: Offline-Online Consistency\n",
    "def test_offline_online_consistency():\n",
    "    entity_id = \"W0001\"\n",
    "    \n",
    "    # Get offline features (historical)\n",
    "    offline_features = feature_store.get_offline_features(\n",
    "        entity_ids=[entity_id],\n",
    "        date='2024-01-01'\n",
    "    )\n",
    "    \n",
    "    # Get online features (current)\n",
    "    online_features = feature_store.get_online_features(\n",
    "        entity_ids=[entity_id]\n",
    "    )\n",
    "    \n",
    "    # Compare (should match for same timestamp)\n",
    "    assert offline_features['avg_vdd'] == online_features['avg_vdd']\n",
    "\n",
    "# Test 2: Point-in-Time Correctness\n",
    "def test_point_in_time_correctness():\n",
    "    # Features for 2024-01-01 should not include data from 2024-01-02\n",
    "    features_jan1 = feature_store.get_offline_features(\n",
    "        date='2024-01-01'\n",
    "    )\n",
    "    \n",
    "    # All feature timestamps should be <= 2024-01-01\n",
    "    assert features_jan1['feature_timestamp'].max() <= datetime(2024, 1, 1)\n",
    "\n",
    "# Test 3: Feature Lineage Validation\n",
    "def test_feature_lineage():\n",
    "    lineage = feature_store.get_feature_lineage('neighbor_yield_avg')\n",
    "    \n",
    "    # Should trace back to source table\n",
    "    assert 'stdf.wafer_test' in lineage['source_tables']\n",
    "```\n",
    "\n",
    "**Monitoring Testing:**\n",
    "```python\n",
    "# Test 1: Drift Detection Sensitivity\n",
    "def test_drift_detection():\n",
    "    # Generate drifted data\n",
    "    baseline = np.random.normal(1.2, 0.02, 1000)\n",
    "    drifted = np.random.normal(1.25, 0.02, 1000)  # Mean shifted\n",
    "    \n",
    "    # Should detect drift\n",
    "    drift_detected = monitor.detect_feature_drift(baseline, drifted)\n",
    "    assert drift_detected == True\n",
    "\n",
    "# Test 2: Alert Thresholds\n",
    "def test_alert_thresholds():\n",
    "    # Accuracy drop within threshold (no alert)\n",
    "    monitor.log_accuracy(baseline=0.95, current=0.93)  # 2% drop\n",
    "    assert monitor.alerts == []\n",
    "    \n",
    "    # Accuracy drop exceeds threshold (alert)\n",
    "    monitor.log_accuracy(baseline=0.95, current=0.89)  # 6% drop\n",
    "    assert len(monitor.alerts) == 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 16. **Debugging and Troubleshooting**\n",
    "\n",
    "**Common Issues:**\n",
    "\n",
    "**1. Training-Serving Skew:**\n",
    "```\n",
    "Symptom: Training accuracy 95%, production accuracy 75%\n",
    "Root cause: Different feature logic for offline vs online\n",
    "\n",
    "Debugging:\n",
    "1. Check feature definitions (offline vs online)\n",
    "2. Compare feature values for same entity (should match)\n",
    "3. Verify point-in-time correctness (no data leakage)\n",
    "\n",
    "Fix:\n",
    "- Unify feature computation logic\n",
    "- Add integration test (offline-online consistency)\n",
    "```\n",
    "\n",
    "**2. Feature Store Latency Spikes:**\n",
    "```\n",
    "Symptom: P99 latency 50ms (normally 5ms)\n",
    "Root cause: Cache eviction, database query fallback\n",
    "\n",
    "Debugging:\n",
    "1. Check cache hit rate (should be >85%)\n",
    "2. Monitor cache size (evictions?)\n",
    "3. Check database load (slow queries?)\n",
    "\n",
    "Fix:\n",
    "- Increase cache TTL (reduce evictions)\n",
    "- Pre-warm cache (scheduled job)\n",
    "- Optimize database queries (indexes)\n",
    "```\n",
    "\n",
    "**3. Drift False Positives:**\n",
    "```\n",
    "Symptom: Drift alerts every day (not real drift)\n",
    "Root cause: Normal variance, not distribution shift\n",
    "\n",
    "Debugging:\n",
    "1. Check sample size (too small?)\n",
    "2. Review threshold (too sensitive?)\n",
    "3. Visualize distributions (real shift or noise?)\n",
    "\n",
    "Fix:\n",
    "- Increase sample size (1000 ‚Üí 5000)\n",
    "- Adjust threshold (2 std ‚Üí 3 std)\n",
    "- Use longer windows (1 day ‚Üí 7 days)\n",
    "```\n",
    "\n",
    "**4. Data Quality Issues:**\n",
    "```\n",
    "Symptom: Model exceptions, NaN predictions\n",
    "Root cause: Missing data, out-of-range values\n",
    "\n",
    "Debugging:\n",
    "1. Check data quality report (validation issues?)\n",
    "2. Identify problematic fields (which nulls/violations?)\n",
    "3. Trace to source (pipeline bug? sensor failure?)\n",
    "\n",
    "Fix:\n",
    "- Add null handling (imputation, defaults)\n",
    "- Add range validation (clip values)\n",
    "- Fix upstream pipeline (data ingestion bug)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 17. **Advanced Topics**\n",
    "\n",
    "**1. Feature Stores at Scale:**\n",
    "- **Uber's Michelangelo:** 10,000+ features, 100+ models, petabyte scale\n",
    "- **Airbnb's Zipline:** Real-time + batch features, Kafka + Spark\n",
    "- **LinkedIn's Feathr:** Declarative feature definitions, automated backfill\n",
    "\n",
    "**2. Automated Feature Engineering:**\n",
    "- **Featuretools:** Automated deep feature synthesis (DFS)\n",
    "- **AutoFeat:** Automatic feature generation and selection\n",
    "- **tsfresh:** Time-series feature extraction\n",
    "\n",
    "**3. Feature Store as a Service:**\n",
    "- **Tecton:** Cloud-native feature store (AWS, GCP, Azure)\n",
    "- **Feast:** Open-source feature store (Linux Foundation)\n",
    "- **SageMaker Feature Store:** AWS managed service\n",
    "\n",
    "**4. Privacy-Preserving Feature Stores:**\n",
    "- **Differential privacy:** Add noise to features (protect individual privacy)\n",
    "- **Federated features:** Compute features across devices without centralizing data\n",
    "- **Homomorphic encryption:** Compute on encrypted features\n",
    "\n",
    "---\n",
    "\n",
    "### 18. **Key Takeaways Summary**\n",
    "\n",
    "‚úÖ **Feature stores eliminate training-serving skew** by ensuring identical feature computation for training (offline) and inference (online)\n",
    "\n",
    "‚úÖ **Point-in-time correctness prevents data leakage** - only use data available at prediction time, not future information\n",
    "\n",
    "‚úÖ **Real-time feature serving requires <5-10ms p99 latency** - use caching, batching, and derived features for optimization\n",
    "\n",
    "‚úÖ **Drift detection catches model degradation early** - monitor feature distributions (KS test, PSI) and accuracy trends\n",
    "\n",
    "‚úÖ **Data quality validation prevents garbage-in-garbage-out** - schema validation, range checks, distribution monitoring\n",
    "\n",
    "‚úÖ **Feature versioning enables reproducibility** - track feature definition changes, ensure model-feature compatibility\n",
    "\n",
    "‚úÖ **Monitoring requires multi-layer approach** - feature store health, model performance, data quality\n",
    "\n",
    "‚úÖ **Alert strategy prevents fatigue** - P0-P3 severity levels, sustained thresholds, actionable context\n",
    "\n",
    "‚úÖ **Post-silicon applications are critical** - STDF feature stores, wafer-level aggregations, real-time binning\n",
    "\n",
    "‚úÖ **Production checklist:** Offline-online consistency, point-in-time correctness, lineage tracking, monitoring dashboards\n",
    "\n",
    "---\n",
    "\n",
    "### 19. **Production Readiness Checklist**\n",
    "\n",
    "**Feature Store:**\n",
    "- [ ] Offline store implemented (Parquet/Delta Lake on S3)\n",
    "- [ ] Online store implemented (Redis/DynamoDB with <5ms p99)\n",
    "- [ ] Point-in-time correctness validated (historical backtests pass)\n",
    "- [ ] Feature versioning enabled (can reproduce 6 months ago)\n",
    "- [ ] Lineage tracking (raw data ‚Üí features ‚Üí models)\n",
    "- [ ] Offline-online consistency tests (integration tests pass)\n",
    "\n",
    "**Monitoring:**\n",
    "- [ ] Model performance monitoring (accuracy, drift, latency)\n",
    "- [ ] Feature drift detection (KS test, PSI for all features)\n",
    "- [ ] Data quality validation (schema, ranges, distributions)\n",
    "- [ ] Dashboards (real-time metrics, alerts, trends)\n",
    "- [ ] Alert integration (Slack, PagerDuty, email)\n",
    "- [ ] Runbooks (troubleshooting guides for common issues)\n",
    "\n",
    "**Operations:**\n",
    "- [ ] Scheduled batch jobs (offline features updated daily)\n",
    "- [ ] Real-time pipelines (streaming features, <1s latency)\n",
    "- [ ] Cache warming (pre-fetch features for high-probability entities)\n",
    "- [ ] Backup and recovery (feature store backups, disaster recovery)\n",
    "- [ ] Cost optimization (compression, retention, TTL eviction)\n",
    "- [ ] Documentation (feature catalog, API docs, runbooks)\n",
    "\n",
    "---\n",
    "\n",
    "### 20. **Next Steps in Learning**\n",
    "\n",
    "**Notebook 130: ML Observability & Debugging**\n",
    "- Distributed tracing for ML pipelines (trace feature ‚Üí model ‚Üí prediction)\n",
    "- Model debugging with SHAP/LIME (explain predictions)\n",
    "- Performance profiling (identify latency bottlenecks)\n",
    "\n",
    "**Notebook 131: Container Orchestration for ML**\n",
    "- Kubernetes for model serving (horizontal scaling, health checks)\n",
    "- Docker multi-stage builds (optimize image size)\n",
    "- Service mesh (Istio for traffic management, observability)\n",
    "\n",
    "**Beyond MLOps:**\n",
    "- **Real-Time ML:** Stream processing (Kafka, Flink), online learning\n",
    "- **Edge Deployment:** TensorFlow Lite, ONNX, model quantization\n",
    "- **Federated Learning:** Train across devices without centralizing data\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've mastered production feature stores and real-time monitoring systems.** üéâ\n",
    "\n",
    "You now understand:\n",
    "- ‚úÖ Feature store architecture (offline vs online serving)\n",
    "- ‚úÖ Training-serving skew prevention (point-in-time correctness)\n",
    "- ‚úÖ Real-time feature serving (<5ms p99 latency)\n",
    "- ‚úÖ Drift detection (KS test, PSI, concept drift)\n",
    "- ‚úÖ Data quality monitoring (schema, ranges, distributions)\n",
    "- ‚úÖ Production-grade monitoring (dashboards, alerts, runbooks)\n",
    "\n",
    "**You're now equipped to build enterprise ML infrastructure that scales to billions of predictions.** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb86c5dc",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### When to Use Feature Stores\n",
    "- **Feature reuse across teams**: Multiple models need same engineered features (device temperature trends, parametric moving averages)\n",
    "- **Training-serving consistency**: Features computed identically in both batch training and real-time inference\n",
    "- **Feature lineage & governance**: Track feature versions, dependencies, and transformations for compliance\n",
    "- **Low-latency serving**: Sub-millisecond feature retrieval for online prediction (test floor decisions)\n",
    "- **Temporal consistency**: Point-in-time lookups prevent data leakage in time-series predictions\n",
    "\n",
    "### Limitations\n",
    "- **Infrastructure overhead**: Requires deployment of feature store (Feast, Tecton), storage backends (Redis, DynamoDB), and compute for transformations\n",
    "- **Learning curve**: Teams must adopt new workflows (feature registration, versioning, retrieval APIs)\n",
    "- **Latency trade-offs**: Online stores fast but expensive; offline stores cheap but slower\n",
    "- **Not for simple projects**: Overkill for single-model prototypes or teams <5 people\n",
    "\n",
    "### Alternatives\n",
    "- **Manual pipelines**: Direct feature engineering in training/serving code (simpler but error-prone)\n",
    "- **Data warehouses**: Store features in BigQuery/Snowflake (lacks low-latency serving)\n",
    "- **Model-specific preprocessing**: Embed feature logic in model (duplicates code, training-serving skew risk)\n",
    "\n",
    "### Best Practices\n",
    "- **Start simple**: Begin with offline store for batch training; add online store when latency matters\n",
    "- **Feature validation**: Use Great Expectations or custom checks to validate feature distributions\n",
    "- **Monitoring**: Track feature staleness, null rates, and distribution drift in production\n",
    "- **Materialization schedules**: Balance freshness vs. compute cost (hourly for critical features, daily for stable ones)\n",
    "- **Access control**: Implement RBAC for sensitive features (device performance data, proprietary test parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea2498",
   "metadata": {},
   "source": [
    "## üìä Diagnostic Checks Summary\n",
    "\n",
    "### Implementation Checklist\n",
    "‚úÖ **Offline Feature Store**\n",
    "- PostgreSQL/Parquet storage configured for historical features\n",
    "- Feature registration with schema validation\n",
    "- Point-in-time correct joins for training data generation\n",
    "\n",
    "‚úÖ **Online Feature Store**\n",
    "- Redis/DynamoDB deployed for low-latency serving (<10ms p99)\n",
    "- Materialization pipeline syncs offline ‚Üí online stores\n",
    "- Feature retrieval API integrated into inference service\n",
    "\n",
    "‚úÖ **Feature Transformations**\n",
    "- On-demand transformations: Real-time computations (temperature normalization, voltage deltas)\n",
    "- Batch transformations: Pre-computed aggregations (24hr moving avg current, weekly yield trends)\n",
    "- Streaming transformations: Kafka-based real-time feature updates\n",
    "\n",
    "‚úÖ **Monitoring & Observability**\n",
    "- Feature freshness alerts (data age >1hr for critical features)\n",
    "- Null rate tracking (>5% nulls trigger investigation)\n",
    "- Distribution drift detection (KS test p-value <0.05)\n",
    "- Query latency monitoring (p95 <5ms for online features)\n",
    "\n",
    "### Quality Metrics\n",
    "- **Training-serving consistency**: Feature values match within 0.1% between offline training and online serving\n",
    "- **Latency SLA**: 99th percentile online feature retrieval <10ms (target: 3ms)\n",
    "- **Data freshness**: Critical features <1hr old, stable features <24hr old\n",
    "- **Coverage**: >95% of feature requests served from cache (online store hit rate)\n",
    "\n",
    "### Post-Silicon Validation Applications\n",
    "**1. Parametric Feature Store for ATE Testing**\n",
    "- Features: Vdd_rolling_mean_24hr, Idd_percentile_95, freq_deviation_z_score\n",
    "- Use case: Predict test failures before running expensive long-duration tests\n",
    "- Business value: 30-40% reduction in test time by skipping predicted-fail devices\n",
    "\n",
    "**2. Spatial Features for Wafer Map Analysis**\n",
    "- Features: neighbor_fail_rate_5mm, radial_position_normalized, quadrant_yield_delta\n",
    "- Use case: Real-time die binning decisions during wafer sort\n",
    "- Business value: Improved bin accuracy reduces overkill (good dies marked bad) by 15-25%\n",
    "\n",
    "**3. Temporal Features for Yield Trending**\n",
    "- Features: lot_yield_ema_7day, fab_defect_density_trend, equipment_uptime_ratio\n",
    "- Use case: Predict lot-level yield before final test completion\n",
    "- Business value: Early yield excursions detected 2-3 days sooner, faster root cause analysis\n",
    "\n",
    "### Business ROI Estimation\n",
    "\n",
    "**Scenario 1: Medium-Volume Semiconductor Fab (100K wafers/year)**\n",
    "- Feature reuse across 15 models: $2.5M/year engineering time savings\n",
    "- Training-serving consistency eliminates skew: $4M/year reduced overkill/underkill\n",
    "- Low-latency serving enables real-time binning: $6M/year improved device mix\n",
    "- **Total ROI: $12.5M/year** (cost: $500K infrastructure + $300K team training = $11.7M net)\n",
    "\n",
    "**Scenario 2: High-Volume Automotive Semiconductor (500K wafers/year)**\n",
    "- Enterprise feature platform (50+ models): $12M/year engineering productivity\n",
    "- Point-in-time correct features prevent data leakage: $8M/year improved model accuracy\n",
    "- Sub-5ms serving latency for inline decisions: $18M/year test time reduction\n",
    "- Feature governance for ISO 26262 compliance: $5M/year audit cost savings\n",
    "- **Total ROI: $43M/year** (cost: $2.5M infrastructure + $1.2M team/ops = $39.3M net)\n",
    "\n",
    "**Scenario 3: Advanced Node R&D Fab (<10K wafers/year, high complexity)**\n",
    "- Feature lineage for experiment reproducibility: $1.5M/year research velocity\n",
    "- Cross-team feature sharing (design, test, yield teams): $3M/year collaboration efficiency\n",
    "- Feature versioning for A/B testing: $2.5M/year faster model iteration\n",
    "- **Total ROI: $7M/year** (cost: $400K infrastructure + $200K training = $6.4M net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0328f72e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Mastery Achievement\n",
    "\n",
    "**You now have production-grade expertise in:**\n",
    "- ‚úÖ Designing offline + online feature stores with Redis/PostgreSQL for <10ms serving latency\n",
    "- ‚úÖ Implementing point-in-time correct feature joins to prevent data leakage in time-series models\n",
    "- ‚úÖ Building materialization pipelines that sync batch features to real-time stores\n",
    "- ‚úÖ Monitoring feature freshness, null rates, and distribution drift in production\n",
    "- ‚úÖ Applying feature stores to semiconductor yield prediction, parametric testing, and wafer map analysis\n",
    "\n",
    "**Next Steps:**\n",
    "- **Advanced Feature Engineering**: Time-series features, spatial aggregations, graph-based features\n",
    "- **Feature Selection**: SHAP-based feature importance, correlation analysis, recursive elimination\n",
    "- **Feature Store at Scale**: Multi-region replication, disaster recovery, horizontal scaling strategies"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
