{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a05e474b",
   "metadata": {},
   "source": [
    "# 126: Continuous Training Pipelines - Automated Model Retraining and Drift Detection\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** continuous training paradigm for adaptive ML systems (models auto-retrain on new data)\n",
    "- **Implement** retraining triggers based on schedules, drift detection, and performance degradation\n",
    "- **Build** automated validation gates to prevent bad models from deploying\n",
    "- **Deploy** continuous training pipelines with Airflow orchestration\n",
    "- **Apply** drift detection to post-silicon test data (detect when device characteristics change)\n",
    "- **Monitor** model performance decay and trigger timely retraining\n",
    "\n",
    "## üìö What is Continuous Training?\n",
    "\n",
    "**Continuous training** is the MLOps practice of **automatically retraining models** as new data arrives, concept drift occurs, or performance degrades. Unlike static models (trained once, deployed forever), continuous training keeps models fresh and accurate.\n",
    "\n",
    "**Why Continuous Training?**\n",
    "- ‚úÖ **Adapt to change**: Models degrade over time as real-world patterns shift (customer behavior, device characteristics, market trends)\n",
    "- ‚úÖ **Improve with data**: New data improves model accuracy (more training samples = better generalization)\n",
    "- ‚úÖ **Detect drift**: Statistical tests detect data drift (input distribution changes) or concept drift (relationship between X and Y changes)\n",
    "- ‚úÖ **Reduce manual work**: Automate retraining instead of manual model updates every quarter\n",
    "\n",
    "**Continuous Training vs Batch Retraining:**\n",
    "- **Batch**: Retrain monthly/quarterly on schedule (simple but ignores drift, may miss critical changes)\n",
    "- **Continuous**: Retrain when drift detected or performance drops (adaptive but requires monitoring infrastructure)\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "### **Use Case 1: Yield Prediction Model Continuous Retraining**\n",
    "**Input:** Yield prediction model trained on Q1 wafer test data, deployed to production  \n",
    "**Problem:** Q2 shows 15% accuracy drop (new test equipment, process changes, different lot characteristics)  \n",
    "**Output:** Drift detection triggers automatic retraining on Q2 data, model accuracy recovers to baseline  \n",
    "**Value:** $2.8M/year from preventing false-positive/false-negative yield predictions (early detection of failing devices)\n",
    "\n",
    "### **Use Case 2: Parametric Test Outlier Detection with Drift Monitoring**\n",
    "**Input:** Outlier detection model for flagging suspicious test results (voltage, current, frequency out of spec)  \n",
    "**Problem:** New device generation has different parametric ranges, model flags normal devices as outliers (false alarms)  \n",
    "**Output:** Data drift detected via KS-test on parametric distributions, model retrains on new generation data  \n",
    "**Value:** $1.9M/year from reduced false alarms (engineering time saved investigating non-issues)\n",
    "\n",
    "### **Use Case 3: Test Time Prediction with Performance-Based Retraining**\n",
    "**Input:** Regression model predicting test duration for capacity planning  \n",
    "**Problem:** Model accuracy degrades from 95% to 78% (MAPE increases) after test flow changes  \n",
    "**Output:** Performance monitoring triggers retraining when MAPE exceeds 10% threshold  \n",
    "**Value:** $1.5M/year from accurate test scheduling (optimize tester utilization, reduce idle time)\n",
    "\n",
    "### **Use Case 4: Wafer Map Pattern Classification Auto-Retraining**\n",
    "**Input:** CNN classifying wafer map defect patterns (edge fail, center fail, scratch, random)  \n",
    "**Problem:** New defect pattern appears (not in training data), model defaults to \"unknown\" class  \n",
    "**Output:** Schedule-based weekly retraining incorporates new labeled wafer maps, model learns new patterns  \n",
    "**Value:** $1.2M/year from faster root cause analysis (classify new defect patterns immediately)\n",
    "\n",
    "**Total Post-Silicon Value:** $2.8M + $1.9M + $1.5M + $1.2M = **$7.4M/year**\n",
    "\n",
    "## üîÑ Continuous Training Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[üìä New Data Arrives] --> B[üîç Drift Detection]\n",
    "    B --> C{Drift Detected?}\n",
    "    C -->|Yes| D[‚è∞ Trigger Retraining]\n",
    "    C -->|No| E[üìÖ Check Schedule]\n",
    "    E --> F{Scheduled Retrain?}\n",
    "    F -->|Yes| D\n",
    "    F -->|No| G[‚úÖ Monitor Performance]\n",
    "    \n",
    "    D --> H[üèãÔ∏è Train New Model]\n",
    "    H --> I[‚úÖ Validation Gates]\n",
    "    I --> J{Pass Gates?}\n",
    "    J -->|No| K[‚ùå Reject Model]\n",
    "    J -->|Yes| L[üöÄ Deploy to Production]\n",
    "    L --> M[üìà Monitor Metrics]\n",
    "    M --> A\n",
    "    \n",
    "    K --> N[üìß Alert Team]\n",
    "    N --> A\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style L fill:#e1ffe1\n",
    "    style K fill:#ffe1e1\n",
    "    style J fill:#fff4e1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **Notebook 123: Model Monitoring & Drift Detection** - Drift detection techniques (KS-test, PSI, concept drift)\n",
    "- **Notebook 109: ML Pipelines with Airflow** - Orchestration for continuous training workflows\n",
    "\n",
    "**Next Steps:**\n",
    "- **Notebook 127: Model Governance & Compliance** - Governance for auto-deployed models\n",
    "- **Notebook 128: Shadow Mode Deployment** - Validate retrained models before full rollout\n",
    "\n",
    "---\n",
    "\n",
    "Let's build adaptive ML systems with continuous training! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268a76f",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "**Note**: Continuous training requires orchestration tools (Airflow) and monitoring libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e499a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install continuous training libraries\n",
    "# !pip install scikit-learn pandas numpy mlflow schedule scipy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.stats import ks_2samp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Continuous training libraries loaded\")\n",
    "print(\"Focus: Automated retraining, drift detection, validation gates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32426f6f",
   "metadata": {},
   "source": [
    "## 2. Retraining Trigger Mechanisms\n",
    "\n",
    "**Purpose:** Implement trigger logic that determines WHEN to retrain models.\n",
    "\n",
    "**Key Points:**\n",
    "- **Schedule-based triggers**: Time-based (daily, weekly, monthly) - simplest approach\n",
    "- **Drift-based triggers**: Detect data drift or concept drift using statistical tests\n",
    "- **Performance-based triggers**: Monitor production metrics (accuracy, F1, RMSE drops below threshold)\n",
    "- **Hybrid triggers**: Combine multiple conditions (e.g., drift AND schedule)\n",
    "\n",
    "**Why This Matters:** Proper triggers prevent unnecessary retraining (waste compute) and delayed retraining (accuracy degrades)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157e9cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrainingTrigger:\n",
    "    \"\"\"\n",
    "    Manages multiple trigger conditions for automated model retraining.\n",
    "    \n",
    "    Supports:\n",
    "    - Schedule-based triggers (time intervals)\n",
    "    - Data drift triggers (KS test for feature distribution changes)\n",
    "    - Performance degradation triggers (metric thresholds)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, schedule_days=7, drift_threshold=0.05, perf_threshold=0.80):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            schedule_days: Retrain every N days regardless of drift/performance\n",
    "            drift_threshold: p-value threshold for KS test (reject if p < threshold)\n",
    "            perf_threshold: Minimum acceptable accuracy (retrain if below)\n",
    "        \"\"\"\n",
    "        self.schedule_days = schedule_days\n",
    "        self.drift_threshold = drift_threshold\n",
    "        self.perf_threshold = perf_threshold\n",
    "        self.last_retrain_date = datetime.now()\n",
    "        \n",
    "    def check_schedule_trigger(self):\n",
    "        \"\"\"Check if scheduled retraining interval has passed.\"\"\"\n",
    "        days_since_retrain = (datetime.now() - self.last_retrain_date).days\n",
    "        triggered = days_since_retrain >= self.schedule_days\n",
    "        \n",
    "        return {\n",
    "            'triggered': triggered,\n",
    "            'reason': f'Scheduled retrain (every {self.schedule_days} days)',\n",
    "            'days_since_retrain': days_since_retrain\n",
    "        }\n",
    "    \n",
    "    def check_drift_trigger(self, train_data, production_data, feature_cols):\n",
    "        \"\"\"\n",
    "        Detect data drift using Kolmogorov-Smirnov (KS) test.\n",
    "        \n",
    "        KS test compares distributions:\n",
    "        - Null hypothesis: Training and production data from same distribution\n",
    "        - p-value < threshold ‚Üí reject null ‚Üí drift detected ‚Üí retrain\n",
    "        \"\"\"\n",
    "        drift_detected = False\n",
    "        drift_features = []\n",
    "        \n",
    "        for feature in feature_cols:\n",
    "            # KS test for each feature\n",
    "            statistic, p_value = ks_2samp(\n",
    "                train_data[feature], \n",
    "                production_data[feature]\n",
    "            )\n",
    "            \n",
    "            if p_value < self.drift_threshold:\n",
    "                drift_detected = True\n",
    "                drift_features.append((feature, p_value, statistic))\n",
    "        \n",
    "        return {\n",
    "            'triggered': drift_detected,\n",
    "            'reason': 'Data drift detected',\n",
    "            'drift_features': drift_features,\n",
    "            'num_drifted': len(drift_features)\n",
    "        }\n",
    "    \n",
    "    def check_performance_trigger(self, current_accuracy):\n",
    "        \"\"\"Check if production model performance dropped below threshold.\"\"\"\n",
    "        triggered = current_accuracy < self.perf_threshold\n",
    "        \n",
    "        return {\n",
    "            'triggered': triggered,\n",
    "            'reason': f'Performance degradation (accuracy={current_accuracy:.3f} < {self.perf_threshold})',\n",
    "            'current_accuracy': current_accuracy\n",
    "        }\n",
    "    \n",
    "    def should_retrain(self, train_data=None, production_data=None, \n",
    "                       feature_cols=None, current_accuracy=None):\n",
    "        \"\"\"\n",
    "        Master decision function: Check all trigger conditions.\n",
    "        \n",
    "        Returns True if ANY trigger condition met (OR logic).\n",
    "        \"\"\"\n",
    "        triggers = {}\n",
    "        \n",
    "        # Check schedule\n",
    "        schedule_result = self.check_schedule_trigger()\n",
    "        triggers['schedule'] = schedule_result\n",
    "        \n",
    "        # Check drift (if data provided)\n",
    "        if train_data is not None and production_data is not None and feature_cols is not None:\n",
    "            drift_result = self.check_drift_trigger(train_data, production_data, feature_cols)\n",
    "            triggers['drift'] = drift_result\n",
    "        \n",
    "        # Check performance (if accuracy provided)\n",
    "        if current_accuracy is not None:\n",
    "            perf_result = self.check_performance_trigger(current_accuracy)\n",
    "            triggers['performance'] = perf_result\n",
    "        \n",
    "        # Decision: Retrain if ANY trigger fired\n",
    "        should_retrain = any(t['triggered'] for t in triggers.values())\n",
    "        \n",
    "        return {\n",
    "            'should_retrain': should_retrain,\n",
    "            'triggers': triggers,\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "\n",
    "# Example: Initialize trigger system\n",
    "trigger = RetrainingTrigger(\n",
    "    schedule_days=7,        # Weekly retraining\n",
    "    drift_threshold=0.05,   # 5% significance level for KS test\n",
    "    perf_threshold=0.80     # Retrain if accuracy drops below 80%\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Retraining trigger system initialized\")\n",
    "print(f\"Schedule: Every {trigger.schedule_days} days\")\n",
    "print(f\"Drift threshold: p-value < {trigger.drift_threshold}\")\n",
    "print(f\"Performance threshold: accuracy >= {trigger.perf_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3178fdc",
   "metadata": {},
   "source": [
    "## 3. Automated Retraining Pipeline\n",
    "\n",
    "**Purpose:** Build end-to-end pipeline that automatically retrains, validates, and deploys models.\n",
    "\n",
    "**Key Points:**\n",
    "- **Data fetching**: Pull latest data from production (feature store, database)\n",
    "- **Feature engineering**: Consistent transformations (use feature store definitions)\n",
    "- **Model training**: Same hyperparameters OR hyperparameter tuning\n",
    "- **Validation gates**: Multiple checks before deployment (accuracy, fairness, business rules)\n",
    "- **Model versioning**: Track each retrained model with metadata (timestamp, trigger reason, metrics)\n",
    "\n",
    "**Why This Matters:** Automation ensures consistency and reduces human error in production retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f65d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousTrainingPipeline:\n",
    "    \"\"\"\n",
    "    End-to-end automated retraining pipeline with validation gates.\n",
    "    \n",
    "    Workflow:\n",
    "    1. Fetch latest data\n",
    "    2. Engineer features\n",
    "    3. Train new model\n",
    "    4. Validate against production model\n",
    "    5. Deploy if passes all gates\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_class=RandomForestClassifier, validation_holdout=0.2):\n",
    "        self.model_class = model_class\n",
    "        self.validation_holdout = validation_holdout\n",
    "        self.production_model = None\n",
    "        self.production_metrics = {}\n",
    "        self.retrain_history = []\n",
    "        \n",
    "    def fetch_data(self, start_date, end_date):\n",
    "        \"\"\"Simulate fetching production data (replace with real data source).\"\"\"\n",
    "        # In production: Query database, feature store, or data warehouse\n",
    "        # Example: SELECT * FROM stdf_tests WHERE test_date BETWEEN start_date AND end_date\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "        \n",
    "        data = pd.DataFrame({\n",
    "            'vdd': np.random.normal(1.2, 0.05, n_samples),\n",
    "            'idd': np.random.normal(50, 5, n_samples),\n",
    "            'frequency': np.random.normal(2400, 100, n_samples),\n",
    "            'temperature': np.random.normal(25, 5, n_samples),\n",
    "            'yield': np.random.choice([0, 1], n_samples, p=[0.1, 0.9])\n",
    "        })\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def engineer_features(self, data):\n",
    "        \"\"\"Feature engineering (consistent with training pipeline).\"\"\"\n",
    "        features = data.copy()\n",
    "        \n",
    "        # Derived features\n",
    "        features['power'] = features['vdd'] * features['idd']\n",
    "        features['power_efficiency'] = features['frequency'] / features['power']\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def train_model(self, X_train, y_train):\n",
    "        \"\"\"Train new model candidate.\"\"\"\n",
    "        model = self.model_class(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    def validate_model(self, model, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Validation gates: Check if new model meets quality thresholds.\n",
    "        \n",
    "        Gates:\n",
    "        1. Accuracy > 0.80 (absolute threshold)\n",
    "        2. Better than production model (relative threshold)\n",
    "        3. F1 score > 0.75 (class balance check)\n",
    "        \"\"\"\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "        \n",
    "        gates_passed = []\n",
    "        gates_failed = []\n",
    "        \n",
    "        # Gate 1: Absolute accuracy threshold\n",
    "        if accuracy >= 0.80:\n",
    "            gates_passed.append(f\"‚úÖ Accuracy gate ({accuracy:.3f} >= 0.80)\")\n",
    "        else:\n",
    "            gates_failed.append(f\"‚ùå Accuracy gate ({accuracy:.3f} < 0.80)\")\n",
    "        \n",
    "        # Gate 2: Better than production (if production model exists)\n",
    "        if self.production_model is not None:\n",
    "            prod_accuracy = self.production_metrics.get('accuracy', 0)\n",
    "            if accuracy >= prod_accuracy:\n",
    "                gates_passed.append(f\"‚úÖ Production comparison ({accuracy:.3f} >= {prod_accuracy:.3f})\")\n",
    "            else:\n",
    "                gates_failed.append(f\"‚ùå Production comparison ({accuracy:.3f} < {prod_accuracy:.3f})\")\n",
    "        \n",
    "        # Gate 3: F1 score threshold\n",
    "        if f1 >= 0.75:\n",
    "            gates_passed.append(f\"‚úÖ F1 score gate ({f1:.3f} >= 0.75)\")\n",
    "        else:\n",
    "            gates_failed.append(f\"‚ùå F1 score gate ({f1:.3f} < 0.75)\")\n",
    "        \n",
    "        passed = len(gates_failed) == 0\n",
    "        \n",
    "        return {\n",
    "            'passed': passed,\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'gates_passed': gates_passed,\n",
    "            'gates_failed': gates_failed\n",
    "        }\n",
    "    \n",
    "    def deploy_model(self, model, metrics, reason):\n",
    "        \"\"\"Deploy new model to production.\"\"\"\n",
    "        self.production_model = model\n",
    "        self.production_metrics = metrics\n",
    "        \n",
    "        # Record deployment\n",
    "        self.retrain_history.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'reason': reason,\n",
    "            'accuracy': metrics['accuracy'],\n",
    "            'f1': metrics['f1']\n",
    "        })\n",
    "        \n",
    "        print(f\"üöÄ Model deployed to production\")\n",
    "        print(f\"   Reason: {reason}\")\n",
    "        print(f\"   Accuracy: {metrics['accuracy']:.3f}\")\n",
    "        print(f\"   F1 Score: {metrics['f1']:.3f}\")\n",
    "    \n",
    "    def run_retraining(self, start_date, end_date, trigger_reason):\n",
    "        \"\"\"\n",
    "        Execute full retraining workflow.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Retraining result (deployed, metrics, gates)\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Starting retraining pipeline...\")\n",
    "        print(f\"   Trigger: {trigger_reason}\")\n",
    "        \n",
    "        # Step 1: Fetch data\n",
    "        data = self.fetch_data(start_date, end_date)\n",
    "        print(f\"‚úÖ Fetched {len(data)} samples\")\n",
    "        \n",
    "        # Step 2: Engineer features\n",
    "        features = self.engineer_features(data)\n",
    "        feature_cols = ['vdd', 'idd', 'frequency', 'temperature', 'power', 'power_efficiency']\n",
    "        X = features[feature_cols]\n",
    "        y = features['yield']\n",
    "        \n",
    "        # Step 3: Split data (use holdout for validation)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=self.validation_holdout, random_state=42\n",
    "        )\n",
    "        print(f\"‚úÖ Train: {len(X_train)}, Validation: {len(X_val)}\")\n",
    "        \n",
    "        # Step 4: Train new model\n",
    "        model = self.train_model(X_train, y_train)\n",
    "        print(f\"‚úÖ Model trained\")\n",
    "        \n",
    "        # Step 5: Validate model\n",
    "        validation_result = self.validate_model(model, X_val, y_val)\n",
    "        \n",
    "        print(f\"\\nüìä Validation Results:\")\n",
    "        for gate in validation_result['gates_passed']:\n",
    "            print(f\"   {gate}\")\n",
    "        for gate in validation_result['gates_failed']:\n",
    "            print(f\"   {gate}\")\n",
    "        \n",
    "        # Step 6: Deploy if passed\n",
    "        if validation_result['passed']:\n",
    "            self.deploy_model(model, validation_result, trigger_reason)\n",
    "            return {\n",
    "                'deployed': True,\n",
    "                'metrics': validation_result,\n",
    "                'reason': trigger_reason\n",
    "            }\n",
    "        else:\n",
    "            print(f\"‚ùå Model rejected (failed validation gates)\")\n",
    "            return {\n",
    "                'deployed': False,\n",
    "                'metrics': validation_result,\n",
    "                'reason': trigger_reason\n",
    "            }\n",
    "\n",
    "# Example: Initialize and run pipeline\n",
    "pipeline = ContinuousTrainingPipeline(\n",
    "    model_class=RandomForestClassifier,\n",
    "    validation_holdout=0.2\n",
    ")\n",
    "\n",
    "# Simulate retraining\n",
    "result = pipeline.run_retraining(\n",
    "    start_date=datetime.now() - timedelta(days=30),\n",
    "    end_date=datetime.now(),\n",
    "    trigger_reason=\"Initial training\"\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Deployment status: {result['deployed']}\")\n",
    "print(f\"Accuracy: {result['metrics']['accuracy']:.3f}\")\n",
    "print(f\"F1 Score: {result['metrics']['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b46794",
   "metadata": {},
   "source": [
    "## 4. End-to-End Continuous Training System (Post-Silicon Example)\n",
    "\n",
    "**Purpose:** Demonstrate complete CT system for yield prediction with trigger detection and automated retraining.\n",
    "\n",
    "**Key Points:**\n",
    "- **Trigger monitoring**: Check schedule, drift, and performance continuously\n",
    "- **Automatic execution**: Run retraining pipeline when triggered\n",
    "- **Model versioning**: Track all models (production + candidates)\n",
    "- **Rollback capability**: Revert to previous version if new model fails in production\n",
    "\n",
    "**Why This Matters:** Real-world CT requires orchestration of triggers, retraining, validation, and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6793f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate production environment over time\n",
    "print(\"üè≠ Simulating Production Environment for Yield Prediction CT System\\n\")\n",
    "\n",
    "# Initial production model training\n",
    "pipeline = ContinuousTrainingPipeline(model_class=RandomForestClassifier)\n",
    "trigger = RetrainingTrigger(schedule_days=7, drift_threshold=0.05, perf_threshold=0.85)\n",
    "\n",
    "# Train initial model\n",
    "initial_result = pipeline.run_retraining(\n",
    "    start_date=datetime.now() - timedelta(days=60),\n",
    "    end_date=datetime.now() - timedelta(days=30),\n",
    "    trigger_reason=\"Initial model deployment\"\n",
    ")\n",
    "\n",
    "# Record as production baseline\n",
    "trigger.last_retrain_date = datetime.now() - timedelta(days=10)  # Simulate 10 days ago\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìä Production Monitoring - Day 10\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Simulate new production data (with drift)\n",
    "train_data_original = pipeline.fetch_data(\n",
    "    datetime.now() - timedelta(days=60),\n",
    "    datetime.now() - timedelta(days=30)\n",
    ")\n",
    "\n",
    "production_data_drifted = pipeline.fetch_data(\n",
    "    datetime.now() - timedelta(days=7),\n",
    "    datetime.now()\n",
    ")\n",
    "\n",
    "# Introduce drift (Vdd voltage shifted due to process change)\n",
    "production_data_drifted['vdd'] = production_data_drifted['vdd'] + 0.08  # +80mV shift\n",
    "\n",
    "# Simulate performance degradation\n",
    "production_accuracy = 0.82  # Dropped from initial 0.90\n",
    "\n",
    "# Check if retraining should trigger\n",
    "feature_cols = ['vdd', 'idd', 'frequency', 'temperature']\n",
    "trigger_decision = trigger.should_retrain(\n",
    "    train_data=train_data_original,\n",
    "    production_data=production_data_drifted,\n",
    "    feature_cols=feature_cols,\n",
    "    current_accuracy=production_accuracy\n",
    ")\n",
    "\n",
    "print(\"üîç Trigger Analysis:\")\n",
    "print(f\"   Schedule trigger: {trigger_decision['triggers']['schedule']['triggered']}\")\n",
    "print(f\"   Days since retrain: {trigger_decision['triggers']['schedule']['days_since_retrain']}\")\n",
    "\n",
    "if 'drift' in trigger_decision['triggers']:\n",
    "    drift_info = trigger_decision['triggers']['drift']\n",
    "    print(f\"   Drift trigger: {drift_info['triggered']}\")\n",
    "    if drift_info['triggered']:\n",
    "        print(f\"   Drifted features: {drift_info['num_drifted']}\")\n",
    "        for feature, p_value, statistic in drift_info['drift_features']:\n",
    "            print(f\"      - {feature}: p={p_value:.4f}, KS-stat={statistic:.3f}\")\n",
    "\n",
    "if 'performance' in trigger_decision['triggers']:\n",
    "    perf_info = trigger_decision['triggers']['performance']\n",
    "    print(f\"   Performance trigger: {perf_info['triggered']}\")\n",
    "    print(f\"   Current accuracy: {perf_info['current_accuracy']:.3f}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  DECISION: {'RETRAIN REQUIRED' if trigger_decision['should_retrain'] else 'NO RETRAIN NEEDED'}\")\n",
    "\n",
    "# Execute retraining if triggered\n",
    "if trigger_decision['should_retrain']:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Executing Automated Retraining\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Determine primary trigger reason\n",
    "    reasons = []\n",
    "    if trigger_decision['triggers']['schedule']['triggered']:\n",
    "        reasons.append(\"Scheduled retrain\")\n",
    "    if trigger_decision['triggers'].get('drift', {}).get('triggered', False):\n",
    "        reasons.append(\"Data drift detected\")\n",
    "    if trigger_decision['triggers'].get('performance', {}).get('triggered', False):\n",
    "        reasons.append(\"Performance degradation\")\n",
    "    \n",
    "    trigger_reason = \" + \".join(reasons)\n",
    "    \n",
    "    # Run retraining pipeline\n",
    "    retrain_result = pipeline.run_retraining(\n",
    "        start_date=datetime.now() - timedelta(days=30),\n",
    "        end_date=datetime.now(),\n",
    "        trigger_reason=trigger_reason\n",
    "    )\n",
    "    \n",
    "    if retrain_result['deployed']:\n",
    "        trigger.last_retrain_date = datetime.now()  # Update last retrain timestamp\n",
    "        print(f\"\\n‚úÖ Continuous training cycle completed successfully\")\n",
    "        print(f\"   Model version: {len(pipeline.retrain_history)}\")\n",
    "        print(f\"   Previous accuracy: {production_accuracy:.3f}\")\n",
    "        print(f\"   New accuracy: {retrain_result['metrics']['accuracy']:.3f}\")\n",
    "        print(f\"   Improvement: {retrain_result['metrics']['accuracy'] - production_accuracy:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Model rejected - production model retained\")\n",
    "\n",
    "# Show retraining history\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìà Retraining History\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for i, record in enumerate(pipeline.retrain_history, 1):\n",
    "    print(f\"Version {i}:\")\n",
    "    print(f\"   Timestamp: {record['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"   Reason: {record['reason']}\")\n",
    "    print(f\"   Accuracy: {record['accuracy']:.3f}\")\n",
    "    print(f\"   F1 Score: {record['f1']:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e418ff",
   "metadata": {},
   "source": [
    "## 5. Pipeline Orchestration with Airflow\n",
    "\n",
    "**Purpose:** Integrate continuous training into production workflows using Apache Airflow DAGs.\n",
    "\n",
    "**Key Points:**\n",
    "- **DAGs (Directed Acyclic Graphs)**: Define task dependencies (data fetch ‚Üí train ‚Üí validate ‚Üí deploy)\n",
    "- **Scheduling**: Cron expressions for regular checks (daily, weekly)\n",
    "- **Retries**: Automatic retry on failure (network issues, data availability)\n",
    "- **Monitoring**: Track task status, execution time, failures\n",
    "- **Alerting**: Notify team when retraining fails or model degrades\n",
    "\n",
    "**Why This Matters:** Airflow is industry standard for ML pipeline orchestration (used by Netflix, Airbnb, Spotify)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8dc1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow DAG for Continuous Training (Conceptual Example)\n",
    "# Note: This is simplified pseudocode - real Airflow requires installation and setup\n",
    "\n",
    "airflow_dag_code = \"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define default arguments\n",
    "default_args = {\n",
    "    'owner': 'mlops-team',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['mlops@company.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "# Create DAG\n",
    "dag = DAG(\n",
    "    'yield_prediction_continuous_training',\n",
    "    default_args=default_args,\n",
    "    description='Automated yield prediction model retraining',\n",
    "    schedule_interval='0 2 * * 0',  # Every Sunday at 2 AM\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['ml', 'continuous-training', 'yield-prediction']\n",
    ")\n",
    "\n",
    "# Task 1: Check triggers\n",
    "def check_triggers_task(**context):\n",
    "    trigger = RetrainingTrigger(schedule_days=7, drift_threshold=0.05, perf_threshold=0.85)\n",
    "    \n",
    "    # Fetch data and check triggers\n",
    "    decision = trigger.should_retrain(\n",
    "        train_data=fetch_train_data(),\n",
    "        production_data=fetch_production_data(),\n",
    "        feature_cols=['vdd', 'idd', 'frequency', 'temperature'],\n",
    "        current_accuracy=get_production_accuracy()\n",
    "    )\n",
    "    \n",
    "    # Push decision to XCom (Airflow's inter-task communication)\n",
    "    context['ti'].xcom_push(key='should_retrain', value=decision['should_retrain'])\n",
    "    context['ti'].xcom_push(key='trigger_reason', value=str(decision['triggers']))\n",
    "    \n",
    "    return decision['should_retrain']\n",
    "\n",
    "check_triggers = PythonOperator(\n",
    "    task_id='check_retraining_triggers',\n",
    "    python_callable=check_triggers_task,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Task 2: Fetch latest data\n",
    "def fetch_data_task(**context):\n",
    "    # Pull trigger decision\n",
    "    should_retrain = context['ti'].xcom_pull(task_ids='check_retraining_triggers', key='should_retrain')\n",
    "    \n",
    "    if not should_retrain:\n",
    "        return \"Skipping - no retrain needed\"\n",
    "    \n",
    "    # Fetch from data warehouse\n",
    "    data = fetch_stdf_data(days=30)\n",
    "    \n",
    "    # Save to temporary location\n",
    "    data.to_parquet('/tmp/retrain_data.parquet')\n",
    "    \n",
    "    return f\"Fetched {len(data)} samples\"\n",
    "\n",
    "fetch_data = PythonOperator(\n",
    "    task_id='fetch_training_data',\n",
    "    python_callable=fetch_data_task,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Task 3: Train model\n",
    "def train_model_task(**context):\n",
    "    should_retrain = context['ti'].xcom_pull(task_ids='check_retraining_triggers', key='should_retrain')\n",
    "    \n",
    "    if not should_retrain:\n",
    "        return \"Skipping - no retrain needed\"\n",
    "    \n",
    "    # Load data\n",
    "    data = pd.read_parquet('/tmp/retrain_data.parquet')\n",
    "    \n",
    "    # Train model\n",
    "    pipeline = ContinuousTrainingPipeline()\n",
    "    result = pipeline.run_retraining(\n",
    "        start_date=datetime.now() - timedelta(days=30),\n",
    "        end_date=datetime.now(),\n",
    "        trigger_reason=context['ti'].xcom_pull(task_ids='check_retraining_triggers', key='trigger_reason')\n",
    "    )\n",
    "    \n",
    "    # Push result\n",
    "    context['ti'].xcom_push(key='training_result', value=result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "train_model = PythonOperator(\n",
    "    task_id='train_new_model',\n",
    "    python_callable=train_model_task,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Task 4: Validate model\n",
    "def validate_model_task(**context):\n",
    "    result = context['ti'].xcom_pull(task_ids='train_new_model', key='training_result')\n",
    "    \n",
    "    if result['deployed']:\n",
    "        return \"Model passed validation gates\"\n",
    "    else:\n",
    "        raise ValueError(\"Model failed validation - check gates\")\n",
    "\n",
    "validate_model = PythonOperator(\n",
    "    task_id='validate_new_model',\n",
    "    python_callable=validate_model_task,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Task 5: Deploy to production\n",
    "def deploy_model_task(**context):\n",
    "    result = context['ti'].xcom_pull(task_ids='train_new_model', key='training_result')\n",
    "    \n",
    "    if result['deployed']:\n",
    "        # Update model registry\n",
    "        # Copy model to production serving location\n",
    "        # Update feature store references\n",
    "        return \"Model deployed to production\"\n",
    "    else:\n",
    "        return \"No deployment - validation failed\"\n",
    "\n",
    "deploy_model = PythonOperator(\n",
    "    task_id='deploy_to_production',\n",
    "    python_callable=deploy_model_task,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Task 6: Send notification\n",
    "def notify_task(**context):\n",
    "    result = context['ti'].xcom_pull(task_ids='train_new_model', key='training_result')\n",
    "    \n",
    "    # Send Slack/email notification\n",
    "    send_notification(\n",
    "        subject=\"Yield Prediction Model Retrained\",\n",
    "        body=f\"Deployed: {result['deployed']}\\\\nAccuracy: {result['metrics']['accuracy']:.3f}\"\n",
    "    )\n",
    "    \n",
    "    return \"Notification sent\"\n",
    "\n",
    "notify = PythonOperator(\n",
    "    task_id='send_notification',\n",
    "    python_callable=notify_task,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Define task dependencies (execution order)\n",
    "check_triggers >> fetch_data >> train_model >> validate_model >> deploy_model >> notify\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìã Airflow DAG Structure for Continuous Training:\")\n",
    "print(\"=\"*60)\n",
    "print(airflow_dag_code)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ DAG defines 6 tasks:\")\n",
    "print(\"   1. Check triggers (schedule, drift, performance)\")\n",
    "print(\"   2. Fetch latest data (from data warehouse)\")\n",
    "print(\"   3. Train new model (retraining pipeline)\")\n",
    "print(\"   4. Validate model (validation gates)\")\n",
    "print(\"   5. Deploy to production (if passed)\")\n",
    "print(\"   6. Send notification (Slack/email)\")\n",
    "print(\"\\nSchedule: Every Sunday at 2 AM (weekly retraining)\")\n",
    "print(\"Retries: 2 attempts with 5-minute delay\")\n",
    "print(\"Alerts: Email on failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3bd021",
   "metadata": {},
   "source": [
    "## 6. Real-World Project Templates\n",
    "\n",
    "**Purpose:** 8 production-ready continuous training projects (4 post-silicon validation + 4 general AI/ML).\n",
    "\n",
    "**Pattern:** Each project includes objectives, triggers, validation gates, and success criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296e1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = {\n",
    "    \"post_silicon\": [\n",
    "        {\n",
    "            \"name\": \"Yield Prediction CT with Multi-Fab Drift Detection\",\n",
    "            \"objective\": \"Continuous training system for wafer yield prediction across multiple fabs with fab-specific drift detection\",\n",
    "            \"triggers\": [\n",
    "                \"Schedule: Weekly retrain on Sunday 2 AM\",\n",
    "                \"Data drift: KS test p-value < 0.05 for Vdd, Idd, frequency (fab-specific baselines)\",\n",
    "                \"Performance: Accuracy drops below 88% on rolling 7-day validation\",\n",
    "                \"Manual: Triggered when process change implemented (e.g., new litho tool)\"\n",
    "            ],\n",
    "            \"validation_gates\": [\n",
    "                \"Accuracy >= 90% on holdout test set (1000 wafers)\",\n",
    "                \"F1 score >= 0.85 (prevent class imbalance issues)\",\n",
    "                \"Better than production model by >= 1%\",\n",
    "                \"Fairness: Accuracy variance across fabs < 5%\",\n",
    "                \"Business rule: Reject if predicts >15% yield loss (unrealistic)\"\n",
    "            ],\n",
    "            \"features\": [\n",
    "                \"STDF parametric data: Vdd, Idd, frequency, power, temperature\",\n",
    "                \"Derived features: power_efficiency, voltage_margin, test_coverage\",\n",
    "                \"Fab metadata: fab_id, tool_id, operator_shift\",\n",
    "                \"Temporal: day_of_week, lot_age, time_since_last_calibration\",\n",
    "                \"Rolling aggregates: 7-day mean Vdd, std Idd per tool\"\n",
    "            ],\n",
    "            \"orchestration\": \"Airflow DAG with 7 tasks (trigger check ‚Üí fetch data ‚Üí feature engineering ‚Üí train ‚Üí validate ‚Üí deploy ‚Üí notify)\",\n",
    "            \"success_criteria\": \"Maintain 90%+ accuracy for 6 months, <2% accuracy degradation between retrains, zero false rejections of good models\",\n",
    "            \"value\": \"Save $2M/year by maintaining yield prediction accuracy (prevent scrap, optimize test coverage)\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Test Time Prediction Retraining on Program Changes\",\n",
    "            \"objective\": \"Auto-retrain test time model when test programs updated (new tests added, sequences changed)\",\n",
    "            \"triggers\": [\n",
    "                \"Event-driven: Test program version change detected in config management\",\n",
    "                \"Performance: MAPE increases from baseline 3% to >5%\",\n",
    "                \"Schedule: Nightly if any test program changed in last 24 hours\",\n",
    "                \"Data drift: Test execution time distribution shifts (KS test)\"\n",
    "            ],\n",
    "            \"validation_gates\": [\n",
    "                \"MAPE <= 4% on validation set (100 lots)\",\n",
    "                \"Per-test prediction error < 10% for 95% of tests\",\n",
    "                \"Prediction time < 50ms (real-time constraint)\",\n",
    "                \"Better than production model by >= 0.5% MAPE\"\n",
    "            ],\n",
    "            \"features\": [\n",
    "                \"Test program: test_name, test_category, parallelization_factor\",\n",
    "                \"Historical: test_time_mean, test_time_std (last 1000 runs)\",\n",
    "                \"Device context: device_type, package, temperature\",\n",
    "                \"Load context: tester_utilization, time_of_day, concurrent_jobs\"\n",
    "            ],\n",
    "            \"orchestration\": \"Kubernetes CronJob + event-driven trigger (listen to test program Git commits)\",\n",
    "            \"success_criteria\": \"Maintain MAPE <4% across all test programs, <1 hour retrain latency from program change, 100% uptime\",\n",
    "            \"value\": \"Optimize test floor capacity planning (prevent 20% underutilization = $500K/year)\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Binning Model CT for Dynamic Spec Adaptation\",\n",
    "            \"objective\": \"Retrain binning model when product specs change (voltage limits, frequency targets, customer requirements)\",\n",
    "            \"triggers\": [\n",
    "                \"Manual: Triggered by product engineer when specs updated\",\n",
    "                \"Schedule: Monthly review of binning accuracy\",\n",
    "                \"Performance: Bin mismatch rate >3% (predicted vs actual customer bin)\",\n",
    "                \"Data drift: Test limit distribution changes (new voltage corners)\"\n",
    "            ],\n",
    "            \"validation_gates\": [\n",
    "                \"Bin accuracy >= 97% (predicted bin matches customer bin)\",\n",
    "                \"Zero misclassifications of fails as premium bins (revenue risk)\",\n",
    "                \"Bin distribution matches expected mix (prevent yield loss)\",\n",
    "                \"Fairness: Bin accuracy consistent across device types\"\n",
    "            ],\n",
    "            \"features\": [\n",
    "                \"Parametric test results: Vdd_min, Vdd_max, Fmax, leakage_current\",\n",
    "                \"Spec margins: distance_to_spec_limit (for each parameter)\",\n",
    "                \"Correlation features: Vdd_Fmax_ratio, power_at_nominal\",\n",
    "                \"Historical: device_family_avg_bin (population prior)\"\n",
    "            ],\n",
    "            \"orchestration\": \"Manual trigger via API + monthly Airflow DAG for validation\",\n",
    "            \"success_criteria\": \"97%+ binning accuracy, <0.1% premium bin false positives, <24 hour latency from spec change to deployment\",\n",
    "            \"value\": \"Ensure revenue optimization ($50M/year for flagship product), prevent warranty claims from misbinning\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Wafer Map Anomaly Detection Auto-Retraining\",\n",
    "            \"objective\": \"Continuous training for spatial pattern anomaly detection (catch new defect signatures within 30 days)\",\n",
    "            \"triggers\": [\n",
    "                \"Schedule: Monthly retrain (capture seasonal process variations)\",\n",
    "                \"Data drift: Spatial autocorrelation changes (Moran's I statistic)\",\n",
    "                \"Performance: Precision drops below 80% (too many false anomalies)\",\n",
    "                \"Manual: After new defect type discovered (root cause analysis)\"\n",
    "            ],\n",
    "            \"validation_gates\": [\n",
    "                \"Precision >= 85% (anomalies are real defects, not noise)\",\n",
    "                \"Recall >= 75% (catch majority of anomalies)\",\n",
    "                \"Spatial coverage: Detect anomalies in all wafer regions (edge, center)\",\n",
    "                \"False positive rate < 5% (prevent fab disruption)\"\n",
    "            ],\n",
    "            \"features\": [\n",
    "                \"Spatial: die_x, die_y, distance_to_wafer_center, radial_zone\",\n",
    "                \"Parametric: Vdd, Idd, frequency (per die)\",\n",
    "                \"Derived: local_yield (3x3 neighborhood), spatial_gradient\",\n",
    "                \"Contextual: wafer_id, lot_id, fab_tool_id, process_step\"\n",
    "            ],\n",
    "            \"orchestration\": \"Airflow DAG (monthly) + Kubeflow pipeline for model training (GPU for autoencoder)\",\n",
    "            \"success_criteria\": \"80%+ precision and recall for 6 months, detect new defect type within 30 days, <5% false positive rate\",\n",
    "            \"value\": \"Accelerate defect detection (save $1M/year in scrap), enable proactive yield improvement\"\n",
    "        }\n",
    "    ],\n",
    "    \"general_ml\": [\n",
    "        {\n",
    "            \"name\": \"E-Commerce Recommendation CT with Seasonal Adaptation\",\n",
    "            \"objective\": \"Retrain recommendation model to adapt to seasonal trends (holidays, back-to-school, prime day)\",\n",
    "            \"triggers\": [\n",
    "                \"Schedule: Weekly retrain during high-traffic seasons, monthly otherwise\",\n",
    "                \"Performance: CTR drops below 3.5% (rolling 7-day average)\",\n",
    "                \"Data drift: User behavior distribution shifts (KS test on session duration, categories)\",\n",
    "                \"Event-driven: Major catalog updates (>10% new products)\"\n",
    "            ],\n",
    "            \"validation_gates\": [\n",
    "                \"CTR >= 4.0% on holdout users (last 3 days)\",\n",
    "                \"Diversity: Recommend products from >= 15 categories (prevent filter bubble)\",\n",
    "                \"Novelty: 20% recommendations are products user hasn't seen\",\n",
    "                \"Better than production by >= 0.3% CTR\"\n",
    "            ],\n",
    "            \"features\": [\n",
    "                \"User: purchase_history, browsing_history, demographics, lifetime_value\",\n",
    "                \"Product: category, price, ratings, inventory_status\",\n",
    "                \"Context: time_of_day, device_type, session_duration\",\n",
    "                \"Temporal: days_to_holiday, trending_score (last 24 hours)\"\n",
    "            ],\n",
    "            \"orchestration\": \"Kubeflow pipeline with dynamic scheduling (weekly/monthly based on traffic)\",\n",
    "            \"success_criteria\": \"Maintain 4%+ CTR year-round, <12 hour retrain latency, zero downtime deployments\",\n",
    "            \"value\": \"Increase revenue $5M/year (4% CTR vs 3.5% baseline = 15% more conversions)\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Fraud Detection CT with Adversarial Drift Handling\",\n",
    "            \"objective\": \"Retrain fraud model to adapt to evolving fraud patterns (fraudsters change tactics to evade detection)\",\n",
    "            \"triggers\": [\n",
    "                \"Schedule: Daily retrain (fraud evolves rapidly)\",\n",
    "                \"Performance: Precision drops below 90% (too many false positives = customer friction)\",\n",
    "                \"Data drift: Transaction amount distribution changes, new merchant categories\",\n",
    "                \"Manual: After fraud ring discovered (update labels for past transactions)\"\n",
    "            ],\n",
    "            \"validation_gates\": [\n",
    "                \"Precision >= 92% (minimize false positives)\",\n",
    "                \"Recall >= 75% (catch majority of fraud)\",\n",
    "                \"Better than production by >= 1% F1 score\",\n",
    "                \"Fairness: False positive rate consistent across demographics\",\n",
    "                \"Latency: Inference < 100ms (real-time transaction approval)\"\n",
    "            ],\n",
    "            \"features\": [\n",
    "                \"Transaction: amount, merchant_category, time, location\",\n",
    "                \"User behavior: avg_transaction_amount, transaction_frequency, device_fingerprint\",\n",
    "                \"Network: merchant_fraud_rate (last 30 days), peer_group_behavior\",\n",
    "                \"Temporal: time_since_last_transaction, velocity (transactions/hour)\"\n",
    "            ],\n",
    "            \"orchestration\": \"Airflow DAG (daily) with A/B testing for 24 hours before full deployment\",\n",
    "            \"success_criteria\": \"92%+ precision and 75%+ recall for 3 months, <6 hour retrain latency, <$50K false positive cost/month\",\n",
    "            \"value\": \"Prevent $20M/year fraud losses, reduce false positive friction (save 10K customer complaints/month)\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Churn Prediction CT with Feature Drift Monitoring\",\n",
    "            \"objective\": \"Continuous training for customer churn prediction with proactive feature drift detection\",\n",
    "            \"triggers\": [\n",
    "                \"Schedule: Biweekly retrain (churn patterns shift gradually)\",\n",
    "                \"Data drift: User engagement distribution changes (sessions, purchases)\",\n",
    "                \"Performance: AUC-ROC drops below 0.82\",\n",
    "                \"Feature drift: >20% of features show drift (KS test p < 0.05)\"\n",
    "            ],\n",
    "            \"validation_gates\": [\n",
    "                \"AUC-ROC >= 0.85 on holdout users (last 60 days)\",\n",
    "                \"Top 10% predicted churners have >= 50% actual churn rate (targeting efficiency)\",\n",
    "                \"Better than production by >= 0.02 AUC-ROC\",\n",
    "                \"Calibration: Predicted probabilities match observed frequencies (reliability diagram)\"\n",
    "            ],\n",
    "            \"features\": [\n",
    "                \"User activity: login_frequency, session_duration, feature_usage\",\n",
    "                \"Transactions: purchase_frequency, avg_order_value, days_since_last_purchase\",\n",
    "                \"Engagement: support_tickets, app_ratings, email_open_rate\",\n",
    "                \"Cohort: tenure, acquisition_channel, subscription_tier\"\n",
    "            ],\n",
    "            \"orchestration\": \"Airflow DAG (biweekly) with feature drift dashboard (Evidently AI)\",\n",
    "            \"success_criteria\": \"Maintain 0.85+ AUC-ROC for 6 months, top 10% churners have 50%+ actual churn, <3 day retrain latency\",\n",
    "            \"value\": \"Reduce churn 15% (save $3M/year), improve retention campaign ROI 2x\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Demand Forecasting CT with External Signal Integration\",\n",
    "            \"objective\": \"Retrain demand forecast model with external signals (weather, events, economic indicators)\",\n",
    "            \"triggers\": [\n",
    "                \"Schedule: Weekly retrain (capture weekly seasonality)\",\n",
    "                \"Performance: MAPE increases from 8% to >12%\",\n",
    "                \"Data drift: Sales distribution shifts (new product launch, competitor action)\",\n",
    "                \"External event: Major holiday, weather event (hurricane), economic shock\"\n",
    "            ],\n",
    "            \"validation_gates\": [\n",
    "                \"MAPE <= 10% on next 4 weeks forecast\",\n",
    "                \"Bias < 5% (prevent systematic over/under-forecasting)\",\n",
    "                \"Better than production by >= 1% MAPE\",\n",
    "                \"Coverage: 80% prediction interval captures actual demand 80% of time\"\n",
    "            ],\n",
    "            \"features\": [\n",
    "                \"Time series: sales_lag_1w, sales_lag_4w, sales_lag_52w (year-over-year)\",\n",
    "                \"Trend: linear_trend, seasonal_decomposition (STL)\",\n",
    "                \"External: weather_forecast, local_events, holiday_indicator\",\n",
    "                \"Product: promotions, price_changes, inventory_level\"\n",
    "            ],\n",
    "            \"orchestration\": \"Airflow DAG (weekly) with external API calls (weather, events data)\",\n",
    "            \"success_criteria\": \"Maintain 10% MAPE for 6 months, <1 day forecast latency, 80% prediction interval coverage\",\n",
    "            \"value\": \"Optimize inventory $2M/year (reduce stockouts 30%, overstock 25%)\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üéØ 8 Continuous Training Project Templates\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüì¶ POST-SILICON VALIDATION PROJECTS (4)\\n\")\n",
    "for i, project in enumerate(projects[\"post_silicon\"], 1):\n",
    "    print(f\"{i}. {project['name']}\")\n",
    "    print(f\"   Objective: {project['objective']}\")\n",
    "    print(f\"   Triggers: {len(project['triggers'])} types ({', '.join([t.split(':')[0] for t in project['triggers']])})\")\n",
    "    print(f\"   Validation Gates: {len(project['validation_gates'])} checks\")\n",
    "    print(f\"   Orchestration: {project['orchestration']}\")\n",
    "    print(f\"   Success: {project['success_criteria']}\")\n",
    "    print(f\"   üí∞ Value: {project['value']}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nüåê GENERAL AI/ML PROJECTS (4)\\n\")\n",
    "for i, project in enumerate(projects[\"general_ml\"], 1):\n",
    "    print(f\"{i}. {project['name']}\")\n",
    "    print(f\"   Objective: {project['objective']}\")\n",
    "    print(f\"   Triggers: {len(project['triggers'])} types ({', '.join([t.split(':')[0] for t in project['triggers']])})\")\n",
    "    print(f\"   Validation Gates: {len(project['validation_gates'])} checks\")\n",
    "    print(f\"   Orchestration: {project['orchestration']}\")\n",
    "    print(f\"   Success: {project['success_criteria']}\")\n",
    "    print(f\"   üí∞ Value: {project['value']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ All projects include: Multi-trigger logic, validation gates, orchestration, success metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371563d9",
   "metadata": {},
   "source": [
    "## 7. üéì Key Takeaways & Best Practices\n",
    "\n",
    "### üìå Core Concepts\n",
    "\n",
    "**1. Continuous Training Fundamentals**\n",
    "- **Definition**: Automated model retraining when performance degrades, data drifts, or schedule triggers\n",
    "- **Purpose**: Maintain model accuracy in production as data distributions change\n",
    "- **Components**: Triggers (when to retrain), pipeline (how to retrain), gates (when to deploy), monitoring (track performance)\n",
    "- **CT vs CI/CD**: CT focuses on model updates (data-driven), CI/CD on code updates (developer-driven) - both needed for MLOps\n",
    "\n",
    "**2. Why Models Degrade Over Time**\n",
    "- **Data drift**: Feature distributions change (e.g., Vdd voltage shifted from 1.2V to 1.28V due to process change)\n",
    "- **Concept drift**: Relationship between features and target changes (e.g., yield formula changes with new test coverage)\n",
    "- **Upstream changes**: Data pipeline modifications, sensor calibration, measurement errors\n",
    "- **Population shift**: Production data differs from training data (sampling bias, temporal effects)\n",
    "\n",
    "**Without CT**: Models become stale (accuracy drops from 92% to 78% over 6 months)  \n",
    "**With CT**: Models stay fresh (maintain 90%+ accuracy indefinitely)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Trigger Mechanisms\n",
    "\n",
    "**3. Schedule-Based Triggers**\n",
    "- **How it works**: Retrain at fixed intervals (daily, weekly, monthly)\n",
    "- **Pros**: Simple, predictable, ensures models always incorporate latest data\n",
    "- **Cons**: May retrain unnecessarily (waste compute), may miss urgent degradation between intervals\n",
    "- **When to use**: Baseline strategy for all models, sufficient when drift is gradual\n",
    "\n",
    "**Example cron schedules**:\n",
    "- Daily: `0 2 * * *` (2 AM every day - low traffic time)\n",
    "- Weekly: `0 2 * * 0` (Sunday 2 AM)\n",
    "- Monthly: `0 2 1 * *` (1st of month 2 AM)\n",
    "\n",
    "**4. Data Drift Triggers**\n",
    "- **How it works**: Statistical tests detect distribution changes (KS test, Chi-square, PSI)\n",
    "- **Kolmogorov-Smirnov (KS) test**: Compare training vs production distributions (p-value < 0.05 ‚Üí drift)\n",
    "- **Population Stability Index (PSI)**: Measure distribution shift (PSI > 0.2 = significant drift)\n",
    "- **Per-feature drift**: Test each feature independently, trigger if >=20% features drift\n",
    "- **Pros**: Data-driven (retrain only when needed), catch drift before performance degrades\n",
    "- **Cons**: Requires production data logging, threshold tuning (too sensitive = false alarms)\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "for feature in feature_cols:\n",
    "    statistic, p_value = ks_2samp(train_data[feature], prod_data[feature])\n",
    "    if p_value < 0.05:  # Reject null hypothesis (distributions differ)\n",
    "        trigger_retrain(reason=f\"Drift in {feature}\")\n",
    "```\n",
    "\n",
    "**5. Performance-Based Triggers**\n",
    "- **How it works**: Monitor production metrics (accuracy, F1, MAPE), retrain if below threshold\n",
    "- **Requires**: Ground truth labels in production (e.g., yield labels after test completion)\n",
    "- **Metrics to track**: Accuracy, precision, recall, F1, AUC-ROC, MAPE, RMSE (depends on problem)\n",
    "- **Threshold setting**: Historical baseline - 5% (e.g., if baseline 90%, trigger at 85%)\n",
    "- **Pros**: Directly tied to business impact, no false alarms (only retrain when truly needed)\n",
    "- **Cons**: Reactive (waits for degradation), requires labeled production data (not always available)\n",
    "\n",
    "**Rolling window validation**: Evaluate on last 7 days of production data to detect gradual degradation\n",
    "\n",
    "**6. Hybrid Trigger Strategies**\n",
    "- **Combine multiple triggers**: Schedule AND (drift OR performance) - ensures regular updates + urgency\n",
    "- **Priority levels**: Performance drop (urgent, retrain immediately) > Data drift (important, retrain next cycle) > Schedule (routine)\n",
    "- **Override mechanisms**: Manual trigger for emergencies (spec change, major bug fix)\n",
    "\n",
    "**Example logic**:\n",
    "```python\n",
    "should_retrain = (\n",
    "    schedule_trigger.triggered() OR\n",
    "    (drift_trigger.triggered() AND performance_trigger.degraded()) OR\n",
    "    manual_override\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Retraining Pipeline Architecture\n",
    "\n",
    "**7. Pipeline Stages**\n",
    "1. **Data fetching**: Pull latest data from production (last 30-90 days typically)\n",
    "2. **Feature engineering**: Apply same transformations as training (use feature store for consistency)\n",
    "3. **Model training**: Train with same hyperparameters OR run hyperparameter tuning\n",
    "4. **Validation**: Check quality on holdout set (last 20% of data or time-based split)\n",
    "5. **Deployment**: If passed gates, replace production model (with rollback capability)\n",
    "6. **Monitoring**: Track new model performance in production (A/B test, shadow mode)\n",
    "\n",
    "**8. Validation Gates (Quality Checks)**\n",
    "- **Absolute threshold**: Accuracy >= 85% (minimum acceptable performance)\n",
    "- **Relative threshold**: New model >= production model + 1% (ensure improvement)\n",
    "- **Business rules**: No predictions of >15% yield loss (unrealistic, likely bug)\n",
    "- **Fairness checks**: Accuracy variance across demographics/fabs < 5%\n",
    "- **Latency constraints**: Inference time < 100ms (real-time applications)\n",
    "- **All gates must pass**: If any gate fails, reject model (keep production model)\n",
    "\n",
    "**Example validation**:\n",
    "```python\n",
    "def validate_model(new_model, prod_model, X_val, y_val):\n",
    "    new_acc = accuracy_score(y_val, new_model.predict(X_val))\n",
    "    prod_acc = accuracy_score(y_val, prod_model.predict(X_val))\n",
    "    \n",
    "    gates = {\n",
    "        'absolute': new_acc >= 0.85,\n",
    "        'relative': new_acc >= prod_acc + 0.01,\n",
    "        'f1': f1_score(y_val, new_model.predict(X_val)) >= 0.75\n",
    "    }\n",
    "    \n",
    "    return all(gates.values())\n",
    "```\n",
    "\n",
    "**9. Model Versioning & Rollback**\n",
    "- **Version tracking**: Store model with metadata (timestamp, trigger reason, metrics, Git commit)\n",
    "- **Registry**: Use MLflow, W&B, or custom registry to track all versions\n",
    "- **Rollback**: If new model fails in production (detected by monitoring), revert to previous version\n",
    "- **Retention**: Keep last 5-10 versions for quick rollback, archive older versions\n",
    "\n",
    "**Metadata example**:\n",
    "```python\n",
    "{\n",
    "    'model_id': 'yield_pred_v23',\n",
    "    'timestamp': '2024-01-15T02:30:00Z',\n",
    "    'trigger': 'Data drift in Vdd + scheduled retrain',\n",
    "    'accuracy': 0.923,\n",
    "    'f1': 0.897,\n",
    "    'training_data': 'STDF 2023-12-15 to 2024-01-15',\n",
    "    'git_commit': 'a3f7b2c',\n",
    "    'deployed': True\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Orchestration with Airflow\n",
    "\n",
    "**10. Airflow DAG Basics**\n",
    "- **DAG (Directed Acyclic Graph)**: Defines task dependencies (A ‚Üí B ‚Üí C)\n",
    "- **Tasks**: Individual operations (fetch_data, train_model, validate, deploy)\n",
    "- **Operators**: PythonOperator (run Python function), BashOperator (run shell command), custom operators\n",
    "- **Scheduling**: Cron expressions (`0 2 * * 0` = weekly), or dynamic triggers\n",
    "- **XCom**: Inter-task communication (pass data between tasks)\n",
    "\n",
    "**11. CT-Specific DAG Design**\n",
    "- **Branching**: If trigger not fired, skip retraining tasks (save compute)\n",
    "- **Retries**: Automatic retry on transient failures (network issues, database timeout)\n",
    "- **Alerting**: Send notification on success/failure (Slack, email, PagerDuty)\n",
    "- **Dependencies**: Ensure tasks run in order (can't deploy before validation)\n",
    "\n",
    "**DAG structure for CT**:\n",
    "```\n",
    "check_triggers ‚Üí fetch_data ‚Üí engineer_features ‚Üí train_model\n",
    "                                                       ‚Üì\n",
    "                                            validate_model\n",
    "                                                       ‚Üì\n",
    "                                            deploy_to_production\n",
    "                                                       ‚Üì\n",
    "                                            send_notification\n",
    "```\n",
    "\n",
    "**12. Alternative Orchestration Tools**\n",
    "- **Kubeflow**: Kubernetes-native ML pipelines (better for complex workflows, GPU training)\n",
    "- **Prefect**: Modern Python-first orchestrator (easier than Airflow, better developer experience)\n",
    "- **AWS Step Functions**: Serverless orchestration (AWS-specific)\n",
    "- **Argo Workflows**: Kubernetes-native (popular for MLOps on K8s)\n",
    "- **Metaflow**: Netflix's framework (great for data science workflows)\n",
    "\n",
    "**Airflow vs Kubeflow**:\n",
    "- Airflow: General-purpose, mature, great for batch workflows\n",
    "- Kubeflow: ML-specific, native Kubernetes, better for distributed training\n",
    "\n",
    "---\n",
    "\n",
    "### üè≠ Post-Silicon Validation Applications\n",
    "\n",
    "**13. Yield Prediction CT**\n",
    "- **Challenge**: Process changes (new litho tool, voltage adjustments) cause data drift\n",
    "- **Solution**: Weekly scheduled retrain + KS test drift detection on Vdd, Idd, frequency\n",
    "- **Validation**: Accuracy >= 90%, fairness across fabs (variance < 5%)\n",
    "- **Value**: Maintain yield prediction accuracy ‚Üí prevent $2M/year scrap losses\n",
    "\n",
    "**14. Test Time Prediction CT**\n",
    "- **Challenge**: Test programs change frequently (new tests, sequence updates)\n",
    "- **Solution**: Event-driven trigger (Git commit to test program) + nightly retrain\n",
    "- **Validation**: MAPE <= 4%, prediction time < 50ms\n",
    "- **Value**: Accurate capacity planning ‚Üí prevent 20% test floor underutilization ($500K/year)\n",
    "\n",
    "**15. Binning Model CT**\n",
    "- **Challenge**: Product specs change (voltage limits, frequency targets)\n",
    "- **Solution**: Manual trigger when specs updated + monthly validation\n",
    "- **Validation**: Bin accuracy >= 97%, zero false positives for premium bins\n",
    "- **Value**: Revenue optimization ($50M/year), prevent warranty claims\n",
    "\n",
    "**16. Wafer Map Anomaly Detection CT**\n",
    "- **Challenge**: New defect patterns emerge over time (need model to learn them)\n",
    "- **Solution**: Monthly retrain + spatial drift detection (Moran's I statistic)\n",
    "- **Validation**: Precision >= 85%, recall >= 75%, false positive rate < 5%\n",
    "- **Value**: Detect defects within 30 days ‚Üí accelerate root cause analysis ($1M/year savings)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "**17. Retraining Too Frequently**\n",
    "- **Problem**: Daily retraining when drift is slow ‚Üí waste compute, increase complexity\n",
    "- **Solution**: Start with weekly/monthly schedule, add drift triggers only if needed\n",
    "- **Cost**: Daily retraining can cost $1000+/month in cloud compute (vs $100/month weekly)\n",
    "\n",
    "**18. Insufficient Validation**\n",
    "- **Problem**: Deploy new model without proper testing ‚Üí production failures\n",
    "- **Solution**: Multiple validation gates (accuracy, fairness, business rules), A/B testing before full deployment\n",
    "- **Example failure**: New model predicts 50% yield loss (bug) ‚Üí deploys ‚Üí fab halts production\n",
    "\n",
    "**19. Ignoring Data Quality**\n",
    "- **Problem**: Retrain on corrupted data (sensor failure, pipeline bug) ‚Üí model worse than before\n",
    "- **Solution**: Data quality checks before training (missing values, outliers, schema validation)\n",
    "- **Example**: Vdd sensor malfunction reports 0V ‚Üí model learns incorrect pattern\n",
    "\n",
    "**20. No Rollback Plan**\n",
    "- **Problem**: New model degrades in production, no way to revert quickly\n",
    "- **Solution**: Keep last 3-5 model versions, automated rollback on performance drop\n",
    "- **Detection**: Monitor rolling 24-hour accuracy, rollback if drops >3%\n",
    "\n",
    "**21. Training-Serving Skew**\n",
    "- **Problem**: Features computed differently in training vs production ‚Üí model fails\n",
    "- **Solution**: Use feature store (consistent feature definitions), validate feature distributions\n",
    "- **Example**: Training uses offline batch features, production uses real-time features ‚Üí different values\n",
    "\n",
    "**22. Overfitting to Recent Data**\n",
    "- **Problem**: Retrain on last 7 days only ‚Üí model forgets long-term patterns\n",
    "- **Solution**: Use last 30-90 days of data, balance recent (high weight) + historical (low weight)\n",
    "- **Weighting**: Recent data 2x weight, older data 1x weight (time-based importance)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Best Practices\n",
    "\n",
    "**23. Start Simple, Iterate**\n",
    "- **Phase 1**: Manual retraining (understand drift patterns)\n",
    "- **Phase 2**: Scheduled retraining (weekly/monthly)\n",
    "- **Phase 3**: Add drift detection triggers\n",
    "- **Phase 4**: Add performance triggers + A/B testing\n",
    "- **Don't**: Build complex multi-trigger system on day 1\n",
    "\n",
    "**24. Monitor Everything**\n",
    "- **Training metrics**: Accuracy, F1, training time, data volume\n",
    "- **Deployment metrics**: Models deployed, rejected, rolled back\n",
    "- **Production metrics**: Inference latency, throughput, error rate\n",
    "- **Business metrics**: Revenue impact, cost savings (tie CT to ROI)\n",
    "\n",
    "**25. Automate Validation Reports**\n",
    "- **Generate**: Comparison report (new model vs production model)\n",
    "- **Include**: Metrics, feature importances, prediction distributions, validation gate results\n",
    "- **Share**: Email to team after each retrain (transparency)\n",
    "\n",
    "**26. Handle Edge Cases**\n",
    "- **Insufficient data**: If <1000 samples in retrain window, skip retrain (wait for more data)\n",
    "- **Training failures**: Retry 2x, if still fails, alert team (don't deploy broken model)\n",
    "- **Validation failures**: Log reason (which gate failed), investigate (data quality? threshold too strict?)\n",
    "\n",
    "**27. Document Trigger Decisions**\n",
    "- **Log**: Every trigger check (timestamp, trigger type, decision, reason)\n",
    "- **Dashboard**: Show trigger history (when did we retrain? why?)\n",
    "- **Analyze**: Review trigger patterns monthly (are we retraining too much? too little?)\n",
    "\n",
    "**28. Use A/B Testing Before Full Deployment**\n",
    "- **Shadow mode**: Run new model in parallel with production (log predictions, don't serve)\n",
    "- **A/B test**: Serve new model to 10% of traffic for 24 hours\n",
    "- **Full deployment**: If A/B test passes, deploy to 100%\n",
    "- **Safety**: Limits blast radius if new model has issues\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Production Checklist\n",
    "\n",
    "**Before deploying CT system**:\n",
    "- [ ] Trigger logic implemented and tested (schedule, drift, performance)\n",
    "- [ ] Validation gates defined with business stakeholders (thresholds agreed)\n",
    "- [ ] Orchestration pipeline tested end-to-end (Airflow DAG runs successfully)\n",
    "- [ ] Model versioning and rollback tested (can revert to previous version)\n",
    "- [ ] Monitoring dashboards created (trigger checks, retraining status, model metrics)\n",
    "- [ ] Alerting configured (Slack/email on failures)\n",
    "- [ ] Data quality checks added (schema validation, outlier detection)\n",
    "- [ ] Feature store integration (consistent features in training and production)\n",
    "- [ ] A/B testing infrastructure ready (traffic splitting, metric tracking)\n",
    "- [ ] Documentation written (runbook for failures, trigger tuning guide)\n",
    "- [ ] Team training complete (know how to debug, override, rollback)\n",
    "- [ ] Cost estimated (compute budget for retraining)\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ When to Use Continuous Training\n",
    "\n",
    "**‚úÖ Use CT when**:\n",
    "- Data distributions change over time (seasonality, trends, process changes)\n",
    "- Model performance degrades in production (drift, concept shift)\n",
    "- New data arrives regularly (daily/weekly production data)\n",
    "- Business requires up-to-date models (fraud, recommendations, demand forecasting)\n",
    "- Cost of stale model is high (lost revenue, safety risk)\n",
    "\n",
    "**‚ùå Don't use CT when**:\n",
    "- Data is static (no new data after initial training)\n",
    "- Model performance is stable (no degradation over 6+ months)\n",
    "- Retraining cost > benefit (expensive training, minimal accuracy gain)\n",
    "- Business doesn't require freshness (historical analysis, one-time prediction)\n",
    "\n",
    "**Alternatives**:\n",
    "- **Manual retraining**: Retrain when performance drops (alerts trigger manual investigation)\n",
    "- **Online learning**: Update model incrementally with each new sample (no batch retraining)\n",
    "- **Ensemble with new models**: Keep old model, add new model, ensemble predictions\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "**After mastering continuous training**:\n",
    "1. **Model Governance (Notebook 127)**: Model cards, audit trails, compliance for regulated industries\n",
    "2. **Production Monitoring (Notebook 128)**: Real-time monitoring, alerting, incident response for deployed models\n",
    "3. **CI/CD for ML (Notebook 129)**: Automate code + model deployment with GitHub Actions, Jenkins\n",
    "4. **Advanced MLOps (Notebook 130)**: Multi-model pipelines, model ensembles, AutoML integration\n",
    "\n",
    "**Recommended resources**:\n",
    "- Book: \"Machine Learning Design Patterns\" (Lakshmanan et al.) - Chapter on continuous training\n",
    "- Paper: \"The ML Test Score\" (Google) - Validation framework for production ML systems\n",
    "- Course: \"Full Stack Deep Learning\" - Production ML best practices\n",
    "- Tool docs: Airflow documentation, Kubeflow pipelines, MLflow model registry\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Remember**: Continuous training is not optional for production ML - it's the difference between models that stay accurate (save millions) and models that degrade silently (cost millions). Start simple, monitor everything, iterate based on production feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd4a4e",
   "metadata": {},
   "source": [
    "## üîë Key Takeaways\n",
    "\n",
    "**When to Use Continuous Training:**\n",
    "- Model degrades over time (concept drift, data shift)\n",
    "- Frequent new data available (daily/weekly batches)\n",
    "- Business requires up-to-date predictions\n",
    "- Manual retraining too slow or error-prone\n",
    "\n",
    "**Limitations:**\n",
    "- Infrastructure complexity (orchestration, monitoring, rollback)\n",
    "- Training costs accumulate (compute, storage)\n",
    "- Risk of catastrophic forgetting (new data replaces old patterns)\n",
    "- Regulatory challenges (model versioning, auditability)\n",
    "\n",
    "**Alternatives:**\n",
    "- Periodic batch retraining (weekly/monthly schedule)\n",
    "- Online learning (real-time updates per sample)\n",
    "- Trigger-based retraining (drift threshold exceeded)\n",
    "- Ensemble with new + old models\n",
    "\n",
    "**Best Practices:**\n",
    "- Monitor drift metrics continuously (PSI, KL divergence)\n",
    "- Implement automated rollback on quality degradation\n",
    "- Version all artifacts (data, code, models, configs)\n",
    "- Use canary/shadow deployments for validation\n",
    "- Document retraining triggers and thresholds\n",
    "\n",
    "**Next Steps:**\n",
    "- 127: Model Governance & Compliance (audit continuous training)\n",
    "- 154: Model Monitoring & Observability (drift detection)\n",
    "- 156: ML Pipeline Orchestration (advanced workflows)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
