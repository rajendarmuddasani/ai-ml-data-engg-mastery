{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a37af406",
   "metadata": {},
   "source": [
    "# 122: MLflow Complete Guide\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Master** MLflow Tracking: Log parameters, metrics, artifacts systematically\n",
    "- **Build** MLflow Projects: Reproducible ML workflows with packaging\n",
    "- **Deploy** MLflow Models: Serve models via REST API, batch, cloud\n",
    "- **Manage** MLflow Registry: Version control, stage transitions, model governance\n",
    "- **Integrate** MLflow with production: CI/CD, monitoring, scaling strategies\n",
    "- **Apply** advanced patterns: Nested runs, autologging, custom metrics\n",
    "\n",
    "## üìö What is MLflow?\n",
    "\n",
    "**MLflow** is an open-source platform for managing the complete machine learning lifecycle, including experimentation, reproducibility, deployment, and central model registry.\n",
    "\n",
    "**Four Core Components:**\n",
    "- ‚úÖ **MLflow Tracking**: Record and query experiments (parameters, metrics, artifacts)\n",
    "- ‚úÖ **MLflow Projects**: Package ML code in reproducible format (conda, Docker)\n",
    "- ‚úÖ **MLflow Models**: Deploy models to diverse serving environments (REST, batch, Spark)\n",
    "- ‚úÖ **MLflow Registry**: Central model store with versioning and stage management\n",
    "\n",
    "**Why MLflow?**\n",
    "- **Open-source**: No vendor lock-in, active community (20K+ GitHub stars)\n",
    "- **Framework-agnostic**: Works with sklearn, TensorFlow, PyTorch, XGBoost, LightGBM\n",
    "- **Production-ready**: Used by Uber, Databricks, Microsoft, Netflix\n",
    "- **Simple API**: 3 lines to start tracking: `mlflow.start_run()`, `mlflow.log_metric()`, `mlflow.end_run()`\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Use Case 1: Multi-Algorithm Yield Prediction Comparison**\n",
    "- **Input**: STDF parametric data (Vdd, Idd, freq, temp) for 50,000 devices\n",
    "- **Models**: Random Forest, XGBoost, Gradient Boosting, Neural Network\n",
    "- **Tracking**: Log all hyperparameters, cross-validation scores, training time\n",
    "- **Output**: Best model (XGBoost, 93.5% accuracy) promoted to production\n",
    "- **Value**: Systematic comparison eliminates guesswork, 8% accuracy improvement\n",
    "\n",
    "**Use Case 2: Test Time Optimization Model Registry**\n",
    "- **Scenario**: 5 model versions for test time reduction (v1.0: 10% ‚Üí v2.5: 28% reduction)\n",
    "- **Registry**: Track each version with metadata (data range, accuracy, test time savings)\n",
    "- **Deployment**: Stage v2.5 on 10% of ATE, monitor false negative rate\n",
    "- **Promotion**: If FNR < 0.5%, promote to Production, archive v2.0\n",
    "- **Value**: $800K annual savings, full audit trail for quality compliance\n",
    "\n",
    "**Use Case 3: Wafer Map Anomaly Detection Experiments**\n",
    "- **Challenge**: Tested 47 different feature engineering strategies over 3 months\n",
    "- **Tracking**: Log spatial features, PCA components, contamination parameters, F1 scores\n",
    "- **Best**: Spatial autocorrelation + PCA(20) ‚Üí F1 = 0.89\n",
    "- **Artifacts**: Save wafer map visualizations, feature importance plots\n",
    "- **Value**: Without MLflow, would have lost track of experiments, wasted 50+ hours recreating results\n",
    "\n",
    "**Use Case 4: Device Binning Model Deployment**\n",
    "- **Models**: 3 binning strategies (performance-based, power-based, hybrid)\n",
    "- **Projects**: Package each strategy as MLflow Project with dependencies\n",
    "- **Deployment**: Serve via REST API (<30ms latency) to binning automation system\n",
    "- **Monitoring**: Track bin distribution drift, alert if Premium bin % drops >5%\n",
    "- **Value**: 98.5% binning accuracy, automated deployment eliminates manual errors\n",
    "\n",
    "## üîÑ MLflow Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Data Preparation] --> B[Experiment Tracking]\n",
    "    B --> C[Log Parameters]\n",
    "    B --> D[Log Metrics]\n",
    "    B --> E[Log Artifacts]\n",
    "    C --> F[Compare Experiments]\n",
    "    D --> F\n",
    "    E --> F\n",
    "    F --> G{Best Model?}\n",
    "    G -->|Yes| H[Register Model]\n",
    "    G -->|No| B\n",
    "    H --> I[Staging]\n",
    "    I --> J[Validation Tests]\n",
    "    J --> K{Pass?}\n",
    "    K -->|Yes| L[Production]\n",
    "    K -->|No| I\n",
    "    L --> M[Serve Predictions]\n",
    "    M --> N[Monitor Performance]\n",
    "    N --> O{Drift Detected?}\n",
    "    O -->|Yes| A\n",
    "    O -->|No| M\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style L fill:#e1ffe1\n",
    "    style O fill:#fff4e1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **121_MLOps_Fundamentals.ipynb** - MLOps lifecycle, deployment patterns\n",
    "- **010_Linear_Regression.ipynb** - ML model basics\n",
    "- **041_Model_Evaluation_Metrics.ipynb** - Evaluation techniques\n",
    "\n",
    "**Next Steps:**\n",
    "- **123_Model_Monitoring_Drift_Detection.ipynb** - Monitor deployed models\n",
    "- **124_Feature_Store_Implementation.ipynb** - Centralized feature management\n",
    "- **125_ML_Pipeline_Orchestration.ipynb** - Airflow, Kubeflow integration\n",
    "\n",
    "---\n",
    "\n",
    "Let's master MLflow for production ML! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3305ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MLflow and dependencies\n",
    "# !pip install mlflow scikit-learn pandas numpy matplotlib seaborn xgboost\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(\"MLflow tracking URI:\", mlflow.get_tracking_uri())\n",
    "print(\"Start MLflow UI: mlflow ui --port 5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b71f8",
   "metadata": {},
   "source": [
    "## 2. MLflow Tracking: Complete Guide\n",
    "\n",
    "**MLflow Tracking** is a logging API for recording:\n",
    "- **Parameters**: Hyperparameters, config values (immutable)\n",
    "- **Metrics**: Performance scores, loss values (can update over iterations)\n",
    "- **Artifacts**: Files (models, plots, data, any file)\n",
    "- **Tags**: Metadata (model type, data version, environment)\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Experiment**: Collection of runs (e.g., \"Yield Prediction Experiments\")\n",
    "- **Run**: Single execution of ML code (e.g., \"RandomForest_run_42\")\n",
    "- **Tracking URI**: Where data is stored (local file, database, remote server)\n",
    "\n",
    "**Post-Silicon Example**: Compare 5 algorithms for yield prediction, track all experiments in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd587a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Basic tracking example\n",
    "# Generate synthetic STDF data\n",
    "np.random.seed(42)\n",
    "n_devices = 5000\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Vdd_V': np.random.normal(1.2, 0.05, n_devices),\n",
    "    'Idd_mA': np.random.normal(50, 5, n_devices),\n",
    "    'freq_MHz': np.random.normal(1000, 50, n_devices),\n",
    "    'temp_C': np.random.normal(25, 5, n_devices)\n",
    "})\n",
    "\n",
    "# Create yield target\n",
    "data['yield'] = (\n",
    "    (data['Vdd_V'] >= 1.15) & (data['Vdd_V'] <= 1.25) &\n",
    "    (data['Idd_mA'] <= 55) &\n",
    "    (data['freq_MHz'] >= 950) &\n",
    "    (data['temp_C'] <= 30)\n",
    ").astype(int)\n",
    "\n",
    "X = data.drop('yield', axis=1)\n",
    "y = data['yield']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Dataset: {len(data)} devices, {y.sum()} passing ({y.mean()*100:.1f}%)\")\n",
    "print(f\"Training: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a4f30b",
   "metadata": {},
   "source": [
    "### A. Basic Tracking API\n",
    "\n",
    "**Core tracking functions:**\n",
    "- `mlflow.set_experiment(name)` - Create or set active experiment\n",
    "- `mlflow.start_run(run_name)` - Start new run (returns context manager)\n",
    "- `mlflow.log_param(key, value)` - Log single parameter\n",
    "- `mlflow.log_params(dict)` - Log multiple parameters\n",
    "- `mlflow.log_metric(key, value, step)` - Log metric (can log multiple times with different steps)\n",
    "- `mlflow.log_artifact(path)` - Log file artifact\n",
    "- `mlflow.end_run()` - End current run (automatic with context manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbf437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive tracking example\n",
    "mlflow.set_experiment(\"Yield_Prediction_Complete\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"RandomForest_Comprehensive\") as run:\n",
    "    # 1. Log parameters (hyperparameters, config)\n",
    "    params = {\n",
    "        'model_type': 'RandomForest',\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'min_samples_split': 5,\n",
    "        'random_state': 42,\n",
    "        'data_size': len(X_train)\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    # 2. Log tags (metadata)\n",
    "    mlflow.set_tags({\n",
    "        'data_source': 'STDF_synthetic',\n",
    "        'environment': 'development',\n",
    "        'ml_engineer': 'data_science_team',\n",
    "        'use_case': 'yield_prediction'\n",
    "    })\n",
    "    \n",
    "    # 3. Train model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        max_depth=params['max_depth'],\n",
    "        min_samples_split=params['min_samples_split'],\n",
    "        random_state=params['random_state']\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 4. Log metrics\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # 5. Log artifacts (plots)\n",
    "    # Feature importance plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    sns.barplot(data=importance, x='importance', y='feature', ax=ax)\n",
    "    ax.set_title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png')\n",
    "    mlflow.log_artifact('feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    mlflow.log_artifact('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Log model with signature\n",
    "    signature = infer_signature(X_train, model.predict(X_train))\n",
    "    mlflow.sklearn.log_model(model, \"model\", signature=signature)\n",
    "    \n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "    print(f\"Metrics: Accuracy={metrics['accuracy']:.4f}, F1={metrics['f1_score']:.4f}, AUC={metrics['roc_auc']:.4f}\")\n",
    "    print(f\"View in UI: http://127.0.0.1:5000/#/experiments/{run.info.experiment_id}/runs/{run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450b944",
   "metadata": {},
   "source": [
    "### B. Autologging (Automatic Tracking)\n",
    "\n",
    "**Autologging** automatically logs parameters, metrics, and models for supported frameworks.\n",
    "\n",
    "**Supported frameworks:**\n",
    "- `mlflow.sklearn.autolog()` - scikit-learn\n",
    "- `mlflow.tensorflow.autolog()` - TensorFlow/Keras\n",
    "- `mlflow.pytorch.autolog()` - PyTorch\n",
    "- `mlflow.xgboost.autolog()` - XGBoost\n",
    "- `mlflow.lightgbm.autolog()` - LightGBM\n",
    "\n",
    "**What gets logged automatically:**\n",
    "- All hyperparameters\n",
    "- Training/validation metrics\n",
    "- Model artifact\n",
    "- Model signature\n",
    "- Training dataset info\n",
    "\n",
    "**Trade-off**: Less control, but 90% faster setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0df755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autologging example\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "with mlflow.start_run(run_name=\"GradientBoosting_Autolog\"):\n",
    "    # Just train the model - MLflow logs everything automatically!\n",
    "    gb_model = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # MLflow automatically logged:\n",
    "    # - All parameters (n_estimators, learning_rate, etc.)\n",
    "    # - Training score\n",
    "    # - Model artifact\n",
    "    # - Model signature\n",
    "    \n",
    "    y_pred = gb_model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Manual log for test accuracy (not auto-logged)\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "    \n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"Check MLflow UI - all parameters and model logged automatically!\")\n",
    "\n",
    "# Turn off autologging\n",
    "mlflow.sklearn.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20a4839",
   "metadata": {},
   "source": [
    "### C. Comparing Multiple Models\n",
    "\n",
    "**Real-world scenario**: Test 5 algorithms, pick the best one.\n",
    "\n",
    "**Strategy**: Run all experiments in same MLflow Experiment, compare in UI or programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa537af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'DecisionTree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, max_depth=5, random_state=42, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "mlflow.set_experiment(\"Model_Comparison_Yield\")\n",
    "\n",
    "results = []\n",
    "for model_name, model in models.items():\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_comparison\"):\n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
    "        \n",
    "        # Metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba) if hasattr(model, 'predict_proba') else acc\n",
    "        \n",
    "        # Log\n",
    "        mlflow.log_params({'model_type': model_name, 'data_size': len(X_train)})\n",
    "        mlflow.log_metrics({'accuracy': acc, 'f1_score': f1, 'auc': auc})\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        results.append({'Model': model_name, 'Accuracy': acc, 'F1': f1, 'AUC': auc})\n",
    "        print(f\"{model_name}: Acc={acc:.4f}, F1={f1:.4f}, AUC={auc:.4f}\")\n",
    "\n",
    "# Compare results\n",
    "results_df = pd.DataFrame(results).sort_values('F1', ascending=False)\n",
    "print(\"\\n=== Model Ranking (by F1 Score) ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(f\"\\nBest model: {results_df.iloc[0]['Model']} (F1={results_df.iloc[0]['F1']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6553046b",
   "metadata": {},
   "source": [
    "## 3. MLflow Models: Deployment\n",
    "\n",
    "**MLflow Models** standardize model packaging for deployment across platforms.\n",
    "\n",
    "**Key Features:**\n",
    "- **Multi-framework support**: sklearn, TensorFlow, PyTorch, ONNX, custom Python\n",
    "- **Model signature**: Input/output schema validation\n",
    "- **Flavor system**: Multiple representations of same model (sklearn flavor + python_function flavor)\n",
    "- **Deployment targets**: REST API, batch, Spark UDF, cloud (AWS SageMaker, Azure ML)\n",
    "\n",
    "**Model URI formats:**\n",
    "- `runs:/<run_id>/model` - Model from specific run\n",
    "- `models:/<model_name>/<version>` - Model from registry by version\n",
    "- `models:/<model_name>/<stage>` - Model from registry by stage (Staging/Production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87239e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for inference\n",
    "# Option 1: Load from run\n",
    "run_id = mlflow.search_runs(\n",
    "    experiment_names=[\"Model_Comparison_Yield\"],\n",
    "    order_by=[\"metrics.f1_score DESC\"],\n",
    "    max_results=1\n",
    ").iloc[0]['run_id']\n",
    "\n",
    "model_uri = f\"runs:/{run_id}/model\"\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Make predictions\n",
    "sample_device = pd.DataFrame({\n",
    "    'Vdd_V': [1.21],\n",
    "    'Idd_mA': [49.5],\n",
    "    'freq_MHz': [1025],\n",
    "    'temp_C': [26.5]\n",
    "})\n",
    "\n",
    "prediction = loaded_model.predict(sample_device)\n",
    "print(f\"Sample device prediction: {'PASS' if prediction[0] == 1 else 'FAIL'}\")\n",
    "print(f\"Device parameters: Vdd={sample_device['Vdd_V'][0]}V, Idd={sample_device['Idd_mA'][0]}mA\")\n",
    "\n",
    "# Batch prediction\n",
    "batch_devices = X_test.head(10)\n",
    "batch_predictions = loaded_model.predict(batch_devices)\n",
    "print(f\"\\nBatch prediction (10 devices): {batch_predictions}\")\n",
    "print(f\"Pass rate: {batch_predictions.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c3895b",
   "metadata": {},
   "source": [
    "## 4. MLflow Model Registry\n",
    "\n",
    "**Model Registry** provides centralized model store with:\n",
    "- **Versioning**: Automatic version numbering (v1, v2, v3...)\n",
    "- **Stage management**: None ‚Üí Staging ‚Üí Production ‚Üí Archived\n",
    "- **Model lineage**: Track which run/experiment produced the model\n",
    "- **Annotations**: Add descriptions, tags, comments to versions\n",
    "- **Access control**: (Enterprise) Role-based permissions\n",
    "\n",
    "**Typical workflow:**\n",
    "1. Train model ‚Üí log to MLflow Tracking\n",
    "2. Register model ‚Üí creates version in registry\n",
    "3. Transition to Staging ‚Üí testing/validation\n",
    "4. Transition to Production ‚Üí live serving\n",
    "5. New model arrives ‚Üí archive old Production model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd9ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Model Registry workflow\n",
    "client = MlflowClient()\n",
    "\n",
    "# 1. Register best model from comparison\n",
    "model_name = \"yield_predictor_production\"\n",
    "\n",
    "# Get best run\n",
    "best_run = mlflow.search_runs(\n",
    "    experiment_names=[\"Model_Comparison_Yield\"],\n",
    "    order_by=[\"metrics.f1_score DESC\"],\n",
    "    max_results=1\n",
    ").iloc[0]\n",
    "\n",
    "model_uri = f\"runs:/{best_run['run_id']}/model\"\n",
    "\n",
    "# Register model\n",
    "try:\n",
    "    model_version = mlflow.register_model(model_uri, model_name)\n",
    "    print(f\"Registered {model_name} version {model_version.version}\")\n",
    "except Exception as e:\n",
    "    print(f\"Model already exists or error: {e}\")\n",
    "    # Get latest version\n",
    "    model_version = client.get_latest_versions(model_name)[0]\n",
    "\n",
    "# 2. Add description and tags\n",
    "client.update_model_version(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    description=f\"Yield predictor trained on {best_run['params.data_size']} devices. \"\n",
    "                f\"F1 score: {best_run['metrics.f1_score']:.4f}. \"\n",
    "                f\"Model type: {best_run['params.model_type']}. \"\n",
    "                f\"Production-ready after validation.\"\n",
    ")\n",
    "\n",
    "client.set_model_version_tag(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    key=\"validation_status\",\n",
    "    value=\"passed\"\n",
    ")\n",
    "\n",
    "client.set_model_version_tag(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    key=\"deployment_date\",\n",
    "    value=\"2025-12-13\"\n",
    ")\n",
    "\n",
    "# 3. Transition to Staging\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    stage=\"Staging\",\n",
    "    archive_existing_versions=False\n",
    ")\n",
    "print(f\"Model transitioned to Staging\")\n",
    "\n",
    "# 4. Simulate validation in staging\n",
    "print(\"Validating model in Staging environment...\")\n",
    "staging_model = mlflow.pyfunc.load_model(f\"models:/{model_name}/Staging\")\n",
    "staging_predictions = staging_model.predict(X_test)\n",
    "staging_accuracy = accuracy_score(y_test, staging_predictions)\n",
    "print(f\"Staging validation accuracy: {staging_accuracy:.4f}\")\n",
    "\n",
    "# 5. Promote to Production (if validation passes)\n",
    "if staging_accuracy > 0.85:\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=model_version.version,\n",
    "        stage=\"Production\",\n",
    "        archive_existing_versions=True  # Archive previous production model\n",
    "    )\n",
    "    print(f\"‚úÖ Model promoted to Production (accuracy {staging_accuracy:.4f} > threshold 0.85)\")\n",
    "else:\n",
    "    print(f\"‚ùå Model failed validation (accuracy {staging_accuracy:.4f} < threshold 0.85)\")\n",
    "\n",
    "# 6. List all versions\n",
    "print(f\"\\n=== All versions of {model_name} ===\")\n",
    "for mv in client.search_model_versions(f\"name='{model_name}'\"):\n",
    "    print(f\"Version {mv.version}: {mv.current_stage} (Run: {mv.run_id[:8]}...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7281ec62",
   "metadata": {},
   "source": [
    "## 5. Advanced MLflow Patterns\n",
    "\n",
    "### A. Nested Runs (Parent-Child Hierarchy)\n",
    "\n",
    "**Use case**: Hyperparameter tuning - parent run for grid search, child runs for each configuration.\n",
    "\n",
    "**Benefits:**\n",
    "- Organize related experiments hierarchically\n",
    "- Compare child runs within parent context\n",
    "- Track overall best result in parent run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested runs for hyperparameter tuning\n",
    "mlflow.set_experiment(\"Nested_Runs_Example\")\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name=\"GridSearch_Parent\") as parent_run:\n",
    "    best_f1 = 0\n",
    "    best_params = None\n",
    "    \n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for n_estimators in param_grid['n_estimators']:\n",
    "            with mlflow.start_run(run_name=f\"depth{max_depth}_trees{n_estimators}\", nested=True):\n",
    "                # Train model\n",
    "                model = RandomForestClassifier(\n",
    "                    max_depth=max_depth,\n",
    "                    n_estimators=n_estimators,\n",
    "                    random_state=42\n",
    "                )\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # Evaluate\n",
    "                y_pred = model.predict(X_test)\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                \n",
    "                # Log to child run\n",
    "                mlflow.log_params({'max_depth': max_depth, 'n_estimators': n_estimators})\n",
    "                mlflow.log_metric('f1_score', f1)\n",
    "                \n",
    "                # Track best\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_params = {'max_depth': max_depth, 'n_estimators': n_estimators}\n",
    "                \n",
    "                print(f\"depth={max_depth}, trees={n_estimators}: F1={f1:.4f}\")\n",
    "    \n",
    "    # Log best result to parent run\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric('best_f1', best_f1)\n",
    "    print(f\"\\nBest configuration: {best_params}, F1={best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd29cb",
   "metadata": {},
   "source": [
    "### B. Custom Metrics (Logging Over Time)\n",
    "\n",
    "**Use case**: Track training loss per epoch, validation accuracy per iteration.\n",
    "\n",
    "**Pattern**: Use `step` parameter in `log_metric()` to create time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log metrics over time (simulating training epochs)\n",
    "with mlflow.start_run(run_name=\"Training_Progress_Tracking\"):\n",
    "    # Simulate 20 epochs of training\n",
    "    for epoch in range(1, 21):\n",
    "        # Simulate improving accuracy\n",
    "        train_acc = 0.5 + (epoch / 20) * 0.4 + np.random.uniform(-0.02, 0.02)\n",
    "        val_acc = 0.48 + (epoch / 20) * 0.38 + np.random.uniform(-0.03, 0.03)\n",
    "        \n",
    "        # Log at each epoch\n",
    "        mlflow.log_metric(\"train_accuracy\", train_acc, step=epoch)\n",
    "        mlflow.log_metric(\"val_accuracy\", val_acc, step=epoch)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}: Train={train_acc:.4f}, Val={val_acc:.4f}\")\n",
    "    \n",
    "    # Log final metrics\n",
    "    mlflow.log_metric(\"final_train_accuracy\", train_acc)\n",
    "    mlflow.log_metric(\"final_val_accuracy\", val_acc)\n",
    "    \n",
    "    print(\"Check MLflow UI - metrics plotted over epochs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f1a4a9",
   "metadata": {},
   "source": [
    "## üéØ Real-World MLflow Projects\n",
    "\n",
    "### **Post-Silicon Validation Projects**\n",
    "\n",
    "#### **Project 1: Automated Model Comparison Pipeline**\n",
    "**Objective**: Systematically compare 10+ algorithms for yield prediction, track all experiments\n",
    "- **Algorithms**: RF, XGBoost, LightGBM, CatBoost, Neural Network, SVM, Logistic Regression, KNN, Gradient Boosting, AdaBoost\n",
    "- **Tracking**: Log all hyperparameters, cross-validation scores (5-fold), training time, inference latency\n",
    "- **Experiments**: Create separate MLflow experiment for each use case (yield, test_time, binning)\n",
    "- **Comparison**: Use MLflow UI to visualize accuracy vs training time scatter plot\n",
    "- **Output**: Best model automatically promoted to registry Staging stage\n",
    "- **Success Metric**: Find optimal model in 2 hours instead of 2 weeks manual testing\n",
    "\n",
    "#### **Project 2: Test Time Optimization Model Registry**\n",
    "**Objective**: Manage multiple versions of test time reduction model with full lineage\n",
    "- **Challenge**: 8 model versions over 6 months (v1.0: 8% reduction ‚Üí v2.3: 32% reduction)\n",
    "- **Registry**: Track each version with tags (data_range, accuracy, test_time_savings, deployed_stations)\n",
    "- **Stages**: None ‚Üí Staging (10% ATE) ‚Üí Production (100% ATE) ‚Üí Archived\n",
    "- **Artifacts**: Save test sequence optimization recommendations as JSON per version\n",
    "- **Monitoring**: Log false negative rate, test time distribution per version\n",
    "- **Rollback**: Instant rollback to v2.1 if v2.3 increases FNR above threshold\n",
    "- **Compliance**: Full audit trail for ISO 9001 quality requirements\n",
    "\n",
    "#### **Project 3: Wafer Map Classifier with Nested Runs**\n",
    "**Objective**: Hyperparameter tuning for spatial anomaly detection with organized tracking\n",
    "- **Parent run**: \"SpatialFeature_GridSearch\" (tracks overall best configuration)\n",
    "- **Child runs**: 120 combinations (8 PCA components √ó 5 contamination values √ó 3 spatial kernels)\n",
    "- **Metrics**: F1 score, precision, recall, inference time per wafer map\n",
    "- **Artifacts**: Save best/worst wafer map examples, confusion matrices\n",
    "- **Best config**: PCA(15) + contamination(0.05) + Gaussian kernel ‚Üí F1=0.91\n",
    "- **Production**: Deploy best model via MLflow Models REST API (<200ms per wafer)\n",
    "\n",
    "#### **Project 4: Device Binning Model Deployment**\n",
    "**Objective**: Deploy binning classifier with MLflow Models to production ATE\n",
    "- **Model**: Multi-class classifier (Premium/Standard/Economy bins)\n",
    "- **Signature**: Define input schema (15 electrical parameters) and output (bin_label + confidence)\n",
    "- **Deployment**: Serve via `mlflow models serve` REST API\n",
    "- **Integration**: ATE calls API with device parameters, receives bin recommendation\n",
    "- **Monitoring**: Track bin distribution drift (expected 60/30/10 split)\n",
    "- **Update process**: Retrain monthly, register new version, A/B test on 5% of devices\n",
    "- **Value**: 98.7% binning accuracy, $1.2M revenue optimization annually\n",
    "\n",
    "---\n",
    "\n",
    "### **General AI/ML Projects**\n",
    "\n",
    "#### **Project 5: Customer Churn Prediction with Autologging**\n",
    "**Objective**: Rapid experimentation for churn prediction using MLflow autologging\n",
    "- **Framework**: XGBoost, LightGBM (both support autologging)\n",
    "- **Data**: Customer usage metrics (100K users, 50 features)\n",
    "- **Tracking**: Enable autologging ‚Üí train 20 models in 30 minutes with full tracking\n",
    "- **Comparison**: Compare all runs in MLflow UI (AUC vs training time)\n",
    "- **Best model**: LightGBM with class weights ‚Üí AUC=0.87, 2min training\n",
    "- **Deployment**: Register to model registry, serve via Flask API\n",
    "- **Business impact**: Identify 15% of high-risk customers, reduce churn by 22%\n",
    "\n",
    "#### **Project 6: Recommendation System Model Registry**\n",
    "**Objective**: Manage lifecycle of recommendation models with staged rollouts\n",
    "- **Models**: Content-based (v1.0), Collaborative filtering (v1.5), Hybrid (v2.0)\n",
    "- **Registry**: Track each approach with metadata (training_data_size, cold_start_performance)\n",
    "- **A/B testing**: Deploy v2.0 to Staging ‚Üí serve to 10% of users ‚Üí measure CTR improvement\n",
    "- **Promotion**: If CTR improves by >5%, promote to Production, archive v1.5\n",
    "- **Artifacts**: Save user embeddings, item embeddings, similarity matrices per version\n",
    "- **Monitoring**: Track CTR, conversion rate, user engagement metrics\n",
    "- **Success**: Hybrid model (v2.0) ‚Üí 18% CTR improvement, promoted to 100% traffic\n",
    "\n",
    "#### **Project 7: Fraud Detection with Nested Experiments**\n",
    "**Objective**: Optimize fraud detection with hierarchical experiment tracking\n",
    "- **Parent runs**: Different feature engineering strategies (4 approaches)\n",
    "- **Child runs**: Model variations within each strategy (5 models √ó 4 strategies = 20 runs)\n",
    "- **Metrics**: Precision, recall, F1, false positive rate, inference latency\n",
    "- **Best**: Feature strategy \"temporal_patterns\" + LightGBM ‚Üí F1=0.89, FPR=0.8%\n",
    "- **Production**: Deploy via MLflow Models with <50ms latency requirement\n",
    "- **Monitoring**: Track prediction distribution, alert if fraud rate changes >2%\n",
    "- **Value**: Block $3.5M fraud annually, maintain customer satisfaction (low FPR)\n",
    "\n",
    "#### **Project 8: Time Series Forecasting with Metric Tracking**\n",
    "**Objective**: Track model performance over time for demand forecasting\n",
    "- **Model**: Prophet + XGBoost ensemble\n",
    "- **Training**: Retrain weekly with expanding window (last 365 days)\n",
    "- **Tracking**: Log MAPE, MAE, RMSE at each retrain (52 runs per year)\n",
    "- **Metrics over time**: Use step parameter to track performance degradation\n",
    "- **Alerting**: If MAPE increases >15% for 2 consecutive weeks, trigger investigation\n",
    "- **Seasonality**: Track model performance across seasons (holiday vs normal periods)\n",
    "- **Registry**: Version models by quarter (Q1_2025, Q2_2025, etc.)\n",
    "- **Business value**: Reduce inventory costs by 28%, improve forecast accuracy to MAPE < 8%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c1306",
   "metadata": {},
   "source": [
    "## üìö Comprehensive Takeaways\n",
    "\n",
    "### **üéØ MLflow Complete Overview**\n",
    "\n",
    "**MLflow** is an open-source platform for managing the ML lifecycle from experimentation to production deployment.\n",
    "\n",
    "**Four Pillars:**\n",
    "1. **Tracking**: Log experiments (parameters, metrics, artifacts)\n",
    "2. **Projects**: Package ML code for reproducibility\n",
    "3. **Models**: Deploy models to multiple targets\n",
    "4. **Registry**: Version and manage production models\n",
    "\n",
    "**Why MLflow Wins:**\n",
    "- ‚úÖ **Open-source**: No vendor lock-in, free forever\n",
    "- ‚úÖ **Framework-agnostic**: sklearn, TensorFlow, PyTorch, XGBoost, custom\n",
    "- ‚úÖ **Simple API**: 3 lines to start tracking experiments\n",
    "- ‚úÖ **Production-ready**: Used by Uber, Databricks, Microsoft, Netflix\n",
    "- ‚úÖ **Active community**: 20K+ GitHub stars, frequent updates\n",
    "\n",
    "---\n",
    "\n",
    "### **üîß MLflow Tracking Deep Dive**\n",
    "\n",
    "#### **1. Core Tracking API**\n",
    "\n",
    "**Hierarchy:**\n",
    "- **Experiment**: Collection of related runs (e.g., \"Yield_Prediction_2025\")\n",
    "- **Run**: Single execution (e.g., \"RandomForest_v1_20250113\")\n",
    "- **Metrics, Parameters, Artifacts**: Data logged within run\n",
    "\n",
    "**Essential functions:**\n",
    "```python\n",
    "# Setup\n",
    "mlflow.set_experiment(\"experiment_name\")  # Create/select experiment\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")  # Local or \"http://server:5000\"\n",
    "\n",
    "# Tracking\n",
    "with mlflow.start_run(run_name=\"descriptive_name\"):\n",
    "    mlflow.log_param(\"learning_rate\", 0.01)        # Single parameter\n",
    "    mlflow.log_params({\"n_estimators\": 100, ...})  # Multiple parameters\n",
    "    \n",
    "    mlflow.log_metric(\"accuracy\", 0.92)            # Single metric\n",
    "    mlflow.log_metrics({\"f1\": 0.89, \"auc\": 0.94})  # Multiple metrics\n",
    "    mlflow.log_metric(\"loss\", 0.15, step=10)       # Metric at epoch 10\n",
    "    \n",
    "    mlflow.log_artifact(\"plot.png\")                # Log file\n",
    "    mlflow.sklearn.log_model(model, \"model\")       # Log model\n",
    "    mlflow.set_tag(\"environment\", \"production\")    # Add metadata\n",
    "```\n",
    "\n",
    "#### **2. What to Log**\n",
    "\n",
    "**Parameters (immutable):**\n",
    "- Hyperparameters: `n_estimators`, `learning_rate`, `max_depth`\n",
    "- Config: `batch_size`, `random_seed`, `optimizer`\n",
    "- Data info: `train_size`, `test_size`, `data_version`\n",
    "\n",
    "**Metrics (can update):**\n",
    "- Performance: `accuracy`, `f1_score`, `auc`, `precision`, `recall`\n",
    "- Loss: `train_loss`, `val_loss` (logged per epoch with `step`)\n",
    "- Business metrics: `revenue_impact`, `cost_savings`, `latency_ms`\n",
    "\n",
    "**Artifacts (files):**\n",
    "- Models: `model.pkl`, `model.h5`\n",
    "- Plots: `confusion_matrix.png`, `feature_importance.png`, `roc_curve.png`\n",
    "- Data: `predictions.csv`, `feature_stats.json`\n",
    "- Reports: `model_card.md`, `validation_report.pdf`\n",
    "\n",
    "**Tags (metadata):**\n",
    "- `environment`: dev/staging/production\n",
    "- `ml_engineer`: team_member_name\n",
    "- `data_version`: v2.3\n",
    "- `use_case`: yield_prediction\n",
    "\n",
    "#### **3. Autologging**\n",
    "\n",
    "**Instant setup** for supported frameworks:\n",
    "```python\n",
    "# Enable autologging\n",
    "mlflow.sklearn.autolog()   # scikit-learn\n",
    "mlflow.xgboost.autolog()   # XGBoost\n",
    "mlflow.tensorflow.autolog()  # TensorFlow/Keras\n",
    "mlflow.pytorch.autolog()   # PyTorch\n",
    "\n",
    "# Train model - MLflow logs everything automatically!\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Disable when done\n",
    "mlflow.sklearn.autolog(disable=True)\n",
    "```\n",
    "\n",
    "**What gets auto-logged:**\n",
    "- All model hyperparameters\n",
    "- Training/validation metrics\n",
    "- Model artifact (serialized model)\n",
    "- Model signature (input/output schema)\n",
    "- Training dataset metadata\n",
    "\n",
    "**When to use:**\n",
    "- ‚úÖ Rapid prototyping (test 10 models in 10 minutes)\n",
    "- ‚úÖ Standard workflows (no custom metrics needed)\n",
    "- ‚ùå Complex pipelines (need fine-grained control)\n",
    "- ‚ùå Custom metrics (autologging won't capture them)\n",
    "\n",
    "#### **4. Comparing Experiments**\n",
    "\n",
    "**Programmatic comparison:**\n",
    "```python\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_names=[\"Yield_Prediction\"],\n",
    "    filter_string=\"metrics.accuracy > 0.9\",\n",
    "    order_by=[\"metrics.f1_score DESC\"],\n",
    "    max_results=10\n",
    ")\n",
    "\n",
    "print(runs[['run_id', 'params.model_type', 'metrics.accuracy', 'metrics.f1_score']])\n",
    "```\n",
    "\n",
    "**UI comparison:**\n",
    "- MLflow UI: http://127.0.0.1:5000\n",
    "- Select multiple runs ‚Üí \"Compare\" button\n",
    "- Visualize: Parallel coordinates plot, scatter plot (accuracy vs training_time)\n",
    "\n",
    "---\n",
    "\n",
    "### **üöÄ MLflow Models**\n",
    "\n",
    "**Model packaging** for deployment across platforms.\n",
    "\n",
    "#### **1. Model Flavors**\n",
    "\n",
    "**Flavor** = representation of model in specific format.\n",
    "\n",
    "**Every MLflow model has:**\n",
    "- **python_function flavor** (universal): Works everywhere, slower\n",
    "- **Native flavor** (framework-specific): sklearn, tensorflow, pytorch - faster, optimized\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Log model with automatic flavors\n",
    "mlflow.sklearn.log_model(model, \"model\", signature=signature)\n",
    "\n",
    "# Logged as:\n",
    "# - sklearn flavor (for sklearn-native loading)\n",
    "# - python_function flavor (for framework-agnostic loading)\n",
    "```\n",
    "\n",
    "#### **2. Model Signature**\n",
    "\n",
    "**Signature** defines input/output schema for validation.\n",
    "\n",
    "```python\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Infer from data\n",
    "signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "# Manual definition\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "input_schema = Schema([\n",
    "    ColSpec(\"double\", \"Vdd_V\"),\n",
    "    ColSpec(\"double\", \"Idd_mA\"),\n",
    "    ColSpec(\"double\", \"freq_MHz\"),\n",
    "    ColSpec(\"double\", \"temp_C\")\n",
    "])\n",
    "output_schema = Schema([ColSpec(\"long\")])\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "# Log with signature\n",
    "mlflow.sklearn.log_model(model, \"model\", signature=signature)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Validates input data at prediction time\n",
    "- Documents expected input format\n",
    "- Prevents errors in production\n",
    "\n",
    "#### **3. Loading Models**\n",
    "\n",
    "**Three URI formats:**\n",
    "```python\n",
    "# From specific run\n",
    "model = mlflow.pyfunc.load_model(\"runs:/<run_id>/model\")\n",
    "\n",
    "# From registry by version\n",
    "model = mlflow.pyfunc.load_model(\"models:/<model_name>/1\")\n",
    "\n",
    "# From registry by stage\n",
    "model = mlflow.pyfunc.load_model(\"models:/<model_name>/Production\")\n",
    "```\n",
    "\n",
    "**Making predictions:**\n",
    "```python\n",
    "# Single prediction\n",
    "prediction = model.predict(pd.DataFrame([{...}]))\n",
    "\n",
    "# Batch prediction\n",
    "predictions = model.predict(df)\n",
    "```\n",
    "\n",
    "#### **4. Deployment Options**\n",
    "\n",
    "**A. REST API (local)**\n",
    "```bash\n",
    "mlflow models serve -m runs:/<run_id>/model -p 5001\n",
    "\n",
    "# Test\n",
    "curl -X POST http://127.0.0.1:5001/invocations \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\"dataframe_records\": [{\"Vdd_V\": 1.2, \"Idd_mA\": 50, ...}]}'\n",
    "```\n",
    "\n",
    "**B. Batch (Spark UDF)**\n",
    "```python\n",
    "# Load as Spark UDF\n",
    "predict_udf = mlflow.pyfunc.spark_udf(spark, \"runs:/<run_id>/model\")\n",
    "\n",
    "# Apply to Spark DataFrame\n",
    "df = df.withColumn(\"prediction\", predict_udf(*df.columns))\n",
    "```\n",
    "\n",
    "**C. Cloud deployment**\n",
    "- **AWS SageMaker**: `mlflow.sagemaker.deploy()`\n",
    "- **Azure ML**: `mlflow.azureml.deploy()`\n",
    "- **Google Cloud**: Use `gcloud` CLI with MLflow model artifact\n",
    "\n",
    "**D. Docker container**\n",
    "```bash\n",
    "# Build Docker image\n",
    "mlflow models build-docker -m runs:/<run_id>/model -n my_model\n",
    "\n",
    "# Run container\n",
    "docker run -p 5001:8080 my_model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üì¶ MLflow Model Registry**\n",
    "\n",
    "**Centralized model store** with versioning, stages, and governance.\n",
    "\n",
    "#### **1. Registry Workflow**\n",
    "\n",
    "```python\n",
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "\n",
    "# Register model\n",
    "model_version = mlflow.register_model(\n",
    "    model_uri=\"runs:/<run_id>/model\",\n",
    "    name=\"yield_predictor\"\n",
    ")\n",
    "# Creates version 1, 2, 3... automatically\n",
    "\n",
    "# Add description\n",
    "client.update_model_version(\n",
    "    name=\"yield_predictor\",\n",
    "    version=1,\n",
    "    description=\"Trained on 50K devices, F1=0.92\"\n",
    ")\n",
    "\n",
    "# Add tags\n",
    "client.set_model_version_tag(\n",
    "    name=\"yield_predictor\",\n",
    "    version=1,\n",
    "    key=\"validation_status\",\n",
    "    value=\"passed\"\n",
    ")\n",
    "\n",
    "# Transition stages\n",
    "client.transition_model_version_stage(\n",
    "    name=\"yield_predictor\",\n",
    "    version=1,\n",
    "    stage=\"Staging\"  # None ‚Üí Staging ‚Üí Production ‚Üí Archived\n",
    ")\n",
    "\n",
    "# Promote to production\n",
    "client.transition_model_version_stage(\n",
    "    name=\"yield_predictor\",\n",
    "    version=1,\n",
    "    stage=\"Production\",\n",
    "    archive_existing_versions=True  # Demote previous production model\n",
    ")\n",
    "```\n",
    "\n",
    "#### **2. Stage Management**\n",
    "\n",
    "**Four stages:**\n",
    "- **None**: Newly registered, not yet validated\n",
    "- **Staging**: Testing/validation in progress\n",
    "- **Production**: Live model serving predictions\n",
    "- **Archived**: Retired models (kept for audit/rollback)\n",
    "\n",
    "**Typical flow:**\n",
    "1. Register model ‚Üí None\n",
    "2. Validate accuracy/fairness ‚Üí Transition to Staging\n",
    "3. A/B test in staging ‚Üí Monitor metrics\n",
    "4. If successful ‚Üí Transition to Production (archive old)\n",
    "5. If new model arrives ‚Üí Archive current Production\n",
    "\n",
    "#### **3. Model Lineage**\n",
    "\n",
    "**Every registered model tracks:**\n",
    "- Source run ID (which experiment/run produced it)\n",
    "- Training parameters (from run)\n",
    "- Training metrics (from run)\n",
    "- Artifacts (plots, data from run)\n",
    "- Stage history (when transitioned, by whom)\n",
    "\n",
    "**Query lineage:**\n",
    "```python\n",
    "# Get model version details\n",
    "mv = client.get_model_version(\"yield_predictor\", 1)\n",
    "print(f\"Run ID: {mv.run_id}\")\n",
    "print(f\"Stage: {mv.current_stage}\")\n",
    "print(f\"Description: {mv.description}\")\n",
    "\n",
    "# Get source run details\n",
    "run = client.get_run(mv.run_id)\n",
    "print(f\"Parameters: {run.data.params}\")\n",
    "print(f\"Metrics: {run.data.metrics}\")\n",
    "```\n",
    "\n",
    "**Audit trail:**\n",
    "- \"Show me which model version was in production on 2024-03-15\"\n",
    "- \"Which experiment produced the current production model?\"\n",
    "- \"What were the training parameters for version 3?\"\n",
    "\n",
    "---\n",
    "\n",
    "### **üéì Advanced Patterns**\n",
    "\n",
    "#### **1. Nested Runs**\n",
    "\n",
    "**Use case**: Organize hyperparameter tuning experiments.\n",
    "\n",
    "```python\n",
    "with mlflow.start_run(run_name=\"GridSearch_Parent\"):\n",
    "    for param1 in [10, 20, 30]:\n",
    "        for param2 in [0.01, 0.1]:\n",
    "            with mlflow.start_run(run_name=f\"p1_{param1}_p2_{param2}\", nested=True):\n",
    "                # Train with this config\n",
    "                # Log metrics to child run\n",
    "                pass\n",
    "    \n",
    "    # Log best config to parent run\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"best_f1\", best_f1)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Parent run shows overall best result\n",
    "- Child runs show individual configurations\n",
    "- Hierarchical organization in UI\n",
    "\n",
    "#### **2. Metric Tracking Over Time**\n",
    "\n",
    "**Use case**: Track loss per epoch, validation accuracy over iterations.\n",
    "\n",
    "```python\n",
    "with mlflow.start_run():\n",
    "    for epoch in range(100):\n",
    "        train_loss = train_one_epoch()\n",
    "        val_accuracy = validate()\n",
    "        \n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n",
    "```\n",
    "\n",
    "**Result**: MLflow UI plots metrics as time-series graphs.\n",
    "\n",
    "#### **3. Custom Python Models**\n",
    "\n",
    "**Use case**: Deploy complex pipelines (preprocessing + model + postprocessing).\n",
    "\n",
    "```python\n",
    "import mlflow.pyfunc\n",
    "\n",
    "class YieldPredictorPipeline(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        # Load model and preprocessor\n",
    "        import pickle\n",
    "        self.preprocessor = pickle.load(open(context.artifacts[\"preprocessor\"], \"rb\"))\n",
    "        self.model = pickle.load(open(context.artifacts[\"model\"], \"rb\"))\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        # Preprocess\n",
    "        X = self.preprocessor.transform(model_input)\n",
    "        # Predict\n",
    "        predictions = self.model.predict(X)\n",
    "        # Postprocess\n",
    "        return {\"prediction\": predictions.tolist(), \"confidence\": 0.95}\n",
    "\n",
    "# Log custom model\n",
    "artifacts = {\"preprocessor\": \"preprocessor.pkl\", \"model\": \"model.pkl\"}\n",
    "mlflow.pyfunc.log_model(\"custom_model\", python_model=YieldPredictorPipeline(), artifacts=artifacts)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **‚öôÔ∏è MLflow Configuration**\n",
    "\n",
    "#### **Tracking URI Options**\n",
    "\n",
    "**Local file system:**\n",
    "```python\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")  # Default\n",
    "```\n",
    "\n",
    "**SQLite database:**\n",
    "```python\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "```\n",
    "\n",
    "**PostgreSQL (production):**\n",
    "```python\n",
    "mlflow.set_tracking_uri(\"postgresql://user:pass@host:5432/mlflow_db\")\n",
    "```\n",
    "\n",
    "**Remote server:**\n",
    "```python\n",
    "mlflow.set_tracking_uri(\"http://mlflow-server:5000\")\n",
    "```\n",
    "\n",
    "**Databricks:**\n",
    "```python\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(\"/Users/user@example.com/experiments/yield_prediction\")\n",
    "```\n",
    "\n",
    "#### **Artifact Storage**\n",
    "\n",
    "**Local:**\n",
    "```python\n",
    "# Stored in ./mlruns/<experiment_id>/<run_id>/artifacts/\n",
    "```\n",
    "\n",
    "**S3:**\n",
    "```python\n",
    "mlflow.set_tracking_uri(\"http://mlflow-server:5000\")\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"https://s3.amazonaws.com\"\n",
    "# Artifacts stored in s3://bucket/path/\n",
    "```\n",
    "\n",
    "**Azure Blob:**\n",
    "```python\n",
    "os.environ[\"AZURE_STORAGE_CONNECTION_STRING\"] = \"...\"\n",
    "# Artifacts stored in wasbs://container@account/path/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üöÄ Production Best Practices**\n",
    "\n",
    "#### **1. Experiment Organization**\n",
    "\n",
    "**Strategy**: One experiment per use case/project.\n",
    "\n",
    "```\n",
    "Experiments:\n",
    "- Yield_Prediction_2025\n",
    "  - RandomForest_baseline\n",
    "  - XGBoost_v1\n",
    "  - XGBoost_v2_optimized\n",
    "- Test_Time_Optimization\n",
    "  - Sequential_Model\n",
    "  - Parallel_Model\n",
    "- Wafer_Map_Clustering\n",
    "  - KMeans\n",
    "  - DBSCAN\n",
    "  - Hierarchical\n",
    "```\n",
    "\n",
    "**Don't**: Create one experiment per run (clutters UI).\n",
    "\n",
    "#### **2. Naming Conventions**\n",
    "\n",
    "**Run names**: `<model_type>_<variant>_<date>`\n",
    "- Example: `RandomForest_tuned_20250113`\n",
    "\n",
    "**Tags**: Use consistent keys\n",
    "- `environment`: dev/staging/production\n",
    "- `data_version`: v1.0, v2.1\n",
    "- `use_case`: yield/binning/test_time\n",
    "\n",
    "**Model names** (registry): `<use_case>_<model_type>`\n",
    "- Example: `yield_predictor_xgboost`\n",
    "\n",
    "#### **3. Logging Strategy**\n",
    "\n",
    "**Always log:**\n",
    "- ‚úÖ All hyperparameters (even defaults)\n",
    "- ‚úÖ Data size (train/test split sizes)\n",
    "- ‚úÖ Random seed (for reproducibility)\n",
    "- ‚úÖ Training time (for cost analysis)\n",
    "- ‚úÖ Model artifact (for deployment)\n",
    "\n",
    "**Post-silicon specific:**\n",
    "- ‚úÖ STDF data date range\n",
    "- ‚úÖ Test floor location/equipment\n",
    "- ‚úÖ Device type/process node\n",
    "- ‚úÖ Pass/fail thresholds\n",
    "\n",
    "#### **4. Model Registry Governance**\n",
    "\n",
    "**Policy**: Only models meeting criteria can reach Production.\n",
    "\n",
    "**Validation gates:**\n",
    "1. **Accuracy gate**: Accuracy > 0.90\n",
    "2. **Fairness gate**: No bias across device types\n",
    "3. **Latency gate**: Inference < 50ms (for real-time use)\n",
    "4. **A/B test**: Staging model performs ‚â• baseline\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "def promote_to_production(model_name, version, client):\n",
    "    # Get model metrics\n",
    "    mv = client.get_model_version(model_name, version)\n",
    "    run = client.get_run(mv.run_id)\n",
    "    accuracy = float(run.data.metrics[\"accuracy\"])\n",
    "    \n",
    "    # Validation gates\n",
    "    if accuracy < 0.90:\n",
    "        print(f\"‚ùå Failed accuracy gate: {accuracy:.4f} < 0.90\")\n",
    "        return False\n",
    "    \n",
    "    # Promote\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=version,\n",
    "        stage=\"Production\",\n",
    "        archive_existing_versions=True\n",
    "    )\n",
    "    print(f\"‚úÖ Promoted to Production\")\n",
    "    return True\n",
    "```\n",
    "\n",
    "#### **5. Monitoring Production Models**\n",
    "\n",
    "**What to track:**\n",
    "- Prediction volume (predictions/day)\n",
    "- Latency (p50, p95, p99)\n",
    "- Prediction distribution (drift detection)\n",
    "- Error rate (exceptions, timeouts)\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "# Log production metrics daily\n",
    "with mlflow.start_run(run_name=f\"production_monitor_{date}\"):\n",
    "    mlflow.log_metric(\"daily_predictions\", 100000)\n",
    "    mlflow.log_metric(\"p95_latency_ms\", 45)\n",
    "    mlflow.log_metric(\"error_rate\", 0.002)\n",
    "    mlflow.log_artifact(\"prediction_distribution.png\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **‚ö†Ô∏è Common Pitfalls**\n",
    "\n",
    "#### **1. Logging Too Much**\n",
    "- **Problem**: Logging every intermediate step ‚Üí 1000 metrics per run ‚Üí slow UI\n",
    "- **Solution**: Log only essential metrics, aggregate intermediate results\n",
    "\n",
    "#### **2. Not Logging Enough**\n",
    "- **Problem**: Forgot to log random seed ‚Üí can't reproduce result\n",
    "- **Solution**: Log ALL parameters that affect outcome (hyperparameters, seeds, data versions)\n",
    "\n",
    "#### **3. Inconsistent Naming**\n",
    "- **Problem**: Runs named \"test1\", \"test2\", \"final\", \"final_v2\" ‚Üí can't find anything\n",
    "- **Solution**: Use consistent naming: `<model>_<variant>_<date>`\n",
    "\n",
    "#### **4. No Model Signatures**\n",
    "- **Problem**: Production API receives wrong input format ‚Üí crashes\n",
    "- **Solution**: Always use signatures for validation\n",
    "\n",
    "#### **5. Cluttered Experiments**\n",
    "- **Problem**: 500 runs in one experiment \"Experiments\" ‚Üí impossible to navigate\n",
    "- **Solution**: One experiment per project/use case\n",
    "\n",
    "#### **6. No Rollback Plan**\n",
    "- **Problem**: Production model fails, previous version deleted\n",
    "- **Solution**: Use `archive_existing_versions=True` (keeps old models), test rollback procedure\n",
    "\n",
    "---\n",
    "\n",
    "### **üîÆ Next Steps**\n",
    "\n",
    "**After mastering MLflow:**\n",
    "1. **123_Model_Monitoring_Drift_Detection.ipynb** ‚Üí Monitor deployed models for drift\n",
    "2. **124_Feature_Store_Implementation.ipynb** ‚Üí Centralize feature engineering\n",
    "3. **125_ML_Pipeline_Orchestration.ipynb** ‚Üí Automate with Airflow/Kubeflow\n",
    "4. **131_Docker_Fundamentals.ipynb** ‚Üí Containerize MLflow deployments\n",
    "\n",
    "**Hands-On Practice:**\n",
    "- Set up MLflow tracking server (shared team server)\n",
    "- Track 10 experiments for real use case\n",
    "- Register model, test stage transitions\n",
    "- Deploy model via REST API, test with curl\n",
    "- Implement nested runs for hyperparameter tuning\n",
    "- Create custom Python model for complex pipeline\n",
    "\n",
    "---\n",
    "\n",
    "**You now have complete mastery of MLflow for production ML! üöÄ**\n",
    "\n",
    "**Key skill acquired**: Systematic experiment tracking, model versioning, and deployment - the foundation of professional ML engineering."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
