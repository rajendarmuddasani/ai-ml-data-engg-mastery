{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5745a5d3",
   "metadata": {},
   "source": [
    "# 128: Shadow Mode Deployment - Risk-Free Model Validation and Gradual Rollouts\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** shadow mode deployment for zero-risk model validation (new model runs in parallel without serving)\n",
    "- **Implement** A/B testing infrastructure for statistical model comparison\n",
    "- **Build** canary deployment for gradual rollout (5% traffic ‚Üí 25% ‚Üí 100%)\n",
    "- **Deploy** blue-green deployment for instant rollback capability\n",
    "- **Apply** shadow mode to semiconductor yield predictions (validate new model without affecting production decisions)\n",
    "- **Monitor** prediction differences, latency, and accuracy between models\n",
    "\n",
    "## üìö What is Shadow Mode Deployment?\n",
    "\n",
    "**Shadow mode** is a deployment strategy where a **new model runs in parallel** with production, making predictions on the same inputs, but **predictions are not served** to users. This allows safe validation of model performance, latency, and behavior before full deployment.\n",
    "\n",
    "**Why Shadow Mode?**\n",
    "- ‚úÖ **Zero risk**: New model runs alongside production without affecting users (no downtime, no bad predictions)\n",
    "- ‚úÖ **Real-world validation**: Test on actual production traffic (not just validation set)\n",
    "- ‚úÖ **Performance comparison**: Compare accuracy, latency, prediction distribution between models\n",
    "- ‚úÖ **Detect issues early**: Catch bugs, edge cases, or unexpected behavior before full rollout\n",
    "\n",
    "**Deployment Strategies Comparison:**\n",
    "\n",
    "| Strategy | Risk Level | Rollout Speed | Rollback Time | Use Case |\n",
    "|----------|-----------|---------------|---------------|----------|\n",
    "| **Big Bang** | High (100% at once) | Fast (minutes) | Slow (redeploy old) | Emergency fixes only |\n",
    "| **Shadow Mode** | Zero (no traffic served) | Slow (validation takes days) | Instant (just stop shadow) | New models, major changes |\n",
    "| **Canary** | Low (5% traffic) | Medium (hours to days) | Fast (shift traffic back) | Gradual rollouts, low risk |\n",
    "| **A/B Testing** | Medium (50/50 split) | Medium (days to weeks) | Fast (shift all to winner) | Statistical comparison |\n",
    "| **Blue-Green** | Low (instant switch) | Fast (seconds) | Instant (switch back) | High availability systems |\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "### **Use Case 1: Shadow Mode Validation for New Yield Prediction Model**\n",
    "**Input:** New XGBoost yield prediction model (v2.0) replacing Random Forest (v1.5) in production  \n",
    "**Approach:** Run both models on 100% of wafer test data for 2 weeks, log all predictions, compare accuracy  \n",
    "**Output:** v2.0 shows 12% accuracy improvement (88% ‚Üí 99%), 20ms latency increase acceptable (30ms ‚Üí 50ms)  \n",
    "**Value:** $4.5M/year from improved yield prediction (fewer false positives/negatives in wafer disposition decisions)\n",
    "\n",
    "### **Use Case 2: Canary Deployment for Test Time Predictor**\n",
    "**Input:** Retrained test time prediction model (LightGBM) after test flow changes  \n",
    "**Approach:** Route 5% of test jobs to new model ‚Üí monitor MAPE for 24 hours ‚Üí increase to 25% ‚Üí 100%  \n",
    "**Output:** Gradual rollout catches 15% accuracy drop on edge cases (specific device types), rollback at 25% stage  \n",
    "**Value:** $3.2M/year from preventing inaccurate test scheduling (avoid tester idle time and overtime costs)\n",
    "\n",
    "### **Use Case 3: A/B Testing for Parametric Outlier Detection**\n",
    "**Input:** New isolation forest algorithm vs existing LOF for parametric anomaly detection  \n",
    "**Approach:** Split wafer lots 50/50 (A: isolation forest, B: LOF), run for 1 month, compare false positive rate  \n",
    "**Output:** Isolation forest reduces false alarms by 35% (fewer good devices flagged as outliers)  \n",
    "**Value:** $2.8M/year from reduced engineering time investigating false alarms\n",
    "\n",
    "### **Use Case 4: Blue-Green Deployment for Critical Binning Model**\n",
    "**Input:** Production binning model (classifies devices into performance bins) requires zero downtime  \n",
    "**Approach:** Deploy new model to \"green\" environment ‚Üí smoke test ‚Üí instant traffic switch ‚Üí keep \"blue\" ready for rollback  \n",
    "**Output:** Zero-downtime deployment with instant rollback capability (switch back to blue in <10 seconds)  \n",
    "**Value:** $2.1M/year from preventing production downtime (binning model must run 24/7, no interruptions allowed)\n",
    "\n",
    "**Total Post-Silicon Value:** $4.5M + $3.2M + $2.8M + $2.1M = **$12.6M/year**\n",
    "\n",
    "## üîÑ Shadow Mode Deployment Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[üìä Production Traffic] --> B[üîÄ Traffic Router]\n",
    "    B --> C[üü¢ Production Model v1.5]\n",
    "    B --> D[üîµ Shadow Model v2.0]\n",
    "    \n",
    "    C --> E[‚úÖ Serve Prediction]\n",
    "    D --> F[üìù Log Prediction Only]\n",
    "    \n",
    "    E --> G[üíæ Production Logs]\n",
    "    F --> H[üíæ Shadow Logs]\n",
    "    \n",
    "    G --> I[üìä Comparison Analysis]\n",
    "    H --> I\n",
    "    \n",
    "    I --> J{Shadow Better?}\n",
    "    J -->|Yes| K[üöÄ Canary 5%]\n",
    "    J -->|No| L[‚ùå Reject Shadow]\n",
    "    \n",
    "    K --> M[üìà Monitor Metrics]\n",
    "    M --> N{Metrics Good?}\n",
    "    N -->|Yes| O[‚¨ÜÔ∏è Increase to 25%]\n",
    "    N -->|No| P[‚¨áÔ∏è Rollback to 0%]\n",
    "    \n",
    "    O --> Q[üìä Monitor 25%]\n",
    "    Q --> R{Still Good?}\n",
    "    R -->|Yes| S[üéâ Full Rollout 100%]\n",
    "    R -->|No| P\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style S fill:#e1ffe1\n",
    "    style L fill:#ffe1e1\n",
    "    style P fill:#ffe1e1\n",
    "    style J fill:#fff4e1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **Notebook 106: A/B Testing for ML Models** - Statistical testing frameworks for model comparison\n",
    "- **Notebook 125: ML Testing & Validation** - Validation metrics and gates\n",
    "\n",
    "**Next Steps:**\n",
    "- **Notebook 129: Advanced MLOps - Feature Stores** - Feature consistency across shadow and production\n",
    "- **Notebook 130: ML Observability & Debugging** - Debug prediction differences in shadow mode\n",
    "\n",
    "---\n",
    "\n",
    "Let's deploy ML models safely with shadow mode! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cc62e4",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "**Note**: Shadow mode deployment requires routing infrastructure and metrics tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe455b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install deployment and testing libraries\n",
    "# !pip install scikit-learn pandas numpy scipy matplotlib seaborn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_percentage_error\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Shadow mode deployment libraries loaded\")\n",
    "print(\"Focus: Shadow mode, A/B testing, canary deployment, gradual rollout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc648c",
   "metadata": {},
   "source": [
    "## 2. Shadow Mode Implementation\n",
    "\n",
    "**Purpose:** Implement shadow mode system that runs new model alongside production without serving predictions.\n",
    "\n",
    "**Key Points:**\n",
    "- **Dual prediction**: Both models predict on same input, only production model serves result\n",
    "- **Logging**: Shadow predictions logged with metadata (timestamp, model version, confidence)\n",
    "- **Comparison**: Analyze prediction differences, accuracy, latency\n",
    "- **Zero user impact**: Users never see shadow model predictions (no risk)\n",
    "\n",
    "**Why This Matters:** Shadow mode is the safest way to validate models in production before exposing users to potential errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c06974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShadowDeployment:\n",
    "    \"\"\"\n",
    "    Shadow mode deployment system for safe model validation.\n",
    "    \n",
    "    Runs new (shadow) model in parallel with production model:\n",
    "    - Production model serves predictions to users\n",
    "    - Shadow model logs predictions for analysis\n",
    "    - Compare performance without user impact\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, production_model, shadow_model, production_version, shadow_version):\n",
    "        self.production_model = production_model\n",
    "        self.shadow_model = shadow_model\n",
    "        self.production_version = production_version\n",
    "        self.shadow_version = shadow_version\n",
    "        self.shadow_log = []\n",
    "        self.comparison_metrics = {}\n",
    "        \n",
    "    def predict(self, X, log_shadow=True):\n",
    "        \"\"\"\n",
    "        Make prediction with production model, optionally log shadow prediction.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "            log_shadow: Whether to run and log shadow model (default True)\n",
    "        \n",
    "        Returns:\n",
    "            Production model prediction (what user receives)\n",
    "        \"\"\"\n",
    "        # Production prediction (served to user)\n",
    "        prod_pred = self.production_model.predict(X)\n",
    "        prod_time = datetime.now()\n",
    "        \n",
    "        # Shadow prediction (logged only, NOT served)\n",
    "        if log_shadow:\n",
    "            shadow_pred = self.shadow_model.predict(X)\n",
    "            shadow_time = datetime.now()\n",
    "            \n",
    "            # Log shadow prediction\n",
    "            self.shadow_log.append({\n",
    "                'timestamp': shadow_time,\n",
    "                'production_pred': prod_pred,\n",
    "                'shadow_pred': shadow_pred,\n",
    "                'production_version': self.production_version,\n",
    "                'shadow_version': self.shadow_version,\n",
    "                'agreement': np.array_equal(prod_pred, shadow_pred)\n",
    "            })\n",
    "        \n",
    "        return prod_pred  # Only production prediction returned\n",
    "    \n",
    "    def get_agreement_rate(self):\n",
    "        \"\"\"Calculate percentage of predictions where models agree.\"\"\"\n",
    "        if not self.shadow_log:\n",
    "            return None\n",
    "        \n",
    "        agreements = [log['agreement'] for log in self.shadow_log]\n",
    "        agreement_rate = np.mean(agreements)\n",
    "        \n",
    "        return {\n",
    "            'agreement_rate': agreement_rate,\n",
    "            'total_predictions': len(self.shadow_log),\n",
    "            'agreements': sum(agreements),\n",
    "            'disagreements': len(agreements) - sum(agreements)\n",
    "        }\n",
    "    \n",
    "    def compare_accuracy(self, y_true):\n",
    "        \"\"\"\n",
    "        Compare production vs shadow model accuracy.\n",
    "        \n",
    "        Args:\n",
    "            y_true: Ground truth labels (available after predictions)\n",
    "        \n",
    "        Returns:\n",
    "            Accuracy comparison metrics\n",
    "        \"\"\"\n",
    "        if not self.shadow_log:\n",
    "            return None\n",
    "        \n",
    "        # Extract predictions\n",
    "        prod_preds = np.array([log['production_pred'][0] for log in self.shadow_log])\n",
    "        shadow_preds = np.array([log['shadow_pred'][0] for log in self.shadow_log])\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        prod_accuracy = accuracy_score(y_true, prod_preds)\n",
    "        shadow_accuracy = accuracy_score(y_true, shadow_preds)\n",
    "        \n",
    "        # Statistical significance test (McNemar's test for paired predictions)\n",
    "        # Tests if difference in accuracies is statistically significant\n",
    "        prod_correct = (prod_preds == y_true).astype(int)\n",
    "        shadow_correct = (shadow_preds == y_true).astype(int)\n",
    "        \n",
    "        # McNemar contingency table\n",
    "        both_correct = np.sum((prod_correct == 1) & (shadow_correct == 1))\n",
    "        both_wrong = np.sum((prod_correct == 0) & (shadow_correct == 0))\n",
    "        prod_only = np.sum((prod_correct == 1) & (shadow_correct == 0))\n",
    "        shadow_only = np.sum((prod_correct == 0) & (shadow_correct == 1))\n",
    "        \n",
    "        # McNemar test statistic\n",
    "        if (prod_only + shadow_only) > 0:\n",
    "            mcnemar_stat = ((abs(prod_only - shadow_only) - 1) ** 2) / (prod_only + shadow_only)\n",
    "            p_value = 1 - stats.chi2.cdf(mcnemar_stat, df=1)\n",
    "        else:\n",
    "            mcnemar_stat = 0\n",
    "            p_value = 1.0\n",
    "        \n",
    "        return {\n",
    "            'production_accuracy': prod_accuracy,\n",
    "            'shadow_accuracy': shadow_accuracy,\n",
    "            'accuracy_difference': shadow_accuracy - prod_accuracy,\n",
    "            'accuracy_improvement_pct': ((shadow_accuracy - prod_accuracy) / prod_accuracy) * 100 if prod_accuracy > 0 else 0,\n",
    "            'mcnemar_statistic': mcnemar_stat,\n",
    "            'p_value': p_value,\n",
    "            'statistically_significant': p_value < 0.05,\n",
    "            'contingency_table': {\n",
    "                'both_correct': both_correct,\n",
    "                'both_wrong': both_wrong,\n",
    "                'production_only_correct': prod_only,\n",
    "                'shadow_only_correct': shadow_only\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_disagreement_cases(self, X, y_true, top_n=10):\n",
    "        \"\"\"\n",
    "        Get cases where models disagree (for debugging and analysis).\n",
    "        \n",
    "        Args:\n",
    "            X: Input features (for context)\n",
    "            y_true: Ground truth labels\n",
    "            top_n: Number of disagreement cases to return\n",
    "        \n",
    "        Returns:\n",
    "            List of disagreement cases with context\n",
    "        \"\"\"\n",
    "        disagreements = []\n",
    "        \n",
    "        for i, log in enumerate(self.shadow_log):\n",
    "            if not log['agreement']:\n",
    "                disagreements.append({\n",
    "                    'index': i,\n",
    "                    'production_pred': log['production_pred'][0],\n",
    "                    'shadow_pred': log['shadow_pred'][0],\n",
    "                    'true_label': y_true[i],\n",
    "                    'production_correct': log['production_pred'][0] == y_true[i],\n",
    "                    'shadow_correct': log['shadow_pred'][0] == y_true[i],\n",
    "                    'input_features': X[i] if X is not None else None\n",
    "                })\n",
    "        \n",
    "        return disagreements[:top_n]\n",
    "    \n",
    "    def generate_shadow_report(self, y_true, X=None):\n",
    "        \"\"\"Generate comprehensive shadow mode validation report.\"\"\"\n",
    "        agreement = self.get_agreement_rate()\n",
    "        accuracy_comp = self.compare_accuracy(y_true)\n",
    "        disagreements = self.get_disagreement_cases(X, y_true, top_n=5)\n",
    "        \n",
    "        report = {\n",
    "            'summary': {\n",
    "                'production_version': self.production_version,\n",
    "                'shadow_version': self.shadow_version,\n",
    "                'total_predictions': len(self.shadow_log),\n",
    "                'agreement_rate': agreement['agreement_rate'],\n",
    "                'production_accuracy': accuracy_comp['production_accuracy'],\n",
    "                'shadow_accuracy': accuracy_comp['shadow_accuracy'],\n",
    "                'accuracy_improvement': accuracy_comp['accuracy_difference'],\n",
    "                'statistically_significant': accuracy_comp['statistically_significant']\n",
    "            },\n",
    "            'detailed_metrics': accuracy_comp,\n",
    "            'disagreement_analysis': {\n",
    "                'total_disagreements': agreement['disagreements'],\n",
    "                'sample_cases': disagreements\n",
    "            },\n",
    "            'recommendation': self._generate_recommendation(accuracy_comp, agreement)\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _generate_recommendation(self, accuracy_comp, agreement):\n",
    "        \"\"\"Generate deployment recommendation based on shadow mode results.\"\"\"\n",
    "        acc_improvement = accuracy_comp['accuracy_improvement_pct']\n",
    "        agreement_rate = agreement['agreement_rate']\n",
    "        significant = accuracy_comp['statistically_significant']\n",
    "        \n",
    "        if acc_improvement > 2 and significant:\n",
    "            return \\\"PROMOTE: Shadow model shows significant improvement (>2%) - proceed to canary deployment\\\"\n",
    "        elif acc_improvement > 0 and agreement_rate > 0.95:\n",
    "            return \\\"PROMOTE: Shadow model shows improvement with high agreement - low-risk canary deployment\\\"\n",
    "        elif -1 < acc_improvement < 1 and agreement_rate > 0.98:\n",
    "            return \\\"NEUTRAL: Models perform similarly - consider other factors (latency, complexity)\\\"\n",
    "        elif acc_improvement < -1:\n",
    "            return \\\"REJECT: Shadow model underperforms production - do not deploy\\\"\n",
    "        else:\n",
    "            return \\\"INVESTIGATE: Mixed results - analyze disagreement cases before decision\\\"\n",
    "\n",
    "# Example: Shadow mode for yield prediction\n",
    "print(\\\"üî¨ Shadow Mode Deployment: Yield Prediction Model\\\\n\\\")\n",
    "print(\\\"=\\\"*80)\n",
    "\n",
    "# Simulate production scenario\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# Generate test data\n",
    "X_test = pd.DataFrame({\n",
    "    'vdd': np.random.normal(1.2, 0.05, n_samples),\n",
    "    'idd': np.random.normal(50, 5, n_samples),\n",
    "    'frequency': np.random.normal(2400, 100, n_samples),\n",
    "    'temperature': np.random.normal(25, 5, n_samples)\n",
    "})\n",
    "\n",
    "# True labels (ground truth available later)\n",
    "y_true = np.random.choice([0, 1], n_samples, p=[0.1, 0.9])\n",
    "\n",
    "# Train production and shadow models\n",
    "X_train = X_test[:400]\n",
    "y_train = y_true[:400]\n",
    "\n",
    "production_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "production_model.fit(X_train, y_train)\n",
    "\n",
    "# Shadow model with more trees (improved version)\n",
    "shadow_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "shadow_model.fit(X_train, y_train)\n",
    "\n",
    "# Initialize shadow deployment\n",
    "shadow_deploy = ShadowDeployment(\n",
    "    production_model=production_model,\n",
    "    shadow_model=shadow_model,\n",
    "    production_version=\\\"v2.1.0\\\",\n",
    "    shadow_version=\\\"v2.2.0\\\"\n",
    ")\n",
    "\n",
    "print(\\\"‚úÖ Shadow deployment initialized\\\")\n",
    "print(f\\\"   Production: v2.1.0 (50 trees)\\\")\n",
    "print(f\\\"   Shadow: v2.2.0 (100 trees)\\\")\n",
    "\n",
    "# Simulate production traffic (100 predictions)\n",
    "print(f\\\"\\\\nüö¶ Processing production traffic (shadow mode active)...\\\")\n",
    "\n",
    "X_production = X_test[400:500]\n",
    "y_production = y_true[400:500]\n",
    "\n",
    "for i in range(len(X_production)):\n",
    "    # Production prediction (served to user)\n",
    "    # Shadow prediction (logged only)\n",
    "    pred = shadow_deploy.predict(X_production.iloc[[i]], log_shadow=True)\n",
    "\n",
    "print(f\\\"‚úÖ Processed {len(X_production)} predictions\\\")\n",
    "print(f\\\"   Users received: Production model predictions only\\\")\n",
    "print(f\\\"   Logged: Both production and shadow predictions\\\")\n",
    "\n",
    "# Analyze shadow mode results\n",
    "print(f\\\"\\\\n{'='*80}\\\")\n",
    "print(\\\"üìä SHADOW MODE ANALYSIS\\\")\n",
    "print(f\\\"{'='*80}\\\\n\\\")\n",
    "\n",
    "# Agreement rate\n",
    "agreement = shadow_deploy.get_agreement_rate()\n",
    "print(f\\\"1Ô∏è‚É£ PREDICTION AGREEMENT\\\")\n",
    "print(f\\\"   Agreement rate: {agreement['agreement_rate']:.1%}\\\")\n",
    "print(f\\\"   Agreements: {agreement['agreements']}/{agreement['total_predictions']}\\\")\n",
    "print(f\\\"   Disagreements: {agreement['disagreements']}/{agreement['total_predictions']}\\\")\n",
    "\n",
    "# Accuracy comparison\n",
    "accuracy_comp = shadow_deploy.compare_accuracy(y_production)\n",
    "print(f\\\"\\\\n2Ô∏è‚É£ ACCURACY COMPARISON\\\")\n",
    "print(f\\\"   Production (v2.1.0): {accuracy_comp['production_accuracy']:.3f}\\\")\n",
    "print(f\\\"   Shadow (v2.2.0): {accuracy_comp['shadow_accuracy']:.3f}\\\")\n",
    "print(f\\\"   Improvement: {accuracy_comp['accuracy_improvement']:+.3f} ({accuracy_comp['accuracy_improvement_pct']:+.1f}%)\\\")\n",
    "print(f\\\"   Statistically significant: {accuracy_comp['statistically_significant']} (p={accuracy_comp['p_value']:.4f})\\\")\n",
    "\n",
    "# Disagreement analysis\n",
    "print(f\\\"\\\\n3Ô∏è‚É£ DISAGREEMENT ANALYSIS\\\")\n",
    "disagreements = shadow_deploy.get_disagreement_cases(X_production.values, y_production, top_n=3)\n",
    "print(f\\\"   Total disagreements: {len(disagreements)}\\\")\n",
    "\n",
    "if disagreements:\n",
    "    print(f\\\"\\\\n   Sample cases (first 3):\\\")\n",
    "    for i, case in enumerate(disagreements[:3], 1):\n",
    "        print(f\\\"   Case {i}:\\\")\n",
    "        print(f\\\"      Production pred: {case['production_pred']} (correct: {case['production_correct']})\\\")\n",
    "        print(f\\\"      Shadow pred: {case['shadow_pred']} (correct: {case['shadow_correct']})\\\")\n",
    "        print(f\\\"      True label: {case['true_label']}\\\")\n",
    "\n",
    "# Generate comprehensive report\n",
    "print(f\\\"\\\\n{'='*80}\\\")\n",
    "print(\\\"üìÑ SHADOW MODE VALIDATION REPORT\\\")\n",
    "print(f\\\"{'='*80}\\\\n\\\")\n",
    "\n",
    "report = shadow_deploy.generate_shadow_report(y_production, X_production.values)\n",
    "\n",
    "print(f\\\"Production Version: {report['summary']['production_version']}\\\")\n",
    "print(f\\\"Shadow Version: {report['summary']['shadow_version']}\\\")\n",
    "print(f\\\"Total Predictions: {report['summary']['total_predictions']}\\\")\n",
    "print(f\\\"\\\\nPerformance:\\\")\n",
    "print(f\\\"  Agreement Rate: {report['summary']['agreement_rate']:.1%}\\\")\n",
    "print(f\\\"  Production Accuracy: {report['summary']['production_accuracy']:.3f}\\\")\n",
    "print(f\\\"  Shadow Accuracy: {report['summary']['shadow_accuracy']:.3f}\\\")\n",
    "print(f\\\"  Improvement: {report['summary']['accuracy_improvement']:+.3f}\\\")\n",
    "print(f\\\"  Statistical Significance: {report['summary']['statistically_significant']}\\\")\n",
    "\n",
    "print(f\\\"\\\\nüéØ RECOMMENDATION: {report['recommendation']}\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c7550",
   "metadata": {},
   "source": [
    "## 3. A/B Testing with Statistical Significance\n",
    "\n",
    "**Purpose:** Implement A/B testing framework to compare models with statistical rigor.\n",
    "\n",
    "**Key Points:**\n",
    "- **Traffic splitting**: 50% users see model A, 50% see model B (randomized assignment)\n",
    "- **Statistical testing**: t-test, chi-square test, or bootstrapping to validate differences\n",
    "- **Sample size**: Calculate required sample size for statistical power (typically 80%)\n",
    "- **Significance level**: Œ± = 0.05 (95% confidence that difference is real, not random)\n",
    "\n",
    "**Why This Matters:** A/B testing provides statistical proof that new model is better (not just lucky on test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c97c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABTest:\n",
    "    \"\"\"\n",
    "    A/B testing framework for comparing two models with statistical significance.\n",
    "    \n",
    "    Splits traffic between model A (control) and model B (treatment),\n",
    "    measures performance difference, and calculates statistical significance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_a, model_b, model_a_name=\\\"Control\\\", model_b_name=\\\"Treatment\\\", split_ratio=0.5):\n",
    "        self.model_a = model_a\n",
    "        self.model_b = model_b\n",
    "        self.model_a_name = model_a_name\n",
    "        self.model_b_name = model_b_name\n",
    "        self.split_ratio = split_ratio\n",
    "        self.results_a = []\n",
    "        self.results_b = []\n",
    "        \n",
    "    def assign_variant(self, user_id=None):\n",
    "        \\\"\\\"\\\"\n",
    "        Assign user to variant A or B.\n",
    "        \n",
    "        Uses hash of user_id for consistent assignment (same user always gets same variant).\n",
    "        If no user_id, random assignment.\n",
    "        \\\"\\\"\\\"\n",
    "        if user_id is not None:\n",
    "            # Consistent hashing (same user always gets same variant)\n",
    "            hash_val = hash(user_id) % 100\n",
    "            return 'A' if hash_val < (self.split_ratio * 100) else 'B'\n",
    "        else:\n",
    "            # Random assignment\n",
    "            return 'A' if np.random.random() < self.split_ratio else 'B'\n",
    "    \n",
    "    def predict(self, X, user_id=None):\n",
    "        \\\"\\\"\\\"Make prediction based on variant assignment.\\\"\\\"\\\"\n",
    "        variant = self.assign_variant(user_id)\n",
    "        \n",
    "        if variant == 'A':\n",
    "            pred = self.model_a.predict(X)\n",
    "            return pred, variant\n",
    "        else:\n",
    "            pred = self.model_b.predict(X)\n",
    "            return pred, variant\n",
    "    \n",
    "    def log_result(self, variant, prediction, true_label):\n",
    "        \\\"\\\"\\\"Log prediction result for analysis.\\\"\\\"\\\"\n",
    "        correct = (prediction == true_label)\n",
    "        \n",
    "        if variant == 'A':\n",
    "            self.results_a.append({'prediction': prediction, 'true_label': true_label, 'correct': correct})\n",
    "        else:\n",
    "            self.results_b.append({'prediction': prediction, 'true_label': true_label, 'correct': correct})\n",
    "    \n",
    "    def calculate_metrics(self, results):\n",
    "        \\\"\\\"\\\"Calculate performance metrics for a variant.\\\"\\\"\\\"\n",
    "        if not results:\n",
    "            return None\n",
    "        \n",
    "        correct_count = sum(r['correct'] for r in results)\n",
    "        total_count = len(results)\n",
    "        accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'correct': correct_count,\n",
    "            'total': total_count,\n",
    "            'error_rate': 1 - accuracy\n",
    "        }\n",
    "    \n",
    "    def statistical_test(self, alpha=0.05):\n",
    "        \\\"\\\"\\\"\n",
    "        Perform statistical significance test (proportion z-test).\n",
    "        \n",
    "        Tests null hypothesis: accuracy_A = accuracy_B\n",
    "        Alternative: accuracy_A ‚â† accuracy_B (two-tailed test)\n",
    "        \n",
    "        Returns: Test result with p-value and confidence interval\n",
    "        \\\"\\\"\\\"\n",
    "        metrics_a = self.calculate_metrics(self.results_a)\n",
    "        metrics_b = self.calculate_metrics(self.results_b)\n",
    "        \n",
    "        if metrics_a is None or metrics_b is None:\n",
    "            return None\n",
    "        \n",
    "        # Proportion z-test for comparing two proportions (accuracies)\n",
    "        n_a = metrics_a['total']\n",
    "        n_b = metrics_b['total']\n",
    "        p_a = metrics_a['accuracy']\n",
    "        p_b = metrics_b['accuracy']\n",
    "        \n",
    "        # Pooled proportion\n",
    "        p_pool = (metrics_a['correct'] + metrics_b['correct']) / (n_a + n_b)\n",
    "        \n",
    "        # Standard error\n",
    "        se = np.sqrt(p_pool * (1 - p_pool) * (1/n_a + 1/n_b))\n",
    "        \n",
    "        # Z-statistic\n",
    "        z_stat = (p_b - p_a) / se if se > 0 else 0\n",
    "        \n",
    "        # P-value (two-tailed test)\n",
    "        p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "        \n",
    "        # Confidence interval (95% by default)\n",
    "        z_critical = stats.norm.ppf(1 - alpha/2)\n",
    "        se_diff = np.sqrt(p_a * (1 - p_a) / n_a + p_b * (1 - p_b) / n_b)\n",
    "        ci_lower = (p_b - p_a) - z_critical * se_diff\n",
    "        ci_upper = (p_b - p_a) + z_critical * se_diff\n",
    "        \n",
    "        return {\n",
    "            'model_a_accuracy': p_a,\n",
    "            'model_b_accuracy': p_b,\n",
    "            'accuracy_difference': p_b - p_a,\n",
    "            'relative_improvement_pct': ((p_b - p_a) / p_a) * 100 if p_a > 0 else 0,\n",
    "            'z_statistic': z_stat,\n",
    "            'p_value': p_value,\n",
    "            'statistically_significant': p_value < alpha,\n",
    "            'confidence_interval': (ci_lower, ci_upper),\n",
    "            'alpha': alpha,\n",
    "            'sample_size_a': n_a,\n",
    "            'sample_size_b': n_b\n",
    "        }\n",
    "    \n",
    "    def calculate_required_sample_size(self, baseline_rate, min_detectable_effect, alpha=0.05, power=0.8):\n",
    "        \\\"\\\"\\\"\n",
    "        Calculate required sample size per variant for statistical power.\n",
    "        \n",
    "        Args:\n",
    "            baseline_rate: Current accuracy/conversion rate (e.g., 0.90)\n",
    "            min_detectable_effect: Minimum improvement to detect (e.g., 0.02 for 2%)\n",
    "            alpha: Significance level (default 0.05)\n",
    "            power: Statistical power (default 0.8 for 80%)\n",
    "        \n",
    "        Returns:\n",
    "            Required sample size per variant\n",
    "        \\\"\\\"\\\"\n",
    "        # Z-scores for alpha and power\n",
    "        z_alpha = stats.norm.ppf(1 - alpha/2)\n",
    "        z_beta = stats.norm.ppf(power)\n",
    "        \n",
    "        # Expected rates\n",
    "        p1 = baseline_rate\n",
    "        p2 = baseline_rate + min_detectable_effect\n",
    "        \n",
    "        # Pooled variance\n",
    "        p_avg = (p1 + p2) / 2\n",
    "        \n",
    "        # Sample size calculation (per variant)\n",
    "        n = (z_alpha * np.sqrt(2 * p_avg * (1 - p_avg)) + \n",
    "             z_beta * np.sqrt(p1 * (1 - p1) + p2 * (1 - p2))) ** 2 / (p2 - p1) ** 2\n",
    "        \n",
    "        return int(np.ceil(n))\n",
    "    \n",
    "    def generate_ab_report(self):\n",
    "        \\\"\\\"\\\"Generate comprehensive A/B test report.\\\"\\\"\\\"\n",
    "        metrics_a = self.calculate_metrics(self.results_a)\n",
    "        metrics_b = self.calculate_metrics(self.results_b)\n",
    "        stat_test = self.statistical_test()\n",
    "        \n",
    "        report = {\n",
    "            'variant_a': {\n",
    "                'name': self.model_a_name,\n",
    "                'metrics': metrics_a\n",
    "            },\n",
    "            'variant_b': {\n",
    "                'name': self.model_b_name,\n",
    "                'metrics': metrics_b\n",
    "            },\n",
    "            'statistical_test': stat_test,\n",
    "            'recommendation': self._generate_ab_recommendation(stat_test)\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _generate_ab_recommendation(self, stat_test):\n",
    "        \\\"\\\"\\\"Generate recommendation based on A/B test results.\\\"\\\"\\\"\n",
    "        if stat_test is None:\n",
    "            return \\\"INSUFFICIENT DATA: Continue collecting data\\\"\n",
    "        \n",
    "        improvement = stat_test['accuracy_difference']\n",
    "        significant = stat_test['statistically_significant']\n",
    "        p_value = stat_test['p_value']\n",
    "        \n",
    "        if significant and improvement > 0.02:\n",
    "            return f\\\"PROMOTE MODEL B: Statistically significant improvement of {improvement:.1%} (p={p_value:.4f})\\\"\n",
    "        elif significant and improvement > 0:\n",
    "            return f\\\"PROMOTE MODEL B: Statistically significant improvement of {improvement:.1%}, but small magnitude\\\"\n",
    "        elif significant and improvement < 0:\n",
    "            return f\\\"REJECT MODEL B: Statistically significant degradation of {improvement:.1%} (p={p_value:.4f})\\\"\n",
    "        else:\n",
    "            return f\\\"INCONCLUSIVE: No statistically significant difference (p={p_value:.4f}) - need more data or models perform equally\\\"\n",
    "\n",
    "# Example: A/B test for binning model\n",
    "print(\\\"üß™ A/B Test: Binning Model Update\\\\n\\\")\n",
    "print(\\\"=\\\"*80)\n",
    "\n",
    "# Simulate A/B test\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate test data\n",
    "n_test = 1000\n",
    "X_ab_test = pd.DataFrame({\n",
    "    'vdd': np.random.normal(1.2, 0.05, n_test),\n",
    "    'idd': np.random.normal(50, 5, n_test),\n",
    "    'frequency': np.random.normal(2400, 100, n_test)\n",
    "})\n",
    "y_ab_test = np.random.choice([0, 1], n_test, p=[0.05, 0.95])\n",
    "\n",
    "# Train models (A = baseline, B = improved)\n",
    "X_train_ab = X_ab_test[:800]\n",
    "y_train_ab = y_ab_test[:800]\n",
    "\n",
    "model_a = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)\n",
    "model_a.fit(X_train_ab, y_train_ab)\n",
    "\n",
    "model_b = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42)\n",
    "model_b.fit(X_train_ab, y_train_ab)\n",
    "\n",
    "# Initialize A/B test\n",
    "ab_test = ABTest(\n",
    "    model_a=model_a,\n",
    "    model_b=model_b,\n",
    "    model_a_name=\\\"Binning v1.0 (Baseline)\\\",\n",
    "    model_b_name=\\\"Binning v2.0 (Improved)\\\",\n",
    "    split_ratio=0.5\n",
    ")\n",
    "\n",
    "print(\\\"‚úÖ A/B test initialized\\\")\n",
    "print(f\\\"   Model A: {ab_test.model_a_name}\\\")\n",
    "print(f\\\"   Model B: {ab_test.model_b_name}\\\")\n",
    "print(f\\\"   Traffic split: 50/50\\\\n\\\")\n",
    "\n",
    "# Calculate required sample size\n",
    "baseline_acc = 0.95  # Expected baseline accuracy\n",
    "min_effect = 0.02  # Want to detect 2% improvement\n",
    "required_n = ab_test.calculate_required_sample_size(baseline_acc, min_effect)\n",
    "\n",
    "print(f\\\"üìä Sample Size Calculation\\\")\n",
    "print(f\\\"   Baseline accuracy: {baseline_acc:.1%}\\\")\n",
    "print(f\\\"   Minimum detectable effect: {min_effect:.1%}\\\")\n",
    "print(f\\\"   Required sample size per variant: {required_n}\\\")\n",
    "print(f\\\"   Total required: {required_n * 2}\\\\n\\\")\n",
    "\n",
    "# Run A/B test\n",
    "print(f\\\"üö¶ Running A/B test on {len(X_ab_test) - 800} devices...\\\")\n",
    "\n",
    "X_test_ab = X_ab_test[800:]\n",
    "y_test_ab = y_ab_test[800:]\n",
    "\n",
    "for i in range(len(X_test_ab)):\n",
    "    # Assign variant and predict\n",
    "    pred, variant = ab_test.predict(X_test_ab.iloc[[i]], user_id=f\\\"device_{i}\\\")\n",
    "    \n",
    "    # Log result (simulate ground truth available)\n",
    "    ab_test.log_result(variant, pred[0], y_test_ab.iloc[i])\n",
    "\n",
    "print(f\\\"‚úÖ A/B test completed\\\\n\\\")\n",
    "\n",
    "# Generate report\n",
    "print(f\\\"{'='*80}\\\")\n",
    "print(\\\"üìÑ A/B TEST RESULTS\\\")\n",
    "print(f\\\"{'='*80}\\\\n\\\")\n",
    "\n",
    "report = ab_test.generate_ab_report()\n",
    "\n",
    "print(f\\\"VARIANT A: {report['variant_a']['name']}\\\")\n",
    "print(f\\\"  Samples: {report['variant_a']['metrics']['total']}\\\")\n",
    "print(f\\\"  Accuracy: {report['variant_a']['metrics']['accuracy']:.3f}\\\")\n",
    "print(f\\\"  Correct: {report['variant_a']['metrics']['correct']}/{report['variant_a']['metrics']['total']}\\\\n\\\")\n",
    "\n",
    "print(f\\\"VARIANT B: {report['variant_b']['name']}\\\")\n",
    "print(f\\\"  Samples: {report['variant_b']['metrics']['total']}\\\")\n",
    "print(f\\\"  Accuracy: {report['variant_b']['metrics']['accuracy']:.3f}\\\")\n",
    "print(f\\\"  Correct: {report['variant_b']['metrics']['correct']}/{report['variant_b']['metrics']['total']}\\\\n\\\")\n",
    "\n",
    "print(f\\\"STATISTICAL ANALYSIS\\\")\n",
    "stat = report['statistical_test']\n",
    "print(f\\\"  Accuracy difference: {stat['accuracy_difference']:+.3f} ({stat['relative_improvement_pct']:+.1f}%)\\\")\n",
    "print(f\\\"  95% CI: [{stat['confidence_interval'][0]:+.3f}, {stat['confidence_interval'][1]:+.3f}]\\\")\n",
    "print(f\\\"  Z-statistic: {stat['z_statistic']:.3f}\\\")\n",
    "print(f\\\"  P-value: {stat['p_value']:.4f}\\\")\n",
    "print(f\\\"  Statistically significant (Œ±=0.05): {stat['statistically_significant']}\\\\n\\\")\n",
    "\n",
    "print(f\\\"üéØ RECOMMENDATION:\\\\n{report['recommendation']}\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48febeca",
   "metadata": {},
   "source": [
    "## 4. Canary Deployment - Gradual Traffic Rollout\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement canary deployment strategy that gradually increases traffic to new model while monitoring metrics.\n",
    "\n",
    "**Key Points:**\n",
    "- **Gradual rollout**: Start with 1-5% traffic, increase if metrics look good\n",
    "- **Automated rollback**: Revert to old model if performance degrades\n",
    "- **Health checks**: Monitor latency, error rate, accuracy in real-time\n",
    "- **Traffic control**: Adjust percentage based on confidence level\n",
    "\n",
    "**Why This Matters:** Limits blast radius of bad deployments. If new model has issues, only small fraction of users affected before automatic rollback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf83a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CanaryDeployment:\n",
    "    \"\"\"\n",
    "    Canary deployment strategy: gradually increase traffic to new model.\n",
    "    \n",
    "    Starts with small percentage (e.g., 5%), monitors metrics, and increases\n",
    "    if performance is acceptable. Automatically rolls back if issues detected.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stable_model, canary_model, stable_version, canary_version):\n",
    "        self.stable_model = stable_model\n",
    "        self.canary_model = canary_model\n",
    "        self.stable_version = stable_version\n",
    "        self.canary_version = canary_version\n",
    "        self.canary_percentage = 0\n",
    "        self.metrics_stable = []\n",
    "        self.metrics_canary = []\n",
    "        \n",
    "    def set_canary_percentage(self, percentage):\n",
    "        \\\"\\\"\\\"Set traffic percentage for canary model (0-100).\\\"\\\"\\\"\n",
    "        if not 0 <= percentage <= 100:\n",
    "            raise ValueError(\\\"Percentage must be between 0 and 100\\\")\n",
    "        \n",
    "        old_pct = self.canary_percentage\n",
    "        self.canary_percentage = percentage\n",
    "        print(f\\\"üìä Canary traffic adjusted: {old_pct}% ‚Üí {percentage}%\\\")\n",
    "        \n",
    "        return percentage\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \\\"\\\"\\\"Route prediction to stable or canary model based on traffic split.\\\"\\\"\\\"\n",
    "        # Randomly route based on canary percentage\n",
    "        use_canary = np.random.random() * 100 < self.canary_percentage\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        if use_canary:\n",
    "            prediction = self.canary_model.predict(X)\n",
    "            model_used = 'canary'\n",
    "            version = self.canary_version\n",
    "        else:\n",
    "            prediction = self.stable_model.predict(X)\n",
    "            model_used = 'stable'\n",
    "            version = self.stable_version\n",
    "        \n",
    "        latency = (time.time() - start_time) * 1000  # milliseconds\n",
    "        \n",
    "        return prediction, model_used, version, latency\n",
    "    \n",
    "    def log_prediction(self, model_used, prediction, true_label, latency):\n",
    "        \\\"\\\"\\\"Log prediction result for monitoring.\\\"\\\"\\\"\n",
    "        result = {\n",
    "            'prediction': prediction,\n",
    "            'true_label': true_label,\n",
    "            'correct': prediction == true_label,\n",
    "            'latency_ms': latency\n",
    "        }\n",
    "        \n",
    "        if model_used == 'canary':\n",
    "            self.metrics_canary.append(result)\n",
    "        else:\n",
    "            self.metrics_stable.append(result)\n",
    "    \n",
    "    def calculate_health_metrics(self, metrics):\n",
    "        \\\"\\\"\\\"Calculate health metrics for a model.\\\"\\\"\\\"\n",
    "        if not metrics:\n",
    "            return None\n",
    "        \n",
    "        correct = sum(m['correct'] for m in metrics)\n",
    "        total = len(metrics)\n",
    "        latencies = [m['latency_ms'] for m in metrics]\n",
    "        \n",
    "        return {\n",
    "            'accuracy': correct / total if total > 0 else 0,\n",
    "            'total_requests': total,\n",
    "            'avg_latency_ms': np.mean(latencies),\n",
    "            'p50_latency_ms': np.percentile(latencies, 50),\n",
    "            'p95_latency_ms': np.percentile(latencies, 95),\n",
    "            'p99_latency_ms': np.percentile(latencies, 99)\n",
    "        }\n",
    "    \n",
    "    def check_health(self, min_requests=50):\n",
    "        \\\"\\\"\\\"\n",
    "        Check if canary is healthy compared to stable.\n",
    "        \n",
    "        Returns: (is_healthy, reason)\n",
    "        \\\"\\\"\\\"\n",
    "        canary_health = self.calculate_health_metrics(self.metrics_canary)\n",
    "        stable_health = self.calculate_health_metrics(self.metrics_stable)\n",
    "        \n",
    "        if canary_health is None:\n",
    "            return False, \\\"Insufficient canary data\\\"\n",
    "        \n",
    "        if canary_health['total_requests'] < min_requests:\n",
    "            return False, f\\\"Need at least {min_requests} requests (have {canary_health['total_requests']})\\\"\\n        \n",
    "        if stable_health is None:\n",
    "            return True, \\\"No stable baseline yet\\\"\n",
    "        \n",
    "        # Check accuracy degradation\n",
    "        accuracy_drop = stable_health['accuracy'] - canary_health['accuracy']\n",
    "        if accuracy_drop > 0.05:  # 5% accuracy drop\n",
    "            return False, f\\\"Accuracy degradation: {accuracy_drop:.1%} drop\\\"\n",
    "        \n",
    "        # Check latency increase\n",
    "        latency_increase = (canary_health['p95_latency_ms'] - stable_health['p95_latency_ms']) / stable_health['p95_latency_ms']\n",
    "        if latency_increase > 0.5:  # 50% latency increase\n",
    "            return False, f\\\"Latency degradation: {latency_increase:.1%} increase in p95\\\"\n",
    "        \n",
    "        return True, \\\"All metrics healthy\\\"\n",
    "    \n",
    "    def auto_rollout_strategy(self, stages=[5, 25, 50, 100], min_requests_per_stage=100):\n",
    "        \\\"\\\"\\\"\n",
    "        Define automatic rollout strategy.\n",
    "        \n",
    "        Args:\n",
    "            stages: Traffic percentages to test (e.g., [5, 25, 50, 100])\n",
    "            min_requests_per_stage: Minimum requests before advancing to next stage\n",
    "        \n",
    "        Returns: Rollout plan\n",
    "        \\\"\\\"\\\"\n",
    "        return {\n",
    "            'stages': stages,\n",
    "            'min_requests_per_stage': min_requests_per_stage,\n",
    "            'current_stage': 0\n",
    "        }\n",
    "    \n",
    "    def advance_rollout(self, rollout_plan):\n",
    "        \\\"\\\"\\\"\n",
    "        Advance to next rollout stage if health checks pass.\n",
    "        \n",
    "        Returns: (advanced, new_percentage, reason)\n",
    "        \\\"\\\"\\\"\n",
    "        is_healthy, health_reason = self.check_health(rollout_plan['min_requests_per_stage'])\n",
    "        \n",
    "        if not is_healthy:\n",
    "            # Rollback to stable\n",
    "            self.set_canary_percentage(0)\n",
    "            return False, 0, f\\\"ROLLBACK: {health_reason}\\\"\n",
    "        \n",
    "        # Advance to next stage\n",
    "        current_stage = rollout_plan['current_stage']\n",
    "        stages = rollout_plan['stages']\n",
    "        \n",
    "        if current_stage >= len(stages):\n",
    "            return False, self.canary_percentage, \\\"Already at final stage\\\"\n",
    "        \n",
    "        new_percentage = stages[current_stage]\n",
    "        self.set_canary_percentage(new_percentage)\n",
    "        rollout_plan['current_stage'] += 1\n",
    "        \n",
    "        return True, new_percentage, f\\\"Advanced to stage {current_stage + 1}/{len(stages)}\\\"\n",
    "    \n",
    "    def generate_canary_report(self):\n",
    "        \\\"\\\"\\\"Generate canary deployment status report.\\\"\\\"\\\"\n",
    "        stable_health = self.calculate_health_metrics(self.metrics_stable)\n",
    "        canary_health = self.calculate_health_metrics(self.metrics_canary)\n",
    "        is_healthy, health_reason = self.check_health()\n",
    "        \n",
    "        return {\n",
    "            'canary_percentage': self.canary_percentage,\n",
    "            'stable_version': self.stable_version,\n",
    "            'canary_version': self.canary_version,\n",
    "            'stable_health': stable_health,\n",
    "            'canary_health': canary_health,\n",
    "            'health_check': {\n",
    "                'is_healthy': is_healthy,\n",
    "                'reason': health_reason\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Example: Canary deployment for test time optimization model\n",
    "print(\\\"üê§ Canary Deployment: Test Time Optimization Model\\\\n\\\")\n",
    "print(\\\"=\\\"*80)\n",
    "\n",
    "# Generate test data\n",
    "np.random.seed(42)\n",
    "n_canary_test = 500\n",
    "\n",
    "X_canary_test = pd.DataFrame({\n",
    "    'vdd': np.random.normal(1.2, 0.05, n_canary_test),\n",
    "    'frequency': np.random.normal(2400, 100, n_canary_test),\n",
    "    'temperature': np.random.normal(25, 5, n_canary_test)\n",
    "})\n",
    "y_canary_test = np.random.normal(100, 10, n_canary_test)  # Test time in ms\n",
    "\n",
    "# Train models\n",
    "X_train_canary = X_canary_test[:400]\n",
    "y_train_canary = y_canary_test[:400]\n",
    "\n",
    "stable_model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "stable_model.fit(X_train_canary, y_train_canary)\n",
    "\n",
    "canary_model = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42)\n",
    "canary_model.fit(X_train_canary, y_train_canary)\n",
    "\n",
    "# Initialize canary deployment\n",
    "canary = CanaryDeployment(\n",
    "    stable_model=stable_model,\n",
    "    canary_model=canary_model,\n",
    "    stable_version=\\\"v3.1.0\\\",\n",
    "    canary_version=\\\"v3.2.0\\\"\n",
    ")\n",
    "\n",
    "print(\\\"‚úÖ Canary deployment initialized\\\")\n",
    "print(f\\\"   Stable: {canary.stable_version}\\\")\n",
    "print(f\\\"   Canary: {canary.canary_version}\\\")\n",
    "print(f\\\"   Current canary traffic: {canary.canary_percentage}%\\\\n\\\")\n",
    "\n",
    "# Define rollout strategy\n",
    "rollout_plan = canary.auto_rollout_strategy(\n",
    "    stages=[5, 25, 50, 100],\n",
    "    min_requests_per_stage=50\n",
    ")\n",
    "\n",
    "print(\\\"üìã Rollout Strategy\\\")\n",
    "print(f\\\"   Stages: {rollout_plan['stages']}\\\")\n",
    "print(f\\\"   Minimum requests per stage: {rollout_plan['min_requests_per_stage']}\\\\n\\\")\n",
    "\n",
    "# Simulate gradual rollout\n",
    "X_test_canary = X_canary_test[400:]\n",
    "y_test_canary = y_canary_test[400:]\n",
    "\n",
    "print(\\\"üöÄ Starting gradual rollout...\\\\n\\\")\n",
    "\n",
    "for stage_idx, target_pct in enumerate(rollout_plan['stages']):\n",
    "    print(f\\\"{'='*80}\\\")\n",
    "    print(f\\\"STAGE {stage_idx + 1}: Target {target_pct}% canary traffic\\\")\n",
    "    print(f\\\"{'='*80}\\\")\n",
    "    \n",
    "    # Set canary percentage\n",
    "    canary.set_canary_percentage(target_pct)\n",
    "    \n",
    "    # Process requests for this stage\n",
    "    requests_this_stage = rollout_plan['min_requests_per_stage']\n",
    "    stage_start = stage_idx * requests_this_stage\n",
    "    stage_end = stage_start + requests_this_stage\n",
    "    \n",
    "    if stage_end > len(X_test_canary):\n",
    "        stage_end = len(X_test_canary)\n",
    "        requests_this_stage = stage_end - stage_start\n",
    "    \n",
    "    print(f\\\"üìä Processing {requests_this_stage} requests...\\\\n\\\")\n",
    "    \n",
    "    for i in range(stage_start, stage_end):\n",
    "        pred, model_used, version, latency = canary.predict(X_test_canary.iloc[[i]])\n",
    "        canary.log_prediction(model_used, pred[0], y_test_canary.iloc[i], latency)\n",
    "    \n",
    "    # Check health after stage\n",
    "    report = canary.generate_canary_report()\n",
    "    \n",
    "    print(f\\\"STABLE ({report['stable_version']}):\\\" )\n",
    "    if report['stable_health']:\n",
    "        print(f\\\"  Requests: {report['stable_health']['total_requests']}\\\")\n",
    "        print(f\\\"  Avg latency: {report['stable_health']['avg_latency_ms']:.2f}ms\\\")\n",
    "        print(f\\\"  P95 latency: {report['stable_health']['p95_latency_ms']:.2f}ms\\\\n\\\")\n",
    "    \n",
    "    print(f\\\"CANARY ({report['canary_version']}):\\\" )\n",
    "    if report['canary_health']:\n",
    "        print(f\\\"  Requests: {report['canary_health']['total_requests']}\\\")\n",
    "        print(f\\\"  Avg latency: {report['canary_health']['avg_latency_ms']:.2f}ms\\\")\n",
    "        print(f\\\"  P95 latency: {report['canary_health']['p95_latency_ms']:.2f}ms\\\\n\\\")\n",
    "    \n",
    "    print(f\\\"HEALTH CHECK: {'‚úÖ PASS' if report['health_check']['is_healthy'] else '‚ùå FAIL'}\\\")\n",
    "    print(f\\\"  Reason: {report['health_check']['reason']}\\\\n\\\")\n",
    "    \n",
    "    if not report['health_check']['is_healthy']:\n",
    "        print(\\\"üö® ROLLBACK TRIGGERED - Canary deployment aborted\\\\n\\\")\n",
    "        break\n",
    "    \n",
    "    if stage_idx < len(rollout_plan['stages']) - 1:\n",
    "        print(f\\\"‚úÖ Stage {stage_idx + 1} successful - advancing to next stage\\\\n\\\")\n",
    "\n",
    "print(f\\\"{'='*80}\\\")\n",
    "print(\\\"‚úÖ CANARY DEPLOYMENT COMPLETED SUCCESSFULLY\\\")\n",
    "print(f\\\"   Final canary traffic: {canary.canary_percentage}%\\\")\n",
    "print(f\\\"   Total stable requests: {len(canary.metrics_stable)}\\\")\n",
    "print(f\\\"   Total canary requests: {len(canary.metrics_canary)}\\\")\n",
    "print(f\\\"{'='*80}\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ad143",
   "metadata": {},
   "source": [
    "## 5. Blue-Green Deployment - Zero Downtime Switching\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement blue-green deployment pattern for instant traffic switching with zero downtime and quick rollback capability.\n",
    "\n",
    "**Key Points:**\n",
    "- **Two environments**: Blue (current production), Green (new version)\n",
    "- **Instant switch**: Load balancer redirects 100% traffic at once\n",
    "- **Quick rollback**: Switch back to Blue if issues detected\n",
    "- **Zero downtime**: No service interruption during deployment\n",
    "\n",
    "**Why This Matters:** Enables instant rollback if catastrophic issues found. Entire environment pre-validated before any user traffic hits it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97515a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlueGreenDeployment:\n",
    "    \"\"\"\n",
    "    Blue-green deployment: maintain two identical environments (blue and green).\n",
    "    \n",
    "    Traffic routes to one environment (e.g., blue). New version deploys to idle\n",
    "    environment (green). After validation, switch all traffic to green instantly.\n",
    "    If issues, instant rollback to blue.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.environments = {\n",
    "            'blue': None,\n",
    "            'green': None\n",
    "        }\n",
    "        self.versions = {\n",
    "            'blue': None,\n",
    "            'green': None\n",
    "        }\n",
    "        self.active_env = 'blue'\n",
    "        self.metrics = {\n",
    "            'blue': [],\n",
    "            'green': []\n",
    "        }\n",
    "    \n",
    "    def deploy_to_environment(self, env, model, version):\n",
    "        \\\"\\\"\\\"Deploy model to specified environment (blue or green).\\\"\\\"\\\"\n",
    "        if env not in ['blue', 'green']:\n",
    "            raise ValueError(\\\"Environment must be 'blue' or 'green'\\\")\n",
    "        \n",
    "        self.environments[env] = model\n",
    "        self.versions[env] = version\n",
    "        \n",
    "        print(f\\\"‚úÖ Deployed {version} to {env.upper()} environment\\\")\n",
    "        \n",
    "        return env\n",
    "    \n",
    "    def get_active_model(self):\n",
    "        \\\"\\\"\\\"Get currently active model.\\\"\\\"\\\"\n",
    "        return self.environments[self.active_env], self.versions[self.active_env]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \\\"\\\"\\\"Make prediction using active environment.\\\"\\\"\\\"\n",
    "        model, version = self.get_active_model()\n",
    "        \n",
    "        if model is None:\n",
    "            raise RuntimeError(f\\\"No model deployed to active environment ({self.active_env})\\\" )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        prediction = model.predict(X)\n",
    "        latency = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return prediction, self.active_env, version, latency\n",
    "    \n",
    "    def log_prediction(self, env, prediction, true_label, latency):\n",
    "        \\\"\\\"\\\"Log prediction result for monitoring.\\\"\\\"\\\"\n",
    "        self.metrics[env].append({\n",
    "            'prediction': prediction,\n",
    "            'true_label': true_label,\n",
    "            'correct': prediction == true_label,\n",
    "            'latency_ms': latency\n",
    "        })\n",
    "    \n",
    "    def validate_environment(self, env, X_val, y_val):\n",
    "        \\\"\\\"\\\"\n",
    "        Validate model in specified environment before switching traffic.\n",
    "        \n",
    "        Returns: (passed, metrics)\n",
    "        \\\"\\\"\\\"\n",
    "        if self.environments[env] is None:\n",
    "            return False, \\\"No model deployed to environment\\\"\n",
    "        \n",
    "        model = self.environments[env]\n",
    "        \n",
    "        # Run validation predictions\n",
    "        print(f\\\"üîç Validating {env.upper()} environment...\\\\n\\\")\n",
    "        \n",
    "        validation_results = []\n",
    "        for i in range(len(X_val)):\n",
    "            start_time = time.time()\n",
    "            pred = model.predict(X_val.iloc[[i]])\n",
    "            latency = (time.time() - start_time) * 1000\n",
    "            \n",
    "            validation_results.append({\n",
    "                'prediction': pred[0],\n",
    "                'true_label': y_val.iloc[i],\n",
    "                'correct': pred[0] == y_val.iloc[i],\n",
    "                'latency_ms': latency\n",
    "            })\n",
    "        \n",
    "        # Calculate metrics\n",
    "        correct = sum(r['correct'] for r in validation_results)\n",
    "        total = len(validation_results)\n",
    "        accuracy = correct / total\n",
    "        latencies = [r['latency_ms'] for r in validation_results]\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'avg_latency_ms': np.mean(latencies),\n",
    "            'p95_latency_ms': np.percentile(latencies, 95),\n",
    "            'p99_latency_ms': np.percentile(latencies, 99),\n",
    "            'total_validated': total\n",
    "        }\n",
    "        \n",
    "        # Validation criteria\n",
    "        passed = True\n",
    "        reasons = []\n",
    "        \n",
    "        if accuracy < 0.90:  # Minimum 90% accuracy\n",
    "            passed = False\n",
    "            reasons.append(f\\\"Accuracy {accuracy:.1%} below threshold (90%)\\\" )\n",
    "        \n",
    "        if metrics['p95_latency_ms'] > 50:  # Maximum 50ms p95 latency\n",
    "            passed = False\n",
    "            reasons.append(f\\\"P95 latency {metrics['p95_latency_ms']:.1f}ms exceeds threshold (50ms)\\\")\n",
    "        \n",
    "        if passed:\n",
    "            reasons.append(\\\"All validation checks passed\\\")\n",
    "        \n",
    "        return passed, metrics, reasons\n",
    "    \n",
    "    def switch_traffic(self, target_env):\n",
    "        \\\"\\\"\\\"Switch all traffic to target environment.\\\"\\\"\\\"\n",
    "        if target_env not in ['blue', 'green']:\n",
    "            raise ValueError(\\\"Target environment must be 'blue' or 'green'\\\")\n",
    "        \n",
    "        if self.environments[target_env] is None:\n",
    "            raise RuntimeError(f\\\"Cannot switch to {target_env} - no model deployed\\\")\n",
    "        \n",
    "        old_env = self.active_env\n",
    "        self.active_env = target_env\n",
    "        \n",
    "        print(f\\\"üîÑ Traffic switched: {old_env.upper()} ‚Üí {target_env.upper()}\\\")\n",
    "        print(f\\\"   Active version: {self.versions[target_env]}\\\")\n",
    "        \n",
    "        return target_env\n",
    "    \n",
    "    def rollback(self):\n",
    "        \\\"\\\"\\\"Rollback to previous environment.\\\"\\\"\\\"\n",
    "        # Switch to inactive environment\n",
    "        inactive_env = 'green' if self.active_env == 'blue' else 'blue'\n",
    "        \n",
    "        if self.environments[inactive_env] is None:\n",
    "            raise RuntimeError(f\\\"Cannot rollback - no model in {inactive_env} environment\\\")\n",
    "        \n",
    "        print(f\\\"üö® ROLLBACK INITIATED\\\")\n",
    "        old_active = self.active_env\n",
    "        self.switch_traffic(inactive_env)\n",
    "        print(f\\\"   Rolled back from {old_active.upper()} to {inactive_env.upper()}\\\")\n",
    "        \n",
    "        return inactive_env\n",
    "    \n",
    "    def get_deployment_status(self):\n",
    "        \\\"\\\"\\\"Get current deployment status.\\\"\\\"\\\"\n",
    "        return {\n",
    "            'active_environment': self.active_env,\n",
    "            'active_version': self.versions[self.active_env],\n",
    "            'blue': {\n",
    "                'version': self.versions['blue'],\n",
    "                'deployed': self.environments['blue'] is not None,\n",
    "                'request_count': len(self.metrics['blue'])\n",
    "            },\n",
    "            'green': {\n",
    "                'version': self.versions['green'],\n",
    "                'deployed': self.environments['green'] is not None,\n",
    "                'request_count': len(self.metrics['green'])\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Example: Blue-green deployment for wafer map classification\n",
    "print(\\\"üîµüü¢ Blue-Green Deployment: Wafer Map Classification\\\\n\\\")\n",
    "print(\\\"=\\\"*80)\n",
    "\n",
    "# Generate test data\n",
    "np.random.seed(42)\n",
    "n_bg_test = 300\n",
    "\n",
    "X_bg_test = pd.DataFrame({\n",
    "    'die_x': np.random.randint(0, 30, n_bg_test),\n",
    "    'die_y': np.random.randint(0, 30, n_bg_test),\n",
    "    'yield_pct': np.random.uniform(85, 99, n_bg_test),\n",
    "    'test_time_ms': np.random.normal(100, 10, n_bg_test)\n",
    "})\n",
    "y_bg_test = np.random.choice(['center', 'edge', 'random'], n_bg_test, p=[0.6, 0.3, 0.1])\n",
    "\n",
    "# Train models\n",
    "X_train_bg = X_bg_test[:200]\n",
    "y_train_bg = y_bg_test[:200]\n",
    "\n",
    "# Blue environment: Current production model\n",
    "blue_model = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)\n",
    "blue_model.fit(X_train_bg, y_train_bg)\n",
    "\n",
    "# Green environment: New model to deploy\n",
    "green_model = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42)\n",
    "green_model.fit(X_train_bg, y_train_bg)\n",
    "\n",
    "# Initialize blue-green deployment\n",
    "bg_deploy = BlueGreenDeployment()\n",
    "\n",
    "# Deploy current production to blue\n",
    "bg_deploy.deploy_to_environment('blue', blue_model, 'v1.5.0')\n",
    "print(f\\\"   Blue environment: {bg_deploy.versions['blue']}\\\\n\\\")\n",
    "\n",
    "# Serve traffic from blue (current production)\n",
    "print(\\\"üìä Serving production traffic from BLUE environment...\\\\n\\\")\n",
    "\n",
    "X_prod_traffic = X_bg_test[200:250]\n",
    "y_prod_traffic = y_bg_test[200:250]\n",
    "\n",
    "for i in range(len(X_prod_traffic)):\n",
    "    pred, env, version, latency = bg_deploy.predict(X_prod_traffic.iloc[[i]])\n",
    "    bg_deploy.log_prediction(env, pred, y_prod_traffic.iloc[i], latency)\n",
    "\n",
    "print(f\\\"‚úÖ Processed {len(X_prod_traffic)} requests on BLUE\\\\n\\\")\n",
    "\n",
    "# Deploy new version to green (idle environment)\n",
    "print(f\\\"{'='*80}\\\")\n",
    "print(\\\"DEPLOYING NEW VERSION TO GREEN ENVIRONMENT\\\")\n",
    "print(f\\\"{'='*80}\\\\n\\\")\n",
    "\n",
    "bg_deploy.deploy_to_environment('green', green_model, 'v2.0.0')\n",
    "print(f\\\"   Green environment: {bg_deploy.versions['green']}\\\\n\\\")\n",
    "\n",
    "# Validate green environment before switching\n",
    "print(f\\\"{'='*80}\\\")\n",
    "print(\\\"VALIDATING GREEN ENVIRONMENT\\\")\n",
    "print(f\\\"{'='*80}\\\\n\\\")\n",
    "\n",
    "X_validation = X_bg_test[250:280]\n",
    "y_validation = y_bg_test[250:280]\n",
    "\n",
    "passed, metrics, reasons = bg_deploy.validate_environment('green', X_validation, y_validation)\n",
    "\n",
    "print(f\\\"VALIDATION METRICS (GREEN):\\\")\n",
    "print(f\\\"  Accuracy: {metrics['accuracy']:.3f}\\\")\n",
    "print(f\\\"  Avg latency: {metrics['avg_latency_ms']:.2f}ms\\\")\n",
    "print(f\\\"  P95 latency: {metrics['p95_latency_ms']:.2f}ms\\\")\n",
    "print(f\\\"  P99 latency: {metrics['p99_latency_ms']:.2f}ms\\\")\n",
    "print(f\\\"  Samples validated: {metrics['total_validated']}\\\\n\\\")\n",
    "\n",
    "print(f\\\"VALIDATION RESULT: {'‚úÖ PASSED' if passed else '‚ùå FAILED'}\\\")\n",
    "for reason in reasons:\n",
    "    print(f\\\"  - {reason}\\\")\n",
    "print()\n",
    "\n",
    "# Switch traffic if validation passed\n",
    "if passed:\n",
    "    print(f\\\"{'='*80}\\\")\n",
    "    print(\\\"SWITCHING TRAFFIC TO GREEN\\\")\n",
    "    print(f\\\"{'='*80}\\\\n\\\")\n",
    "    \n",
    "    bg_deploy.switch_traffic('green')\n",
    "    print()\n",
    "    \n",
    "    # Serve traffic from green\n",
    "    print(\\\"üìä Serving production traffic from GREEN environment...\\\\n\\\")\n",
    "    \n",
    "    X_new_traffic = X_bg_test[280:]\n",
    "    y_new_traffic = y_bg_test[280:]\n",
    "    \n",
    "    for i in range(len(X_new_traffic)):\n",
    "        pred, env, version, latency = bg_deploy.predict(X_new_traffic.iloc[[i]])\n",
    "        bg_deploy.log_prediction(env, pred, y_new_traffic.iloc[i], latency)\n",
    "    \n",
    "    print(f\\\"‚úÖ Processed {len(X_new_traffic)} requests on GREEN\\\\n\\\")\n",
    "    \n",
    "    # Get deployment status\n",
    "    status = bg_deploy.get_deployment_status()\n",
    "    \n",
    "    print(f\\\"{'='*80}\\\")\n",
    "    print(\\\"DEPLOYMENT STATUS\\\")\n",
    "    print(f\\\"{'='*80}\\\\n\\\")\n",
    "    \n",
    "    print(f\\\"ACTIVE ENVIRONMENT: {status['active_environment'].upper()}\\\")\n",
    "    print(f\\\"  Version: {status['active_version']}\\\\n\\\")\n",
    "    \n",
    "    print(f\\\"BLUE ENVIRONMENT:\\\")\n",
    "    print(f\\\"  Version: {status['blue']['version']}\\\")\n",
    "    print(f\\\"  Deployed: {status['blue']['deployed']}\\\")\n",
    "    print(f\\\"  Requests served: {status['blue']['request_count']}\\\\n\\\")\n",
    "    \n",
    "    print(f\\\"GREEN ENVIRONMENT:\\\")\n",
    "    print(f\\\"  Version: {status['green']['version']}\\\")\n",
    "    print(f\\\"  Deployed: {status['green']['deployed']}\\\")\n",
    "    print(f\\\"  Requests served: {status['green']['request_count']}\\\\n\\\")\n",
    "    \n",
    "    print(f\\\"‚úÖ BLUE-GREEN DEPLOYMENT COMPLETED SUCCESSFULLY\\\")\n",
    "    print(f\\\"   Production now running: {status['active_version']}\\\")\n",
    "    \n",
    "else:\n",
    "    print(\\\"‚ùå GREEN environment validation failed - deployment aborted\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddded579",
   "metadata": {},
   "source": [
    "## 6. üöÄ Real-World Project Templates\n",
    "\n",
    "### Project 1: Shadow Mode Validation System for Yield Prediction Model\n",
    "\n",
    "**Objective:** Build shadow mode system to validate new yield prediction model using 7 days of real production traffic before promoting.\n",
    "\n",
    "**Business Value:** Zero risk validation prevents deploying models that could cause incorrect yield estimates (costly fab decisions). Shadow mode proves new model accuracy on real data patterns not in test set.\n",
    "\n",
    "**Features to Implement:**\n",
    "- Dual prediction logging (production served, shadow logged only)\n",
    "- McNemar's test for statistical significance of accuracy difference\n",
    "- Disagreement case analysis (identify where models differ most)\n",
    "- Automated promotion decision (PROMOTE if >2% improvement + statistically significant)\n",
    "- Wafer-level correlation analysis (check if shadow degrades on specific wafer types)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Process 10,000+ production predictions through shadow mode\n",
    "- Agreement rate >95% (models mostly agree)\n",
    "- If shadow improves accuracy >2% with p<0.05, auto-promote\n",
    "- Zero production impact (shadow logging adds <5ms latency)\n",
    "- Comprehensive validation report with recommendation\n",
    "\n",
    "**STDF Data Application:**\n",
    "- Production model: Trained on 6 months historical STDF (wafer test)\n",
    "- Shadow model: Retrained with new features (spatial correlations, process parameters)\n",
    "- Validation: Run both models on live STDF stream, compare predictions vs actual yield\n",
    "- Metrics: MAE (mean absolute error), correlation with true yield, spatial agreement\n",
    "\n",
    "---\n",
    "\n",
    "### Project 2: A/B Testing Framework for Customer Churn Prediction\n",
    "\n",
    "**Objective:** Build A/B testing system to statistically prove new churn model outperforms baseline before full rollout.\n",
    "\n",
    "**Business Value:** Statistical rigor prevents \"lucky\" test set results from reaching production. 50/50 traffic split provides unbiased comparison on real user data.\n",
    "\n",
    "**Features to Implement:**\n",
    "- Consistent user assignment (hash user_id for same variant every time)\n",
    "- Sample size calculator (determine required N for statistical power)\n",
    "- Proportion z-test for accuracy comparison (two-tailed test)\n",
    "- Confidence interval calculation (quantify improvement range)\n",
    "- Automated recommendation (promote if significant + positive improvement)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Calculate required sample size (e.g., 2,000 per variant for 2% improvement detection)\n",
    "- Run A/B test until statistical significance achieved or inconclusive\n",
    "- P-value <0.05 for meaningful differences\n",
    "- 95% confidence interval excludes zero for improvements\n",
    "- Generate executive summary with recommendation\n",
    "\n",
    "**Data Application:**\n",
    "- Model A: Random Forest (50 trees, baseline 88% accuracy)\n",
    "- Model B: XGBoost (200 trees, hypothesized 90% accuracy)\n",
    "- Metric: Churn prediction accuracy, false positive rate (annoying non-churners)\n",
    "- Test: Run for 14 days or until 5,000 predictions per variant\n",
    "\n",
    "---\n",
    "\n",
    "### Project 3: Canary Deployment System for Binning Model\n",
    "\n",
    "**Objective:** Implement gradual rollout (5% ‚Üí 25% ‚Üí 50% ‚Üí 100%) with automated rollback for wafer binning model.\n",
    "\n",
    "**Business Value:** Limits blast radius of bad deployments. If new model has defect, only 5% of devices misclassified before automatic rollback prevents further damage.\n",
    "\n",
    "**Features to Implement:**\n",
    "- Gradual traffic increase (5%, 25%, 50%, 100% stages)\n",
    "- Health checks per stage (accuracy, latency, error rate)\n",
    "- Automated rollback triggers (accuracy drop >5%, latency increase >50%)\n",
    "- Staged validation (require 100+ predictions per stage before advancing)\n",
    "- Real-time monitoring dashboard (canary vs stable metrics)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Start with 5% canary traffic, validate 100+ predictions\n",
    "- Advance to next stage only if health checks pass\n",
    "- Rollback immediately if accuracy drops >5% or latency spikes >50%\n",
    "- Complete rollout in <2 hours if all stages pass\n",
    "- Zero manual intervention (fully automated rollout or rollback)\n",
    "\n",
    "**STDF Data Application:**\n",
    "- Stable model: Binning v1.0 (95% accuracy, 10ms latency)\n",
    "- Canary model: Binning v2.0 (expected 96% accuracy, new features)\n",
    "- Rollout: Start with 5% of devices, monitor bin category agreement\n",
    "- Rollback: If canary assigns wrong bins (e.g., BIN1 vs BIN7 disagreement >5%)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 4: Blue-Green Deployment for Fraud Detection Model\n",
    "\n",
    "**Objective:** Build blue-green deployment system for instant traffic switching and zero-downtime fraud detection model updates.\n",
    "\n",
    "**Business Value:** Zero downtime during deployment (no service interruption). Instant rollback if catastrophic issues (e.g., model predicts all fraud, blocks all transactions).\n",
    "\n",
    "**Features to Implement:**\n",
    "- Two identical environments (blue = production, green = new version)\n",
    "- Pre-deployment validation (run green model on validation set before switch)\n",
    "- Instant traffic switch (load balancer change, 100% traffic at once)\n",
    "- One-click rollback (switch back to blue if issues detected)\n",
    "- Smoke tests (basic functionality checks before traffic switch)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Deploy new model to green environment (production still on blue, zero impact)\n",
    "- Validate green model on 1,000 transactions (accuracy, latency, false positive rate)\n",
    "- Switch 100% traffic to green instantly (no downtime)\n",
    "- If issues detected in first 10 minutes, rollback to blue in <30 seconds\n",
    "- Track deployment success rate (target: 95% successful switches)\n",
    "\n",
    "**Data Application:**\n",
    "- Blue: Fraud model v3.1 (logistic regression, 92% accuracy)\n",
    "- Green: Fraud model v4.0 (neural network, expected 94% accuracy)\n",
    "- Validation: Run 1,000 transactions through green, check false positive rate <5%\n",
    "- Switch: If validation passes, route all traffic to green\n",
    "- Rollback: If fraud rate spikes or false positives surge, instant switch to blue\n",
    "\n",
    "---\n",
    "\n",
    "### Project 5: Shadow Mode for Test Time Optimization Model\n",
    "\n",
    "**Objective:** Validate new test time prediction model using shadow mode on real production test flow before deployment.\n",
    "\n",
    "**Business Value:** Test time optimization directly impacts fab throughput. Shadow mode ensures new model doesn't underestimate time (causing test failures) or overestimate (reducing throughput).\n",
    "\n",
    "**Features to Implement:**\n",
    "- Shadow model runs parallel to production (production time estimate used, shadow logged)\n",
    "- Prediction error analysis (compare shadow vs actual test time)\n",
    "- Throughput impact simulation (calculate fab capacity change if shadow deployed)\n",
    "- Statistical comparison (paired t-test for prediction error reduction)\n",
    "- Automated recommendation (promote if shadow reduces MAE >10% with p<0.05)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Process 5,000+ test predictions through shadow mode\n",
    "- Calculate MAE for both models (production vs shadow)\n",
    "- Shadow reduces MAE >10% (statistically significant improvement)\n",
    "- No catastrophic errors (shadow never underestimates by >50ms for critical tests)\n",
    "- Generate report with throughput impact estimate\n",
    "\n",
    "**STDF Data Application:**\n",
    "- Production model: Linear regression (MAE = 15ms)\n",
    "- Shadow model: Random Forest with test parameter interactions (expected MAE = 10ms)\n",
    "- Validation: Log both predictions, compare vs actual test_time from STDF\n",
    "- Metric: MAE, RMSE, correlation with actual, throughput gain estimate\n",
    "\n",
    "---\n",
    "\n",
    "### Project 6: Canary Deployment for Recommendation Engine\n",
    "\n",
    "**Objective:** Gradually roll out new recommendation algorithm (collaborative filtering ‚Üí neural collaborative filtering) with automated rollback.\n",
    "\n",
    "**Business Value:** Recommendations drive revenue (click-through rate, conversion). Canary rollout limits risk of bad recommendations to small user fraction.\n",
    "\n",
    "**Features to Implement:**\n",
    "- Staged rollout (1% ‚Üí 5% ‚Üí 25% ‚Üí 100% traffic)\n",
    "- Business metrics tracking (CTR, conversion rate, revenue per user)\n",
    "- Statistical testing per stage (t-test for CTR difference)\n",
    "- Automated rollback triggers (CTR drop >5%, revenue drop >10%)\n",
    "- Minimum exposure time per stage (24 hours for user behavior to stabilize)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Start with 1% canary traffic (low-risk validation)\n",
    "- Track CTR, conversion, revenue per stage\n",
    "- Advance only if canary maintains or improves metrics (statistically significant)\n",
    "- Complete rollout in 7 days if all stages pass\n",
    "- Rollback if any stage shows metric degradation\n",
    "\n",
    "**Data Application:**\n",
    "- Stable: Collaborative filtering (CTR = 3.2%, conversion = 1.5%)\n",
    "- Canary: Neural collaborative filtering (expected CTR = 3.5%, conversion = 1.7%)\n",
    "- Metrics: Click-through rate, add-to-cart rate, purchase conversion\n",
    "- Test: Run for 24 hours per stage, ensure statistical significance before advancing\n",
    "\n",
    "---\n",
    "\n",
    "### Project 7: A/B Testing for Wafer Map Defect Pattern Classification\n",
    "\n",
    "**Objective:** A/B test new CNN-based wafer map classifier against rule-based baseline to prove improved defect detection.\n",
    "\n",
    "**Business Value:** Defect pattern detection (center, edge, scratch, random) guides root cause analysis. Improved accuracy reduces time to identify process issues.\n",
    "\n",
    "**Features to Implement:**\n",
    "- 50/50 traffic split (wafer-level assignment for consistency)\n",
    "- Multi-class accuracy comparison (center, edge, scratch, random patterns)\n",
    "- Confusion matrix analysis (identify where models disagree)\n",
    "- Statistical significance testing (chi-square test for classification differences)\n",
    "- Defect type stratification (ensure balanced test across all patterns)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Process 1,000+ wafer maps (500 per variant)\n",
    "- Calculate accuracy, precision, recall per defect type\n",
    "- Chi-square test for statistical significance (p<0.05)\n",
    "- CNN improves accuracy by >5% (e.g., 85% ‚Üí 90%)\n",
    "- No degradation on any defect type (avoid trading accuracy across patterns)\n",
    "\n",
    "**STDF Data Application:**\n",
    "- Model A: Rule-based classifier (accuracy = 85%, simple spatial rules)\n",
    "- Model B: CNN classifier (ResNet-based, trained on 10K labeled wafer maps)\n",
    "- Data: STDF wafer test results with die_x, die_y, pass/fail status\n",
    "- Metric: Accuracy per defect type, confusion matrix, F1-score\n",
    "\n",
    "---\n",
    "\n",
    "### Project 8: Blue-Green Deployment for Real-Time Sentiment Analysis\n",
    "\n",
    "**Objective:** Deploy new transformer-based sentiment model using blue-green pattern for zero downtime and instant rollback.\n",
    "\n",
    "**Business Value:** Sentiment analysis drives customer support routing and product feedback. Zero downtime critical for 24/7 support operations.\n",
    "\n",
    "**Features to Implement:**\n",
    "- Parallel environments (blue = LSTM model, green = BERT model)\n",
    "- Pre-deployment validation (1,000 samples, accuracy + latency checks)\n",
    "- Instant traffic switch (DNS/load balancer change)\n",
    "- Smoke tests (validate basic functionality: positive/negative/neutral classification)\n",
    "- Automated rollback (if latency >100ms or accuracy <90% in first 100 predictions)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Deploy BERT model to green (blue still serving production traffic)\n",
    "- Validate green: Accuracy >92%, p95 latency <100ms\n",
    "- Switch 100% traffic to green (zero downtime)\n",
    "- Monitor first 1,000 predictions (rollback if issues detected)\n",
    "- Track deployment time (target: <15 minutes from deploy to green ‚Üí traffic switch)\n",
    "\n",
    "**Data Application:**\n",
    "- Blue: LSTM model (accuracy = 89%, latency = 50ms)\n",
    "- Green: BERT model (accuracy = 93%, latency = 80ms)\n",
    "- Validation: 1,000 customer reviews, check accuracy and latency\n",
    "- Switch: If validation passes, route all API traffic to green\n",
    "- Rollback: If latency exceeds SLA or accuracy drops, instant switch to blue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d13fb5",
   "metadata": {},
   "source": [
    "## 7. üéØ Comprehensive Takeaways: Mastering Safe Model Deployment\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Deployment Strategy Selection Matrix**\n",
    "\n",
    "| Strategy | Risk Level | Validation Time | Rollback Speed | Best For |\n",
    "|----------|-----------|----------------|---------------|----------|\n",
    "| **Shadow Mode** | Zero | Days/weeks | N/A (no traffic) | Initial validation, high-risk models |\n",
    "| **A/B Testing** | Low | Hours/days | Manual (minutes) | Statistical proof needed, similar performance |\n",
    "| **Canary** | Low-Medium | Hours | Automatic (seconds) | Gradual confidence building, medium risk |\n",
    "| **Blue-Green** | Medium | Minutes | Instant (<1s) | Zero downtime required, known good model |\n",
    "\n",
    "**Decision Framework:**\n",
    "- **New model type (architecture change):** Start with shadow mode (7-14 days)\n",
    "- **Incremental improvement (same architecture):** A/B test or canary (1-3 days)\n",
    "- **Critical uptime requirement:** Blue-green (minutes)\n",
    "- **Statistical proof needed:** A/B testing (sufficient sample size)\n",
    "- **Unknown production patterns:** Shadow mode first, then canary\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Shadow Mode Best Practices**\n",
    "\n",
    "**When to Use Shadow Mode:**\n",
    "- ‚úÖ New model architecture (e.g., linear regression ‚Üí neural network)\n",
    "- ‚úÖ High-risk predictions (financial, safety-critical, regulatory)\n",
    "- ‚úÖ Uncertain about production data distribution vs training data\n",
    "- ‚úÖ Need to validate model on real user behavior patterns\n",
    "- ‚úÖ Want zero risk before any production traffic\n",
    "\n",
    "**Key Implementation Points:**\n",
    "- **Dual prediction:** Production model serves users, shadow model logs only\n",
    "- **No user impact:** Shadow predictions never returned to users\n",
    "- **Statistical validation:** Use McNemar's test for paired predictions (not independent t-test)\n",
    "- **Duration:** Run 7-14 days to capture weekly patterns (e.g., weekday vs weekend)\n",
    "- **Sample size:** Minimum 1,000 predictions for meaningful statistical tests\n",
    "\n",
    "**Common Pitfalls:**\n",
    "- ‚ùå **Insufficient duration:** Running shadow mode for only 1-2 days misses weekly patterns\n",
    "- ‚ùå **Wrong statistical test:** Using t-test instead of McNemar's test (predictions are paired, not independent)\n",
    "- ‚ùå **Ignoring disagreement cases:** Not analyzing where models differ most (critical debugging signal)\n",
    "- ‚ùå **Shadow logging overhead:** Adding >10ms latency defeats purpose (should be <5ms)\n",
    "- ‚ùå **No automated decision:** Manually deciding to promote instead of statistical thresholds\n",
    "\n",
    "**Production Checklist:**\n",
    "- [ ] Shadow logging adds <5ms latency (no production impact)\n",
    "- [ ] Log both predictions with request_id for pairing\n",
    "- [ ] Run for at least 7 days (capture weekly patterns)\n",
    "- [ ] Collect 1,000+ paired predictions (statistical power)\n",
    "- [ ] Analyze disagreement cases (debug model differences)\n",
    "- [ ] Calculate McNemar's test statistic (paired test)\n",
    "- [ ] Set promotion threshold (e.g., >2% improvement + p<0.05)\n",
    "- [ ] Generate automated recommendation (PROMOTE/REJECT/INVESTIGATE)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **A/B Testing Statistical Rigor**\n",
    "\n",
    "**When to Use A/B Testing:**\n",
    "- ‚úÖ Need statistical proof of improvement (not just test set luck)\n",
    "- ‚úÖ Similar model architectures (incremental changes)\n",
    "- ‚úÖ Sufficient traffic for sample size (can wait days/weeks if needed)\n",
    "- ‚úÖ Metric differences expected to be small (1-3%)\n",
    "- ‚úÖ Can tolerate both models serving production traffic\n",
    "\n",
    "**Statistical Foundations:**\n",
    "\n",
    "**Hypothesis Testing:**\n",
    "- **Null hypothesis (H‚ÇÄ):** accuracy_A = accuracy_B (no difference)\n",
    "- **Alternative (H‚ÇÅ):** accuracy_A ‚â† accuracy_B (two-tailed test)\n",
    "- **Test statistic:** Z = (p_B - p_A) / SE (proportion z-test)\n",
    "- **Significance level (Œ±):** 0.05 (5% false positive rate)\n",
    "- **P-value:** Probability of observing difference if H‚ÇÄ true\n",
    "\n",
    "**Sample Size Calculation:**\n",
    "```\n",
    "n = (Z_Œ± * ‚àö(2pÃÑ(1-pÃÑ)) + Z_Œ≤ * ‚àö(p‚ÇÅ(1-p‚ÇÅ) + p‚ÇÇ(1-p‚ÇÇ)))¬≤ / (p‚ÇÇ - p‚ÇÅ)¬≤\n",
    "\n",
    "Where:\n",
    "- p‚ÇÅ = baseline accuracy (e.g., 0.90)\n",
    "- p‚ÇÇ = expected new accuracy (e.g., 0.92)\n",
    "- Z_Œ± = 1.96 (for Œ±=0.05, two-tailed)\n",
    "- Z_Œ≤ = 0.84 (for power=0.80)\n",
    "- pÃÑ = (p‚ÇÅ + p‚ÇÇ) / 2\n",
    "```\n",
    "\n",
    "**Example:** Baseline accuracy = 90%, want to detect 2% improvement (92%), need **2,149 samples per variant** for 80% power.\n",
    "\n",
    "**Common Mistakes:**\n",
    "- ‚ùå **Peeking problem:** Checking results multiple times increases false positive rate\n",
    "- ‚ùå **Insufficient power:** Running test without calculating required sample size\n",
    "- ‚ùå **One-tailed test when two-tailed appropriate:** Testing only for improvement misses degradations\n",
    "- ‚ùå **Ignoring confidence intervals:** P-value tells significance, CI tells magnitude\n",
    "- ‚ùå **Sequential testing without correction:** Multiple comparisons require Bonferroni correction\n",
    "\n",
    "**A/B Testing Checklist:**\n",
    "- [ ] Calculate required sample size (power analysis)\n",
    "- [ ] Consistent user assignment (hash user_id, not random each time)\n",
    "- [ ] 50/50 traffic split (equal sample sizes maximize power)\n",
    "- [ ] Run until sample size reached (don't peek early)\n",
    "- [ ] Use proportion z-test (for accuracy/conversion metrics)\n",
    "- [ ] Calculate confidence interval (quantify improvement range)\n",
    "- [ ] Check for statistical significance (p<0.05) AND practical significance (>1-2% improvement)\n",
    "- [ ] Generate recommendation with both statistical and business criteria\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Canary Deployment Automation**\n",
    "\n",
    "**When to Use Canary:**\n",
    "- ‚úÖ Want gradual confidence building (start small, increase if healthy)\n",
    "- ‚úÖ Can monitor metrics in real-time (latency, error rate, accuracy)\n",
    "- ‚úÖ Need automated rollback (no manual intervention)\n",
    "- ‚úÖ Medium-risk changes (not critical enough for shadow, not proven enough for blue-green)\n",
    "- ‚úÖ Can wait hours/days for full rollout\n",
    "\n",
    "**Rollout Strategy:**\n",
    "\n",
    "**Typical Stages:**\n",
    "1. **5% canary:** Initial validation (100-500 requests)\n",
    "2. **25% canary:** Confidence building (500-1,000 requests)\n",
    "3. **50% canary:** Near-equal validation (1,000+ requests)\n",
    "4. **100% canary:** Full rollout (shadow becomes stable)\n",
    "\n",
    "**Health Check Thresholds:**\n",
    "- **Accuracy degradation:** Rollback if drop >5% (e.g., 95% ‚Üí 90%)\n",
    "- **Latency increase:** Rollback if p95 latency increases >50% (e.g., 20ms ‚Üí 30ms)\n",
    "- **Error rate spike:** Rollback if error rate >2x baseline (e.g., 0.1% ‚Üí 0.2%)\n",
    "- **Minimum sample size:** Require 50-100 requests per stage before advancing\n",
    "\n",
    "**Automated Rollback Logic:**\n",
    "```python\n",
    "def check_canary_health(canary_metrics, stable_metrics):\n",
    "    # Accuracy check\n",
    "    if stable_metrics.accuracy - canary_metrics.accuracy > 0.05:\n",
    "        return ROLLBACK, \"Accuracy degradation >5%\"\n",
    "    \n",
    "    # Latency check (p95)\n",
    "    latency_increase = (canary_metrics.p95_latency - stable_metrics.p95_latency) / stable_metrics.p95_latency\n",
    "    if latency_increase > 0.5:\n",
    "        return ROLLBACK, \"P95 latency increase >50%\"\n",
    "    \n",
    "    # Error rate check\n",
    "    if canary_metrics.error_rate > stable_metrics.error_rate * 2:\n",
    "        return ROLLBACK, \"Error rate doubled\"\n",
    "    \n",
    "    return HEALTHY, \"All metrics within thresholds\"\n",
    "```\n",
    "\n",
    "**Common Pitfalls:**\n",
    "- ‚ùå **Too aggressive rollout:** Jumping 5% ‚Üí 100% skips validation stages\n",
    "- ‚ùå **No minimum sample size:** Advancing stage with only 10 requests (insufficient data)\n",
    "- ‚ùå **Ignoring latency:** Focusing only on accuracy misses performance degradation\n",
    "- ‚ùå **Manual rollback:** Requiring human intervention defeats purpose (should be automatic)\n",
    "- ‚ùå **No stage timing:** Advancing too quickly (should wait 30-60 minutes per stage)\n",
    "\n",
    "**Canary Deployment Checklist:**\n",
    "- [ ] Start with 5% traffic (low blast radius)\n",
    "- [ ] Require minimum sample size per stage (50-100 requests)\n",
    "- [ ] Monitor accuracy, latency, error rate (not just accuracy)\n",
    "- [ ] Automated rollback triggers (no manual intervention)\n",
    "- [ ] Wait 30-60 minutes per stage (allow metrics to stabilize)\n",
    "- [ ] Log all decisions (stage advances, rollbacks for debugging)\n",
    "- [ ] Alert on rollback (notify team of automatic rollback)\n",
    "- [ ] Exponential stages (5%, 25%, 50%, 100% not 5%, 10%, 15%...)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Blue-Green Deployment Patterns**\n",
    "\n",
    "**When to Use Blue-Green:**\n",
    "- ‚úÖ Zero downtime required (24/7 service, SLA critical)\n",
    "- ‚úÖ Instant rollback needed (<1 second)\n",
    "- ‚úÖ Model validated in staging (confident in new version)\n",
    "- ‚úÖ Infrastructure capacity for two environments (2x cost during deployment)\n",
    "- ‚úÖ Database schema unchanged (or backward compatible)\n",
    "\n",
    "**Architecture:**\n",
    "- **Blue environment:** Current production (e.g., v1.5.0)\n",
    "- **Green environment:** New version (e.g., v2.0.0)\n",
    "- **Load balancer:** Routes 100% traffic to active environment\n",
    "- **Switch:** Change load balancer target (blue ‚Üí green)\n",
    "- **Rollback:** Change load balancer target (green ‚Üí blue)\n",
    "\n",
    "**Pre-Switch Validation:**\n",
    "- **Smoke tests:** Basic functionality (model loads, predicts, returns valid output)\n",
    "- **Validation set:** Run 1,000+ samples, check accuracy and latency\n",
    "- **Integration tests:** Verify API contracts, database connections\n",
    "- **Load tests:** Ensure green handles production traffic volume\n",
    "- **Health checks:** Confirm all services responding (model server, database, cache)\n",
    "\n",
    "**Deployment Flow:**\n",
    "1. **Deploy to green** (production still on blue, zero impact)\n",
    "2. **Validate green** (smoke tests, validation set, health checks)\n",
    "3. **Switch traffic** (load balancer: blue ‚Üí green, instant change)\n",
    "4. **Monitor green** (first 10 minutes critical, watch for errors)\n",
    "5. **Deprecate blue** (keep running for rollback, decommission after 24 hours)\n",
    "\n",
    "**Common Mistakes:**\n",
    "- ‚ùå **No pre-switch validation:** Switching without testing green first\n",
    "- ‚ùå **Database schema changes:** Breaking backward compatibility (green can't read blue's data)\n",
    "- ‚ùå **Immediate blue decommission:** Removing blue right after switch (no rollback option)\n",
    "- ‚ùå **No health checks:** Switching without confirming green is healthy\n",
    "- ‚ùå **Ignoring stateful services:** Switching without draining connections (websockets, long-polling)\n",
    "\n",
    "**Blue-Green Checklist:**\n",
    "- [ ] Deploy new version to green environment\n",
    "- [ ] Run smoke tests (basic functionality)\n",
    "- [ ] Validate on 1,000+ samples (accuracy, latency)\n",
    "- [ ] Health checks pass (all services responding)\n",
    "- [ ] Load test green (handles production traffic volume)\n",
    "- [ ] Switch traffic (load balancer change, instant)\n",
    "- [ ] Monitor green for 10 minutes (watch for errors)\n",
    "- [ ] Keep blue running 24 hours (rollback option)\n",
    "- [ ] Document deployment (version, switch time, issues)\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Metrics and Monitoring**\n",
    "\n",
    "**Key Metrics to Track:**\n",
    "\n",
    "**Accuracy Metrics:**\n",
    "- **Overall accuracy:** Percentage of correct predictions\n",
    "- **Precision/Recall:** For imbalanced datasets (e.g., fraud detection)\n",
    "- **F1-score:** Harmonic mean of precision and recall\n",
    "- **Confusion matrix:** Where models disagree (false positives vs false negatives)\n",
    "- **Stratified accuracy:** Per category (don't let model trade accuracy across classes)\n",
    "\n",
    "**Performance Metrics:**\n",
    "- **Latency (average):** Mean prediction time\n",
    "- **Latency (p50):** Median prediction time (typical user experience)\n",
    "- **Latency (p95):** 95th percentile (worst 5% of requests)\n",
    "- **Latency (p99):** 99th percentile (outliers, critical for SLA)\n",
    "- **Throughput:** Predictions per second\n",
    "\n",
    "**Operational Metrics:**\n",
    "- **Error rate:** Percentage of failed predictions (exceptions, timeouts)\n",
    "- **Availability:** Percentage of time model is responding\n",
    "- **Traffic distribution:** Percentage to each variant (canary vs stable, A vs B)\n",
    "- **Rollback count:** Number of automatic rollbacks (should be low)\n",
    "\n",
    "**Business Metrics:**\n",
    "- **Revenue impact:** $ change from new model (for recommendation, pricing models)\n",
    "- **User engagement:** Click-through rate, conversion rate\n",
    "- **Time saved:** For automation models (e.g., test time reduction)\n",
    "- **Cost reduction:** For optimization models (e.g., fab yield improvement)\n",
    "\n",
    "**Monitoring Dashboards:**\n",
    "```\n",
    "Shadow Mode Dashboard:\n",
    "- Agreement rate (%) over time\n",
    "- Accuracy: Production vs Shadow\n",
    "- Disagreement cases (top 10)\n",
    "- Statistical test results (p-value, confidence interval)\n",
    "- Recommendation (PROMOTE/REJECT/INVESTIGATE)\n",
    "\n",
    "A/B Test Dashboard:\n",
    "- Traffic split (% to A vs B)\n",
    "- Accuracy: A vs B over time\n",
    "- Sample size (current vs required)\n",
    "- Statistical test results (z-statistic, p-value)\n",
    "- Confidence interval (improvement range)\n",
    "\n",
    "Canary Dashboard:\n",
    "- Canary traffic percentage\n",
    "- Accuracy: Stable vs Canary\n",
    "- Latency: Stable vs Canary (p50, p95, p99)\n",
    "- Error rate: Stable vs Canary\n",
    "- Health status (HEALTHY/ROLLBACK)\n",
    "\n",
    "Blue-Green Dashboard:\n",
    "- Active environment (blue/green)\n",
    "- Traffic distribution (should be 100% to active)\n",
    "- Latency: Blue vs Green\n",
    "- Error rate: Blue vs Green\n",
    "- Deployment history (recent switches)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Statistical Testing Deep Dive**\n",
    "\n",
    "**McNemar's Test (Shadow Mode):**\n",
    "- **Use case:** Comparing paired predictions (same samples, two models)\n",
    "- **Null hypothesis:** Models have equal error rates\n",
    "- **Test statistic:** œá¬≤ = (b - c)¬≤ / (b + c), where b = prod_correct_only, c = shadow_correct_only\n",
    "- **P-value:** From chi-square distribution with 1 degree of freedom\n",
    "- **Example:** If b=50, c=30, œá¬≤=(50-30)¬≤/(50+30)=5.0, p=0.025 (significant)\n",
    "\n",
    "**Proportion Z-Test (A/B Testing):**\n",
    "- **Use case:** Comparing independent proportions (accuracy_A vs accuracy_B)\n",
    "- **Null hypothesis:** p_A = p_B (proportions equal)\n",
    "- **Test statistic:** Z = (p_B - p_A) / SE, where SE = ‚àö(pÃÑ(1-pÃÑ)(1/n_A + 1/n_B))\n",
    "- **P-value:** From normal distribution (two-tailed)\n",
    "- **Example:** If p_A=0.90, p_B=0.92, n=1000 each, Z=1.88, p=0.06 (not significant at Œ±=0.05)\n",
    "\n",
    "**Paired T-Test (Canary Latency):**\n",
    "- **Use case:** Comparing latencies on same requests (paired measurements)\n",
    "- **Null hypothesis:** Mean latency difference = 0\n",
    "- **Test statistic:** t = (Œº_diff - 0) / (s_diff / ‚àön)\n",
    "- **P-value:** From t-distribution with n-1 degrees of freedom\n",
    "- **Example:** If mean_diff=5ms, s_diff=10ms, n=100, t=5.0, p<0.001 (significant)\n",
    "\n",
    "**Chi-Square Test (Blue-Green Validation):**\n",
    "- **Use case:** Comparing categorical distributions (e.g., defect type classification)\n",
    "- **Null hypothesis:** Distributions are equal\n",
    "- **Test statistic:** œá¬≤ = Œ£ (O - E)¬≤ / E, where O=observed, E=expected\n",
    "- **P-value:** From chi-square distribution with (rows-1)*(cols-1) degrees of freedom\n",
    "- **Example:** 2x2 table (model A vs B, correct vs wrong), œá¬≤=10.5, df=1, p=0.001 (significant)\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Rollback Strategies and Automation**\n",
    "\n",
    "**Rollback Trigger Conditions:**\n",
    "\n",
    "**Canary Rollback:**\n",
    "- Accuracy drop >5% (e.g., 95% ‚Üí 90%)\n",
    "- P95 latency increase >50% (e.g., 20ms ‚Üí 30ms)\n",
    "- Error rate doubles (e.g., 0.1% ‚Üí 0.2%)\n",
    "- Any health check failure (service not responding)\n",
    "\n",
    "**Blue-Green Rollback:**\n",
    "- Error rate >1% in first 100 requests\n",
    "- Latency >100ms for >10% of requests\n",
    "- Any critical service failure (database connection, cache)\n",
    "- Manual trigger (on-call engineer detects issue)\n",
    "\n",
    "**A/B Test Stop Conditions:**\n",
    "- Statistical significance achieved (p<0.05) with sufficient sample size\n",
    "- Accuracy difference >10% (obviously better/worse, no need to continue)\n",
    "- Error rate spike (stop test, investigate)\n",
    "- Business metric degradation (e.g., revenue drop >5%)\n",
    "\n",
    "**Automated Rollback Implementation:**\n",
    "```python\n",
    "class AutoRollback:\n",
    "    def __init__(self, deployment):\n",
    "        self.deployment = deployment\n",
    "        self.rollback_triggers = []\n",
    "    \n",
    "    def add_trigger(self, condition, threshold, action):\n",
    "        self.rollback_triggers.append({\n",
    "            'condition': condition,\n",
    "            'threshold': threshold,\n",
    "            'action': action\n",
    "        })\n",
    "    \n",
    "    def check_triggers(self, metrics):\n",
    "        for trigger in self.rollback_triggers:\n",
    "            if trigger['condition'](metrics) > trigger['threshold']:\n",
    "                # Trigger rollback\n",
    "                self.deployment.rollback()\n",
    "                self.alert_team(trigger['action'])\n",
    "                self.log_rollback(trigger, metrics)\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "rollback = AutoRollback(canary_deployment)\n",
    "rollback.add_trigger(\n",
    "    condition=lambda m: m.stable_accuracy - m.canary_accuracy,\n",
    "    threshold=0.05,\n",
    "    action='Accuracy degradation >5%'\n",
    ")\n",
    "rollback.add_trigger(\n",
    "    condition=lambda m: (m.canary_p95_latency - m.stable_p95_latency) / m.stable_p95_latency,\n",
    "    threshold=0.5,\n",
    "    action='P95 latency increase >50%'\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Post-Silicon Validation Applications**\n",
    "\n",
    "**Shadow Mode Use Cases:**\n",
    "- **Yield prediction:** Validate new model on 10,000+ devices before trusting it for fab decisions\n",
    "- **Binning optimization:** Run shadow binning for 7 days, compare with production bins\n",
    "- **Test time estimation:** Shadow model predicts time, compare with actual STDF test_time_ms\n",
    "- **Parametric outlier detection:** Shadow flags outliers, validate against production system\n",
    "\n",
    "**A/B Testing Use Cases:**\n",
    "- **Wafer map classification:** A/B test CNN vs rule-based, measure accuracy per defect type\n",
    "- **Test coverage optimization:** Test which features to measure (A=all, B=optimized subset)\n",
    "- **Probe card selection:** Compare yield with different probe strategies\n",
    "\n",
    "**Canary Deployment Use Cases:**\n",
    "- **Binning model update:** Gradual rollout (5% devices first, monitor bin agreement)\n",
    "- **Test flow optimization:** New test sequence on 5% devices, expand if yield unchanged\n",
    "- **Parametric limit adjustment:** Canary tighter limits on 5% devices, rollback if yield drops\n",
    "\n",
    "**Blue-Green Use Cases:**\n",
    "- **Critical yield model:** Deploy to green, validate on 1,000 wafers, instant switch\n",
    "- **Fab scheduling optimization:** Blue-green ensures zero downtime (24/7 fab operations)\n",
    "- **Real-time SPC:** Instant rollback if control charts show out-of-control points\n",
    "\n",
    "---\n",
    "\n",
    "### 10. **Sample Size and Statistical Power**\n",
    "\n",
    "**Power Analysis Fundamentals:**\n",
    "- **Statistical power (1-Œ≤):** Probability of detecting true effect (typically 80%)\n",
    "- **Significance level (Œ±):** Probability of false positive (typically 5%)\n",
    "- **Effect size:** Minimum difference to detect (e.g., 2% accuracy improvement)\n",
    "- **Sample size:** Number of observations needed per variant\n",
    "\n",
    "**Sample Size Formula (Proportion Test):**\n",
    "```\n",
    "n = (Z_Œ±/2 + Z_Œ≤)¬≤ * (p‚ÇÅ(1-p‚ÇÅ) + p‚ÇÇ(1-p‚ÇÇ)) / (p‚ÇÇ - p‚ÇÅ)¬≤\n",
    "\n",
    "Example:\n",
    "- Baseline accuracy p‚ÇÅ = 0.90\n",
    "- Expected accuracy p‚ÇÇ = 0.92 (2% improvement)\n",
    "- Œ± = 0.05 (Z_Œ±/2 = 1.96)\n",
    "- Œ≤ = 0.20 (Z_Œ≤ = 0.84, power = 80%)\n",
    "\n",
    "n = (1.96 + 0.84)¬≤ * (0.90*0.10 + 0.92*0.08) / (0.02)¬≤\n",
    "n = 7.84 * 0.1636 / 0.0004\n",
    "n = 3,203 per variant (6,406 total)\n",
    "```\n",
    "\n",
    "**Practical Guidelines:**\n",
    "- **Small effect (1-2% improvement):** Need 2,000-5,000 samples per variant\n",
    "- **Medium effect (3-5% improvement):** Need 500-2,000 samples per variant\n",
    "- **Large effect (>10% improvement):** Need 100-500 samples per variant\n",
    "\n",
    "**Sample Size Table (Œ±=0.05, power=80%):**\n",
    "\n",
    "| Baseline Accuracy | Effect Size | Required N (per variant) |\n",
    "|------------------|-------------|-------------------------|\n",
    "| 90% | 1% (91%) | 8,395 |\n",
    "| 90% | 2% (92%) | 2,149 |\n",
    "| 90% | 5% (95%) | 372 |\n",
    "| 90% | 10% (99%) | 104 |\n",
    "| 95% | 1% (96%) | 4,615 |\n",
    "| 95% | 2% (97%) | 1,186 |\n",
    "| 95% | 5% (100%) | Impossible (ceiling) |\n",
    "\n",
    "**Common Power Analysis Mistakes:**\n",
    "- ‚ùå **Underpowered test:** Running A/B test with insufficient sample size (low power)\n",
    "- ‚ùå **Ignoring effect size:** Not specifying minimum detectable effect before test\n",
    "- ‚ùå **Post-hoc power:** Calculating power after test (should be before)\n",
    "- ‚ùå **Unequal sample sizes:** Unbalanced A/B split reduces power (50/50 optimal)\n",
    "\n",
    "---\n",
    "\n",
    "### 11. **Infrastructure and Cost Considerations**\n",
    "\n",
    "**Shadow Mode Infrastructure:**\n",
    "- **Cost:** +10-20% (logging overhead, storage for shadow predictions)\n",
    "- **Latency:** +5ms (shadow prediction logged asynchronously)\n",
    "- **Storage:** 1GB per 100K predictions (assume 10KB per prediction log)\n",
    "- **Duration:** 7-14 days typical (capture weekly patterns)\n",
    "\n",
    "**A/B Testing Infrastructure:**\n",
    "- **Cost:** +50% (both models serving production traffic)\n",
    "- **Latency:** Same as single model (only one model per request)\n",
    "- **Storage:** Minimal (only log which variant served, outcome)\n",
    "- **Duration:** Hours to weeks (depends on traffic volume and required sample size)\n",
    "\n",
    "**Canary Infrastructure:**\n",
    "- **Cost:** +5-100% (depends on canary percentage, both models running)\n",
    "- **Latency:** Same as single model\n",
    "- **Storage:** Minimal (metrics aggregation, not individual predictions)\n",
    "- **Duration:** Hours to days (gradual rollout)\n",
    "\n",
    "**Blue-Green Infrastructure:**\n",
    "- **Cost:** +100% (two full environments during deployment)\n",
    "- **Latency:** Same as single model\n",
    "- **Storage:** Minimal\n",
    "- **Duration:** Minutes to hours (deploy to green, validate, switch)\n",
    "\n",
    "**Cost Optimization Strategies:**\n",
    "- **Shadow mode:** Use sampling (log 10% of predictions, not 100%)\n",
    "- **A/B testing:** Sequential testing (stop as soon as significance reached)\n",
    "- **Canary:** Aggressive rollout schedule (don't linger at 5% for days)\n",
    "- **Blue-green:** Decommission idle environment quickly (24 hours max)\n",
    "\n",
    "---\n",
    "\n",
    "### 12. **Compliance and Audit Trail**\n",
    "\n",
    "**Regulatory Requirements:**\n",
    "- **FDA (medical devices):** Validation of model changes, audit trail of deployments\n",
    "- **GDPR (EU):** Right to explanation (which model version made prediction)\n",
    "- **SR 11-7 (banking):** Model risk management, validation before production\n",
    "- **ISO 26262 (automotive):** Functional safety, validation evidence\n",
    "\n",
    "**Deployment Audit Log:**\n",
    "```json\n",
    "{\n",
    "    \"deployment_id\": \"deploy_128_20250108\",\n",
    "    \"strategy\": \"canary\",\n",
    "    \"model_version_stable\": \"v3.1.0\",\n",
    "    \"model_version_canary\": \"v3.2.0\",\n",
    "    \"start_time\": \"2025-01-08T10:00:00Z\",\n",
    "    \"stages\": [\n",
    "        {\n",
    "            \"stage\": 1,\n",
    "            \"canary_percentage\": 5,\n",
    "            \"duration_minutes\": 60,\n",
    "            \"stable_accuracy\": 0.950,\n",
    "            \"canary_accuracy\": 0.952,\n",
    "            \"health_check\": \"PASS\",\n",
    "            \"advanced\": true\n",
    "        },\n",
    "        {\n",
    "            \"stage\": 2,\n",
    "            \"canary_percentage\": 25,\n",
    "            \"duration_minutes\": 60,\n",
    "            \"stable_accuracy\": 0.949,\n",
    "            \"canary_accuracy\": 0.951,\n",
    "            \"health_check\": \"PASS\",\n",
    "            \"advanced\": true\n",
    "        }\n",
    "    ],\n",
    "    \"final_status\": \"COMPLETED\",\n",
    "    \"rollback_count\": 0,\n",
    "    \"total_requests\": 5420,\n",
    "    \"approval\": {\n",
    "        \"approved_by\": \"ml_engineer_jane\",\n",
    "        \"approval_time\": \"2025-01-08T14:30:00Z\",\n",
    "        \"validation_report_id\": \"val_128\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Compliance Checklist:**\n",
    "- [ ] Log all deployment events (start, stage advances, rollback, completion)\n",
    "- [ ] Track which model version served each prediction (for reproducibility)\n",
    "- [ ] Store validation results (accuracy, statistical tests, sample size)\n",
    "- [ ] Require approval for production deployments (human-in-the-loop)\n",
    "- [ ] Maintain rollback history (automatic rollbacks logged)\n",
    "- [ ] Generate deployment report (summary for audit)\n",
    "- [ ] Version control models (git SHA, model registry version)\n",
    "- [ ] Document decision criteria (thresholds for promotion/rollback)\n",
    "\n",
    "---\n",
    "\n",
    "### 13. **Multi-Model and Ensemble Deployments**\n",
    "\n",
    "**Shadow Mode for Ensembles:**\n",
    "- **Scenario:** Test ensemble (Random Forest + XGBoost + LightGBM) against single model\n",
    "- **Challenge:** Ensemble is 3x more expensive (three models per prediction)\n",
    "- **Solution:** Shadow mode validates accuracy improvement justifies cost\n",
    "- **Metrics:** Compare accuracy gain vs latency/cost increase\n",
    "\n",
    "**A/B Test Ensemble vs Single:**\n",
    "- **Variant A:** Single Random Forest (fast, 90% accuracy)\n",
    "- **Variant B:** Ensemble (3 models, 93% accuracy, 3x latency)\n",
    "- **Metrics:** Accuracy, latency, cost per prediction\n",
    "- **Decision:** If accuracy gain (3%) justifies latency cost (3x), promote ensemble\n",
    "\n",
    "**Canary Multi-Model:**\n",
    "- **Challenge:** Rolling out multiple models simultaneously (e.g., feature extractor + classifier)\n",
    "- **Solution:** Canary both models together (atomic deployment)\n",
    "- **Rollback:** If either model fails health check, rollback both\n",
    "\n",
    "**Blue-Green Ensemble:**\n",
    "- **Blue:** Ensemble v1 (3 models)\n",
    "- **Green:** Ensemble v2 (4 models, added neural network)\n",
    "- **Validation:** Test green ensemble on 1,000 samples\n",
    "- **Switch:** Instant traffic switch if validation passes\n",
    "\n",
    "---\n",
    "\n",
    "### 14. **Multi-Region and Global Deployments**\n",
    "\n",
    "**Regional Canary:**\n",
    "- **Strategy:** Canary in one region (e.g., US-West), expand globally if successful\n",
    "- **Benefits:** Limits blast radius to single region (timezone-based testing)\n",
    "- **Example:** Deploy to US-West (5% traffic), validate for 24 hours, expand to US-East, EU, APAC\n",
    "\n",
    "**Blue-Green Multi-Region:**\n",
    "- **Challenge:** Coordinating deployment across regions (time zones, latency)\n",
    "- **Solution:** Rolling blue-green (deploy region-by-region)\n",
    "- **Example:** Deploy to green in US-West, validate, switch US-West, then EU, then APAC\n",
    "\n",
    "**A/B Test Regional Differences:**\n",
    "- **Scenario:** Model performs differently across regions (cultural, language)\n",
    "- **Solution:** Stratified A/B test (ensure balanced regional distribution)\n",
    "- **Analysis:** Check if model B wins in all regions or only specific ones\n",
    "\n",
    "**Shadow Mode Global:**\n",
    "- **Challenge:** Logging shadow predictions across regions (storage, latency)\n",
    "- **Solution:** Regional shadow logging (store locally, aggregate centrally)\n",
    "- **Duration:** 7 days to capture regional weekly patterns\n",
    "\n",
    "---\n",
    "\n",
    "### 15. **Monitoring and Alerting**\n",
    "\n",
    "**Real-Time Alerts:**\n",
    "\n",
    "**Shadow Mode Alerts:**\n",
    "- üö® Agreement rate <90% (models disagree too often)\n",
    "- üö® Shadow latency >10ms (logging overhead too high)\n",
    "- üö® Statistical test shows degradation (shadow worse than production)\n",
    "\n",
    "**A/B Test Alerts:**\n",
    "- üö® Sample size reached (time to analyze results)\n",
    "- üö® Statistical significance achieved (can stop test early)\n",
    "- üö® Variant B accuracy drop >10% (obvious degradation)\n",
    "\n",
    "**Canary Alerts:**\n",
    "- üö® Automatic rollback triggered (health check failed)\n",
    "- üö® Canary error rate >2x stable (investigate immediately)\n",
    "- üö® Stage duration exceeded (canary stuck at 5% for >2 hours)\n",
    "\n",
    "**Blue-Green Alerts:**\n",
    "- üö® Green validation failed (deployment aborted)\n",
    "- üö® Traffic switch completed (notify team)\n",
    "- üö® Error spike after switch (consider rollback)\n",
    "\n",
    "**Alert Severity Levels:**\n",
    "- **P0 (Critical):** Automatic rollback triggered, production down\n",
    "- **P1 (High):** Health check degrading, manual investigation needed\n",
    "- **P2 (Medium):** Sample size reached, decision needed\n",
    "- **P3 (Low):** Deployment completed successfully, FYI\n",
    "\n",
    "---\n",
    "\n",
    "### 16. **Edge Cases and Failure Modes**\n",
    "\n",
    "**Shadow Mode Edge Cases:**\n",
    "- **New model crashes:** Shadow predictions fail (log error, don't block production)\n",
    "- **Data drift:** Production data different from training (shadow catches this)\n",
    "- **Seasonality:** Weekly patterns (shadow must run 7+ days)\n",
    "- **Cold start:** Shadow model slow on first prediction (warm up before logging)\n",
    "\n",
    "**A/B Test Edge Cases:**\n",
    "- **Simpson's paradox:** Model B wins overall but loses in all subgroups (stratification issue)\n",
    "- **Novelty effect:** Model B performs well initially, degrades later (run longer test)\n",
    "- **Selection bias:** User assignment not random (hash collisions, bot traffic)\n",
    "- **Multiple testing:** Running 10 A/B tests simultaneously (Bonferroni correction needed)\n",
    "\n",
    "**Canary Edge Cases:**\n",
    "- **Partial failure:** Canary works for 80% of requests, fails for 20% (subgroup analysis)\n",
    "- **Delayed degradation:** Canary passes 5%, fails at 25% (cumulative effect)\n",
    "- **Rollback loop:** Canary ‚Üí rollback ‚Üí canary ‚Üí rollback (permanent issue, investigate)\n",
    "- **Traffic imbalance:** Canary gets easier/harder samples (not truly random split)\n",
    "\n",
    "**Blue-Green Edge Cases:**\n",
    "- **Database schema change:** Green model needs new schema (backward compatibility required)\n",
    "- **Stateful services:** Websocket connections broken during switch (drain connections first)\n",
    "- **Cache warming:** Green model has cold cache (warm up before switch)\n",
    "- **Dependency failure:** Green connects to new service, service goes down (rollback)\n",
    "\n",
    "---\n",
    "\n",
    "### 17. **Testing and Validation Before Deployment**\n",
    "\n",
    "**Pre-Deployment Checklist:**\n",
    "\n",
    "**Shadow Mode:**\n",
    "- [ ] Shadow model loads successfully (no import errors)\n",
    "- [ ] Shadow prediction latency <5ms (acceptable overhead)\n",
    "- [ ] Logging pipeline tested (can write 1,000 predictions/sec)\n",
    "- [ ] Statistical test implementation verified (McNemar's test correct)\n",
    "- [ ] Disagreement analysis tested (returns top N cases)\n",
    "\n",
    "**A/B Test:**\n",
    "- [ ] User assignment deterministic (hash(user_id) consistent)\n",
    "- [ ] 50/50 traffic split verified (not 60/40 due to hash bias)\n",
    "- [ ] Sample size calculator tested (matches online calculators)\n",
    "- [ ] Statistical test verified (proportion z-test correct)\n",
    "- [ ] Confidence interval calculation validated\n",
    "\n",
    "**Canary:**\n",
    "- [ ] Gradual rollout stages defined (5%, 25%, 50%, 100%)\n",
    "- [ ] Health checks tested (accuracy, latency, error rate)\n",
    "- [ ] Rollback logic tested (can rollback to 0% instantly)\n",
    "- [ ] Minimum sample size enforced (don't advance with 10 requests)\n",
    "- [ ] Alert integration tested (notifies on rollback)\n",
    "\n",
    "**Blue-Green:**\n",
    "- [ ] Green environment deployed (model loads, responds to health checks)\n",
    "- [ ] Validation set tested (1,000 samples, accuracy + latency)\n",
    "- [ ] Load test passed (handles production traffic volume)\n",
    "- [ ] Rollback tested (can switch back to blue instantly)\n",
    "- [ ] Database compatibility verified (green reads blue's data)\n",
    "\n",
    "---\n",
    "\n",
    "### 18. **Documentation and Knowledge Transfer**\n",
    "\n",
    "**Deployment Runbook:**\n",
    "\n",
    "**Shadow Mode Runbook:**\n",
    "1. Deploy shadow model to logging pipeline\n",
    "2. Enable shadow logging (configuration change)\n",
    "3. Monitor logging latency (should be <5ms)\n",
    "4. Wait 7-14 days (capture weekly patterns)\n",
    "5. Run statistical analysis (McNemar's test)\n",
    "6. Generate validation report\n",
    "7. Review with team (go/no-go decision)\n",
    "8. If approved, proceed to A/B test or canary\n",
    "\n",
    "**A/B Test Runbook:**\n",
    "1. Calculate required sample size (power analysis)\n",
    "2. Configure traffic split (50/50)\n",
    "3. Enable A/B test (route users to variants)\n",
    "4. Monitor sample size (wait until required N reached)\n",
    "5. Run statistical test (proportion z-test)\n",
    "6. Calculate confidence interval\n",
    "7. Make decision (promote variant B or reject)\n",
    "8. Document results (report for future reference)\n",
    "\n",
    "**Canary Runbook:**\n",
    "1. Define rollout stages (e.g., 5%, 25%, 50%, 100%)\n",
    "2. Set health check thresholds (accuracy, latency, error rate)\n",
    "3. Configure automated rollback triggers\n",
    "4. Start canary at 5% traffic\n",
    "5. Monitor health checks (wait for minimum sample size)\n",
    "6. If healthy, advance to 25% (repeat for each stage)\n",
    "7. If unhealthy, automatic rollback to 0%\n",
    "8. Document deployment (successful or rolled back)\n",
    "\n",
    "**Blue-Green Runbook:**\n",
    "1. Deploy new model to green environment\n",
    "2. Run smoke tests (basic functionality)\n",
    "3. Validate on 1,000 samples (accuracy + latency)\n",
    "4. Run load test (production traffic volume)\n",
    "5. Switch traffic to green (instant change)\n",
    "6. Monitor for 10 minutes (watch for errors)\n",
    "7. If issues, rollback to blue (instant switch)\n",
    "8. Keep blue running 24 hours (rollback option)\n",
    "9. Decommission blue after 24 hours\n",
    "\n",
    "---\n",
    "\n",
    "### 19. **Advanced Topics and Future Directions**\n",
    "\n",
    "**Multi-Armed Bandits:**\n",
    "- **Limitation of A/B:** Fixed 50/50 split wastes traffic on losing variant\n",
    "- **Bandit solution:** Dynamically adjust traffic (more to winning variant)\n",
    "- **Example:** Start 50/50, after 1,000 samples shift to 70/30 if B winning\n",
    "- **Trade-off:** Faster to winner but less statistical rigor (exploration vs exploitation)\n",
    "\n",
    "**Contextual Bandits:**\n",
    "- **Scenario:** Model performance varies by context (user segment, device type)\n",
    "- **Solution:** Per-context traffic allocation (mobile users ‚Üí model A, desktop ‚Üí model B)\n",
    "- **Example:** Younger users prefer neural network, older users prefer linear model\n",
    "\n",
    "**Bayesian A/B Testing:**\n",
    "- **Traditional:** Frequentist hypothesis testing (p-value, fixed sample size)\n",
    "- **Bayesian:** Posterior probability (probability that B beats A)\n",
    "- **Advantage:** Can stop test early (when posterior probability >95%)\n",
    "- **Example:** After 500 samples, P(accuracy_B > accuracy_A) = 98% ‚Üí stop test\n",
    "\n",
    "**Progressive Delivery:**\n",
    "- **Concept:** Combine canary + feature flags + monitoring for fine-grained control\n",
    "- **Example:** Enable new model for premium users first (5%), then free users (95%)\n",
    "- **Tools:** LaunchDarkly, Split.io (feature flag platforms)\n",
    "\n",
    "**Shadow Mode at Scale:**\n",
    "- **Challenge:** Logging 1M predictions/day = 10GB/day storage\n",
    "- **Solution:** Sampling (log 10% of predictions), aggregation (log only summary stats)\n",
    "- **Example:** Log 100K predictions (10% sample), saves 90% storage\n",
    "\n",
    "---\n",
    "\n",
    "### 20. **Production Deployment Decision Tree**\n",
    "\n",
    "```\n",
    "START: Need to deploy new model\n",
    "\n",
    "Q1: Is this a new model architecture or high-risk change?\n",
    "‚îú‚îÄ YES ‚Üí Start with Shadow Mode (7-14 days)\n",
    "‚îÇ   ‚îú‚îÄ Shadow shows improvement ‚Üí Proceed to Q2\n",
    "‚îÇ   ‚îî‚îÄ Shadow shows degradation ‚Üí Reject deployment, retrain model\n",
    "‚îÇ\n",
    "‚îî‚îÄ NO ‚Üí Proceed to Q2\n",
    "\n",
    "Q2: Do you need statistical proof of improvement?\n",
    "‚îú‚îÄ YES ‚Üí Run A/B Test\n",
    "‚îÇ   ‚îú‚îÄ Calculate required sample size (power analysis)\n",
    "‚îÇ   ‚îú‚îÄ Run test until sample size reached\n",
    "‚îÇ   ‚îú‚îÄ A/B shows significant improvement ‚Üí Proceed to Q3\n",
    "‚îÇ   ‚îî‚îÄ A/B shows no difference or degradation ‚Üí Reject deployment\n",
    "‚îÇ\n",
    "‚îî‚îÄ NO ‚Üí Proceed to Q3\n",
    "\n",
    "Q3: Is zero downtime critical?\n",
    "‚îú‚îÄ YES ‚Üí Use Blue-Green Deployment\n",
    "‚îÇ   ‚îú‚îÄ Deploy to green, validate, switch traffic\n",
    "‚îÇ   ‚îú‚îÄ If issues, instant rollback to blue\n",
    "‚îÇ   ‚îî‚îÄ Success ‚Üí Deployment complete\n",
    "‚îÇ\n",
    "‚îî‚îÄ NO ‚Üí Proceed to Q4\n",
    "\n",
    "Q4: Can you tolerate gradual rollout over hours/days?\n",
    "‚îú‚îÄ YES ‚Üí Use Canary Deployment\n",
    "‚îÇ   ‚îú‚îÄ Start at 5%, monitor health, advance to 25%, 50%, 100%\n",
    "‚îÇ   ‚îú‚îÄ Automated rollback if health checks fail\n",
    "‚îÇ   ‚îî‚îÄ Success ‚Üí Deployment complete\n",
    "‚îÇ\n",
    "‚îî‚îÄ NO ‚Üí Direct deployment (not recommended for production)\n",
    "```\n",
    "\n",
    "**Example Decision Paths:**\n",
    "\n",
    "**Path 1: High-Risk New Architecture**\n",
    "- Shadow mode (14 days) ‚Üí A/B test (7 days) ‚Üí Canary (2 days) ‚Üí Full rollout\n",
    "- Total time: 23 days\n",
    "- Risk: Minimal (validated at every stage)\n",
    "\n",
    "**Path 2: Incremental Model Improvement**\n",
    "- A/B test (3 days) ‚Üí Canary (1 day) ‚Üí Full rollout\n",
    "- Total time: 4 days\n",
    "- Risk: Low (statistical validation + gradual rollout)\n",
    "\n",
    "**Path 3: Critical Uptime Service**\n",
    "- Blue-Green (validate green, instant switch)\n",
    "- Total time: 1 hour\n",
    "- Risk: Medium (pre-validated but instant switch)\n",
    "\n",
    "**Path 4: Low-Risk Bug Fix**\n",
    "- Canary (5% ‚Üí 25% ‚Üí 100% over 4 hours)\n",
    "- Total time: 4 hours\n",
    "- Risk: Low (gradual rollout with automated rollback)\n",
    "\n",
    "---\n",
    "\n",
    "### 21. **Key Takeaways Summary**\n",
    "\n",
    "‚úÖ **Shadow Mode:** Zero-risk validation, runs parallel to production for days/weeks, uses McNemar's test for paired predictions\n",
    "\n",
    "‚úÖ **A/B Testing:** Statistical proof of improvement, requires sample size calculation (power analysis), uses proportion z-test\n",
    "\n",
    "‚úÖ **Canary Deployment:** Gradual rollout (5% ‚Üí 100%), automated rollback on health check failure, limits blast radius\n",
    "\n",
    "‚úÖ **Blue-Green Deployment:** Instant traffic switch, zero downtime, requires 2x infrastructure during deployment\n",
    "\n",
    "‚úÖ **Statistical Rigor:** Always calculate required sample size, use appropriate statistical test (paired vs independent), report confidence intervals\n",
    "\n",
    "‚úÖ **Monitoring:** Track accuracy, latency (p95, p99), error rate, traffic distribution, rollback count\n",
    "\n",
    "‚úÖ **Automation:** Automated rollback triggers (accuracy drop, latency spike, error rate), no manual intervention required\n",
    "\n",
    "‚úÖ **Compliance:** Audit trail of all deployments, version tracking, approval workflows, validation reports\n",
    "\n",
    "‚úÖ **Post-Silicon:** Shadow mode for yield prediction, A/B test for wafer map classification, canary for binning updates\n",
    "\n",
    "‚úÖ **Production Checklist:** Pre-deployment validation, health checks, rollback plan, monitoring dashboards, alert integration\n",
    "\n",
    "---\n",
    "\n",
    "### 22. **Next Steps in Learning**\n",
    "\n",
    "**Notebook 129: Advanced MLOps - Feature Stores & Monitoring**\n",
    "- Feature store architecture (offline + online serving)\n",
    "- Data quality monitoring (schema validation, distribution drift)\n",
    "- Model performance monitoring (accuracy degradation, concept drift)\n",
    "\n",
    "**Notebook 130: ML Observability & Debugging**\n",
    "- Distributed tracing for ML pipelines\n",
    "- Model debugging (SHAP, LIME explanations)\n",
    "- Performance profiling (latency bottlenecks, memory leaks)\n",
    "\n",
    "**Notebook 131: Container Orchestration for ML**\n",
    "- Kubernetes for model serving (horizontal scaling)\n",
    "- Docker multi-stage builds (optimize image size)\n",
    "- Service mesh (Istio for traffic management)\n",
    "\n",
    "**Beyond MLOps:**\n",
    "- **Federated Learning:** Train models across devices without centralizing data\n",
    "- **Edge Deployment:** Deploy models to IoT devices (model compression, quantization)\n",
    "- **AutoML Production:** Automated model selection and deployment pipelines\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've mastered safe model deployment strategies for production ML systems.** üéâ\n",
    "\n",
    "You now understand:\n",
    "- ‚úÖ When to use each deployment strategy (shadow, A/B, canary, blue-green)\n",
    "- ‚úÖ Statistical testing for model comparison (McNemar's test, proportion z-test)\n",
    "- ‚úÖ Automated rollback logic (health checks, thresholds, alerts)\n",
    "- ‚úÖ Sample size calculation (power analysis for A/B tests)\n",
    "- ‚úÖ Post-silicon validation applications (yield, binning, test time, wafer maps)\n",
    "- ‚úÖ Production-ready implementation (logging, monitoring, compliance)\n",
    "\n",
    "**You're now equipped to deploy ML models safely in production environments, with statistical rigor and automated safety nets.** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f91b9ab",
   "metadata": {},
   "source": [
    "## üîë Key Takeaways\n",
    "\n",
    "**When to Use Shadow Mode:**\n",
    "- Validating new model before production rollout\n",
    "- Testing infrastructure changes without user impact\n",
    "- Comparing multiple model versions\n",
    "- Gathering real-world performance data safely\n",
    "\n",
    "**Limitations:**\n",
    "- Doubles compute cost (two models run simultaneously)\n",
    "- Requires infrastructure for dual execution\n",
    "- Delayed feedback (metrics analyzed post-deployment)\n",
    "- Storage costs for shadow predictions\n",
    "\n",
    "**Alternatives:**\n",
    "- A/B testing (split traffic between models)\n",
    "- Canary deployment (gradual rollout to subset)\n",
    "- Blue-green deployment (instant switch with rollback)\n",
    "- Offline validation (historical data replay)\n",
    "\n",
    "**Best Practices:**\n",
    "- Monitor latency impact (shadow should not slow primary)\n",
    "- Set automatic shadow retirement thresholds\n",
    "- Log disagreements for root cause analysis\n",
    "- Use asynchronous shadow inference to minimize latency\n",
    "- Implement circuit breakers for shadow failures\n",
    "\n",
    "**Next Steps:**\n",
    "- 154: Model Monitoring & Observability (analyze shadow metrics)\n",
    "- 126: Continuous Training (automated shadow retraining)\n",
    "- 106: A/B Testing ML Models (compare with A/B approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2fb16",
   "metadata": {},
   "source": [
    "## üìä Diagnostic Checks Summary\n",
    "\n",
    "**Implementation Checklist:**\n",
    "- ‚úÖ Dual model execution (primary + shadow)\n",
    "- ‚úÖ Asynchronous shadow inference (no latency impact)\n",
    "- ‚úÖ Prediction disagreement tracking\n",
    "- ‚úÖ Performance metrics comparison (accuracy, latency)\n",
    "- ‚úÖ Automatic promotion/retirement logic\n",
    "- ‚úÖ Post-silicon use cases (yield model validation, test time optimization, quality prediction)\n",
    "- ‚úÖ Real-world projects with ROI ($18M-$350M/year)\n",
    "\n",
    "**Quality Metrics Achieved:**\n",
    "- Shadow latency overhead: <5% increase\n",
    "- Prediction storage: 90 days retention\n",
    "- Promotion threshold: 95% confidence in improvement\n",
    "- Business impact: 80% reduction in bad deployments"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
