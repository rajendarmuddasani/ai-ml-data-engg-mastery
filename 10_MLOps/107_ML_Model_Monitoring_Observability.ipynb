{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3b64c2",
   "metadata": {},
   "source": [
    "# 107: ML Model Monitoring & Observability\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** drift types: data drift, concept drift, prediction drift, and target drift\n",
    "- **Implement** statistical tests for detecting distribution changes (KS test, PSI, Jensen-Shannon)\n",
    "- **Build** real-time monitoring dashboards tracking model health metrics\n",
    "- **Apply** monitoring to semiconductor yield prediction models in production\n",
    "- **Evaluate** when to retrain, roll back, or escalate based on drift severity\n",
    "\n",
    "## üìö What is ML Model Monitoring?\n",
    "\n",
    "ML model monitoring is the continuous observation of deployed models to detect performance degradation before it impacts business outcomes. Unlike traditional software where behavior is deterministic, ML models degrade silently as the world changes‚Äînew device types appear, test equipment calibrates differently, manufacturing processes evolve. Monitoring catches these shifts early, triggering alerts when predictions become unreliable.\n",
    "\n",
    "Effective monitoring tracks three layers: **data layer** (input feature distributions), **prediction layer** (output distribution and confidence), and **outcome layer** (ground truth labels when available). Data drift can occur without impacting performance (benign shift), or it can signal concept drift where feature-target relationships change. For example, a yield prediction model trained on 7nm devices may fail catastrophically on 5nm devices without proper monitoring.\n",
    "\n",
    "In semiconductor manufacturing, model monitoring is mission-critical because errors cost millions. A drifting binning algorithm might misclassify premium devices as scrap ($10K loss per wafer), while going undetected for weeks in manual QA. Real-time monitoring with automatic rollback safeguards ensures production stability while enabling continuous model improvements.\n",
    "\n",
    "**Why ML Model Monitoring?**\n",
    "- ‚úÖ **Early Detection**: Catch drift before accuracy drops 20% and costs spike\n",
    "- ‚úÖ **Root Cause Analysis**: Isolate which features drifted (e.g., new test equipment calibration)\n",
    "- ‚úÖ **Automated Response**: Auto-retrain, rollback to baseline, or alert engineers\n",
    "- ‚úÖ **Compliance**: Audit trail for regulated industries (automotive, medical devices)\n",
    "- ‚úÖ **Continuous Learning**: Data-driven retraining decisions vs arbitrary schedules\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Use Case 1: Yield Prediction Model Drift**\n",
    "- **Scenario**: New wafer fab process (7nm ‚Üí 5nm) changes parametric distributions\n",
    "- **Monitoring**: Track Vdd, Idd, frequency distributions with PSI (Population Stability Index)\n",
    "- **Alert**: PSI > 0.25 for Vdd ‚Üí model predictions unreliable\n",
    "- **Action**: Retrain on last 30 days of 5nm data, A/B test before full deployment\n",
    "- **Impact**: Prevented $2M in false rejects by catching drift 3 days after process change\n",
    "\n",
    "**Use Case 2: Test Equipment Calibration Drift**\n",
    "- **Scenario**: Tester recalibration shifts current measurements by +2mA\n",
    "- **Monitoring**: Kolmogorov-Smirnov test on Idd distribution (p < 0.001 ‚Üí drift detected)\n",
    "- **Alert**: Data drift in Idd but model accuracy unchanged (benign drift)\n",
    "- **Action**: Document calibration change, update training data normalization\n",
    "- **Impact**: Avoided unnecessary model retrain, saved 2 weeks of engineering time\n",
    "\n",
    "**Use Case 3: Seasonal Pattern Shift**\n",
    "- **Scenario**: Summer temperature increase affects test chamber conditions\n",
    "- **Monitoring**: Track prediction confidence (model uncertainty increases)\n",
    "- **Alert**: Mean prediction confidence drops from 0.92 to 0.78\n",
    "- **Action**: Add temperature as explicit feature, retrain quarterly model\n",
    "- **Impact**: Maintained 95% accuracy through seasonal variation\n",
    "\n",
    "**Use Case 4: Concept Drift in Binning Logic**\n",
    "- **Scenario**: Customer requirements change (stricter specs for automotive market)\n",
    "- **Monitoring**: Target distribution shifts (more BIN2, fewer BIN1)\n",
    "- **Alert**: Label drift detected, feature-target correlation weakens\n",
    "- **Action**: Collect 10K new labels under new specs, full model retrain\n",
    "- **Impact**: Prevented shipping out-of-spec devices, avoided customer returns\n",
    "\n",
    "## üîÑ Monitoring Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Production Traffic] --> B[Log Predictions]\n",
    "    B --> C[Feature Store]\n",
    "    C --> D[Monitoring Service]\n",
    "    \n",
    "    D --> E[Data Drift Detection]\n",
    "    D --> F[Prediction Drift Detection]\n",
    "    D --> G[Performance Monitoring]\n",
    "    \n",
    "    E --> H{KS Test / PSI}\n",
    "    F --> I{Confidence Drop?}\n",
    "    G --> J{Accuracy Drop?}\n",
    "    \n",
    "    H -->|p < 0.05| K[Data Drift Alert]\n",
    "    I -->|Yes| L[Prediction Drift Alert]\n",
    "    J -->|>5% Drop| M[Performance Alert]\n",
    "    \n",
    "    K --> N{Severity}\n",
    "    L --> N\n",
    "    M --> N\n",
    "    \n",
    "    N -->|Critical| O[Auto Rollback]\n",
    "    N -->|High| P[Trigger Retrain]\n",
    "    N -->|Medium| Q[Engineer Review]\n",
    "    N -->|Low| R[Log & Monitor]\n",
    "    \n",
    "    O --> S[Notify Oncall]\n",
    "    P --> T[Retrain Pipeline]\n",
    "    Q --> U[Dashboard Alert]\n",
    "    R --> V[Metrics DB]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style O fill:#ffe1e1\n",
    "    style P fill:#fff5e1\n",
    "    style V fill:#e1ffe1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **041**: Model Evaluation - Understanding baseline metrics\n",
    "- **106**: A/B Testing - Comparing model versions statistically\n",
    "- **031**: Time Series - Temporal patterns and seasonality\n",
    "\n",
    "**This Notebook (107):**\n",
    "- Drift detection algorithms (KS test, PSI, JS divergence)\n",
    "- Real-time monitoring implementation\n",
    "- Alert thresholds and escalation policies\n",
    "- Root cause analysis for drift\n",
    "- Automated retraining triggers\n",
    "\n",
    "**Next Steps:**\n",
    "- **108**: Feature Stores - Centralized feature management for consistency\n",
    "- **109**: ML Pipelines - Automated retraining and deployment\n",
    "- **131**: Cloud Deployment - Scalable monitoring infrastructure\n",
    "\n",
    "---\n",
    "\n",
    "Let's build production-grade monitoring systems! üîç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf963423",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae04df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp, wasserstein_distance\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Monitoring environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a50ee72",
   "metadata": {},
   "source": [
    "## 2. Generate Training and Production Data\n",
    "\n",
    "**Purpose:** Simulate model trained on historical data, then deployed to production with distribution shifts.\n",
    "\n",
    "**Key Points:**\n",
    "- **Training data**: Historical semiconductor test data (stable period)\n",
    "- **Production data**: Live data with gradual drift (new process, equipment changes)\n",
    "- **Drift types**: Covariate shift (X changes), concept drift (X‚Üíy relationship changes)\n",
    "- **Why this matters**: Real production always differs from training‚Äîmonitoring quantifies how much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6837d278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data (stable period, 6 months historical)\n",
    "n_train = 5000\n",
    "np.random.seed(42)\n",
    "\n",
    "# Feature distributions (7nm process)\n",
    "train_vdd = np.random.normal(1.2, 0.08, n_train)\n",
    "train_idd = np.random.normal(50, 8, n_train)\n",
    "train_freq = np.random.normal(2000, 150, n_train)\n",
    "train_temp = np.random.normal(85, 12, n_train)\n",
    "train_vth = np.random.normal(0.4, 0.03, n_train)\n",
    "\n",
    "# True relationship (training period)\n",
    "train_power = train_vdd * train_idd\n",
    "train_yield = (\n",
    "    100 - 0.35 * train_power + 12 * train_vth - 0.01 * train_temp * train_freq / 1000\n",
    "    + np.random.normal(0, 2.5, n_train)\n",
    ")\n",
    "train_yield = np.clip(train_yield, 60, 100)\n",
    "\n",
    "df_train = pd.DataFrame({\n",
    "    'vdd': train_vdd,\n",
    "    'idd': train_idd,\n",
    "    'freq': train_freq,\n",
    "    'temp': train_temp,\n",
    "    'vth': train_vth,\n",
    "    'yield_pct': train_yield,\n",
    "    'period': 'train'\n",
    "})\n",
    "\n",
    "# Production data Week 1-4 (NO DRIFT - baseline performance)\n",
    "n_prod_stable = 1000\n",
    "prod_stable_vdd = np.random.normal(1.2, 0.08, n_prod_stable)\n",
    "prod_stable_idd = np.random.normal(50, 8, n_prod_stable)\n",
    "prod_stable_freq = np.random.normal(2000, 150, n_prod_stable)\n",
    "prod_stable_temp = np.random.normal(85, 12, n_prod_stable)\n",
    "prod_stable_vth = np.random.normal(0.4, 0.03, n_prod_stable)\n",
    "\n",
    "prod_stable_power = prod_stable_vdd * prod_stable_idd\n",
    "prod_stable_yield = (\n",
    "    100 - 0.35 * prod_stable_power + 12 * prod_stable_vth \n",
    "    - 0.01 * prod_stable_temp * prod_stable_freq / 1000\n",
    "    + np.random.normal(0, 2.5, n_prod_stable)\n",
    ")\n",
    "prod_stable_yield = np.clip(prod_stable_yield, 60, 100)\n",
    "\n",
    "df_prod_stable = pd.DataFrame({\n",
    "    'vdd': prod_stable_vdd,\n",
    "    'idd': prod_stable_idd,\n",
    "    'freq': prod_stable_freq,\n",
    "    'temp': prod_stable_temp,\n",
    "    'vth': prod_stable_vth,\n",
    "    'yield_pct': prod_stable_yield,\n",
    "    'period': 'prod_stable'\n",
    "})\n",
    "\n",
    "# Production data Week 5-8 (DATA DRIFT - process change to 5nm)\n",
    "n_prod_drift = 1000\n",
    "prod_drift_vdd = np.random.normal(1.15, 0.07, n_prod_drift)  # Lower voltage\n",
    "prod_drift_idd = np.random.normal(45, 7, n_prod_drift)      # Lower current\n",
    "prod_drift_freq = np.random.normal(2200, 160, n_prod_drift) # Higher freq\n",
    "prod_drift_temp = np.random.normal(85, 12, n_prod_drift)\n",
    "prod_drift_vth = np.random.normal(0.38, 0.025, n_prod_drift) # Lower Vth\n",
    "\n",
    "# CONCEPT DRIFT: Relationship changes (new process physics)\n",
    "prod_drift_power = prod_drift_vdd * prod_drift_idd\n",
    "prod_drift_yield = (\n",
    "    100 - 0.40 * prod_drift_power + 15 * prod_drift_vth  # Different coefficients!\n",
    "    - 0.012 * prod_drift_temp * prod_drift_freq / 1000\n",
    "    + np.random.normal(0, 3.0, n_prod_drift)  # Higher noise\n",
    ")\n",
    "prod_drift_yield = np.clip(prod_drift_yield, 60, 100)\n",
    "\n",
    "df_prod_drift = pd.DataFrame({\n",
    "    'vdd': prod_drift_vdd,\n",
    "    'idd': prod_drift_idd,\n",
    "    'freq': prod_drift_freq,\n",
    "    'temp': prod_drift_temp,\n",
    "    'vth': prod_drift_vth,\n",
    "    'yield_pct': prod_drift_yield,\n",
    "    'period': 'prod_drift'\n",
    "})\n",
    "\n",
    "print(f\"Training data: {len(df_train)} samples (6 months historical)\")\n",
    "print(f\"Production stable: {len(df_prod_stable)} samples (Week 1-4, no drift)\")\n",
    "print(f\"Production drift: {len(df_prod_drift)} samples (Week 5-8, 5nm process)\\n\")\n",
    "\n",
    "print(\"Feature statistics comparison:\")\n",
    "print(f\"\\nVdd (voltage):\")\n",
    "print(f\"  Train:  Œº={df_train['vdd'].mean():.3f}, œÉ={df_train['vdd'].std():.3f}\")\n",
    "print(f\"  Stable: Œº={df_prod_stable['vdd'].mean():.3f}, œÉ={df_prod_stable['vdd'].std():.3f}\")\n",
    "print(f\"  Drift:  Œº={df_prod_drift['vdd'].mean():.3f}, œÉ={df_prod_drift['vdd'].std():.3f} ‚ö†Ô∏è SHIFTED\")\n",
    "\n",
    "print(f\"\\nIdd (current):\")\n",
    "print(f\"  Train:  Œº={df_train['idd'].mean():.3f}, œÉ={df_train['idd'].std():.3f}\")\n",
    "print(f\"  Stable: Œº={df_prod_stable['idd'].mean():.3f}, œÉ={df_prod_stable['idd'].std():.3f}\")\n",
    "print(f\"  Drift:  Œº={df_prod_drift['idd'].mean():.3f}, œÉ={df_prod_drift['idd'].std():.3f} ‚ö†Ô∏è SHIFTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b2c31d",
   "metadata": {},
   "source": [
    "## 3. Train Baseline Model\n",
    "\n",
    "**Purpose:** Train yield prediction model on historical data (baseline for drift detection).\n",
    "\n",
    "**Key Points:**\n",
    "- **Training period**: 6 months historical STDF data\n",
    "- **Features**: Vdd, Idd, frequency, temperature, Vth\n",
    "- **Model**: Random Forest (production baseline)\n",
    "- **Why this matters**: Need baseline performance metrics to detect degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ac8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline Random Forest model\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Baseline predictions\n",
    "y_pred_baseline = rf_model.predict(X_test_scaled)\n",
    "baseline_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "baseline_f1 = f1_score(y_test, y_pred_baseline, average='weighted')\n",
    "\n",
    "print(f\"Baseline Model Performance:\")\n",
    "print(f\"  Accuracy: {baseline_accuracy:.4f}\")\n",
    "print(f\"  F1 Score: {baseline_f1:.4f}\")\n",
    "\n",
    "# Feature importance (for monitoring which features drift most)\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 5 Most Important Features:\")\n",
    "print(feature_importance_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ae6ed2",
   "metadata": {},
   "source": [
    "## 4. Simulate Production Data Drift\n",
    "\n",
    "**Purpose:** Generate new production data with distribution shifts to simulate real-world drift scenarios.\n",
    "\n",
    "**Key Points:**\n",
    "- **Covariate Shift**: Feature distributions change (e.g., higher Vdd voltages due to process variation)\n",
    "- **Concept Drift**: Relationship between features and target changes (e.g., new failure modes emerge)\n",
    "- **Realistic Scenarios**: In post-silicon, drift happens due to process improvements, new test equipment, or product variants\n",
    "\n",
    "**Why This Matters:** Production data rarely matches training data perfectly. Monitoring drift prevents silent model degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09633f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate production data with drift\n",
    "np.random.seed(123)\n",
    "\n",
    "# Production data: higher voltage (process drift), different frequency distribution\n",
    "production_vdd = np.random.normal(1.25, 0.08, 200)  # Higher mean (was 1.2)\n",
    "production_idd = np.random.normal(155, 28, 200)  # Higher mean (was 150)\n",
    "production_freq = np.random.normal(2900, 180, 200)  # Lower mean (was 3000)\n",
    "production_power = production_vdd * production_idd / 1000\n",
    "production_temp = np.random.normal(82, 9, 200)  # Higher mean (was 80)\n",
    "\n",
    "# Concept drift: new failure mode (high temp + high power = fail more often)\n",
    "production_pass_prob = 0.65 + 0.15 * (production_vdd - 1.1) / 0.3 - 0.25 * (production_temp > 85)\n",
    "production_pass = (np.random.random(200) < production_pass_prob).astype(int)\n",
    "\n",
    "production_df = pd.DataFrame({\n",
    "    'Vdd': production_vdd,\n",
    "    'Idd': production_idd,\n",
    "    'Frequency': production_freq,\n",
    "    'Power': production_power,\n",
    "    'Temperature': production_temp,\n",
    "    'Pass': production_pass\n",
    "})\n",
    "\n",
    "# Model predictions on production data\n",
    "X_production = production_df[feature_cols].values\n",
    "X_production_scaled = scaler.transform(X_production)\n",
    "y_production_pred = rf_model.predict(X_production_scaled)\n",
    "production_accuracy = accuracy_score(production_df['Pass'], y_production_pred)\n",
    "\n",
    "print(f\"Production Model Performance:\")\n",
    "print(f\"  Accuracy: {production_accuracy:.4f} (Baseline: {baseline_accuracy:.4f})\")\n",
    "print(f\"  Degradation: {(baseline_accuracy - production_accuracy):.4f}\")\n",
    "print(f\"\\n‚ö†Ô∏è Model performance dropped! Investigating drift...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9121b8c",
   "metadata": {},
   "source": [
    "## 5. Kolmogorov-Smirnov (KS) Test for Feature Drift\n",
    "\n",
    "**Purpose:** Detect feature distribution shifts using statistical hypothesis testing.\n",
    "\n",
    "**Key Points:**\n",
    "- **KS Statistic**: Maximum vertical distance between cumulative distribution functions (CDFs)\n",
    "- **P-value < 0.05**: Statistically significant drift detected (reject null hypothesis of same distribution)\n",
    "- **Per-Feature Monitoring**: Track which specific features are drifting\n",
    "- **Actionable Threshold**: KS statistic > 0.2 often indicates meaningful drift in production\n",
    "\n",
    "**Why This Matters:** Early drift detection prevents deploying models on out-of-distribution data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa64541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# KS test for each feature\n",
    "ks_results = []\n",
    "for col in feature_cols:\n",
    "    train_values = X_train[col].values\n",
    "    prod_values = production_df[col].values\n",
    "    ks_stat, p_value = ks_2samp(train_values, prod_values)\n",
    "    \n",
    "    ks_results.append({\n",
    "        'Feature': col,\n",
    "        'KS_Statistic': ks_stat,\n",
    "        'P_Value': p_value,\n",
    "        'Drift_Detected': 'Yes' if p_value < 0.05 else 'No',\n",
    "        'Severity': 'High' if ks_stat > 0.2 else ('Medium' if ks_stat > 0.1 else 'Low')\n",
    "    })\n",
    "\n",
    "ks_df = pd.DataFrame(ks_results)\n",
    "print(\"Feature Drift Detection (Kolmogorov-Smirnov Test):\\n\")\n",
    "print(ks_df.to_string(index=False))\n",
    "print(f\"\\nüö® Drifted Features: {ks_df[ks_df['Drift_Detected'] == 'Yes']['Feature'].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc448ed5",
   "metadata": {},
   "source": [
    "## 6. Population Stability Index (PSI)\n",
    "\n",
    "**Purpose:** Quantify distribution shift magnitude for continuous monitoring.\n",
    "\n",
    "**Key Points:**\n",
    "- **PSI Formula**: `Œ£[(Actual% - Expected%) √ó ln(Actual% / Expected%)]`\n",
    "- **Threshold Guidelines**: PSI < 0.1 (stable), 0.1-0.25 (moderate drift), > 0.25 (severe drift)\n",
    "- **Industry Standard**: Widely used in credit scoring and risk modeling to monitor feature stability\n",
    "- **Binning Strategy**: Divide feature range into 10 bins based on training data quantiles\n",
    "\n",
    "**Why This Matters:** PSI provides a single metric to track drift severity over time, enabling automated alerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9373ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psi(expected, actual, bins=10):\n",
    "    \"\"\"Calculate Population Stability Index (PSI) for a feature.\"\"\"\n",
    "    # Create bins based on expected (training) data quantiles\n",
    "    breakpoints = np.percentile(expected, np.linspace(0, 100, bins + 1))\n",
    "    breakpoints[-1] += 0.0001  # Ensure max value is included\n",
    "    \n",
    "    # Count observations in each bin\n",
    "    expected_counts = np.histogram(expected, bins=breakpoints)[0]\n",
    "    actual_counts = np.histogram(actual, bins=breakpoints)[0]\n",
    "    \n",
    "    # Convert to percentages (add small epsilon to avoid log(0))\n",
    "    expected_pct = (expected_counts + 1e-6) / len(expected)\n",
    "    actual_pct = (actual_counts + 1e-6) / len(actual)\n",
    "    \n",
    "    # PSI formula: Œ£[(Actual% - Expected%) √ó ln(Actual% / Expected%)]\n",
    "    psi_values = (actual_pct - expected_pct) * np.log(actual_pct / expected_pct)\n",
    "    psi = np.sum(psi_values)\n",
    "    \n",
    "    return psi\n",
    "\n",
    "# Calculate PSI for each feature\n",
    "psi_results = []\n",
    "for col in feature_cols:\n",
    "    psi = calculate_psi(X_train[col].values, production_df[col].values)\n",
    "    \n",
    "    if psi < 0.1:\n",
    "        status = 'Stable'\n",
    "    elif psi < 0.25:\n",
    "        status = 'Moderate Drift'\n",
    "    else:\n",
    "        status = 'Severe Drift'\n",
    "    \n",
    "    psi_results.append({'Feature': col, 'PSI': psi, 'Status': status})\n",
    "\n",
    "psi_df = pd.DataFrame(psi_results)\n",
    "print(\"Population Stability Index (PSI) Analysis:\\n\")\n",
    "print(psi_df.to_string(index=False))\n",
    "print(f\"\\n‚ö†Ô∏è Features with Drift: {psi_df[psi_df['PSI'] > 0.1]['Feature'].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1897f8",
   "metadata": {},
   "source": [
    "## 7. Jensen-Shannon Divergence\n",
    "\n",
    "**Purpose:** Measure distributional similarity using information theory (symmetric version of KL divergence).\n",
    "\n",
    "**Key Points:**\n",
    "- **Range**: 0 (identical distributions) to 1 (completely different)\n",
    "- **Symmetric**: JS(P||Q) = JS(Q||P), unlike KL divergence\n",
    "- **Smooth Metric**: Finite even when distributions have non-overlapping support\n",
    "- **Threshold**: JS > 0.1 indicates noticeable drift, > 0.3 indicates severe drift\n",
    "\n",
    "**Why This Matters:** JS divergence is more robust than KL divergence and provides intuitive drift severity scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86849769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "def calculate_js_divergence(expected, actual, bins=20):\n",
    "    \"\"\"Calculate Jensen-Shannon divergence between two distributions.\"\"\"\n",
    "    # Create bins\n",
    "    combined = np.concatenate([expected, actual])\n",
    "    breakpoints = np.percentile(combined, np.linspace(0, 100, bins + 1))\n",
    "    \n",
    "    # Histogram counts\n",
    "    expected_counts = np.histogram(expected, bins=breakpoints)[0]\n",
    "    actual_counts = np.histogram(actual, bins=breakpoints)[0]\n",
    "    \n",
    "    # Normalize to probabilities (add epsilon to avoid division by zero)\n",
    "    expected_prob = (expected_counts + 1e-10) / expected_counts.sum()\n",
    "    actual_prob = (actual_counts + 1e-10) / actual_counts.sum()\n",
    "    \n",
    "    # JS divergence (returns value between 0 and 1)\n",
    "    js_div = jensenshannon(expected_prob, actual_prob)\n",
    "    \n",
    "    return js_div\n",
    "\n",
    "# Calculate JS divergence for each feature\n",
    "js_results = []\n",
    "for col in feature_cols:\n",
    "    js_div = calculate_js_divergence(X_train[col].values, production_df[col].values)\n",
    "    \n",
    "    if js_div < 0.1:\n",
    "        severity = 'Low'\n",
    "    elif js_div < 0.3:\n",
    "        severity = 'Medium'\n",
    "    else:\n",
    "        severity = 'High'\n",
    "    \n",
    "    js_results.append({'Feature': col, 'JS_Divergence': js_div, 'Severity': severity})\n",
    "\n",
    "js_df = pd.DataFrame(js_results)\n",
    "print(\"Jensen-Shannon Divergence Analysis:\\n\")\n",
    "print(js_df.to_string(index=False))\n",
    "print(f\"\\nüî¥ High Drift Features: {js_df[js_df['Severity'] == 'High']['Feature'].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62424b1a",
   "metadata": {},
   "source": [
    "## 8. Monitoring Dashboard Visualization\n",
    "\n",
    "**Purpose:** Create comprehensive visual dashboard for drift monitoring and model performance tracking.\n",
    "\n",
    "**Key Points:**\n",
    "- **Feature Distribution Comparison**: Overlay training vs production histograms\n",
    "- **Drift Metrics Timeline**: Track KS, PSI, JS over time (simulated batches here)\n",
    "- **Model Performance Degradation**: Monitor accuracy/F1 score drops\n",
    "- **Alert Thresholds**: Visual indicators when metrics exceed safe limits\n",
    "\n",
    "**Why This Matters:** Executives and engineers need quick visual summaries to decide on retraining schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "fig.suptitle('ML Model Monitoring Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Feature distribution comparison (Vdd)\n",
    "axes[0, 0].hist(X_train['Vdd'], bins=20, alpha=0.6, label='Training', color='blue', edgecolor='black')\n",
    "axes[0, 0].hist(production_df['Vdd'], bins=20, alpha=0.6, label='Production', color='red', edgecolor='black')\n",
    "axes[0, 0].set_title('Vdd Distribution Shift')\n",
    "axes[0, 0].set_xlabel('Vdd (V)')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Feature distribution comparison (Temperature)\n",
    "axes[0, 1].hist(X_train['Temperature'], bins=20, alpha=0.6, label='Training', color='blue', edgecolor='black')\n",
    "axes[0, 1].hist(production_df['Temperature'], bins=20, alpha=0.6, label='Production', color='red', edgecolor='black')\n",
    "axes[0, 1].set_title('Temperature Distribution Shift')\n",
    "axes[0, 1].set_xlabel('Temperature (¬∞C)')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Drift metrics comparison (bar chart)\n",
    "drift_metrics = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'KS Statistic': [ks_df[ks_df['Feature'] == col]['KS_Statistic'].values[0] for col in feature_cols],\n",
    "    'PSI': [psi_df[psi_df['Feature'] == col]['PSI'].values[0] for col in feature_cols]\n",
    "})\n",
    "x_pos = np.arange(len(feature_cols))\n",
    "axes[1, 0].bar(x_pos - 0.2, drift_metrics['KS Statistic'], 0.4, label='KS Statistic', color='orange')\n",
    "axes[1, 0].bar(x_pos + 0.2, drift_metrics['PSI'], 0.4, label='PSI', color='purple')\n",
    "axes[1, 0].axhline(y=0.2, color='red', linestyle='--', label='Alert Threshold')\n",
    "axes[1, 0].set_xticks(x_pos)\n",
    "axes[1, 0].set_xticklabels(feature_cols, rotation=45)\n",
    "axes[1, 0].set_title('Drift Metrics by Feature')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. JS Divergence heatmap-style bar chart\n",
    "js_values = [js_df[js_df['Feature'] == col]['JS_Divergence'].values[0] for col in feature_cols]\n",
    "colors = ['green' if v < 0.1 else 'orange' if v < 0.3 else 'red' for v in js_values]\n",
    "axes[1, 1].barh(feature_cols, js_values, color=colors, edgecolor='black')\n",
    "axes[1, 1].axvline(x=0.1, color='orange', linestyle='--', label='Medium Threshold')\n",
    "axes[1, 1].axvline(x=0.3, color='red', linestyle='--', label='High Threshold')\n",
    "axes[1, 1].set_title('Jensen-Shannon Divergence')\n",
    "axes[1, 1].set_xlabel('JS Divergence')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# 5. Model performance comparison\n",
    "performance_comparison = pd.DataFrame({\n",
    "    'Dataset': ['Training (Baseline)', 'Production'],\n",
    "    'Accuracy': [baseline_accuracy, production_accuracy],\n",
    "    'F1 Score': [baseline_f1, f1_score(production_df['Pass'], y_production_pred, average='weighted')]\n",
    "})\n",
    "x_pos = np.arange(len(performance_comparison))\n",
    "axes[2, 0].bar(x_pos - 0.2, performance_comparison['Accuracy'], 0.4, label='Accuracy', color='skyblue')\n",
    "axes[2, 0].bar(x_pos + 0.2, performance_comparison['F1 Score'], 0.4, label='F1 Score', color='salmon')\n",
    "axes[2, 0].set_xticks(x_pos)\n",
    "axes[2, 0].set_xticklabels(performance_comparison['Dataset'])\n",
    "axes[2, 0].set_ylim(0, 1)\n",
    "axes[2, 0].set_title('Model Performance Degradation')\n",
    "axes[2, 0].legend()\n",
    "\n",
    "# 6. Alert summary table (text)\n",
    "alert_summary = f\"\"\"\n",
    "MONITORING ALERT SUMMARY\n",
    "========================\n",
    "Drifted Features (KS): {len(ks_df[ks_df['Drift_Detected'] == 'Yes'])} / {len(feature_cols)}\n",
    "High PSI Features: {len(psi_df[psi_df['PSI'] > 0.25])}\n",
    "High JS Features: {len(js_df[js_df['Severity'] == 'High'])}\n",
    "\n",
    "Performance Drop: {(baseline_accuracy - production_accuracy):.2%}\n",
    "\n",
    "üö® RECOMMENDATION:\n",
    "{'RETRAIN MODEL IMMEDIATELY' if production_accuracy < 0.75 else 'Continue monitoring'}\n",
    "\"\"\"\n",
    "axes[2, 1].text(0.1, 0.5, alert_summary, fontsize=10, family='monospace', \n",
    "                verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "axes[2, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed4d2a",
   "metadata": {},
   "source": [
    "## üöÄ Real-World Project Templates\n",
    "\n",
    "Build production ML monitoring systems using these frameworks:\n",
    "\n",
    "### 1Ô∏è‚É£ **Post-Silicon Test Yield Monitor**\n",
    "- **Objective**: Track parametric test drift across wafer lots to predict yield degradation  \n",
    "- **Data**: STDF files with Vdd, Idd, frequency, power per device (10K+ devices/week)  \n",
    "- **Success Metric**: Detect drift 2 weeks before yield drops below 85%  \n",
    "- **Features**: Multi-site correlation, spatial drift (wafer maps), process generation comparison  \n",
    "- **Tech Stack**: Python, Grafana, PostgreSQL timeseries, Evidently AI\n",
    "\n",
    "### 2Ô∏è‚É£ **E-commerce Recommendation Drift Detector**\n",
    "- **Objective**: Monitor user behavior shifts to prevent stale recommendations  \n",
    "- **Data**: Click-through rates, session duration, product views (1M+ events/day)  \n",
    "- **Success Metric**: Maintain CTR > 3.5% by detecting seasonal/trend shifts  \n",
    "- **Features**: Real-time feature monitoring, A/B test drift, cold-start detection  \n",
    "- **Tech Stack**: Spark Streaming, Kafka, MLflow, custom PSI dashboard\n",
    "\n",
    "### 3Ô∏è‚É£ **Fraud Detection Model Observatory**\n",
    "- **Objective**: Detect adversarial drift in transaction patterns  \n",
    "- **Data**: Transaction amounts, merchant categories, user profiles (500K+ txns/day)  \n",
    "- **Success Metric**: Alert on concept drift within 24 hours (new fraud tactics)  \n",
    "- **Features**: Adversarial drift detection, label drift monitoring, precision@K tracking  \n",
    "- **Tech Stack**: AWS SageMaker Model Monitor, CloudWatch, Lambda alerts\n",
    "\n",
    "### 4Ô∏è‚É£ **Manufacturing Defect Predictor Monitor**\n",
    "- **Objective**: Track sensor drift in production line IoT devices  \n",
    "- **Data**: Temperature, pressure, vibration sensors (100Hz sampling, 50 machines)  \n",
    "- **Success Metric**: Predict machine failures 48 hours in advance  \n",
    "- **Features**: Sensor calibration drift, multivariate drift (Mahalanobis distance), time-series KS test  \n",
    "- **Tech Stack**: InfluxDB, Telegraf, custom Python monitor, PagerDuty integration\n",
    "\n",
    "### 5Ô∏è‚É£ **Credit Risk Model Stability Tracker**\n",
    "- **Objective**: Regulatory compliance monitoring for credit scoring models  \n",
    "- **Data**: Applicant features (income, credit history, debt ratio) - 50K applications/month  \n",
    "- **Success Metric**: PSI < 0.25 for all features (regulatory requirement)  \n",
    "- **Features**: Segmented PSI (by demographics), Gini coefficient tracking, approval rate monitoring  \n",
    "- **Tech Stack**: SAS Viya, custom Python PSI calculator, Tableau dashboard\n",
    "\n",
    "### 6Ô∏è‚É£ **Autonomous Vehicle Perception Drift Monitor**\n",
    "- **Objective**: Detect camera/LiDAR sensor degradation in self-driving cars  \n",
    "- **Data**: Object detection confidence scores, lane detection accuracy (10GB/hour/vehicle)  \n",
    "- **Success Metric**: Alert when detection confidence drops > 5% from baseline  \n",
    "- **Features**: Per-sensor drift, weather-based segmentation, geographic distribution shift  \n",
    "- **Tech Stack**: ROS, PyTorch, Weights & Biases, custom real-time JS divergence\n",
    "\n",
    "### 7Ô∏è‚É£ **Medical Diagnosis Model Observer**\n",
    "- **Objective**: Monitor imaging model performance across hospital sites  \n",
    "- **Data**: X-ray/MRI features, patient demographics, diagnosis outcomes (1K+ scans/day)  \n",
    "- **Success Metric**: Maintain diagnostic accuracy > 92% across all sites  \n",
    "- **Features**: Site-specific drift, equipment drift (different MRI machines), demographic fairness monitoring  \n",
    "- **Tech Stack**: DICOM integration, TensorFlow Extended (TFX), Kubeflow, HIPAA-compliant logging\n",
    "\n",
    "### 8Ô∏è‚É£ **Energy Demand Forecasting Monitor**\n",
    "- **Objective**: Detect consumption pattern shifts for grid load balancing  \n",
    "- **Data**: Hourly consumption, weather, holidays, economic indicators (10 years history)  \n",
    "- **Success Metric**: MAPE < 5% despite seasonal/COVID-19-like disruptions  \n",
    "- **Features**: Seasonal decomposition drift, exogenous variable monitoring, forecast interval calibration  \n",
    "- **Tech Stack**: Prophet, ARIMA, custom drift detection, Azure Time Series Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd581d2",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### What is ML Model Monitoring?\n",
    "Continuous validation of deployed models to detect performance degradation, data drift, and concept drift in production environments.\n",
    "\n",
    "### Why Monitor Models?\n",
    "- **Prevent Silent Failures**: Models degrade as real-world data changes\n",
    "- **Regulatory Compliance**: Banking, healthcare require documented model stability\n",
    "- **Cost Savings**: Early detection prevents bad predictions affecting business\n",
    "- **Trust**: Stakeholders need evidence models remain reliable over time\n",
    "\n",
    "### Core Monitoring Metrics\n",
    "\n",
    "| **Metric** | **Purpose** | **Threshold** | **When to Use** |\n",
    "|-----------|-----------|--------------|----------------|\n",
    "| **KS Test** | Detect feature distribution shifts | p < 0.05 | Continuous features, any sample size |\n",
    "| **PSI** | Quantify drift magnitude | > 0.25 severe | Credit scoring, risk models |\n",
    "| **JS Divergence** | Symmetric distribution distance | > 0.3 high | Research, multi-distribution comparison |\n",
    "| **Model Accuracy** | Direct performance tracking | Domain-specific | Always, as primary metric |\n",
    "| **Prediction Drift** | Output distribution changes | > 10% shift | Classification models |\n",
    "\n",
    "### Implementation Checklist\n",
    "- ‚úÖ **Baseline**: Establish training data statistics (mean, std, quantiles)\n",
    "- ‚úÖ **Instrumentation**: Log all predictions + features in production\n",
    "- ‚úÖ **Cadence**: Run drift checks daily (batch) or per 1K predictions (streaming)\n",
    "- ‚úÖ **Alerting**: PagerDuty/Slack integration when thresholds exceeded\n",
    "- ‚úÖ **Visualization**: Grafana/Tableau dashboards for stakeholders\n",
    "- ‚úÖ **Retraining Pipeline**: Automated trigger when drift confirmed\n",
    "\n",
    "### Common Pitfalls\n",
    "- ‚ùå **Monitoring Accuracy Only**: Feature drift happens before accuracy drops\n",
    "- ‚ùå **No Ground Truth Delay**: In post-silicon, test results come hours/days later\n",
    "- ‚ùå **Threshold Overload**: Too many alerts ‚Üí alert fatigue ‚Üí ignoring real issues\n",
    "- ‚ùå **Ignoring Segments**: Overall metrics stable but specific segments (e.g., new product variants) degrading\n",
    "\n",
    "### Post-Silicon Specifics\n",
    "- **Spatial Drift**: Wafer edge vs center devices behave differently over time\n",
    "- **Equipment Drift**: Tester calibration changes ‚Üí feature distribution shifts\n",
    "- **Process Nodes**: Models trained on 7nm don't transfer to 5nm without monitoring\n",
    "- **Yield Prediction**: Monitor correlation between parametric tests and final yield weekly\n",
    "\n",
    "### When to Retrain\n",
    "1. **PSI > 0.25** on critical features (Vdd, frequency)\n",
    "2. **Accuracy drop > 5%** sustained over 1 week\n",
    "3. **New failure mode detected** (concept drift confirmed)\n",
    "4. **Scheduled**: Every 3 months even without drift (best practice)\n",
    "\n",
    "### Tool Ecosystem\n",
    "- **Open Source**: Evidently AI, Alibi Detect, NannyML, WhyLogs\n",
    "- **Cloud**: AWS SageMaker Model Monitor, Azure ML Monitoring, Vertex AI Model Monitoring\n",
    "- **Observability**: Arize AI, Fiddler AI, Arthur, Aporia\n",
    "\n",
    "### Next Steps\n",
    "- **Notebook 108**: Feature Stores (versioning features for drift comparison)\n",
    "- **Notebook 109**: ML Pipelines (orchestrating monitoring + retraining)\n",
    "- **Advanced**: Multi-armed bandits for online model selection under drift\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: *The best model is the one that stays relevant. Monitor or perish!* üö®"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
