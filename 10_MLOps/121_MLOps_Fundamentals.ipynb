{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e235a46b",
   "metadata": {},
   "source": [
    "# 121: MLOps Fundamentals\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** MLOps: lifecycle management, CI/CD for ML, production challenges\n",
    "- **Master** experiment tracking: logging metrics, parameters, artifacts with MLflow\n",
    "- **Build** ML pipelines: data ‚Üí train ‚Üí evaluate ‚Üí deploy automated workflows\n",
    "- **Implement** model versioning: registry, staging, production promotion\n",
    "- **Deploy** models to production: REST APIs, batch inference, edge deployment\n",
    "- **Monitor** ML systems: performance tracking, drift detection, alerting\n",
    "\n",
    "## üìö What is MLOps?\n",
    "\n",
    "**MLOps (Machine Learning Operations)** applies DevOps principles to ML systems - automating the end-to-end ML lifecycle from data preparation to production deployment and monitoring. Unlike traditional software, ML systems require managing data, models, and experiments alongside code.\n",
    "\n",
    "**Core concepts:**\n",
    "- **Reproducibility**: Track experiments (code, data, hyperparameters, metrics)\n",
    "- **Automation**: CI/CD pipelines for training, testing, deployment\n",
    "- **Monitoring**: Track model performance, data drift, system health\n",
    "- **Governance**: Model versioning, lineage, compliance, security\n",
    "\n",
    "**Why MLOps?**\n",
    "- ‚úÖ **Faster deployment**: Days to production (vs months with manual processes)\n",
    "- ‚úÖ **Reliability**: Automated testing prevents bad models from reaching production\n",
    "- ‚úÖ **Scalability**: Deploy 100+ models without linear team growth\n",
    "- ‚úÖ **Collaboration**: Data scientists, ML engineers, DevOps work seamlessly\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Yield Prediction Model Pipeline**\n",
    "- Input: STDF data (wafer test, final test), device parameters (Vdd, Idd, freq), historical yield (100K devices)\n",
    "- Pipeline: Data validation ‚Üí Feature engineering ‚Üí Model training (Random Forest, XGBoost) ‚Üí Evaluation (F1, AUC) ‚Üí Registry ‚Üí REST API deployment\n",
    "- Output: Yield prediction API (95% accuracy), real-time inference (<50ms), daily retraining (automated)\n",
    "- Value: Identify failing devices 2 days earlier, reduce scrap 12%, improve yield 3%\n",
    "\n",
    "**Test Time Optimization Model**\n",
    "- Input: Test execution data (1M devices √ó 100 tests), test correlations, coverage matrix\n",
    "- Pipeline: Correlation analysis ‚Üí ML ranking (Gradient Boosting) ‚Üí Test selection ‚Üí A/B testing ‚Üí Production rollout\n",
    "- Output: Optimized test suite (25% time reduction, <1% coverage loss), confidence intervals, ROI dashboard\n",
    "- Value: Save $500K/year per product, automated monthly retraining, shadow mode validation\n",
    "\n",
    "**Anomaly Detection System**\n",
    "- Input: Parametric test results (real-time stream, 1000 devices/hour), control limits, historical distributions\n",
    "- Pipeline: Feature extraction ‚Üí Isolation Forest training ‚Üí Model registry ‚Üí Edge deployment (test floor) ‚Üí Alert system\n",
    "- Output: Real-time anomaly detection (<1s latency), email/SMS alerts, 24/7 monitoring\n",
    "- Value: Detect excursions within 5 min (vs 2 hrs manual), reduce false positives 40%\n",
    "\n",
    "**Device Binning Classifier**\n",
    "- Input: Parametric measurements (Vdd, Idd, freq, temp), spec limits, bin definitions (PASS, FAIL_VDD, etc.)\n",
    "- Pipeline: Multi-class classification (SVM, Neural Network) ‚Üí SHAP explanations ‚Üí Model validation ‚Üí Production API\n",
    "- Output: Intelligent binning with confidence scores, feature importance, bin prediction accuracy 98%\n",
    "- Value: Reduce test escapes 60%, improve bin accuracy, explain predictions to stakeholders\n",
    "\n",
    "## üîÑ MLOps Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Data Collection] --> B[Data Validation]\n",
    "    B --> C[Feature Engineering]\n",
    "    C --> D[Model Training]\n",
    "    D --> E[Experiment Tracking]\n",
    "    E --> F{Performance OK?}\n",
    "    F -->|No| D\n",
    "    F -->|Yes| G[Model Registry]\n",
    "    G --> H[Staging Environment]\n",
    "    H --> I[A/B Testing]\n",
    "    I --> J{Pass Tests?}\n",
    "    J -->|No| D\n",
    "    J -->|Yes| K[Production Deployment]\n",
    "    K --> L[Monitoring & Logging]\n",
    "    L --> M{Drift Detected?}\n",
    "    M -->|Yes| A\n",
    "    M -->|No| L\n",
    "    \n",
    "    N[CI/CD Pipeline] -.-> D\n",
    "    N -.-> K\n",
    "    O[Model Governance] -.-> G\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style K fill:#e1ffe1\n",
    "    style M fill:#fffacd\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- 010: Linear Regression (ML model basics)\n",
    "- 041: Model Evaluation (metrics, validation)\n",
    "- 051: Deep Learning Basics (neural networks)\n",
    "\n",
    "**Next Steps:**\n",
    "- 122: MLflow Complete Guide (experiment tracking platform)\n",
    "- 123: Model Monitoring & Drift Detection (production health)\n",
    "- 131: Docker Fundamentals (containerization for deployment)\n",
    "\n",
    "---\n",
    "\n",
    "Let's build production ML systems! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca85fb",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "**Note**: MLOps tools integrate with existing ML workflows. We'll install MLflow for experiment tracking and model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c50c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MLOps packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'mlflow',          # Experiment tracking and model registry\n",
    "    'scikit-learn',    # ML models\n",
    "    'pandas',          # Data processing\n",
    "    'numpy',           # Numerical operations\n",
    "    'matplotlib',      # Visualization\n",
    "    'seaborn',         # Statistical plots\n",
    "    'requests',        # API calls\n",
    "    'flask',           # REST API server\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"‚úì {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])\n",
    "\n",
    "# Imports\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\n‚úÖ All packages ready!\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(\"\\nTo start MLflow UI:\")\n",
    "print(\"  mlflow ui --port 5000\")\n",
    "print(\"  Open browser: http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684d623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment tracking example: Yield prediction model\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic STDF data\n",
    "np.random.seed(42)\n",
    "n_devices = 5000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Vdd_V': np.random.normal(1.2, 0.05, n_devices),\n",
    "    'Idd_mA': np.random.normal(50, 5, n_devices),\n",
    "    'freq_MHz': np.random.normal(1000, 50, n_devices),\n",
    "    'temp_C': np.random.normal(25, 5, n_devices),\n",
    "})\n",
    "\n",
    "# Create yield target (devices fail if params out of spec)\n",
    "df['yield'] = ((df['Vdd_V'] > 1.15) & (df['Vdd_V'] < 1.25) &\n",
    "               (df['Idd_mA'] < 60) & (df['freq_MHz'] > 950)).astype(int)\n",
    "\n",
    "# Split data\n",
    "X = df.drop('yield', axis=1)\n",
    "y = df['yield']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set MLflow experiment\n",
    "mlflow.set_experiment(\\\"yield_prediction\\\")\\n\\n# Train with experiment tracking\n",
    "with mlflow.start_run(run_name=\\\"rf_baseline\\\"):\\n    \n",
    "    # Log parameters\n",
    "    n_estimators = 100\n",
    "    max_depth = 10\n",
    "    mlflow.log_param(\\\"n_estimators\\\", n_estimators)\n",
    "    mlflow.log_param(\\\"max_depth\\\", max_depth)\n",
    "    mlflow.log_param(\\\"model_type\\\", \\\"RandomForest\\\")\n",
    "    mlflow.log_param(\\\"data_size\\\", len(df))\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\\\"accuracy\\\", accuracy)\n",
    "    mlflow.log_metric(\\\"f1_score\\\", f1)\n",
    "    mlflow.log_metric(\\\"auc\\\", auc)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model, \\\"model\\\")\n",
    "    \n",
    "    # Log feature importance plot\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    ax.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    mlflow.log_figure(fig, \\\"feature_importance.png\\\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Log confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    mlflow.log_figure(fig, \\\"confusion_matrix.png\\\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\\\"‚úÖ Experiment logged successfully!\\\")\n",
    "    print(f\\\"Accuracy: {accuracy:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}\\\")\n",
    "    print(f\\\"\\\\nView results: mlflow ui --port 5000\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Hyperparameter Tuning with Tracking\\n\",\n",
    "    \"\\n\",\n",
    "    \"### üìù Systematic Experimentation\\n\",\n",
    "    \"\\n\",\n",
    "    \"MLflow enables comparing 100+ hyperparameter combinations:\\n\",\n",
    "    \"- Grid search / Random search / Bayesian optimization\\n\",\n",
    "    \"- Log all trials automatically\\n\",\n",
    "    \"- Sort by metric to find best configuration\\n\",\n",
    "    \"- Visualize parameter effects in UI\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Hyperparameter tuning with MLflow\n",
    "    \"from sklearn.model_selection import ParameterGrid\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define parameter grid\\n\",\n",
    "    \"param_grid = {\\n\",\n",
    "    \"    'n_estimators': [50, 100, 200],\\n\",\n",
    "    \"    'max_depth': [5, 10, 20, None],\\n\",\n",
    "    \"    'min_samples_split': [2, 5, 10]\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Grid search with tracking\n",
    "n\",\n",
    "    \"best_auc = 0\\n\",\n",
    "    \"best_params = None\\n\",\n",
    "    \"\\n\",\n",
    "    \"for params in ParameterGrid(param_grid):\\n\",\n",
    "    \"    with mlflow.start_run(run_name=f\\\"rf_{params['n_estimators']}_{params['max_depth']}\\\"):\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Log all parameters\\n\",\n",
    "    \"        mlflow.log_params(params)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Train model\\n\",\n",
    "    \"        model = RandomForestClassifier(**params, random_state=42)\\n\",\n",
    "    \"        model.fit(X_train, y_train)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Evaluate\\n\",\n",
    "    \"        y_pred_proba = model.predict_proba(X_test)[:, 1]\\n\",\n",
    "    \"        auc = roc_auc_score(y_test, y_pred_proba)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Log metrics\\n\",\n",
    "    \"        mlflow.log_metric(\\\"auc\\\", auc)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Track best model\\n\",\n",
    "    \"        if auc > best_auc:\\n\",\n",
    "    \"            best_auc = auc\\n\",\n",
    "    \"            best_params = params\\n\",\n",
    "    \"            mlflow.sklearn.log_model(model, \\\"best_model\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\n‚úÖ Tested {len(list(ParameterGrid(param_grid)))} configurations\\\")\n",
    "print(f\\\"Best AUC: {best_auc:.4f}\\\")\n",
    "print(f\\\"Best params: {best_params}\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059fb0f9",
   "metadata": {},
   "source": [
    "## 2. Experiment Tracking with MLflow\n",
    "\n",
    "### üìù Why Experiment Tracking?\n",
    "\n",
    "**The problem**: Data scientists run 100+ experiments, forget what worked, can't reproduce results\n",
    "\n",
    "**MLflow tracking** logs:\n",
    "- **Parameters**: Hyperparameters (learning_rate, n_estimators, etc.)\n",
    "- **Metrics**: accuracy, F1, AUC, loss over epochs\n",
    "- **Artifacts**: Models, plots, datasets, code snapshots\n",
    "- **Metadata**: Environment, Git commit, runtime\n",
    "\n",
    "**Benefits**:\n",
    "- Compare 100+ experiments in UI (sort by metric, filter by parameter)\n",
    "- Reproduce any experiment (exact code, data, hyperparameters)\n",
    "- Share results with team (URL to experiment, not screenshots)\n",
    "- Track lineage (which data/model produced which result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7dece9",
   "metadata": {},
   "source": [
    "## 3. Model Registry & Versioning\n",
    "\n",
    "**The Challenge**: Multiple model versions in production, which one is current?\n",
    "\n",
    "**MLflow Model Registry** provides:\n",
    "- **Centralized storage** for all models\n",
    "- **Version control** with lineage tracking\n",
    "- **Stage transitions** (Staging ‚Üí Production)\n",
    "- **Model metadata** (who, when, why promoted)\n",
    "\n",
    "**Post-Silicon Example**: Yield predictor model lifecycle\n",
    "- v1.0: Initial Random Forest (85% accuracy) ‚Üí Staging\n",
    "- v1.1: Tuned hyperparameters (88% accuracy) ‚Üí Production\n",
    "- v2.0: XGBoost with feature engineering (92% accuracy) ‚Üí Production\n",
    "- v1.1: Archived (replaced by v2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db2d6aa",
   "metadata": {},
   "source": [
    "## 4. Model Deployment Patterns\n",
    "\n",
    "**Three Common Deployment Patterns:**\n",
    "\n",
    "### **A. REST API (Real-time Inference)**\n",
    "- **Use Case**: Web app needs yield prediction for single device\n",
    "- **Latency**: <100ms\n",
    "- **Tool**: MLflow Models + Flask/FastAPI\n",
    "\n",
    "### **B. Batch Inference**\n",
    "- **Use Case**: Daily analysis of 10,000 wafers from test floor\n",
    "- **Latency**: Minutes to hours acceptable\n",
    "- **Tool**: MLflow Models + Apache Spark\n",
    "\n",
    "### **C. Edge Deployment**\n",
    "- **Use Case**: Real-time inference on ATE (Automated Test Equipment)\n",
    "- **Latency**: <10ms\n",
    "- **Tool**: ONNX Runtime or TensorFlow Lite\n",
    "\n",
    "**Post-Silicon Example**: Test floor yield predictor\n",
    "- **Input**: Device parameters from STDF (Vdd, Idd, freq, temp)\n",
    "- **Output**: Yield probability + risk score\n",
    "- **SLA**: 99.9% uptime, <50ms latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8662ec",
   "metadata": {},
   "source": [
    "## 5. Monitoring & Drift Detection\n",
    "\n",
    "**What to Monitor in Production:**\n",
    "\n",
    "### **A. Model Performance Metrics**\n",
    "- Accuracy, F1, AUC (requires ground truth labels)\n",
    "- Prediction confidence distribution\n",
    "- Error rate trends\n",
    "\n",
    "### **B. Data Drift**\n",
    "- **Feature drift**: Input distributions change (e.g., Vdd range shifts from 1.2V¬±0.05 to 1.25V¬±0.03)\n",
    "- **Concept drift**: Relationship between features and target changes\n",
    "- **Detection**: Statistical tests (KS test, PSI - Population Stability Index)\n",
    "\n",
    "### **C. System Health**\n",
    "- Latency (p50, p95, p99)\n",
    "- Throughput (predictions/sec)\n",
    "- Resource usage (CPU, memory)\n",
    "\n",
    "**Post-Silicon Alert Example**: \n",
    "\"‚ö†Ô∏è Vdd distribution shifted by 2 standard deviations. Model retrain recommended. Current accuracy estimate: 82% (down from 92%).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b7374",
   "metadata": {},
   "source": [
    "## 6. CI/CD for ML Pipelines\n",
    "\n",
    "**Traditional CI/CD** (software engineering):\n",
    "- Code ‚Üí Build ‚Üí Test ‚Üí Deploy\n",
    "\n",
    "**ML CI/CD** (additional steps):\n",
    "- Data validation ‚Üí Feature engineering ‚Üí Model training ‚Üí Model validation ‚Üí A/B testing ‚Üí Gradual rollout\n",
    "\n",
    "**Key Differences:**\n",
    "- **Data is code**: Data changes require testing\n",
    "- **Model testing**: Beyond unit tests (accuracy, fairness, robustness)\n",
    "- **Gradual rollout**: Canary deployments (5% traffic ‚Üí 50% ‚Üí 100%)\n",
    "\n",
    "**Post-Silicon CI/CD Pipeline:**\n",
    "1. **Trigger**: New STDF data arrives (daily at 2 AM)\n",
    "2. **Data validation**: Check schema, ranges, missing values\n",
    "3. **Feature engineering**: Calculate derived metrics\n",
    "4. **Model training**: Train on last 30 days of data\n",
    "5. **Model validation**: Accuracy > 90% threshold?\n",
    "6. **Model registry**: Register as new version\n",
    "7. **A/B test**: Deploy to 10% of test stations\n",
    "8. **Monitoring**: Compare metrics vs baseline\n",
    "9. **Promote**: If A/B successful, promote to 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575b6dbe",
   "metadata": {},
   "source": [
    "## üéØ Real-World MLOps Projects\n",
    "\n",
    "### **Post-Silicon Validation Projects**\n",
    "\n",
    "#### **Project 1: Automated Yield Prediction Pipeline**\n",
    "**Objective**: Build end-to-end MLOps pipeline for wafer yield prediction\n",
    "- **Data**: STDF files from test floor (daily refresh)\n",
    "- **Model**: Gradient Boosting (track experiments with MLflow)\n",
    "- **Deployment**: REST API for real-time predictions (<50ms latency)\n",
    "- **Monitoring**: Data drift detection (PSI < 0.2), accuracy tracking\n",
    "- **CI/CD**: Daily retrain at 2 AM, auto-promote if accuracy > 92%\n",
    "- **Business Value**: Predict yield 24 hours earlier, reduce scrap by 15%\n",
    "\n",
    "#### **Project 2: Test Time Optimization Model Lifecycle**\n",
    "**Objective**: Deploy ML model to reduce ATE test time while maintaining quality\n",
    "- **Features**: Device parameters, test sequence, historical pass/fail\n",
    "- **Model**: XGBoost (log to MLflow with test time reduction metric)\n",
    "- **Registry**: Track versions (v1.0: 10% reduction ‚Üí v2.0: 25% reduction)\n",
    "- **Deployment**: Batch inference on nightly test results\n",
    "- **Monitoring**: False negative rate < 0.5%, test time savings\n",
    "- **Success Metric**: $500K annual savings, 0% quality degradation\n",
    "\n",
    "#### **Project 3: Anomaly Detection System with MLOps**\n",
    "**Objective**: Real-time anomaly detection on test floor with full MLOps lifecycle\n",
    "- **Data**: Streaming STDF data (parametric measurements)\n",
    "- **Model**: Isolation Forest (experiment tracking for contamination parameter)\n",
    "- **Deployment**: Edge deployment on test controllers (<10ms inference)\n",
    "- **Monitoring**: Alert when anomaly rate > 5%, drift in normal behavior baseline\n",
    "- **CI/CD**: Weekly retrain with new normal behavior patterns\n",
    "- **Business Value**: Detect equipment failures 4 hours earlier, $2M avoidance/year\n",
    "\n",
    "#### **Project 4: Device Binning Classifier with Explainability**\n",
    "**Objective**: Automated device binning with SHAP explainability and MLOps tracking\n",
    "- **Data**: Final test STDF (performance parameters)\n",
    "- **Model**: Random Forest (track feature importance over time in MLflow)\n",
    "- **Registry**: Version models with business rules (bin definitions change quarterly)\n",
    "- **Deployment**: REST API with SHAP explanations (\"Device binned as Premium because Vdd stability = 98%\")\n",
    "- **Monitoring**: Bin distribution drift (expected 60/30/10 split)\n",
    "- **Success Metric**: 98% binning accuracy, full auditability\n",
    "\n",
    "---\n",
    "\n",
    "### **General AI/ML Projects**\n",
    "\n",
    "#### **Project 5: Customer Churn Prediction MLOps Pipeline**\n",
    "**Objective**: Production-ready churn prediction with complete MLOps lifecycle\n",
    "- **Data**: Customer usage metrics (refresh weekly)\n",
    "- **Model**: LightGBM (experiment with feature engineering strategies)\n",
    "- **Deployment**: Batch predictions, integrate with CRM via API\n",
    "- **Monitoring**: Concept drift (customer behavior changes), precision/recall tracking\n",
    "- **CI/CD**: Retrain when drift detected or monthly\n",
    "- **Business Value**: Reduce churn by 20%, proactive retention campaigns\n",
    "\n",
    "#### **Project 6: Recommendation System with A/B Testing**\n",
    "**Objective**: Deploy recommendation model with rigorous A/B testing\n",
    "- **Data**: User interactions, product catalog\n",
    "- **Model**: Collaborative filtering (log user engagement metrics to MLflow)\n",
    "- **Registry**: Track candidate models (v1: content-based, v2: collaborative, v3: hybrid)\n",
    "- **Deployment**: Canary release (5% ‚Üí 25% ‚Üí 100% traffic)\n",
    "- **Monitoring**: Click-through rate, conversion rate, user satisfaction\n",
    "- **Success Metric**: 15% increase in conversion rate\n",
    "\n",
    "#### **Project 7: Fraud Detection Real-Time Inference**\n",
    "**Objective**: Low-latency fraud detection with model monitoring\n",
    "- **Data**: Transaction data (streaming)\n",
    "- **Model**: Neural network (track precision/recall tradeoff experiments)\n",
    "- **Deployment**: REST API (<100ms SLA for payment processing)\n",
    "- **Monitoring**: False positive rate (customer friction), model drift daily\n",
    "- **CI/CD**: Blue-green deployment (instant rollback if FPR spikes)\n",
    "- **Business Value**: Block $5M fraud annually, <1% false positive rate\n",
    "\n",
    "#### **Project 8: Demand Forecasting with MLOps Governance**\n",
    "**Objective**: Enterprise demand forecasting with model governance\n",
    "- **Data**: Sales history, seasonality, promotions (daily updates)\n",
    "- **Model**: Prophet + XGBoost ensemble (track component contributions)\n",
    "- **Registry**: Maintain model lineage (data version + code version + hyperparameters)\n",
    "- **Deployment**: Scheduled batch predictions (nightly forecasts for next 30 days)\n",
    "- **Monitoring**: MAPE tracking, alert when > 15%, automatic retrain trigger\n",
    "- **Success Metric**: Reduce inventory costs by 25%, improve forecast accuracy to MAPE < 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2b947c",
   "metadata": {},
   "source": [
    "## üìö Comprehensive Takeaways\n",
    "\n",
    "### **üéØ What is MLOps?**\n",
    "\n",
    "**MLOps** = Machine Learning + DevOps = Systematic approach to deploying, monitoring, and managing ML models in production\n",
    "\n",
    "**Core Principles:**\n",
    "1. **Reproducibility**: Every experiment, model, and prediction must be traceable\n",
    "2. **Automation**: Manual steps = errors; automate data validation ‚Üí training ‚Üí deployment\n",
    "3. **Monitoring**: Models degrade over time; continuous monitoring is non-negotiable\n",
    "4. **Collaboration**: Data scientists, ML engineers, DevOps work from same platform\n",
    "5. **Governance**: Model lineage, approvals, audit trails for regulated industries\n",
    "\n",
    "---\n",
    "\n",
    "### **üîß MLOps Lifecycle Stages**\n",
    "\n",
    "#### **1. Experiment Tracking**\n",
    "**Tools**: MLflow, Weights & Biases, Neptune.ai\n",
    "\n",
    "**What to Track:**\n",
    "- **Parameters**: Hyperparameters, feature engineering choices, data version\n",
    "- **Metrics**: Accuracy, F1, AUC, business metrics (revenue impact, latency)\n",
    "- **Artifacts**: Trained models, plots, feature importance, confusion matrices\n",
    "- **Metadata**: Git commit hash, dataset hash, training duration\n",
    "\n",
    "**Best Practices:**\n",
    "```python\n",
    "with mlflow.start_run(run_name=\"descriptive_name\"):\n",
    "    mlflow.log_param(\"learning_rate\", 0.01)  # Log ALL hyperparameters\n",
    "    mlflow.log_metric(\"val_accuracy\", 0.92)  # Log validation metrics\n",
    "    mlflow.sklearn.log_model(model, \"model\")  # Log model artifact\n",
    "    mlflow.log_artifact(\"feature_importance.png\")  # Log visualizations\n",
    "```\n",
    "\n",
    "**Post-Silicon Tip**: Track semiconductor-specific metrics (yield improvement %, test time reduction, false negative rate)\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Model Registry & Versioning**\n",
    "\n",
    "**Why Versioning Matters:**\n",
    "- Production model degrades ‚Üí need to rollback to v1.2 (stable version)\n",
    "- A/B test v2.0 vs v1.5 ‚Üí need both versions deployed\n",
    "- Regulatory audit ‚Üí \"Show me the exact model used on 2024-03-15\"\n",
    "\n",
    "**MLflow Registry Stages:**\n",
    "- **None**: Experimental models, not production-ready\n",
    "- **Staging**: Validated models undergoing A/B testing\n",
    "- **Production**: Live models serving predictions\n",
    "- **Archived**: Retired models (keep for audit trails)\n",
    "\n",
    "**Best Practices:**\n",
    "- **Semantic versioning**: v1.0.0 (major.minor.patch)\n",
    "- **Model cards**: Document model purpose, training data, limitations, fairness\n",
    "- **Automated promotion**: If staging model outperforms production by >5%, auto-promote\n",
    "\n",
    "**Post-Silicon Example:**\n",
    "```\n",
    "Yield Predictor Registry:\n",
    "- v1.0: Random Forest, 85% accuracy ‚Üí Archived\n",
    "- v1.5: Tuned RF, 88% accuracy ‚Üí Archived\n",
    "- v2.0: XGBoost, 92% accuracy ‚Üí Production\n",
    "- v2.1: XGBoost + new features, 93% accuracy ‚Üí Staging (A/B testing)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Model Deployment Patterns**\n",
    "\n",
    "**A. REST API (Real-Time Inference)**\n",
    "\n",
    "**When to Use:**\n",
    "- Low latency required (<100ms)\n",
    "- Single predictions (web app, mobile app)\n",
    "- Synchronous workflows\n",
    "\n",
    "**Tools:** Flask, FastAPI, MLflow Models, TensorFlow Serving\n",
    "\n",
    "**Code Pattern:**\n",
    "```python\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    prediction = model.predict(pd.DataFrame([data]))\n",
    "    return jsonify({'prediction': prediction[0]})\n",
    "```\n",
    "\n",
    "**SLA Considerations:**\n",
    "- **Latency**: p99 < 100ms\n",
    "- **Throughput**: 1000 requests/sec\n",
    "- **Availability**: 99.9% uptime (load balancing, health checks)\n",
    "\n",
    "**Post-Silicon Use Case**: Test floor yield prediction API for real-time device routing\n",
    "\n",
    "---\n",
    "\n",
    "**B. Batch Inference**\n",
    "\n",
    "**When to Use:**\n",
    "- Large volumes (millions of predictions)\n",
    "- Latency not critical (minutes/hours acceptable)\n",
    "- Daily/weekly prediction jobs\n",
    "\n",
    "**Tools:** Apache Spark, Dask, Airflow scheduling\n",
    "\n",
    "**Code Pattern:**\n",
    "```python\n",
    "# Load production model\n",
    "model = mlflow.pyfunc.load_model(\"models:/yield_predictor/Production\")\n",
    "\n",
    "# Load batch data (e.g., 1M devices from today's test floor)\n",
    "devices_df = spark.read.parquet(\"s3://stdf-data/2024-12-13/\")\n",
    "\n",
    "# Batch predict\n",
    "predictions = model.predict(devices_df.toPandas())\n",
    "\n",
    "# Save results\n",
    "predictions_df.write.parquet(\"s3://predictions/2024-12-13/\")\n",
    "```\n",
    "\n",
    "**Post-Silicon Use Case**: Nightly batch processing of 10,000 wafers\n",
    "\n",
    "---\n",
    "\n",
    "**C. Edge Deployment**\n",
    "\n",
    "**When to Use:**\n",
    "- Ultra-low latency (<10ms)\n",
    "- Offline inference (no internet)\n",
    "- Resource-constrained devices (ATE, IoT)\n",
    "\n",
    "**Tools:** ONNX Runtime, TensorFlow Lite, TensorRT\n",
    "\n",
    "**Optimization Techniques:**\n",
    "- **Model quantization**: Float32 ‚Üí Int8 (4x smaller, 4x faster)\n",
    "- **Pruning**: Remove 90% of weights with <1% accuracy loss\n",
    "- **Knowledge distillation**: Compress large model into small student model\n",
    "\n",
    "**Post-Silicon Use Case**: Real-time inference on ATE during device testing\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Monitoring & Drift Detection**\n",
    "\n",
    "**What to Monitor:**\n",
    "\n",
    "**A. Model Performance**\n",
    "- **Online metrics**: Prediction latency, throughput, error rate\n",
    "- **Offline metrics**: Accuracy, F1 (requires ground truth labels)\n",
    "\n",
    "**B. Data Drift**\n",
    "- **Feature drift**: Input distributions shift (e.g., temperature range changes)\n",
    "- **Concept drift**: Feature-target relationships change (e.g., new process node)\n",
    "\n",
    "**Detection Methods:**\n",
    "\n",
    "**Kolmogorov-Smirnov Test:**\n",
    "```python\n",
    "from scipy.stats import ks_2samp\n",
    "statistic, pvalue = ks_2samp(training_vdd, production_vdd)\n",
    "if pvalue < 0.05:\n",
    "    print(\"Drift detected! Retrain model.\")\n",
    "```\n",
    "\n",
    "**Population Stability Index (PSI):**\n",
    "- PSI < 0.1: No drift\n",
    "- PSI 0.1-0.2: Minor drift, monitor\n",
    "- PSI > 0.2: Major drift, retrain immediately\n",
    "\n",
    "**C. System Health**\n",
    "- CPU/memory usage\n",
    "- Prediction latency (p50, p95, p99)\n",
    "- API error rates\n",
    "\n",
    "**Alerting Strategy:**\n",
    "- **Critical**: Model accuracy drops >10% ‚Üí page on-call engineer\n",
    "- **Warning**: PSI > 0.15 ‚Üí email data science team\n",
    "- **Info**: New model version deployed ‚Üí Slack notification\n",
    "\n",
    "**Post-Silicon Example:**\n",
    "\"üö® ALERT: Vdd drift detected (PSI=0.24). Yield predictor accuracy estimated at 82% (down from 92%). Auto-retrain triggered.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. CI/CD for ML**\n",
    "\n",
    "**Traditional CI/CD vs ML CI/CD:**\n",
    "\n",
    "| Stage | Software Engineering | Machine Learning |\n",
    "|-------|---------------------|------------------|\n",
    "| **Build** | Compile code | Train model |\n",
    "| **Test** | Unit tests, integration tests | Data validation, model validation, bias tests |\n",
    "| **Deploy** | Blue-green, canary | A/B testing, shadow mode, gradual rollout |\n",
    "| **Monitor** | Latency, errors | Drift, accuracy, fairness |\n",
    "\n",
    "**ML Pipeline Stages:**\n",
    "\n",
    "1. **Data Validation**\n",
    "   - Schema check: Expected columns present?\n",
    "   - Range check: Values within historical bounds?\n",
    "   - Missing values: <5% threshold?\n",
    "\n",
    "2. **Feature Engineering**\n",
    "   - Deterministic transforms (same code = same features)\n",
    "   - Version feature engineering code\n",
    "\n",
    "3. **Model Training**\n",
    "   - Automated hyperparameter tuning\n",
    "   - Cross-validation for robustness\n",
    "   - Track all experiments in MLflow\n",
    "\n",
    "4. **Model Validation**\n",
    "   - **Accuracy gate**: Accuracy > 90% threshold?\n",
    "   - **Fairness gate**: No bias across subgroups?\n",
    "   - **Robustness gate**: Performance stable on OOD data?\n",
    "\n",
    "5. **Model Registration**\n",
    "   - Register as new version in model registry\n",
    "   - Tag with metadata (data version, code commit, accuracy)\n",
    "\n",
    "6. **Staging Deployment**\n",
    "   - Deploy to staging environment\n",
    "   - Run integration tests (API response format correct?)\n",
    "\n",
    "7. **A/B Testing**\n",
    "   - Deploy to 5% of traffic\n",
    "   - Monitor metrics vs baseline (champion vs challenger)\n",
    "   - Statistical significance test (p < 0.05)\n",
    "\n",
    "8. **Production Promotion**\n",
    "   - If challenger wins A/B test, promote to 100%\n",
    "   - Archive previous production model (keep for rollback)\n",
    "\n",
    "**Orchestration Tools:**\n",
    "- **Airflow**: Complex DAGs, scheduling\n",
    "- **Kubeflow**: Kubernetes-native ML pipelines\n",
    "- **MLflow Projects**: Reproducible runs with conda/docker\n",
    "- **GitHub Actions**: Simple CI/CD for small teams\n",
    "\n",
    "**Post-Silicon Pipeline:**\n",
    "```\n",
    "Daily at 2 AM:\n",
    "1. Fetch yesterday's STDF files ‚Üí validate schema\n",
    "2. Engineer features ‚Üí calculate derived metrics\n",
    "3. Train yield predictor ‚Üí log to MLflow\n",
    "4. Validate accuracy > 90% ‚Üí gate\n",
    "5. Register model ‚Üí transition to Staging\n",
    "6. Deploy to 10% of test stations ‚Üí A/B test\n",
    "7. Monitor for 24 hours ‚Üí compare metrics\n",
    "8. If accuracy stable, promote to Production ‚Üí all stations\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **‚öôÔ∏è MLOps Tools Ecosystem**\n",
    "\n",
    "#### **Experiment Tracking**\n",
    "- **MLflow** (open-source): Lightweight, Python-friendly, self-hosted\n",
    "- **Weights & Biases**: Managed service, beautiful dashboards, team collaboration\n",
    "- **Neptune.ai**: Enterprise features, model registry, integrations\n",
    "\n",
    "#### **Model Registry**\n",
    "- **MLflow Registry**: Built into MLflow, stage transitions\n",
    "- **ModelDB**: Open-source, Spark integration\n",
    "- **Vertex AI Model Registry**: Google Cloud managed\n",
    "\n",
    "#### **Deployment**\n",
    "- **MLflow Models**: Multi-framework support (sklearn, TensorFlow, PyTorch)\n",
    "- **TensorFlow Serving**: High-performance TensorFlow deployment\n",
    "- **TorchServe**: PyTorch models as REST API\n",
    "- **Seldon Core**: Kubernetes-native, advanced deployment patterns\n",
    "\n",
    "#### **Monitoring**\n",
    "- **Evidently AI**: Drift detection, model quality reports\n",
    "- **Fiddler AI**: Enterprise monitoring, explainability\n",
    "- **Arize AI**: ML observability platform\n",
    "- **WhyLabs**: Data/model monitoring, anomaly detection\n",
    "\n",
    "#### **Orchestration**\n",
    "- **Airflow**: Workflow scheduling, complex DAGs\n",
    "- **Kubeflow**: End-to-end ML on Kubernetes\n",
    "- **Metaflow**: Netflix's human-centric ML framework\n",
    "- **Prefect**: Modern workflow orchestration, Python-first\n",
    "\n",
    "---\n",
    "\n",
    "### **üöÄ When to Use MLOps**\n",
    "\n",
    "**You NEED MLOps if:**\n",
    "- ‚úÖ Models deployed to production (not just notebooks)\n",
    "- ‚úÖ Multiple data scientists experimenting (need to compare 100+ runs)\n",
    "- ‚úÖ Models retrained regularly (weekly/monthly)\n",
    "- ‚úÖ Regulatory requirements (audit trails, reproducibility)\n",
    "- ‚úÖ Business-critical predictions (downtime = revenue loss)\n",
    "\n",
    "**You DON'T need MLOps if:**\n",
    "- ‚ùå One-off analysis (quick insight, never reused)\n",
    "- ‚ùå Static models (trained once, never updated)\n",
    "- ‚ùå Prototype stage (MVP, validating idea)\n",
    "\n",
    "**Post-Silicon Context:**\n",
    "- Test floor models (yield prediction, binning) ‚Üí NEED MLOps (retrain weekly, regulatory audits)\n",
    "- Exploratory analysis (one-time wafer map investigation) ‚Üí DON'T need MLOps\n",
    "\n",
    "---\n",
    "\n",
    "### **üéì Best Practices**\n",
    "\n",
    "#### **1. Start Simple, Scale Gradually**\n",
    "- **Week 1**: Log experiments to CSV files\n",
    "- **Week 2**: Adopt MLflow for experiment tracking\n",
    "- **Month 1**: Set up model registry, manual deployments\n",
    "- **Month 2**: Automate deployments with CI/CD\n",
    "- **Month 3**: Add drift monitoring, alerting\n",
    "\n",
    "#### **2. Automate Everything**\n",
    "- Manual deployment = 2 hours + human error risk\n",
    "- Automated pipeline = 10 minutes + reproducible\n",
    "\n",
    "#### **3. Monitor from Day 1**\n",
    "- \"Model deployed successfully! üéâ\" ‚Üí 6 months later ‚Üí \"Why is accuracy 60%?\"\n",
    "- Deploy monitoring BEFORE production launch\n",
    "\n",
    "#### **4. Version EVERYTHING**\n",
    "- Data version (hash, timestamp, location)\n",
    "- Code version (Git commit SHA)\n",
    "- Model version (registry version number)\n",
    "- Environment version (requirements.txt, Docker image)\n",
    "\n",
    "#### **5. Build Rollback Mechanisms**\n",
    "- Production model crashes ‚Üí instant rollback to v1.5 (last stable)\n",
    "- Blue-green deployment: Keep old version running until new version validated\n",
    "\n",
    "#### **6. Document Model Decisions**\n",
    "- Model card: \"Why Random Forest? Explainability > 2% accuracy gain from deep learning\"\n",
    "- Experiment notes: \"Tried feature X, no improvement, wasted 3 days\"\n",
    "\n",
    "---\n",
    "\n",
    "### **‚ö†Ô∏è Common Pitfalls**\n",
    "\n",
    "#### **1. Over-Engineering Too Early**\n",
    "- **Mistake**: Spend 3 months building Kubernetes MLOps platform before training first model\n",
    "- **Fix**: Start with MLflow + Flask API, scale when needed\n",
    "\n",
    "#### **2. Ignoring Data Quality**\n",
    "- **Mistake**: \"Model accuracy dropped from 92% to 60%... oh, data pipeline broke 3 weeks ago\"\n",
    "- **Fix**: Data validation in CI/CD pipeline (check schema, ranges, nulls)\n",
    "\n",
    "#### **3. No Monitoring = Silent Failures**\n",
    "- **Mistake**: Model serves predictions for 6 months, accuracy unknown\n",
    "- **Fix**: Log ground truth labels (delayed), calculate offline metrics weekly\n",
    "\n",
    "#### **4. Training/Serving Skew**\n",
    "- **Mistake**: Training uses pandas, production uses Java ‚Üí feature calculations differ ‚Üí accuracy drops\n",
    "- **Fix**: Same feature engineering code for training AND serving (use FeatureStore or shared library)\n",
    "\n",
    "#### **5. Forgetting Model Governance**\n",
    "- **Mistake**: \"Which model version was used for this prediction?\" ‚Üí No audit trail\n",
    "- **Fix**: Log model version, input features, prediction, timestamp for every request\n",
    "\n",
    "---\n",
    "\n",
    "### **üîÆ Next Steps**\n",
    "\n",
    "**After mastering this notebook:**\n",
    "1. **122_MLflow_Complete_Guide.ipynb** ‚Üí Deep dive into MLflow tracking, registry, projects\n",
    "2. **123_Model_Monitoring_Drift_Detection.ipynb** ‚Üí Advanced drift detection, alerting strategies\n",
    "3. **124_ML_CI_CD_Pipelines.ipynb** ‚Üí Airflow, GitHub Actions, automated retraining\n",
    "4. **131_Docker_Fundamentals.ipynb** ‚Üí Containerize ML models for reproducible deployments\n",
    "\n",
    "**Hands-On Practice:**\n",
    "- Deploy Notebook 121 experiment tracking example locally\n",
    "- Set up MLflow UI (port 5000), explore experiments\n",
    "- Build REST API for yield predictor (Flask + MLflow)\n",
    "- Simulate drift detection with synthetic STDF data\n",
    "\n",
    "---\n",
    "\n",
    "### **üìä MLOps Maturity Model**\n",
    "\n",
    "**Level 0: Manual Process**\n",
    "- Notebooks on laptops\n",
    "- Manual model training\n",
    "- Email models to deployment team\n",
    "\n",
    "**Level 1: Experiment Tracking**\n",
    "- MLflow logging\n",
    "- Centralized metric comparison\n",
    "- Manual deployment with scripts\n",
    "\n",
    "**Level 2: Automated Training**\n",
    "- CI/CD pipeline trains models\n",
    "- Automated validation gates\n",
    "- Model registry with staging\n",
    "\n",
    "**Level 3: Automated Deployment**\n",
    "- A/B testing automated\n",
    "- Gradual rollout (canary)\n",
    "- Monitoring dashboards\n",
    "\n",
    "**Level 4: Full MLOps**\n",
    "- Continuous training (CT)\n",
    "- Automatic drift detection ‚Üí retrain\n",
    "- Self-healing pipelines\n",
    "- Comprehensive governance\n",
    "\n",
    "**Most post-silicon teams**: Level 1-2  \n",
    "**Target for production systems**: Level 3-4\n",
    "\n",
    "---\n",
    "\n",
    "**You now have the MLOps foundation to deploy, monitor, and manage ML models in production! üöÄ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CI/CD pipeline script (conceptual)\n",
    "import os\n",
    "import mlflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def ml_pipeline():\n",
    "    \"\"\"Automated ML pipeline for daily model retraining\"\"\"\n",
    "    \n",
    "    # 1. Data validation\n",
    "    print(\"Step 1: Validating new STDF data...\")\n",
    "    # Load new data (placeholder)\n",
    "    # validate_data_schema(new_data)\n",
    "    # validate_data_quality(new_data)\n",
    "    \n",
    "    # 2. Feature engineering\n",
    "    print(\"Step 2: Engineering features...\")\n",
    "    # features = engineer_features(new_data)\n",
    "    \n",
    "    # 3. Model training\n",
    "    print(\"Step 3: Training model...\")\n",
    "    with mlflow.start_run(run_name=\"automated_retrain\"):\n",
    "        # Train model (using previous synthetic data for demo)\n",
    "        model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # 4. Model validation\n",
    "        print(\"Step 4: Validating model performance...\")\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        \n",
    "        # Accuracy threshold gate\n",
    "        ACCURACY_THRESHOLD = 0.85\n",
    "        if accuracy < ACCURACY_THRESHOLD:\n",
    "            print(f\"‚ùå Model failed validation: {accuracy:.4f} < {ACCURACY_THRESHOLD}\")\n",
    "            print(\"Pipeline aborted. Alert data science team.\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"‚úÖ Model passed validation: {accuracy:.4f}\")\n",
    "        \n",
    "        # 5. Register model\n",
    "        print(\"Step 5: Registering model...\")\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        model_uri = f\"runs:/{mlflow.active_run().info.run_id}/model\"\n",
    "        mlflow.register_model(model_uri, \"yield_predictor\")\n",
    "        \n",
    "        # 6. Transition to staging\n",
    "        print(\"Step 6: Promoting to Staging for A/B test...\")\n",
    "        # client.transition_model_version_stage(...)\n",
    "        \n",
    "        print(\"‚úÖ Pipeline completed successfully\")\n",
    "        return True\n",
    "\n",
    "# Run pipeline\n",
    "# In production, this would be triggered by cron job or Airflow DAG\n",
    "print(\"Simulating automated ML pipeline...\")\n",
    "# ml_pipeline()\n",
    "print(\"Pipeline would run daily at 2 AM via cron: 0 2 * * * python ml_pipeline.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data drift detection\n",
    "from scipy.stats import ks_2samp\n",
    "import numpy as np\n",
    "\n",
    "# Reference data (training distribution)\n",
    "reference_vdd = np.random.normal(1.2, 0.05, 1000)\n",
    "\n",
    "# Current production data (potentially drifted)\n",
    "current_vdd = np.random.normal(1.25, 0.03, 500)  # Mean shifted!\n",
    "\n",
    "# Kolmogorov-Smirnov test\n",
    "statistic, pvalue = ks_2samp(reference_vdd, current_vdd)\n",
    "\n",
    "print(f\"KS Statistic: {statistic:.4f}\")\n",
    "print(f\"P-value: {pvalue:.4f}\")\n",
    "\n",
    "if pvalue < 0.05:\n",
    "    print(\"‚ö†Ô∏è DRIFT DETECTED: Vdd distribution has significantly changed\")\n",
    "    print(\"Action: Retrain model with recent data\")\n",
    "else:\n",
    "    print(\"‚úÖ NO DRIFT: Distribution stable\")\n",
    "\n",
    "# Population Stability Index (PSI)\n",
    "def calculate_psi(reference, current, bins=10):\n",
    "    \"\"\"Calculate Population Stability Index\"\"\"\n",
    "    ref_hist, bin_edges = np.histogram(reference, bins=bins)\n",
    "    cur_hist, _ = np.histogram(current, bins=bin_edges)\n",
    "    \n",
    "    # Convert to percentages\n",
    "    ref_pct = ref_hist / len(reference)\n",
    "    cur_pct = cur_hist / len(current)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    ref_pct = np.where(ref_pct == 0, 0.0001, ref_pct)\n",
    "    cur_pct = np.where(cur_pct == 0, 0.0001, cur_pct)\n",
    "    \n",
    "    # PSI formula\n",
    "    psi = np.sum((cur_pct - ref_pct) * np.log(cur_pct / ref_pct))\n",
    "    \n",
    "    return psi\n",
    "\n",
    "psi = calculate_psi(reference_vdd, current_vdd)\n",
    "print(f\"\\nPSI: {psi:.4f}\")\n",
    "\n",
    "if psi < 0.1:\n",
    "    print(\"‚úÖ PSI < 0.1: No significant change\")\n",
    "elif psi < 0.2:\n",
    "    print(\"‚ö†Ô∏è PSI 0.1-0.2: Minor drift detected, monitor closely\")\n",
    "else:\n",
    "    print(\"üö® PSI > 0.2: Major drift! Model retrain required\")\n",
    "\n",
    "# Log drift metrics to MLflow\n",
    "mlflow.log_metric(\"vdd_ks_statistic\", statistic)\n",
    "mlflow.log_metric(\"vdd_psi\", psi)\n",
    "mlflow.log_metric(\"drift_detected\", 1 if pvalue < 0.05 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2927aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REST API deployment with Flask\n",
    "from flask import Flask, request, jsonify\n",
    "import mlflow.pyfunc\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load model once at startup\n",
    "model = mlflow.pyfunc.load_model(\"models:/yield_predictor/Production\")\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    Predict yield for device parameters\n",
    "    \n",
    "    Request body:\n",
    "    {\n",
    "        \"Vdd_V\": 1.2,\n",
    "        \"Idd_mA\": 48.5,\n",
    "        \"freq_MHz\": 1050,\n",
    "        \"temp_C\": 27\n",
    "    }\n",
    "    \"\"\"\n",
    "    data = request.json\n",
    "    \n",
    "    # Convert to DataFrame (model expects this format)\n",
    "    import pandas as pd\n",
    "    input_df = pd.DataFrame([data])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(input_df)\n",
    "    probability = prediction[0]\n",
    "    \n",
    "    # Return result\n",
    "    return jsonify({\n",
    "        'yield_probability': float(probability),\n",
    "        'risk_level': 'LOW' if probability > 0.9 else 'MEDIUM' if probability > 0.7 else 'HIGH',\n",
    "        'recommendation': 'PASS' if probability > 0.8 else 'RETEST'\n",
    "    })\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    \"\"\"Health check endpoint for monitoring\"\"\"\n",
    "    return jsonify({'status': 'healthy', 'model': 'yield_predictor', 'version': 'production'})\n",
    "\n",
    "# Run server\n",
    "# app.run(host='0.0.0.0', port=5001)\n",
    "print(\"REST API ready. Start with: app.run(host='0.0.0.0', port=5001)\")\n",
    "print(\"Test with: curl -X POST http://localhost:5001/predict -H 'Content-Type: application/json' -d '{\\\"Vdd_V\\\": 1.2, \\\"Idd_mA\\\": 50, \\\"freq_MHz\\\": 1000, \\\"temp_C\\\": 25}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a4415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model registry workflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Register the best model\n",
    "model_name = \"yield_predictor\"\n",
    "model_uri = f\"runs:/{mlflow.active_run().info.run_id}/best_model\"\n",
    "\n",
    "# Register model\n",
    "model_version = mlflow.register_model(model_uri, model_name)\n",
    "\n",
    "print(f\"Model {model_name} version {model_version.version} registered\")\n",
    "\n",
    "# Transition to staging\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    stage=\"Staging\",\n",
    "    archive_existing_versions=False\n",
    ")\n",
    "\n",
    "print(f\"Model transitioned to Staging\")\n",
    "\n",
    "# After validation, promote to production\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_version.version,\n",
    "    stage=\"Production\",\n",
    "    archive_existing_versions=True  # Archive previous production model\n",
    ")\n",
    "\n",
    "print(f\"Model promoted to Production\")\n",
    "\n",
    "# Load production model for inference\n",
    "production_model = mlflow.pyfunc.load_model(f\"models:/{model_name}/Production\")\n",
    "print(f\"Production model loaded and ready for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Grid search with MLflow tracking\n",
    "with mlflow.start_run(run_name=\"grid_search_rf\"):\n",
    "    grid_search = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Log best parameters\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        mlflow.log_param(f\"best_{param}\", value)\n",
    "    \n",
    "    # Log best score\n",
    "    mlflow.log_metric(\"best_cv_f1\", grid_search.best_score_)\n",
    "    \n",
    "    # Test set evaluation\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    test_f1 = f1_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"test_f1\", test_f1)\n",
    "    \n",
    "    # Log best model\n",
    "    mlflow.sklearn.log_model(grid_search.best_estimator_, \"best_model\")\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV F1: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Test F1: {test_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
