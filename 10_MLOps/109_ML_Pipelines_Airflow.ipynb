{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4bf8dce",
   "metadata": {},
   "source": [
    "# 109: ML Pipelines with Apache Airflow\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** DAG (Directed Acyclic Graph) design for ML workflows\n",
    "- **Implement** end-to-end ML pipeline: data extraction ‚Üí training ‚Üí validation ‚Üí deployment\n",
    "- **Build** automated retraining schedules with dependency management\n",
    "- **Apply** Airflow to semiconductor test data pipelines (STDF ‚Üí features ‚Üí models)\n",
    "- **Evaluate** pipeline monitoring, failure recovery, and backfilling strategies\n",
    "\n",
    "## üìö What are ML Pipelines?\n",
    "\n",
    "ML pipelines orchestrate the sequence of steps from raw data to deployed model predictions. Unlike one-off notebook experiments, production ML requires repeatable, monitored workflows that handle failures gracefully. Apache Airflow represents pipelines as DAGs where nodes are tasks (Python functions, SQL queries, model training) and edges are dependencies (\"train model only after data validation passes\").\n",
    "\n",
    "Airflow's killer features: **scheduling** (daily retraining at 2 AM), **dependency management** (skip deployment if accuracy < threshold), **retries** (network glitches don't break pipelines), **monitoring** (SLA alerts if pipeline exceeds 4 hours), and **backfilling** (reprocess last 30 days after bug fix). Tasks run in isolated environments (Docker containers), enabling language polyglot pipelines (Python preprocessing ‚Üí Spark training ‚Üí R validation).\n",
    "\n",
    "In semiconductor manufacturing, Airflow orchestrates nightly STDF data ingestion (extract from test servers ‚Üí parse to DataFrames ‚Üí quality checks ‚Üí feature engineering ‚Üí model retraining ‚Üí deploy if improved ‚Üí notify engineers). When a tester goes offline, the pipeline detects missing data, skips dependent tasks, and alerts on-call. Manual interventions (approve deployment) integrate seamlessly via sensors.\n",
    "\n",
    "**Why ML Pipelines with Airflow?**\n",
    "- ‚úÖ **Automation**: Zero-touch retraining, no manual notebook runs\n",
    "- ‚úÖ **Reliability**: Automatic retries, failure notifications, SLA monitoring\n",
    "- ‚úÖ **Scalability**: Distribute tasks across workers (Kubernetes, Celery)\n",
    "- ‚úÖ **Observability**: Web UI shows every run, logs, task durations\n",
    "- ‚úÖ **Version Control**: DAGs as code, track changes in Git\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Use Case 1: Daily Yield Model Retraining**\n",
    "- **Pipeline**: STDF extraction ‚Üí data quality checks ‚Üí feature engineering ‚Üí model training ‚Üí A/B test ‚Üí deploy if wins\n",
    "- **Schedule**: 2 AM daily (after all test data uploaded)\n",
    "- **Trigger**: 10K+ new devices tested (skip if insufficient data)\n",
    "- **Monitoring**: SLA = 4 hours, alert if accuracy < 90%\n",
    "- **Impact**: Model adapts to process drift within 24 hours (vs weekly manual retrains)\n",
    "\n",
    "**Use Case 2: Multi-Fab Data Aggregation**\n",
    "- **Pipeline**: Fab1 STDF + Fab2 STDF + Fab3 STDF ‚Üí merge ‚Üí normalize ‚Üí feature store update\n",
    "- **Schedule**: Hourly (streaming-like batch processing)\n",
    "- **Dependencies**: Wait for all 3 fabs, timeout after 2 hours\n",
    "- **Backfill**: Reprocess 90 days when Fab2 fixes timestamp bug\n",
    "- **Impact**: Unified feature store across global manufacturing network\n",
    "\n",
    "**Use Case 3: Automated Model Validation Pipeline**\n",
    "- **Pipeline**: Candidate model ‚Üí offline metrics ‚Üí simulation ‚Üí champion/challenger A/B test ‚Üí gradual rollout\n",
    "- **Trigger**: New model registered in MLflow\n",
    "- **Human-in-loop**: Approval sensor before production deployment\n",
    "- **Rollback**: Auto-rollback if production accuracy drops >5%\n",
    "- **Impact**: 10 model deployments/month (vs 2/month manual)\n",
    "\n",
    "**Use Case 4: STDF Quality Monitoring Pipeline**\n",
    "- **Pipeline**: Ingest STDF ‚Üí schema validation ‚Üí statistical checks ‚Üí alert on anomalies\n",
    "- **Schedule**: Every 15 minutes (near real-time)\n",
    "- **Checks**: Missing parameters, out-of-range values, duplicate records\n",
    "- **Action**: Quarantine bad batches, notify data engineering\n",
    "- **Impact**: Catch data quality issues before model training (prevents garbage-in-garbage-out)\n",
    "\n",
    "## üîÑ Airflow Pipeline Architecture\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Scheduler] --> B[DAG Definition]\n",
    "    B --> C[Task Queue]\n",
    "    \n",
    "    C --> D[Worker 1]\n",
    "    C --> E[Worker 2]\n",
    "    C --> F[Worker N]\n",
    "    \n",
    "    D --> G[Extract STDF]\n",
    "    E --> H[Train Model]\n",
    "    F --> I[Deploy Model]\n",
    "    \n",
    "    G --> J[Data Quality Check]\n",
    "    J --> K{Checks Pass?}\n",
    "    \n",
    "    K -->|Yes| L[Feature Engineering]\n",
    "    K -->|No| M[Alert & Skip]\n",
    "    \n",
    "    L --> H\n",
    "    H --> N[Model Validation]\n",
    "    N --> O{Accuracy OK?}\n",
    "    \n",
    "    O -->|Yes| I\n",
    "    O -->|No| P[Notify Team]\n",
    "    \n",
    "    I --> Q[Production Serving]\n",
    "    \n",
    "    B --> R[Metadata DB]\n",
    "    R --> S[Web UI]\n",
    "    R --> T[Logs]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style Q fill:#e1ffe1\n",
    "    style M fill:#ffe1e1\n",
    "    style P fill:#ffe1e1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **091**: SQL Advanced - Data extraction queries\n",
    "- **107**: Model Monitoring - Detecting when to retrain\n",
    "- **108**: Feature Stores - Centralized feature management\n",
    "\n",
    "**This Notebook (109):**\n",
    "- Airflow DAG creation and task definition\n",
    "- Task dependencies and branching logic\n",
    "- Scheduling (cron expressions, triggers)\n",
    "- Failure handling and retries\n",
    "- Pipeline monitoring and SLAs\n",
    "\n",
    "**Next Steps:**\n",
    "- **131**: Cloud Deployment - Airflow on Kubernetes\n",
    "- **132**: CI/CD for ML - Automated testing and deployment\n",
    "\n",
    "---\n",
    "\n",
    "Let's automate ML workflows end-to-end! üîÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6f3f9c",
   "metadata": {},
   "source": [
    "## 1. Setup and Airflow Concepts\n",
    "\n",
    "**Note:** This notebook teaches Airflow concepts. Full deployment requires:\n",
    "- `pip install apache-airflow`\n",
    "- Metadata database (PostgreSQL recommended)\n",
    "- Executor (LocalExecutor, CeleryExecutor, KubernetesExecutor)\n",
    "\n",
    "We'll demonstrate pipeline design with simulated task execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2988bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"‚úÖ Pipeline simulation environment ready!\")\n",
    "print(\"\\nüìù Airflow Installation:\")\n",
    "print(\"   pip install 'apache-airflow==2.7.3' --constraint requirements.txt\")\n",
    "print(\"   airflow db init\")\n",
    "print(\"   airflow users create --username admin --password admin --role Admin\")\n",
    "print(\"   airflow webserver -p 8080\")\n",
    "print(\"   airflow scheduler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8732cf",
   "metadata": {},
   "source": [
    "## 2. DAG Design: Yield Model Retraining Pipeline\n",
    "\n",
    "**Purpose:** Design end-to-end pipeline from data ingestion to model deployment.\n",
    "\n",
    "**Key Points:**\n",
    "- **Task 1**: Extract STDF data from test servers\n",
    "- **Task 2**: Data quality validation (schema, ranges, completeness)\n",
    "- **Task 3**: Feature engineering (aggregations, transformations)\n",
    "- **Task 4**: Model training (RandomForest on engineered features)\n",
    "- **Task 5**: Model validation (compare to baseline)\n",
    "- **Task 6**: Deploy if improved (otherwise skip)\n",
    "- **Why this matters**: Dependencies ensure data quality before expensive training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated Airflow DAG structure (actual DAG would be in airflow/dags/ folder)\n",
    "\n",
    "class PipelineTask:\n",
    "    \"\"\"Simulated Airflow task for demonstration.\"\"\"\n",
    "    def __init__(self, task_id, dependencies=None):\n",
    "        self.task_id = task_id\n",
    "        self.dependencies = dependencies or []\n",
    "        self.status = 'pending'\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.result = None\n",
    "    \n",
    "    def execute(self, context=None):\n",
    "        \"\"\"Simulate task execution.\"\"\"\n",
    "        self.status = 'running'\n",
    "        self.start_time = datetime.now()\n",
    "        print(f\"[{self.start_time.strftime('%H:%M:%S')}] ‚ñ∂Ô∏è  {self.task_id} started\")\n",
    "        \n",
    "        # Simulate work\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        self.end_time = datetime.now()\n",
    "        self.status = 'success'\n",
    "        duration = (self.end_time - self.start_time).total_seconds()\n",
    "        print(f\"[{self.end_time.strftime('%H:%M:%S')}] ‚úÖ {self.task_id} completed ({duration:.2f}s)\")\n",
    "        \n",
    "        return self.result\n",
    "\n",
    "# Define pipeline tasks\n",
    "task_extract_stdf = PipelineTask('extract_stdf_data')\n",
    "task_validate_data = PipelineTask('validate_data_quality', dependencies=[task_extract_stdf])\n",
    "task_engineer_features = PipelineTask('engineer_features', dependencies=[task_validate_data])\n",
    "task_train_model = PipelineTask('train_yield_model', dependencies=[task_engineer_features])\n",
    "task_validate_model = PipelineTask('validate_model_accuracy', dependencies=[task_train_model])\n",
    "task_deploy_model = PipelineTask('deploy_to_production', dependencies=[task_validate_model])\n",
    "\n",
    "# Pipeline metadata\n",
    "pipeline_config = {\n",
    "    'dag_id': 'yield_model_retrain_daily',\n",
    "    'schedule': '0 2 * * *',  # 2 AM daily (cron expression)\n",
    "    'start_date': datetime(2025, 12, 1),\n",
    "    'catchup': False,  # Don't backfill missed runs\n",
    "    'max_active_runs': 1,  # One run at a time\n",
    "    'sla_minutes': 240,  # 4 hour SLA\n",
    "    'tasks': [\n",
    "        task_extract_stdf,\n",
    "        task_validate_data,\n",
    "        task_engineer_features,\n",
    "        task_train_model,\n",
    "        task_validate_model,\n",
    "        task_deploy_model\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Pipeline DAG Design:\")\n",
    "print(f\"  DAG ID: {pipeline_config['dag_id']}\")\n",
    "print(f\"  Schedule: {pipeline_config['schedule']} (daily at 2 AM)\")\n",
    "print(f\"  SLA: {pipeline_config['sla_minutes']} minutes\")\n",
    "print(f\"\\nTask Dependency Graph:\")\n",
    "print(\"  extract_stdf_data\")\n",
    "print(\"    ‚Üì\")\n",
    "print(\"  validate_data_quality\")\n",
    "print(\"    ‚Üì\")\n",
    "print(\"  engineer_features\")\n",
    "print(\"    ‚Üì\")\n",
    "print(\"  train_yield_model\")\n",
    "print(\"    ‚Üì\")\n",
    "print(\"  validate_model_accuracy\")\n",
    "print(\"    ‚Üì\")\n",
    "print(\"  deploy_to_production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cff72d",
   "metadata": {},
   "source": [
    "## 3. DAG Execution Simulation\n",
    "\n",
    "**Purpose:** Run the training pipeline DAG to demonstrate task execution flow.\n",
    "\n",
    "**Key Points:**\n",
    "- **Sequential Execution**: Tasks run in dependency order (extract ‚Üí transform ‚Üí train ‚Üí evaluate)\n",
    "- **State Management**: Track task status (queued ‚Üí running ‚Üí success/failed)\n",
    "- **Idempotency**: Re-running DAG produces same results (critical for debugging)\n",
    "- **Logging**: Capture stdout/stderr for each task for troubleshooting\n",
    "\n",
    "**Why This Matters:** In production, Airflow scheduler executes DAGs automatically on schedule. Understanding execution flow is critical for debugging failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf4b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate DAG execution\n",
    "import time\n",
    "\n",
    "def execute_dag(dag):\n",
    "    \"\"\"Simulate Airflow DAG execution with task dependencies.\"\"\"\n",
    "    print(f\"üöÄ Starting DAG: {dag['dag_id']}\")\n",
    "    print(f\"Schedule: {dag['schedule']}\\n\")\n",
    "    \n",
    "    # Topologically sort tasks by dependencies\n",
    "    task_order = ['extract_data', 'transform_features', 'train_model', 'evaluate_model']\n",
    "    \n",
    "    task_states = {}\n",
    "    for task_name in task_order:\n",
    "        task = dag['tasks'][task_name]\n",
    "        print(f\"‚ñ∂ Executing task: {task_name}\")\n",
    "        print(f\"  Description: {task['description']}\")\n",
    "        \n",
    "        # Simulate task execution time\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Simulate success (in real Airflow, could be success/failed/retry)\n",
    "        task_states[task_name] = 'success'\n",
    "        print(f\"  ‚úÖ Status: {task_states[task_name]}\\n\")\n",
    "    \n",
    "    print(f\"üéâ DAG Execution Complete!\")\n",
    "    print(f\"  Total Tasks: {len(task_order)}\")\n",
    "    print(f\"  Successful: {sum([1 for s in task_states.values() if s == 'success'])}\")\n",
    "    \n",
    "    return task_states\n",
    "\n",
    "# Execute the DAG\n",
    "execution_states = execute_dag(ml_training_dag)\n",
    "\n",
    "# Visualize task execution timeline\n",
    "execution_times = {\n",
    "    'extract_data': 45,  # seconds\n",
    "    'transform_features': 120,\n",
    "    'train_model': 300,\n",
    "    'evaluate_model': 30\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "tasks = list(execution_times.keys())\n",
    "times = list(execution_times.values())\n",
    "colors = ['green' if execution_states[t] == 'success' else 'red' for t in tasks]\n",
    "\n",
    "plt.barh(tasks, times, color=colors, edgecolor='black')\n",
    "plt.xlabel('Execution Time (seconds)')\n",
    "plt.title('DAG Task Execution Timeline')\n",
    "plt.axvline(x=600, color='red', linestyle='--', label='SLA: 10 minutes')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal Pipeline Time: {sum(times)} seconds ({sum(times)/60:.1f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c6339b",
   "metadata": {},
   "source": [
    "## 4. Task Failure Handling & Retries\n",
    "\n",
    "**Purpose:** Configure retry logic and failure notifications for robust pipelines.\n",
    "\n",
    "**Key Points:**\n",
    "- **Retries**: Automatically retry failed tasks (e.g., network timeouts) up to N times\n",
    "- **Exponential Backoff**: Wait 2^retry_number minutes between retries (prevent overwhelming systems)\n",
    "- **Alerts**: Send Slack/PagerDuty notifications on permanent failures\n",
    "- **Circuit Breaker**: Stop downstream tasks if critical task fails (e.g., data extraction)\n",
    "\n",
    "**Why This Matters:** Production data pipelines fail 5-10% of the time. Proper retry logic prevents manual intervention for transient errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate task retry logic\n",
    "def execute_task_with_retries(task_name, max_retries=3, failure_rate=0.3):\n",
    "    \"\"\"Simulate task execution with retry logic.\"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        # Simulate random failure (30% failure rate)\n",
    "        success = np.random.random() > failure_rate\n",
    "        \n",
    "        print(f\"  Attempt {attempt}/{max_retries}: \", end=\"\")\n",
    "        if success:\n",
    "            print(f\"‚úÖ SUCCESS\")\n",
    "            return 'success'\n",
    "        else:\n",
    "            print(f\"‚ùå FAILED (network timeout)\")\n",
    "            if attempt < max_retries:\n",
    "                backoff_seconds = 2 ** attempt\n",
    "                print(f\"    ‚è≥ Retrying in {backoff_seconds} seconds...\")\n",
    "                time.sleep(0.2)  # Simulate backoff (shortened for demo)\n",
    "    \n",
    "    print(f\"  üö® Task {task_name} failed after {max_retries} attempts!\")\n",
    "    return 'failed'\n",
    "\n",
    "# Example: Retry flaky extract_data task\n",
    "print(\"Testing Retry Logic for 'extract_data' task:\\n\")\n",
    "np.random.seed(42)  # For reproducibility\n",
    "result = execute_task_with_retries('extract_data', max_retries=3, failure_rate=0.5)\n",
    "\n",
    "# Visualize retry success rates\n",
    "retry_scenarios = []\n",
    "for failure_rate in [0.1, 0.3, 0.5, 0.7]:\n",
    "    successes = 0\n",
    "    for _ in range(100):\n",
    "        if execute_task_with_retries('test_task', max_retries=3, failure_rate=failure_rate) == 'success':\n",
    "            successes += 1\n",
    "    retry_scenarios.append({'Failure Rate': f'{failure_rate*100:.0f}%', 'Success Rate': successes})\n",
    "\n",
    "retry_df = pd.DataFrame(retry_scenarios)\n",
    "print(f\"\\n\\nRetry Strategy Effectiveness (100 trials each):\")\n",
    "print(retry_df)\n",
    "\n",
    "# Alert configuration\n",
    "alert_config = {\n",
    "    'on_failure_callback': 'send_slack_alert',\n",
    "    'sla_miss_callback': 'send_pagerduty_alert',\n",
    "    'email_on_failure': True,\n",
    "    'email_to': 'ml-ops-team@company.com'\n",
    "}\n",
    "\n",
    "print(f\"\\n\\nüìß Alert Configuration:\")\n",
    "for key, value in alert_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9477b11a",
   "metadata": {},
   "source": [
    "## 5. DAG Monitoring Dashboard\n",
    "\n",
    "**Purpose:** Visualize pipeline health metrics for operational oversight.\n",
    "\n",
    "**Key Points:**\n",
    "- **DAG Run History**: Track success/failure rates over time\n",
    "- **Task Duration Trends**: Identify tasks getting slower (data volume growth)\n",
    "- **SLA Violations**: Alert when pipelines exceed time budgets\n",
    "- **Resource Usage**: Monitor CPU/memory per task for optimization\n",
    "\n",
    "**Why This Matters:** Data engineers need dashboards to proactively fix bottlenecks before stakeholders complain about delayed models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc4e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate DAG run history (30 days)\n",
    "np.random.seed(100)\n",
    "dates = pd.date_range(end=pd.Timestamp.now(), periods=30, freq='D')\n",
    "dag_runs = []\n",
    "\n",
    "for date in dates:\n",
    "    # Simulate success/failure (90% success rate)\n",
    "    status = 'success' if np.random.random() > 0.1 else 'failed'\n",
    "    duration = np.random.normal(480, 60) if status == 'success' else np.random.normal(200, 50)  # seconds\n",
    "    \n",
    "    dag_runs.append({\n",
    "        'date': date,\n",
    "        'status': status,\n",
    "        'duration_seconds': max(duration, 0),\n",
    "        'tasks_succeeded': 4 if status == 'success' else np.random.randint(0, 4),\n",
    "        'tasks_failed': 0 if status == 'success' else np.random.randint(1, 5)\n",
    "    })\n",
    "\n",
    "dag_history_df = pd.DataFrame(dag_runs)\n",
    "\n",
    "# Visualization Dashboard\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Airflow DAG Monitoring Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. DAG Run Success Rate Over Time\n",
    "success_rate = dag_history_df.groupby(dag_history_df['date'].dt.date)['status'].apply(\n",
    "    lambda x: (x == 'success').sum() / len(x) * 100\n",
    ")\n",
    "axes[0, 0].plot(success_rate.index, success_rate.values, marker='o', color='green', linewidth=2)\n",
    "axes[0, 0].axhline(y=95, color='red', linestyle='--', label='Target: 95%')\n",
    "axes[0, 0].set_title('DAG Success Rate (30 Days)')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Success Rate (%)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Task Duration Trend\n",
    "axes[0, 1].plot(dag_history_df['date'], dag_history_df['duration_seconds'], color='blue', alpha=0.6)\n",
    "axes[0, 1].axhline(y=600, color='red', linestyle='--', label='SLA: 10 minutes')\n",
    "axes[0, 1].set_title('Pipeline Duration Trend')\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Duration (seconds)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Task Success vs Failure Count\n",
    "status_counts = dag_history_df['status'].value_counts()\n",
    "axes[1, 0].bar(status_counts.index, status_counts.values, color=['green', 'red'], edgecolor='black')\n",
    "axes[1, 0].set_title('DAG Run Outcomes (30 Days)')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "# 4. SLA Violations\n",
    "sla_violations = dag_history_df[dag_history_df['duration_seconds'] > 600]\n",
    "axes[1, 1].text(0.1, 0.6, f\"\"\"\n",
    "SLA MONITORING SUMMARY\n",
    "======================\n",
    "Total Runs: {len(dag_history_df)}\n",
    "Successful: {(dag_history_df['status'] == 'success').sum()}\n",
    "Failed: {(dag_history_df['status'] == 'failed').sum()}\n",
    "\n",
    "SLA Violations: {len(sla_violations)} runs > 10 min\n",
    "Average Duration: {dag_history_df['duration_seconds'].mean():.0f}s\n",
    "\n",
    "üü¢ Uptime: {(dag_history_df['status'] == 'success').mean() * 100:.1f}%\n",
    "\"\"\", fontsize=11, family='monospace', verticalalignment='center',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä DAG Health Metrics (Last 30 Days):\")\n",
    "print(f\"  Success Rate: {(dag_history_df['status'] == 'success').mean() * 100:.1f}%\")\n",
    "print(f\"  Average Duration: {dag_history_df['duration_seconds'].mean():.0f} seconds\")\n",
    "print(f\"  SLA Violations: {len(sla_violations)} / {len(dag_history_df)} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd910d74",
   "metadata": {},
   "source": [
    "## 6. Backfilling Historical Data\n",
    "\n",
    "**Purpose:** Re-run DAG for past dates to fill data gaps or fix pipeline bugs.\n",
    "\n",
    "**Key Points:**\n",
    "- **Use Case**: Bug in feature engineering found ‚Üí need to regenerate training data for last 90 days\n",
    "- **Backfill Command**: `airflow dags backfill --start-date 2024-01-01 --end-date 2024-03-31`\n",
    "- **Idempotency**: Tasks check if output already exists ‚Üí skip recomputation (save costs)\n",
    "- **Parallelism**: Run multiple backfill instances simultaneously (date partitioning)\n",
    "\n",
    "**Why This Matters:** Data bugs are common. Backfilling prevents manual data fixes and ensures reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c5965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate backfill scenario\n",
    "backfill_dates = pd.date_range(start='2024-01-01', end='2024-01-10', freq='D')\n",
    "\n",
    "print(\"üîÑ Backfilling DAG for date range: 2024-01-01 to 2024-01-10\")\n",
    "print(f\"Total runs to execute: {len(backfill_dates)}\\n\")\n",
    "\n",
    "backfill_results = []\n",
    "for date in backfill_dates:\n",
    "    # Simulate backfill execution (check if data exists, skip if yes)\n",
    "    data_exists = np.random.random() > 0.7  # 30% already exist\n",
    "    \n",
    "    if data_exists:\n",
    "        print(f\"  {date.date()}: ‚è© SKIPPED (data already exists)\")\n",
    "        status = 'skipped'\n",
    "        duration = 0\n",
    "    else:\n",
    "        print(f\"  {date.date()}: ‚ñ∂ RUNNING backfill...\")\n",
    "        time.sleep(0.1)\n",
    "        status = 'success'\n",
    "        duration = np.random.normal(480, 60)\n",
    "        print(f\"  {date.date()}: ‚úÖ COMPLETED ({duration:.0f}s)\")\n",
    "    \n",
    "    backfill_results.append({\n",
    "        'date': date,\n",
    "        'status': status,\n",
    "        'duration_seconds': duration\n",
    "    })\n",
    "\n",
    "backfill_df = pd.DataFrame(backfill_results)\n",
    "\n",
    "print(f\"\\n\\nüìà Backfill Summary:\")\n",
    "print(f\"  Total Dates: {len(backfill_dates)}\")\n",
    "print(f\"  Skipped (already existed): {(backfill_df['status'] == 'skipped').sum()}\")\n",
    "print(f\"  Executed: {(backfill_df['status'] == 'success').sum()}\")\n",
    "print(f\"  Total Time: {backfill_df['duration_seconds'].sum():.0f} seconds ({backfill_df['duration_seconds'].sum()/60:.1f} minutes)\")\n",
    "\n",
    "# Visualize backfill progress\n",
    "plt.figure(figsize=(10, 5))\n",
    "colors = ['gray' if s == 'skipped' else 'green' for s in backfill_df['status']]\n",
    "plt.bar(range(len(backfill_df)), backfill_df['duration_seconds'], color=colors, edgecolor='black')\n",
    "plt.xticks(range(len(backfill_df)), [d.strftime('%m-%d') for d in backfill_df['date']], rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Duration (seconds)')\n",
    "plt.title('Backfill Execution Timeline')\n",
    "plt.legend(['Skipped', 'Executed'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a49a9b1",
   "metadata": {},
   "source": [
    "## üöÄ Real-World Project Templates\n",
    "\n",
    "Build production ML pipelines using these architectures:\n",
    "\n",
    "### 1Ô∏è‚É£ **Post-Silicon Yield Prediction Pipeline**\n",
    "- **Objective**: Daily retraining pipeline for wafer yield forecasting models  \n",
    "- **DAG Tasks**: Extract STDF files ‚Üí Parse parametric data ‚Üí Feature engineering ‚Üí Train RF model ‚Üí Validate ‚Üí Deploy  \n",
    "- **Success Metric**: < 30 min end-to-end, 99% uptime  \n",
    "- **Features**: Parallel wafer processing, incremental training, automated A/B testing  \n",
    "- **Tech Stack**: Airflow, Spark (STDF parsing), MLflow (model registry), Kubernetes (training jobs)\n",
    "\n",
    "### 2Ô∏è‚É£ **E-Commerce Recommendation Retraining**\n",
    "- **Objective**: Weekly collaborative filtering model update with fresh user interactions  \n",
    "- **DAG Tasks**: S3 clickstream ‚Üí Feature aggregation ‚Üí Matrix factorization ‚Üí Evaluate top-K ‚Üí Deploy to Redis  \n",
    "- **Success Metric**: Maintain CTR > 3.5% with weekly updates  \n",
    "- **Features**: Cold-start user handling, popularity bias correction, diversity constraints  \n",
    "- **Tech Stack**: Airflow, Spark (ALS), Feast (feature store), SageMaker endpoints\n",
    "\n",
    "### 3Ô∏è‚É£ **Fraud Detection Real-Time Pipeline**\n",
    "- **Objective**: Hourly feature refresh + model retraining on new fraud patterns  \n",
    "- **DAG Tasks**: Kafka ‚Üí Feature extraction ‚Üí Streaming aggregations ‚Üí GBDT training ‚Üí Deploy via Docker  \n",
    "- **Success Metric**: Detect new fraud tactics within 24 hours  \n",
    "- **Features**: Streaming features (transaction velocity), adversarial validation, explainability logging  \n",
    "- **Tech Stack**: Airflow, Flink (streaming), XGBoost, SHAP, PostgreSQL\n",
    "\n",
    "### 4Ô∏è‚É£ **Autonomous Vehicle Model Pipeline**\n",
    "- **Objective**: Nightly perception model retraining from fleet data  \n",
    "- **DAG Tasks**: S3 sensor logs ‚Üí Video labeling ‚Üí Image augmentation ‚Üí CNN training ‚Üí TensorRT optimization ‚Üí OTA deploy  \n",
    "- **Success Metric**: mAP > 0.85 for object detection, < 50ms inference  \n",
    "- **Features**: Active learning (select hard examples), multi-GPU training, quantization  \n",
    "- **Tech Stack**: Airflow, PyTorch, TensorRT, CVAT (labeling), Weights & Biases\n",
    "\n",
    "### 5Ô∏è‚É£ **Healthcare Readmission Risk Pipeline**\n",
    "- **Objective**: Monthly HIPAA-compliant model updates for patient risk scoring  \n",
    "- **DAG Tasks**: EMR extraction ‚Üí De-identification ‚Üí Feature engineering ‚Üí Logistic regression ‚Üí Explainability ‚Üí Audit log  \n",
    "- **Success Metric**: AUROC > 0.80, full audit trail for compliance  \n",
    "- **Features**: Temporal cross-validation, fairness metrics (demographic parity), LIME explanations  \n",
    "- **Tech Stack**: Airflow (on-prem), Snowflake (encrypted), scikit-learn, Aequitas (fairness)\n",
    "\n",
    "### 6Ô∏è‚É£ **Financial Trading Signal Pipeline**\n",
    "- **Objective**: Every 5 minutes: retrain short-term momentum models  \n",
    "- **DAG Tasks**: Market data API ‚Üí Technical indicators ‚Üí Ensemble model ‚Üí Backtesting ‚Üí Deploy to low-latency C++ engine  \n",
    "- **Success Metric**: Sharpe ratio > 1.5, < 1ms inference latency  \n",
    "- **Features**: Walk-forward validation, transaction cost simulation, market regime detection  \n",
    "- **Tech Stack**: Airflow, kdb+/q, LightGBM, custom C++ inference\n",
    "\n",
    "### 7Ô∏è‚É£ **Supply Chain Demand Forecasting**\n",
    "- **Objective**: Daily SKU-level demand forecasts for 10K products  \n",
    "- **DAG Tasks**: POS sales ‚Üí Promotional calendar ‚Üí Weather API ‚Üí Prophet forecasting ‚Üí Inventory optimizer ‚Üí ERP upload  \n",
    "- **Success Metric**: < 10% MAPE for 90% of SKUs  \n",
    "- **Features**: Hierarchical forecasting, promotional lift modeling, safety stock calculation  \n",
    "- **Tech Stack**: Airflow, Spark, Prophet/Chronos, Snowflake, SAP integration\n",
    "\n",
    "### 8Ô∏è‚É£ **Smart Grid Load Forecasting Pipeline**\n",
    "- **Objective**: Hourly regional electricity demand forecasts  \n",
    "- **DAG Tasks**: Smart meter aggregation ‚Üí Weather forecast ‚Üí Holiday calendar ‚Üí LSTM training ‚Üí Grid balancing API  \n",
    "- **Success Metric**: < 3% MAPE for 24-hour ahead, < 5 min pipeline time  \n",
    "- **Features**: Multi-horizon forecasting, weather scenario ensembles, renewable energy integration  \n",
    "- **Tech Stack**: Airflow, TensorFlow, InfluxDB (timeseries), Grafana alerts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9b4e21",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### What are ML Pipelines?\n",
    "Automated workflows that orchestrate data ingestion, feature engineering, model training, evaluation, and deployment with dependency management and scheduling.\n",
    "\n",
    "### Why Airflow for ML?\n",
    "- **Python-Native**: Define workflows as code (DAGs) using Python\n",
    "- **Rich Ecosystem**: 200+ operators for AWS, GCP, Spark, Kubernetes, databases\n",
    "- **Dependency Management**: Enforces task execution order automatically\n",
    "- **Scalability**: Distribute tasks across worker nodes (Celery/Kubernetes executors)\n",
    "- **Monitoring**: Built-in UI for tracking DAG runs, logs, and failures\n",
    "\n",
    "### Core Airflow Concepts\n",
    "\n",
    "| **Concept** | **Definition** | **Example** |\n",
    "|------------|---------------|------------|\n",
    "| **DAG** | Directed Acyclic Graph (workflow definition) | `ml_training_dag = DAG(dag_id=\"train_model\", schedule=\"@daily\")` |\n",
    "| **Task** | Single unit of work | `extract_data = PythonOperator(task_id=\"extract\", python_callable=extract_fn)` |\n",
    "| **Operator** | Task template | `BashOperator`, `PythonOperator`, `SparkSubmitOperator` |\n",
    "| **Executor** | Task runner | `LocalExecutor` (dev), `CeleryExecutor` (prod), `KubernetesExecutor` (cloud) |\n",
    "| **Schedule** | Cron or preset | `@daily`, `@hourly`, `0 2 * * *` (2 AM daily) |\n",
    "| **Sensor** | Waits for condition | `S3KeySensor` (wait for file), `ExternalTaskSensor` |\n",
    "\n",
    "### DAG Design Best Practices\n",
    "\n",
    "**‚úÖ Good DAG Design:**\n",
    "```python\n",
    "extract_data >> transform_features >> train_model >> evaluate_model >> deploy_model\n",
    "# Clear linear dependency, easy to debug\n",
    "```\n",
    "\n",
    "**‚ùå Bad DAG Design:**\n",
    "```python\n",
    "task1 >> [task2, task3, task4] >> task5\n",
    "task2 >> task6\n",
    "# Hidden dependencies, hard to troubleshoot failures\n",
    "```\n",
    "\n",
    "**Principles:**\n",
    "- **Idempotency**: Re-running same DAG produces same result (no side effects)\n",
    "- **Atomicity**: Each task should be independently retry-able\n",
    "- **Modularity**: Break complex tasks into smaller testable units\n",
    "- **Observability**: Log inputs/outputs, metrics, and errors explicitly\n",
    "\n",
    "### Retry Configuration\n",
    "\n",
    "```python\n",
    "default_args = {\n",
    "    'retries': 3,  # Retry up to 3 times\n",
    "    'retry_delay': timedelta(minutes=5),  # Wait 5 min between retries\n",
    "    'retry_exponential_backoff': True,  # 5min ‚Üí 10min ‚Üí 20min\n",
    "    'execution_timeout': timedelta(hours=2),  # Kill if > 2 hours\n",
    "    'on_failure_callback': send_slack_alert,  # Custom alert function\n",
    "}\n",
    "```\n",
    "\n",
    "### Airflow vs Alternatives\n",
    "\n",
    "| **Tool** | **Best For** | **Strengths** | **Weaknesses** |\n",
    "|---------|-------------|--------------|----------------|\n",
    "| **Airflow** | Python-centric, batch workflows | Mature, huge community, flexible | Complex setup, not for streaming |\n",
    "| **Prefect** | Modern Python orchestration | Better UX, dynamic DAGs | Smaller ecosystem |\n",
    "| **Kubeflow Pipelines** | Kubernetes-native ML | Native K8s integration, versioning | Steep learning curve |\n",
    "| **Luigi** | Simple pipelines | Lightweight, no database | Limited features, less active |\n",
    "| **Dagster** | Data pipelines with types | Type safety, great testing | Newer, fewer integrations |\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "- ‚ùå **Database in Task Logic**: Don't query DBs inside tasks ‚Üí use XCom for small data, external storage for large\n",
    "- ‚ùå **No SLA Monitoring**: Pipelines silently slow down ‚Üí set `sla` parameter\n",
    "- ‚ùå **Hardcoded Dates**: Use Airflow macros: `{{ ds }}` (execution date), `{{ prev_ds }}`\n",
    "- ‚ùå **Too Many Small Tasks**: 100 tasks/DAG ‚Üí scheduler overhead. Aim for 10-30 tasks.\n",
    "- ‚ùå **Ignoring Backpressure**: Don't queue 1000 DAG runs ‚Üí use `max_active_runs=3`\n",
    "\n",
    "### Post-Silicon Pipeline Patterns\n",
    "\n",
    "**Wafer Test Data Pipeline:**\n",
    "1. **Extract**: Download STDF files from test equipment servers (daily)\n",
    "2. **Parse**: Convert binary STDF ‚Üí Parquet (Spark job)\n",
    "3. **Aggregate**: Compute wafer-level statistics (yield%, test time)\n",
    "4. **Feature Engineering**: Spatial features (die neighbors), temporal trends\n",
    "5. **Train**: Random Forest for yield prediction\n",
    "6. **Validate**: Backtest on last 30 days\n",
    "7. **Deploy**: Update model in production API\n",
    "\n",
    "**Monitoring Triggers:**\n",
    "- Retrain if PSI > 0.25 on Vdd or Frequency features\n",
    "- Alert if wafer yield < 85% (business threshold)\n",
    "- Backfill last 90 days if feature engineering bug found\n",
    "\n",
    "### Production Deployment\n",
    "\n",
    "**Typical Airflow Stack:**\n",
    "- **Scheduler**: 1-3 instances (HA setup with heartbeat)\n",
    "- **Web Server**: 2+ instances (behind load balancer)\n",
    "- **Workers**: 10-100 Celery workers (depending on task parallelism)\n",
    "- **Database**: PostgreSQL (metadata storage, not for large data!)\n",
    "- **Message Broker**: Redis or RabbitMQ (for Celery executor)\n",
    "- **Storage**: S3/GCS for logs, model artifacts, intermediate data\n",
    "\n",
    "**Cost Optimization:**\n",
    "- Use `KubernetesPodOperator` for auto-scaling training jobs\n",
    "- Set `pool` limits to prevent resource exhaustion\n",
    "- Clean up old DAG runs: `airflow dags delete --older-than 90`\n",
    "\n",
    "### Performance Benchmarks (Typical)\n",
    "\n",
    "- **DAG Parse Time**: < 5 seconds (for 30 tasks DAG)\n",
    "- **Scheduler Latency**: < 10 seconds (from schedule time to task start)\n",
    "- **Task Overhead**: ~1-2 seconds per task (setup + teardown)\n",
    "- **Max Throughput**: 10K+ tasks/hour (with KubernetesExecutor)\n",
    "\n",
    "### Next Steps\n",
    "- **Advanced**: Dynamic DAG generation, SubDAGs, TaskGroups\n",
    "- **Integration**: Airflow + MLflow + Feast (complete MLOps stack)\n",
    "- **Scaling**: Multi-cluster Airflow, cross-region DAGs\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: *Orchestration is infrastructure. Invest when pipelines become unmanageable manually!* üõ†Ô∏è"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
