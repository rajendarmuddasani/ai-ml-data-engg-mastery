{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fad1042",
   "metadata": {},
   "source": [
    "# 123: Model Monitoring & Drift Detection\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** drift types: Data drift, concept drift, prediction drift\n",
    "- **Detect** data drift using statistical tests (KS, PSI, Chi-square)\n",
    "- **Monitor** model performance degradation in production\n",
    "- **Implement** alerting systems for drift detection\n",
    "- **Apply** monitoring to post-silicon validation models\n",
    "- **Build** comprehensive monitoring dashboards\n",
    "\n",
    "## üìö What is Model Monitoring?\n",
    "\n",
    "**Model monitoring** is the continuous observation of ML models in production to detect:\n",
    "- **Performance degradation**: Accuracy drops from 92% to 78%\n",
    "- **Data drift**: Input distributions change (e.g., voltage range shifts)\n",
    "- **Concept drift**: Relationship between features and target changes\n",
    "- **Prediction drift**: Output distribution changes unexpectedly\n",
    "\n",
    "**Why Models Fail Silently:**\n",
    "- ‚úÖ **Data changes**: New device types, process node changes, equipment drift\n",
    "- ‚úÖ **Seasonal patterns**: Holiday effects, temperature variations\n",
    "- ‚úÖ **Adversarial shifts**: Gaming the system, evolving fraud tactics\n",
    "- ‚úÖ **Infrastructure issues**: Feature pipeline bugs, data quality problems\n",
    "\n",
    "**Without monitoring**: Model serves bad predictions for weeks/months before someone notices.\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Use Case 1: Yield Predictor Drift Detection**\n",
    "- **Scenario**: Yield prediction model trained on 1.2V ¬± 0.05V, but new lot uses 1.25V ¬± 0.03V\n",
    "- **Detection**: PSI on Vdd distribution = 0.28 (major drift, threshold 0.2)\n",
    "- **Impact**: Accuracy drops from 92% to 84% without retraining\n",
    "- **Alert**: \"‚ö†Ô∏è Vdd drift detected (PSI=0.28). Model retrain recommended.\"\n",
    "- **Value**: Detect drift within 24 hours instead of discovering after 2 weeks of bad predictions\n",
    "\n",
    "**Use Case 2: Test Time Optimizer Performance Monitoring**\n",
    "- **Scenario**: Model predicts which tests to skip (25% time reduction, <0.5% FNR)\n",
    "- **Monitoring**: Track false negative rate daily, alert if >0.5% for 3 consecutive days\n",
    "- **Drift**: New test program version changes test sequence ‚Üí concept drift\n",
    "- **Detection**: FNR spikes to 1.2% on day 1 of new program\n",
    "- **Response**: Instant rollback to previous model, retrain on new test program data\n",
    "- **Value**: Prevent $50K in escapes, maintain quality standards\n",
    "\n",
    "**Use Case 3: Wafer Map Anomaly Detector Monitoring**\n",
    "- **Scenario**: Spatial anomaly detection for equipment failures\n",
    "- **Metrics**: Anomaly detection rate, false positive rate, spatial pattern distribution\n",
    "- **Drift**: New lithography tool introduced ‚Üí different spatial signatures\n",
    "- **Detection**: Anomaly rate drops from 5% to 1% (model missing new patterns)\n",
    "- **Alert**: \"üö® Anomaly rate anomaly! Investigating equipment changes...\"\n",
    "- **Value**: Identify equipment issues 6 hours earlier, $1.5M avoidance\n",
    "\n",
    "**Use Case 4: Device Binning Classifier Monitoring**\n",
    "- **Scenario**: Multi-class binning (Premium/Standard/Economy at 60/30/10 split)\n",
    "- **Monitoring**: Track bin distribution daily, alert if shifts >5%\n",
    "- **Drift**: Premium bin % drops to 50% (from 60%)\n",
    "- **Root cause**: New process step affects performance parameters\n",
    "- **Response**: Investigate process change, retrain with new data, update bin thresholds\n",
    "- **Value**: Revenue optimization, prevent $800K in mis-binned devices\n",
    "\n",
    "## üîÑ Model Monitoring Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Production Model] --> B[Collect Predictions]\n",
    "    A --> C[Collect Input Features]\n",
    "    A --> D[Collect Ground Truth]\n",
    "    \n",
    "    B --> E[Prediction Drift Analysis]\n",
    "    C --> F[Data Drift Detection]\n",
    "    D --> G[Performance Monitoring]\n",
    "    \n",
    "    F --> H{Drift Detected?}\n",
    "    G --> I{Performance Drop?}\n",
    "    E --> J{Distribution Shift?}\n",
    "    \n",
    "    H -->|Yes| K[Alert Data Science Team]\n",
    "    I -->|Yes| K\n",
    "    J -->|Yes| K\n",
    "    \n",
    "    K --> L{Severity?}\n",
    "    L -->|Critical| M[Immediate Rollback]\n",
    "    L -->|High| N[Retrain Model]\n",
    "    L -->|Medium| O[Investigate Root Cause]\n",
    "    \n",
    "    M --> P[Deploy Previous Version]\n",
    "    N --> Q[Retrain with Recent Data]\n",
    "    O --> R[Monitor Closely]\n",
    "    \n",
    "    H -->|No| S[Continue Monitoring]\n",
    "    I -->|No| S\n",
    "    J -->|No| S\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style K fill:#ffe1e1\n",
    "    style M fill:#ff9999\n",
    "    style S fill:#e1ffe1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **121_MLOps_Fundamentals.ipynb** - MLOps lifecycle, deployment\n",
    "- **122_MLflow_Complete_Guide.ipynb** - Experiment tracking, model registry\n",
    "- **041_Model_Evaluation_Metrics.ipynb** - Accuracy, F1, AUC metrics\n",
    "\n",
    "**Next Steps:**\n",
    "- **124_Feature_Store_Implementation.ipynb** - Centralized feature management\n",
    "- **125_ML_Pipeline_Orchestration.ipynb** - Automated retraining pipelines\n",
    "- **131_Docker_Fundamentals.ipynb** - Containerized monitoring services\n",
    "\n",
    "---\n",
    "\n",
    "Let's master model monitoring and drift detection! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8991ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install monitoring libraries\n",
    "# !pip install evidently alibi-detect scipy scikit-learn pandas numpy matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Model monitoring libraries loaded\")\n",
    "print(\"Focus: Data drift, concept drift, model performance degradation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f5b48d",
   "metadata": {},
   "source": [
    "## 2. Data Drift Detection\n",
    "\n",
    "**Data drift** occurs when input feature distributions change over time.\n",
    "\n",
    "**Types:**\n",
    "- **Covariate shift**: P(X) changes, but P(Y|X) stays same\n",
    "- **Prior probability shift**: P(Y) changes\n",
    "- **Concept drift**: P(Y|X) changes (covered in Section 3)\n",
    "\n",
    "**Why it matters**: Model trained on old distribution performs poorly on new distribution.\n",
    "\n",
    "**Post-Silicon Example**: Vdd trained range 1.2V¬±0.05, production shifts to 1.25V¬±0.03 ‚Üí model unreliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f463ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reference (training) and current (production) data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Reference data (what model was trained on)\n",
    "n_ref = 5000\n",
    "reference_data = pd.DataFrame({\n",
    "    'Vdd_V': np.random.normal(1.2, 0.05, n_ref),\n",
    "    'Idd_mA': np.random.normal(50, 5, n_ref),\n",
    "    'freq_MHz': np.random.normal(1000, 50, n_ref),\n",
    "    'temp_C': np.random.normal(25, 5, n_ref)\n",
    "})\n",
    "\n",
    "# Current production data (simulating drift)\n",
    "n_curr = 1000\n",
    "\n",
    "# Scenario 1: NO DRIFT (distribution same as training)\n",
    "current_no_drift = pd.DataFrame({\n",
    "    'Vdd_V': np.random.normal(1.2, 0.05, n_curr),\n",
    "    'Idd_mA': np.random.normal(50, 5, n_curr),\n",
    "    'freq_MHz': np.random.normal(1000, 50, n_curr),\n",
    "    'temp_C': np.random.normal(25, 5, n_curr)\n",
    "})\n",
    "\n",
    "# Scenario 2: DRIFT (Vdd shifted, temp increased variance)\n",
    "current_with_drift = pd.DataFrame({\n",
    "    'Vdd_V': np.random.normal(1.25, 0.03, n_curr),  # Mean shifted!\n",
    "    'Idd_mA': np.random.normal(50, 5, n_curr),\n",
    "    'freq_MHz': np.random.normal(1000, 50, n_curr),\n",
    "    'temp_C': np.random.normal(27, 8, n_curr)  # Mean + variance changed!\n",
    "})\n",
    "\n",
    "print(\"Reference data (training):\")\n",
    "print(reference_data.describe())\n",
    "print(\"\\nCurrent data (NO drift):\")\n",
    "print(current_no_drift.describe())\n",
    "print(\"\\nCurrent data (WITH drift):\")\n",
    "print(current_with_drift.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb934c83",
   "metadata": {},
   "source": [
    "### A. Kolmogorov-Smirnov Test (Continuous Features)\n",
    "\n",
    "**KS Test** compares two distributions, tests if they come from same underlying distribution.\n",
    "\n",
    "**How it works:**\n",
    "- Compares empirical CDFs (cumulative distribution functions)\n",
    "- **Statistic**: Maximum distance between CDFs (0 to 1)\n",
    "- **P-value**: Probability distributions are same\n",
    "- **Decision**: If p < 0.05, reject null hypothesis ‚Üí **drift detected**\n",
    "\n",
    "**When to use**: Continuous numerical features (Vdd, Idd, freq, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5927cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolmogorov-Smirnov test for each feature\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def ks_drift_test(reference, current, feature_name, alpha=0.05):\n",
    "    \"\"\"Perform KS test for drift detection\"\"\"\n",
    "    statistic, pvalue = ks_2samp(reference, current)\n",
    "    \n",
    "    drift_detected = pvalue < alpha\n",
    "    \n",
    "    result = {\n",
    "        'feature': feature_name,\n",
    "        'ks_statistic': statistic,\n",
    "        'p_value': pvalue,\n",
    "        'drift_detected': drift_detected,\n",
    "        'severity': 'HIGH' if statistic > 0.2 else 'MEDIUM' if statistic > 0.1 else 'LOW'\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test on NO DRIFT scenario\n",
    "print(\"=== KS Test: NO DRIFT Scenario ===\")\n",
    "for col in reference_data.columns:\n",
    "    result = ks_drift_test(reference_data[col], current_no_drift[col], col)\n",
    "    status = \"üö® DRIFT\" if result['drift_detected'] else \"‚úÖ NO DRIFT\"\n",
    "    print(f\"{col:12s}: KS={result['ks_statistic']:.4f}, p={result['p_value']:.4f} ‚Üí {status}\")\n",
    "\n",
    "print(\"\\n=== KS Test: WITH DRIFT Scenario ===\")\n",
    "drift_results = []\n",
    "for col in reference_data.columns:\n",
    "    result = ks_drift_test(reference_data[col], current_with_drift[col], col)\n",
    "    drift_results.append(result)\n",
    "    status = \"üö® DRIFT\" if result['drift_detected'] else \"‚úÖ NO DRIFT\"\n",
    "    print(f\"{col:12s}: KS={result['ks_statistic']:.4f}, p={result['p_value']:.4f} ‚Üí {status} ({result['severity']})\")\n",
    "\n",
    "# Summary\n",
    "drifted_features = [r['feature'] for r in drift_results if r['drift_detected']]\n",
    "print(f\"\\nüìä Summary: {len(drifted_features)}/{len(drift_results)} features drifted\")\n",
    "if drifted_features:\n",
    "    print(f\"Drifted features: {', '.join(drifted_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dad528",
   "metadata": {},
   "source": [
    "### B. Population Stability Index (PSI)\n",
    "\n",
    "**PSI** measures distribution shift using binned percentages.\n",
    "\n",
    "**Formula:**\n",
    "$$PSI = \\sum_{i=1}^{n} (P_{current,i} - P_{reference,i}) \\times \\ln\\left(\\frac{P_{current,i}}{P_{reference,i}}\\right)$$\n",
    "\n",
    "**Interpretation:**\n",
    "- **PSI < 0.1**: No significant change ‚úÖ\n",
    "- **PSI 0.1-0.2**: Minor drift, monitor closely ‚ö†Ô∏è\n",
    "- **PSI > 0.2**: Major drift, retrain recommended üö®\n",
    "\n",
    "**Advantages**: Industry standard (banking, credit scoring), intuitive thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d920b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population Stability Index implementation\n",
    "def calculate_psi(reference, current, bins=10):\n",
    "    \"\"\"Calculate PSI for drift detection\"\"\"\n",
    "    # Create bins from reference data\n",
    "    _, bin_edges = np.histogram(reference, bins=bins)\n",
    "    \n",
    "    # Count samples in each bin\n",
    "    ref_counts, _ = np.histogram(reference, bins=bin_edges)\n",
    "    curr_counts, _ = np.histogram(current, bins=bin_edges)\n",
    "    \n",
    "    # Convert to percentages\n",
    "    ref_pct = ref_counts / len(reference)\n",
    "    curr_pct = curr_counts / len(current)\n",
    "    \n",
    "    # Avoid division by zero (add small epsilon)\n",
    "    ref_pct = np.where(ref_pct == 0, 0.0001, ref_pct)\n",
    "    curr_pct = np.where(curr_pct == 0, 0.0001, curr_pct)\n",
    "    \n",
    "    # Calculate PSI\n",
    "    psi = np.sum((curr_pct - ref_pct) * np.log(curr_pct / ref_pct))\n",
    "    \n",
    "    # Interpretation\n",
    "    if psi < 0.1:\n",
    "        status = \"‚úÖ NO CHANGE\"\n",
    "        severity = \"LOW\"\n",
    "    elif psi < 0.2:\n",
    "        status = \"‚ö†Ô∏è MINOR DRIFT\"\n",
    "        severity = \"MEDIUM\"\n",
    "    else:\n",
    "        status = \"üö® MAJOR DRIFT\"\n",
    "        severity = \"HIGH\"\n",
    "    \n",
    "    return {\n",
    "        'psi': psi,\n",
    "        'status': status,\n",
    "        'severity': severity,\n",
    "        'recommendation': 'Continue monitoring' if psi < 0.1 else 'Monitor closely' if psi < 0.2 else 'Retrain model'\n",
    "    }\n",
    "\n",
    "# Test PSI on NO DRIFT scenario\n",
    "print(\"=== PSI Test: NO DRIFT Scenario ===\")\n",
    "for col in reference_data.columns:\n",
    "    result = calculate_psi(reference_data[col], current_no_drift[col])\n",
    "    print(f\"{col:12s}: PSI={result['psi']:.4f} ‚Üí {result['status']} ({result['recommendation']})\")\n",
    "\n",
    "# Test PSI on WITH DRIFT scenario\n",
    "print(\"\\n=== PSI Test: WITH DRIFT Scenario ===\")\n",
    "psi_results = []\n",
    "for col in reference_data.columns:\n",
    "    result = calculate_psi(reference_data[col], current_with_drift[col])\n",
    "    result['feature'] = col\n",
    "    psi_results.append(result)\n",
    "    print(f\"{col:12s}: PSI={result['psi']:.4f} ‚Üí {result['status']} ({result['recommendation']})\")\n",
    "\n",
    "# Identify critical features\n",
    "critical_features = [r['feature'] for r in psi_results if r['psi'] > 0.2]\n",
    "if critical_features:\n",
    "    print(f\"\\nüö® ALERT: Critical drift detected in: {', '.join(critical_features)}\")\n",
    "    print(f\"Action required: Model retrain recommended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3717ebe8",
   "metadata": {},
   "source": [
    "### C. Visualizing Drift\n",
    "\n",
    "**Visual drift detection** helps stakeholders understand distribution changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e76a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize drift for Vdd and temp\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Vdd - NO DRIFT\n",
    "axes[0, 0].hist(reference_data['Vdd_V'], bins=30, alpha=0.5, label='Reference (Training)', color='blue', density=True)\n",
    "axes[0, 0].hist(current_no_drift['Vdd_V'], bins=30, alpha=0.5, label='Current (No Drift)', color='green', density=True)\n",
    "axes[0, 0].set_title('Vdd Distribution - NO DRIFT')\n",
    "axes[0, 0].set_xlabel('Vdd (V)')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].axvline(reference_data['Vdd_V'].mean(), color='blue', linestyle='--', label='Ref mean')\n",
    "axes[0, 0].axvline(current_no_drift['Vdd_V'].mean(), color='green', linestyle='--', label='Curr mean')\n",
    "\n",
    "# Vdd - WITH DRIFT\n",
    "axes[0, 1].hist(reference_data['Vdd_V'], bins=30, alpha=0.5, label='Reference (Training)', color='blue', density=True)\n",
    "axes[0, 1].hist(current_with_drift['Vdd_V'], bins=30, alpha=0.5, label='Current (Drifted)', color='red', density=True)\n",
    "axes[0, 1].set_title('Vdd Distribution - WITH DRIFT (Mean shifted)')\n",
    "axes[0, 1].set_xlabel('Vdd (V)')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].legend()\n",
    "vdd_psi = calculate_psi(reference_data['Vdd_V'], current_with_drift['Vdd_V'])\n",
    "axes[0, 1].text(0.05, 0.95, f\"PSI={vdd_psi['psi']:.3f}\\n{vdd_psi['status']}\", \n",
    "                transform=axes[0, 1].transAxes, fontsize=12, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "# Temp - NO DRIFT\n",
    "axes[1, 0].hist(reference_data['temp_C'], bins=30, alpha=0.5, label='Reference (Training)', color='blue', density=True)\n",
    "axes[1, 0].hist(current_no_drift['temp_C'], bins=30, alpha=0.5, label='Current (No Drift)', color='green', density=True)\n",
    "axes[1, 0].set_title('Temperature Distribution - NO DRIFT')\n",
    "axes[1, 0].set_xlabel('Temperature (¬∞C)')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Temp - WITH DRIFT\n",
    "axes[1, 1].hist(reference_data['temp_C'], bins=30, alpha=0.5, label='Reference (Training)', color='blue', density=True)\n",
    "axes[1, 1].hist(current_with_drift['temp_C'], bins=30, alpha=0.5, label='Current (Drifted)', color='red', density=True)\n",
    "axes[1, 1].set_title('Temperature Distribution - WITH DRIFT (Variance increased)')\n",
    "axes[1, 1].set_xlabel('Temperature (¬∞C)')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].legend()\n",
    "temp_psi = calculate_psi(reference_data['temp_C'], current_with_drift['temp_C'])\n",
    "axes[1, 1].text(0.05, 0.95, f\"PSI={temp_psi['psi']:.3f}\\n{temp_psi['status']}\", \n",
    "                transform=axes[1, 1].transAxes, fontsize=12, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visual inspection confirms drift detection:\")\n",
    "print(f\"- Vdd: Mean shift from {reference_data['Vdd_V'].mean():.3f}V to {current_with_drift['Vdd_V'].mean():.3f}V\")\n",
    "print(f\"- Temp: Std increased from {reference_data['temp_C'].std():.2f}¬∞C to {current_with_drift['temp_C'].std():.2f}¬∞C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b4aaed",
   "metadata": {},
   "source": [
    "## 3. Concept Drift Detection\n",
    "\n",
    "**Concept drift** occurs when the relationship between features (X) and target (Y) changes.\n",
    "\n",
    "**Example**: \n",
    "- Training: Vdd=1.2V + Idd<55mA ‚Üí 95% yield\n",
    "- Production: Vdd=1.2V + Idd<55mA ‚Üí 85% yield (process changed!)\n",
    "\n",
    "**Types:**\n",
    "- **Sudden drift**: Abrupt change (equipment replacement, new process step)\n",
    "- **Gradual drift**: Slow change over time (equipment degradation)\n",
    "- **Recurring drift**: Seasonal patterns (temperature effects)\n",
    "\n",
    "**Detection challenge**: Requires ground truth labels (actual outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate concept drift scenario\n",
    "np.random.seed(42)\n",
    "\n",
    "# Original relationship (training data)\n",
    "n_train = 5000\n",
    "X_train = pd.DataFrame({\n",
    "    'Vdd_V': np.random.normal(1.2, 0.05, n_train),\n",
    "    'Idd_mA': np.random.normal(50, 5, n_train),\n",
    "    'freq_MHz': np.random.normal(1000, 50, n_train),\n",
    "    'temp_C': np.random.normal(25, 5, n_train)\n",
    "})\n",
    "\n",
    "# Original yield relationship\n",
    "y_train = (\n",
    "    (X_train['Vdd_V'] >= 1.15) & (X_train['Vdd_V'] <= 1.25) &\n",
    "    (X_train['Idd_mA'] <= 55) &\n",
    "    (X_train['freq_MHz'] >= 950) &\n",
    "    (X_train['temp_C'] <= 30)\n",
    ").astype(int)\n",
    "\n",
    "# Train model on original relationship\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Production data - simulate concept drift\n",
    "# Features same distribution, but relationship changed!\n",
    "n_prod = 2000\n",
    "X_production = pd.DataFrame({\n",
    "    'Vdd_V': np.random.normal(1.2, 0.05, n_prod),  # Same distribution\n",
    "    'Idd_mA': np.random.normal(50, 5, n_prod),\n",
    "    'freq_MHz': np.random.normal(1000, 50, n_prod),\n",
    "    'temp_C': np.random.normal(25, 5, n_prod)\n",
    "})\n",
    "\n",
    "# NEW yield relationship (process changed, tighter specs!)\n",
    "y_production_actual = (\n",
    "    (X_production['Vdd_V'] >= 1.18) & (X_production['Vdd_V'] <= 1.22) &  # Tighter!\n",
    "    (X_production['Idd_mA'] <= 52) &  # Lower threshold!\n",
    "    (X_production['freq_MHz'] >= 980) &  # Higher threshold!\n",
    "    (X_production['temp_C'] <= 28)  # Tighter!\n",
    ").astype(int)\n",
    "\n",
    "# Model predictions (using OLD relationship)\n",
    "y_production_pred = model.predict(X_production)\n",
    "\n",
    "# Compare performance\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "production_accuracy = accuracy_score(y_production_actual, y_production_pred)\n",
    "production_f1 = f1_score(y_production_actual, y_production_pred)\n",
    "\n",
    "print(\"=== Concept Drift Impact ===\")\n",
    "print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Production accuracy: {production_accuracy:.4f} ‚ö†Ô∏è (dropped {(train_accuracy - production_accuracy)*100:.1f}%)\")\n",
    "print(f\"Production F1: {production_f1:.4f}\")\n",
    "print(f\"\\nProblem: Feature distributions unchanged, but X‚ÜíY relationship changed\")\n",
    "print(f\"Root cause: Process specifications tightened\")\n",
    "print(f\"Solution: Retrain model with recent production data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0a5ba",
   "metadata": {},
   "source": [
    "## 4. Performance Monitoring Over Time\n",
    "\n",
    "**Track model metrics continuously** to detect gradual degradation.\n",
    "\n",
    "**What to monitor:**\n",
    "- Accuracy, F1, Precision, Recall (requires ground truth)\n",
    "- Prediction confidence distribution\n",
    "- Prediction rate (predictions/day)\n",
    "- Latency (p50, p95, p99)\n",
    "\n",
    "**Post-Silicon example**: Track yield predictor F1 score daily, alert if drops below 0.88 for 3 consecutive days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1005ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate 30 days of production monitoring\n",
    "np.random.seed(42)\n",
    "\n",
    "monitoring_log = []\n",
    "for day in range(1, 31):\n",
    "    # Simulate gradual concept drift (accuracy degrades over time)\n",
    "    drift_factor = max(0, 1 - (day / 50))  # Gradual degradation\n",
    "    \n",
    "    # Generate daily production data\n",
    "    n_daily = 500\n",
    "    X_daily = pd.DataFrame({\n",
    "        'Vdd_V': np.random.normal(1.2, 0.05, n_daily),\n",
    "        'Idd_mA': np.random.normal(50, 5, n_daily),\n",
    "        'freq_MHz': np.random.normal(1000, 50, n_daily),\n",
    "        'temp_C': np.random.normal(25, 5, n_daily)\n",
    "    })\n",
    "    \n",
    "    # Ground truth (with concept drift)\n",
    "    y_true = (\n",
    "        (X_daily['Vdd_V'] >= 1.15 + (1-drift_factor)*0.03) & \n",
    "        (X_daily['Vdd_V'] <= 1.25 - (1-drift_factor)*0.03) &\n",
    "        (X_daily['Idd_mA'] <= 55 - (1-drift_factor)*3) &\n",
    "        (X_daily['freq_MHz'] >= 950 + (1-drift_factor)*30) &\n",
    "        (X_daily['temp_C'] <= 30 - (1-drift_factor)*2)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Model predictions\n",
    "    y_pred = model.predict(X_daily)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Add noise to make it realistic\n",
    "    accuracy += np.random.normal(0, 0.01)\n",
    "    f1 += np.random.normal(0, 0.01)\n",
    "    \n",
    "    monitoring_log.append({\n",
    "        'day': day,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'predictions_count': n_daily\n",
    "    })\n",
    "\n",
    "monitoring_df = pd.DataFrame(monitoring_log)\n",
    "\n",
    "# Plot performance over time\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Accuracy trend\n",
    "axes[0].plot(monitoring_df['day'], monitoring_df['accuracy'], marker='o', label='Daily Accuracy')\n",
    "axes[0].axhline(y=0.88, color='r', linestyle='--', label='Alert Threshold (0.88)')\n",
    "axes[0].axhline(y=0.90, color='orange', linestyle='--', label='Warning Threshold (0.90)')\n",
    "axes[0].fill_between(monitoring_df['day'], 0.88, 1.0, alpha=0.1, color='green', label='Safe Zone')\n",
    "axes[0].fill_between(monitoring_df['day'], 0.0, 0.88, alpha=0.1, color='red', label='Alert Zone')\n",
    "axes[0].set_xlabel('Day')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Model Accuracy Over Time - Gradual Degradation')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 score trend\n",
    "axes[1].plot(monitoring_df['day'], monitoring_df['f1_score'], marker='s', color='purple', label='Daily F1 Score')\n",
    "axes[1].axhline(y=0.85, color='r', linestyle='--', label='Alert Threshold (0.85)')\n",
    "axes[1].fill_between(monitoring_df['day'], 0.85, 1.0, alpha=0.1, color='green')\n",
    "axes[1].fill_between(monitoring_df['day'], 0.0, 0.85, alpha=0.1, color='red')\n",
    "axes[1].set_xlabel('Day')\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[1].set_title('Model F1 Score Over Time')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Alert logic\n",
    "print(\"=== Performance Monitoring Alerts ===\")\n",
    "for i, row in monitoring_df.iterrows():\n",
    "    if row['accuracy'] < 0.88:\n",
    "        print(f\"Day {row['day']}: üö® ALERT - Accuracy {row['accuracy']:.4f} < 0.88 threshold\")\n",
    "    elif row['accuracy'] < 0.90:\n",
    "        print(f\"Day {row['day']}: ‚ö†Ô∏è  WARNING - Accuracy {row['accuracy']:.4f} < 0.90 threshold\")\n",
    "\n",
    "# Check for sustained degradation\n",
    "consecutive_low = 0\n",
    "for acc in monitoring_df['accuracy']:\n",
    "    if acc < 0.88:\n",
    "        consecutive_low += 1\n",
    "        if consecutive_low >= 3:\n",
    "            print(f\"\\nüö® CRITICAL: Accuracy below threshold for {consecutive_low} consecutive days\")\n",
    "            print(\"Action: Trigger automatic model retrain\")\n",
    "            break\n",
    "    else:\n",
    "        consecutive_low = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ce3b0",
   "metadata": {},
   "source": [
    "## 5. Complete Monitoring System\n",
    "\n",
    "**Production-ready monitoring** combines drift detection + performance tracking + automated alerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete monitoring class\n",
    "class ModelMonitor:\n",
    "    \"\"\"Production model monitoring system\"\"\"\n",
    "    \n",
    "    def __init__(self, reference_data, model, alert_thresholds=None):\n",
    "        self.reference_data = reference_data\n",
    "        self.model = model\n",
    "        self.thresholds = alert_thresholds or {\n",
    "            'psi': 0.2,\n",
    "            'ks_pvalue': 0.05,\n",
    "            'accuracy': 0.88,\n",
    "            'f1_score': 0.85\n",
    "        }\n",
    "        self.monitoring_history = []\n",
    "    \n",
    "    def check_data_drift(self, current_data):\n",
    "        \"\"\"Check for data drift using PSI and KS tests\"\"\"\n",
    "        drift_report = {'features': {}, 'overall_status': 'OK'}\n",
    "        \n",
    "        for col in self.reference_data.columns:\n",
    "            # PSI test\n",
    "            psi_result = calculate_psi(self.reference_data[col], current_data[col])\n",
    "            \n",
    "            # KS test\n",
    "            ks_stat, ks_pval = ks_2samp(self.reference_data[col], current_data[col])\n",
    "            \n",
    "            drift_detected = (psi_result['psi'] > self.thresholds['psi'] or \n",
    "                            ks_pval < self.thresholds['ks_pvalue'])\n",
    "            \n",
    "            drift_report['features'][col] = {\n",
    "                'psi': psi_result['psi'],\n",
    "                'ks_statistic': ks_stat,\n",
    "                'ks_pvalue': ks_pval,\n",
    "                'drift_detected': drift_detected,\n",
    "                'severity': psi_result['severity']\n",
    "            }\n",
    "            \n",
    "            if drift_detected:\n",
    "                drift_report['overall_status'] = 'DRIFT_DETECTED'\n",
    "        \n",
    "        return drift_report\n",
    "    \n",
    "    def check_performance(self, X, y_true):\n",
    "        \"\"\"Check model performance\"\"\"\n",
    "        y_pred = self.model.predict(X)\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        \n",
    "        performance_alert = (accuracy < self.thresholds['accuracy'] or \n",
    "                           f1 < self.thresholds['f1_score'])\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1,\n",
    "            'alert': performance_alert,\n",
    "            'status': 'DEGRADED' if performance_alert else 'OK'\n",
    "        }\n",
    "    \n",
    "    def monitor(self, current_data, y_true):\n",
    "        \"\"\"Run complete monitoring check\"\"\"\n",
    "        # Check drift\n",
    "        drift_report = self.check_data_drift(current_data)\n",
    "        \n",
    "        # Check performance\n",
    "        performance = self.check_performance(current_data, y_true)\n",
    "        \n",
    "        # Combined report\n",
    "        report = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'data_drift': drift_report,\n",
    "            'performance': performance,\n",
    "            'overall_health': 'CRITICAL' if (drift_report['overall_status'] == 'DRIFT_DETECTED' and \n",
    "                                             performance['status'] == 'DEGRADED') else\n",
    "                             'WARNING' if (drift_report['overall_status'] == 'DRIFT_DETECTED' or \n",
    "                                          performance['status'] == 'DEGRADED') else 'HEALTHY'\n",
    "        }\n",
    "        \n",
    "        self.monitoring_history.append(report)\n",
    "        return report\n",
    "    \n",
    "    def generate_alert(self, report):\n",
    "        \"\"\"Generate human-readable alert\"\"\"\n",
    "        if report['overall_health'] == 'HEALTHY':\n",
    "            return \"‚úÖ Model healthy. No action required.\"\n",
    "        \n",
    "        alert_msg = []\n",
    "        \n",
    "        # Data drift alerts\n",
    "        if report['data_drift']['overall_status'] == 'DRIFT_DETECTED':\n",
    "            drifted = [f for f, r in report['data_drift']['features'].items() \n",
    "                      if r['drift_detected']]\n",
    "            alert_msg.append(f\"‚ö†Ô∏è DATA DRIFT: {', '.join(drifted)}\")\n",
    "            for feat in drifted:\n",
    "                psi = report['data_drift']['features'][feat]['psi']\n",
    "                alert_msg.append(f\"  - {feat}: PSI={psi:.3f}\")\n",
    "        \n",
    "        # Performance alerts\n",
    "        if report['performance']['status'] == 'DEGRADED':\n",
    "            acc = report['performance']['accuracy']\n",
    "            f1 = report['performance']['f1_score']\n",
    "            alert_msg.append(f\"üö® PERFORMANCE DEGRADATION:\")\n",
    "            alert_msg.append(f\"  - Accuracy: {acc:.4f} (threshold: {self.thresholds['accuracy']})\")\n",
    "            alert_msg.append(f\"  - F1 Score: {f1:.4f} (threshold: {self.thresholds['f1_score']})\")\n",
    "        \n",
    "        # Recommendation\n",
    "        if report['overall_health'] == 'CRITICAL':\n",
    "            alert_msg.append(\"\\nüîß RECOMMENDED ACTION: Immediate model retrain\")\n",
    "        elif report['overall_health'] == 'WARNING':\n",
    "            alert_msg.append(\"\\nüîç RECOMMENDED ACTION: Investigate root cause, plan retrain\")\n",
    "        \n",
    "        return \"\\n\".join(alert_msg)\n",
    "\n",
    "# Test monitoring system\n",
    "monitor = ModelMonitor(reference_data, model)\n",
    "\n",
    "# Day 1: No drift\n",
    "report_day1 = monitor.monitor(current_no_drift, \n",
    "                               (current_no_drift['Vdd_V'] <= 1.25).astype(int))\n",
    "print(\"=== Day 1 Monitoring Report ===\")\n",
    "print(monitor.generate_alert(report_day1))\n",
    "\n",
    "# Day 15: With drift\n",
    "print(\"\\n=== Day 15 Monitoring Report ===\")\n",
    "report_day15 = monitor.monitor(current_with_drift,\n",
    "                                y_production_actual[:1000])\n",
    "print(monitor.generate_alert(report_day15))\n",
    "\n",
    "print(f\"\\nOverall health: {report_day15['overall_health']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58df0cb",
   "metadata": {},
   "source": [
    "## üéØ Real-World Monitoring Projects\n",
    "\n",
    "### **Post-Silicon Validation Projects**\n",
    "\n",
    "#### **Project 1: Yield Predictor Continuous Monitoring**\n",
    "**Objective**: Deploy comprehensive monitoring for production yield prediction model\n",
    "- **Metrics**: Accuracy, F1, AUC (daily, with 24-hour ground truth delay)\n",
    "- **Data drift**: PSI on Vdd, Idd, freq, temp (hourly checks)\n",
    "- **Alerts**: Slack notification if PSI > 0.2 or accuracy < 0.90 for 2 consecutive days\n",
    "- **Dashboard**: Grafana dashboard showing 30-day trend, drift heatmap, alert history\n",
    "- **Auto-retrain**: Trigger retrain job if critical drift detected for 3 days\n",
    "- **Success**: Detect process changes within 6 hours vs 2 weeks manual detection\n",
    "\n",
    "#### **Project 2: Test Time Optimizer Monitoring with FNR Tracking**\n",
    "**Objective**: Monitor test time reduction model, ensure false negative rate < 0.5%\n",
    "- **Primary metric**: False negative rate (escapes to customer)\n",
    "- **Secondary metrics**: Test time savings %, throughput (devices/hour)\n",
    "- **Drift detection**: Chi-square test on test_sequence distribution (categorical drift)\n",
    "- **Alert thresholds**: FNR > 0.5% ‚Üí instant rollback, FNR 0.4-0.5% ‚Üí warning\n",
    "- **Root cause analysis**: Log test program version changes, equipment IDs\n",
    "- **Business value**: Prevent $100K in customer escapes, maintain quality standards\n",
    "\n",
    "#### **Project 3: Wafer Map Anomaly Detector Monitoring**\n",
    "**Objective**: Monitor spatial anomaly detection for equipment failures\n",
    "- **Metrics**: Anomaly detection rate, false positive rate, spatial coverage\n",
    "- **Data drift**: Track wafer coordinate distributions, spatial autocorrelation changes\n",
    "- **Concept drift**: Monitor equipment IDs, lithography tool changes\n",
    "- **Visualization**: Daily wafer map gallery (detected anomalies highlighted)\n",
    "- **Alerts**: Anomaly rate drops >50% (model missing new patterns) OR spikes >200% (false positives)\n",
    "- **Integration**: Trigger equipment maintenance alerts based on spatial patterns\n",
    "- **Value**: Identify equipment issues 4 hours earlier, $2M+ avoidance/year\n",
    "\n",
    "#### **Project 4: Device Binning Drift Dashboard**\n",
    "**Objective**: Real-time monitoring of binning distribution for revenue optimization\n",
    "- **Target distribution**: Premium(60%) / Standard(30%) / Economy(10%)\n",
    "- **Drift detection**: Chi-square test on bin distribution (daily)\n",
    "- **Performance**: Track binning accuracy, revenue per wafer\n",
    "- **Alerts**: Premium bin % drops below 55% or above 65% (investigate process changes)\n",
    "- **Root cause**: Correlate bin shifts with fab events (process changes, equipment)\n",
    "- **Dashboard**: Streamlit app showing bin trend, revenue impact, alert log\n",
    "- **Value**: $800K revenue optimization, proactive process optimization\n",
    "\n",
    "---\n",
    "\n",
    "### **General AI/ML Projects**\n",
    "\n",
    "#### **Project 5: Customer Churn Predictor Monitoring**\n",
    "**Objective**: Monitor churn prediction model for concept drift (customer behavior changes)\n",
    "- **Metrics**: Precision, recall, F1 (monthly with subscription renewal data)\n",
    "- **Data drift**: PSI on usage_minutes, support_tickets, payment_history\n",
    "- **Concept drift**: Monitor churn rate over time (sudden spikes indicate drift)\n",
    "- **Seasonal patterns**: Track holiday effects, quarterly business cycles\n",
    "- **Alerts**: Precision < 0.75 (too many false positives, wasted retention offers)\n",
    "- **Auto-retrain**: Quarterly retrain with last 12 months of data\n",
    "- **Business value**: Reduce churn by 18%, optimize retention campaign spend\n",
    "\n",
    "#### **Project 6: Fraud Detection Real-Time Monitoring**\n",
    "**Objective**: Monitor fraud detection model with sub-minute alerting\n",
    "- **Metrics**: Precision (false positive rate), recall (fraud catch rate)\n",
    "- **Data drift**: Track transaction_amount, merchant_category, location distributions\n",
    "- **Concept drift**: Fraudsters adapt tactics ‚Üí relationship between features and fraud changes\n",
    "- **Real-time**: Stream monitoring with 1-minute aggregation windows\n",
    "- **Alerts**: FPR > 2% (customer friction) ‚Üí instant alert, recall < 85% (missing fraud) ‚Üí critical\n",
    "- **A/B testing**: Shadow mode for new models (compare with production)\n",
    "- **Value**: Block $4M fraud annually, maintain <1% false positive rate\n",
    "\n",
    "#### **Project 7: Recommendation System Performance Tracking**\n",
    "**Objective**: Monitor recommendation CTR and engagement metrics\n",
    "- **Online metrics**: Click-through rate, conversion rate, time-on-site\n",
    "- **Offline metrics**: Coverage, diversity, novelty (prevent filter bubble)\n",
    "- **Data drift**: User preference shifts (genre popularity, seasonal trends)\n",
    "- **Concept drift**: New content categories, changing user behavior\n",
    "- **Prediction drift**: Track recommendation distribution (diversity vs popularity)\n",
    "- **Alerts**: CTR drops >10% from baseline, diversity score < 0.3\n",
    "- **Success**: 22% CTR improvement, balanced diversity/relevance\n",
    "\n",
    "#### **Project 8: Demand Forecasting Monitoring Dashboard**\n",
    "**Objective**: Track forecast accuracy over time with seasonal adjustments\n",
    "- **Metrics**: MAPE, MAE, RMSE (daily evaluation with next-day actuals)\n",
    "- **Data drift**: Sales volume distribution, promotion frequency, market trends\n",
    "- **Concept drift**: COVID effects, supply chain disruptions, competitor actions\n",
    "- **Seasonal patterns**: Weekly, monthly, quarterly cycles\n",
    "- **Visualization**: Forecast vs actual plots, error distribution, drift heatmap\n",
    "- **Alerts**: MAPE > 15% for 7 consecutive days ‚Üí retrain trigger\n",
    "- **Auto-retrain**: Weekly retrain with expanding window (last 365 days)\n",
    "- **Business value**: Reduce inventory costs by 30%, improve forecast to MAPE < 9%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce57a98",
   "metadata": {},
   "source": [
    "## üìö Comprehensive Takeaways\n",
    "\n",
    "### **üéØ Why Model Monitoring Matters**\n",
    "\n",
    "**Silent failures**: Production models degrade without anyone noticing until business impact severe.\n",
    "\n",
    "**Real-world example (post-silicon):**\n",
    "- Yield predictor trained on 1.2V¬±0.05V process\n",
    "- New lot uses 1.25V¬±0.03V (process change)\n",
    "- Model accuracy drops from 92% to 78%\n",
    "- **Without monitoring**: Discovered after 2 weeks, 5000 mis-predicted devices\n",
    "- **With monitoring**: Detected in 4 hours, automated retrain triggered, $200K saved\n",
    "\n",
    "**Types of degradation:**\n",
    "1. **Data drift**: Input distributions change\n",
    "2. **Concept drift**: X‚ÜíY relationship changes  \n",
    "3. **Prediction drift**: Output distribution shifts\n",
    "4. **Performance degradation**: Metrics decline over time\n",
    "\n",
    "---\n",
    "\n",
    "### **üîß Drift Detection Methods**\n",
    "\n",
    "#### **1. Kolmogorov-Smirnov (KS) Test**\n",
    "\n",
    "**For**: Continuous numerical features\n",
    "\n",
    "**How it works:**\n",
    "- Compares empirical CDFs of reference vs current data\n",
    "- **Statistic**: Max distance between CDFs (0 to 1)\n",
    "- **P-value**: Probability both distributions are same\n",
    "- **Decision**: p < 0.05 ‚Üí reject null ‚Üí **drift detected**\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "from scipy.stats import ks_2samp\n",
    "statistic, pvalue = ks_2samp(reference_data, current_data)\n",
    "drift = pvalue < 0.05\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Non-parametric (no distribution assumptions)\n",
    "- ‚úÖ Sensitive to both location and shape changes\n",
    "- ‚úÖ Statistical rigor (p-value)\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Requires sufficient sample size (>100 recommended)\n",
    "- ‚ùå May be overly sensitive with large samples\n",
    "\n",
    "**When to use**: Continuous features like Vdd, temperature, voltage, current\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Population Stability Index (PSI)**\n",
    "\n",
    "**For**: Any numerical feature (bins continuous into categories)\n",
    "\n",
    "**Formula:**\n",
    "$$PSI = \\sum_{i=1}^{n} (P_{current,i} - P_{reference,i}) \\times \\ln\\left(\\frac{P_{current,i}}{P_{reference,i}}\\right)$$\n",
    "\n",
    "**Interpretation thresholds:**\n",
    "- **PSI < 0.1**: No change ‚úÖ\n",
    "- **PSI 0.1-0.2**: Minor drift, monitor ‚ö†Ô∏è\n",
    "- **PSI > 0.2**: Major drift, retrain üö®\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "def calculate_psi(reference, current, bins=10):\n",
    "    ref_hist, bin_edges = np.histogram(reference, bins=bins)\n",
    "    curr_hist, _ = np.histogram(current, bins=bin_edges)\n",
    "    \n",
    "    ref_pct = ref_hist / len(reference)\n",
    "    curr_pct = curr_hist / len(current)\n",
    "    \n",
    "    # Avoid log(0)\n",
    "    ref_pct = np.where(ref_pct == 0, 0.0001, ref_pct)\n",
    "    curr_pct = np.where(curr_pct == 0, 0.0001, curr_pct)\n",
    "    \n",
    "    psi = np.sum((curr_pct - ref_pct) * np.log(curr_pct / ref_pct))\n",
    "    return psi\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Industry standard (banking, credit scoring, fintech)\n",
    "- ‚úÖ Intuitive thresholds (0.1, 0.2)\n",
    "- ‚úÖ Works for any distribution\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Sensitive to number of bins (default 10)\n",
    "- ‚ùå Asymmetric (PSI(A,B) ‚â† PSI(B,A))\n",
    "\n",
    "**When to use**: Credit scores, risk models, any feature with established PSI thresholds\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Chi-Square Test**\n",
    "\n",
    "**For**: Categorical features (device_type, test_program, bin_category)\n",
    "\n",
    "**How it works:**\n",
    "- Compares observed vs expected frequencies in categories\n",
    "- **Statistic**: Measures deviation from expected\n",
    "- **P-value**: Probability distributions are same\n",
    "- **Decision**: p < 0.05 ‚Üí **drift detected**\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create contingency table\n",
    "ref_counts = reference_categorical.value_counts()\n",
    "curr_counts = current_categorical.value_counts()\n",
    "\n",
    "# Chi-square test\n",
    "chi2, pvalue, dof, expected = chi2_contingency([ref_counts, curr_counts])\n",
    "drift = pvalue < 0.05\n",
    "```\n",
    "\n",
    "**When to use**: Device binning distribution, test program mix, categorical features\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Wasserstein Distance (Earth Mover's Distance)**\n",
    "\n",
    "**For**: Measuring \"effort\" to transform one distribution into another\n",
    "\n",
    "**Advantages**: More interpretable than KS for practitioners (units same as data)\n",
    "\n",
    "**Code:**\n",
    "```python\n",
    "from scipy.stats import wasserstein_distance\n",
    "distance = wasserstein_distance(reference_data, current_data)\n",
    "```\n",
    "\n",
    "**When to use**: When you want drift magnitude in original units (e.g., \"Vdd shifted by 0.05V\")\n",
    "\n",
    "---\n",
    "\n",
    "### **üìä Concept Drift Detection**\n",
    "\n",
    "**Challenge**: Requires ground truth labels (delayed in production)\n",
    "\n",
    "**Strategies:**\n",
    "\n",
    "#### **1. Performance Monitoring (Gold Standard)**\n",
    "```python\n",
    "# Daily batch evaluation\n",
    "y_pred = model.predict(X_production)\n",
    "# Wait for ground truth (24 hours for semiconductor test)\n",
    "accuracy = accuracy_score(y_true_delayed, y_pred)\n",
    "\n",
    "if accuracy < threshold:\n",
    "    trigger_retrain()\n",
    "```\n",
    "\n",
    "**Pros**: Direct measurement of model effectiveness  \n",
    "**Cons**: Requires ground truth (may be delayed days/weeks)\n",
    "\n",
    "#### **2. Prediction Drift**\n",
    "```python\n",
    "# Monitor prediction distribution\n",
    "ref_pred_mean = reference_predictions.mean()\n",
    "curr_pred_mean = current_predictions.mean()\n",
    "\n",
    "# If prediction distribution shifts significantly ‚Üí investigate\n",
    "if abs(curr_pred_mean - ref_pred_mean) > 0.1:\n",
    "    alert(\"Prediction drift detected\")\n",
    "```\n",
    "\n",
    "**Use case**: Early warning before ground truth available\n",
    "\n",
    "#### **3. Error Distribution Monitoring**\n",
    "```python\n",
    "# Track error patterns\n",
    "ref_errors = y_true_ref - y_pred_ref\n",
    "curr_errors = y_true_curr - y_pred_curr\n",
    "\n",
    "# If error distribution changes ‚Üí concept drift\n",
    "ks_stat, pval = ks_2samp(ref_errors, curr_errors)\n",
    "```\n",
    "\n",
    "**Advantage**: Detects subtle concept drift\n",
    "\n",
    "---\n",
    "\n",
    "### **‚öôÔ∏è Production Monitoring Architecture**\n",
    "\n",
    "#### **Components:**\n",
    "\n",
    "**1. Data Collection:**\n",
    "```python\n",
    "# Log every prediction\n",
    "prediction_log = {\n",
    "    'timestamp': datetime.now(),\n",
    "    'input_features': X.to_dict(),\n",
    "    'prediction': y_pred,\n",
    "    'prediction_confidence': model.predict_proba(X)[0][1],\n",
    "    'model_version': '2.1',\n",
    "    'latency_ms': 45\n",
    "}\n",
    "# Store in database (Postgres, MongoDB, S3)\n",
    "```\n",
    "\n",
    "**2. Drift Detection Pipeline:**\n",
    "```python\n",
    "# Scheduled job (hourly/daily)\n",
    "def drift_detection_job():\n",
    "    # Fetch last 24 hours of data\n",
    "    current_data = fetch_recent_predictions()\n",
    "    \n",
    "    # Load reference data\n",
    "    reference_data = load_training_data()\n",
    "    \n",
    "    # Run drift tests\n",
    "    drift_report = {}\n",
    "    for feature in features:\n",
    "        psi = calculate_psi(reference_data[feature], current_data[feature])\n",
    "        drift_report[feature] = {'psi': psi, 'drift': psi > 0.2}\n",
    "    \n",
    "    # If drift detected ‚Üí alert\n",
    "    if any(r['drift'] for r in drift_report.values()):\n",
    "        send_alert(drift_report)\n",
    "    \n",
    "    # Log to monitoring dashboard\n",
    "    log_to_grafana(drift_report)\n",
    "```\n",
    "\n",
    "**3. Alerting System:**\n",
    "```python\n",
    "# Multi-channel alerts\n",
    "def send_alert(report):\n",
    "    if report['severity'] == 'CRITICAL':\n",
    "        # Immediate alerts\n",
    "        send_pagerduty(report)  # Wake up on-call engineer\n",
    "        send_slack('#ml-alerts', report)\n",
    "        send_email(ml_team, report)\n",
    "    elif report['severity'] == 'WARNING':\n",
    "        send_slack('#ml-monitoring', report)\n",
    "    else:\n",
    "        log_to_dashboard(report)\n",
    "```\n",
    "\n",
    "**4. Auto-Retrain Trigger:**\n",
    "```python\n",
    "# Automated response\n",
    "def evaluate_retrain_need(drift_report, performance):\n",
    "    score = 0\n",
    "    \n",
    "    # Data drift scoring\n",
    "    high_psi_features = sum(1 for f in drift_report if f['psi'] > 0.2)\n",
    "    score += high_psi_features * 10\n",
    "    \n",
    "    # Performance scoring\n",
    "    if performance['accuracy'] < 0.88:\n",
    "        score += 50\n",
    "    \n",
    "    # Decision\n",
    "    if score > 60:\n",
    "        trigger_retrain_pipeline()\n",
    "        return \"RETRAIN_TRIGGERED\"\n",
    "    elif score > 30:\n",
    "        return \"MONITOR_CLOSELY\"\n",
    "    else:\n",
    "        return \"OK\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üìà Monitoring Dashboards**\n",
    "\n",
    "#### **Essential Visualizations:**\n",
    "\n",
    "**1. Drift Heatmap:**\n",
    "```\n",
    "            Day 1   Day 2   Day 3   ...\n",
    "Vdd_V       0.02    0.15    0.28    <- PSI values\n",
    "Idd_mA      0.05    0.06    0.04\n",
    "freq_MHz    0.10    0.12    0.25\n",
    "temp_C      0.03    0.18    0.22\n",
    "\n",
    "Color coding: Green (<0.1), Yellow (0.1-0.2), Red (>0.2)\n",
    "```\n",
    "\n",
    "**2. Performance Trend:**\n",
    "```\n",
    "Accuracy over time (30 days)\n",
    "- Line plot with threshold lines\n",
    "- Shaded regions (safe/warning/critical)\n",
    "- Annotations for events (model updates, data changes)\n",
    "```\n",
    "\n",
    "**3. Prediction Distribution:**\n",
    "```\n",
    "Histogram comparison:\n",
    "- Blue: Reference predictions (training)\n",
    "- Green: Current predictions (no drift)\n",
    "- Red: Current predictions (with drift)\n",
    "```\n",
    "\n",
    "**4. Feature Drift Radar Chart:**\n",
    "```\n",
    "Spider plot showing PSI for all features\n",
    "- Each axis = one feature\n",
    "- Reference line at PSI=0.1, 0.2\n",
    "- Easy to spot which features drifted\n",
    "```\n",
    "\n",
    "#### **Dashboard Tools:**\n",
    "\n",
    "**Open-source:**\n",
    "- **Evidently**: Pre-built drift dashboards, interactive reports\n",
    "- **Grafana**: Time-series metrics, alerts, custom dashboards\n",
    "- **Streamlit**: Custom Python dashboards, rapid prototyping\n",
    "\n",
    "**Commercial:**\n",
    "- **Arize AI**: ML observability platform, automatic drift detection\n",
    "- **Fiddler AI**: Model monitoring + explainability\n",
    "- **WhyLabs**: Data/model quality monitoring\n",
    "\n",
    "---\n",
    "\n",
    "### **üéì Best Practices**\n",
    "\n",
    "#### **1. Choose Right Drift Test for Feature Type**\n",
    "\n",
    "| Feature Type | Recommended Test | Why |\n",
    "|--------------|------------------|-----|\n",
    "| Continuous numerical | KS Test or PSI | Sensitive to distribution changes |\n",
    "| Categorical | Chi-Square | Tests category frequencies |\n",
    "| Ordinal | PSI or Wasserstein | Preserves order information |\n",
    "| High-dimensional | PCA + KS | Reduce dimensions first |\n",
    "\n",
    "#### **2. Set Appropriate Thresholds**\n",
    "\n",
    "**Don't use defaults blindly:**\n",
    "- PSI thresholds (0.1, 0.2) are industry standard but may need tuning\n",
    "- For high-frequency monitoring (hourly), use higher thresholds (reduce false alarms)\n",
    "- For critical models (fraud, medical), use lower thresholds (catch drift early)\n",
    "\n",
    "**Calibrate thresholds:**\n",
    "```python\n",
    "# Use validation data to find optimal threshold\n",
    "val_psi_values = [calculate_psi(train, val_fold) for val_fold in val_folds]\n",
    "threshold = np.percentile(val_psi_values, 95)  # 95th percentile\n",
    "```\n",
    "\n",
    "#### **3. Monitor at Multiple Time Scales**\n",
    "\n",
    "- **Hourly**: Detect sudden changes (equipment failure, data pipeline bug)\n",
    "- **Daily**: Standard monitoring cadence\n",
    "- **Weekly**: Trend analysis, seasonal patterns\n",
    "- **Monthly**: Long-term drift, model retraining schedule\n",
    "\n",
    "#### **4. Combine Multiple Signals**\n",
    "\n",
    "**Don't rely on single metric:**\n",
    "```python\n",
    "# Ensemble drift detection\n",
    "signals = {\n",
    "    'data_drift': psi > 0.2,\n",
    "    'concept_drift': accuracy < 0.88,\n",
    "    'prediction_drift': pred_distribution_shift > 0.15,\n",
    "    'latency_spike': p95_latency > 100ms\n",
    "}\n",
    "\n",
    "# Alert if 2+ signals triggered\n",
    "if sum(signals.values()) >= 2:\n",
    "    send_alert(\"Multiple drift signals detected\")\n",
    "```\n",
    "\n",
    "#### **5. Root Cause Analysis Logging**\n",
    "\n",
    "**Log context for debugging:**\n",
    "```python\n",
    "prediction_log = {\n",
    "    # Prediction data\n",
    "    'features': X,\n",
    "    'prediction': y_pred,\n",
    "    \n",
    "    # Context (helps identify root cause)\n",
    "    'data_source': 'wafer_test_station_3',\n",
    "    'test_program_version': '2.1.5',\n",
    "    'equipment_id': 'ATE-007',\n",
    "    'fab_location': 'FAB12',\n",
    "    'process_node': '7nm',\n",
    "    'lot_id': 'LOT-2025-001',\n",
    "    \n",
    "    # Model metadata\n",
    "    'model_version': '2.3',\n",
    "    'inference_latency_ms': 45\n",
    "}\n",
    "```\n",
    "\n",
    "**When drift detected**, correlate with:\n",
    "- Equipment changes\n",
    "- Process changes\n",
    "- Software updates\n",
    "- External events (temperature, power)\n",
    "\n",
    "---\n",
    "\n",
    "### **‚ö†Ô∏è Common Pitfalls**\n",
    "\n",
    "#### **1. Over-Monitoring (Alert Fatigue)**\n",
    "- **Problem**: Too many false alarms ‚Üí team ignores alerts\n",
    "- **Solution**: Tune thresholds, require multiple consecutive violations\n",
    "\n",
    "#### **2. Under-Monitoring (Missing Critical Drift)**\n",
    "- **Problem**: Only monitor once/week ‚Üí miss sudden changes\n",
    "- **Solution**: Hourly drift checks, daily performance checks\n",
    "\n",
    "#### **3. No Action Plan**\n",
    "- **Problem**: Alert triggers, but no one knows what to do\n",
    "- **Solution**: Runbook with decision tree (rollback vs retrain vs investigate)\n",
    "\n",
    "#### **4. Ignoring Seasonal Patterns**\n",
    "- **Problem**: Holiday shopping surge ‚Üí \"drift detected\" ‚Üí unnecessary retrain\n",
    "- **Solution**: Use seasonally-adjusted baselines, exclude known patterns\n",
    "\n",
    "#### **5. Not Testing Monitoring System**\n",
    "- **Problem**: Monitoring code has bug ‚Üí false confidence\n",
    "- **Solution**: Inject synthetic drift, verify alerts trigger correctly\n",
    "\n",
    "---\n",
    "\n",
    "### **üîÆ Next Steps**\n",
    "\n",
    "**After mastering monitoring:**\n",
    "1. **124_Feature_Store_Implementation.ipynb** ‚Üí Centralize feature engineering\n",
    "2. **125_ML_Pipeline_Orchestration.ipynb** ‚Üí Automate retrain on drift\n",
    "3. **126_AB_Testing_ML_Models.ipynb** ‚Üí Safe model rollouts\n",
    "4. **131_Docker_Fundamentals.ipynb** ‚Üí Containerize monitoring services\n",
    "\n",
    "**Hands-On Practice:**\n",
    "- Implement PSI monitoring for real dataset\n",
    "- Build Streamlit dashboard showing drift heatmap\n",
    "- Set up automated alerting (email/Slack)\n",
    "- Create auto-retrain trigger (if PSI > 0.2 for 3 days)\n",
    "- Test monitoring with synthetic drift\n",
    "\n",
    "---\n",
    "\n",
    "**You now have complete mastery of model monitoring and drift detection! üöÄ**\n",
    "\n",
    "**Key skill acquired**: Detect silent model failures early, maintain production model health, prevent business impact from model degradation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
