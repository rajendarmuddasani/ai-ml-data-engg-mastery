{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2238914",
   "metadata": {},
   "source": [
    "# 127: Model Governance & Compliance - Model Cards, Bias Detection, and Regulatory Standards\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** model governance framework for transparency, fairness, and compliance\n",
    "- **Implement** model cards for comprehensive model documentation (Google's ML transparency standard)\n",
    "- **Detect** bias in ML models using fairness metrics (demographic parity, equalized odds, disparate impact)\n",
    "- **Apply** explainability techniques to semiconductor test models (SHAP, LIME for parametric predictions)\n",
    "- **Build** compliance frameworks for regulated industries (GDPR, CCPA, FDA, SOX, industry-specific regulations)\n",
    "- **Monitor** model governance metrics and audit trails\n",
    "\n",
    "## üìö What is Model Governance?\n",
    "\n",
    "**Model governance** is the set of **policies, processes, and controls** ensuring ML models are developed, deployed, and monitored **ethically, transparently, and in compliance** with regulations. It answers: Who built this model? What data was used? How accurate is it? Is it fair? Who approved deployment?\n",
    "\n",
    "**Why Model Governance?**\n",
    "- ‚úÖ **Regulatory compliance**: GDPR (right to explanation), CCPA (data privacy), FDA (medical device ML), SOX (financial models)\n",
    "- ‚úÖ **Risk management**: Prevent discriminatory models, detect bias before deployment, audit trail for incidents\n",
    "- ‚úÖ **Trust and transparency**: Stakeholders understand model decisions (engineering, legal, customers)\n",
    "- ‚úÖ **Reproducibility**: Document training data, hyperparameters, validation metrics (recreate model from scratch)\n",
    "\n",
    "**Governance Components:**\n",
    "- **Model Cards**: Standardized documentation (purpose, performance, limitations, fairness, ethical considerations)\n",
    "- **Bias Detection**: Statistical tests for demographic parity, equalized odds, disparate impact ratios\n",
    "- **Explainability**: SHAP values, LIME, feature importance (understand which features drive predictions)\n",
    "- **Audit Logs**: Track who trained, deployed, accessed model (compliance, security, debugging)\n",
    "- **Approval Workflows**: Multi-level approval (data scientist ‚Üí ML engineer ‚Üí compliance ‚Üí deployment)\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "### **Use Case 1: Yield Prediction Model Card for Regulatory Compliance**\n",
    "**Input:** Yield prediction model used for wafer disposition (ship vs scrap decisions worth $50K-$500K per lot)  \n",
    "**Output:** Model card documenting training data, accuracy metrics, limitations, approval chain  \n",
    "**Value:** $3.2M/year from avoiding compliance violations (SOX requires audit trail for financial decisions, model card provides documentation)\n",
    "\n",
    "### **Use Case 2: Bias Detection in Test Coverage Optimization**\n",
    "**Input:** ML model recommending which tests to skip for faster test time (adaptive testing)  \n",
    "**Problem:** Model skips critical tests for certain device types (bias against edge cases)  \n",
    "**Output:** Fairness metrics detect disparate impact (some device families under-tested), model retrained with balanced sampling  \n",
    "**Value:** $2.5M/year from preventing field failures (catch defects before shipping, avoid recalls)\n",
    "\n",
    "### **Use Case 3: SHAP Explainability for Parametric Outlier Detection**\n",
    "**Input:** Random Forest flagging parametric test results as outliers (voltage, current, frequency anomalies)  \n",
    "**Problem:** Engineers don't trust black-box model (why was this flagged? which parameter out of spec?)  \n",
    "**Output:** SHAP values explain each prediction (e.g., \"voltage 3.5V vs expected 3.3V contributed 80% to outlier score\")  \n",
    "**Value:** $1.9M/year from faster root cause analysis (engineers debug test failures 50% faster with explanations)\n",
    "\n",
    "### **Use Case 4: Audit Trail for Model Deployment Approvals**\n",
    "**Input:** Production ML models deployed without approval tracking (who authorized deployment? when? based on what validation?)  \n",
    "**Output:** Governance system logs all model deployments with approvals, validation metrics, rollback capability  \n",
    "**Value:** $1.6M/year from risk mitigation (prevent unapproved model deployments, compliance with internal controls)\n",
    "\n",
    "**Total Post-Silicon Value:** $3.2M + $2.5M + $1.9M + $1.6M = **$9.2M/year**\n",
    "\n",
    "## üîÑ Model Governance Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[üèãÔ∏è Model Training] --> B[üìÑ Create Model Card]\n",
    "    B --> C[üîç Bias Detection]\n",
    "    C --> D{Fairness Pass?}\n",
    "    D -->|No| E[‚ùå Reject Model]\n",
    "    D -->|Yes| F[üí° Explainability Analysis]\n",
    "    \n",
    "    F --> G[üìä Validation Metrics]\n",
    "    G --> H[‚úÖ Approval Workflow]\n",
    "    H --> I{Approved?}\n",
    "    I -->|No| E\n",
    "    I -->|Yes| J[üöÄ Deploy to Production]\n",
    "    \n",
    "    J --> K[üìà Monitor Governance Metrics]\n",
    "    K --> L{Compliance Issue?}\n",
    "    L -->|Yes| M[‚ö†Ô∏è Alert Compliance Team]\n",
    "    L -->|No| N[‚úÖ Audit Log Update]\n",
    "    \n",
    "    E --> O[üìß Notify Team]\n",
    "    M --> O\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style J fill:#e1ffe1\n",
    "    style E fill:#ffe1e1\n",
    "    style D fill:#fff4e1\n",
    "    style I fill:#fff4e1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **Notebook 104: Model Interpretability & Explainability** - SHAP, LIME techniques for model explanation\n",
    "- **Notebook 125: ML Testing & Validation** - Validation frameworks for model quality gates\n",
    "\n",
    "**Next Steps:**\n",
    "- **Notebook 128: Shadow Mode Deployment** - Safe deployment with governance checks\n",
    "- **Notebook 129: Advanced MLOps - Feature Stores** - Data governance for feature engineering\n",
    "\n",
    "---\n",
    "\n",
    "Let's build trustworthy ML systems with governance! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b0d2a",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "**Note**: Model governance requires fairness libraries (Fairlearn, AIF360) and explainability tools (SHAP, LIME)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e38acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install governance and fairness libraries\n",
    "# !pip install scikit-learn pandas numpy fairlearn shap matplotlib seaborn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Model governance libraries loaded\")\n",
    "print(\"Focus: Model cards, bias detection, compliance, explainability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced3593f",
   "metadata": {},
   "source": [
    "## 2. Model Cards - Documentation Standard\n",
    "\n",
    "**Purpose:** Implement model cards for comprehensive model documentation (Google's standard for ML transparency).\n",
    "\n",
    "**Key Points:**\n",
    "- **Model cards**: Structured documentation describing model purpose, performance, limitations, fairness\n",
    "- **Sections**: Model details, intended use, metrics, training data, ethical considerations, caveats\n",
    "- **Audience**: Technical (data scientists), business (executives), compliance (auditors)\n",
    "- **Living document**: Update with each model version, retraining, or performance change\n",
    "\n",
    "**Why This Matters:** Model cards provide transparency, enable informed decisions, and satisfy regulatory requirements (EU AI Act requires documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98fc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCard:\n",
    "    \"\"\"\n",
    "    Model card generator following Google's model cards specification.\n",
    "    \n",
    "    Provides structured documentation for ML models including:\n",
    "    - Model details (architecture, version, training)\n",
    "    - Intended use cases and limitations\n",
    "    - Performance metrics (overall and per-group)\n",
    "    - Training data characteristics\n",
    "    - Ethical considerations and fairness\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, model_version, use_case):\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        self.use_case = use_case\n",
    "        self.created_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        self.card_data = {}\n",
    "        \n",
    "    def add_model_details(self, model_type, developer, training_date, framework):\n",
    "        \"\"\"Add technical model details.\"\"\"\n",
    "        self.card_data['model_details'] = {\n",
    "            'name': self.model_name,\n",
    "            'version': self.model_version,\n",
    "            'type': model_type,\n",
    "            'developer': developer,\n",
    "            'training_date': training_date,\n",
    "            'framework': framework,\n",
    "            'license': 'Proprietary'\n",
    "        }\n",
    "        \n",
    "    def add_intended_use(self, primary_uses, out_of_scope_uses, users):\n",
    "        \"\"\"Document intended and prohibited use cases.\"\"\"\n",
    "        self.card_data['intended_use'] = {\n",
    "            'primary_uses': primary_uses,\n",
    "            'out_of_scope': out_of_scope_uses,\n",
    "            'primary_users': users\n",
    "        }\n",
    "        \n",
    "    def add_metrics(self, overall_metrics, per_group_metrics=None):\n",
    "        \"\"\"\n",
    "        Add performance metrics.\n",
    "        \n",
    "        Args:\n",
    "            overall_metrics: dict of overall performance (accuracy, F1, etc.)\n",
    "            per_group_metrics: dict of metrics broken down by sensitive groups\n",
    "        \"\"\"\n",
    "        self.card_data['metrics'] = {\n",
    "            'overall': overall_metrics,\n",
    "            'per_group': per_group_metrics or {}\n",
    "        }\n",
    "        \n",
    "    def add_training_data(self, data_source, size, time_period, features):\n",
    "        \"\"\"Document training data characteristics.\"\"\"\n",
    "        self.card_data['training_data'] = {\n",
    "            'source': data_source,\n",
    "            'size': size,\n",
    "            'time_period': time_period,\n",
    "            'features': features\n",
    "        }\n",
    "        \n",
    "    def add_evaluation_data(self, data_source, size, time_period):\n",
    "        \"\"\"Document evaluation/test data.\"\"\"\n",
    "        self.card_data['evaluation_data'] = {\n",
    "            'source': data_source,\n",
    "            'size': size,\n",
    "            'time_period': time_period\n",
    "        }\n",
    "        \n",
    "    def add_ethical_considerations(self, fairness_assessment, bias_mitigation, privacy_measures):\n",
    "        \"\"\"Document ethical considerations and mitigation strategies.\"\"\"\n",
    "        self.card_data['ethical_considerations'] = {\n",
    "            'fairness_assessment': fairness_assessment,\n",
    "            'bias_mitigation': bias_mitigation,\n",
    "            'privacy_measures': privacy_measures\n",
    "        }\n",
    "        \n",
    "    def add_caveats_and_recommendations(self, limitations, recommendations):\n",
    "        \"\"\"Document model limitations and usage recommendations.\"\"\"\n",
    "        self.card_data['caveats'] = {\n",
    "            'limitations': limitations,\n",
    "            'recommendations': recommendations\n",
    "        }\n",
    "        \n",
    "    def export_json(self, filepath=None):\n",
    "        \"\"\"Export model card as JSON.\"\"\"\n",
    "        card_json = {\n",
    "            'model_card': {\n",
    "                'created': self.created_date,\n",
    "                'use_case': self.use_case,\n",
    "                **self.card_data\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if filepath:\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(card_json, f, indent=2)\n",
    "            print(f\\\"‚úÖ Model card exported to {filepath}\")\n",
    "        \n",
    "        return card_json\n",
    "    \n",
    "    def generate_markdown(self):\n",
    "        \"\"\"Generate human-readable Markdown model card.\"\"\"\n",
    "        md = f\\\"# Model Card: {self.model_name} (v{self.model_version})\\\\n\\\\n\\\"\\n        md += f\\\"**Created**: {self.created_date}\\\\n\\\"\\n        md += f\\\"**Use Case**: {self.use_case}\\\\n\\\\n\\\"\\n        \\n        # Model Details\\n        if 'model_details' in self.card_data:\\n            md += \\\"## Model Details\\\\n\\\\n\\\"\\n            details = self.card_data['model_details']\\n            for key, value in details.items():\\n                md += f\\\"- **{key.replace('_', ' ').title()}**: {value}\\\\n\\\"\\n            md += \\\"\\\\n\\\"\\n        \\n        # Intended Use\\n        if 'intended_use' in self.card_data:\\n            md += \\\"## Intended Use\\\\n\\\\n\\\"\\n            intended = self.card_data['intended_use']\\n            md += \\\"**Primary Uses:**\\\\n\\\"\\n            for use in intended['primary_uses']:\\n                md += f\\\"- {use}\\\\n\\\"\\n            md += \\\"\\\\n**Out of Scope:**\\\\n\\\"\\n            for use in intended['out_of_scope']:\\n                md += f\\\"- {use}\\\\n\\\"\\n            md += \\\"\\\\n\\\"\\n        \\n        # Metrics\\n        if 'metrics' in self.card_data:\\n            md += \\\"## Performance Metrics\\\\n\\\\n\\\"\\n            md += \\\"**Overall Performance:**\\\\n\\\"\\n            for metric, value in self.card_data['metrics']['overall'].items():\\n                md += f\\\"- {metric}: {value}\\\\n\\\"\\n            \\n            if self.card_data['metrics']['per_group']:\\n                md += \\\"\\\\n**Per-Group Performance:**\\\\n\\\"\\n                for group, metrics in self.card_data['metrics']['per_group'].items():\\n                    md += f\\\"\\\\n*{group}*:\\\\n\\\"\\n                    for metric, value in metrics.items():\\n                        md += f\\\"- {metric}: {value}\\\\n\\\"\\n            md += \\\"\\\\n\\\"\\n        \\n        # Ethical Considerations\\n        if 'ethical_considerations' in self.card_data:\\n            md += \\\"## Ethical Considerations\\\\n\\\\n\\\"\\n            ethical = self.card_data['ethical_considerations']\\n            md += f\\\"**Fairness**: {ethical['fairness_assessment']}\\\\n\\\\n\\\"\\n            md += f\\\"**Bias Mitigation**: {ethical['bias_mitigation']}\\\\n\\\\n\\\"\\n            md += f\\\"**Privacy**: {ethical['privacy_measures']}\\\\n\\\\n\\\"\\n        \\n        # Caveats\\n        if 'caveats' in self.card_data:\\n            md += \\\"## Limitations & Recommendations\\\\n\\\\n\\\"\\n            md += \\\"**Limitations:**\\\\n\\\"\\n            for limitation in self.card_data['caveats']['limitations']:\\n                md += f\\\"- {limitation}\\\\n\\\"\\n            md += \\\"\\\\n**Recommendations:**\\\\n\\\"\\n            for rec in self.card_data['caveats']['recommendations']:\\n                md += f\\\"- {rec}\\\\n\\\"\\n        \\n        return md\n",
    "\n",
    "# Example: Create model card for yield prediction model\n",
    "card = ModelCard(\\n    model_name=\\\"Wafer Yield Predictor\\\",\\n    model_version=\\\"2.1.0\\\",\\n    use_case=\\\"Predict wafer test yield for fab capacity planning\\\"\\n)\n",
    "\n",
    "# Add model details\\ncard.add_model_details(\\n    model_type=\\\"Random Forest Classifier (100 trees)\\\",\\n    developer=\\\"Post-Silicon AI Team\\\",\\n    training_date=\\\"2024-12-01\\\",\\n    framework=\\\"scikit-learn 1.3.0\\\"\\n)\n",
    "\n",
    "# Add intended use\\ncard.add_intended_use(\\n    primary_uses=[\\n        \\\"Predict wafer yield (pass/fail) for production lots\\\",\\n        \\\"Fab capacity planning and resource allocation\\\",\\n        \\\"Early detection of yield excursions (>5% drop)\\\"\\n    ],\\n    out_of_scope_uses=[\\n        \\\"Final test yield prediction (different data distribution)\\\",\\n        \\\"Individual die-level prediction (model trained on wafer-level)\\\",\\n        \\\"Root cause analysis (model is black-box, use separate tools)\\\"\\n    ],\\n    users=[\\\"Fab operations\\\", \\\"Capacity planners\\\", \\\"Yield engineers\\\"]\\n)\n",
    "\n",
    "# Add metrics\\ncard.add_metrics(\\n    overall_metrics={\\n        'accuracy': 0.923,\\n        'precision': 0.91,\\n        'recall': 0.89,\\n        'f1_score': 0.90,\\n        'auc_roc': 0.95\\n    },\\n    per_group_metrics={\\n        'Fab A': {'accuracy': 0.925, 'f1_score': 0.91},\\n        'Fab B': {'accuracy': 0.920, 'f1_score': 0.89},\\n        'Fab C': {'accuracy': 0.924, 'f1_score': 0.90}\\n    }\\n)\n",
    "\n",
    "# Add training data\\ncard.add_training_data(\\n    data_source=\\\"STDF wafer test data (production)\\\",\\n    size=\\\"50,000 wafers (2023-06-01 to 2024-11-30)\\\",\\n    time_period=\\\"18 months\\\",\\n    features=[\\\"Vdd\\\", \\\"Idd\\\", \\\"Frequency\\\", \\\"Temperature\\\", \\\"Power\\\", \\\"Test Coverage\\\"]\\n)\n",
    "\n",
    "# Add ethical considerations\\ncard.add_ethical_considerations(\\n    fairness_assessment=\\\"Accuracy variance across 3 fabs is <0.5% (no significant bias)\\\",\\n    bias_mitigation=\\\"Stratified sampling ensures equal representation of all fabs in training data\\\",\\n    privacy_measures=\\\"Wafer IDs anonymized, no personally identifiable information (PII)\\\"\\n)\n",
    "\n",
    "# Add caveats\\ncard.add_caveats_and_recommendations(\\n    limitations=[\\n        \\\"Model accuracy degrades when Vdd distribution shifts >50mV (data drift)\\\",\\n        \\\"Not validated for new product lines (different test coverage)\\\",\\n        \\\"Performance drops to 85% accuracy for lots with <100 die (small sample size)\\\"\\n    ],\\n    recommendations=[\\n        \\\"Retrain weekly or when data drift detected (KS test p-value < 0.05)\\\",\\n        \\\"Monitor per-fab accuracy monthly to detect bias\\\",\\n        \\\"Use ensemble with physics-based model for critical decisions (>$1M impact)\\\",\\n        \\\"Do not use for lots with <100 die (insufficient data)\\\"\\n    ]\\n)\n",
    "\n",
    "# Generate and display markdown\\nmarkdown_card = card.generate_markdown()\\nprint(markdown_card)\\n\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*80)\\nprint(\\\"‚úÖ Model card generated\\\")\\nprint(\\\"üíæ Can export to: JSON (programmatic), Markdown (human-readable), HTML (web display)\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e6fd6",
   "metadata": {},
   "source": [
    "## 3. Bias Detection & Fairness Evaluation\n",
    "\n",
    "**Purpose:** Implement fairness metrics to detect and quantify bias in ML models.\n",
    "\n",
    "**Key Points:**\n",
    "- **Fairness metrics**: Demographic parity, equalized odds, disparate impact, calibration\n",
    "- **Sensitive attributes**: Protected groups (fab location, device type, shift) that should not cause bias\n",
    "- **Fairness constraints**: Ensure similar performance across groups (accuracy variance <5%)\n",
    "- **Mitigation strategies**: Reweighting, resampling, adversarial debiasing, fairness-aware algorithms\n",
    "\n",
    "**Why This Matters:** Biased models lead to unfair outcomes (revenue loss, customer complaints), regulatory fines, and reputational damage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eebc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairnessEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate model fairness across sensitive groups.\n",
    "    \n",
    "    Implements multiple fairness metrics:\n",
    "    - Demographic parity: P(pred=1 | group=A) ‚âà P(pred=1 | group=B)\n",
    "    - Equalized odds: TPR and FPR equal across groups\n",
    "    - Disparate impact: Ratio of positive rates (should be >0.8)\n",
    "    - Calibration: Predicted probabilities match observed outcomes per group\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sensitive_attribute='group'):\n",
    "        self.sensitive_attribute = sensitive_attribute\n",
    "        \n",
    "    def demographic_parity(self, y_pred, sensitive_features):\n",
    "        \"\"\"\n",
    "        Measure demographic parity: positive rate should be similar across groups.\n",
    "        \n",
    "        Returns: dict of positive rates per group and max difference\n",
    "        \"\"\"\n",
    "        groups = np.unique(sensitive_features)\n",
    "        positive_rates = {}\n",
    "        \n",
    "        for group in groups:\n",
    "            group_mask = sensitive_features == group\n",
    "            positive_rate = np.mean(y_pred[group_mask])\n",
    "            positive_rates[group] = positive_rate\n",
    "        \n",
    "        # Calculate max difference (fairness violation if >10%)\n",
    "        max_diff = max(positive_rates.values()) - min(positive_rates.values())\n",
    "        \n",
    "        return {\n",
    "            'positive_rates': positive_rates,\n",
    "            'max_difference': max_diff,\n",
    "            'violation': max_diff > 0.10,\n",
    "            'interpretation': 'Demographic parity satisfied' if max_diff <= 0.10 else 'Demographic parity violated'\n",
    "        }\n",
    "    \n",
    "    def equalized_odds(self, y_true, y_pred, sensitive_features):\n",
    "        \"\"\"\n",
    "        Measure equalized odds: TPR and FPR should be equal across groups.\n",
    "        \n",
    "        TPR = True Positive Rate (sensitivity)\n",
    "        FPR = False Positive Rate\n",
    "        \"\"\"\n",
    "        groups = np.unique(sensitive_features)\n",
    "        group_metrics = {}\n",
    "        \n",
    "        for group in groups:\n",
    "            group_mask = sensitive_features == group\n",
    "            y_true_group = y_true[group_mask]\n",
    "            y_pred_group = y_pred[group_mask]\n",
    "            \n",
    "            # Calculate TPR and FPR\n",
    "            tp = np.sum((y_true_group == 1) & (y_pred_group == 1))\n",
    "            fn = np.sum((y_true_group == 1) & (y_pred_group == 0))\n",
    "            fp = np.sum((y_true_group == 0) & (y_pred_group == 1))\n",
    "            tn = np.sum((y_true_group == 0) & (y_pred_group == 0))\n",
    "            \n",
    "            tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "            \n",
    "            group_metrics[group] = {'tpr': tpr, 'fpr': fpr}\n",
    "        \n",
    "        # Calculate max difference in TPR and FPR\n",
    "        tpr_values = [m['tpr'] for m in group_metrics.values()]\n",
    "        fpr_values = [m['fpr'] for m in group_metrics.values()]\n",
    "        \n",
    "        tpr_diff = max(tpr_values) - min(tpr_values)\n",
    "        fpr_diff = max(fpr_values) - min(fpr_values)\n",
    "        \n",
    "        return {\n",
    "            'group_metrics': group_metrics,\n",
    "            'tpr_difference': tpr_diff,\n",
    "            'fpr_difference': fpr_diff,\n",
    "            'violation': tpr_diff > 0.10 or fpr_diff > 0.10,\n",
    "            'interpretation': 'Equalized odds satisfied' if (tpr_diff <= 0.10 and fpr_diff <= 0.10) else 'Equalized odds violated'\n",
    "        }\n",
    "    \n",
    "    def disparate_impact(self, y_pred, sensitive_features, reference_group=None):\n",
    "        \"\"\"\n",
    "        Measure disparate impact: ratio of positive rates (80% rule).\n",
    "        \n",
    "        Disparate impact ratio = min(P(pred=1|group)) / max(P(pred=1|group))\n",
    "        Should be >= 0.8 (80% rule from US employment law)\n",
    "        \"\"\"\n",
    "        groups = np.unique(sensitive_features)\n",
    "        positive_rates = {}\n",
    "        \n",
    "        for group in groups:\n",
    "            group_mask = sensitive_features == group\n",
    "            positive_rate = np.mean(y_pred[group_mask])\n",
    "            positive_rates[group] = positive_rate\n",
    "        \n",
    "        min_rate = min(positive_rates.values())\n",
    "        max_rate = max(positive_rates.values())\n",
    "        \n",
    "        di_ratio = min_rate / max_rate if max_rate > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'positive_rates': positive_rates,\n",
    "            'disparate_impact_ratio': di_ratio,\n",
    "            'violation': di_ratio < 0.8,\n",
    "            'interpretation': 'Passes 80% rule' if di_ratio >= 0.8 else 'Fails 80% rule (disparate impact detected)'\n",
    "        }\n",
    "    \n",
    "    def accuracy_parity(self, y_true, y_pred, sensitive_features, threshold=0.05):\n",
    "        \"\"\"\n",
    "        Measure accuracy parity: accuracy should be similar across groups.\n",
    "        \n",
    "        Variance in accuracy across groups should be < threshold (e.g., 5%)\n",
    "        \"\"\"\n",
    "        groups = np.unique(sensitive_features)\n",
    "        accuracies = {}\n",
    "        \n",
    "        for group in groups:\n",
    "            group_mask = sensitive_features == group\n",
    "            accuracy = np.mean(y_true[group_mask] == y_pred[group_mask])\n",
    "            accuracies[group] = accuracy\n",
    "        \n",
    "        max_diff = max(accuracies.values()) - min(accuracies.values())\n",
    "        \n",
    "        return {\n",
    "            'accuracies': accuracies,\n",
    "            'max_difference': max_diff,\n",
    "            'violation': max_diff > threshold,\n",
    "            'interpretation': f'Accuracy parity satisfied (variance < {threshold})' if max_diff <= threshold else f'Accuracy parity violated (variance > {threshold})'\n",
    "        }\n",
    "    \n",
    "    def comprehensive_report(self, y_true, y_pred, sensitive_features):\n",
    "        \"\"\"Generate comprehensive fairness report with all metrics.\"\"\"\n",
    "        report = {\n",
    "            'demographic_parity': self.demographic_parity(y_pred, sensitive_features),\n",
    "            'equalized_odds': self.equalized_odds(y_true, y_pred, sensitive_features),\n",
    "            'disparate_impact': self.disparate_impact(y_pred, sensitive_features),\n",
    "            'accuracy_parity': self.accuracy_parity(y_true, y_pred, sensitive_features)\n",
    "        }\n",
    "        \n",
    "        # Overall fairness assessment\n",
    "        violations = sum([metric['violation'] for metric in report.values()])\n",
    "        report['overall_assessment'] = {\n",
    "            'total_violations': violations,\n",
    "            'status': 'FAIR' if violations == 0 else 'BIASED',\n",
    "            'recommendation': 'Model passes all fairness checks' if violations == 0 else f'{violations} fairness violations detected - investigate and mitigate'\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Example: Evaluate fairness for binning model across device types\n",
    "print(\\\"üîç Fairness Evaluation: Binning Model Across Device Types\\\\n\\\")\\nprint(\\\"=\\\"*80)\\n\\n# Simulate binning predictions (3 device types)\\nnp.random.seed(42)\\nn_samples = 1000\\n\\n# Device types (sensitive attribute)\\ndevice_types = np.random.choice(['Type_A', 'Type_B', 'Type_C'], n_samples)\\n\\n# True bins (0=fail, 1=pass)\\ny_true = np.random.choice([0, 1], n_samples, p=[0.15, 0.85])\\n\\n# Predictions with intentional bias (Type_B has lower positive rate)\\ny_pred = y_true.copy()\\n\\n# Introduce bias: Type_B has 10% lower pass rate\\ntype_b_mask = device_types == 'Type_B'\\ntype_b_indices = np.where(type_b_mask & (y_pred == 1))[0]\\nflip_indices = np.random.choice(type_b_indices, size=int(0.10 * len(type_b_indices)), replace=False)\\ny_pred[flip_indices] = 0\\n\\n# Evaluate fairness\\nevaluator = FairnessEvaluator()\\nfairness_report = evaluator.comprehensive_report(y_true, y_pred, device_types)\\n\\n# Display results\\nprint(\\\"\\\\n1Ô∏è‚É£ DEMOGRAPHIC PARITY\\\")\\ndp = fairness_report['demographic_parity']\\nfor device, rate in dp['positive_rates'].items():\\n    print(f\\\"   {device}: {rate:.3f} pass rate\\\")\\nprint(f\\\"   Max difference: {dp['max_difference']:.3f}\\\")\\nprint(f\\\"   Status: {dp['interpretation']}\\\")\\n\\nprint(\\\"\\\\n2Ô∏è‚É£ EQUALIZED ODDS\\\")\\neo = fairness_report['equalized_odds']\\nfor device, metrics in eo['group_metrics'].items():\\n    print(f\\\"   {device}: TPR={metrics['tpr']:.3f}, FPR={metrics['fpr']:.3f}\\\")\\nprint(f\\\"   TPR difference: {eo['tpr_difference']:.3f}\\\")\\nprint(f\\\"   FPR difference: {eo['fpr_difference']:.3f}\\\")\\nprint(f\\\"   Status: {eo['interpretation']}\\\")\\n\\nprint(\\\"\\\\n3Ô∏è‚É£ DISPARATE IMPACT (80% Rule)\\\")\\ndi = fairness_report['disparate_impact']\\nprint(f\\\"   Disparate impact ratio: {di['disparate_impact_ratio']:.3f}\\\")\\nprint(f\\\"   Status: {di['interpretation']}\\\")\\n\\nprint(\\\"\\\\n4Ô∏è‚É£ ACCURACY PARITY\\\")\\nap = fairness_report['accuracy_parity']\\nfor device, acc in ap['accuracies'].items():\\n    print(f\\\"   {device}: {acc:.3f} accuracy\\\")\\nprint(f\\\"   Max difference: {ap['max_difference']:.3f}\\\")\\nprint(f\\\"   Status: {ap['interpretation']}\\\")\\n\\nprint(\\\"\\\\n\\\" + \\\"=\\\"*80)\\noverall = fairness_report['overall_assessment']\\nprint(f\\\"\\\\nüìä OVERALL FAIRNESS ASSESSMENT\\\")\\nprint(f\\\"   Status: {overall['status']}\\\")\\nprint(f\\\"   Violations: {overall['total_violations']}/4 metrics\\\")\\nprint(f\\\"   Recommendation: {overall['recommendation']}\\\")\\n\\nif overall['status'] == 'BIASED':\\n    print(\\\"\\\\n‚ö†Ô∏è  ACTION REQUIRED:\\\")\\n    print(\\\"   1. Investigate root cause (data imbalance, feature bias)\\\")\\n    print(\\\"   2. Apply mitigation (reweighting, resampling, fairness constraints)\\\")\\n    print(\\\"   3. Re-evaluate fairness after mitigation\\\")\\n    print(\\\"   4. Document in model card\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e592a8",
   "metadata": {},
   "source": [
    "## 4. Audit Trails & Model Lineage\n",
    "\n",
    "**Purpose:** Implement comprehensive audit trails for model governance and compliance.\n",
    "\n",
    "**Key Points:**\n",
    "- **Model lineage**: Track data ‚Üí features ‚Üí training ‚Üí deployment pipeline\n",
    "- **Audit log**: Record all model decisions (who, what, when, why)\n",
    "- **Versioning**: Track model versions, training data versions, code versions (Git commit)\n",
    "- **Approval workflow**: Document who approved model for production, when, based on what criteria\n",
    "\n",
    "**Why This Matters:** Audit trails enable accountability, regulatory compliance (GDPR Article 22 - right to explanation), and incident investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae5c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAuditTrail:\n",
    "    \"\"\"\n",
    "    Comprehensive audit trail system for model governance.\n",
    "    \n",
    "    Tracks:\n",
    "    - Training events (data, hyperparameters, metrics)\n",
    "    - Deployment events (who, when, approval)\n",
    "    - Inference events (predictions, inputs, outputs)\n",
    "    - Governance events (fairness checks, compliance reviews)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_id):\n",
    "        self.model_id = model_id\n",
    "        self.audit_log = []\n",
    "        \n",
    "    def log_training(self, version, data_source, features, metrics, trained_by):\n",
    "        \"\"\"Log model training event.\"\"\"\n",
    "        event = {\n",
    "            'event_type': 'TRAINING',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_version': version,\n",
    "            'data_source': data_source,\n",
    "            'features': features,\n",
    "            'metrics': metrics,\n",
    "            'trained_by': trained_by,\n",
    "            'git_commit': self._get_git_commit()  # Placeholder\n",
    "        }\n",
    "        self.audit_log.append(event)\n",
    "        return event\n",
    "        \n",
    "    def log_validation(self, version, validation_results, validated_by):\n",
    "        \"\"\"Log model validation event.\"\"\"\n",
    "        event = {\n",
    "            'event_type': 'VALIDATION',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_version': version,\n",
    "            'validation_results': validation_results,\n",
    "            'validated_by': validated_by\n",
    "        }\n",
    "        self.audit_log.append(event)\n",
    "        return event\n",
    "        \n",
    "    def log_fairness_check(self, version, fairness_metrics, passed, checked_by):\n",
    "        \"\"\"Log fairness evaluation event.\"\"\"\n",
    "        event = {\n",
    "            'event_type': 'FAIRNESS_CHECK',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_version': version,\n",
    "            'fairness_metrics': fairness_metrics,\n",
    "            'passed': passed,\n",
    "            'checked_by': checked_by\n",
    "        }\n",
    "        self.audit_log.append(event)\n",
    "        return event\n",
    "        \n",
    "    def log_approval(self, version, approved_by, approval_criteria, comments):\n",
    "        \"\"\"Log model approval for production deployment.\"\"\"\n",
    "        event = {\n",
    "            'event_type': 'APPROVAL',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_version': version,\n",
    "            'approved_by': approved_by,\n",
    "            'approval_criteria': approval_criteria,\n",
    "            'comments': comments\n",
    "        }\n",
    "        self.audit_log.append(event)\n",
    "        return event\n",
    "        \n",
    "    def log_deployment(self, version, environment, deployed_by):\n",
    "        \"\"\"Log model deployment event.\"\"\"\n",
    "        event = {\n",
    "            'event_type': 'DEPLOYMENT',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_version': version,\n",
    "            'environment': environment,\n",
    "            'deployed_by': deployed_by\n",
    "        }\n",
    "        self.audit_log.append(event)\n",
    "        return event\n",
    "        \n",
    "    def log_inference(self, version, input_sample, prediction, confidence=None):\n",
    "        \\\"\\\"\\\"Log individual inference (for high-stakes predictions).\\\"\\\"\\\"\n",
    "        event = {\n",
    "            'event_type': 'INFERENCE',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_version': version,\n",
    "            'input_sample': input_sample,\n",
    "            'prediction': prediction,\n",
    "            'confidence': confidence\n",
    "        }\n",
    "        self.audit_log.append(event)\n",
    "        return event\n",
    "        \n",
    "    def log_incident(self, version, incident_type, description, reported_by):\n",
    "        \\\"\\\"\\\"Log model-related incident.\\\"\\\"\\\"\n",
    "        event = {\n",
    "            'event_type': 'INCIDENT',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_version': version,\n",
    "            'incident_type': incident_type,\n",
    "            'description': description,\n",
    "            'reported_by': reported_by,\n",
    "            'severity': self._determine_severity(incident_type)\n",
    "        }\n",
    "        self.audit_log.append(event)\n",
    "        return event\n",
    "        \n",
    "    def get_lineage(self, version):\n",
    "        \\\"\\\"\\\"Get complete lineage for a specific model version.\\\"\\\"\\\"\n",
    "        lineage = [event for event in self.audit_log if event.get('model_version') == version]\n",
    "        return sorted(lineage, key=lambda x: x['timestamp'])\n",
    "        \n",
    "    def get_timeline(self):\n",
    "        \\\"\\\"\\\"Get chronological timeline of all events.\\\"\\\"\\\"\n",
    "        return sorted(self.audit_log, key=lambda x: x['timestamp'])\n",
    "        \n",
    "    def export_compliance_report(self, version):\n",
    "        \\\"\\\"\\\"Generate compliance report for auditors/regulators.\\\"\\\"\\\"\n",
    "        lineage = self.get_lineage(version)\n",
    "        \n",
    "        report = {\n",
    "            'model_id': self.model_id,\n",
    "            'model_version': version,\n",
    "            'report_generated': datetime.now().isoformat(),\n",
    "            'events': lineage,\n",
    "            'summary': {\n",
    "                'total_events': len(lineage),\n",
    "                'training_events': sum(1 for e in lineage if e['event_type'] == 'TRAINING'),\n",
    "                'validation_events': sum(1 for e in lineage if e['event_type'] == 'VALIDATION'),\n",
    "                'fairness_checks': sum(1 for e in lineage if e['event_type'] == 'FAIRNESS_CHECK'),\n",
    "                'approvals': sum(1 for e in lineage if e['event_type'] == 'APPROVAL'),\n",
    "                'deployments': sum(1 for e in lineage if e['event_type'] == 'DEPLOYMENT'),\n",
    "                'incidents': sum(1 for e in lineage if e['event_type'] == 'INCIDENT')\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "        \n",
    "    def _get_git_commit(self):\n",
    "        \\\"\\\"\\\"Get current Git commit hash (placeholder).\\\"\\\"\\\"\n",
    "        return \\\"a3f7b2c\\\"  # In production: subprocess.check_output(['git', 'rev-parse', 'HEAD'])\n",
    "        \n",
    "    def _determine_severity(self, incident_type):\n",
    "        \\\"\\\"\\\"Determine incident severity.\\\"\\\"\\\"\n",
    "        severity_map = {\n",
    "            'performance_degradation': 'MEDIUM',\n",
    "            'fairness_violation': 'HIGH',\n",
    "            'prediction_error': 'MEDIUM',\n",
    "            'security_breach': 'CRITICAL',\n",
    "            'compliance_violation': 'CRITICAL'\n",
    "        }\n",
    "        return severity_map.get(incident_type, 'LOW')\n",
    "\n",
    "# Example: Model lifecycle with complete audit trail\n",
    "print(\\\"üìã Model Audit Trail: Yield Prediction Model v2.1.0\\\\n\\\")\n",
    "print(\\\"=\\\"*80)\\n\\naudit = ModelAuditTrail(model_id=\\\"yield_predictor_001\\\")\n",
    "\n",
    "# Step 1: Training\\ntraining_event = audit.log_training(\\n    version=\\\"2.1.0\\\",\\n    data_source=\\\"STDF wafer test (2023-06 to 2024-11)\\\",\\n    features=[\\\"Vdd\\\", \\\"Idd\\\", \\\"Frequency\\\", \\\"Temperature\\\", \\\"Power\\\"],\\n    metrics={'accuracy': 0.923, 'f1_score': 0.90},\\n    trained_by=\\\"Alice Chen (ML Engineer)\\\"\\n)\\nprint(f\\\"‚úÖ Training logged: {training_event['timestamp']}\\\")\\nprint(f\\\"   Version: {training_event['model_version']}\\\")\\nprint(f\\\"   Accuracy: {training_event['metrics']['accuracy']}\\\")\\n\\n# Step 2: Validation\\nvalidation_event = audit.log_validation(\\n    version=\\\"2.1.0\\\",\\n    validation_results={\\n        'passed_gates': ['accuracy >= 0.90', 'better_than_production'],\\n        'failed_gates': []\\n    },\\n    validated_by=\\\"Bob Martinez (ML Engineer)\\\"\\n)\\nprint(f\\\"\\\\n‚úÖ Validation logged: {validation_event['timestamp']}\\\")\\nprint(f\\\"   All gates passed: {len(validation_event['validation_results']['failed_gates']) == 0}\\\")\\n\\n# Step 3: Fairness check\\nfairness_event = audit.log_fairness_check(\\n    version=\\\"2.1.0\\\",\\n    fairness_metrics={\\n        'demographic_parity': 'PASS',\\n        'accuracy_variance': 0.005,  # 0.5% across fabs\\n        'disparate_impact_ratio': 0.95\\n    },\\n    passed=True,\\n    checked_by=\\\"Carol Thompson (AI Ethics Lead)\\\"\\n)\\nprint(f\\\"\\\\n‚úÖ Fairness check logged: {fairness_event['timestamp']}\\\")\\nprint(f\\\"   Status: {'PASSED' if fairness_event['passed'] else 'FAILED'}\\\")\\nprint(f\\\"   Accuracy variance: {fairness_event['fairness_metrics']['accuracy_variance']:.3f}\\\")\\n\\n# Step 4: Approval\\napproval_event = audit.log_approval(\\n    version=\\\"2.1.0\\\",\\n    approved_by=\\\"David Lee (Director, Post-Silicon AI)\\\",\\n    approval_criteria=[\\n        \\\"Accuracy >= 0.92 (target: 0.90)\\\",\\n        \\\"Fairness checks passed\\\",\\n        \\\"Better than production model by 2%\\\",\\n        \\\"Model card reviewed and approved\\\"\\n    ],\\n    comments=\\\"Approved for production deployment. Monitor per-fab accuracy weekly.\\\"\\n)\\nprint(f\\\"\\\\n‚úÖ Approval logged: {approval_event['timestamp']}\\\")\\nprint(f\\\"   Approved by: {approval_event['approved_by']}\\\")\\nprint(f\\\"   Comments: {approval_event['comments']}\\\")\\n\\n# Step 5: Deployment\\ndeployment_event = audit.log_deployment(\\n    version=\\\"2.1.0\\\",\\n    environment=\\\"Production (Fabs A, B, C)\\\",\\n    deployed_by=\\\"Eve Johnson (DevOps Engineer)\\\"\\n)\\nprint(f\\\"\\\\n‚úÖ Deployment logged: {deployment_event['timestamp']}\\\")\\nprint(f\\\"   Environment: {deployment_event['environment']}\\\")\\n\\n# Step 6: Log some inferences (high-stakes predictions)\\ninference1 = audit.log_inference(\\n    version=\\\"2.1.0\\\",\\n    input_sample={'wafer_id': 'W12345', 'vdd': 1.25, 'idd': 52, 'freq': 2450},\\n    prediction='PASS',\\n    confidence=0.94\\n)\\nprint(f\\\"\\\\nüìä Inference logged: Wafer W12345 ‚Üí {inference1['prediction']} (conf: {inference1['confidence']})\\\")\\n\\n# Step 7: Incident (example: performance degradation detected)\\nincident_event = audit.log_incident(\\n    version=\\\"2.1.0\\\",\\n    incident_type=\\\"performance_degradation\\\",\\n    description=\\\"Accuracy dropped to 0.88 on Fab C (data drift detected in Vdd distribution)\\\",\\n    reported_by=\\\"Automated Monitoring System\\\"\\n)\\nprint(f\\\"\\\\n‚ö†Ô∏è  Incident logged: {incident_event['timestamp']}\\\")\\nprint(f\\\"   Type: {incident_event['incident_type']}\\\")\\nprint(f\\\"   Severity: {incident_event['severity']}\\\")\\nprint(f\\\"   Description: {incident_event['description']}\\\")\\n\\n# Generate compliance report\\nprint(f\\\"\\\\n{'-'*80}\\\")\\nprint(\\\"üìÑ COMPLIANCE REPORT\\\")\\nprint(f\\\"{'-'*80}\\\\n\\\")\\n\\nreport = audit.export_compliance_report(version=\\\"2.1.0\\\")\\nprint(f\\\"Model: {report['model_id']} v{report['model_version']}\\\")\\nprint(f\\\"Report generated: {report['report_generated']}\\\")\\nprint(f\\\"\\\\nEvent Summary:\\\")\\nfor event_type, count in report['summary'].items():\\n    if event_type != 'total_events':\\n        print(f\\\"  {event_type.replace('_', ' ').title()}: {count}\\\")\\n\\nprint(f\\\"\\\\nComplete audit trail with {report['summary']['total_events']} events available for regulatory review\\\")\\nprint(\\\"‚úÖ Full lineage from training ‚Üí validation ‚Üí fairness ‚Üí approval ‚Üí deployment ‚Üí production\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e86f323",
   "metadata": {},
   "source": [
    "## 5. Regulatory Compliance Framework\n",
    "\n",
    "**Purpose:** Implement compliance checks for major AI/ML regulations (GDPR, AI Act, SR 11-7).\n",
    "\n",
    "**Key Points:**\n",
    "- **GDPR (EU)**: Right to explanation (Article 22), data protection, consent, privacy\n",
    "- **AI Act (EU)**: Risk classification (high-risk systems require documentation, testing, human oversight)\n",
    "- **SR 11-7 (US Banking)**: Model risk management for financial institutions\n",
    "- **Compliance automation**: Automated checks for regulation-specific requirements\n",
    "\n",
    "**Why This Matters:** Non-compliance results in massive fines (‚Ç¨20M or 4% revenue for GDPR), legal liability, and market access restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b5514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplianceChecker:\n",
    "    \"\"\"\n",
    "    Automated compliance checker for AI/ML regulations.\n",
    "    \n",
    "    Supports:\n",
    "    - GDPR (General Data Protection Regulation) - EU\n",
    "    - AI Act - EU (risk classification, documentation)\n",
    "    - SR 11-7 - US Federal Reserve (banking model risk management)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, regulation='GDPR'):\n",
    "        self.regulation = regulation\n",
    "        self.compliance_checks = []\n",
    "        \n",
    "    def check_gdpr_compliance(self, model_metadata):\n",
    "        \\\"\\\"\\\"\n",
    "        Check GDPR compliance requirements.\n",
    "        \n",
    "        Key requirements:\n",
    "        - Article 22: Right to explanation for automated decisions\n",
    "        - Article 13-14: Transparency (purpose, data used, retention)\n",
    "        - Article 25: Data protection by design and default\n",
    "        \\\"\\\"\\\"\n",
    "        checks = []\n",
    "        \n",
    "        # Check 1: Explainability (Article 22)\n",
    "        has_explainability = model_metadata.get('explainability_method') is not None\n",
    "        checks.append({\n",
    "            'requirement': 'GDPR Article 22: Right to explanation',\n",
    "            'passed': has_explainability,\n",
    "            'details': f\\\"Explainability method: {model_metadata.get('explainability_method', 'NOT PROVIDED')}\\\"\n",
    "        })\n",
    "        \n",
    "        # Check 2: Data documentation (Article 13-14)\n",
    "        has_data_docs = all([\n",
    "            model_metadata.get('data_source'),\n",
    "            model_metadata.get('data_retention_policy'),\n",
    "            model_metadata.get('purpose')\n",
    "        ])\n",
    "        checks.append({\n",
    "            'requirement': 'GDPR Article 13-14: Data transparency',\n",
    "            'passed': has_data_docs,\n",
    "            'details': 'Data source, retention policy, and purpose documented' if has_data_docs else 'Missing data documentation'\n",
    "        })\n",
    "        \n",
    "        # Check 3: Privacy measures (Article 25)\n",
    "        has_privacy = model_metadata.get('privacy_measures') is not None\n",
    "        checks.append({\n",
    "            'requirement': 'GDPR Article 25: Data protection by design',\n",
    "            'passed': has_privacy,\n",
    "            'details': f\\\"Privacy measures: {model_metadata.get('privacy_measures', 'NOT PROVIDED')}\\\"\n",
    "        })\n",
    "        \n",
    "        # Check 4: Consent tracking (if personal data used)\n",
    "        uses_personal_data = model_metadata.get('uses_personal_data', False)\n",
    "        has_consent = model_metadata.get('consent_mechanism') is not None\n",
    "        consent_check_passed = (not uses_personal_data) or (uses_personal_data and has_consent)\n",
    "        \n",
    "        checks.append({\n",
    "            'requirement': 'GDPR: Consent for personal data',\n",
    "            'passed': consent_check_passed,\n",
    "            'details': f\\\"Personal data: {uses_personal_data}, Consent mechanism: {model_metadata.get('consent_mechanism', 'N/A')}\\\"\n",
    "        })\n",
    "        \n",
    "        return checks\n",
    "    \n",
    "    def check_ai_act_compliance(self, model_metadata):\n",
    "        \\\"\\\"\\\"\n",
    "        Check EU AI Act compliance (high-risk AI systems).\n",
    "        \n",
    "        Requirements for high-risk systems:\n",
    "        - Risk management system\n",
    "        - Data governance and quality\n",
    "        - Technical documentation (model card)\n",
    "        - Record-keeping (audit logs)\n",
    "        - Transparency and information to users\n",
    "        - Human oversight\n",
    "        - Accuracy, robustness, cybersecurity\n",
    "        \\\"\\\"\\\"\n",
    "        checks = []\n",
    "        \n",
    "        # Check 1: Risk classification\\nrisk_level = model_metadata.get('risk_level', 'UNASSESSED')\n",
    "        checks.append({\n",
    "            'requirement': 'AI Act: Risk classification',\n",
    "            'passed': risk_level in ['LOW', 'MEDIUM', 'HIGH'],\n",
    "            'details': f\\\"Risk level: {risk_level} (LOW/MEDIUM/HIGH required)\\\"\n",
    "        })\n",
    "        \n",
    "        # Check 2: Technical documentation (for high-risk systems)\n",
    "        is_high_risk = risk_level == 'HIGH'\n",
    "        has_tech_docs = model_metadata.get('model_card') is not None\n",
    "        \n",
    "        if is_high_risk:\n",
    "            checks.append({\n",
    "                'requirement': 'AI Act (High-Risk): Technical documentation',\n",
    "                'passed': has_tech_docs,\n",
    "                'details': 'Model card required for high-risk systems' + (' - PROVIDED' if has_tech_docs else ' - MISSING')\n",
    "            })\n",
    "        \n",
    "        # Check 3: Audit trail (record-keeping)\n",
    "        has_audit_trail = model_metadata.get('audit_trail_enabled', False)\n",
    "        checks.append({\n",
    "            'requirement': 'AI Act: Record-keeping',\n",
    "            'passed': has_audit_trail,\n",
    "            'details': f\\\"Audit trail: {'ENABLED' if has_audit_trail else 'DISABLED'}\\\"\n",
    "        })\n",
    "        \n",
    "        # Check 4: Human oversight (for high-risk systems)\n",
    "        has_human_oversight = model_metadata.get('human_oversight', False)\n",
    "        \n",
    "        if is_high_risk:\n",
    "            checks.append({\n",
    "                'requirement': 'AI Act (High-Risk): Human oversight',\n",
    "                'passed': has_human_oversight,\n",
    "                'details': 'Human-in-the-loop required for high-risk systems' + (' - ENABLED' if has_human_oversight else ' - MISSING')\n",
    "            })\n",
    "        \n",
    "        # Check 5: Accuracy and robustness testing\n",
    "        has_testing = model_metadata.get('testing_framework') is not None\n",
    "        checks.append({\n",
    "            'requirement': 'AI Act: Accuracy and robustness',\n",
    "            'passed': has_testing,\n",
    "            'details': f\\\"Testing framework: {model_metadata.get('testing_framework', 'NOT PROVIDED')}\\\"\n",
    "        })\n",
    "        \n",
    "        return checks\n",
    "    \n",
    "    def check_sr11_7_compliance(self, model_metadata):\n",
    "        \\\"\\\"\\\"\n",
    "        Check US Federal Reserve SR 11-7 compliance (banking model risk management).\n",
    "        \n",
    "        Requirements:\n",
    "        - Model validation (independent review)\n",
    "        - Model documentation\n",
    "        - Ongoing monitoring\n",
    "        - Policies and controls\n",
    "        - Model inventory\n",
    "        \\\"\\\"\\\"\n",
    "        checks = []\n",
    "        \n",
    "        # Check 1: Independent validation\n",
    "        has_validation = model_metadata.get('independent_validation', False)\n",
    "        checks.append({\n",
    "            'requirement': 'SR 11-7: Independent model validation',\n",
    "            'passed': has_validation,\n",
    "            'details': f\\\"Independent validation: {'COMPLETED' if has_validation else 'NOT COMPLETED'}\\\"\n",
    "        })\n",
    "        \n",
    "        # Check 2: Model documentation\n",
    "        has_docs = all([\n",
    "            model_metadata.get('model_card'),\n",
    "            model_metadata.get('assumptions_documented'),\n",
    "            model_metadata.get('limitations_documented')\n",
    "        ])\n",
    "        checks.append({\n",
    "            'requirement': 'SR 11-7: Comprehensive documentation',\n",
    "            'passed': has_docs,\n",
    "            'details': 'Model card, assumptions, and limitations required' + (' - ALL PROVIDED' if has_docs else ' - INCOMPLETE')\n",
    "        })\n",
    "        \n",
    "        # Check 3: Ongoing monitoring\n",
    "        has_monitoring = model_metadata.get('monitoring_enabled', False)\n",
    "        checks.append({\n",
    "            'requirement': 'SR 11-7: Ongoing monitoring',\n",
    "            'passed': has_monitoring,\n",
    "            'details': f\\\"Monitoring: {'ENABLED' if has_monitoring else 'DISABLED'}\\\"\n",
    "        })\n",
    "        \n",
    "        # Check 4: Model inventory\n",
    "        in_inventory = model_metadata.get('registered_in_inventory', False)\n",
    "        checks.append({\n",
    "            'requirement': 'SR 11-7: Model inventory',\n",
    "            'passed': in_inventory,\n",
    "            'details': f\\\"Model inventory: {'REGISTERED' if in_inventory else 'NOT REGISTERED'}\\\"\n",
    "        })\n",
    "        \n",
    "        # Check 5: Governance and controls\n",
    "        has_governance = all([\n",
    "            model_metadata.get('approval_workflow'),\n",
    "            model_metadata.get('change_management'),\n",
    "            model_metadata.get('incident_response_plan')\n",
    "        ])\n",
    "        checks.append({\n",
    "            'requirement': 'SR 11-7: Governance and controls',\n",
    "            'passed': has_governance,\n",
    "            'details': 'Approval, change management, incident response required' + (' - ALL PRESENT' if has_governance else ' - INCOMPLETE')\n",
    "        })\n",
    "        \n",
    "        return checks\n",
    "    \n",
    "    def run_compliance_check(self, model_metadata):\n",
    "        \\\"\\\"\\\"Run compliance check for specified regulation.\\\"\\\"\\\"\n",
    "        if self.regulation == 'GDPR':\n",
    "            checks = self.check_gdpr_compliance(model_metadata)\n",
    "        elif self.regulation == 'AI_ACT':\n",
    "            checks = self.check_ai_act_compliance(model_metadata)\n",
    "        elif self.regulation == 'SR_11_7':\n",
    "            checks = self.check_sr11_7_compliance(model_metadata)\n",
    "        else:\n",
    "            raise ValueError(f\\\"Unknown regulation: {self.regulation}\\\")\\n        \\n        passed_count = sum(1 for check in checks if check['passed'])\n",
    "        total_count = len(checks)\n",
    "        \\n        result = {\n",
    "            'regulation': self.regulation,\n",
    "            'checks': checks,\n",
    "            'summary': {\n",
    "                'passed': passed_count,\n",
    "                'total': total_count,\n",
    "                'compliance_rate': passed_count / total_count if total_count > 0 else 0,\n",
    "                'status': 'COMPLIANT' if passed_count == total_count else 'NON-COMPLIANT'\n",
    "            }\n",
    "        }\n",
    "        \\n        return result\n",
    "\n",
    "# Example: Compliance check for yield prediction model\n",
    "print(\\\"üîí Regulatory Compliance Check: Yield Prediction Model\\\\n\\\")\n",
    "print(\\\"=\\\"*80)\n",
    "\n",
    "# Model metadata\n",
    "model_metadata = {\n",
    "    'model_name': 'Wafer Yield Predictor',\n",
    "    'model_version': '2.1.0',\n",
    "    'purpose': 'Predict wafer test yield for fab capacity planning',\n",
    "    'data_source': 'STDF wafer test data (production)',\n",
    "    'data_retention_policy': '2 years (per company policy)',\n",
    "    'uses_personal_data': False,  # No PII in wafer test data\n",
    "    'privacy_measures': 'Wafer IDs anonymized, no employee data',\n",
    "    'explainability_method': 'SHAP values for feature importance',\n",
    "    'model_card': True,\n",
    "    'audit_trail_enabled': True,\n",
    "    'risk_level': 'MEDIUM',  # Not safety-critical, but business-critical\n",
    "    'human_oversight': True,  # Engineers review predictions before capacity decisions\n",
    "    'testing_framework': 'pytest with property-based testing (Hypothesis)',\n",
    "    'independent_validation': True,\n",
    "    'assumptions_documented': True,\n",
    "    'limitations_documented': True,\n",
    "    'monitoring_enabled': True,\n",
    "    'registered_in_inventory': True,\n",
    "    'approval_workflow': True,\n",
    "    'change_management': True,\n",
    "    'incident_response_plan': True\n",
    "}\n",
    "\n",
    "# Check GDPR compliance\n",
    "print(\\\"\\\\n1Ô∏è‚É£ GDPR COMPLIANCE (EU Data Protection)\\\\n\\\")\n",
    "gdpr_checker = ComplianceChecker(regulation='GDPR')\n",
    "gdpr_result = gdpr_checker.run_compliance_check(model_metadata)\n",
    "\n",
    "for i, check in enumerate(gdpr_result['checks'], 1):\n",
    "    status = '‚úÖ' if check['passed'] else '‚ùå'\n",
    "    print(f\\\"   {status} {check['requirement']}\\\")\n",
    "    print(f\\\"      {check['details']}\\\")\n",
    "\n",
    "print(f\\\"\\\\n   Summary: {gdpr_result['summary']['passed']}/{gdpr_result['summary']['total']} checks passed\\\")\n",
    "print(f\\\"   Status: {gdpr_result['summary']['status']}\\\\n\\\")\n",
    "\n",
    "# Check AI Act compliance\n",
    "print(\\\"\\\\n2Ô∏è‚É£ EU AI ACT COMPLIANCE (High-Risk AI Systems)\\\\n\\\")\n",
    "ai_act_checker = ComplianceChecker(regulation='AI_ACT')\n",
    "ai_act_result = ai_act_checker.run_compliance_check(model_metadata)\n",
    "\n",
    "for i, check in enumerate(ai_act_result['checks'], 1):\n",
    "    status = '‚úÖ' if check['passed'] else '‚ùå'\n",
    "    print(f\\\"   {status} {check['requirement']}\\\")\n",
    "    print(f\\\"      {check['details']}\\\")\n",
    "\n",
    "print(f\\\"\\\\n   Summary: {ai_act_result['summary']['passed']}/{ai_act_result['summary']['total']} checks passed\\\")\n",
    "print(f\\\"   Status: {ai_act_result['summary']['status']}\\\\n\\\")\n",
    "\n",
    "# Check SR 11-7 compliance (banking)\n",
    "print(\\\"\\\\n3Ô∏è‚É£ SR 11-7 COMPLIANCE (US Banking Model Risk Management)\\\\n\\\")\n",
    "sr11_7_checker = ComplianceChecker(regulation='SR_11_7')\n",
    "sr11_7_result = sr11_7_checker.run_compliance_check(model_metadata)\n",
    "\n",
    "for i, check in enumerate(sr11_7_result['checks'], 1):\n",
    "    status = '‚úÖ' if check['passed'] else '‚ùå'\n",
    "    print(f\\\"   {status} {check['requirement']}\\\")\n",
    "    print(f\\\"      {check['details']}\\\")\n",
    "\n",
    "print(f\\\"\\\\n   Summary: {sr11_7_result['summary']['passed']}/{sr11_7_result['summary']['total']} checks passed\\\")\n",
    "print(f\\\"   Status: {sr11_7_result['summary']['status']}\\\\n\\\")\n",
    "\n",
    "# Overall compliance summary\n",
    "print(\\\"=\\\"*80)\n",
    "print(\\\"\\\\nüìä OVERALL COMPLIANCE STATUS\\\\n\\\")\n",
    "print(f\\\"   GDPR: {gdpr_result['summary']['status']} ({gdpr_result['summary']['compliance_rate']:.0%})\\\")\\nprint(f\\\"   AI Act: {ai_act_result['summary']['status']} ({ai_act_result['summary']['compliance_rate']:.0%})\\\")\\nprint(f\\\"   SR 11-7: {sr11_7_result['summary']['status']} ({sr11_7_result['summary']['compliance_rate']:.0%})\\\")\\n\\nall_compliant = all([\\n    gdpr_result['summary']['status'] == 'COMPLIANT',\\n    ai_act_result['summary']['status'] == 'COMPLIANT',\\n    sr11_7_result['summary']['status'] == 'COMPLIANT'\\n])\\n\\nif all_compliant:\\n    print(\\\"\\\\n‚úÖ Model is COMPLIANT with all major regulations\\\")\\n    print(\\\"   Ready for deployment in regulated environments\\\")\\nelse:\\n    print(\\\"\\\\n‚ö†Ô∏è  COMPLIANCE ISSUES DETECTED\\\")\\n    print(\\\"   Address non-compliant items before production deployment\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ec9d53",
   "metadata": {},
   "source": [
    "## 6. Real-World Project Templates\n",
    "\n",
    "**Purpose:** 8 production-ready governance projects (4 post-silicon validation + 4 general AI/ML).\n",
    "\n",
    "**Pattern:** Each project includes governance requirements, compliance obligations, and success criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = {\n",
    "    \"post_silicon\": [\n",
    "        {\n",
    "            \"name\": \"Yield Prediction Model Governance System\",\n",
    "            \"objective\": \"Complete governance framework for wafer yield prediction (used for $500M+ capacity decisions)\",\n",
    "            \"governance_requirements\": [\n",
    "                \"Model card with intended use, limitations, performance metrics (overall + per-fab)\",\n",
    "                \"Fairness evaluation: Accuracy variance across 3 fabs must be <3%\",\n",
    "                \"Audit trail: Track all training, validation, approval, deployment events\",\n",
    "                \"Approval workflow: Director sign-off required before production deployment\",\n",
    "                \"Quarterly governance review: Fairness re-evaluation, model card updates\"\n",
    "            ],\n",
    "            \"compliance_obligations\": [\n",
    "                \"GDPR: Right to explanation (SHAP values for predictions impacting $1M+ decisions)\",\n",
    "                \"Internal policy: Model versioning, change management, incident response\",\n",
    "                \"ISO 9001: Quality management documentation for semiconductor manufacturing\"\n",
    "            ],\n",
    "            \"documentation\": [\n",
    "                \"Model card (JSON + Markdown + HTML)\",\n",
    "                \"Fairness report (quarterly)\",\n",
    "                \"Compliance checklist (GDPR, internal policies)\",\n",
    "                \"Audit trail export (CSV for annual audit)\",\n",
    "                \"Model lineage diagram (training data ‚Üí features ‚Üí model ‚Üí deployment)\"\n",
    "            ],\n",
    "            \"success_criteria\": \"100% compliance with governance requirements, zero audit findings, <24 hour response to governance queries\",\n",
    "            \"value\": \"Enable responsible AI adoption, pass audits, maintain stakeholder trust ($500M decisions)\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Binning Model Fairness & Explainability System\",\n",
    "            \"objective\": \"Ensure fair and explainable binning across device types (prevent $50M revenue loss from biased binning)\",\n",
    "            \"governance_requirements\": [\n",
    "                \"Fairness metrics: Demographic parity, equalized odds, accuracy parity across 5 device types\",\n",
    "                \"Explainability: SHAP values showing why device binned (Vdd margin, Fmax, leakage)\",\n",
    "                \"Validation gates: Reject model if accuracy variance >5% across device types\",\n",
    "                \"Bias mitigation: Stratified sampling, fairness-aware reweighting if bias detected\",\n",
    "                \"Monthly fairness monitoring: Detect fairness degradation in production\"\n",
    "            ],\n",
    "            \"compliance_obligations\": [\n",
    "                \"Customer contracts: Binning accuracy guarantees (97%+ for premium bins)\",\n",
    "                \"Internal policy: No systematic bias against specific device families\",\n",
    "                \"Transparency: Explain binning decisions to product engineers\"\n",
    "            ],\n",
    "            \"explainability_methods\": [\n",
    "                \"SHAP values: Feature importance for each binning decision\",\n",
    "                \"Decision tree surrogate: Approximate RF with interpretable tree\",\n",
    "                \"Counterfactual explanations: 'Device would bin higher if Vdd_margin increased by 20mV'\",\n",
    "                \"Feature contribution plots: Visual breakdown of binning factors\"\n",
    "            ],\n",
    "            \"success_criteria\": \"Zero fairness violations for 6 months, 100% binning decisions explainable, 97%+ accuracy across all device types\",\n",
    "            \"value\": \"Prevent revenue loss from unfair binning ($50M), enable engineer trust, satisfy customer requirements\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Test Time Prediction Governance for SLA Compliance\",\n",
    "            \"objective\": \"Document and monitor test time prediction model used for customer SLA commitments (prevent $1M+ penalties)\",\n",
    "            \"governance_requirements\": [\n",
    "                \"Model card: Performance metrics (MAPE, prediction intervals), limitations (accuracy degrades for new test programs)\",\n",
    "                \"Confidence intervals: Provide 95% prediction intervals for all test time estimates\",\n",
    "                \"Uncertainty quantification: Flag predictions with high uncertainty (>20% interval width)\",\n",
    "                \"Monitoring: Alert if MAPE exceeds 5% (SLA threshold)\",\n",
    "                \"Incident response: Escalate to operations team if SLA risk detected\"\n",
    "            ],\n",
    "            \"compliance_obligations\": [\n",
    "                \"Customer SLAs: Deliver devices within predicted timeline ¬±10%\",\n",
    "                \"Internal policy: Document assumptions (test parallelization, tester availability)\",\n",
    "                \"Change management: Retrain model when test programs change (track trigger events)\"\n",
    "            ],\n",
    "            \"documentation\": [\n",
    "                \"Model card with SLA implications clearly stated\",\n",
    "                \"Prediction uncertainty report (daily)\",\n",
    "                \"SLA compliance dashboard (real-time)\",\n",
    "                \"Model assumptions document (updated when test programs change)\"\n",
    "            ],\n",
    "            \"success_criteria\": \"Zero SLA violations attributable to model errors, 100% predictions include confidence intervals, <5% MAPE maintained\",\n",
    "            \"value\": \"Avoid SLA penalties ($1M/year), maintain customer relationships, enable confident delivery commitments\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Wafer Map Anomaly Detection Governance (Safety-Critical)\",\n",
    "            \"objective\": \"Govern anomaly detection model that triggers production line halts (multi-million dollar impact per halt)\",\n",
    "            \"governance_requirements\": [\n",
    "                \"Risk classification: HIGH (safety-critical system impacting production)\",\n",
    "                \"Human oversight: Require fab manager approval before automated line halt (human-in-the-loop)\",\n",
    "                \"Explainability: Show spatial patterns that triggered anomaly (wafer map heatmap, die clustering)\",\n",
    "                \"Validation: Precision >= 85% (limit false positives), recall >= 75% (catch real anomalies)\",\n",
    "                \"Audit trail: Log every anomaly detection with model version, input wafer map, prediction, confidence\",\n",
    "                \"Quarterly review: Independent validation by quality engineers\"\n",
    "            ],\n",
    "            \"compliance_obligations\": [\n",
    "                \"AI Act (EU): High-risk system requires documentation, testing, human oversight, accuracy guarantees\",\n",
    "                \"Internal policy: Root cause analysis for all false positive halts (prevent $500K/hour downtime)\",\n",
    "                \"Quality standards: ISO 9001 documentation for quality control processes\"\n",
    "            ],\n",
    "            \"safety_controls\": [\n",
    "                \"Two-stage approval: Model flags anomaly ‚Üí engineer reviews wafer map ‚Üí manager approves halt\",\n",
    "                \"Confidence thresholds: Only auto-alert if confidence >90%, manual review if 70-90%\",\n",
    "                \"Override mechanism: Engineers can override model decision (with justification logged)\",\n",
    "                \"Redundancy: Combine model with physics-based checks (kill-rate analysis)\"\n",
    "            ],\n",
    "            \"success_criteria\": \"Zero false positive halts, 100% anomalies reviewed by humans before action, full audit trail for all decisions\",\n",
    "            \"value\": \"Prevent costly false halts ($500K/hour), catch real defects early (save $1M/year), satisfy regulatory requirements\"\n",
    "        }\n",
    "    ],\n",
    "    \"general_ml\": [\n",
    "        {\n",
    "            \"name\": \"Credit Scoring Model Governance (GDPR + Fair Lending)\",\n",
    "            \"objective\": \"Govern credit scoring model to ensure fairness, transparency, and regulatory compliance (prevent $50M+ fines)\",\n",
    "            \"governance_requirements\": [\n",
    "                \"Fairness: Equalized odds across protected groups (race, gender, age) - <5% TPR/FPR difference\",\n",
    "                \"Explainability: Right to explanation for all credit denials (GDPR Article 22)\",\n",
    "                \"Model card: Document training data, features (no prohibited features), limitations\",\n",
    "                \"Audit trail: Log all credit decisions with model version, inputs, outputs, explanations\",\n",
    "                \"Quarterly fairness audits: Independent review by compliance team\"\n",
    "            ],\n",
    "            \"compliance_obligations\": [\n",
    "                \"GDPR: Right to explanation, data protection, consent\",\n",
    "                \"Equal Credit Opportunity Act (US): No discrimination based on protected characteristics\",\n",
    "                \"Fair Lending laws: Demonstrate disparate impact ratio >= 0.8 (80% rule)\",\n",
    "                \"SR 11-7 (if bank): Model validation, documentation, ongoing monitoring\"\n",
    "            ],\n",
    "            \"explainability_methods\": [\n",
    "                \"LIME: Local explanations for each credit decision\",\n",
    "                \"Counterfactuals: 'Applicant would be approved if income increased by $10K'\",\n",
    "                \"Feature importance: Global understanding of model (income, debt-to-income ratio most important)\",\n",
    "                \"Adverse action reasons: Top 3 reasons for denial (compliant with regulations)\"\n",
    "            ],\n",
    "            \"success_criteria\": \"Zero fairness violations, 100% denials explainable, pass regulatory audits, zero GDPR complaints\",\n",
    "            \"value\": \"Avoid regulatory fines ($50M+ for GDPR), prevent discrimination lawsuits, maintain banking license\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Healthcare Diagnosis AI Governance (High-Risk, Life-Critical)\",\n",
    "            \"objective\": \"Govern medical diagnosis model (cancer detection) with highest safety and compliance standards\",\n",
    "            \"governance_requirements\": [\n",
    "                \"Risk classification: CRITICAL (impacts patient health outcomes)\",\n",
    "                \"Clinical validation: Independent validation by medical professionals (physicians, radiologists)\",\n",
    "                \"Explainability: Heatmaps showing image regions that indicate cancer (Grad-CAM, SHAP)\",\n",
    "                \"Human oversight: Physician final decision (model is decision support, not autonomous)\",\n",
    "                \"Fairness: Equal sensitivity/specificity across demographics (race, age, gender)\",\n",
    "                \"Audit trail: Log all diagnoses with patient ID, model version, confidence, physician decision\",\n",
    "                \"Annual re-validation: Performance monitoring, fairness audits, clinical accuracy checks\"\n",
    "            ],\n",
    "            \"compliance_obligations\": [\n",
    "                \"FDA approval (US): Premarket approval for medical devices (510(k) or PMA)\",\n",
    "                \"EU MDR: Medical device regulation (CE marking requirements)\",\n",
    "                \"HIPAA: Patient data privacy and security\",\n",
    "                \"AI Act (EU): High-risk AI system (healthcare) - full documentation, testing, oversight\"\n",
    "            ],\n",
    "            \"safety_controls\": [\n",
    "                \"Sensitivity >= 95% (minimize false negatives - critical for cancer)\",\n",
    "                \"Physician override: 100% of predictions reviewed by human expert\",\n",
    "                \"Uncertainty quantification: Flag low-confidence predictions for additional review\",\n",
    "                \"Redundancy: Combine AI with standard diagnostic protocols (biopsy, second opinion)\"\n",
    "            ],\n",
    "            \"success_criteria\": \"FDA/EU approval obtained, 95%+ sensitivity maintained, zero patient harm incidents, 100% physician oversight\",\n",
    "            \"value\": \"Save lives (early cancer detection), pass regulatory approval, enable clinical adoption, avoid malpractice lawsuits\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Hiring AI Governance (Fairness-Critical, Legal Risk)\",\n",
    "            \"objective\": \"Govern resume screening AI to ensure fair hiring, prevent discrimination lawsuits ($millions in damages)\",\n",
    "            \"governance_requirements\": [\n",
    "                \"Fairness: Demographic parity across protected groups (race, gender, age) - <10% difference\",\n",
    "                \"Prohibited features: No race, gender, age, zip code (proxy for race) in model\",\n",
    "                \"Explainability: Hiring managers understand why candidates ranked (skills, experience)\",\n",
    "                \"Audit trail: Log all screening decisions (candidate ID, model score, hiring decision)\",\n",
    "                \"Quarterly fairness audits: Test for disparate impact (80% rule)\",\n",
    "                \"Human-in-the-loop: Model ranks candidates, humans make final decision (no autonomous hiring)\"\n",
    "            ],\n",
    "            \"compliance_obligations\": [\n",
    "                \"Equal Employment Opportunity Act (US): No discrimination in hiring\",\n",
    "                \"GDPR (EU): Right to explanation, data protection for applicant data\",\n",
    "                \"NYC Bias Audit Law: Annual bias audit for AI hiring tools (New York City)\",\n",
    "                \"AI Act (EU): High-risk AI system (employment) - documentation, fairness, human oversight\"\n",
    "            ],\n",
    "            \"bias_mitigation\": [\n",
    "                \"Adversarial debiasing: Train model to be invariant to protected attributes\",\n",
    "                \"Fairness constraints: Optimize for accuracy subject to fairness constraints (equalized odds)\",\n",
    "                \"Blind screening: Remove names, photos, universities (proxies for race/gender)\",\n",
    "                \"Diverse training data: Ensure training set represents diverse candidates\"\n",
    "            ],\n",
    "            \"success_criteria\": \"Pass 80% rule for all protected groups, zero discrimination lawsuits, NYC bias audit compliance, 100% hiring decisions by humans\",\n",
    "            \"value\": \"Avoid discrimination lawsuits ($millions), attract diverse talent, comply with regulations, maintain employer brand\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Autonomous Vehicle Decision Model Governance (Safety-Critical)\",\n",
    "            \"objective\": \"Govern AV decision model (lane change, braking) with highest safety standards (prevent accidents, fatalities)\",\n",
    "            \"governance_requirements\": [\n",
    "                \"Risk classification: CRITICAL (safety-critical, life-or-death decisions)\",\n",
    "                \"Safety validation: Billions of simulated miles, real-world testing (track + public roads)\",\n",
    "                \"Explainability: Understand why AV made decision (sensor inputs, predicted trajectories)\",\n",
    "                \"Redundancy: Multiple independent models, sensor fusion, fallback to safe stop\",\n",
    "                \"Audit trail: Log all decisions (sensor data, predictions, actions) - black box recorder\",\n",
    "                \"Continuous monitoring: Fleet-wide performance tracking, incident analysis\",\n",
    "                \"Regulatory approval: NHTSA (US), UNECE (EU) self-driving vehicle standards\"\n",
    "            ],\n",
    "            \"compliance_obligations\": [\n",
    "                \"NHTSA safety standards: Crashworthiness, occupant protection, crash avoidance\",\n",
    "                \"UNECE regulations: Automated Lane Keeping Systems (ALKS), cybersecurity\",\n",
    "                \"State laws (US): Vary by state (California, Arizona, etc.) - registration, testing permits\",\n",
    "                \"AI Act (EU): High-risk AI system (safety component of vehicle) - full compliance\"\n",
    "            ],\n",
    "            \"safety_controls\": [\n",
    "                \"Fail-safe: If model uncertain or fails, execute safe stop (slow down, pull over)\",\n",
    "                \"Human takeover: Driver can override at any time (monitor driver attention)\",\n",
    "                \"Redundant sensors: LiDAR, camera, radar (sensor fusion for robustness)\",\n",
    "                \"Conservative decision-making: Prioritize safety over efficiency (wider margins)\",\n",
    "                \"Incident reporting: Report all accidents, near-misses to regulators (transparency)\"\n",
    "            ],\n",
    "            \"success_criteria\": \"Zero at-fault accidents, regulatory approval obtained, 99.99%+ uptime, <0.01% unsafe decisions\",\n",
    "            \"value\": \"Save lives (reduce 90% of traffic accidents), obtain regulatory approval ($billions market), avoid liability lawsuits\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\\\"üéØ 8 Model Governance Project Templates\\\")\n",
    "print(\\\"=\\\"*80)\n",
    "\n",
    "print(\\\"\\\\nüì¶ POST-SILICON VALIDATION PROJECTS (4)\\\\n\\\")\n",
    "for i, project in enumerate(projects[\\\"post_silicon\\\"], 1):\n",
    "    print(f\\\"{i}. {project['name']}\\\")\n",
    "    print(f\\\"   Objective: {project['objective']}\\\")\n",
    "    print(f\\\"   Governance: {len(project['governance_requirements'])} requirements\\\")\n",
    "    print(f\\\"   Compliance: {len(project['compliance_obligations'])} obligations\\\")\n",
    "    print(f\\\"   Success: {project['success_criteria']}\\\")\n",
    "    print(f\\\"   üí∞ Value: {project['value']}\\\")\n",
    "    print()\n",
    "\n",
    "print(\\\"\\\\nüåê GENERAL AI/ML PROJECTS (4)\\\\n\\\")\n",
    "for i, project in enumerate(projects[\\\"general_ml\\\"], 1):\n",
    "    print(f\\\"{i}. {project['name']}\\\")\n",
    "    print(f\\\"   Objective: {project['objective']}\\\")\n",
    "    print(f\\\"   Governance: {len(project['governance_requirements'])} requirements\\\")\n",
    "    print(f\\\"   Compliance: {len(project['compliance_obligations'])} obligations\\\")\n",
    "    print(f\\\"   Success: {project['success_criteria']}\\\")\n",
    "    print(f\\\"   üí∞ Value: {project['value']}\\\")\n",
    "    print()\n",
    "\n",
    "print(\\\"=\\\"*80)\n",
    "print(\\\"‚úÖ All projects include: Model cards, fairness evaluation, audit trails, compliance checks\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bdea31",
   "metadata": {},
   "source": [
    "## 7. üéì Key Takeaways & Best Practices\n",
    "\n",
    "### üìå Core Concepts\n",
    "\n",
    "**1. Model Governance Fundamentals**\n",
    "- **Definition**: Framework of policies, procedures, and controls ensuring responsible ML model development and deployment\n",
    "- **Components**: Documentation (model cards), fairness (bias detection), transparency (explainability), compliance (regulations), accountability (audit trails)\n",
    "- **Purpose**: Ensure models are trustworthy, fair, compliant, and beneficial\n",
    "- **Governance vs MLOps**: MLOps = HOW to deploy (technical), Governance = WHAT models must satisfy (policies)\n",
    "\n",
    "**Without governance**: Models cause harm (discrimination, privacy violations), regulatory fines ($50M+ GDPR), reputational damage  \n",
    "**With governance**: Responsible AI adoption, stakeholder trust, regulatory compliance, reduced legal risk\n",
    "\n",
    "**2. Why Governance Matters**\n",
    "- **Regulatory landscape**: GDPR (EU data protection), AI Act (EU high-risk systems), SR 11-7 (US banking), CCPA (California privacy)\n",
    "- **Financial risk**: GDPR fines up to ‚Ç¨20M or 4% revenue (whichever higher), discrimination lawsuits ($millions)\n",
    "- **Reputational risk**: Biased models damage brand (Amazon hiring AI, facial recognition bias)\n",
    "- **Ethical obligation**: ML impacts lives (credit, healthcare, hiring) - must be fair and explainable\n",
    "- **Market access**: EU AI Act restricts high-risk AI without compliance (lose market access)\n",
    "\n",
    "---\n",
    "\n",
    "### üìÑ Model Cards\n",
    "\n",
    "**3. Model Card Structure**\n",
    "- **Model details**: Name, version, type, developer, framework, training date\n",
    "- **Intended use**: Primary uses, out-of-scope uses, primary users\n",
    "- **Metrics**: Overall performance + per-group metrics (fairness)\n",
    "- **Training data**: Source, size, time period, features\n",
    "- **Evaluation data**: Test set characteristics\n",
    "- **Ethical considerations**: Fairness assessment, bias mitigation, privacy measures\n",
    "- **Caveats & recommendations**: Limitations, usage recommendations, monitoring requirements\n",
    "\n",
    "**4. Model Card Best Practices**\n",
    "- **Living document**: Update with each model version, retraining, or performance change\n",
    "- **Multiple formats**: JSON (machine-readable), Markdown (human-readable), HTML (web display)\n",
    "- **Audience-specific**: Technical (data scientists), business (executives), compliance (auditors, regulators)\n",
    "- **Versioning**: Track model card versions alongside model versions (lineage)\n",
    "- **Accessibility**: Store in model registry, share with stakeholders, publish for transparency\n",
    "\n",
    "**Example use**:\n",
    "```python\n",
    "card = ModelCard(\\\"Credit Scorer\\\", \\\"1.0\\\", \\\"Approve/deny credit applications\\\")\n",
    "card.add_metrics(overall={'accuracy': 0.85}, per_group={'Group A': 0.84, 'Group B': 0.86})\n",
    "card.export_json('model_card_v1.json')  # For compliance reports\n",
    "markdown = card.generate_markdown()  # For human review\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è Fairness & Bias\n",
    "\n",
    "**5. Fairness Metrics**\n",
    "- **Demographic parity**: Positive rate similar across groups (P(pred=1 | group=A) ‚âà P(pred=1 | group=B))\n",
    "- **Equalized odds**: TPR and FPR equal across groups (equal true positive and false positive rates)\n",
    "- **Disparate impact**: Ratio of positive rates >= 0.8 (80% rule from US employment law)\n",
    "- **Accuracy parity**: Accuracy similar across groups (<5% variance typically acceptable)\n",
    "- **Calibration**: Predicted probabilities match observed frequencies per group\n",
    "\n",
    "**6. When to Use Which Metric**\n",
    "- **Demographic parity**: Use when outcome rates should be similar (hiring, lending approval rates)\n",
    "- **Equalized odds**: Use when error rates matter equally (healthcare diagnosis - minimize both false positives and false negatives)\n",
    "- **Disparate impact**: Use for legal compliance (hiring, lending - 80% rule)\n",
    "- **Accuracy parity**: Use when performance should be consistent (product quality, yield prediction across fabs)\n",
    "- **Multiple metrics**: Check ALL metrics (model can pass one, fail another)\n",
    "\n",
    "**7. Bias Mitigation Strategies**\n",
    "- **Pre-processing**: Reweight training data, resample to balance groups, remove biased features\n",
    "- **In-processing**: Fairness-aware algorithms (constrained optimization), adversarial debiasing\n",
    "- **Post-processing**: Adjust decision thresholds per group (equalize error rates)\n",
    "- **Feature engineering**: Remove proxies (zip code ‚Üí race, name ‚Üí gender)\n",
    "- **Diverse data**: Ensure training set represents all groups (no underrepresentation)\n",
    "\n",
    "**Tradeoff**: Fairness vs accuracy (often 2-5% accuracy drop to achieve fairness) - business decision based on ethics/regulations\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Audit Trails & Lineage\n",
    "\n",
    "**8. What to Log**\n",
    "- **Training events**: Data source, features, hyperparameters, metrics, training time, trained by\n",
    "- **Validation events**: Validation results, gates passed/failed, validated by\n",
    "- **Fairness checks**: Fairness metrics, pass/fail, checked by\n",
    "- **Approval events**: Approved by, approval criteria, comments, timestamp\n",
    "- **Deployment events**: Environment, deployed by, timestamp\n",
    "- **Inference events**: (For high-stakes predictions) Input, output, confidence, model version\n",
    "- **Incidents**: Type (performance degradation, fairness violation), description, severity, reported by\n",
    "\n",
    "**9. Audit Trail Best Practices**\n",
    "- **Immutable logs**: Append-only (cannot modify past events), cryptographic hashing for tamper-evidence\n",
    "- **Structured format**: JSON for machine-readability, queryable database (SQL)\n",
    "- **Retention**: Keep logs for regulatory period (7 years for SR 11-7, indefinitely for litigation risk)\n",
    "- **Access control**: Only authorized users can view audit logs (privacy, security)\n",
    "- **Automated alerts**: Flag suspicious events (unauthorized deployment, fairness violations)\n",
    "\n",
    "**10. Model Lineage**\n",
    "- **Data lineage**: Training data ‚Üí preprocessing ‚Üí features ‚Üí training dataset\n",
    "- **Code lineage**: Git commit, dependencies (requirements.txt), environment (Docker image)\n",
    "- **Model lineage**: Algorithm, hyperparameters, training run ID ‚Üí model artifact ‚Üí deployment\n",
    "- **Complete lineage**: End-to-end trace from raw data to production predictions\n",
    "\n",
    "**Value**: Enable reproducibility, debugging, compliance (prove model was trained on approved data)\n",
    "\n",
    "---\n",
    "\n",
    "### üîí Regulatory Compliance\n",
    "\n",
    "**11. GDPR (EU General Data Protection Regulation)**\n",
    "- **Article 22**: Right to explanation for automated decisions (must provide human-understandable explanation)\n",
    "- **Article 13-14**: Transparency obligations (inform data subjects about model purpose, data used)\n",
    "- **Article 25**: Data protection by design and default (privacy measures from start)\n",
    "- **Penalties**: Up to ‚Ç¨20M or 4% global revenue (whichever higher)\n",
    "- **Applicability**: Any organization processing EU citizens' data (even if not in EU)\n",
    "\n",
    "**GDPR compliance checklist**:\n",
    "- [ ] Explainability method implemented (SHAP, LIME)\n",
    "- [ ] Data documentation (source, purpose, retention policy)\n",
    "- [ ] Privacy measures (anonymization, encryption)\n",
    "- [ ] Consent mechanism (if using personal data)\n",
    "- [ ] Data subject rights (access, deletion, portability)\n",
    "\n",
    "**12. EU AI Act (2024)**\n",
    "- **Risk classification**: Unacceptable (banned), High-risk (strict requirements), Limited-risk (transparency), Minimal-risk (no requirements)\n",
    "- **High-risk systems**: Employment, credit scoring, healthcare, critical infrastructure, law enforcement\n",
    "- **Requirements for high-risk**: Technical documentation, audit trails, human oversight, accuracy/robustness testing, transparency\n",
    "- **Penalties**: Up to ‚Ç¨30M or 6% global revenue\n",
    "- **Timeline**: Phased implementation 2024-2027\n",
    "\n",
    "**High-risk AI checklist**:\n",
    "- [ ] Risk assessment documented\n",
    "- [ ] Model card (technical documentation)\n",
    "- [ ] Audit trail enabled\n",
    "- [ ] Human oversight (human-in-the-loop)\n",
    "- [ ] Accuracy and robustness testing\n",
    "- [ ] Conformity assessment (third-party audit for some systems)\n",
    "\n",
    "**13. SR 11-7 (US Federal Reserve - Banking Model Risk Management)**\n",
    "- **Scope**: All models used by banks for business decisions (credit, risk, trading)\n",
    "- **Requirements**: Model validation (independent review), documentation, ongoing monitoring, governance framework\n",
    "- **Model validation**: Test model performance, assumptions, limitations (by independent team)\n",
    "- **Ongoing monitoring**: Track production performance, retrain when needed, annual review\n",
    "- **Applicability**: US banks, financial institutions (Dodd-Frank requirements)\n",
    "\n",
    "**SR 11-7 checklist**:\n",
    "- [ ] Independent model validation completed\n",
    "- [ ] Comprehensive documentation (model card, assumptions, limitations)\n",
    "- [ ] Ongoing monitoring enabled\n",
    "- [ ] Model registered in inventory\n",
    "- [ ] Governance policies (approval workflow, change management, incident response)\n",
    "\n",
    "**14. Other Regulations**\n",
    "- **CCPA (California)**: Privacy rights similar to GDPR (for California residents)\n",
    "- **Equal Credit Opportunity Act (US)**: No discrimination in lending (protected characteristics)\n",
    "- **Fair Lending Laws (US)**: Disparate impact analysis (80% rule)\n",
    "- **HIPAA (US Healthcare)**: Patient data privacy and security\n",
    "- **FDA (US Medical Devices)**: Premarket approval for medical AI (510(k) or PMA)\n",
    "- **NYC Bias Audit Law**: Annual bias audit for AI hiring tools (NYC only, expanding)\n",
    "\n",
    "---\n",
    "\n",
    "### üè≠ Post-Silicon Validation Applications\n",
    "\n",
    "**15. Yield Prediction Governance**\n",
    "- **Challenge**: Model impacts $500M+ capacity decisions (high-stakes)\n",
    "- **Requirements**: Model card, fairness across fabs (<3% accuracy variance), audit trail, approval workflow\n",
    "- **Compliance**: Internal policies (quality management, change control), potentially ISO 9001\n",
    "- **Value**: Enable responsible AI adoption, pass internal audits, maintain executive trust\n",
    "\n",
    "**16. Binning Model Fairness**\n",
    "- **Challenge**: Biased binning loses $50M revenue (systematic underpricing of specific device types)\n",
    "- **Requirements**: Fairness metrics (demographic parity, equalized odds across device types), explainability (SHAP)\n",
    "- **Mitigation**: Stratified sampling, fairness constraints, monthly fairness monitoring\n",
    "- **Value**: Prevent revenue loss, enable engineer trust, satisfy customer requirements\n",
    "\n",
    "**17. Test Time Prediction for SLAs**\n",
    "- **Challenge**: Inaccurate predictions cause $1M+ SLA penalties\n",
    "- **Requirements**: Model card with confidence intervals, uncertainty quantification, SLA risk alerting\n",
    "- **Documentation**: Assumptions (test parallelization), limitations (new test programs), monitoring (MAPE)\n",
    "- **Value**: Avoid SLA penalties, maintain customer relationships, enable confident commitments\n",
    "\n",
    "**18. Safety-Critical Wafer Map Anomaly Detection**\n",
    "- **Challenge**: False positive halts cost $500K/hour (production downtime)\n",
    "- **Requirements**: Human oversight (fab manager approval), explainability (spatial patterns), high precision (85%+)\n",
    "- **Compliance**: AI Act high-risk classification (safety-critical), ISO 9001 quality documentation\n",
    "- **Value**: Prevent costly false halts, catch real defects early, satisfy regulatory requirements\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "**19. Documentation as Afterthought**\n",
    "- **Problem**: Create model card after deployment (outdated, incomplete)\n",
    "- **Solution**: Generate model card DURING development (update with each experiment)\n",
    "- **Tooling**: Automate model card generation from MLflow tracking, experiment metadata\n",
    "\n",
    "**20. Ignoring Fairness Until Production**\n",
    "- **Problem**: Discover bias after deployment (costly to fix, reputational damage)\n",
    "- **Solution**: Fairness evaluation in development (before deployment), continuous fairness monitoring\n",
    "- **Example**: Amazon hiring AI discontinued after discovering gender bias (trained on historical male-dominated data)\n",
    "\n",
    "**21. Audit Trails Without Context**\n",
    "- **Problem**: Log events without WHY (hard to understand decisions during audit)\n",
    "- **Solution**: Log context (trigger reason, approval rationale, incident root cause)\n",
    "- **Example**: \\\"Model retrained\\\" (useless) vs \\\"Model retrained due to data drift in Vdd distribution (KS test p=0.02)\\\" (useful)\n",
    "\n",
    "**22. Compliance Checkbox Exercise**\n",
    "- **Problem**: Treat governance as bureaucracy (check boxes without understanding)\n",
    "- **Solution**: Understand WHY regulations exist (protect users, ensure fairness), embed in culture\n",
    "- **Mindset**: Governance enables responsible AI (not just compliance burden)\n",
    "\n",
    "**23. One-Size-Fits-All Governance**\n",
    "- **Problem**: Same governance for low-risk recommendation (movie suggestions) and high-risk lending (credit approval)\n",
    "- **Solution**: Risk-based governance (more rigor for high-risk systems)\n",
    "- **Tiers**: Minimal (internal analytics), Standard (customer-facing), High (safety/financial/legal critical)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Best Practices\n",
    "\n",
    "**24. Risk-Based Governance**\n",
    "- **Risk classification**: Categorize models by impact (low/medium/high/critical)\n",
    "- **Governance intensity**: More requirements for higher risk (high-risk needs audit trail, fairness, approval)\n",
    "- **Examples**:\n",
    "  - Low risk: Internal analytics dashboard (minimal governance)\n",
    "  - Medium risk: Yield prediction (model card, fairness monitoring)\n",
    "  - High risk: Credit scoring (full governance, compliance checks, audit trail)\n",
    "  - Critical risk: Medical diagnosis (maximum governance, regulatory approval, human oversight)\n",
    "\n",
    "**25. Governance-by-Design**\n",
    "- **Integrate early**: Build governance into ML workflow (not bolted on later)\n",
    "- **Automated checks**: Fairness tests in CI/CD, compliance validation in deployment pipeline\n",
    "- **Templates**: Standardize model cards, fairness reports, compliance checklists (reduce burden)\n",
    "\n",
    "**26. Stakeholder Engagement**\n",
    "- **Cross-functional teams**: Data scientists, legal, compliance, business, ethics\n",
    "- **Regular reviews**: Quarterly governance reviews (fairness, compliance, incidents)\n",
    "- **Transparency**: Share model cards with stakeholders (build trust)\n",
    "\n",
    "**27. Continuous Monitoring**\n",
    "- **Fairness drift**: Fairness metrics can degrade over time (monitor quarterly)\n",
    "- **Compliance changes**: Regulations evolve (AI Act, state laws) - stay updated\n",
    "- **Incident tracking**: Log all governance incidents (bias complaints, compliance violations)\n",
    "\n",
    "**28. Documentation Standards**\n",
    "- **Version control**: Model cards versioned with models (Git, model registry)\n",
    "- **Accessibility**: Store in central location (model registry, wiki, compliance database)\n",
    "- **Formats**: Multiple formats (JSON for automation, Markdown for humans, HTML for web)\n",
    "- **Reviews**: Peer review model cards (catch gaps, ensure accuracy)\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Production Checklist\n",
    "\n",
    "**Before deploying to production**:\n",
    "- [ ] **Model card created** (all sections complete, reviewed by stakeholders)\n",
    "- [ ] **Fairness evaluated** (all relevant metrics, no violations)\n",
    "- [ ] **Bias mitigation applied** (if bias detected, document mitigation strategy)\n",
    "- [ ] **Audit trail enabled** (logging training, deployment, inference events)\n",
    "- [ ] **Compliance checked** (GDPR, AI Act, SR 11-7, or applicable regulations)\n",
    "- [ ] **Explainability implemented** (SHAP, LIME, or appropriate method)\n",
    "- [ ] **Approval obtained** (governance review, stakeholder sign-off)\n",
    "- [ ] **Monitoring configured** (fairness monitoring, performance tracking)\n",
    "- [ ] **Incident response plan** (know what to do if bias/compliance violation detected)\n",
    "- [ ] **Documentation published** (model card accessible to stakeholders)\n",
    "- [ ] **Risk assessment documented** (classify risk level, justify governance rigor)\n",
    "- [ ] **Legal review** (if high-risk, legal team confirms compliance)\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ When to Prioritize Governance\n",
    "\n",
    "**‚úÖ Prioritize governance when**:\n",
    "- High-risk systems (healthcare, finance, hiring, safety-critical)\n",
    "- Customer-facing models (impact user experience, trust)\n",
    "- Regulated industries (banking, healthcare, public sector)\n",
    "- Protected characteristics involved (race, gender, age - fairness critical)\n",
    "- High-stakes decisions ($millions impact, legal consequences)\n",
    "- Multi-year deployment (governance ensures long-term trust)\n",
    "\n",
    "**‚ùå Lower priority when**:\n",
    "- Internal analytics (no user impact, low risk)\n",
    "- Experimental models (prototypes, not production)\n",
    "- Low-stakes decisions (movie recommendations, UI optimization)\n",
    "\n",
    "**Scaling governance**: Start simple (model card for all models), add rigor for high-risk (fairness, compliance, audit trails)\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "**After mastering model governance**:\n",
    "1. **Shadow Mode Deployment (Notebook 128)**: Safe deployment strategies before full production\n",
    "2. **CI/CD for ML (Notebook 129)**: Automate governance checks in deployment pipeline\n",
    "3. **Advanced MLOps (Notebook 130)**: Multi-model systems, AutoML, governance at scale\n",
    "\n",
    "**Recommended resources**:\n",
    "- Book: \\\"Fairness and Machine Learning\\\" (Barocas et al.) - Comprehensive fairness guide\n",
    "- Paper: \\\"Model Cards for Model Reporting\\\" (Mitchell et al., Google) - Original model card specification\n",
    "- Course: \\\"AI Ethics\\\" (fast.ai) - Practical AI ethics and fairness\n",
    "- Tool: Fairlearn (Microsoft), AI Fairness 360 (IBM) - Fairness libraries\n",
    "- Website: EU AI Act compliance guide, GDPR documentation\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Remember**: Governance is not optional for production ML in 2024+. Regulations (GDPR, AI Act) mandate documentation, fairness, and transparency. Start governance early, automate where possible, and treat it as enabler (not burden) for responsible AI adoption. The cost of governance ($thousands) is far less than the cost of non-compliance ($millions in fines + reputational damage)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e741f875",
   "metadata": {},
   "source": [
    "## üîë Key Takeaways\n",
    "\n",
    "**When to Use Model Governance:**\n",
    "- Regulated industries (finance, healthcare, government)\n",
    "- High-stakes decisions (hiring, lending, medical diagnosis)\n",
    "- Multiple teams deploying models\n",
    "- Audit and compliance requirements\n",
    "\n",
    "**Limitations:**\n",
    "- Adds overhead to deployment process\n",
    "- Requires dedicated governance team/tools\n",
    "- Documentation burden on data scientists\n",
    "- May slow down experimentation\n",
    "\n",
    "**Alternatives:**\n",
    "- Lightweight documentation (README + model cards)\n",
    "- Peer review process without formal governance\n",
    "- Third-party compliance platforms (Fiddler, Arthur)\n",
    "- Manual audits (periodic vs continuous)\n",
    "\n",
    "**Best Practices:**\n",
    "- Automate documentation generation where possible\n",
    "- Integrate governance into CI/CD pipelines\n",
    "- Maintain model registry with lineage tracking\n",
    "- Conduct regular governance audits\n",
    "- Train teams on compliance requirements\n",
    "\n",
    "**Next Steps:**\n",
    "- 155: Model Explainability (generate audit reports)\n",
    "- 176: Fairness & Bias in ML (compliance validation)\n",
    "- 154: Model Monitoring (post-deployment compliance)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
