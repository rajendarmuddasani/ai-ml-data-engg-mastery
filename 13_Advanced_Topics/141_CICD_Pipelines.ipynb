{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0e1ce45",
   "metadata": {},
   "source": [
    "# 141: CI/CD Pipelines for ML Systems\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** continuous integration and continuous deployment for ML workflows\n",
    "- **Implement** automated testing pipelines for ML code and models\n",
    "- **Build** deployment pipelines with automated validation gates\n",
    "- **Apply** CI/CD to post-silicon validation (automated test data pipelines)\n",
    "- **Evaluate** pipeline performance and deployment reliability metrics\n",
    "\n",
    "## üìö What is CI/CD for ML?\n",
    "\n",
    "**CI/CD (Continuous Integration/Continuous Deployment)** automates the build, test, and deployment process for software. For ML systems, CI/CD extends beyond code to include data validation, model training, evaluation, and deployment.\n",
    "\n",
    "**CI (Continuous Integration):**\n",
    "- Automated code quality checks (linting, unit tests, type checking)\n",
    "- Data validation (schema checks, distribution tests)\n",
    "- Model training tests (can model train successfully?)\n",
    "- Automated testing on every code commit\n",
    "\n",
    "**CD (Continuous Deployment/Delivery):**\n",
    "- Automated model packaging (Docker containers, model artifacts)\n",
    "- Deployment validation (shadow mode, canary testing)\n",
    "- Infrastructure provisioning (Kubernetes clusters, serving endpoints)\n",
    "- Rollback mechanisms (revert to previous version on failure)\n",
    "\n",
    "**Why CI/CD for ML?**\n",
    "- ‚úÖ **Faster iteration:** Deploy models in hours (vs weeks manual process)\n",
    "- ‚úÖ **Reliability:** Automated tests catch errors before production\n",
    "- ‚úÖ **Reproducibility:** Pipeline-as-code ensures consistent deployments\n",
    "- ‚úÖ **Collaboration:** Teams work on shared codebase with automated integration\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**1. Automated Test Data Pipeline**\n",
    "- Input: STDF files uploaded to S3 ‚Üí Trigger CI/CD\n",
    "- Output: Parsed data ‚Üí Feature engineering ‚Üí Model retraining ‚Üí Deployment\n",
    "- Value: Daily model updates with zero manual intervention = **$8M-$15M/year**\n",
    "\n",
    "**2. Model Validation Gates**\n",
    "- Input: Newly trained yield model\n",
    "- Output: Automated accuracy check (>85%) ‚Üí Shadow mode ‚Üí Canary (10% traffic) ‚Üí Full rollout\n",
    "- Value: 80% fewer bad deployments = **$5M-$12M/year**\n",
    "\n",
    "**3. Cross-Fab Deployment**\n",
    "- Input: Model trained in Fab A\n",
    "- Output: Automated testing in Fab B staging ‚Üí Validation ‚Üí Production deploy\n",
    "- Value: 50% faster multi-fab rollouts = **$3M-$8M/year**\n",
    "\n",
    "**4. Continuous Test Optimization**\n",
    "- Input: Test sequence changes\n",
    "- Output: Automated validation (test time, coverage, yield impact) ‚Üí Deploy if pass\n",
    "- Value: Safe experimentation, 15% faster optimization = **$10M-$25M/year**\n",
    "\n",
    "## üîÑ CI/CD Workflow for ML\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Code Commit] --> B[CI: Lint/Test]\n",
    "    B --> C[CI: Data Validation]\n",
    "    C --> D[CI: Model Training]\n",
    "    D --> E[CI: Model Evaluation]\n",
    "    E --> F{Pass Gates?}\n",
    "    F -->|No| G[Notify Team]\n",
    "    F -->|Yes| H[CD: Package Model]\n",
    "    H --> I[CD: Deploy to Staging]\n",
    "    I --> J[CD: Validation Tests]\n",
    "    J --> K{Staging Pass?}\n",
    "    K -->|No| G\n",
    "    K -->|Yes| L[CD: Canary Deploy]\n",
    "    L --> M[CD: Monitor Metrics]\n",
    "    M --> N{Metrics OK?}\n",
    "    N -->|No| O[Rollback]\n",
    "    N -->|Yes| P[CD: Full Production]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style P fill:#e1ffe1\n",
    "    style O fill:#ffe1e1\n",
    "    style G fill:#fff4e1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- 009: Git Version Control (branching, merging, pull requests)\n",
    "- 131: Docker & Containerization (containerizing ML applications)\n",
    "- 156: ML Pipeline Orchestration (Airflow/Kubeflow workflows)\n",
    "\n",
    "**Next Steps:**\n",
    "- 136: CI/CD ML Pipelines (advanced ML-specific patterns)\n",
    "- 154: Model Monitoring (post-deployment observability)\n",
    "- 127: Model Governance (compliance in automated pipelines)\n",
    "\n",
    "---\n",
    "\n",
    "Let's build automated ML deployment pipelines! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf103c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Any, Callable\n",
    "from enum import Enum\n",
    "import hashlib\n",
    "import uuid\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dde578",
   "metadata": {},
   "source": [
    "## 2. üî® CI Pipeline - Build, Test, Quality Gates\n",
    "\n",
    "**Purpose:** Implement continuous integration pipeline with automated build, testing, and quality validation.\n",
    "\n",
    "**Key Components:**\n",
    "- **Build Stage**: Compile code, build Docker images, package artifacts (Python wheel, JAR, npm package)\n",
    "- **Test Stage**: Run unit tests (pytest, JUnit), integration tests, contract tests (API validation)\n",
    "- **Quality Stage**: Linting (pylint, eslint), code coverage (>80% threshold), security scan (OWASP, Snyk)\n",
    "- **Artifact Storage**: Push validated artifacts to registry (Docker Hub, Artifactory, PyPI)\n",
    "\n",
    "**CI Pipeline Stages:**\n",
    "\n",
    "1. **Checkout Code**: Clone repository at specific commit SHA\n",
    "2. **Build**: Create reproducible artifacts (Docker image with commit hash tag)\n",
    "3. **Unit Tests**: Fast tests (1000 tests in <2 minutes), mock external dependencies\n",
    "4. **Integration Tests**: Test with real dependencies (database, message queue, external APIs)\n",
    "5. **Code Quality**: Linting errors fail pipeline, coverage <80% fails pipeline\n",
    "6. **Security Scan**: Check for CVEs in dependencies (fail on critical/high severity)\n",
    "7. **Publish Artifacts**: Push to registry only if all stages pass\n",
    "\n",
    "**Why CI Pipeline?**\n",
    "- **Fast feedback**: Developer knows within 10 minutes if changes break anything\n",
    "- **Prevent regressions**: Automated tests catch bugs before merge\n",
    "- **Code quality**: Enforce standards (linting, coverage) automatically\n",
    "- **Security**: Scan dependencies for vulnerabilities pre-deployment\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "\n",
    "**Scenario:** STDF parser library used by 20 teams. Need to validate all changes work across Python 3.8, 3.9, 3.10, 3.11, 3.12 with different database backends (PostgreSQL, MySQL, SQLite).\n",
    "\n",
    "**CI Pipeline Implementation:**\n",
    "```yaml\n",
    "# .github/workflows/ci.yml\n",
    "name: CI Pipeline\n",
    "on: [push, pull_request]\n",
    "\n",
    "jobs:\n",
    "  test-matrix:\n",
    "    runs-on: ubuntu-latest\n",
    "    strategy:\n",
    "      matrix:\n",
    "        python: [3.8, 3.9, 3.10, 3.11, 3.12]\n",
    "        database: [postgres, mysql, sqlite]\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: ${{ matrix.python }}\n",
    "      - name: Install dependencies\n",
    "        run: pip install -r requirements.txt\n",
    "      - name: Run unit tests\n",
    "        run: pytest tests/unit --cov=stdf_parser --cov-report=xml\n",
    "      - name: Run integration tests\n",
    "        run: pytest tests/integration --database=${{ matrix.database }}\n",
    "      - name: Check coverage\n",
    "        run: |\n",
    "          coverage report --fail-under=85\n",
    "      - name: Security scan\n",
    "        run: bandit -r stdf_parser\n",
    "```\n",
    "\n",
    "**Value:** Catch incompatibility issues before deployment (e.g., Python 3.12 breaks 5% of tests) ‚Üí prevents production incidents ($500K/year savings from prevented downtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bce777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CI Pipeline Implementation - Build, Test, Quality Gates\n",
    "\n",
    "class PipelineStage(Enum):\n",
    "    \"\"\"Pipeline stage types\"\"\"\n",
    "    CHECKOUT = \"checkout\"\n",
    "    BUILD = \"build\"\n",
    "    UNIT_TEST = \"unit_test\"\n",
    "    INTEGRATION_TEST = \"integration_test\"\n",
    "    QUALITY = \"quality\"\n",
    "    SECURITY = \"security\"\n",
    "    PUBLISH = \"publish\"\n",
    "\n",
    "class StageStatus(Enum):\n",
    "    \"\"\"Stage execution status\"\"\"\n",
    "    PENDING = \"pending\"\n",
    "    RUNNING = \"running\"\n",
    "    SUCCESS = \"success\"\n",
    "    FAILURE = \"failure\"\n",
    "    SKIPPED = \"skipped\"\n",
    "\n",
    "@dataclass\n",
    "class StageResult:\n",
    "    \"\"\"Result of pipeline stage execution\"\"\"\n",
    "    stage: PipelineStage\n",
    "    status: StageStatus\n",
    "    duration_seconds: float\n",
    "    logs: List[str] = field(default_factory=list)\n",
    "    metrics: Dict[str, Any] = field(default_factory=dict)\n",
    "    artifacts: List[str] = field(default_factory=list)\n",
    "\n",
    "@dataclass\n",
    "class TestResult:\n",
    "    \"\"\"Test execution results\"\"\"\n",
    "    total_tests: int\n",
    "    passed: int\n",
    "    failed: int\n",
    "    skipped: int\n",
    "    duration_seconds: float\n",
    "    coverage_percent: float = 0.0\n",
    "    \n",
    "    @property\n",
    "    def pass_rate(self) -> float:\n",
    "        return (self.passed / self.total_tests * 100) if self.total_tests > 0 else 0.0\n",
    "\n",
    "class CIPipeline:\n",
    "    \"\"\"Continuous Integration Pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name: str, commit_sha: str):\n",
    "        self.project_name = project_name\n",
    "        self.commit_sha = commit_sha\n",
    "        self.pipeline_id = f\"pipeline-{uuid.uuid4().hex[:8]}\"\n",
    "        self.start_time = datetime.now()\n",
    "        self.stages: List[StageResult] = []\n",
    "    \n",
    "    def run_stage(self, stage: PipelineStage, execute_fn: Callable) -> StageResult:\n",
    "        \"\"\"Execute pipeline stage\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üîÑ Running stage: {stage.value.upper()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        start = time.time()\n",
    "        result = StageResult(\n",
    "            stage=stage,\n",
    "            status=StageStatus.RUNNING,\n",
    "            duration_seconds=0.0\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Execute stage function\n",
    "            stage_output = execute_fn()\n",
    "            result.status = StageStatus.SUCCESS\n",
    "            result.logs = stage_output.get('logs', [])\n",
    "            result.metrics = stage_output.get('metrics', {})\n",
    "            result.artifacts = stage_output.get('artifacts', [])\n",
    "            \n",
    "            print(f\"‚úÖ Stage {stage.value} PASSED\")\n",
    "        except Exception as e:\n",
    "            result.status = StageStatus.FAILURE\n",
    "            result.logs.append(f\"ERROR: {str(e)}\")\n",
    "            print(f\"‚ùå Stage {stage.value} FAILED: {e}\")\n",
    "        finally:\n",
    "            result.duration_seconds = time.time() - start\n",
    "            self.stages.append(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def checkout_stage(self) -> Dict[str, Any]:\n",
    "        \"\"\"Checkout code from repository\"\"\"\n",
    "        time.sleep(0.1)\n",
    "        return {\n",
    "            'logs': [\n",
    "                f\"Cloning repository: {self.project_name}\",\n",
    "                f\"Checkout commit: {self.commit_sha}\",\n",
    "                \"Repository cloned successfully\"\n",
    "            ],\n",
    "            'metrics': {'files_changed': 15, 'lines_added': 250, 'lines_removed': 80}\n",
    "        }\n",
    "    \n",
    "    def build_stage(self) -> Dict[str, Any]:\n",
    "        \"\"\"Build Docker image and Python package\"\"\"\n",
    "        time.sleep(0.15)\n",
    "        \n",
    "        image_tag = f\"{self.project_name}:{self.commit_sha[:8]}\"\n",
    "        \n",
    "        return {\n",
    "            'logs': [\n",
    "                \"Installing dependencies from requirements.txt\",\n",
    "                \"Building Python wheel package\",\n",
    "                f\"Building Docker image: {image_tag}\",\n",
    "                f\"Image size: 450MB (optimized from 800MB)\",\n",
    "                \"Build completed successfully\"\n",
    "            ],\n",
    "            'metrics': {\n",
    "                'build_time_seconds': 90,\n",
    "                'image_size_mb': 450,\n",
    "                'layers': 12\n",
    "            },\n",
    "            'artifacts': [\n",
    "                f\"stdf_parser-2.1.0-py3-none-any.whl\",\n",
    "                f\"{image_tag}\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def unit_test_stage(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run unit tests with coverage\"\"\"\n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        # Simulate test execution\n",
    "        total_tests = 1000\n",
    "        failed_tests = random.randint(0, 5)\n",
    "        passed_tests = total_tests - failed_tests\n",
    "        coverage = random.uniform(82, 95)\n",
    "        \n",
    "        test_result = TestResult(\n",
    "            total_tests=total_tests,\n",
    "            passed=passed_tests,\n",
    "            failed=failed_tests,\n",
    "            skipped=0,\n",
    "            duration_seconds=120,\n",
    "            coverage_percent=coverage\n",
    "        )\n",
    "        \n",
    "        logs = [\n",
    "            f\"Running {total_tests} unit tests...\",\n",
    "            f\"Tests passed: {passed_tests}/{total_tests} ({test_result.pass_rate:.1f}%)\",\n",
    "            f\"Code coverage: {coverage:.1f}%\",\n",
    "        ]\n",
    "        \n",
    "        if failed_tests > 0:\n",
    "            logs.append(f\"‚ö†Ô∏è  {failed_tests} tests failed:\")\n",
    "            for i in range(min(failed_tests, 3)):\n",
    "                logs.append(f\"  - test_parse_stdf_voltage_range (AssertionError: Expected 15V, got 20V)\")\n",
    "        \n",
    "        if coverage < 80:\n",
    "            raise Exception(f\"Code coverage {coverage:.1f}% below threshold 80%\")\n",
    "        \n",
    "        return {\n",
    "            'logs': logs,\n",
    "            'metrics': {\n",
    "                'total_tests': total_tests,\n",
    "                'passed': passed_tests,\n",
    "                'failed': failed_tests,\n",
    "                'pass_rate': test_result.pass_rate,\n",
    "                'coverage': coverage,\n",
    "                'duration_seconds': test_result.duration_seconds\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def integration_test_stage(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run integration tests with real dependencies\"\"\"\n",
    "        time.sleep(0.25)\n",
    "        \n",
    "        total_tests = 50\n",
    "        failed_tests = random.randint(0, 2)\n",
    "        passed_tests = total_tests - failed_tests\n",
    "        \n",
    "        logs = [\n",
    "            \"Starting integration tests with real PostgreSQL database\",\n",
    "            f\"Running {total_tests} integration tests...\",\n",
    "            f\"Tests passed: {passed_tests}/{total_tests}\",\n",
    "            \"Testing STDF parsing with real wafer data files\",\n",
    "            \"Validating database writes and reads\",\n",
    "            \"Testing ML model prediction pipeline end-to-end\"\n",
    "        ]\n",
    "        \n",
    "        if failed_tests > 0:\n",
    "            raise Exception(f\"{failed_tests} integration tests failed\")\n",
    "        \n",
    "        return {\n",
    "            'logs': logs,\n",
    "            'metrics': {\n",
    "                'total_tests': total_tests,\n",
    "                'passed': passed_tests,\n",
    "                'duration_seconds': 180\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def quality_stage(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run code quality checks\"\"\"\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        pylint_score = random.uniform(8.5, 10.0)\n",
    "        \n",
    "        logs = [\n",
    "            \"Running pylint code quality check\",\n",
    "            f\"Pylint score: {pylint_score:.2f}/10.00\",\n",
    "            \"Running black code formatter check\",\n",
    "            \"Code formatting: PASSED\",\n",
    "            \"Running mypy type checking\",\n",
    "            \"Type checking: PASSED (0 errors)\"\n",
    "        ]\n",
    "        \n",
    "        if pylint_score < 8.0:\n",
    "            raise Exception(f\"Pylint score {pylint_score:.2f} below threshold 8.0\")\n",
    "        \n",
    "        return {\n",
    "            'logs': logs,\n",
    "            'metrics': {\n",
    "                'pylint_score': pylint_score,\n",
    "                'formatting_issues': 0,\n",
    "                'type_errors': 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def security_stage(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run security vulnerability scan\"\"\"\n",
    "        time.sleep(0.12)\n",
    "        \n",
    "        # Simulate security scan results\n",
    "        vulnerabilities = {\n",
    "            'critical': 0,\n",
    "            'high': random.randint(0, 1),\n",
    "            'medium': random.randint(0, 3),\n",
    "            'low': random.randint(2, 5)\n",
    "        }\n",
    "        \n",
    "        logs = [\n",
    "            \"Running bandit security scan on Python code\",\n",
    "            \"Scanning dependencies for known vulnerabilities (CVE database)\",\n",
    "            f\"Vulnerabilities found:\",\n",
    "            f\"  Critical: {vulnerabilities['critical']}\",\n",
    "            f\"  High: {vulnerabilities['high']}\",\n",
    "            f\"  Medium: {vulnerabilities['medium']}\",\n",
    "            f\"  Low: {vulnerabilities['low']}\"\n",
    "        ]\n",
    "        \n",
    "        if vulnerabilities['critical'] > 0 or vulnerabilities['high'] > 0:\n",
    "            raise Exception(f\"Security scan failed: {vulnerabilities['critical']} critical, {vulnerabilities['high']} high severity vulnerabilities\")\n",
    "        \n",
    "        return {\n",
    "            'logs': logs,\n",
    "            'metrics': vulnerabilities\n",
    "        }\n",
    "    \n",
    "    def publish_stage(self) -> Dict[str, Any]:\n",
    "        \"\"\"Publish artifacts to registry\"\"\"\n",
    "        time.sleep(0.08)\n",
    "        \n",
    "        return {\n",
    "            'logs': [\n",
    "                \"Publishing Python package to Artifactory\",\n",
    "                \"Pushing Docker image to container registry\",\n",
    "                f\"Image pushed: stdf-parser:{self.commit_sha[:8]}\",\n",
    "                \"Artifacts published successfully\"\n",
    "            ],\n",
    "            'artifacts': [\n",
    "                f\"stdf_parser-2.1.0-py3-none-any.whl\",\n",
    "                f\"stdf-parser:{self.commit_sha[:8]}\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def run_pipeline(self) -> bool:\n",
    "        \"\"\"Execute full CI pipeline\"\"\"\n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# CI Pipeline Started\")\n",
    "        print(f\"# Project: {self.project_name}\")\n",
    "        print(f\"# Commit: {self.commit_sha}\")\n",
    "        print(f\"# Pipeline ID: {self.pipeline_id}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        \n",
    "        # Run stages in sequence\n",
    "        stages = [\n",
    "            (PipelineStage.CHECKOUT, self.checkout_stage),\n",
    "            (PipelineStage.BUILD, self.build_stage),\n",
    "            (PipelineStage.UNIT_TEST, self.unit_test_stage),\n",
    "            (PipelineStage.INTEGRATION_TEST, self.integration_test_stage),\n",
    "            (PipelineStage.QUALITY, self.quality_stage),\n",
    "            (PipelineStage.SECURITY, self.security_stage),\n",
    "            (PipelineStage.PUBLISH, self.publish_stage)\n",
    "        ]\n",
    "        \n",
    "        for stage, execute_fn in stages:\n",
    "            result = self.run_stage(stage, execute_fn)\n",
    "            \n",
    "            if result.status == StageStatus.FAILURE:\n",
    "                print(f\"\\n{'!'*70}\")\n",
    "                print(f\"! Pipeline FAILED at stage: {stage.value}\")\n",
    "                print(f\"{'!'*70}\")\n",
    "                self.print_summary()\n",
    "                return False\n",
    "        \n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# ‚úÖ Pipeline PASSED\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        self.print_summary()\n",
    "        return True\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print pipeline execution summary\"\"\"\n",
    "        total_duration = sum(s.duration_seconds for s in self.stages)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"Pipeline Summary\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"{'Stage':<25} {'Status':<15} {'Duration':<15}\")\n",
    "        print(f\"{'-'*70}\")\n",
    "        \n",
    "        for stage in self.stages:\n",
    "            status_emoji = {\n",
    "                StageStatus.SUCCESS: \"‚úÖ\",\n",
    "                StageStatus.FAILURE: \"‚ùå\",\n",
    "                StageStatus.RUNNING: \"üîÑ\",\n",
    "                StageStatus.PENDING: \"‚è∏Ô∏è\",\n",
    "                StageStatus.SKIPPED: \"‚è≠Ô∏è\"\n",
    "            }\n",
    "            emoji = status_emoji.get(stage.status, \"\")\n",
    "            print(f\"{stage.stage.value:<25} {emoji} {stage.status.value:<13} {stage.duration_seconds:<15.2f}s\")\n",
    "        \n",
    "        print(f\"{'-'*70}\")\n",
    "        print(f\"{'Total Duration':<41} {total_duration:.2f}s\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "# Example 1: Successful CI Pipeline\n",
    "print(\"=\"*70)\n",
    "print(\"Example 1: Successful CI Pipeline for STDF Parser\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "pipeline1 = CIPipeline(\n",
    "    project_name=\"stdf-parser\",\n",
    "    commit_sha=\"a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0\"\n",
    ")\n",
    "\n",
    "success = pipeline1.run_pipeline()\n",
    "\n",
    "# Example 2: Failed CI Pipeline (Low Coverage)\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"Example 2: Failed CI Pipeline - Coverage Below Threshold\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Force low coverage by modifying test stage\n",
    "class FailedCIPipeline(CIPipeline):\n",
    "    def unit_test_stage(self) -> Dict[str, Any]:\n",
    "        time.sleep(0.2)\n",
    "        coverage = 75.0  # Below 80% threshold\n",
    "        \n",
    "        raise Exception(f\"Code coverage {coverage:.1f}% below threshold 80%\")\n",
    "\n",
    "pipeline2 = FailedCIPipeline(\n",
    "    project_name=\"stdf-parser\",\n",
    "    commit_sha=\"b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0u1\"\n",
    ")\n",
    "\n",
    "success = pipeline2.run_pipeline()\n",
    "\n",
    "print(\"\\n‚úÖ CI Pipeline implementation complete!\")\n",
    "print(\"   - Automated build, test, quality, and security stages\")\n",
    "print(\"   - Fast feedback loop (<5 minutes for full pipeline)\")\n",
    "print(\"   - Quality gates prevent bad code from reaching production\")\n",
    "print(\"   - Reproducible builds with Docker and commit SHAs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d5cbdf",
   "metadata": {},
   "source": [
    "## 3. üöÄ CD Pipeline - Deployment Strategies & Rollback\n",
    "\n",
    "**Purpose:** Implement continuous deployment with safe deployment strategies (blue-green, canary, rolling) and automated rollback.\n",
    "\n",
    "**Deployment Strategies:**\n",
    "\n",
    "**1. Blue-Green Deployment (Zero Downtime):**\n",
    "- **Blue environment**: Current production (v1.0 serving 100% traffic)\n",
    "- **Green environment**: New version (v2.0 deployed, 0% traffic initially)\n",
    "- **Cutover**: Switch load balancer from blue ‚Üí green instantly (1 second switch)\n",
    "- **Rollback**: Switch back to blue if issues detected (1 second rollback)\n",
    "- **Use case**: Database schema changes, major version upgrades\n",
    "\n",
    "**2. Canary Deployment (Gradual Rollout):**\n",
    "- **Phase 1**: Deploy v2.0 to 5% of servers, route 5% traffic ‚Üí monitor for 10 minutes\n",
    "- **Phase 2**: If metrics OK (error rate <1%, latency P95 <200ms) ‚Üí increase to 25% traffic\n",
    "- **Phase 3**: Increase to 50% ‚Üí 75% ‚Üí 100% over 30 minutes\n",
    "- **Rollback**: If any phase shows degraded metrics ‚Üí revert all traffic to v1.0\n",
    "- **Use case**: ML model deployments, API changes, performance-critical services\n",
    "\n",
    "**3. Rolling Update (Incremental Replacement):**\n",
    "- **Update**: Replace 1 pod at a time (pod 1 ‚Üí wait 2 min ‚Üí pod 2 ‚Üí pod 3)\n",
    "- **Health checks**: Each new pod must pass health check before next pod updated\n",
    "- **Rollback**: If pod fails health check ‚Üí stop rollout, revert updated pods\n",
    "- **Use case**: Kubernetes deployments, stateful services\n",
    "\n",
    "**Why Deployment Strategies?**\n",
    "- **Risk reduction**: Canary catches issues affecting <5% users vs 100% with big bang deployment\n",
    "- **Zero downtime**: Blue-green enables instant cutover without service interruption\n",
    "- **Fast rollback**: Automated rollback within 30 seconds vs 2 hours manual rollback\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "\n",
    "**Scenario:** Deploy ML yield prediction model v2.1 (93% accuracy) to replace v2.0 (91% accuracy). Risk: v2.1 might have latency regression (200ms P95 vs 50ms v2.0) or accuracy degradation on specific device types.\n",
    "\n",
    "**Canary Deployment Plan:**\n",
    "1. **Phase 1**: Deploy v2.1 to 5% traffic (10K predictions/day)\n",
    "   - Monitor: Latency P95 <100ms, accuracy >92%, error rate <0.5%\n",
    "   - Duration: 2 hours (enough data to detect issues)\n",
    "2. **Phase 2**: Increase to 25% traffic (50K predictions/day)\n",
    "   - Monitor: Same metrics, also check prediction distribution (KS test, p-value <0.05)\n",
    "   - Duration: 4 hours\n",
    "3. **Phase 3**: Increase to 100% traffic (200K predictions/day)\n",
    "   - Monitor: Full production metrics\n",
    "   \n",
    "**Automated Rollback Triggers:**\n",
    "- Latency P95 >150ms for 5 minutes ‚Üí rollback\n",
    "- Error rate >1% for 3 minutes ‚Üí rollback\n",
    "- Accuracy drops >2% (manual validation) ‚Üí rollback\n",
    "- Prediction distribution shift >10% (KS test) ‚Üí rollback\n",
    "\n",
    "**Value:** Canary deployment prevents bad model from affecting all users ‚Üí saves $500K/year from prevented accuracy degradation incidents (1-2 incidents/year √ó $250K-500K impact each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443abdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CD Pipeline Implementation - Deployment Strategies\n",
    "\n",
    "class DeploymentStrategy(Enum):\n",
    "    \"\"\"Deployment strategy types\"\"\"\n",
    "    BLUE_GREEN = \"blue_green\"\n",
    "    CANARY = \"canary\"\n",
    "    ROLLING = \"rolling\"\n",
    "\n",
    "class DeploymentPhase(Enum):\n",
    "    \"\"\"Deployment phase\"\"\"\n",
    "    DEPLOY = \"deploy\"\n",
    "    SMOKE_TEST = \"smoke_test\"\n",
    "    MONITOR = \"monitor\"\n",
    "    ROLLBACK = \"rollback\"\n",
    "    COMPLETE = \"complete\"\n",
    "\n",
    "@dataclass\n",
    "class DeploymentMetrics:\n",
    "    \"\"\"Deployment health metrics\"\"\"\n",
    "    error_rate: float  # Percentage\n",
    "    latency_p95_ms: float\n",
    "    throughput_rps: int\n",
    "    cpu_usage: float  # Percentage\n",
    "    memory_usage: float  # Percentage\n",
    "    \n",
    "    def is_healthy(self, thresholds: Dict[str, float]) -> bool:\n",
    "        \"\"\"Check if metrics meet health thresholds\"\"\"\n",
    "        checks = [\n",
    "            self.error_rate <= thresholds.get('max_error_rate', 1.0),\n",
    "            self.latency_p95_ms <= thresholds.get('max_latency_p95', 200),\n",
    "            self.cpu_usage <= thresholds.get('max_cpu', 80),\n",
    "            self.memory_usage <= thresholds.get('max_memory', 80)\n",
    "        ]\n",
    "        return all(checks)\n",
    "\n",
    "@dataclass\n",
    "class DeploymentResult:\n",
    "    \"\"\"Result of deployment\"\"\"\n",
    "    phase: DeploymentPhase\n",
    "    success: bool\n",
    "    traffic_percentage: int\n",
    "    metrics: Optional[DeploymentMetrics] = None\n",
    "    message: str = \"\"\n",
    "    duration_seconds: float = 0.0\n",
    "\n",
    "class CanaryDeployment:\n",
    "    \"\"\"Canary deployment with gradual traffic shifting\"\"\"\n",
    "    \n",
    "    def __init__(self, service_name: str, old_version: str, new_version: str):\n",
    "        self.service_name = service_name\n",
    "        self.old_version = old_version\n",
    "        self.new_version = new_version\n",
    "        self.deployment_id = f\"deploy-{uuid.uuid4().hex[:8]}\"\n",
    "        self.start_time = datetime.now()\n",
    "        self.results: List[DeploymentResult] = []\n",
    "        \n",
    "        # Health check thresholds\n",
    "        self.thresholds = {\n",
    "            'max_error_rate': 1.0,  # 1% max error rate\n",
    "            'max_latency_p95': 200,  # 200ms P95 latency\n",
    "            'max_cpu': 80,  # 80% CPU\n",
    "            'max_memory': 80  # 80% memory\n",
    "        }\n",
    "    \n",
    "    def deploy_version(self, traffic_percent: int) -> DeploymentResult:\n",
    "        \"\"\"Deploy new version to specified traffic percentage\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üöÄ Deploying {self.new_version} to {traffic_percent}% traffic\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        start = time.time()\n",
    "        time.sleep(0.1)  # Simulate deployment\n",
    "        \n",
    "        result = DeploymentResult(\n",
    "            phase=DeploymentPhase.DEPLOY,\n",
    "            success=True,\n",
    "            traffic_percentage=traffic_percent,\n",
    "            message=f\"Deployed {self.new_version} to {traffic_percent}% of servers\"\n",
    "        )\n",
    "        result.duration_seconds = time.time() - start\n",
    "        \n",
    "        print(f\"‚úÖ Deployment complete in {result.duration_seconds:.2f}s\")\n",
    "        return result\n",
    "    \n",
    "    def run_smoke_tests(self) -> DeploymentResult:\n",
    "        \"\"\"Run smoke tests on new deployment\"\"\"\n",
    "        print(f\"\\nüß™ Running smoke tests...\")\n",
    "        \n",
    "        start = time.time()\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "        tests = [\n",
    "            (\"Health check endpoint\", True),\n",
    "            (\"Prediction API endpoint\", True),\n",
    "            (\"Database connectivity\", True),\n",
    "            (\"ML model loaded\", True)\n",
    "        ]\n",
    "        \n",
    "        all_passed = all(result for _, result in tests)\n",
    "        \n",
    "        for test_name, passed in tests:\n",
    "            status = \"‚úÖ PASS\" if passed else \"‚ùå FAIL\"\n",
    "            print(f\"  {status}: {test_name}\")\n",
    "        \n",
    "        result = DeploymentResult(\n",
    "            phase=DeploymentPhase.SMOKE_TEST,\n",
    "            success=all_passed,\n",
    "            traffic_percentage=0,\n",
    "            message=f\"Smoke tests {'passed' if all_passed else 'failed'}\"\n",
    "        )\n",
    "        result.duration_seconds = time.time() - start\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def monitor_metrics(self, traffic_percent: int, duration_minutes: int = 10) -> DeploymentResult:\n",
    "        \"\"\"Monitor deployment metrics\"\"\"\n",
    "        print(f\"\\nüìä Monitoring metrics for {duration_minutes} minutes...\")\n",
    "        \n",
    "        start = time.time()\n",
    "        time.sleep(0.15)\n",
    "        \n",
    "        # Simulate metrics (with small chance of degradation)\n",
    "        is_degraded = random.random() < 0.1  # 10% chance of issues\n",
    "        \n",
    "        if is_degraded:\n",
    "            metrics = DeploymentMetrics(\n",
    "                error_rate=1.5,  # Above 1% threshold\n",
    "                latency_p95_ms=250,  # Above 200ms threshold\n",
    "                throughput_rps=800,\n",
    "                cpu_usage=75,\n",
    "                memory_usage=70\n",
    "            )\n",
    "        else:\n",
    "            metrics = DeploymentMetrics(\n",
    "                error_rate=random.uniform(0.1, 0.5),\n",
    "                latency_p95_ms=random.uniform(45, 95),\n",
    "                throughput_rps=random.randint(900, 1100),\n",
    "                cpu_usage=random.uniform(60, 75),\n",
    "                memory_usage=random.uniform(55, 70)\n",
    "            )\n",
    "        \n",
    "        is_healthy = metrics.is_healthy(self.thresholds)\n",
    "        \n",
    "        print(f\"\\n  Metrics after {duration_minutes} minutes:\")\n",
    "        print(f\"    Error rate: {metrics.error_rate:.2f}% (threshold: {self.thresholds['max_error_rate']}%)\")\n",
    "        print(f\"    Latency P95: {metrics.latency_p95_ms:.1f}ms (threshold: {self.thresholds['max_latency_p95']}ms)\")\n",
    "        print(f\"    Throughput: {metrics.throughput_rps} req/sec\")\n",
    "        print(f\"    CPU usage: {metrics.cpu_usage:.1f}%\")\n",
    "        print(f\"    Memory usage: {metrics.memory_usage:.1f}%\")\n",
    "        \n",
    "        result = DeploymentResult(\n",
    "            phase=DeploymentPhase.MONITOR,\n",
    "            success=is_healthy,\n",
    "            traffic_percentage=traffic_percent,\n",
    "            metrics=metrics,\n",
    "            message=f\"Metrics {'healthy' if is_healthy else 'DEGRADED'}\"\n",
    "        )\n",
    "        result.duration_seconds = time.time() - start\n",
    "        \n",
    "        if is_healthy:\n",
    "            print(f\"\\n  ‚úÖ All metrics within healthy thresholds\")\n",
    "        else:\n",
    "            print(f\"\\n  ‚ùå Metrics degraded! Rollback required.\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def rollback(self) -> DeploymentResult:\n",
    "        \"\"\"Rollback to previous version\"\"\"\n",
    "        print(f\"\\n{'!'*70}\")\n",
    "        print(f\"üîÑ ROLLBACK: Reverting to {self.old_version}\")\n",
    "        print(f\"{'!'*70}\")\n",
    "        \n",
    "        start = time.time()\n",
    "        time.sleep(0.08)\n",
    "        \n",
    "        result = DeploymentResult(\n",
    "            phase=DeploymentPhase.ROLLBACK,\n",
    "            success=True,\n",
    "            traffic_percentage=0,\n",
    "            message=f\"Rolled back to {self.old_version}\"\n",
    "        )\n",
    "        result.duration_seconds = time.time() - start\n",
    "        \n",
    "        print(f\"‚úÖ Rollback complete in {result.duration_seconds:.2f}s\")\n",
    "        return result\n",
    "    \n",
    "    def execute_canary_deployment(self) -> bool:\n",
    "        \"\"\"Execute full canary deployment with gradual rollout\"\"\"\n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# Canary Deployment Started\")\n",
    "        print(f\"# Service: {self.service_name}\")\n",
    "        print(f\"# Version: {self.old_version} ‚Üí {self.new_version}\")\n",
    "        print(f\"# Deployment ID: {self.deployment_id}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        \n",
    "        # Phase 1: Deploy to 5% traffic\n",
    "        result = self.deploy_version(traffic_percent=5)\n",
    "        self.results.append(result)\n",
    "        \n",
    "        if not result.success:\n",
    "            return False\n",
    "        \n",
    "        # Run smoke tests\n",
    "        result = self.run_smoke_tests()\n",
    "        self.results.append(result)\n",
    "        \n",
    "        if not result.success:\n",
    "            self.rollback()\n",
    "            return False\n",
    "        \n",
    "        # Monitor 5% traffic\n",
    "        result = self.monitor_metrics(traffic_percent=5, duration_minutes=10)\n",
    "        self.results.append(result)\n",
    "        \n",
    "        if not result.success:\n",
    "            self.rollback()\n",
    "            return False\n",
    "        \n",
    "        # Phase 2: Increase to 25% traffic\n",
    "        result = self.deploy_version(traffic_percent=25)\n",
    "        self.results.append(result)\n",
    "        \n",
    "        result = self.monitor_metrics(traffic_percent=25, duration_minutes=10)\n",
    "        self.results.append(result)\n",
    "        \n",
    "        if not result.success:\n",
    "            self.rollback()\n",
    "            return False\n",
    "        \n",
    "        # Phase 3: Increase to 50% traffic\n",
    "        result = self.deploy_version(traffic_percent=50)\n",
    "        self.results.append(result)\n",
    "        \n",
    "        result = self.monitor_metrics(traffic_percent=50, duration_minutes=10)\n",
    "        self.results.append(result)\n",
    "        \n",
    "        if not result.success:\n",
    "            self.rollback()\n",
    "            return False\n",
    "        \n",
    "        # Phase 4: Full deployment to 100% traffic\n",
    "        result = self.deploy_version(traffic_percent=100)\n",
    "        self.results.append(result)\n",
    "        \n",
    "        result = self.monitor_metrics(traffic_percent=100, duration_minutes=15)\n",
    "        self.results.append(result)\n",
    "        \n",
    "        if not result.success:\n",
    "            self.rollback()\n",
    "            return False\n",
    "        \n",
    "        # Deployment successful\n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# ‚úÖ Canary Deployment SUCCESSFUL\")\n",
    "        print(f\"# {self.new_version} serving 100% traffic\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        \n",
    "        self.print_summary()\n",
    "        return True\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print deployment summary\"\"\"\n",
    "        total_duration = (datetime.now() - self.start_time).total_seconds()\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"Deployment Summary\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"{'Phase':<25} {'Traffic %':<15} {'Status':<20}\")\n",
    "        print(f\"{'-'*70}\")\n",
    "        \n",
    "        for result in self.results:\n",
    "            status_emoji = \"‚úÖ\" if result.success else \"‚ùå\"\n",
    "            print(f\"{result.phase.value:<25} {result.traffic_percentage:<15} {status_emoji} {result.message}\")\n",
    "        \n",
    "        print(f\"{'-'*70}\")\n",
    "        print(f\"{'Total Duration':<41} {total_duration:.2f}s\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "# Example 3: Successful Canary Deployment\n",
    "print(\"=\"*70)\n",
    "print(\"Example 3: Successful Canary Deployment - ML Model v2.1\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Force success by setting low degradation chance\n",
    "random.seed(42)\n",
    "\n",
    "canary1 = CanaryDeployment(\n",
    "    service_name=\"ml-yield-predictor\",\n",
    "    old_version=\"v2.0\",\n",
    "    new_version=\"v2.1\"\n",
    ")\n",
    "\n",
    "success = canary1.execute_canary_deployment()\n",
    "\n",
    "# Example 4: Failed Canary Deployment with Rollback\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"Example 4: Failed Canary Deployment - Metrics Degraded at 25% Traffic\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Force failure by using different seed\n",
    "random.seed(123)\n",
    "\n",
    "canary2 = CanaryDeployment(\n",
    "    service_name=\"ml-yield-predictor\",\n",
    "    old_version=\"v2.0\",\n",
    "    new_version=\"v2.2\"\n",
    ")\n",
    "\n",
    "success = canary2.execute_canary_deployment()\n",
    "\n",
    "print(\"\\n‚úÖ CD Pipeline implementation complete!\")\n",
    "print(\"   - Canary deployment with gradual traffic shifting (5% ‚Üí 25% ‚Üí 50% ‚Üí 100%)\")\n",
    "print(\"   - Automated health monitoring at each phase\")\n",
    "print(\"   - Automated rollback on metric degradation\")\n",
    "print(\"   - Zero downtime deployment strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c96980",
   "metadata": {},
   "source": [
    "## 4. üî¨ Real-World Projects: Production CI/CD\n",
    "\n",
    "### Project 1: **Complete CI/CD Platform for ML Models** üí∞ **$3.2M/year**\n",
    "**Objective:** Build end-to-end CI/CD platform for ML lifecycle (training, validation, deployment, monitoring) with 100+ model deployments/month.\n",
    "\n",
    "**Key Features:**\n",
    "- **CI Pipeline**: On code commit ‚Üí run model training (1000 samples) ‚Üí validate accuracy >90% ‚Üí check data drift ‚Üí serialize model (ONNX/TorchScript) ‚Üí build Docker image ‚Üí push to registry\n",
    "- **CD Pipeline**: Deploy to staging ‚Üí shadow mode (predictions logged but not used) ‚Üí compare with production model ‚Üí A/B test (10% traffic) ‚Üí gradual rollout (10% ‚Üí 50% ‚Üí 100%) ‚Üí monitor metrics (accuracy, latency, drift)\n",
    "- **Automated Rollback**: If accuracy <92% OR latency P95 >100ms OR prediction distribution shift >10% ‚Üí auto-rollback in 30 seconds\n",
    "- **Multi-Environment**: Dev (for experimentation), staging (for validation), production (live traffic), canary (A/B testing)\n",
    "- **Model Registry**: Track all models with metadata (training date, accuracy, features, hyperparameters), enable rollback to any previous version\n",
    "\n",
    "**Business Value:**\n",
    "- 95% faster model deployment (2 days ‚Üí 2 hours manual validation replaced by automated pipeline)\n",
    "- 10x more frequent updates (monthly ‚Üí weekly) improving accuracy 3% per quarter ($1.5M/year from yield optimization)\n",
    "- Prevent bad model deployments: automated validation catches accuracy degradation pre-production ($1M/year from prevented incidents)\n",
    "- Reduced data scientist labor: 80% less time on deployment tasks ($700K/year from 5 data scientists √ó 40% time savings √ó $175K salary)\n",
    "- **Total: $3.2M/year value**\n",
    "\n",
    "---\n",
    "\n",
    "### Project 2: **GitHub Actions Multi-Cloud CI/CD** üí∞ **$2.4M/year**\n",
    "**Objective:** Implement GitHub Actions workflows deploying to AWS, Azure, GCP with infrastructure provisioning, testing, and deployment automation.\n",
    "\n",
    "**Key Features:**\n",
    "- **Matrix Builds**: Test across Python 3.8-3.12, multiple databases (PostgreSQL, MySQL, SQLite), OS (Ubuntu, macOS, Windows) in parallel (15 min vs 2 hours sequential)\n",
    "- **Terraform Integration**: Provision infrastructure (VPC, EKS, RDS) in CI pipeline, validate with Terratest, deploy application, run tests, teardown ephemeral environments\n",
    "- **Multi-Cloud**: Deploy to AWS (primary), Azure (failover), GCP (analytics workloads) with cloud-specific optimizations\n",
    "- **Secrets Management**: Use GitHub Secrets + AWS Secrets Manager + Vault for secure credential handling (rotate every 30 days)\n",
    "- **Cost Optimization**: Auto-shutdown non-production environments after 6pm, use spot instances for CI runners (70% cost savings), cache dependencies (80% faster builds)\n",
    "\n",
    "**Business Value:**\n",
    "- 90% faster CI pipeline (45 min ‚Üí 4.5 min) with parallel matrix builds and caching\n",
    "- 50% infrastructure cost reduction ($800K/year from spot instances + auto-shutdown + resource optimization)\n",
    "- Multi-cloud resilience: 99.99% availability ($1M/year value from prevented downtime)\n",
    "- Reduced DevOps labor: 60% less manual deployment work ($600K/year from 4 DevOps engineers √ó $150K salary)\n",
    "- **Total: $2.4M/year value**\n",
    "\n",
    "---\n",
    "\n",
    "### Project 3: **Jenkins Pipeline for Legacy System Modernization** üí∞ **$1.9M/year**\n",
    "**Objective:** Migrate legacy monolithic STDF processing system to microservices with Jenkins CI/CD pipelines for each service.\n",
    "\n",
    "**Key Features:**\n",
    "- **Declarative Pipelines**: Jenkinsfile defines build, test, deploy stages for each microservice (20 services)\n",
    "- **Parallel Execution**: Run 20 service pipelines in parallel (reduces total build time 90%: 6 hours ‚Üí 35 min)\n",
    "- **Integration Testing**: Spin up full environment (20 services + databases + message queues) with Docker Compose, run end-to-end tests, teardown\n",
    "- **Blue-Green Deployment**: Maintain 2 production environments, deploy to inactive environment, smoke test, switch traffic, keep old environment for 24 hours (easy rollback)\n",
    "- **Monitoring Integration**: Post-deployment, trigger Grafana dashboard generation, set up alerts (error rate >1%, latency P95 >200ms), notify Slack\n",
    "\n",
    "**Business Value:**\n",
    "- 85% faster build-test-deploy cycle (6 hours ‚Üí 55 min) enabling daily releases vs weekly\n",
    "- Microservices architecture improves scalability: handle 5x more STDF files (200K ‚Üí 1M files/day) without infrastructure increase ($1.2M/year revenue from increased capacity)\n",
    "- Automated testing catches 95% of bugs pre-production ($500K/year from prevented incidents)\n",
    "- Reduced manual deployment labor: 90% less time ($200K/year from 2 DevOps engineers √ó 80 hours/month ‚Üí 8 hours/month)\n",
    "- **Total: $1.9M/year value**\n",
    "\n",
    "---\n",
    "\n",
    "### Project 4: **GitLab CI/CD with Auto-Scaling Runners** üí∞ **$1.6M/year**\n",
    "**Objective:** Build GitLab CI/CD platform with Kubernetes-based auto-scaling runners supporting 500+ pipelines/day.\n",
    "\n",
    "**Key Features:**\n",
    "- **Auto-Scaling Runners**: Kubernetes spawns GitLab runners on-demand (0 runners idle, scale to 50 runners during peak hours), shut down after job completion (5 min idle timeout)\n",
    "- **Docker-in-Docker**: Each pipeline runs in isolated container with Docker daemon (build Docker images, run integration tests with test databases)\n",
    "- **Artifact Caching**: Cache Python packages, npm modules, Docker layers across pipelines (reduces build time 70%: 12 min ‚Üí 3.6 min)\n",
    "- **Dynamic Environments**: For each merge request, create ephemeral environment with unique URL (review-app-mr-123.example.com), teardown after merge\n",
    "- **Pipeline Templates**: Reusable templates for Python, Node.js, Java projects (standardize across 100+ projects)\n",
    "\n",
    "**Business Value:**\n",
    "- 80% infrastructure cost savings (always-on runners ‚Üí auto-scaling on-demand runners: $15K/month ‚Üí $3K/month)\n",
    "- 70% faster pipelines ($800K/year from developer productivity: 200 developers √ó 1 hour/day saved √ó $200/hour)\n",
    "- Review apps enable 90% faster code reviews ($500K/year from 10 reviewers √ó 2 hours/day saved √ó $200/hour)\n",
    "- Standardized pipelines reduce maintenance burden 85% ($160K/year from 1 DevOps engineer √ó 80% time saved √ó $200K salary)\n",
    "- **Total: $1.6M/year value**\n",
    "\n",
    "---\n",
    "\n",
    "### Project 5: **Automated Compliance & Security Gates** üí∞ **$1.4M/year**\n",
    "**Objective:** Integrate security scanning, compliance checks, and approval gates into CI/CD pipelines for SOC2/ISO27001 compliance.\n",
    "\n",
    "**Key Features:**\n",
    "- **Security Scans**: SAST (static analysis with SonarQube), dependency scan (Snyk for CVEs), container scan (Trivy for Docker images), secrets detection (GitGuardian for API keys in code)\n",
    "- **Compliance Gates**: Require manual approval from security team for production deployments, automated audit logs (who deployed what when), immutable artifact signing (Cosign for container images)\n",
    "- **Policy as Code**: Open Policy Agent (OPA) enforces deployment policies (prod deployments only from main branch, require security scan pass, minimum 2 approvals for infrastructure changes)\n",
    "- **Audit Trail**: All pipeline executions logged to S3 (7-year retention), searchable with Elasticsearch, generate compliance reports (who accessed what resources)\n",
    "\n",
    "**Business Value:**\n",
    "- Prevent security vulnerabilities: SAST catches 80% of security issues pre-production ($900K/year from prevented breaches)\n",
    "- Automated compliance reduces audit preparation 95% ($350K/year from 3 weeks ‚Üí 1 day)\n",
    "- Policy enforcement prevents misconfigurations ($100K/year from prevented incidents)\n",
    "- Audit trail meets SOC2/ISO27001 requirements ($50K/year value from passing audits)\n",
    "- **Total: $1.4M/year value**\n",
    "\n",
    "---\n",
    "\n",
    "### Project 6: **ML Model A/B Testing Pipeline** üí∞ **$1.3M/year**\n",
    "**Objective:** Build automated A/B testing framework for comparing ML model versions in production with statistical significance testing.\n",
    "\n",
    "**Key Features:**\n",
    "- **Traffic Splitting**: Route 10% traffic to model A, 10% to model B, 80% to current production model (3-way split)\n",
    "- **Metric Collection**: Log predictions, ground truth (when available), latency, confidence scores for each model version\n",
    "- **Statistical Testing**: After 10K predictions per model ‚Üí run t-test for accuracy difference, Mann-Whitney U test for latency ‚Üí determine winner with 95% confidence\n",
    "- **Auto-Promotion**: If model B significantly better (p-value <0.05, accuracy improvement >1%) ‚Üí auto-promote to 100% traffic\n",
    "- **Multi-Metric Optimization**: Optimize for accuracy AND latency AND cost (weighted score: 0.6 √ó accuracy + 0.3 √ó (1/latency) + 0.1 √ó (1/cost))\n",
    "\n",
    "**Business Value:**\n",
    "- Data-driven model selection: choose best model with statistical confidence ($800K/year from 2% accuracy improvement via A/B testing)\n",
    "- Automated winner selection reduces experiment duration 80% (2 weeks ‚Üí 3 days) ($350K/year from faster iteration)\n",
    "- Multi-metric optimization balances accuracy and cost ($150K/year from 20% cost reduction while maintaining accuracy)\n",
    "- **Total: $1.3M/year value**\n",
    "\n",
    "---\n",
    "\n",
    "### Project 7: **Feature Flag-Based Deployment** üí∞ **$1.1M/year**\n",
    "**Objective:** Implement feature flags (LaunchDarkly/Unleash) for decoupling deployment from release, enabling instant rollback and gradual rollout.\n",
    "\n",
    "**Key Features:**\n",
    "- **Feature Toggles**: Control feature visibility via configuration (no code deployment required), enable/disable features in real-time (<1 second propagation)\n",
    "- **Gradual Rollout**: Enable feature for 5% users ‚Üí 25% ‚Üí 100% over hours/days (independent of deployment schedule)\n",
    "- **Targeting Rules**: Enable features for specific user segments (internal employees, beta testers, premium customers)\n",
    "- **Instant Rollback**: Disable feature flag if issues detected (no deployment rollback required, <5 second rollback)\n",
    "- **Experimentation**: Run A/B tests with feature flags (50% see feature A, 50% see feature B), collect metrics, determine winner\n",
    "\n",
    "**Business Value:**\n",
    "- Instant rollback (5 seconds vs 10 minutes deployment rollback) reduces downtime 95% ($700K/year from prevented downtime)\n",
    "- Decouple deployment from release: deploy daily, release weekly ($250K/year from faster iteration)\n",
    "- Reduced risk: gradual rollout catches issues early ($100K/year from prevented incidents)\n",
    "- A/B testing improves conversion rates 15% ($50K/year from better feature decisions)\n",
    "- **Total: $1.1M/year value**\n",
    "\n",
    "---\n",
    "\n",
    "### Project 8: **Database Migration Pipeline** üí∞ **$950K/year**\n",
    "**Objective:** Automate database schema migrations with zero-downtime deployments and automated rollback for production databases.\n",
    "\n",
    "**Key Features:**\n",
    "- **Schema Versioning**: Track schema changes with Flyway/Liquibase (version 1.0 ‚Üí 1.1 ‚Üí 1.2), atomic migrations (all-or-nothing)\n",
    "- **Backward Compatibility**: Ensure schema changes backward compatible (new code works with old schema, old code works with new schema) for zero-downtime deployments\n",
    "- **Blue-Green Database**: Replicate production database to green environment, apply migrations, switch read/write traffic, keep blue for rollback (24 hour window)\n",
    "- **Rollback Strategy**: For each migration, write rollback script (tested in CI), enable instant rollback if issues detected\n",
    "- **Testing**: Run migrations against production clone in CI, validate data integrity (row counts, foreign keys, constraints), performance test (query latency <100ms)\n",
    "\n",
    "**Business Value:**\n",
    "- Zero downtime deployments vs 4 hour maintenance windows ($650K/year from 99.99% availability)\n",
    "- Automated testing prevents 90% of migration failures ($200K/year from prevented incidents)\n",
    "- Faster migration execution (4 hours ‚Üí 15 min) enables weekly schema changes vs quarterly ($100K/year from faster iteration)\n",
    "- **Total: $950K/year value**\n",
    "\n",
    "---\n",
    "\n",
    "## üí∞ **Total Project Value: $13.85M/year**\n",
    "**Average ROI: 520% (infrastructure + labor costs ~$2.3M/year, value $13.85M/year)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc338f5f",
   "metadata": {},
   "source": [
    "## 5. üéØ Comprehensive Takeaways: CI/CD Mastery\n",
    "\n",
    "### **Core Concepts**\n",
    "\n",
    "**Continuous Integration (CI):**\n",
    "- ‚úÖ **Automated build** on every commit (compile code, build Docker image, package artifacts)\n",
    "- ‚úÖ **Automated testing** (unit tests, integration tests, contract tests) with >80% coverage\n",
    "- ‚úÖ **Quality gates** (linting, code coverage, security scan) prevent bad code from merging\n",
    "- ‚úÖ **Fast feedback** (5-10 minute pipeline) enables developers to fix issues immediately\n",
    "\n",
    "**Continuous Deployment (CD):**\n",
    "- ‚úÖ **Deployment strategies** (blue-green, canary, rolling) enable safe, zero-downtime deployments\n",
    "- ‚úÖ **Automated validation** (smoke tests, health checks, metric monitoring) ensures deployment success\n",
    "- ‚úÖ **Automated rollback** (triggered on metric degradation) reduces downtime from hours to seconds\n",
    "- ‚úÖ **Progressive delivery** (gradual traffic shifting) catches issues affecting <5% users vs 100%\n",
    "\n",
    "**CI/CD Tools:**\n",
    "- ‚úÖ **GitHub Actions**: Cloud-native, tight GitHub integration, matrix builds, marketplace actions\n",
    "- ‚úÖ **Jenkins**: Self-hosted, highly customizable, mature ecosystem, declarative pipelines\n",
    "- ‚úÖ **GitLab CI**: Built-in GitLab, auto-scaling runners, dynamic environments, artifact caching\n",
    "\n",
    "---\n",
    "\n",
    "### **Best Practices**\n",
    "\n",
    "**CI Pipeline Design:**\n",
    "- ‚úÖ **Fast feedback loop**: Keep CI pipeline <10 minutes (run fast tests first, slow tests in parallel)\n",
    "- ‚úÖ **Fail fast**: Run linting and unit tests before expensive integration tests or builds\n",
    "- ‚úÖ **Reproducible builds**: Use Docker for consistent build environment (same Python version, dependencies, OS across dev/CI/prod)\n",
    "- ‚úÖ **Parallel execution**: Run independent stages in parallel (test Python 3.8, 3.9, 3.10 simultaneously)\n",
    "- ‚úÖ **Artifact versioning**: Tag artifacts with commit SHA (enables traceability and rollback to any version)\n",
    "- ‚úÖ **Caching**: Cache dependencies (Python packages, npm modules, Docker layers) to reduce build time 70%\n",
    "\n",
    "**CD Pipeline Design:**\n",
    "- ‚úÖ **Environment parity**: Dev, staging, production should be identical (same OS, Python version, dependencies, configuration)\n",
    "- ‚úÖ **Immutable infrastructure**: Never modify running servers (always deploy new version, teardown old)\n",
    "- ‚úÖ **Health checks**: Every deployment must pass health check before receiving traffic (HTTP 200 on /health endpoint)\n",
    "- ‚úÖ **Smoke tests**: Validate critical functionality post-deployment (can users login? Can API serve predictions?)\n",
    "- ‚úÖ **Monitoring integration**: Auto-create Grafana dashboards, set up alerts, notify Slack on deployment\n",
    "- ‚úÖ **Rollback strategy**: Every deployment must have rollback plan (blue-green: switch traffic back, canary: reduce traffic to 0%)\n",
    "\n",
    "**Deployment Strategies:**\n",
    "- ‚úÖ **Blue-Green**: Best for major version upgrades, database schema changes (instant cutover, instant rollback)\n",
    "- ‚úÖ **Canary**: Best for ML models, performance-critical services (gradual rollout catches issues early)\n",
    "- ‚úÖ **Rolling**: Best for stateless services, Kubernetes deployments (incremental replacement, no extra infrastructure)\n",
    "- ‚úÖ **Traffic shifting**: Start with 5% traffic, monitor metrics (error rate, latency), increase gradually (5% ‚Üí 25% ‚Üí 50% ‚Üí 100%)\n",
    "- ‚úÖ **Monitoring window**: Monitor each phase for 10-15 minutes (enough data to detect issues with statistical significance)\n",
    "\n",
    "**Security & Compliance:**\n",
    "- ‚úÖ **Secrets management**: Never commit secrets to Git (use GitHub Secrets, AWS Secrets Manager, Vault)\n",
    "- ‚úÖ **Least privilege**: CI/CD runners should have minimal permissions (deploy to staging, not delete production)\n",
    "- ‚úÖ **Approval gates**: Require manual approval for production deployments (security team review, compliance check)\n",
    "- ‚úÖ **Audit trail**: Log all pipeline executions (who deployed what when, with what changes) for compliance\n",
    "- ‚úÖ **Vulnerability scanning**: Scan code (SAST with SonarQube), dependencies (Snyk for CVEs), containers (Trivy for Docker images)\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Patterns**\n",
    "\n",
    "**Multi-Environment Deployment:**\n",
    "- Deploy to dev (automatic) ‚Üí staging (automatic) ‚Üí pre-prod (automatic) ‚Üí production (manual approval)\n",
    "- Use infrastructure as code (Terraform) to create ephemeral test environments (spin up for each PR, teardown after merge)\n",
    "\n",
    "**Feature Flags:**\n",
    "- Decouple deployment from release (deploy code with feature disabled, enable feature via flag later)\n",
    "- Gradual rollout (enable feature for 5% users ‚Üí 25% ‚Üí 100% without redeploying code)\n",
    "- Instant rollback (disable feature flag in <5 seconds vs 10 minute deployment rollback)\n",
    "\n",
    "**Database Migrations:**\n",
    "- Ensure backward compatibility (new code works with old schema, old code works with new schema)\n",
    "- Use blue-green database strategy (replicate database, apply migration, switch traffic, keep old for rollback)\n",
    "- Test migrations in CI against production clone (validate data integrity, performance)\n",
    "\n",
    "**Multi-Cloud Deployment:**\n",
    "- Deploy to multiple clouds for resilience (AWS primary, Azure failover, GCP analytics)\n",
    "- Use cloud-agnostic tools (Kubernetes, Terraform) to avoid vendor lock-in\n",
    "- Implement health-based DNS routing (Route53 health checks route traffic to healthy cloud)\n",
    "\n",
    "**Pipeline as Code:**\n",
    "- Store pipeline definitions in Git (Jenkinsfile, .github/workflows/ci.yml, .gitlab-ci.yml)\n",
    "- Version control pipeline changes (track who changed what when)\n",
    "- Reusable pipeline templates (standardize across 100+ projects)\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Pitfalls**\n",
    "\n",
    "**CI Mistakes:**\n",
    "- ‚ùå **Flaky tests**: Tests fail randomly (race conditions, timing issues) ‚Üí Fix tests or disable, don't ignore failures\n",
    "- ‚ùå **Slow pipelines**: 1 hour CI pipeline ‚Üí developers bypass CI ‚Üí Use parallelism, caching, fast tests\n",
    "- ‚ùå **No quality gates**: Merge code with <50% coverage ‚Üí Set minimum thresholds (coverage >80%, linting pass)\n",
    "- ‚ùå **Building in production**: Compiling code on production servers ‚Üí Build artifacts in CI, deploy immutable artifacts\n",
    "\n",
    "**CD Mistakes:**\n",
    "- ‚ùå **Big bang deployment**: Deploy to 100% traffic immediately ‚Üí Use canary deployment (5% ‚Üí 100%)\n",
    "- ‚ùå **No rollback plan**: Deployment fails, takes 2 hours to rollback ‚Üí Automate rollback, test rollback in CI\n",
    "- ‚ùå **Manual deployments**: Click buttons in Jenkins UI ‚Üí Automate with pipeline as code\n",
    "- ‚ùå **Environment drift**: Dev uses Python 3.12, prod uses Python 3.8 ‚Üí Use Docker for environment parity\n",
    "\n",
    "**Deployment Strategy Mistakes:**\n",
    "- ‚ùå **Canary without monitoring**: Deploy to 5% traffic but don't check metrics ‚Üí Auto-monitor error rate, latency\n",
    "- ‚ùå **Blue-green without smoke tests**: Switch all traffic to green without validation ‚Üí Run smoke tests first\n",
    "- ‚ùå **Rolling update too fast**: Replace all pods in 30 seconds ‚Üí Stagger updates (1 pod every 2 minutes)\n",
    "\n",
    "**Security Mistakes:**\n",
    "- ‚ùå **Secrets in code**: Commit AWS keys to Git ‚Üí Use secrets management tools\n",
    "- ‚ùå **CI runners with admin access**: Runner can delete production ‚Üí Use least privilege (deploy only)\n",
    "- ‚ùå **No vulnerability scanning**: Deploy dependencies with known CVEs ‚Üí Run security scan in CI\n",
    "- ‚ùå **No approval gates**: Automatic production deployment ‚Üí Require manual approval for prod\n",
    "\n",
    "---\n",
    "\n",
    "### **Production Checklist**\n",
    "\n",
    "**Before deploying CI/CD to production:**\n",
    "- ‚úÖ **CI Pipeline**: Build, test (>80% coverage), quality (linting, type checking), security scan (<10 min total)\n",
    "- ‚úÖ **CD Pipeline**: Deploy to staging, smoke tests, deploy to production (canary 5% ‚Üí 100%)\n",
    "- ‚úÖ **Automated Rollback**: Trigger on error rate >1% OR latency P95 >200ms OR failed smoke tests\n",
    "- ‚úÖ **Monitoring**: Grafana dashboards, alerts (Slack, PagerDuty), metrics collection (Prometheus)\n",
    "- ‚úÖ **Secrets Management**: All secrets in vault (GitHub Secrets, AWS Secrets Manager), rotate every 30 days\n",
    "- ‚úÖ **Approval Gates**: Manual approval for production deployments (security team, product manager)\n",
    "- ‚úÖ **Audit Trail**: All pipeline executions logged (who deployed what when), searchable (Elasticsearch)\n",
    "- ‚úÖ **Documentation**: Runbooks for common issues (deployment failure, rollback procedure, troubleshooting)\n",
    "- ‚úÖ **Testing**: CI pipeline tested with real code, CD pipeline tested with canary deployment\n",
    "- ‚úÖ **Backup Strategy**: Database backups before migrations, artifact retention (30 days), rollback tested\n",
    "\n",
    "---\n",
    "\n",
    "### **Troubleshooting Guide**\n",
    "\n",
    "**Problem: CI pipeline slow (>30 minutes)**\n",
    "- Run tests in parallel (split test suite into 4 jobs, run simultaneously)\n",
    "- Cache dependencies (Docker layers, Python packages, npm modules)\n",
    "- Use faster test runners (pytest-xdist for parallel test execution)\n",
    "- Profile pipeline (identify slowest stage, optimize or parallelize)\n",
    "\n",
    "**Problem: Flaky tests (fail randomly)**\n",
    "- Identify flaky tests (tests that fail <5% of time)\n",
    "- Fix race conditions (add proper synchronization, wait for async operations)\n",
    "- Disable or quarantine flaky tests (don't let them block CI)\n",
    "- Use test retries as last resort (retry failed tests once, but fix root cause)\n",
    "\n",
    "**Problem: Deployment fails in production (but works in staging)**\n",
    "- Check environment parity (same Python version, dependencies, OS, configuration)\n",
    "- Validate production data (staging uses synthetic data, production has edge cases)\n",
    "- Enable debug logging (trace request flow, identify failure point)\n",
    "- Use blue-green deployment (deploy to green, validate, then switch traffic)\n",
    "\n",
    "**Problem: Rollback too slow (>10 minutes)**\n",
    "- Use blue-green deployment (instant traffic switch back to blue)\n",
    "- Pre-warm rollback environment (keep previous version running for 24 hours)\n",
    "- Automate rollback (trigger on metric degradation, don't wait for manual intervention)\n",
    "- Test rollback procedure in staging (ensure it works before you need it)\n",
    "\n",
    "**Problem: High deployment failure rate (>10%)**\n",
    "- Increase test coverage (aim for >90% with integration tests)\n",
    "- Add smoke tests (validate critical functionality post-deployment)\n",
    "- Use canary deployment (catch issues at 5% traffic vs 100%)\n",
    "- Implement pre-deployment validation (run subset of production traffic through staging)\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "**Immediate (Week 1):**\n",
    "- Set up basic CI pipeline (build ‚Üí test ‚Üí lint) for 1 project\n",
    "- Use GitHub Actions or GitLab CI (free tier, easy to start)\n",
    "- Achieve >80% test coverage with unit tests\n",
    "- Add security scan (Snyk for dependencies, bandit for Python code)\n",
    "\n",
    "**Short-term (1-3 months):**\n",
    "- Implement CD pipeline with staging and production environments\n",
    "- Add canary deployment strategy (5% ‚Üí 25% ‚Üí 50% ‚Üí 100%)\n",
    "- Set up monitoring (Grafana dashboards, Prometheus metrics, Slack alerts)\n",
    "- Automate rollback on metric degradation\n",
    "- Roll out CI/CD to all projects (10-20 projects)\n",
    "\n",
    "**Long-term (3-6 months):**\n",
    "- Multi-cloud deployment (AWS, Azure, GCP)\n",
    "- Advanced deployment strategies (feature flags, A/B testing, database migrations)\n",
    "- Compliance automation (SOC2, ISO27001 audit trail, policy enforcement)\n",
    "- ML-specific CI/CD (model training, validation, drift detection, A/B testing)\n",
    "- Pipeline optimization (reduce build time 90%, improve reliability to >99%)\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Metrics to Track**\n",
    "\n",
    "**CI Metrics:**\n",
    "- Pipeline duration: Target <10 minutes (fast feedback loop)\n",
    "- Pipeline success rate: Target >95% (few failures, mostly due to genuine bugs)\n",
    "- Test coverage: Target >80% (comprehensive testing)\n",
    "- Build frequency: Target >10 builds/day (developers commit frequently)\n",
    "\n",
    "**CD Metrics:**\n",
    "- Deployment frequency: Target >10 deployments/week (fast iteration)\n",
    "- Deployment duration: Target <15 minutes (from commit to production)\n",
    "- Deployment success rate: Target >98% (automated testing catches issues pre-deployment)\n",
    "- Mean time to recovery (MTTR): Target <10 minutes (automated rollback)\n",
    "- Change failure rate: Target <5% (changes causing production incidents)\n",
    "\n",
    "**Business Metrics:**\n",
    "- Lead time (commit to production): Target <2 hours (vs 2 days manual)\n",
    "- Developer productivity: Target 30% increase (less time on manual tasks)\n",
    "- Incident reduction: Target 80% fewer production incidents (automated testing)\n",
    "- Downtime reduction: Target 95% less downtime (automated rollback, canary deployment)\n",
    "\n",
    "---\n",
    "\n",
    "### üéì **Congratulations! You've Mastered CI/CD Pipelines!**\n",
    "\n",
    "You can now:\n",
    "- ‚úÖ **Build CI pipelines** with automated build, test, quality gates, and security scanning\n",
    "- ‚úÖ **Implement CD pipelines** with blue-green, canary, and rolling deployment strategies\n",
    "- ‚úÖ **Automate rollback** on metric degradation (error rate, latency, prediction distribution)\n",
    "- ‚úÖ **Deploy ML models** safely with shadow mode, A/B testing, and gradual rollout\n",
    "- ‚úÖ **Optimize pipelines** with parallelism, caching, and fast feedback loops\n",
    "- ‚úÖ **Ensure security** with secrets management, vulnerability scanning, and approval gates\n",
    "- ‚úÖ **Build production systems** with 95% faster deployments and 98% success rate\n",
    "\n",
    "**Next Notebook:** 142_Cloud_Platforms - AWS, Azure, and GCP deployment strategies for ML systems üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc4e2a",
   "metadata": {},
   "source": [
    "## üìä Diagnostic Checks Summary\n",
    "\n",
    "**Implementation Checklist:**\n",
    "- ‚úÖ CI pipeline configuration (GitHub Actions/GitLab CI with test stages)\n",
    "- ‚úÖ Automated testing (unit tests, integration tests, model validation)\n",
    "- ‚úÖ Data validation gates (schema checks, distribution tests)\n",
    "- ‚úÖ CD pipeline stages (build ‚Üí test ‚Üí staging ‚Üí canary ‚Üí production)\n",
    "- ‚úÖ Rollback mechanisms (automated revert on metric degradation)\n",
    "- ‚úÖ Post-silicon use cases (automated STDF pipeline, model validation, cross-fab deployment)\n",
    "- ‚úÖ Real-world projects with ROI ($26M-$185M/year)\n",
    "\n",
    "**Quality Metrics Achieved:**\n",
    "- Deployment frequency: 10-20 deploys/day (vs 1-2/week manual)\n",
    "- Build time: <15 minutes (fast feedback loop)\n",
    "- Deployment success rate: 95%+ (automated testing catches errors)\n",
    "- Rollback time: <5 minutes (automated detection and revert)\n",
    "- Business impact: 80% fewer production incidents, 70% faster iteration\n",
    "\n",
    "**Post-Silicon Validation Applications:**\n",
    "- **Automated Test Data Pipeline:** STDF upload ‚Üí Parse ‚Üí Feature engineering ‚Üí Model train ‚Üí Deploy (end-to-end in 2 hours)\n",
    "- **Model Validation Gates:** Accuracy >85% + Shadow mode validation ‚Üí Canary (10%) ‚Üí Full rollout (prevents bad models)\n",
    "- **Cross-Fab Deployment:** Train in Fab A ‚Üí Automated testing in Fab B staging ‚Üí Validation ‚Üí Production (multi-site consistency)\n",
    "\n",
    "**Business ROI:**\n",
    "- Faster deployment: 70% faster iteration √ó $5M/year = **$3.5M/year**\n",
    "- Fewer production incidents: 80% reduction √ó $8M/year = **$6.4M/year**\n",
    "- Automated test data pipeline: Daily updates = **$8M-$15M/year** yield improvement\n",
    "- Safe experimentation: Continuous test optimization = **$10M-$25M/year**\n",
    "- **Total value:** $27.9M-$49.9M/year per fab (risk-adjusted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf93eb3e",
   "metadata": {},
   "source": [
    "## üîë Key Takeaways\n",
    "\n",
    "**When to Use CI/CD for ML:**\n",
    "- Multiple team members deploying models (avoid manual coordination)\n",
    "- Frequent model updates (daily/weekly retraining requires automation)\n",
    "- Safety-critical applications (automated testing reduces human error)\n",
    "- Multi-environment deployments (dev ‚Üí staging ‚Üí production consistency)\n",
    "\n",
    "**Limitations:**\n",
    "- Initial setup complexity (Jenkins, GitLab CI, GitHub Actions configuration)\n",
    "- Requires test coverage (untested code defeats purpose of CI/CD)\n",
    "- Pipeline maintenance overhead (YAML/config drift, dependency updates)\n",
    "- Slower feedback than manual deploy (full pipeline may take 20-60 minutes)\n",
    "\n",
    "**Alternatives:**\n",
    "- **Manual deployment** (acceptable for small teams, infrequent updates)\n",
    "- **Scripted deployment** (bash scripts, no automated testing)\n",
    "- **Notebook-based workflows** (Papermill for parameterized notebook execution)\n",
    "- **Cloud-native ML platforms** (SageMaker Pipelines, Vertex AI - higher abstraction)\n",
    "\n",
    "**Best Practices:**\n",
    "- Implement comprehensive testing (unit, integration, model validation, data quality)\n",
    "- Use feature flags for gradual rollouts (enable new model for 10% traffic)\n",
    "- Automate rollback triggers (revert if accuracy drops >5% or latency >200ms)\n",
    "- Version everything (code, data, models, configs - immutable artifacts)\n",
    "- Monitor pipeline health (track build times, failure rates, deployment frequency)\n",
    "- Use staging environments (test in prod-like setup before real production)\n",
    "\n",
    "**Next Steps:**\n",
    "- 136: CI/CD ML Pipelines (MLOps-specific patterns like model registries)\n",
    "- 154: Model Monitoring & Observability (track deployed model performance)\n",
    "- 128: Shadow Mode Deployment (safe validation strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b3627",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "**When to Use CI/CD Pipelines:**\n",
    "- ‚úÖ **Automated testing** - Run unit/integration/E2E tests on every commit (GitHub Actions, GitLab CI, Jenkins)\n",
    "- ‚úÖ **Deployment consistency** - Eliminate \"works on my machine\" with containerized builds\n",
    "- ‚úÖ **Fast feedback** - Detect bugs in <10 minutes vs. manual testing (hours/days)\n",
    "- ‚úÖ **Rollback safety** - Blue/green deployments allow instant rollback on failures\n",
    "- ‚úÖ **Compliance** - Audit trail for all code changes ‚Üí production (SOC 2, HIPAA requirements)\n",
    "\n",
    "**Limitations:**\n",
    "- ‚ùå Build time overhead (10-30 minutes for full CI/CD pipeline vs. instant manual deploy)\n",
    "- ‚ùå Complexity for small teams (CI/CD setup takes 2-4 weeks initially)\n",
    "- ‚ùå Flaky tests slow pipelines (1 unstable test blocks entire deployment)\n",
    "- ‚ùå Infrastructure costs ($200-500/month for CI runners, artifact storage)\n",
    "- ‚ùå Learning curve for YAML/Groovy DSL (GitHub Actions, Jenkins pipelines)\n",
    "\n",
    "**Alternatives:**\n",
    "- **Manual deployment** - SSH + git pull for small projects (not scalable, error-prone)\n",
    "- **Script-based deploy** - Bash scripts with rsync/scp (no rollback, no audit trail)\n",
    "- **Serverless platforms** - AWS Lambda auto-deploy from S3 (limited to serverless architectures)\n",
    "- **Platform-as-a-Service** - Heroku git push (abstracts CI/CD but less control)\n",
    "\n",
    "**Best Practices:**\n",
    "- **Test pyramid** - Many unit tests (fast, 1-2 sec), fewer integration tests (5-10 sec), few E2E tests (1-2 min)\n",
    "- **Parallel stages** - Run linting, unit tests, security scans concurrently (reduce pipeline time 50%)\n",
    "- **Artifact caching** - Cache npm/pip/maven dependencies (save 5-10 min per build)\n",
    "- **Semantic versioning** - Auto-increment versions based on commit messages (major.minor.patch)\n",
    "- **Environment parity** - Dev/staging/prod use identical Docker images (only config differs)\n",
    "- **Canary deployments** - Deploy to 5% traffic first, monitor for 15min, then full rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e123071",
   "metadata": {},
   "source": [
    "## üîç Diagnostic & Mastery + Progress\n",
    "\n",
    "### Implementation Checklist\n",
    "- ‚úÖ **GitHub Actions** - YAML workflows for build, test, deploy  \n",
    "- ‚úÖ **Docker** - Containerize app for consistent builds  \n",
    "- ‚úÖ **Automated tests** - Unit (pytest), integration (docker-compose), E2E (Selenium)  \n",
    "- ‚úÖ **Artifact storage** - Docker registry, S3 for model binaries  \n",
    "- ‚úÖ **Deployment** - Blue/green, canary, rolling updates  \n",
    "\n",
    "### Quality Metrics\n",
    "- **Build time**: <10 minutes for full pipeline (build + test + deploy)  \n",
    "- **Test coverage**: >80% code coverage for critical paths  \n",
    "- **Deployment frequency**: Multiple times per day (vs. weekly manual deploys)  \n",
    "- **Mean time to recovery**: <15 minutes (instant rollback with blue/green)  \n",
    "\n",
    "### Post-Silicon Application\n",
    "**Automated Binning Model Deployment**  \n",
    "- **Input**: New binning model trained weekly on latest ATE data  \n",
    "- **Solution**: CI/CD pipeline auto-tests (accuracy >95%), packages Docker image, deploys to staging (canary 10% traffic), then production  \n",
    "- **Value**: Deploy 2x/week vs. 1x/month manual (catch process drift faster), save $340K/year (4 SRE-days/month √ó $150K salary)  \n",
    "\n",
    "### ROI: $340K-$680K/year (medium team), $1.4M-$2.7M/year (large team)  \n",
    "\n",
    "‚úÖ Build automated CI/CD pipelines with GitHub Actions/Jenkins  \n",
    "‚úÖ Implement blue/green deployments for zero-downtime  \n",
    "‚úÖ Apply to semiconductor ML model deployment  \n",
    "\n",
    "**Session**: 44/60 notebooks done (73.3%) | **Overall**: ~154/175 complete (88%)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
