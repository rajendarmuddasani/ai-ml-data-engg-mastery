{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36ad995",
   "metadata": {},
   "source": [
    "# 142: Cloud Platforms - AWS, Azure, and GCP for ML Systems\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** cloud platform comparison for ML workloads (AWS, Azure, GCP strengths/weaknesses)\n",
    "- **Implement** AWS SageMaker end-to-end ML pipeline (training, deployment, monitoring)\n",
    "- **Build** Azure ML workspace with AutoML and managed endpoints\n",
    "- **Deploy** GCP Vertex AI models with feature store and prediction serving\n",
    "- **Apply** cloud ML services to semiconductor systems (STDF processing, yield prediction, batch inference)\n",
    "- **Optimize** cloud costs with spot instances, reserved capacity, and serverless\n",
    "\n",
    "## üìö What are Cloud ML Platforms?\n",
    "\n",
    "**Cloud ML platforms** provide **managed services** for ML workflows (data storage, training, deployment, monitoring) without infrastructure management. Focus on models, not servers.\n",
    "\n",
    "**Why Cloud ML Platforms?**\n",
    "- ‚úÖ **Managed infrastructure**: No server provisioning, patching, scaling (cloud handles it)\n",
    "- ‚úÖ **Elastic scaling**: Scale from 1 to 1000 GPUs instantly (pay per second, no upfront costs)\n",
    "- ‚úÖ **Pre-built integrations**: Connect storage, databases, monitoring seamlessly\n",
    "- ‚úÖ **Faster iteration**: Deploy models in minutes vs weeks (infrastructure abstraction)\n",
    "\n",
    "**Cloud Platform Comparison:**\n",
    "\n",
    "| Feature | AWS | Azure | GCP |\n",
    "|---------|-----|-------|-----|\n",
    "| **ML Platform** | SageMaker | Azure ML | Vertex AI |\n",
    "| **Auto-scaling** | Excellent (Application Auto Scaling) | Good (VM Scale Sets) | Excellent (GKE Autopilot) |\n",
    "| **Pricing** | Pay-per-second compute | Per-minute billing | Per-second (most granular) |\n",
    "| **GPU Availability** | Best (P3, P4, Inferentia chips) | Good (NC, ND series) | Best (A100, TPU v4) |\n",
    "| **Serverless ML** | Lambda (15-min limit) | Functions (10-min limit) | Cloud Run (60-min limit) |\n",
    "| **Feature Store** | SageMaker Feature Store | Azure Feature Store (preview) | Vertex AI Feature Store |\n",
    "| **Best For** | Enterprise (broadest services) | Microsoft shops (.NET, SQL Server) | Data/AI startups (cutting-edge ML) |\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "### **Use Case 1: AWS SageMaker Batch Transform for Wafer Yield Prediction**\n",
    "**Input:** 100K wafers/night requiring yield predictions (GPU inference for Random Forest on parametric data)  \n",
    "**Output:** SageMaker Batch Transform auto-scales to 50 instances, completes in 2 hours vs 12 hours on single GPU  \n",
    "**Value:** $5.2M/year from faster processing (submit lot dispositioning 10 hours earlier, optimize fab utilization)\n",
    "\n",
    "### **Use Case 2: Azure ML AutoML for Test Coverage Optimization**\n",
    "**Input:** Engineers manually tune XGBoost hyperparameters for test coverage model (2 weeks per iteration)  \n",
    "**Output:** Azure AutoML tries 100 configurations in 8 hours, achieves 12% better accuracy than manual tuning  \n",
    "**Value:** $4.1M/year from improved model quality (skip more unnecessary tests safely, reduce test time 15%)\n",
    "\n",
    "### **Use Case 3: GCP Vertex AI Feature Store for Real-Time Parametric Features**\n",
    "**Input:** Parametric test features (voltage, current, frequency) computed on-demand per prediction (100ms latency)  \n",
    "**Output:** Vertex AI Feature Store caches features with 10ms lookup, reduces P95 latency 80% (100ms ‚Üí 20ms)  \n",
    "**Value:** $3.6M/year from improved throughput (serve 5x more predictions/second, enable real-time binning decisions)\n",
    "\n",
    "### **Use Case 4: AWS Spot Instances for ML Model Training**\n",
    "**Input:** Training yield prediction models on-demand EC2 (p3.8xlarge $12.24/hour, 20 hours/week = $12,730/year)  \n",
    "**Output:** Spot instances with checkpointing save 70% (p3.8xlarge spot $3.67/hour = $3,819/year)  \n",
    "**Value:** $2.9M/year from reduced training costs (train 3.3x more models for same budget, faster experimentation)\n",
    "\n",
    "**Total Post-Silicon Value:** $5.2M + $4.1M + $3.6M + $2.9M = **$15.8M/year**\n",
    "\n",
    "## üîÑ Cloud ML Platform Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[üìä Upload Data] --> B[‚òÅÔ∏è S3/Blob/GCS]\n",
    "    B --> C[üîß Data Processing]\n",
    "    C --> D[üèãÔ∏è Model Training]\n",
    "    D --> E[‚úÖ Model Validation]\n",
    "    E --> F{Accuracy OK?}\n",
    "    \n",
    "    F -->|No| G[üîÑ Tune Hyperparameters]\n",
    "    F -->|Yes| H[üì¶ Model Registry]\n",
    "    \n",
    "    G --> D\n",
    "    H --> I[üöÄ Deploy Endpoint]\n",
    "    I --> J[üìà Monitor Performance]\n",
    "    J --> K{Drift Detected?}\n",
    "    \n",
    "    K -->|Yes| L[‚ö†Ô∏è Trigger Retraining]\n",
    "    K -->|No| M[‚úÖ Serve Predictions]\n",
    "    \n",
    "    L --> C\n",
    "    M --> N[üí∞ Track Costs]\n",
    "    N --> O[üìä Optimize Pricing]\n",
    "    O --> P{Spot/Reserved?}\n",
    "    \n",
    "    P -->|Spot| Q[70% Savings]\n",
    "    P -->|Reserved| R[40% Savings]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style M fill:#e1ffe1\n",
    "    style F fill:#fff4e1\n",
    "    style K fill:#fff4e1\n",
    "    style Q fill:#ccffcc\n",
    "    style R fill:#ccffcc\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **Notebook 122: MLflow** - Model registry and experiment tracking (deploy to cloud)\n",
    "- **Notebook 124: Feature Stores** - Feature engineering for cloud ML platforms\n",
    "\n",
    "**Next Steps:**\n",
    "- **Notebook 144: Performance Optimization** - Optimize cloud ML inference latency\n",
    "- **Notebook 145: Cost Optimization** - Right-sizing, spot instances, reserved capacity\n",
    "\n",
    "---\n",
    "\n",
    "Let's build scalable ML systems on cloud platforms! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5991a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "from enum import Enum\n",
    "import hashlib\n",
    "import uuid\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6652607d",
   "metadata": {},
   "source": [
    "## 2. ‚òÅÔ∏è AWS - Complete ML Infrastructure\n",
    "\n",
    "**Purpose:** Build production ML infrastructure on AWS with SageMaker, EC2, S3, Lambda, and managed services.\n",
    "\n",
    "**AWS Core Services:**\n",
    "- **Compute**: EC2 (virtual machines), Lambda (serverless functions), ECS/EKS (containers), Batch (batch processing)\n",
    "- **Storage**: S3 (object storage, 99.999999999% durability), EBS (block storage for EC2), EFS (shared file system)\n",
    "- **ML Platform**: SageMaker (managed ML training/deployment), Rekognition (computer vision), Comprehend (NLP)\n",
    "- **Database**: RDS (PostgreSQL, MySQL), DynamoDB (NoSQL), Redshift (data warehouse)\n",
    "- **Analytics**: Athena (SQL on S3), Glue (ETL), EMR (managed Spark), Kinesis (real-time streaming)\n",
    "\n",
    "**SageMaker ML Workflow:**\n",
    "\n",
    "1. **Data Preparation**: Store STDF files in S3 ‚Üí Glue ETL parses files ‚Üí Write features to S3 Parquet\n",
    "2. **Training**: SageMaker training job on ml.p3.8xlarge (4 GPUs) ‚Üí Spot instances (70% discount) ‚Üí Model artifact saved to S3\n",
    "3. **Deployment**: SageMaker endpoint with auto-scaling (1-10 instances) ‚Üí API Gateway for REST API ‚Üí Lambda for pre/post-processing\n",
    "4. **Monitoring**: CloudWatch metrics (latency, error rate) ‚Üí SageMaker Model Monitor (data drift) ‚Üí SNS alerts to Slack\n",
    "\n",
    "**Cost Optimization:**\n",
    "- **Spot Instances**: 70% discount for training jobs (terminable with 2-min warning)\n",
    "- **Auto-Scaling**: Scale down to 1 instance during low traffic (vs always-on 10 instances)\n",
    "- **S3 Lifecycle**: Move old STDF files to Glacier after 90 days (90% storage cost savings)\n",
    "- **Reserved Instances**: 1-year commitment for production endpoints (40% discount vs on-demand)\n",
    "\n",
    "**Why AWS?**\n",
    "- **Maturity**: 18 years old (vs Azure 14 years, GCP 10 years), largest service catalog (200+ services)\n",
    "- **SageMaker**: Best-in-class ML platform (managed training, deployment, monitoring, feature store)\n",
    "- **Ecosystem**: Largest community, most third-party integrations, extensive documentation\n",
    "- **Global reach**: 25+ regions, 80+ availability zones (more than Azure/GCP)\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "\n",
    "**Scenario:** Train yield prediction model on 1M STDF records (10GB compressed, 50GB uncompressed). Deploy model for 200K predictions/day with <100ms P95 latency.\n",
    "\n",
    "**AWS Architecture:**\n",
    "```\n",
    "STDF Upload ‚Üí S3 Bucket (stdf-raw-data)\n",
    "            ‚Üì\n",
    "S3 Event Notification ‚Üí Lambda Function (parse_stdf)\n",
    "            ‚Üì\n",
    "Lambda ‚Üí Glue ETL Job ‚Üí S3 Parquet (stdf-features)\n",
    "            ‚Üì\n",
    "SageMaker Training (ml.p3.8xlarge spot) ‚Üí Model Artifact (S3)\n",
    "            ‚Üì\n",
    "SageMaker Endpoint (ml.m5.xlarge, auto-scale 1-10) ‚Üí API Gateway\n",
    "            ‚Üì\n",
    "CloudWatch Metrics ‚Üí CloudWatch Alarm ‚Üí SNS ‚Üí Slack\n",
    "```\n",
    "\n",
    "**Cost Estimate (Monthly):**\n",
    "- S3 Storage: 50GB √ó $0.023/GB = $1.15\n",
    "- Lambda: 200K invocations √ó $0.20/1M = $0.04\n",
    "- Glue ETL: 10 hours/month √ó $0.44/hour = $4.40\n",
    "- SageMaker Training: 2 hours/day √ó $4.10/hour (spot) √ó 30 days = $246\n",
    "- SageMaker Endpoint: ml.m5.xlarge √ó $0.192/hour √ó 730 hours √ó 3 instances (avg) = $420\n",
    "- **Total: $672/month** (vs $5K/month on-premises with 10 GPU servers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0118d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS ML Infrastructure Simulation\n",
    "\n",
    "class AWSService(Enum):\n",
    "    \"\"\"AWS service types\"\"\"\n",
    "    S3 = \"s3\"\n",
    "    LAMBDA = \"lambda\"\n",
    "    SAGEMAKER_TRAINING = \"sagemaker_training\"\n",
    "    SAGEMAKER_ENDPOINT = \"sagemaker_endpoint\"\n",
    "    GLUE = \"glue\"\n",
    "    CLOUDWATCH = \"cloudwatch\"\n",
    "\n",
    "class InstanceType(Enum):\n",
    "    \"\"\"EC2/SageMaker instance types\"\"\"\n",
    "    ML_M5_XLARGE = \"ml.m5.xlarge\"  # 4 vCPU, 16GB RAM, $0.192/hour\n",
    "    ML_P3_8XLARGE = \"ml.p3.8xlarge\"  # 32 vCPU, 244GB RAM, 4 GPUs, $12.24/hour\n",
    "    ML_P3_8XLARGE_SPOT = \"ml.p3.8xlarge_spot\"  # 70% discount, $4.10/hour\n",
    "\n",
    "@dataclass\n",
    "class AWSCost:\n",
    "    \"\"\"AWS service cost tracking\"\"\"\n",
    "    service: AWSService\n",
    "    usage_hours: float\n",
    "    instance_type: Optional[InstanceType] = None\n",
    "    storage_gb: float = 0.0\n",
    "    requests: int = 0\n",
    "    \n",
    "    def calculate_cost(self) -> float:\n",
    "        \"\"\"Calculate cost based on usage\"\"\"\n",
    "        if self.service == AWSService.S3:\n",
    "            return self.storage_gb * 0.023  # $0.023/GB/month\n",
    "        elif self.service == AWSService.LAMBDA:\n",
    "            return (self.requests / 1_000_000) * 0.20  # $0.20 per 1M requests\n",
    "        elif self.service == AWSService.SAGEMAKER_TRAINING:\n",
    "            if self.instance_type == InstanceType.ML_P3_8XLARGE_SPOT:\n",
    "                return self.usage_hours * 4.10  # Spot instance\n",
    "            else:\n",
    "                return self.usage_hours * 12.24  # On-demand\n",
    "        elif self.service == AWSService.SAGEMAKER_ENDPOINT:\n",
    "            return self.usage_hours * 0.192  # ml.m5.xlarge\n",
    "        elif self.service == AWSService.GLUE:\n",
    "            return self.usage_hours * 0.44  # Glue DPU\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "@dataclass\n",
    "class SageMakerTrainingJob:\n",
    "    \"\"\"SageMaker training job\"\"\"\n",
    "    job_name: str\n",
    "    instance_type: InstanceType\n",
    "    instance_count: int\n",
    "    training_data_s3: str\n",
    "    output_s3: str\n",
    "    hyperparameters: Dict[str, Any]\n",
    "    \n",
    "    def run_training(self) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate training job execution\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üöÄ SageMaker Training Job Started\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Job Name: {self.job_name}\")\n",
    "        print(f\"Instance Type: {self.instance_type.value}\")\n",
    "        print(f\"Instance Count: {self.instance_count}\")\n",
    "        print(f\"Training Data: {self.training_data_s3}\")\n",
    "        print(f\"Hyperparameters: {json.dumps(self.hyperparameters, indent=2)}\")\n",
    "        \n",
    "        # Simulate training\n",
    "        start = time.time()\n",
    "        epochs = self.hyperparameters.get('epochs', 10)\n",
    "        \n",
    "        metrics = []\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            time.sleep(0.05)  # Simulate training time\n",
    "            \n",
    "            # Simulate metrics\n",
    "            train_loss = 1.0 / epoch + random.uniform(-0.05, 0.05)\n",
    "            val_loss = 1.0 / epoch + random.uniform(-0.03, 0.08)\n",
    "            accuracy = min(0.95, 0.7 + (epoch / epochs) * 0.25 + random.uniform(-0.02, 0.02))\n",
    "            \n",
    "            metrics.append({\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'accuracy': accuracy\n",
    "            })\n",
    "            \n",
    "            if epoch % 2 == 0 or epoch == epochs:\n",
    "                print(f\"Epoch {epoch}/{epochs}: \"\n",
    "                      f\"train_loss={train_loss:.4f}, \"\n",
    "                      f\"val_loss={val_loss:.4f}, \"\n",
    "                      f\"accuracy={accuracy:.3f}\")\n",
    "        \n",
    "        duration = time.time() - start\n",
    "        \n",
    "        # Save model to S3\n",
    "        model_path = f\"{self.output_s3}/model.tar.gz\"\n",
    "        \n",
    "        print(f\"\\n‚úÖ Training complete in {duration:.2f} seconds\")\n",
    "        print(f\"üì¶ Model saved to: {model_path}\")\n",
    "        print(f\"üéØ Final accuracy: {metrics[-1]['accuracy']:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'model_path': model_path,\n",
    "            'metrics': metrics,\n",
    "            'training_time_seconds': duration,\n",
    "            'final_accuracy': metrics[-1]['accuracy']\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class SageMakerEndpoint:\n",
    "    \"\"\"SageMaker deployment endpoint\"\"\"\n",
    "    endpoint_name: str\n",
    "    model_path: str\n",
    "    instance_type: InstanceType\n",
    "    initial_instance_count: int\n",
    "    auto_scaling_enabled: bool = True\n",
    "    min_instances: int = 1\n",
    "    max_instances: int = 10\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.current_instances = self.initial_instance_count\n",
    "        self.total_predictions = 0\n",
    "        self.prediction_latencies = []\n",
    "    \n",
    "    def deploy(self) -> str:\n",
    "        \"\"\"Deploy model to endpoint\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üöÄ Deploying SageMaker Endpoint\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Endpoint Name: {self.endpoint_name}\")\n",
    "        print(f\"Model: {self.model_path}\")\n",
    "        print(f\"Instance Type: {self.instance_type.value}\")\n",
    "        print(f\"Initial Instances: {self.initial_instance_count}\")\n",
    "        print(f\"Auto-Scaling: {self.min_instances}-{self.max_instances} instances\")\n",
    "        \n",
    "        time.sleep(0.2)  # Simulate deployment time\n",
    "        \n",
    "        print(f\"\\n‚úÖ Endpoint deployed successfully\")\n",
    "        print(f\"üîó Endpoint URL: https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/{self.endpoint_name}/invocations\")\n",
    "        \n",
    "        return f\"https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/{self.endpoint_name}/invocations\"\n",
    "    \n",
    "    def predict(self, features: List[float]) -> Dict[str, Any]:\n",
    "        \"\"\"Make prediction\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # Simulate prediction\n",
    "        prediction = random.random()\n",
    "        confidence = random.uniform(0.85, 0.98)\n",
    "        \n",
    "        latency_ms = random.uniform(20, 80)\n",
    "        time.sleep(latency_ms / 1000)\n",
    "        \n",
    "        self.total_predictions += 1\n",
    "        self.prediction_latencies.append(latency_ms)\n",
    "        \n",
    "        return {\n",
    "            'prediction': prediction,\n",
    "            'confidence': confidence,\n",
    "            'latency_ms': latency_ms\n",
    "        }\n",
    "    \n",
    "    def auto_scale(self, requests_per_second: int):\n",
    "        \"\"\"Simulate auto-scaling based on traffic\"\"\"\n",
    "        # Scale up if >100 RPS per instance\n",
    "        target_instances = max(\n",
    "            self.min_instances,\n",
    "            min(self.max_instances, (requests_per_second // 100) + 1)\n",
    "        )\n",
    "        \n",
    "        if target_instances != self.current_instances:\n",
    "            print(f\"\\nüîÑ Auto-Scaling: {self.current_instances} ‚Üí {target_instances} instances \"\n",
    "                  f\"(traffic: {requests_per_second} RPS)\")\n",
    "            self.current_instances = target_instances\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get endpoint metrics\"\"\"\n",
    "        if not self.prediction_latencies:\n",
    "            return {}\n",
    "        \n",
    "        sorted_latencies = sorted(self.prediction_latencies)\n",
    "        p50_idx = int(len(sorted_latencies) * 0.50)\n",
    "        p95_idx = int(len(sorted_latencies) * 0.95)\n",
    "        p99_idx = int(len(sorted_latencies) * 0.99)\n",
    "        \n",
    "        return {\n",
    "            'total_predictions': self.total_predictions,\n",
    "            'current_instances': self.current_instances,\n",
    "            'latency_p50_ms': sorted_latencies[p50_idx],\n",
    "            'latency_p95_ms': sorted_latencies[p95_idx],\n",
    "            'latency_p99_ms': sorted_latencies[p99_idx],\n",
    "            'latency_avg_ms': sum(self.prediction_latencies) / len(self.prediction_latencies)\n",
    "        }\n",
    "\n",
    "# Example 1: SageMaker Training Job\n",
    "print(\"=\"*70)\n",
    "print(\"Example 1: AWS SageMaker Training - Yield Prediction Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "training_job = SageMakerTrainingJob(\n",
    "    job_name=\"yield-predictor-v2-1-2025-01-14\",\n",
    "    instance_type=InstanceType.ML_P3_8XLARGE_SPOT,\n",
    "    instance_count=1,\n",
    "    training_data_s3=\"s3://stdf-ml-data/features/train.parquet\",\n",
    "    output_s3=\"s3://stdf-ml-models/yield-predictor/v2.1\",\n",
    "    hyperparameters={\n",
    "        'epochs': 10,\n",
    "        'batch_size': 256,\n",
    "        'learning_rate': 0.001,\n",
    "        'hidden_layers': [512, 256, 128],\n",
    "        'dropout': 0.3\n",
    "    }\n",
    ")\n",
    "\n",
    "training_result = training_job.run_training()\n",
    "\n",
    "# Calculate training cost\n",
    "training_hours = training_result['training_time_seconds'] / 3600\n",
    "training_cost = AWSCost(\n",
    "    service=AWSService.SAGEMAKER_TRAINING,\n",
    "    usage_hours=training_hours,\n",
    "    instance_type=InstanceType.ML_P3_8XLARGE_SPOT\n",
    ")\n",
    "\n",
    "print(f\"\\nüí∞ Training Cost:\")\n",
    "print(f\"   Duration: {training_hours:.4f} hours\")\n",
    "print(f\"   Instance: {InstanceType.ML_P3_8XLARGE_SPOT.value} @ $4.10/hour (spot)\")\n",
    "print(f\"   Cost: ${training_cost.calculate_cost():.4f}\")\n",
    "\n",
    "# Example 2: SageMaker Endpoint Deployment\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"Example 2: AWS SageMaker Endpoint - Production Deployment\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "endpoint = SageMakerEndpoint(\n",
    "    endpoint_name=\"yield-predictor-prod\",\n",
    "    model_path=training_result['model_path'],\n",
    "    instance_type=InstanceType.ML_M5_XLARGE,\n",
    "    initial_instance_count=2,\n",
    "    auto_scaling_enabled=True,\n",
    "    min_instances=1,\n",
    "    max_instances=10\n",
    ")\n",
    "\n",
    "endpoint_url = endpoint.deploy()\n",
    "\n",
    "# Simulate traffic with auto-scaling\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"Simulating Production Traffic with Auto-Scaling\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "traffic_patterns = [\n",
    "    (50, \"Low traffic (8am)\"),\n",
    "    (150, \"Medium traffic (10am)\"),\n",
    "    (400, \"Peak traffic (2pm)\"),\n",
    "    (250, \"Evening traffic (6pm)\"),\n",
    "    (75, \"Night traffic (10pm)\")\n",
    "]\n",
    "\n",
    "for rps, description in traffic_patterns:\n",
    "    print(f\"\\n{description}: {rps} requests/sec\")\n",
    "    \n",
    "    # Auto-scale based on traffic\n",
    "    endpoint.auto_scale(requests_per_second=rps)\n",
    "    \n",
    "    # Simulate predictions\n",
    "    for _ in range(min(100, rps)):  # Sample predictions\n",
    "        features = [random.random() for _ in range(10)]\n",
    "        result = endpoint.predict(features)\n",
    "\n",
    "# Get endpoint metrics\n",
    "metrics = endpoint.get_metrics()\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"Endpoint Performance Metrics\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total Predictions: {metrics['total_predictions']:,}\")\n",
    "print(f\"Current Instances: {metrics['current_instances']}\")\n",
    "print(f\"Latency P50: {metrics['latency_p50_ms']:.2f}ms\")\n",
    "print(f\"Latency P95: {metrics['latency_p95_ms']:.2f}ms\")\n",
    "print(f\"Latency P99: {metrics['latency_p99_ms']:.2f}ms\")\n",
    "print(f\"Latency Avg: {metrics['latency_avg_ms']:.2f}ms\")\n",
    "\n",
    "# Calculate monthly endpoint cost\n",
    "monthly_hours = 730  # hours in month\n",
    "avg_instances = 3  # average instance count\n",
    "endpoint_cost = AWSCost(\n",
    "    service=AWSService.SAGEMAKER_ENDPOINT,\n",
    "    usage_hours=monthly_hours * avg_instances,\n",
    "    instance_type=InstanceType.ML_M5_XLARGE\n",
    ")\n",
    "\n",
    "print(f\"\\nüí∞ Monthly Endpoint Cost:\")\n",
    "print(f\"   Instance: {InstanceType.ML_M5_XLARGE.value} @ $0.192/hour\")\n",
    "print(f\"   Average Instances: {avg_instances}\")\n",
    "print(f\"   Cost: ${endpoint_cost.calculate_cost():.2f}/month\")\n",
    "\n",
    "print(\"\\n‚úÖ AWS SageMaker demonstration complete!\")\n",
    "print(\"   - Spot instance training (70% cost savings)\")\n",
    "print(\"   - Auto-scaling endpoint (1-10 instances based on traffic)\")\n",
    "print(\"   - Production-ready ML pipeline (training ‚Üí deployment ‚Üí monitoring)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3320d9",
   "metadata": {},
   "source": [
    "## 3. üî∑ Azure & ‚òÅÔ∏è GCP - Multi-Cloud ML Deployment\n",
    "\n",
    "### **Azure (Enterprise Integration)**\n",
    "\n",
    "**Azure Core Services:**\n",
    "- **Compute**: Virtual Machines, Azure Functions, AKS (managed Kubernetes), Container Instances\n",
    "- **Storage**: Blob Storage (S3 equivalent), Disk Storage, Azure Files\n",
    "- **ML Platform**: Azure Machine Learning (SageMaker equivalent), Cognitive Services (pre-built AI)\n",
    "- **Database**: Azure SQL, Cosmos DB (multi-region NoSQL), Azure Database for PostgreSQL\n",
    "- **Analytics**: Synapse Analytics (data warehouse), Databricks (Spark), Stream Analytics\n",
    "\n",
    "**Azure Strengths:**\n",
    "- **Enterprise Integration**: Tight integration with Active Directory, Office 365, Power BI\n",
    "- **Hybrid Cloud**: Azure Arc manages on-premises + cloud resources together\n",
    "- **Compliance**: 90+ compliance certifications (most of any cloud provider)\n",
    "- **Global Network**: ExpressRoute for dedicated 10Gbps connections to Azure\n",
    "\n",
    "### **GCP (Data & ML Focus)**\n",
    "\n",
    "**GCP Core Services:**\n",
    "- **Compute**: Compute Engine (EC2 equivalent), Cloud Functions, GKE (managed Kubernetes), Cloud Run (serverless containers)\n",
    "- **Storage**: Cloud Storage (S3 equivalent), Persistent Disk, Filestore\n",
    "- **ML Platform**: Vertex AI (unified ML platform), AutoML (no-code ML), TPUs (custom ML accelerators)\n",
    "- **Database**: Cloud SQL, Firestore (NoSQL), Cloud Spanner (globally distributed SQL)\n",
    "- **Analytics**: BigQuery (serverless data warehouse), Dataflow (stream/batch processing), Pub/Sub (messaging)\n",
    "\n",
    "**GCP Strengths:**\n",
    "- **BigQuery**: Best serverless data warehouse (query 50TB in seconds, pay per query)\n",
    "- **Vertex AI**: Unified ML platform (training, deployment, pipelines, feature store)\n",
    "- **TPUs**: Custom ML accelerators (8x faster than GPUs for large models)\n",
    "- **Data Engineering**: Best tools for data pipelines (Dataflow, Pub/Sub, BigQuery)\n",
    "\n",
    "### **Cloud Platform Comparison**\n",
    "\n",
    "| Feature | AWS | Azure | GCP |\n",
    "|---------|-----|-------|-----|\n",
    "| **Market Share** | 32% | 23% | 10% |\n",
    "| **ML Platform** | SageMaker ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Azure ML ‚≠ê‚≠ê‚≠ê‚≠ê | Vertex AI ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Data Warehouse** | Redshift ‚≠ê‚≠ê‚≠ê | Synapse ‚≠ê‚≠ê‚≠ê‚≠ê | BigQuery ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Kubernetes** | EKS ‚≠ê‚≠ê‚≠ê‚≠ê | AKS ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | GKE ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Serverless** | Lambda ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Functions ‚≠ê‚≠ê‚≠ê‚≠ê | Cloud Functions ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Pricing** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (cheapest) |\n",
    "| **Enterprise** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "| **Innovation** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "\n",
    "### **When to Use Which Cloud?**\n",
    "\n",
    "**Choose AWS if:**\n",
    "- ‚úÖ Need largest service catalog (200+ services vs 100+ Azure/GCP)\n",
    "- ‚úÖ Want mature ecosystem (18 years, most third-party integrations)\n",
    "- ‚úÖ SageMaker for ML (best managed ML platform)\n",
    "- ‚úÖ Global reach (25+ regions, most availability zones)\n",
    "\n",
    "**Choose Azure if:**\n",
    "- ‚úÖ Microsoft shop (Active Directory, Office 365, SQL Server integration)\n",
    "- ‚úÖ Hybrid cloud (on-premises + cloud with Azure Arc)\n",
    "- ‚úÖ Enterprise compliance (90+ certifications)\n",
    "- ‚úÖ .NET applications (best platform for C#/.NET workloads)\n",
    "\n",
    "**Choose GCP if:**\n",
    "- ‚úÖ Data-heavy workloads (BigQuery best data warehouse)\n",
    "- ‚úÖ ML research (TPUs, Vertex AI, cutting-edge ML tools)\n",
    "- ‚úÖ Kubernetes-native (GKE most advanced managed Kubernetes)\n",
    "- ‚úÖ Cost-sensitive (generally 20-30% cheaper than AWS/Azure)\n",
    "\n",
    "**Multi-Cloud Strategy:**\n",
    "- Primary cloud: AWS (80% workloads, mature ecosystem)\n",
    "- Secondary cloud: GCP (20% workloads, disaster recovery, BigQuery for analytics)\n",
    "- Avoid: Azure (unless Microsoft integration required, reduces vendor lock-in risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ba693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Cloud Deployment Simulation (Azure + GCP)\n",
    "\n",
    "class CloudProvider(Enum):\n",
    "    \"\"\"Cloud provider types\"\"\"\n",
    "    AWS = \"aws\"\n",
    "    AZURE = \"azure\"\n",
    "    GCP = \"gcp\"\n",
    "\n",
    "@dataclass\n",
    "class MultiCloudDeployment:\n",
    "    \"\"\"Multi-cloud deployment with failover\"\"\"\n",
    "    primary_cloud: CloudProvider\n",
    "    secondary_cloud: CloudProvider\n",
    "    primary_region: str\n",
    "    secondary_region: str\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.primary_healthy = True\n",
    "        self.traffic_distribution = {\n",
    "            self.primary_cloud: 80,\n",
    "            self.secondary_cloud: 20\n",
    "        }\n",
    "    \n",
    "    def health_check(self, cloud: CloudProvider) -> bool:\n",
    "        \"\"\"Simulate health check\"\"\"\n",
    "        # 95% chance primary healthy, 99% secondary healthy\n",
    "        if cloud == self.primary_cloud:\n",
    "            return random.random() < 0.95\n",
    "        else:\n",
    "            return random.random() < 0.99\n",
    "    \n",
    "    def failover(self):\n",
    "        \"\"\"Failover from primary to secondary\"\"\"\n",
    "        print(f\"\\n{'!'*70}\")\n",
    "        print(f\"üîÑ FAILOVER: {self.primary_cloud.value.upper()} ‚Üí {self.secondary_cloud.value.upper()}\")\n",
    "        print(f\"{'!'*70}\")\n",
    "        \n",
    "        # Shift all traffic to secondary\n",
    "        self.traffic_distribution = {\n",
    "            self.primary_cloud: 0,\n",
    "            self.secondary_cloud: 100\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Failover complete in 2 minutes\")\n",
    "        print(f\"   Traffic distribution: {self.traffic_distribution}\")\n",
    "    \n",
    "    def failback(self):\n",
    "        \"\"\"Return to primary cloud\"\"\"\n",
    "        print(f\"\\nüîÑ FAILBACK: Returning to {self.primary_cloud.value.upper()}\")\n",
    "        \n",
    "        # Gradual traffic shift back to primary\n",
    "        self.traffic_distribution = {\n",
    "            self.primary_cloud: 80,\n",
    "            self.secondary_cloud: 20\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Failback complete\")\n",
    "        print(f\"   Traffic distribution: {self.traffic_distribution}\")\n",
    "    \n",
    "    def get_current_provider(self) -> CloudProvider:\n",
    "        \"\"\"Get current active provider\"\"\"\n",
    "        if self.traffic_distribution[self.primary_cloud] > 50:\n",
    "            return self.primary_cloud\n",
    "        else:\n",
    "            return self.secondary_cloud\n",
    "\n",
    "# Example 3: Multi-Cloud Deployment with Failover\n",
    "print(\"=\"*70)\n",
    "print(\"Example 3: Multi-Cloud Deployment - AWS Primary, GCP Secondary\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "multi_cloud = MultiCloudDeployment(\n",
    "    primary_cloud=CloudProvider.AWS,\n",
    "    secondary_cloud=CloudProvider.GCP,\n",
    "    primary_region=\"us-east-1\",\n",
    "    secondary_region=\"us-central1\"\n",
    ")\n",
    "\n",
    "print(f\"\\nInitial Setup:\")\n",
    "print(f\"  Primary: {multi_cloud.primary_cloud.value.upper()} ({multi_cloud.primary_region})\")\n",
    "print(f\"  Secondary: {multi_cloud.secondary_cloud.value.upper()} ({multi_cloud.secondary_region})\")\n",
    "print(f\"  Traffic Distribution: {multi_cloud.traffic_distribution}\")\n",
    "\n",
    "# Simulate health monitoring\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"Health Monitoring (Every 30 seconds)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for check_num in range(1, 6):\n",
    "    print(f\"\\nHealth Check #{check_num}:\")\n",
    "    \n",
    "    primary_healthy = multi_cloud.health_check(multi_cloud.primary_cloud)\n",
    "    secondary_healthy = multi_cloud.health_check(multi_cloud.secondary_cloud)\n",
    "    \n",
    "    status_emoji = \"‚úÖ\" if primary_healthy else \"‚ùå\"\n",
    "    print(f\"  {multi_cloud.primary_cloud.value.upper()}: {status_emoji} {'Healthy' if primary_healthy else 'UNHEALTHY'}\")\n",
    "    \n",
    "    status_emoji = \"‚úÖ\" if secondary_healthy else \"‚ùå\"\n",
    "    print(f\"  {multi_cloud.secondary_cloud.value.upper()}: {status_emoji} {'Healthy' if secondary_healthy else 'UNHEALTHY'}\")\n",
    "    \n",
    "    # Trigger failover if primary unhealthy\n",
    "    if not primary_healthy and multi_cloud.primary_healthy:\n",
    "        multi_cloud.primary_healthy = False\n",
    "        multi_cloud.failover()\n",
    "    elif primary_healthy and not multi_cloud.primary_healthy:\n",
    "        multi_cloud.primary_healthy = True\n",
    "        multi_cloud.failback()\n",
    "    \n",
    "    time.sleep(0.1)\n",
    "\n",
    "# Cloud cost comparison\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"Monthly Cost Comparison (100 predictions/sec, 24/7)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "workload = {\n",
    "    'compute_hours': 730,  # 1 month\n",
    "    'predictions_per_month': 100 * 60 * 60 * 24 * 30,  # 259.2M predictions\n",
    "    'storage_gb': 500,\n",
    "    'data_transfer_gb': 1000\n",
    "}\n",
    "\n",
    "# AWS costs\n",
    "aws_compute = 3 * 0.192 * workload['compute_hours']  # 3x ml.m5.xlarge\n",
    "aws_storage = workload['storage_gb'] * 0.023  # S3\n",
    "aws_transfer = workload['data_transfer_gb'] * 0.09  # Data transfer out\n",
    "aws_total = aws_compute + aws_storage + aws_transfer\n",
    "\n",
    "# Azure costs\n",
    "azure_compute = 3 * 0.20 * workload['compute_hours']  # 3x D4s_v3 (similar to ml.m5.xlarge)\n",
    "azure_storage = workload['storage_gb'] * 0.024  # Blob Storage\n",
    "azure_transfer = workload['data_transfer_gb'] * 0.087  # Data transfer out\n",
    "azure_total = azure_compute + azure_storage + azure_transfer\n",
    "\n",
    "# GCP costs\n",
    "gcp_compute = 3 * 0.17 * workload['compute_hours']  # 3x n1-standard-4 (cheaper than AWS/Azure)\n",
    "gcp_storage = workload['storage_gb'] * 0.020  # Cloud Storage\n",
    "gcp_transfer = workload['data_transfer_gb'] * 0.12  # Data transfer out\n",
    "gcp_total = gcp_compute + gcp_storage + gcp_transfer\n",
    "\n",
    "print(f\"\\n{'Cloud':<10} {'Compute':<15} {'Storage':<15} {'Transfer':<15} {'Total':<15} {'vs AWS':<15}\")\n",
    "print(f\"{'-'*90}\")\n",
    "print(f\"{'AWS':<10} ${aws_compute:<14.2f} ${aws_storage:<14.2f} ${aws_transfer:<14.2f} ${aws_total:<14.2f} {'-':<15}\")\n",
    "print(f\"{'Azure':<10} ${azure_compute:<14.2f} ${azure_storage:<14.2f} ${azure_transfer:<14.2f} ${azure_total:<14.2f} {(azure_total-aws_total)/aws_total*100:+.1f}%\")\n",
    "print(f\"{'GCP':<10} ${gcp_compute:<14.2f} ${gcp_storage:<14.2f} ${gcp_transfer:<14.2f} ${gcp_total:<14.2f} {(gcp_total-aws_total)/aws_total*100:+.1f}%\")\n",
    "\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "print(f\"   - GCP typically 20-30% cheaper than AWS (lower compute costs)\")\n",
    "print(f\"   - Azure similar pricing to AWS (slight premium for enterprise features)\")\n",
    "print(f\"   - All clouds have similar storage costs ($0.020-0.024/GB)\")\n",
    "print(f\"   - Data transfer costs vary (AWS $0.09/GB, Azure $0.087/GB, GCP $0.12/GB)\")\n",
    "\n",
    "# Multi-cloud cost (80% AWS, 20% GCP)\n",
    "multi_cloud_cost = aws_total * 0.8 + gcp_total * 0.2\n",
    "print(f\"\\nüåç Multi-Cloud Strategy (80% AWS, 20% GCP):\")\n",
    "print(f\"   Monthly Cost: ${multi_cloud_cost:.2f}\")\n",
    "print(f\"   vs Single Cloud (AWS): ${aws_total:.2f} ({(multi_cloud_cost-aws_total)/aws_total*100:+.1f}%)\")\n",
    "print(f\"   Benefits: 99.99% availability, vendor independence, disaster recovery\")\n",
    "\n",
    "print(\"\\n‚úÖ Multi-cloud deployment demonstration complete!\")\n",
    "print(\"   - Automated failover on primary cloud outage (2 min failover time)\")\n",
    "print(\"   - Cost comparison across AWS/Azure/GCP\")\n",
    "print(\"   - Multi-cloud strategy balances cost, reliability, and vendor lock-in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab15415",
   "metadata": {},
   "source": [
    "## 4. üî¨ Real-World Projects: Production Cloud Platforms\n",
    "\n",
    "### Project 1: **Complete AWS ML Platform with SageMaker** üí∞ **$4.2M/year**\n",
    "**Objective:** Build end-to-end ML platform on AWS for 50 models, 500K predictions/day, with automated training, deployment, and monitoring.\n",
    "\n",
    "**Key Features:**\n",
    "- **Data Lake**: S3 data lake with STDF files (raw), Parquet features (processed), models (artifacts) organized by date/model/version\n",
    "- **ETL Pipeline**: Glue jobs parse STDF files, extract 200 features, write to S3 Parquet (columnar format, 5x faster queries than CSV)\n",
    "- **Training**: SageMaker training jobs on spot instances (70% discount), hyperparameter tuning (100 trials in parallel), distributed training (4 GPUs)\n",
    "- **Feature Store**: SageMaker Feature Store for offline (training) and online (prediction) features with versioning\n",
    "- **Model Registry**: SageMaker Model Registry tracks 50 models, version history, approval workflow (manual approval for production)\n",
    "- **Deployment**: SageMaker multi-model endpoints (1 endpoint serves 50 models), auto-scaling (1-20 instances), A/B testing (10% traffic to new model)\n",
    "- **Monitoring**: CloudWatch dashboards, SageMaker Model Monitor (data drift, model quality), SNS alerts to Slack\n",
    "\n",
    "**Business Value:**\n",
    "- 92% lower infrastructure cost: $500K on-premises ‚Üí $40K/year AWS (spot instances, auto-scaling, serverless)\n",
    "- 10x faster model deployment: 2 days manual ‚Üí 4 hours automated (SageMaker pipelines)\n",
    "- 50% less ML engineering labor: Managed services reduce team from 6 to 3 engineers ($450K/year savings)\n",
    "- 4% accuracy improvement: Faster iteration (weekly retraining vs monthly) ($2M/year from yield optimization)\n",
    "- **Total: $4.2M/year value**\n",
    "\n",
    "---\n",
    "\n",
    "### Project 2: **Azure Multi-Region DR with 99.99% SLA** üí∞ **$3.8M/year**\n",
    "**Objective:** Deploy ML platform to 3 Azure regions (US, EU, Asia) with automatic failover, <10ms P99 latency, 99.99% uptime.\n",
    "\n",
    "**Key Features:**\n",
    "- **AKS Clusters**: Kubernetes clusters in East US, West Europe, Southeast Asia with node auto-scaling (1-50 nodes), pod auto-scaling (1-100 pods)\n",
    "- **Traffic Manager**: Geo-routing to nearest region (latency <50ms), health checks every 10 seconds, automatic failover (<2 min)\n",
    "- **Cosmos DB**: Multi-region writes (write to any region), strong consistency, automatic conflict resolution, <10ms P99 read latency\n",
    "- **Azure Front Door**: Global CDN caches ML predictions (5 min TTL), reduces origin load 70%, WAF blocks DDoS attacks\n",
    "- **Azure Monitor**: Unified metrics from 3 regions, log aggregation (Elasticsearch), alerts (>1% error rate, >100ms latency)\n",
    "\n",
    "**Business Value:**\n",
    "- 99.99% availability: Multi-region prevents single region outage ($2.5M/year from prevented downtime)\n",
    "- 75% latency reduction: CDN + geo-routing (200ms ‚Üí 50ms P95) ($800K/year from user productivity)\n",
    "- 80% lower cross-region bandwidth: CDN caching reduces data transfer costs ($300K/year savings)\n",
    "- Compliance: GDPR data residency (EU data in EU region) ($200K/year from compliance)\n",
    "- **Total: $3.8M/year value**\n",
    "\n",
    "---\n",
    "\n",
    "### Project 3: **GCP BigQuery Analytics Platform** üí∞ **$3.5M/year**\n",
    "**Objective:** Build serverless analytics platform on GCP BigQuery for analyzing 100TB STDF data with SQL queries in seconds.\n",
    "\n",
    "**Key Features:**\n",
    "- **BigQuery Data Warehouse**: 100TB STDF data partitioned by date, clustered by device_id, compressed (4x storage savings)\n",
    "- **Streaming Ingestion**: Pub/Sub ‚Üí Dataflow ‚Üí BigQuery (real-time ingestion, <1 min latency from STDF upload to queryable)\n",
    "- **BigQuery ML**: Train ML models with SQL (`CREATE MODEL` statement), linear regression, XGBoost, AutoML, deploy as SQL functions\n",
    "- **Looker Dashboards**: Pre-built dashboards (yield trends, spatial heatmaps, test correlations), embedded in Salesforce\n",
    "- **Cost Optimization**: Partition pruning (query only recent data), clustering (skip irrelevant data), BI Engine caching (free query results for 24 hours)\n",
    "\n",
    "**Business Value:**\n",
    "- 97% lower infrastructure cost: $500K Spark cluster ‚Üí $15K/year BigQuery (pay-per-query, no cluster management)\n",
    "- 99.9% faster queries: 1 week Spark ‚Üí 5 seconds BigQuery (serverless, columnar storage)\n",
    "- 85% less data engineering labor: SQL vs Spark/Python ($400K/year savings from 2.5 engineers)\n",
    "- 6% yield improvement: Faster insights enable rapid optimization ($3M/year from yield increase)\n",
    "- **Total: $3.5M/year value**\n",
    "\n",
    "---\n",
    "\n",
    "### Project 4: **Multi-Cloud Kubernetes with Anthos/Arc** üí∞ **$2.9M/year**\n",
    "**Objective:** Manage Kubernetes clusters across AWS EKS, Azure AKS, GCP GKE, on-premises with unified control plane.\n",
    "\n",
    "**Key Features:**\n",
    "- **Google Anthos**: Unified Kubernetes management across GKE, EKS, AKS, on-premises (single pane of glass)\n",
    "- **Service Mesh**: Istio for traffic management (canary, blue-green), security (mTLS), observability (distributed tracing)\n",
    "- **Config Management**: GitOps with Flux/ArgoCD (infrastructure as code, auto-sync from Git, audit trail)\n",
    "- **Multi-Cluster Service**: Services span multiple clusters (app in GKE + EKS, load balanced across both)\n",
    "- **Disaster Recovery**: Automatic failover between clusters (GKE ‚Üí EKS in <5 min)\n",
    "\n",
    "**Business Value:**\n",
    "- Vendor independence: Avoid lock-in, negotiate better pricing (20% discount from cloud providers)\n",
    "- 99.99% availability: Multi-cluster prevents single cluster outage ($2M/year from prevented downtime)\n",
    "- 50% less DevOps labor: Unified management reduces team size ($400K/year savings)\n",
    "- Hybrid cloud: Run ML training on-premises (free GPUs) + inference in cloud ($500K/year savings)\n",
    "- **Total: $2.9M/year value**\n",
    "\n",
    "---\n",
    "\n",
    "### Project 5: **Serverless ML with Lambda/Functions** üí∞ **$2.6M/year**\n",
    "**Objective:** Deploy ML models as serverless functions (AWS Lambda, Azure Functions, GCP Cloud Functions) for 10M predictions/month with <100ms P99 latency.\n",
    "\n",
    "**Key Features:**\n",
    "- **Model Packaging**: Package ML model as Lambda function (Python 3.11, 512MB RAM, 10GB Docker image)\n",
    "- **Cold Start Optimization**: Provisioned concurrency (10 warm instances), lazy loading (load model only on first request)\n",
    "- **Auto-Scaling**: Scale from 0 to 1000 concurrent executions automatically (pay only for invocations, no idle instances)\n",
    "- **API Gateway**: RESTful API with authentication (API keys), rate limiting (100 RPS per user), caching (1 hour TTL)\n",
    "- **Cost**: $0.20 per 1M requests + $0.0000166667/GB-second (vs $1000/month always-on EC2 instance)\n",
    "\n",
    "**Business Value:**\n",
    "- 95% lower cost: $12K/year always-on EC2 ‚Üí $600/year serverless (pay per invocation, auto-shutdown)\n",
    "- Infinite scalability: Handle traffic spikes (10x normal) without pre-provisioning ($1.5M/year from prevented downtime during Black Friday)\n",
    "- Zero maintenance: No server patching, OS updates, auto-scaling configuration ($500K/year labor savings)\n",
    "- Faster deployment: Deploy in 30 seconds (vs 10 minutes EC2) ($600K/year from faster iteration)\n",
    "- **Total: $2.6M/year value**\n",
    "\n",
    "---\n",
    "\n",
    "### Project 6: **Cloud-Native STDF Processing Pipeline** üí∞ **$2.3M/year**\n",
    "**Objective:** Build cloud-native STDF processing with AWS Step Functions, Lambda, S3, DynamoDB for 1M files/month.\n",
    "\n",
    "**Key Features:**\n",
    "- **Event-Driven**: S3 upload triggers Step Functions workflow (parse ‚Üí validate ‚Üí transform ‚Üí store)\n",
    "- **Step Functions**: Orchestrate Lambda functions (parse STDF, extract features, validate data, insert to DynamoDB)\n",
    "- **Parallel Processing**: Process 100 files concurrently (vs 1 file sequential), 100x faster\n",
    "- **Error Handling**: Automatic retries (exponential backoff), dead-letter queue (failed files), alerting (Slack notification)\n",
    "- **Cost**: $0.025 per 1000 state transitions (vs $5K/month Airflow cluster)\n",
    "\n",
    "**Business Value:**\n",
    "- 98% lower infrastructure cost: $60K/year Airflow cluster ‚Üí $1.2K/year Step Functions (pay per execution)\n",
    "- 99% faster processing: 10 hours ‚Üí 6 minutes (parallel processing, serverless)\n",
    "- 90% less DevOps labor: Managed service vs self-hosted Airflow ($350K/year savings)\n",
    "- Handle 10x more files: Scale to 10M files/month without infrastructure changes ($1.6M/year from increased capacity)\n",
    "- **Total: $2.3M/year value**\n",
    "\n",
    "---\n",
    "\n",
    "### Project 7: **AI-Powered Cost Optimization** üí∞ **$2.1M/year**\n",
    "**Objective:** Use ML to optimize cloud costs (right-sizing, spot instances, reserved instances, auto-shutdown) reducing spend 40%.\n",
    "\n",
    "**Key Features:**\n",
    "- **Cost Analytics**: AWS Cost Explorer, Azure Cost Management, GCP Billing Reports (identify top cost drivers)\n",
    "- **ML-Based Predictions**: Predict resource usage, recommend right-sizing (m5.2xlarge ‚Üí m5.xlarge saves 50%)\n",
    "- **Automated Actions**: Auto-shutdown dev environments after 6pm, scale down staging to 1 instance on weekends\n",
    "- **Spot Instance Orchestration**: SpotInst/Karpenter automatically uses spot instances (70% discount), fallback to on-demand if spot unavailable\n",
    "- **Reserved Instance Recommendations**: Analyze usage patterns, recommend 1-year commitments for stable workloads (40% discount)\n",
    "\n",
    "**Business Value:**\n",
    "- 40% cost reduction: $5M/year cloud spend ‚Üí $3M/year (right-sizing, spot instances, auto-shutdown)\n",
    "- Automated optimization: Zero manual effort (ML model recommends, auto-applies changes)\n",
    "- Visibility: Detailed cost attribution (team A: $500K, team B: $300K) enables accountability\n",
    "- **Total: $2.1M/year savings**\n",
    "\n",
    "---\n",
    "\n",
    "### Project 8: **Global Edge ML with CloudFront/CDN** üí∞ **$1.8M/year**\n",
    "**Objective:** Deploy ML models to edge locations (CloudFront Lambda@Edge, Cloudflare Workers) for <10ms P99 latency.\n",
    "\n",
    "**Key Features:**\n",
    "- **Edge Functions**: Deploy lightweight models to 200+ edge locations (near users, <10ms latency)\n",
    "- **Model Compression**: Quantize models (FP32 ‚Üí INT8, 4x smaller), prune weights (remove 50% parameters), optimize for edge (ONNX Runtime)\n",
    "- **Caching**: Cache predictions for common inputs (1 hour TTL), reduces origin load 90%\n",
    "- **Geo-Intelligence**: Use edge location to personalize predictions (US users see US-specific model)\n",
    "\n",
    "**Business Value:**\n",
    "- 95% latency reduction: 200ms origin ‚Üí 10ms edge (model at edge location)\n",
    "- 90% lower origin load: Edge caching reduces traffic to origin ($600K/year from smaller origin)\n",
    "- Improved UX: <10ms latency improves conversion 20% ($1M/year from increased sales)\n",
    "- Global reach: Serve 200+ countries without deploying infrastructure ($200K/year from simplified ops)\n",
    "- **Total: $1.8M/year value**\n",
    "\n",
    "---\n",
    "\n",
    "## üí∞ **Total Project Value: $23.2M/year**\n",
    "**Average ROI: 680% (cloud costs ~$3.4M/year, value $23.2M/year)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea01cbd",
   "metadata": {},
   "source": [
    "## 5. üéØ Comprehensive Takeaways: Cloud Platforms Mastery\n",
    "\n",
    "### **Core Concepts**\n",
    "\n",
    "**Cloud Fundamentals:**\n",
    "- ‚úÖ **Pay-per-use**: No upfront capital for servers (vs $50K-500K for on-premises hardware)\n",
    "- ‚úÖ **Elasticity**: Scale from 1 server to 1000 servers in minutes (handle traffic spikes, seasonal demand)\n",
    "- ‚úÖ **Global reach**: Deploy to 25+ regions worldwide (low latency for users, data residency compliance)\n",
    "- ‚úÖ **Managed services**: SageMaker/Azure ML/Vertex AI vs building ML infrastructure from scratch\n",
    "\n",
    "**AWS Services:**\n",
    "- ‚úÖ **SageMaker**: Best managed ML platform (training, deployment, monitoring, feature store, model registry)\n",
    "- ‚úÖ **S3**: Object storage with 99.999999999% durability, lifecycle policies (move to Glacier after 90 days)\n",
    "- ‚úÖ **Lambda**: Serverless functions ($0.20/1M requests), auto-scaling (0 to 1000 concurrent executions)\n",
    "- ‚úÖ **EC2**: Virtual machines with 600+ instance types (compute, memory, GPU, storage-optimized)\n",
    "\n",
    "**Azure Services:**\n",
    "- ‚úÖ **Azure ML**: Managed ML platform with enterprise integration (Active Directory, Power BI)\n",
    "- ‚úÖ **AKS**: Managed Kubernetes with excellent integration (Azure Monitor, Azure AD, virtual nodes)\n",
    "- ‚úÖ **Cosmos DB**: Multi-region NoSQL with <10ms P99 latency, automatic replication\n",
    "- ‚úÖ **Hybrid Cloud**: Azure Arc manages on-premises + cloud with unified control plane\n",
    "\n",
    "**GCP Services:**\n",
    "- ‚úÖ **Vertex AI**: Unified ML platform (training, deployment, pipelines, AutoML, feature store)\n",
    "- ‚úÖ **BigQuery**: Best serverless data warehouse (query 100TB in seconds, pay per query $5/TB)\n",
    "- ‚úÖ **GKE**: Most advanced managed Kubernetes (autopilot mode, workload identity, binary authorization)\n",
    "- ‚úÖ **TPUs**: Custom ML accelerators (8x faster than GPUs for large transformer models)\n",
    "\n",
    "---\n",
    "\n",
    "### **Best Practices**\n",
    "\n",
    "**Cost Optimization:**\n",
    "- ‚úÖ **Spot Instances**: 70% discount for training jobs (AWS/Azure/GCP), handle 2-minute termination notice\n",
    "- ‚úÖ **Auto-Scaling**: Scale down during low traffic (1 instance 10pm-8am, 10 instances 9am-6pm) saves 60%\n",
    "- ‚úÖ **Reserved Instances**: 1-year commitment saves 40%, 3-year saves 60% (for stable workloads)\n",
    "- ‚úÖ **Serverless**: Lambda/Functions for bursty workloads (pay per invocation vs always-on EC2)\n",
    "- ‚úÖ **Storage Lifecycle**: S3 Intelligent-Tiering moves data automatically (frequent ‚Üí infrequent ‚Üí archive ‚Üí delete)\n",
    "- ‚úÖ **Right-Sizing**: Monitor CPU usage, downsize over-provisioned instances (m5.2xlarge ‚Üí m5.xlarge saves 50%)\n",
    "\n",
    "**Architecture Patterns:**\n",
    "- ‚úÖ **Multi-Region**: Deploy to 2-3 regions for disaster recovery (99.99% availability vs 99.9% single region)\n",
    "- ‚úÖ **Multi-Cloud**: Primary AWS (80%), secondary GCP (20%) prevents vendor lock-in, enables negotiation\n",
    "- ‚úÖ **Microservices**: Deploy services independently (ML model, data processing, API) with Kubernetes\n",
    "- ‚úÖ **Event-Driven**: S3 upload ‚Üí Lambda ‚Üí processing (vs polling, more efficient)\n",
    "- ‚úÖ **Caching**: CloudFront/CDN caches predictions (1 hour TTL), reduces origin load 80%\n",
    "\n",
    "**Security & Compliance:**\n",
    "- ‚úÖ **IAM**: Least privilege (grant minimum permissions), use roles not access keys\n",
    "- ‚úÖ **Encryption**: Encrypt data at rest (S3 SSE-S3, RDS encryption), in transit (TLS/HTTPS)\n",
    "- ‚úÖ **VPC**: Isolate workloads in private subnets (no internet access), use security groups (firewall rules)\n",
    "- ‚úÖ **Compliance**: Choose regions for data residency (EU data in eu-west-1 for GDPR)\n",
    "- ‚úÖ **Secrets Management**: AWS Secrets Manager/Azure Key Vault/GCP Secret Manager (rotate every 30 days)\n",
    "\n",
    "**ML-Specific Patterns:**\n",
    "- ‚úÖ **Feature Store**: Centralized feature repository (SageMaker Feature Store, Vertex AI Feature Store) with versioning\n",
    "- ‚úÖ **Model Registry**: Track all models (SageMaker Model Registry, MLflow) with approval workflow\n",
    "- ‚úÖ **A/B Testing**: Multi-model endpoints route traffic (90% old model, 10% new model) for safe rollout\n",
    "- ‚úÖ **Data Drift Detection**: SageMaker Model Monitor, Vertex AI Model Monitoring (alert on distribution shift >10%)\n",
    "- ‚úÖ **AutoML**: Azure AutoML, GCP AutoML for quick baseline models (no code required)\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Patterns**\n",
    "\n",
    "**Hybrid Cloud:**\n",
    "- Run ML training on-premises (free GPUs already purchased) + inference in cloud (low latency)\n",
    "- Use AWS Outposts/Azure Stack for on-premises cloud services (same APIs as public cloud)\n",
    "\n",
    "**Multi-Cloud Data Replication:**\n",
    "- AWS RDS ‚Üí GCP Cloud SQL replication (5-minute lag) for disaster recovery\n",
    "- Use Change Data Capture (CDC) with Debezium for real-time replication\n",
    "\n",
    "**Cost Attribution:**\n",
    "- Tag all resources (project:yield-prediction, team:ml-platform, env:production)\n",
    "- Use AWS Cost Allocation Tags, Azure Cost Management, GCP Billing Labels\n",
    "- Chargeback to teams based on usage (team A: $50K/month, team B: $30K/month)\n",
    "\n",
    "**FinOps (Financial Operations):**\n",
    "- Automated budget alerts (Slack notification if spend >$100K/month)\n",
    "- Anomaly detection (ML model predicts spend, alerts on >20% deviation)\n",
    "- Commitment optimization (auto-recommend reserved instances based on usage patterns)\n",
    "\n",
    "**Disaster Recovery:**\n",
    "- **RTO** (Recovery Time Objective): How long to restore service (target: <10 minutes)\n",
    "- **RPO** (Recovery Point Objective): How much data loss acceptable (target: <5 minutes)\n",
    "- Multi-region with automated failover achieves RTO <2 min, RPO <5 min\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Pitfalls**\n",
    "\n",
    "**Cost Mistakes:**\n",
    "- ‚ùå **Always-on instances**: Leaving dev/staging instances running 24/7 (waste 66% during nights/weekends)\n",
    "- ‚ùå **Over-provisioned**: Using m5.2xlarge when m5.xlarge sufficient (waste 50% cost)\n",
    "- ‚ùå **No monitoring**: Not tracking costs daily ‚Üí surprise $50K bill ‚Üí Use CloudWatch/Azure Monitor\n",
    "- ‚ùå **Data transfer**: Transferring 10TB between regions costs $900 ‚Üí Use same-region architecture\n",
    "\n",
    "**Architecture Mistakes:**\n",
    "- ‚ùå **Single region**: AWS us-east-1 outage takes down entire service ‚Üí Use multi-region\n",
    "- ‚ùå **Vendor lock-in**: Using proprietary services (DynamoDB, Cosmos DB) ‚Üí Use Postgres for portability\n",
    "- ‚ùå **No auto-scaling**: Always running 10 instances (even during low traffic) ‚Üí Use auto-scaling\n",
    "- ‚ùå **Synchronous processing**: API waits for 10-minute ETL job ‚Üí Use async with SQS/Pub/Sub\n",
    "\n",
    "**Security Mistakes:**\n",
    "- ‚ùå **Public S3 buckets**: Accidentally making buckets public ‚Üí Enable S3 Block Public Access\n",
    "- ‚ùå **Hardcoded credentials**: AWS keys in code committed to Git ‚Üí Use IAM roles\n",
    "- ‚ùå **No encryption**: Storing sensitive data unencrypted ‚Üí Enable S3 SSE-S3, RDS encryption\n",
    "- ‚ùå **Overly permissive IAM**: `AdministratorAccess` for all developers ‚Üí Use least privilege\n",
    "\n",
    "**ML Mistakes:**\n",
    "- ‚ùå **Training on-demand**: Always using on-demand instances ‚Üí Use spot instances (70% savings)\n",
    "- ‚ùå **No model monitoring**: Deploying model without drift detection ‚Üí Use Model Monitor\n",
    "- ‚ùå **Big bang deployment**: Deploying to 100% traffic ‚Üí Use A/B testing (10% ‚Üí 100%)\n",
    "- ‚ùå **No versioning**: Overwriting model artifacts ‚Üí Use Model Registry with versioning\n",
    "\n",
    "---\n",
    "\n",
    "### **Production Checklist**\n",
    "\n",
    "**Before deploying to cloud:**\n",
    "- ‚úÖ **Cost estimate**: Calculate monthly cost (use AWS Pricing Calculator, Azure Pricing Calculator)\n",
    "- ‚úÖ **Multi-region**: Deploy to 2+ regions for disaster recovery (or accept 99.9% availability)\n",
    "- ‚úÖ **Auto-scaling**: Configure auto-scaling (CPU >70% ‚Üí add instance, <30% ‚Üí remove instance)\n",
    "- ‚úÖ **Monitoring**: CloudWatch/Azure Monitor dashboards, alerts (error rate >1%, latency >200ms)\n",
    "- ‚úÖ **Backup strategy**: Automated backups (RDS daily backups, S3 versioning, 30-day retention)\n",
    "- ‚úÖ **IAM roles**: Use roles not access keys, least privilege (read-only vs admin)\n",
    "- ‚úÖ **Encryption**: Enable at-rest (S3 SSE-S3, RDS encryption), in-transit (TLS/HTTPS)\n",
    "- ‚úÖ **Tagging**: Tag all resources (project, team, environment) for cost attribution\n",
    "- ‚úÖ **Budget alerts**: Set budget ($10K/month), alert at 80% ($8K spent)\n",
    "- ‚úÖ **Disaster recovery**: Document RTO/RPO, test failover procedure (quarterly)\n",
    "\n",
    "---\n",
    "\n",
    "### **Cloud Platform Selection Guide**\n",
    "\n",
    "**Choose AWS if:**\n",
    "- ‚úÖ Need largest service catalog (200+ services)\n",
    "- ‚úÖ Want SageMaker for ML (best managed ML platform)\n",
    "- ‚úÖ Require global reach (25+ regions)\n",
    "- ‚úÖ Value mature ecosystem (18 years, most third-party integrations)\n",
    "\n",
    "**Choose Azure if:**\n",
    "- ‚úÖ Microsoft shop (Active Directory, Office 365, SQL Server)\n",
    "- ‚úÖ Need hybrid cloud (on-premises + cloud with Azure Arc)\n",
    "- ‚úÖ Enterprise compliance (90+ certifications)\n",
    "- ‚úÖ Using .NET/C# applications\n",
    "\n",
    "**Choose GCP if:**\n",
    "- ‚úÖ Data-heavy workloads (BigQuery best data warehouse)\n",
    "- ‚úÖ ML research (Vertex AI, TPUs, cutting-edge tools)\n",
    "- ‚úÖ Kubernetes-native (GKE most advanced)\n",
    "- ‚úÖ Cost-sensitive (20-30% cheaper than AWS/Azure)\n",
    "\n",
    "**Multi-Cloud Strategy:**\n",
    "- Primary: AWS (80% workloads, mature ecosystem)\n",
    "- Secondary: GCP (20% workloads, disaster recovery, BigQuery for analytics)\n",
    "- Avoid single-cloud lock-in (negotiate better pricing with multi-cloud threat)\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "**Immediate (Week 1):**\n",
    "- Create AWS/Azure/GCP free tier account (no credit card required)\n",
    "- Deploy simple ML model to SageMaker/Azure ML/Vertex AI (MNIST classifier)\n",
    "- Set up billing alerts ($10 budget, alert at $8)\n",
    "- Practice with CLI (aws, az, gcloud) and infrastructure as code (Terraform)\n",
    "\n",
    "**Short-term (1-3 months):**\n",
    "- Build end-to-end ML pipeline on one cloud (S3 ‚Üí SageMaker training ‚Üí endpoint ‚Üí monitoring)\n",
    "- Implement auto-scaling (scale 1-10 instances based on traffic)\n",
    "- Set up multi-region deployment (2 regions with failover)\n",
    "- Optimize costs (spot instances, reserved instances, auto-shutdown)\n",
    "- Integrate with CI/CD (GitHub Actions deploys to cloud on merge to main)\n",
    "\n",
    "**Long-term (3-6 months):**\n",
    "- Multi-cloud architecture (AWS primary, GCP secondary)\n",
    "- Advanced ML features (feature store, model registry, A/B testing, drift detection)\n",
    "- FinOps implementation (cost attribution, anomaly detection, commitment optimization)\n",
    "- Disaster recovery testing (quarterly failover drills, measure RTO/RPO)\n",
    "- Compliance (GDPR, HIPAA, SOC2) with audit trail and encryption\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Metrics to Track**\n",
    "\n",
    "**Cost Metrics:**\n",
    "- Monthly cloud spend: Target <5% of revenue (vs 15% on-premises infrastructure)\n",
    "- Cost per prediction: Target <$0.001/prediction (vs $0.01 on-premises)\n",
    "- Wasted spend: Target <10% (unused instances, over-provisioning, idle resources)\n",
    "- Reserved instance coverage: Target >60% for stable workloads (40% discount)\n",
    "\n",
    "**Performance Metrics:**\n",
    "- Latency P95: Target <100ms (vs 200ms on-premises)\n",
    "- Availability: Target 99.99% (vs 99.9% single region, 95% on-premises)\n",
    "- Throughput: Target 10K predictions/sec (vs 1K on-premises)\n",
    "- Scaling speed: Target scale from 1 to 100 instances in <5 minutes\n",
    "\n",
    "**Business Metrics:**\n",
    "- Time to deploy: Target <1 hour (vs 1 week on-premises)\n",
    "- Infrastructure cost reduction: Target 80% savings (cloud vs on-premises)\n",
    "- Developer productivity: Target 40% increase (less time on infrastructure)\n",
    "- Innovation speed: Target 3x more experiments (fast provisioning enables experimentation)\n",
    "\n",
    "---\n",
    "\n",
    "### üéì **Congratulations! You've Mastered Cloud Platforms!**\n",
    "\n",
    "You can now:\n",
    "- ‚úÖ **Deploy ML systems** on AWS (SageMaker), Azure (Azure ML), GCP (Vertex AI)\n",
    "- ‚úÖ **Optimize costs** with spot instances, auto-scaling, reserved instances, serverless\n",
    "- ‚úÖ **Build multi-region** architectures for 99.99% availability and disaster recovery\n",
    "- ‚úÖ **Implement multi-cloud** strategies to avoid vendor lock-in and negotiate pricing\n",
    "- ‚úÖ **Leverage managed services** (BigQuery, Cosmos DB, Lambda) for faster development\n",
    "- ‚úÖ **Monitor and troubleshoot** with CloudWatch, Azure Monitor, GCP Monitoring\n",
    "- ‚úÖ **Secure deployments** with IAM, encryption, VPC, secrets management\n",
    "- ‚úÖ **Build production systems** with 80% cost savings and 10x faster deployment\n",
    "\n",
    "**Next Notebook:** 143_Security_Compliance - IAM, encryption, audit trails, and compliance automation üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b2ca23",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "**When to Use**: Scalable ML infrastructure, managed services reduce ops overhead, multi-region deployments, variable workloads (autoscaling)\n",
    "\n",
    "**Limitations**: Vendor lock-in (proprietary APIs), costs escalate at scale ($50K-500K/month), data egress fees expensive, less control vs. on-premise\n",
    "\n",
    "**Best Practices**: Multi-cloud strategy for critical systems, use Terraform/Pulumi for IaC, monitor costs (CloudHealth, Kubecost), reserved instances for predictable workloads (40-60% discount)\n",
    "\n",
    "**Post-Silicon Application**: Multi-fab ML deployment (AWS+Azure), train models in cloud, serve on-premise for latency, save $180K/year infrastructure costs\n",
    "\n",
    "## üîç Diagnostic & Mastery\n",
    "\n",
    "‚úÖ Deploy ML models to AWS (SageMaker, EKS), GCP (Vertex AI, GKE), Azure (AML, AKS)  \n",
    "‚úÖ Use managed services vs. self-hosted K8s tradeoffs  \n",
    "‚úÖ Implement multi-cloud strategy with Terraform  \n",
    "‚úÖ Optimize cloud costs (spot instances, autoscaling, storage tiering)  \n",
    "‚úÖ Apply to semiconductor ML infrastructure  \n",
    "\n",
    "**Next**: 143_Security_Compliance, 145_Cost_Optimization\n",
    "\n",
    "## üìà Progress Update\n",
    "\n",
    "**Completed**: 41 notebooks (previous 39 + 140, 142)  \n",
    "**Progress**: ~85.1% (149/175 notebooks ‚â•15 cells)  \n",
    "**Next**: 7-cell and below notebooks ‚Üí 100% completion üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
