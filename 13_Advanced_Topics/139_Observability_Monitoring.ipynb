{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72da00ff",
   "metadata": {},
   "source": [
    "# 139: Observability & Monitoring - Prometheus, Grafana, and SLOs\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** observability vs monitoring (metrics, logs, traces vs alerting on thresholds)\n",
    "- **Implement** Prometheus metrics collection for ML model serving (latency, throughput, error rate)\n",
    "- **Build** Grafana dashboards with SLO tracking (99.9% availability, P95 latency <100ms)\n",
    "- **Deploy** AlertManager for intelligent alerting (grouping, deduplication, escalation)\n",
    "- **Apply** observability to semiconductor ML systems (yield prediction API, STDF processing pipelines)\n",
    "- **Monitor** golden signals (latency, traffic, errors, saturation) across distributed systems\n",
    "\n",
    "## üìö What is Observability?\n",
    "\n",
    "**Observability** is the ability to **understand system internal state** from external outputs (metrics, logs, traces). Unlike monitoring (react to known failures), observability enables **exploration and debugging** of unknown issues.\n",
    "\n",
    "**Three Pillars of Observability:**\n",
    "- **Metrics**: Numeric time-series data (CPU usage, request latency, error rate) - aggregated, low cardinality\n",
    "- **Logs**: Discrete events with context (request failed, user logged in, model prediction made) - high cardinality, searchable\n",
    "- **Traces**: Request journey across services (API call ‚Üí load balancer ‚Üí app server ‚Üí database ‚Üí model inference) - distributed systems\n",
    "\n",
    "**Why Observability?**\n",
    "- ‚úÖ **Proactive debugging**: Detect issues before users complain (latency spike, error rate increase)\n",
    "- ‚úÖ **Root cause analysis**: Quickly identify source of problems (slow database query, memory leak, model degradation)\n",
    "- ‚úÖ **Capacity planning**: Understand resource usage patterns (auto-scale before saturation)\n",
    "- ‚úÖ **SLO tracking**: Measure reliability (99.9% uptime = 43 minutes downtime/month allowed)\n",
    "\n",
    "**Golden Signals (Google SRE):**\n",
    "1. **Latency**: Response time (P50, P95, P99 percentiles)\n",
    "2. **Traffic**: Requests per second (RPS), throughput\n",
    "3. **Errors**: Error rate (5xx responses, exceptions, timeouts)\n",
    "4. **Saturation**: Resource utilization (CPU, memory, disk, network)\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "### **Use Case 1: Prometheus Metrics for Yield Prediction API**\n",
    "**Input:** ML API serving 5000 RPS with no observability (blind to latency, errors, model performance)  \n",
    "**Output:** Prometheus metrics track P95 latency (85ms), error rate (0.15%), prediction distribution, model version  \n",
    "**Value:** $4.2M/year from preventing outages (detect latency spikes before SLA violations, proactive scaling)\n",
    "\n",
    "### **Use Case 2: Grafana SLO Dashboard for STDF ETL Pipeline**\n",
    "**Input:** STDF batch processing pipeline with manual monitoring (engineers check logs reactively)  \n",
    "**Output:** Grafana dashboard tracks SLO (99.5% jobs complete in <30 minutes), alert on violations  \n",
    "**Value:** $3.5M/year from improved reliability (reduce failed ETL jobs by 60%, faster detection and recovery)\n",
    "\n",
    "### **Use Case 3: Distributed Tracing for Wafer Map Rendering Service**\n",
    "**Input:** Slow wafer map generation (4 seconds P95 latency), unclear which service is bottleneck  \n",
    "**Output:** Jaeger traces show 3.2 seconds spent in image resizing service (70% of total latency)  \n",
    "**Value:** $2.8M/year from performance optimization (optimize image service, reduce P95 to 1.2 seconds)\n",
    "\n",
    "### **Use Case 4: AlertManager for Parametric Test Anomaly Detection**\n",
    "**Input:** Outlier detection model alerts engineer for every anomaly (200 alerts/day, 80% false positives)  \n",
    "**Output:** AlertManager groups alerts, deduplicates, routes to on-call only for critical anomalies (95% noise reduction)  \n",
    "**Value:** $2.3M/year from reduced alert fatigue (engineers focus on real issues, 50% faster incident response)\n",
    "\n",
    "**Total Post-Silicon Value:** $4.2M + $3.5M + $2.8M + $2.3M = **$12.8M/year**\n",
    "\n",
    "## üîÑ Observability Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[üñ•Ô∏è ML Service] --> B[üìä Emit Metrics]\n",
    "    A --> C[üìù Write Logs]\n",
    "    A --> D[üîç Propagate Traces]\n",
    "    \n",
    "    B --> E[Prometheus]\n",
    "    C --> F[Loki/Elasticsearch]\n",
    "    D --> G[Jaeger/Tempo]\n",
    "    \n",
    "    E --> H[Grafana Dashboard]\n",
    "    F --> H\n",
    "    G --> H\n",
    "    \n",
    "    E --> I[AlertManager]\n",
    "    I --> J{Threshold Exceeded?}\n",
    "    J -->|Yes| K[üìß Alert On-Call]\n",
    "    J -->|No| L[‚úÖ Healthy]\n",
    "    \n",
    "    K --> M[üîß Investigate]\n",
    "    M --> N[üìä Query Metrics]\n",
    "    M --> O[üìù Search Logs]\n",
    "    M --> P[üîç Analyze Traces]\n",
    "    \n",
    "    N --> Q[üí° Identify Root Cause]\n",
    "    O --> Q\n",
    "    P --> Q\n",
    "    \n",
    "    Q --> R[üõ†Ô∏è Deploy Fix]\n",
    "    R --> S[üìà Verify Recovery]\n",
    "    S --> L\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style R fill:#e1ffe1\n",
    "    style K fill:#fff4e1\n",
    "    style Q fill:#ffe8cc\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **Notebook 134: Service Mesh (Istio)** - Service mesh provides automatic metrics, tracing\n",
    "- **Notebook 137: Infrastructure as Code** - Deploy observability stack with Terraform\n",
    "\n",
    "**Next Steps:**\n",
    "- **Notebook 140: Logging & Distributed Tracing** - Deep dive into logs and traces\n",
    "- **Notebook 144: Performance Optimization** - Use observability to identify bottlenecks\n",
    "\n",
    "---\n",
    "\n",
    "Let's build observable ML systems with Prometheus and Grafana! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f4065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "from enum import Enum\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60a0887",
   "metadata": {},
   "source": [
    "## 2. üìä Prometheus Metrics - Time-Series Monitoring\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement Prometheus-style metrics collection for ML model serving and training workloads.\n",
    "\n",
    "**Key Points:**\n",
    "- **Metric Types**: Counter (monotonic increase), Gauge (up/down values), Histogram (latency distributions), Summary (quantiles)\n",
    "- **Labels**: Dimensional data (e.g., `{model=\"yield_predictor\", version=\"v2.1\"}`) enable powerful queries\n",
    "- **Scraping**: Prometheus pulls metrics from `/metrics` endpoint every 15 seconds\n",
    "- **PromQL**: Query language for aggregations (`rate()`, `histogram_quantile()`, `avg()`)\n",
    "- **Time-Series Database**: Efficient storage with compression (1 year of data in <100GB)\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Real-Time Monitoring**: Detect issues as they happen (latency spike from 50ms ‚Üí 500ms)\n",
    "- **Historical Analysis**: Correlate model drift with data distribution changes over weeks\n",
    "- **Alerting**: Trigger PagerDuty when prediction latency P99 > 200ms for 5 minutes\n",
    "- **Capacity Planning**: Track GPU utilization trends to predict when to scale (85% ‚Üí add 2 nodes)\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "- **Scenario**: ML model serving API for device yield prediction (1000 req/sec)\n",
    "- **Metrics Collected**:\n",
    "  - Counter: `ml_predictions_total{model=\"yield_predictor\", result=\"success\"}` (total predictions)\n",
    "  - Gauge: `ml_model_accuracy{model=\"yield_predictor\"}` (current accuracy %)\n",
    "  - Histogram: `ml_prediction_latency_seconds{model=\"yield_predictor\"}` (latency distribution with P50/P95/P99)\n",
    "  - Gauge: `ml_gpu_utilization{gpu_id=\"0\"}` (GPU usage %)\n",
    "- **Query Example**: `histogram_quantile(0.99, ml_prediction_latency_seconds)` ‚Üí P99 latency = 85ms\n",
    "- **Alert**: `ml_model_accuracy < 0.90` for 10 minutes ‚Üí Trigger model retraining pipeline\n",
    "- **Result**: 60% faster incident detection (MTTR from 2 hours ‚Üí 45 minutes), $1.8M/year savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeea52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prometheus Metrics Simulation\n",
    "\n",
    "class MetricType(Enum):\n",
    "    \"\"\"Prometheus metric types\"\"\"\n",
    "    COUNTER = \"COUNTER\"      # Monotonically increasing (e.g., total requests)\n",
    "    GAUGE = \"GAUGE\"          # Can go up/down (e.g., CPU usage, accuracy)\n",
    "    HISTOGRAM = \"HISTOGRAM\"  # Latency distributions with buckets\n",
    "    SUMMARY = \"SUMMARY\"      # Similar to histogram, pre-calculated quantiles\n",
    "\n",
    "@dataclass\n",
    "class MetricValue:\n",
    "    \"\"\"Single metric observation\"\"\"\n",
    "    timestamp: datetime\n",
    "    value: float\n",
    "    labels: Dict[str, str] = field(default_factory=dict)\n",
    "\n",
    "class PrometheusMetric:\n",
    "    \"\"\"Base Prometheus metric\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, metric_type: MetricType, help_text: str):\n",
    "        self.name = name\n",
    "        self.metric_type = metric_type\n",
    "        self.help_text = help_text\n",
    "        self.values: List[MetricValue] = []\n",
    "    \n",
    "    def to_prometheus_format(self) -> str:\n",
    "        \"\"\"Export in Prometheus text exposition format\"\"\"\n",
    "        lines = []\n",
    "        lines.append(f\"# HELP {self.name} {self.help_text}\")\n",
    "        lines.append(f\"# TYPE {self.name} {self.metric_type.value.lower()}\")\n",
    "        \n",
    "        for val in self.values[-10:]:  # Last 10 values\n",
    "            labels_str = \",\".join([f'{k}=\"{v}\"' for k, v in val.labels.items()])\n",
    "            if labels_str:\n",
    "                lines.append(f\"{self.name}{{{labels_str}}} {val.value}\")\n",
    "            else:\n",
    "                lines.append(f\"{self.name} {val.value}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "class Counter(PrometheusMetric):\n",
    "    \"\"\"Counter metric (monotonically increasing)\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, help_text: str):\n",
    "        super().__init__(name, MetricType.COUNTER, help_text)\n",
    "        self._counters: Dict[str, float] = defaultdict(float)\n",
    "    \n",
    "    def inc(self, labels: Dict[str, str] = None, amount: float = 1.0):\n",
    "        \"\"\"Increment counter\"\"\"\n",
    "        labels = labels or {}\n",
    "        label_key = json.dumps(labels, sort_keys=True)\n",
    "        self._counters[label_key] += amount\n",
    "        \n",
    "        self.values.append(MetricValue(\n",
    "            timestamp=datetime.now(),\n",
    "            value=self._counters[label_key],\n",
    "            labels=labels\n",
    "        ))\n",
    "\n",
    "class Gauge(PrometheusMetric):\n",
    "    \"\"\"Gauge metric (can go up or down)\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, help_text: str):\n",
    "        super().__init__(name, MetricType.GAUGE, help_text)\n",
    "        self._gauges: Dict[str, float] = defaultdict(float)\n",
    "    \n",
    "    def set(self, value: float, labels: Dict[str, str] = None):\n",
    "        \"\"\"Set gauge value\"\"\"\n",
    "        labels = labels or {}\n",
    "        label_key = json.dumps(labels, sort_keys=True)\n",
    "        self._gauges[label_key] = value\n",
    "        \n",
    "        self.values.append(MetricValue(\n",
    "            timestamp=datetime.now(),\n",
    "            value=value,\n",
    "            labels=labels\n",
    "        ))\n",
    "    \n",
    "    def inc(self, labels: Dict[str, str] = None, amount: float = 1.0):\n",
    "        \"\"\"Increment gauge\"\"\"\n",
    "        labels = labels or {}\n",
    "        label_key = json.dumps(labels, sort_keys=True)\n",
    "        self._gauges[label_key] += amount\n",
    "        self.set(self._gauges[label_key], labels)\n",
    "    \n",
    "    def dec(self, labels: Dict[str, str] = None, amount: float = 1.0):\n",
    "        \"\"\"Decrement gauge\"\"\"\n",
    "        self.inc(labels, -amount)\n",
    "\n",
    "class Histogram(PrometheusMetric):\n",
    "    \"\"\"Histogram metric (latency distributions)\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, help_text: str, buckets: List[float] = None):\n",
    "        super().__init__(name, MetricType.HISTOGRAM, help_text)\n",
    "        self.buckets = buckets or [0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1.0, 2.5, 5.0, 7.5, 10.0]\n",
    "        self._observations: Dict[str, List[float]] = defaultdict(list)\n",
    "    \n",
    "    def observe(self, value: float, labels: Dict[str, str] = None):\n",
    "        \"\"\"Record observation\"\"\"\n",
    "        labels = labels or {}\n",
    "        label_key = json.dumps(labels, sort_keys=True)\n",
    "        self._observations[label_key].append(value)\n",
    "        \n",
    "        self.values.append(MetricValue(\n",
    "            timestamp=datetime.now(),\n",
    "            value=value,\n",
    "            labels=labels\n",
    "        ))\n",
    "    \n",
    "    def get_quantile(self, quantile: float, labels: Dict[str, str] = None) -> float:\n",
    "        \"\"\"Calculate quantile (e.g., 0.99 for P99)\"\"\"\n",
    "        labels = labels or {}\n",
    "        label_key = json.dumps(labels, sort_keys=True)\n",
    "        observations = self._observations.get(label_key, [])\n",
    "        \n",
    "        if not observations:\n",
    "            return 0.0\n",
    "        \n",
    "        sorted_obs = sorted(observations)\n",
    "        index = int(len(sorted_obs) * quantile)\n",
    "        return sorted_obs[min(index, len(sorted_obs) - 1)]\n",
    "    \n",
    "    def get_bucket_counts(self, labels: Dict[str, str] = None) -> Dict[float, int]:\n",
    "        \"\"\"Get counts per bucket (for histogram_quantile in PromQL)\"\"\"\n",
    "        labels = labels or {}\n",
    "        label_key = json.dumps(labels, sort_keys=True)\n",
    "        observations = self._observations.get(label_key, [])\n",
    "        \n",
    "        counts = {}\n",
    "        for bucket in self.buckets:\n",
    "            counts[bucket] = sum(1 for obs in observations if obs <= bucket)\n",
    "        counts[float('inf')] = len(observations)  # +Inf bucket\n",
    "        \n",
    "        return counts\n",
    "\n",
    "class MetricsRegistry:\n",
    "    \"\"\"Prometheus metrics registry\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics: Dict[str, PrometheusMetric] = {}\n",
    "    \n",
    "    def register(self, metric: PrometheusMetric):\n",
    "        \"\"\"Register metric\"\"\"\n",
    "        self.metrics[metric.name] = metric\n",
    "    \n",
    "    def get_metric(self, name: str) -> Optional[PrometheusMetric]:\n",
    "        \"\"\"Get metric by name\"\"\"\n",
    "        return self.metrics.get(name)\n",
    "    \n",
    "    def scrape(self) -> str:\n",
    "        \"\"\"Scrape all metrics (Prometheus /metrics endpoint)\"\"\"\n",
    "        output = []\n",
    "        for metric in self.metrics.values():\n",
    "            output.append(metric.to_prometheus_format())\n",
    "        return \"\\n\\n\".join(output)\n",
    "\n",
    "# Example 1: ML Model Serving Metrics\n",
    "print(\"=\" * 70)\n",
    "print(\"Example 1: ML Model Serving Metrics (Yield Prediction API)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "registry = MetricsRegistry()\n",
    "\n",
    "# Register metrics\n",
    "predictions_total = Counter(\n",
    "    name=\"ml_predictions_total\",\n",
    "    help_text=\"Total number of ML predictions made\"\n",
    ")\n",
    "registry.register(predictions_total)\n",
    "\n",
    "model_accuracy = Gauge(\n",
    "    name=\"ml_model_accuracy\",\n",
    "    help_text=\"Current model accuracy (0.0 to 1.0)\"\n",
    ")\n",
    "registry.register(model_accuracy)\n",
    "\n",
    "prediction_latency = Histogram(\n",
    "    name=\"ml_prediction_latency_seconds\",\n",
    "    help_text=\"ML prediction latency in seconds\"\n",
    ")\n",
    "registry.register(prediction_latency)\n",
    "\n",
    "gpu_utilization = Gauge(\n",
    "    name=\"ml_gpu_utilization_percent\",\n",
    "    help_text=\"GPU utilization percentage\"\n",
    ")\n",
    "registry.register(gpu_utilization)\n",
    "\n",
    "print(\"\\nüìä Simulating ML model serving for 100 requests...\")\n",
    "print(\"   Model: yield_predictor (device yield prediction)\")\n",
    "print(\"   SLA: P99 latency < 200ms, accuracy > 90%\")\n",
    "\n",
    "# Simulate predictions\n",
    "for i in range(100):\n",
    "    # Simulate prediction\n",
    "    labels = {\"model\": \"yield_predictor\", \"version\": \"v2.1\"}\n",
    "    \n",
    "    # Record prediction\n",
    "    predictions_total.inc(labels=labels)\n",
    "    \n",
    "    # Simulate latency (most fast, some slow)\n",
    "    if random.random() < 0.95:\n",
    "        latency = random.uniform(0.03, 0.08)  # 30-80ms (normal)\n",
    "    else:\n",
    "        latency = random.uniform(0.15, 0.25)  # 150-250ms (slow outliers)\n",
    "    \n",
    "    prediction_latency.observe(latency, labels=labels)\n",
    "    \n",
    "    # Simulate GPU utilization\n",
    "    gpu_util = random.uniform(70, 85)\n",
    "    gpu_utilization.set(gpu_util, labels={\"gpu_id\": \"0\"})\n",
    "\n",
    "# Set model accuracy\n",
    "model_accuracy.set(0.932, labels={\"model\": \"yield_predictor\", \"dataset\": \"validation\"})\n",
    "\n",
    "# Calculate metrics\n",
    "print(f\"\\n‚úÖ Metrics Summary:\")\n",
    "print(f\"   Total Predictions: {predictions_total._counters[json.dumps(labels, sort_keys=True)]:.0f}\")\n",
    "print(f\"   Model Accuracy: {0.932:.1%}\")\n",
    "print(f\"   Latency P50: {prediction_latency.get_quantile(0.50, labels) * 1000:.1f}ms\")\n",
    "print(f\"   Latency P95: {prediction_latency.get_quantile(0.95, labels) * 1000:.1f}ms\")\n",
    "print(f\"   Latency P99: {prediction_latency.get_quantile(0.99, labels) * 1000:.1f}ms\")\n",
    "print(f\"   GPU Utilization: {gpu_util:.1f}%\")\n",
    "\n",
    "# Check SLA\n",
    "p99_latency_ms = prediction_latency.get_quantile(0.99, labels) * 1000\n",
    "if p99_latency_ms < 200:\n",
    "    print(f\"\\n‚úÖ SLA Met: P99 latency {p99_latency_ms:.1f}ms < 200ms\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå SLA Breach: P99 latency {p99_latency_ms:.1f}ms > 200ms\")\n",
    "\n",
    "# Example 2: Multi-Model Comparison\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"Example 2: Multi-Model Performance Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä Simulating 3 models: yield_predictor, binning_optimizer, failure_classifier\")\n",
    "\n",
    "models = [\n",
    "    {\"name\": \"yield_predictor\", \"accuracy\": 0.932, \"latency_base\": 0.05},\n",
    "    {\"name\": \"binning_optimizer\", \"accuracy\": 0.878, \"latency_base\": 0.12},\n",
    "    {\"name\": \"failure_classifier\", \"accuracy\": 0.945, \"latency_base\": 0.08}\n",
    "]\n",
    "\n",
    "for model_info in models:\n",
    "    labels = {\"model\": model_info[\"name\"], \"version\": \"v1.0\"}\n",
    "    \n",
    "    # Simulate 50 requests per model\n",
    "    for _ in range(50):\n",
    "        predictions_total.inc(labels=labels)\n",
    "        latency = model_info[\"latency_base\"] + random.uniform(-0.01, 0.02)\n",
    "        prediction_latency.observe(latency, labels=labels)\n",
    "    \n",
    "    # Set accuracy\n",
    "    model_accuracy.set(model_info[\"accuracy\"], labels={\"model\": model_info[\"name\"], \"dataset\": \"validation\"})\n",
    "\n",
    "print(\"\\n‚úÖ Model Performance Summary:\")\n",
    "for model_info in models:\n",
    "    labels = {\"model\": model_info[\"name\"], \"version\": \"v1.0\"}\n",
    "    p50 = prediction_latency.get_quantile(0.50, labels) * 1000\n",
    "    p99 = prediction_latency.get_quantile(0.99, labels) * 1000\n",
    "    acc = model_info[\"accuracy\"]\n",
    "    \n",
    "    print(f\"\\n   {model_info['name']}:\")\n",
    "    print(f\"     Accuracy: {acc:.1%}\")\n",
    "    print(f\"     Latency P50: {p50:.1f}ms\")\n",
    "    print(f\"     Latency P99: {p99:.1f}ms\")\n",
    "\n",
    "# Example 3: Prometheus Scrape Endpoint\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"Example 3: Prometheus /metrics Endpoint\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä Simulating Prometheus scrape (GET /metrics)...\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(registry.scrape())\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n‚úÖ Prometheus metrics demonstrated: Counters, Gauges, Histograms with labels!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5976c62",
   "metadata": {},
   "source": [
    "## 3. üìà Grafana Dashboards - Visualization and Alerting\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Build interactive dashboards to visualize ML system health and trigger alerts on anomalies.\n",
    "\n",
    "**Key Points:**\n",
    "- **Panels**: Time-series graphs, gauges, heatmaps, tables (visualize metrics from Prometheus)\n",
    "- **Templating**: Dynamic dashboards with variables (select model: yield_predictor, binning_optimizer)\n",
    "- **Alerting**: Trigger notifications when metrics cross thresholds (accuracy < 90% for 10 minutes)\n",
    "- **Annotations**: Mark deployments, incidents on graphs (correlate performance drops with changes)\n",
    "- **Drill-Down**: Click graph ‚Üí filter by specific model/service (investigate anomalies)\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Visual Discovery**: Spot trends invisible in raw metrics (gradual latency increase over 3 days)\n",
    "- **Correlation**: Overlay multiple metrics (CPU spike correlates with batch job start time)\n",
    "- **Proactive Alerts**: Notify on-call engineer before users report issues (P99 latency trending up)\n",
    "- **Executive Dashboards**: Show business metrics (model uptime 99.95%, $12K cost savings this month)\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "- **Dashboard 1: STDF Processing Pipeline**\n",
    "  - Panel 1: File processing rate (files/sec) with 15-minute average\n",
    "  - Panel 2: Parsing latency P50/P95/P99 (heatmap showing time-of-day patterns)\n",
    "  - Panel 3: Yield prediction accuracy (gauge with 90% threshold marker)\n",
    "  - Panel 4: Error rate % (alert if >1% for 5 minutes)\n",
    "- **Dashboard 2: ML Model Serving**\n",
    "  - Panel 1: Request rate (req/sec) with capacity line at 1500 req/sec\n",
    "  - Panel 2: Latency percentiles (P50/P95/P99) with SLA threshold (200ms)\n",
    "  - Panel 3: GPU utilization per device (bar chart, alert if >90%)\n",
    "  - Panel 4: Cost per 1000 predictions (trend line showing optimization impact)\n",
    "- **Result**: 40% faster anomaly detection (engineers spot issues in dashboard before alerts fire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafana Dashboard Simulation\n",
    "\n",
    "class PanelType(Enum):\n",
    "    \"\"\"Grafana panel types\"\"\"\n",
    "    GRAPH = \"GRAPH\"          # Time-series line chart\n",
    "    GAUGE = \"GAUGE\"          # Single value with thresholds\n",
    "    HEATMAP = \"HEATMAP\"      # 2D histogram (latency over time)\n",
    "    TABLE = \"TABLE\"          # Tabular data\n",
    "    STAT = \"STAT\"            # Single stat with trend\n",
    "    BAR_CHART = \"BAR_CHART\"  # Bar chart\n",
    "\n",
    "class AlertSeverity(Enum):\n",
    "    \"\"\"Alert severity levels\"\"\"\n",
    "    INFO = \"INFO\"\n",
    "    WARNING = \"WARNING\"\n",
    "    CRITICAL = \"CRITICAL\"\n",
    "\n",
    "@dataclass\n",
    "class GrafanaPanel:\n",
    "    \"\"\"Grafana dashboard panel\"\"\"\n",
    "    title: str\n",
    "    panel_type: PanelType\n",
    "    query: str  # PromQL query\n",
    "    thresholds: List[Tuple[str, float]] = field(default_factory=list)  # [(level, value)]\n",
    "    unit: str = \"\"\n",
    "    \n",
    "    def evaluate(self, value: float) -> Optional[str]:\n",
    "        \"\"\"Evaluate current value against thresholds\"\"\"\n",
    "        for level, threshold in reversed(self.thresholds):\n",
    "            if value >= threshold:\n",
    "                return level\n",
    "        return None\n",
    "\n",
    "@dataclass\n",
    "class AlertRule:\n",
    "    \"\"\"Grafana alert rule\"\"\"\n",
    "    name: str\n",
    "    query: str\n",
    "    condition: str  # e.g., \"> 0.90\"\n",
    "    duration: str   # e.g., \"5m\"\n",
    "    severity: AlertSeverity\n",
    "    message: str\n",
    "    \n",
    "    def evaluate(self, value: float) -> bool:\n",
    "        \"\"\"Evaluate if alert should fire\"\"\"\n",
    "        # Simplified evaluation\n",
    "        operator = self.condition.split()[0]\n",
    "        threshold = float(self.condition.split()[1])\n",
    "        \n",
    "        if operator == \">\":\n",
    "            return value > threshold\n",
    "        elif operator == \"<\":\n",
    "            return value < threshold\n",
    "        elif operator == \">=\":\n",
    "            return value >= threshold\n",
    "        elif operator == \"<=\":\n",
    "            return value <= threshold\n",
    "        \n",
    "        return False\n",
    "\n",
    "class GrafanaDashboard:\n",
    "    \"\"\"Grafana dashboard\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, description: str = \"\"):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.panels: List[GrafanaPanel] = []\n",
    "        self.alerts: List[AlertRule] = []\n",
    "        self.variables: Dict[str, List[str]] = {}\n",
    "    \n",
    "    def add_panel(self, panel: GrafanaPanel):\n",
    "        \"\"\"Add panel to dashboard\"\"\"\n",
    "        self.panels.append(panel)\n",
    "    \n",
    "    def add_alert(self, alert: AlertRule):\n",
    "        \"\"\"Add alert rule\"\"\"\n",
    "        self.alerts.append(alert)\n",
    "    \n",
    "    def add_variable(self, name: str, values: List[str]):\n",
    "        \"\"\"Add template variable\"\"\"\n",
    "        self.variables[name] = values\n",
    "    \n",
    "    def render_panel(self, panel: GrafanaPanel, current_value: float):\n",
    "        \"\"\"Render panel (simulate visualization)\"\"\"\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"üìä {panel.title}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        print(f\"Query: {panel.query}\")\n",
    "        print(f\"Current Value: {current_value:.2f} {panel.unit}\")\n",
    "        \n",
    "        # Check thresholds\n",
    "        status = panel.evaluate(current_value)\n",
    "        if status:\n",
    "            if \"CRITICAL\" in status:\n",
    "                print(f\"Status: üî¥ {status}\")\n",
    "            elif \"WARNING\" in status:\n",
    "                print(f\"Status: üü° {status}\")\n",
    "            else:\n",
    "                print(f\"Status: üü¢ {status}\")\n",
    "        else:\n",
    "            print(f\"Status: üü¢ OK\")\n",
    "        \n",
    "        # Visualize with simple bar\n",
    "        if panel.panel_type == PanelType.GAUGE:\n",
    "            bar_length = int((current_value / 100) * 40)\n",
    "            bar = \"‚ñà\" * bar_length + \"‚ñë\" * (40 - bar_length)\n",
    "            print(f\"\\n{bar} {current_value:.1f}%\")\n",
    "    \n",
    "    def check_alerts(self, metric_values: Dict[str, float]):\n",
    "        \"\"\"Check all alert rules\"\"\"\n",
    "        triggered_alerts = []\n",
    "        \n",
    "        for alert in self.alerts:\n",
    "            # Simplified: assume metric_values contains values for alert queries\n",
    "            for metric_name, value in metric_values.items():\n",
    "                if metric_name in alert.query:\n",
    "                    if alert.evaluate(value):\n",
    "                        triggered_alerts.append((alert, value))\n",
    "        \n",
    "        return triggered_alerts\n",
    "\n",
    "# Example 1: ML Model Serving Dashboard\n",
    "print(\"=\" * 70)\n",
    "print(\"Example 1: ML Model Serving Dashboard\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dashboard = GrafanaDashboard(\n",
    "    name=\"ML Model Serving - Yield Predictor\",\n",
    "    description=\"Monitor ML model serving performance, latency, and accuracy\"\n",
    ")\n",
    "\n",
    "# Add template variable\n",
    "dashboard.add_variable(\"model\", [\"yield_predictor\", \"binning_optimizer\", \"failure_classifier\"])\n",
    "\n",
    "# Panel 1: Request Rate\n",
    "request_rate_panel = GrafanaPanel(\n",
    "    title=\"Request Rate\",\n",
    "    panel_type=PanelType.GRAPH,\n",
    "    query=\"rate(ml_predictions_total{model='$model'}[5m])\",\n",
    "    unit=\"req/sec\",\n",
    "    thresholds=[\n",
    "        (\"OK\", 0),\n",
    "        (\"WARNING\", 800),\n",
    "        (\"CRITICAL\", 1200)\n",
    "    ]\n",
    ")\n",
    "dashboard.add_panel(request_rate_panel)\n",
    "\n",
    "# Panel 2: P99 Latency\n",
    "latency_panel = GrafanaPanel(\n",
    "    title=\"Prediction Latency P99\",\n",
    "    panel_type=PanelType.GRAPH,\n",
    "    query=\"histogram_quantile(0.99, ml_prediction_latency_seconds{model='$model'})\",\n",
    "    unit=\"ms\",\n",
    "    thresholds=[\n",
    "        (\"OK\", 0),\n",
    "        (\"WARNING\", 150),\n",
    "        (\"CRITICAL\", 200)\n",
    "    ]\n",
    ")\n",
    "dashboard.add_panel(latency_panel)\n",
    "\n",
    "# Panel 3: Model Accuracy\n",
    "accuracy_panel = GrafanaPanel(\n",
    "    title=\"Model Accuracy\",\n",
    "    panel_type=PanelType.GAUGE,\n",
    "    query=\"ml_model_accuracy{model='$model'}\",\n",
    "    unit=\"%\",\n",
    "    thresholds=[\n",
    "        (\"CRITICAL\", 0),\n",
    "        (\"WARNING\", 85),\n",
    "        (\"OK\", 90)\n",
    "    ]\n",
    ")\n",
    "dashboard.add_panel(accuracy_panel)\n",
    "\n",
    "# Panel 4: GPU Utilization\n",
    "gpu_panel = GrafanaPanel(\n",
    "    title=\"GPU Utilization\",\n",
    "    panel_type=PanelType.GAUGE,\n",
    "    query=\"ml_gpu_utilization_percent{gpu_id='0'}\",\n",
    "    unit=\"%\",\n",
    "    thresholds=[\n",
    "        (\"OK\", 0),\n",
    "        (\"WARNING\", 85),\n",
    "        (\"CRITICAL\", 95)\n",
    "    ]\n",
    ")\n",
    "dashboard.add_panel(gpu_panel)\n",
    "\n",
    "# Add alert rules\n",
    "dashboard.add_alert(AlertRule(\n",
    "    name=\"High P99 Latency\",\n",
    "    query=\"histogram_quantile(0.99, ml_prediction_latency_seconds{model='yield_predictor'})\",\n",
    "    condition=\"> 0.20\",  # 200ms\n",
    "    duration=\"5m\",\n",
    "    severity=AlertSeverity.CRITICAL,\n",
    "    message=\"P99 latency > 200ms for 5 minutes. Investigate slow queries or scale infrastructure.\"\n",
    "))\n",
    "\n",
    "dashboard.add_alert(AlertRule(\n",
    "    name=\"Low Model Accuracy\",\n",
    "    query=\"ml_model_accuracy{model='yield_predictor'}\",\n",
    "    condition=\"< 0.90\",\n",
    "    duration=\"10m\",\n",
    "    severity=AlertSeverity.WARNING,\n",
    "    message=\"Model accuracy < 90% for 10 minutes. Model drift detected, trigger retraining.\"\n",
    "))\n",
    "\n",
    "dashboard.add_alert(AlertRule(\n",
    "    name=\"High GPU Utilization\",\n",
    "    query=\"ml_gpu_utilization_percent{gpu_id='0'}\",\n",
    "    condition=\"> 90\",\n",
    "    duration=\"15m\",\n",
    "    severity=AlertSeverity.WARNING,\n",
    "    message=\"GPU utilization > 90% for 15 minutes. Consider adding more GPU nodes.\"\n",
    "))\n",
    "\n",
    "# Simulate current metrics\n",
    "print(f\"\\nüìä Dashboard: {dashboard.name}\")\n",
    "print(f\"Description: {dashboard.description}\")\n",
    "\n",
    "current_metrics = {\n",
    "    \"request_rate\": 650,       # req/sec\n",
    "    \"p99_latency\": 85,         # ms\n",
    "    \"model_accuracy\": 93.2,    # %\n",
    "    \"gpu_utilization\": 78.5    # %\n",
    "}\n",
    "\n",
    "# Render panels\n",
    "dashboard.render_panel(request_rate_panel, current_metrics[\"request_rate\"])\n",
    "dashboard.render_panel(latency_panel, current_metrics[\"p99_latency\"])\n",
    "dashboard.render_panel(accuracy_panel, current_metrics[\"model_accuracy\"])\n",
    "dashboard.render_panel(gpu_panel, current_metrics[\"gpu_utilization\"])\n",
    "\n",
    "# Check alerts\n",
    "print(f\"\\n\\n{'=' * 60}\")\n",
    "print(\"üîî Alert Status\")\n",
    "print(f\"{'=' * 60}\")\n",
    "\n",
    "metric_values = {\n",
    "    \"ml_prediction_latency_seconds\": current_metrics[\"p99_latency\"] / 1000,  # Convert to seconds\n",
    "    \"ml_model_accuracy\": current_metrics[\"model_accuracy\"] / 100,\n",
    "    \"ml_gpu_utilization_percent\": current_metrics[\"gpu_utilization\"]\n",
    "}\n",
    "\n",
    "triggered_alerts = dashboard.check_alerts(metric_values)\n",
    "\n",
    "if triggered_alerts:\n",
    "    for alert, value in triggered_alerts:\n",
    "        print(f\"\\nüö® {alert.severity.value}: {alert.name}\")\n",
    "        print(f\"   Value: {value}\")\n",
    "        print(f\"   Condition: {alert.condition}\")\n",
    "        print(f\"   Message: {alert.message}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No alerts triggered. All systems healthy!\")\n",
    "\n",
    "# Example 2: STDF Processing Pipeline Dashboard\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"Example 2: STDF Processing Pipeline Dashboard\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "pipeline_dashboard = GrafanaDashboard(\n",
    "    name=\"STDF Processing Pipeline\",\n",
    "    description=\"Monitor STDF file parsing and yield prediction pipeline\"\n",
    ")\n",
    "\n",
    "# Panel: File Processing Rate\n",
    "processing_panel = GrafanaPanel(\n",
    "    title=\"STDF Files Processed\",\n",
    "    panel_type=PanelType.STAT,\n",
    "    query=\"rate(stdf_files_processed_total[15m])\",\n",
    "    unit=\"files/min\",\n",
    "    thresholds=[\n",
    "        (\"CRITICAL\", 0),\n",
    "        (\"WARNING\", 5),\n",
    "        (\"OK\", 10)\n",
    "    ]\n",
    ")\n",
    "pipeline_dashboard.add_panel(processing_panel)\n",
    "\n",
    "# Panel: Parsing Latency\n",
    "parsing_latency_panel = GrafanaPanel(\n",
    "    title=\"STDF Parsing Latency P95\",\n",
    "    panel_type=PanelType.HEATMAP,\n",
    "    query=\"histogram_quantile(0.95, stdf_parsing_latency_seconds)\",\n",
    "    unit=\"ms\",\n",
    "    thresholds=[\n",
    "        (\"OK\", 0),\n",
    "        (\"WARNING\", 400),\n",
    "        (\"CRITICAL\", 500)\n",
    "    ]\n",
    ")\n",
    "pipeline_dashboard.add_panel(parsing_latency_panel)\n",
    "\n",
    "# Panel: Yield Prediction Throughput\n",
    "throughput_panel = GrafanaPanel(\n",
    "    title=\"Yield Predictions per Second\",\n",
    "    panel_type=PanelType.GRAPH,\n",
    "    query=\"rate(yield_predictions_total[5m])\",\n",
    "    unit=\"predictions/sec\",\n",
    "    thresholds=[\n",
    "        (\"OK\", 0),\n",
    "        (\"WARNING\", 800),\n",
    "        (\"CRITICAL\", 1000)\n",
    "    ]\n",
    ")\n",
    "pipeline_dashboard.add_panel(throughput_panel)\n",
    "\n",
    "print(f\"\\nüìä Dashboard: {pipeline_dashboard.name}\")\n",
    "\n",
    "pipeline_metrics = {\n",
    "    \"file_processing_rate\": 12.5,  # files/min\n",
    "    \"parsing_latency_p95\": 380,    # ms\n",
    "    \"prediction_throughput\": 450   # predictions/sec\n",
    "}\n",
    "\n",
    "pipeline_dashboard.render_panel(processing_panel, pipeline_metrics[\"file_processing_rate\"])\n",
    "pipeline_dashboard.render_panel(parsing_latency_panel, pipeline_metrics[\"parsing_latency_p95\"])\n",
    "pipeline_dashboard.render_panel(throughput_panel, pipeline_metrics[\"prediction_throughput\"])\n",
    "\n",
    "# Example 3: Multi-Model Comparison Dashboard\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"Example 3: Multi-Model Comparison Dashboard\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison_dashboard = GrafanaDashboard(\n",
    "    name=\"Multi-Model Performance Comparison\",\n",
    "    description=\"Compare performance across yield_predictor, binning_optimizer, failure_classifier\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Dashboard: {comparison_dashboard.name}\")\n",
    "print(\"\\nModel Performance Summary (Bar Chart):\")\n",
    "print(f\"{'=' * 60}\")\n",
    "\n",
    "models_comparison = [\n",
    "    {\"name\": \"yield_predictor\", \"accuracy\": 93.2, \"latency_p99\": 85, \"throughput\": 650},\n",
    "    {\"name\": \"binning_optimizer\", \"accuracy\": 87.8, \"latency_p99\": 125, \"throughput\": 420},\n",
    "    {\"name\": \"failure_classifier\", \"accuracy\": 94.5, \"latency_p99\": 95, \"throughput\": 580}\n",
    "]\n",
    "\n",
    "for model in models_comparison:\n",
    "    print(f\"\\n{model['name']}:\")\n",
    "    print(f\"  Accuracy: {'‚ñà' * int(model['accuracy'] / 10)} {model['accuracy']:.1f}%\")\n",
    "    print(f\"  Latency P99: {'‚ñà' * int(model['latency_p99'] / 10)} {model['latency_p99']:.0f}ms\")\n",
    "    print(f\"  Throughput: {'‚ñà' * int(model['throughput'] / 50)} {model['throughput']} req/sec\")\n",
    "\n",
    "print(f\"\\n\\n‚úÖ Grafana dashboards demonstrated: Panels, alerts, thresholds, visualizations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766dc4b2",
   "metadata": {},
   "source": [
    "## 4. üîç Distributed Tracing - OpenTelemetry and Jaeger\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Track requests across microservices to identify bottlenecks and debug latency issues.\n",
    "\n",
    "**Key Points:**\n",
    "- **Spans**: Individual operations (database query, model inference, HTTP request)\n",
    "- **Traces**: Collection of spans forming complete request journey (API ‚Üí Feature Store ‚Üí Model ‚Üí DB)\n",
    "- **Context Propagation**: Pass trace_id across services (correlate spans from different microservices)\n",
    "- **Sampling**: Trace 1% of production traffic (reduce overhead, maintain visibility)\n",
    "- **Baggage**: Carry metadata across spans (user_id, tenant_id, experiment_id)\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Latency Attribution**: Which service caused 2s delay? (Database: 1.8s vs Model: 0.05s)\n",
    "- **Cascading Failures**: Trace shows API timeout caused by slow feature store query\n",
    "- **Optimization**: Identify N+1 queries (100 DB calls for 1 prediction ‚Üí fix with batching)\n",
    "- **Root Cause Analysis**: Trace shows exactly which span failed and why\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "- **Scenario**: STDF processing API latency spike from 200ms ‚Üí 3s\n",
    "- **Trace Investigation**:\n",
    "  - Span 1: API Gateway (10ms) ‚úÖ\n",
    "  - Span 2: Authentication (5ms) ‚úÖ\n",
    "  - Span 3: Feature Store Query (15ms) ‚úÖ\n",
    "  - Span 4: STDF Parser (2950ms) ‚ùå **bottleneck identified!**\n",
    "  - Span 5: Yield Prediction Model (25ms) ‚úÖ\n",
    "  - Span 6: Database Write (12ms) ‚úÖ\n",
    "- **Root Cause**: STDF parser loading full 10GB file into memory (OOM thrashing)\n",
    "- **Fix**: Streaming parser (process 1MB chunks) ‚Üí latency reduced to 180ms\n",
    "- **Result**: 60% faster debugging (MTTR from 2 hours ‚Üí 45 minutes), $1.8M/year savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95662e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed Tracing Simulation\n",
    "\n",
    "class SpanKind(Enum):\n",
    "    \"\"\"Span types\"\"\"\n",
    "    SERVER = \"SERVER\"      # Receiving request\n",
    "    CLIENT = \"CLIENT\"      # Making request\n",
    "    INTERNAL = \"INTERNAL\"  # Internal operation\n",
    "    PRODUCER = \"PRODUCER\"  # Message queue producer\n",
    "    CONSUMER = \"CONSUMER\"  # Message queue consumer\n",
    "\n",
    "@dataclass\n",
    "class Span:\n",
    "    \"\"\"Distributed trace span\"\"\"\n",
    "    span_id: str\n",
    "    trace_id: str\n",
    "    parent_span_id: Optional[str]\n",
    "    operation_name: str\n",
    "    service_name: str\n",
    "    start_time: datetime\n",
    "    duration_ms: float\n",
    "    kind: SpanKind\n",
    "    tags: Dict[str, Any] = field(default_factory=dict)\n",
    "    logs: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    status: str = \"OK\"  # OK, ERROR\n",
    "    \n",
    "    def end_time(self) -> datetime:\n",
    "        \"\"\"Calculate end time\"\"\"\n",
    "        return self.start_time + timedelta(milliseconds=self.duration_ms)\n",
    "    \n",
    "    def add_tag(self, key: str, value: Any):\n",
    "        \"\"\"Add tag (metadata)\"\"\"\n",
    "        self.tags[key] = value\n",
    "    \n",
    "    def log_event(self, message: str, level: str = \"INFO\"):\n",
    "        \"\"\"Add log event\"\"\"\n",
    "        self.logs.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"level\": level,\n",
    "            \"message\": message\n",
    "        })\n",
    "\n",
    "@dataclass\n",
    "class Trace:\n",
    "    \"\"\"Complete distributed trace\"\"\"\n",
    "    trace_id: str\n",
    "    spans: List[Span] = field(default_factory=list)\n",
    "    \n",
    "    def add_span(self, span: Span):\n",
    "        \"\"\"Add span to trace\"\"\"\n",
    "        self.spans.append(span)\n",
    "    \n",
    "    def get_root_span(self) -> Optional[Span]:\n",
    "        \"\"\"Get root span (no parent)\"\"\"\n",
    "        for span in self.spans:\n",
    "            if span.parent_span_id is None:\n",
    "                return span\n",
    "        return None\n",
    "    \n",
    "    def get_critical_path(self) -> List[Span]:\n",
    "        \"\"\"Get critical path (longest latency chain)\"\"\"\n",
    "        # Simplified: return spans sorted by start time\n",
    "        return sorted(self.spans, key=lambda s: s.start_time)\n",
    "    \n",
    "    def total_duration_ms(self) -> float:\n",
    "        \"\"\"Calculate total trace duration\"\"\"\n",
    "        if not self.spans:\n",
    "            return 0.0\n",
    "        root = self.get_root_span()\n",
    "        return root.duration_ms if root else 0.0\n",
    "    \n",
    "    def visualize(self):\n",
    "        \"\"\"Visualize trace as waterfall\"\"\"\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"Trace ID: {self.trace_id}\")\n",
    "        print(f\"Total Duration: {self.total_duration_ms():.2f}ms\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        \n",
    "        root = self.get_root_span()\n",
    "        if not root:\n",
    "            return\n",
    "        \n",
    "        # Print spans in timeline order\n",
    "        for span in self.get_critical_path():\n",
    "            indent = \"  \" * (1 if span.parent_span_id else 0)\n",
    "            bar_length = int((span.duration_ms / root.duration_ms) * 50)\n",
    "            bar = \"‚ñà\" * bar_length\n",
    "            \n",
    "            status_icon = \"‚úÖ\" if span.status == \"OK\" else \"‚ùå\"\n",
    "            print(f\"\\n{status_icon} {indent}{span.service_name}: {span.operation_name}\")\n",
    "            print(f\"   {indent}Duration: {span.duration_ms:.2f}ms {bar}\")\n",
    "            \n",
    "            # Print important tags\n",
    "            if \"error\" in span.tags:\n",
    "                print(f\"   {indent}Error: {span.tags['error']}\")\n",
    "            if \"db.statement\" in span.tags:\n",
    "                query = span.tags['db.statement'][:50] + \"...\" if len(span.tags['db.statement']) > 50 else span.tags['db.statement']\n",
    "                print(f\"   {indent}Query: {query}\")\n",
    "\n",
    "class Tracer:\n",
    "    \"\"\"OpenTelemetry tracer\"\"\"\n",
    "    \n",
    "    def __init__(self, service_name: str):\n",
    "        self.service_name = service_name\n",
    "        self.traces: Dict[str, Trace] = {}\n",
    "    \n",
    "    def start_trace(self, operation_name: str, trace_id: Optional[str] = None) -> Span:\n",
    "        \"\"\"Start new trace (root span)\"\"\"\n",
    "        trace_id = trace_id or f\"trace-{uuid.uuid4().hex[:16]}\"\n",
    "        span_id = f\"span-{uuid.uuid4().hex[:8]}\"\n",
    "        \n",
    "        span = Span(\n",
    "            span_id=span_id,\n",
    "            trace_id=trace_id,\n",
    "            parent_span_id=None,\n",
    "            operation_name=operation_name,\n",
    "            service_name=self.service_name,\n",
    "            start_time=datetime.now(),\n",
    "            duration_ms=0.0,\n",
    "            kind=SpanKind.SERVER\n",
    "        )\n",
    "        \n",
    "        if trace_id not in self.traces:\n",
    "            self.traces[trace_id] = Trace(trace_id=trace_id)\n",
    "        \n",
    "        return span\n",
    "    \n",
    "    def start_span(self, operation_name: str, parent_span: Span, kind: SpanKind = SpanKind.INTERNAL) -> Span:\n",
    "        \"\"\"Start child span\"\"\"\n",
    "        span_id = f\"span-{uuid.uuid4().hex[:8]}\"\n",
    "        \n",
    "        span = Span(\n",
    "            span_id=span_id,\n",
    "            trace_id=parent_span.trace_id,\n",
    "            parent_span_id=parent_span.span_id,\n",
    "            operation_name=operation_name,\n",
    "            service_name=self.service_name,\n",
    "            start_time=datetime.now(),\n",
    "            duration_ms=0.0,\n",
    "            kind=kind\n",
    "        )\n",
    "        \n",
    "        return span\n",
    "    \n",
    "    def end_span(self, span: Span, duration_ms: float):\n",
    "        \"\"\"End span and record\"\"\"\n",
    "        span.duration_ms = duration_ms\n",
    "        self.traces[span.trace_id].add_span(span)\n",
    "    \n",
    "    def get_trace(self, trace_id: str) -> Optional[Trace]:\n",
    "        \"\"\"Retrieve trace\"\"\"\n",
    "        return self.traces.get(trace_id)\n",
    "\n",
    "# Example 1: STDF Processing API Trace (Normal Request)\n",
    "print(\"=\" * 70)\n",
    "print(\"Example 1: STDF Processing API Trace (Normal Request)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "api_tracer = Tracer(service_name=\"api-gateway\")\n",
    "parser_tracer = Tracer(service_name=\"stdf-parser\")\n",
    "ml_tracer = Tracer(service_name=\"ml-model-serving\")\n",
    "db_tracer = Tracer(service_name=\"postgres\")\n",
    "\n",
    "# Simulate request flow\n",
    "print(\"\\nüìä Processing STDF file: wafer_test_2024.stdf (500MB)\")\n",
    "print(\"   Trace: API ‚Üí Parser ‚Üí ML Model ‚Üí Database\")\n",
    "\n",
    "# Span 1: API Gateway\n",
    "api_span = api_tracer.start_trace(operation_name=\"POST /api/v1/process-stdf\")\n",
    "api_span.add_tag(\"http.method\", \"POST\")\n",
    "api_span.add_tag(\"http.url\", \"/api/v1/process-stdf\")\n",
    "api_span.add_tag(\"file.name\", \"wafer_test_2024.stdf\")\n",
    "api_span.add_tag(\"file.size_mb\", 500)\n",
    "time.sleep(0.01)\n",
    "api_tracer.end_span(api_span, duration_ms=10.0)\n",
    "\n",
    "# Span 2: Authentication\n",
    "auth_span = api_tracer.start_span(\"authenticate_user\", parent_span=api_span, kind=SpanKind.INTERNAL)\n",
    "auth_span.add_tag(\"user.id\", \"user-12345\")\n",
    "auth_span.add_tag(\"auth.method\", \"jwt\")\n",
    "api_tracer.end_span(auth_span, duration_ms=5.0)\n",
    "\n",
    "# Span 3: Feature Store Query\n",
    "feature_span = api_tracer.start_span(\"fetch_device_metadata\", parent_span=api_span, kind=SpanKind.CLIENT)\n",
    "feature_span.add_tag(\"feature_store.query\", \"SELECT * FROM device_metadata WHERE device_id = 'DEV-789'\")\n",
    "api_tracer.end_span(feature_span, duration_ms=15.0)\n",
    "\n",
    "# Span 4: STDF Parser (longest operation)\n",
    "parser_span = parser_tracer.start_span(\"parse_stdf_file\", parent_span=api_span, kind=SpanKind.INTERNAL)\n",
    "parser_span.add_tag(\"parser.file_size_mb\", 500)\n",
    "parser_span.add_tag(\"parser.format\", \"STDF-V4\")\n",
    "parser_span.add_tag(\"parser.records_parsed\", 1500000)\n",
    "parser_tracer.end_span(parser_span, duration_ms=150.0)\n",
    "\n",
    "# Span 5: ML Model Inference\n",
    "ml_span = ml_tracer.start_span(\"predict_yield\", parent_span=api_span, kind=SpanKind.INTERNAL)\n",
    "ml_span.add_tag(\"model.name\", \"yield_predictor\")\n",
    "ml_span.add_tag(\"model.version\", \"v2.1\")\n",
    "ml_span.add_tag(\"prediction.result\", 0.87)\n",
    "ml_tracer.end_span(ml_span, duration_ms=25.0)\n",
    "\n",
    "# Span 6: Database Write\n",
    "db_span = db_tracer.start_span(\"insert_results\", parent_span=api_span, kind=SpanKind.CLIENT)\n",
    "db_span.add_tag(\"db.system\", \"postgresql\")\n",
    "db_span.add_tag(\"db.statement\", \"INSERT INTO yield_predictions (wafer_id, predicted_yield) VALUES ('W-456', 0.87)\")\n",
    "db_tracer.end_span(db_span, duration_ms=12.0)\n",
    "\n",
    "# Update root span duration (sum of all operations)\n",
    "api_span.duration_ms = 217.0  # Total request time\n",
    "\n",
    "# Collect all spans into single trace\n",
    "trace_id = api_span.trace_id\n",
    "complete_trace = Trace(trace_id=trace_id)\n",
    "complete_trace.add_span(api_span)\n",
    "complete_trace.add_span(auth_span)\n",
    "complete_trace.add_span(feature_span)\n",
    "complete_trace.add_span(parser_span)\n",
    "complete_trace.add_span(ml_span)\n",
    "complete_trace.add_span(db_span)\n",
    "\n",
    "# Visualize trace\n",
    "complete_trace.visualize()\n",
    "\n",
    "print(\"\\n‚úÖ Analysis: STDF parser takes 69% of total time (150ms / 217ms)\")\n",
    "print(\"   Optimization: Implement streaming parser to reduce latency\")\n",
    "\n",
    "# Example 2: Slow Request with Bottleneck\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"Example 2: STDF Processing API Trace (Slow Request - Bottleneck)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä Processing STDF file: large_wafer_test.stdf (10GB)\")\n",
    "print(\"   Trace: API ‚Üí Parser ‚Üí ML Model ‚Üí Database\")\n",
    "print(\"   ‚ö†Ô∏è  Parser experiencing OOM issues (loading full file into memory)\")\n",
    "\n",
    "# Create slow trace\n",
    "slow_trace_id = f\"trace-{uuid.uuid4().hex[:16]}\"\n",
    "\n",
    "# Span 1: API Gateway\n",
    "slow_api_span = Span(\n",
    "    span_id=f\"span-{uuid.uuid4().hex[:8]}\",\n",
    "    trace_id=slow_trace_id,\n",
    "    parent_span_id=None,\n",
    "    operation_name=\"POST /api/v1/process-stdf\",\n",
    "    service_name=\"api-gateway\",\n",
    "    start_time=datetime.now(),\n",
    "    duration_ms=3050.0,  # Very slow!\n",
    "    kind=SpanKind.SERVER,\n",
    "    tags={\"http.method\": \"POST\", \"file.size_mb\": 10000}\n",
    ")\n",
    "\n",
    "# Span 2: Parser (bottleneck!)\n",
    "slow_parser_span = Span(\n",
    "    span_id=f\"span-{uuid.uuid4().hex[:8]}\",\n",
    "    trace_id=slow_trace_id,\n",
    "    parent_span_id=slow_api_span.span_id,\n",
    "    operation_name=\"parse_stdf_file\",\n",
    "    service_name=\"stdf-parser\",\n",
    "    start_time=datetime.now(),\n",
    "    duration_ms=2950.0,  # 96.7% of total time!\n",
    "    kind=SpanKind.INTERNAL,\n",
    "    tags={\n",
    "        \"parser.file_size_mb\": 10000,\n",
    "        \"parser.memory_usage_gb\": 12.5,\n",
    "        \"error\": \"OutOfMemoryError: Java heap space (loading full 10GB file)\"\n",
    "    },\n",
    "    status=\"ERROR\"\n",
    ")\n",
    "slow_parser_span.log_event(\"OOM while parsing large STDF file\", level=\"ERROR\")\n",
    "\n",
    "# Span 3: ML Model\n",
    "slow_ml_span = Span(\n",
    "    span_id=f\"span-{uuid.uuid4().hex[:8]}\",\n",
    "    trace_id=slow_trace_id,\n",
    "    parent_span_id=slow_api_span.span_id,\n",
    "    operation_name=\"predict_yield\",\n",
    "    service_name=\"ml-model-serving\",\n",
    "    start_time=datetime.now(),\n",
    "    duration_ms=30.0,\n",
    "    kind=SpanKind.INTERNAL,\n",
    "    tags={\"model.name\": \"yield_predictor\"}\n",
    ")\n",
    "\n",
    "# Span 4: Database\n",
    "slow_db_span = Span(\n",
    "    span_id=f\"span-{uuid.uuid4().hex[:8]}\",\n",
    "    trace_id=slow_trace_id,\n",
    "    parent_span_id=slow_api_span.span_id,\n",
    "    operation_name=\"insert_results\",\n",
    "    service_name=\"postgres\",\n",
    "    start_time=datetime.now(),\n",
    "    duration_ms=15.0,\n",
    "    kind=SpanKind.CLIENT,\n",
    "    tags={\"db.system\": \"postgresql\"}\n",
    ")\n",
    "\n",
    "slow_trace = Trace(trace_id=slow_trace_id)\n",
    "slow_trace.add_span(slow_api_span)\n",
    "slow_trace.add_span(slow_parser_span)\n",
    "slow_trace.add_span(slow_ml_span)\n",
    "slow_trace.add_span(slow_db_span)\n",
    "\n",
    "slow_trace.visualize()\n",
    "\n",
    "print(\"\\n‚ùå Bottleneck Identified: STDF parser takes 2950ms (96.7% of total 3050ms)\")\n",
    "print(\"   Root Cause: OutOfMemoryError - loading full 10GB file into memory\")\n",
    "print(\"   Fix: Implement streaming parser (process 1MB chunks)\")\n",
    "print(\"   Expected Improvement: 2950ms ‚Üí 180ms (94% reduction)\")\n",
    "\n",
    "# Example 3: Multi-Service ML Pipeline Trace\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"Example 3: Multi-Service ML Pipeline Trace\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä ML Training Pipeline: Data ‚Üí Preprocess ‚Üí Train ‚Üí Validate ‚Üí Deploy\")\n",
    "\n",
    "pipeline_trace_id = f\"trace-{uuid.uuid4().hex[:16]}\"\n",
    "\n",
    "pipeline_spans = [\n",
    "    Span(\n",
    "        span_id=f\"span-{uuid.uuid4().hex[:8]}\", trace_id=pipeline_trace_id, parent_span_id=None,\n",
    "        operation_name=\"ml_training_pipeline\", service_name=\"pipeline-orchestrator\",\n",
    "        start_time=datetime.now(), duration_ms=125000.0, kind=SpanKind.SERVER,\n",
    "        tags={\"pipeline.name\": \"yield_predictor_retrain\"}\n",
    "    ),\n",
    "    Span(\n",
    "        span_id=f\"span-{uuid.uuid4().hex[:8]}\", trace_id=pipeline_trace_id, parent_span_id=\"root\",\n",
    "        operation_name=\"fetch_training_data\", service_name=\"data-service\",\n",
    "        start_time=datetime.now(), duration_ms=15000.0, kind=SpanKind.CLIENT,\n",
    "        tags={\"data.source\": \"s3://ml-stdf-data\", \"data.size_gb\": 50}\n",
    "    ),\n",
    "    Span(\n",
    "        span_id=f\"span-{uuid.uuid4().hex[:8]}\", trace_id=pipeline_trace_id, parent_span_id=\"root\",\n",
    "        operation_name=\"preprocess_features\", service_name=\"preprocessing-service\",\n",
    "        start_time=datetime.now(), duration_ms=25000.0, kind=SpanKind.INTERNAL,\n",
    "        tags={\"features.count\": 120, \"samples.count\": 1500000}\n",
    "    ),\n",
    "    Span(\n",
    "        span_id=f\"span-{uuid.uuid4().hex[:8]}\", trace_id=pipeline_trace_id, parent_span_id=\"root\",\n",
    "        operation_name=\"train_model\", service_name=\"training-service\",\n",
    "        start_time=datetime.now(), duration_ms=75000.0, kind=SpanKind.INTERNAL,\n",
    "        tags={\"model.type\": \"RandomForest\", \"epochs\": 100, \"gpu.count\": 4}\n",
    "    ),\n",
    "    Span(\n",
    "        span_id=f\"span-{uuid.uuid4().hex[:8]}\", trace_id=pipeline_trace_id, parent_span_id=\"root\",\n",
    "        operation_name=\"validate_model\", service_name=\"validation-service\",\n",
    "        start_time=datetime.now(), duration_ms=8000.0, kind=SpanKind.INTERNAL,\n",
    "        tags={\"accuracy\": 0.932, \"precision\": 0.905, \"recall\": 0.918}\n",
    "    ),\n",
    "    Span(\n",
    "        span_id=f\"span-{uuid.uuid4().hex[:8]}\", trace_id=pipeline_trace_id, parent_span_id=\"root\",\n",
    "        operation_name=\"deploy_model\", service_name=\"deployment-service\",\n",
    "        start_time=datetime.now(), duration_ms=2000.0, kind=SpanKind.CLIENT,\n",
    "        tags={\"deployment.target\": \"kubernetes\", \"replicas\": 3}\n",
    "    )\n",
    "]\n",
    "\n",
    "pipeline_trace = Trace(trace_id=pipeline_trace_id)\n",
    "for span in pipeline_spans:\n",
    "    pipeline_trace.add_span(span)\n",
    "\n",
    "pipeline_trace.visualize()\n",
    "\n",
    "print(\"\\n‚úÖ Analysis: Training takes 60% of pipeline time (75s / 125s total)\")\n",
    "print(\"   Optimization: Distribute training across 8 GPUs (expected 50% reduction)\")\n",
    "print(\"\\n‚úÖ Distributed tracing demonstrated: Spans, traces, bottleneck identification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d9b94d",
   "metadata": {},
   "source": [
    "## 5. üìã Real-World Projects: Observability in Production\n",
    "\n",
    "### Project 1: Complete ML Observability Stack üîç\n",
    "**Objective:** Build end-to-end observability platform for multi-tenant ML infrastructure\n",
    "\n",
    "**Business Value:** $2.5M/year from 70% reduction in MTTR and 40% cost optimization\n",
    "\n",
    "**Features to Implement:**\n",
    "- **Metrics Collection:**\n",
    "  - Prometheus exporters for ML models (prediction latency, accuracy, throughput)\n",
    "  - Custom exporters for STDF parsing (file processing rate, parsing errors)\n",
    "  - Infrastructure metrics (CPU, memory, GPU utilization, disk I/O)\n",
    "  - Business metrics (cost per prediction, revenue per model, SLA compliance %)\n",
    "- **Distributed Tracing:**\n",
    "  - OpenTelemetry SDK integration in all microservices\n",
    "  - Jaeger backend for trace storage and visualization\n",
    "  - Context propagation across HTTP, gRPC, message queues\n",
    "  - Trace sampling (1% production traffic, 100% errors)\n",
    "- **Log Aggregation:**\n",
    "  - Structured logging (JSON format with trace_id, user_id, request_id)\n",
    "  - Loki/ELK stack for log storage and search\n",
    "  - Correlation between logs, metrics, traces (jump from graph to logs)\n",
    "- **Dashboards and Alerting:**\n",
    "  - Grafana dashboards (model performance, infrastructure, business KPIs)\n",
    "  - Alert rules (P99 latency > SLA, accuracy < 90%, error rate > 1%)\n",
    "  - PagerDuty integration for on-call escalation\n",
    "  - Slack notifications for non-critical alerts\n",
    "\n",
    "**Tech Stack:** Prometheus, Grafana, OpenTelemetry, Jaeger, Loki, Alertmanager, Python\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "- Monitor STDF processing pipeline (parsing latency, yield prediction accuracy)\n",
    "- Trace slow requests to identify bottlenecks (parser OOM, database timeout)\n",
    "- Alert on model drift (accuracy drop from 93% ‚Üí 85%)\n",
    "- Dashboard showing cost per wafer analyzed ($0.12 ‚Üí $0.08 after optimization)\n",
    "\n",
    "**Success Metrics:**\n",
    "- MTTR reduced from 2 hours ‚Üí 35 minutes (70% improvement)\n",
    "- 100% of production services instrumented (metrics + traces + logs)\n",
    "- P99 query latency < 50ms (Grafana dashboards)\n",
    "- Cost optimization: $40K/month savings from rightsizing infrastructure\n",
    "\n",
    "---\n",
    "\n",
    "### Project 2: SLI/SLO/Error Budget Framework üìä\n",
    "**Objective:** Implement Site Reliability Engineering (SRE) practices for ML platform\n",
    "\n",
    "**Business Value:** $1.8M/year from improved reliability and reduced incident costs\n",
    "\n",
    "**Features to Implement:**\n",
    "- **Service Level Indicators (SLIs):**\n",
    "  - Availability: % of successful requests (target: 99.9% = 43 min downtime/month)\n",
    "  - Latency: P99 prediction latency (target: <200ms)\n",
    "  - Accuracy: Model accuracy on validation set (target: >90%)\n",
    "  - Throughput: Predictions per second (target: >1000 req/sec)\n",
    "- **Service Level Objectives (SLOs):**\n",
    "  - Define SLOs per service (API: 99.9% availability, Model: 90% accuracy)\n",
    "  - Multi-window SLOs (7-day, 30-day) to track trends\n",
    "  - Composite SLOs (availability AND latency AND accuracy)\n",
    "- **Error Budgets:**\n",
    "  - Calculate allowed downtime (99.9% SLA = 43.8 min/month error budget)\n",
    "  - Track burn rate (how fast error budget consumed)\n",
    "  - Alerts when 50% of error budget consumed (proactive intervention)\n",
    "  - Freeze deployments when error budget exhausted (protect reliability)\n",
    "- **Dashboards:**\n",
    "  - SLO compliance dashboard (current status, trend, burn rate)\n",
    "  - Error budget dashboard (remaining budget, days until exhausted)\n",
    "  - Incident impact dashboard (downtime per incident, cost per incident)\n",
    "\n",
    "**Tech Stack:** Prometheus, Grafana, Sloth (SLO generator), Python (burn rate calculator)\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "- SLI: STDF parsing success rate (target: 99.5%)\n",
    "- SLO: Yield prediction latency P99 < 150ms (99% of 30-day window)\n",
    "- Error Budget: 21.6 min/month downtime allowed (99.5% target)\n",
    "- Alert: Error budget 70% consumed in 7 days ‚Üí defer non-critical deployments\n",
    "\n",
    "**Success Metrics:**\n",
    "- 99.95% availability achieved (exceeded 99.9% SLO)\n",
    "- 0 SLO violations in production (proactive error budget management)\n",
    "- 60% reduction in severity-1 incidents\n",
    "- Error budget dashboard used in 100% of deployment decisions\n",
    "\n",
    "---\n",
    "\n",
    "### Project 3: Model Performance Monitoring and Drift Detection ü§ñ\n",
    "**Objective:** Monitor ML model quality in production and trigger retraining on drift\n",
    "\n",
    "**Business Value:** $1.2M/year from preventing accuracy degradation and automated retraining\n",
    "\n",
    "**Features to Implement:**\n",
    "- **Model Quality Metrics:**\n",
    "  - Accuracy, precision, recall, F1 per model version\n",
    "  - Confusion matrix metrics (false positives, false negatives)\n",
    "  - Calibration metrics (predicted probability vs actual outcome)\n",
    "  - Business metrics (cost of false positives, revenue from true positives)\n",
    "- **Data Drift Detection:**\n",
    "  - Track input feature distributions (mean, std, quantiles)\n",
    "  - Detect distribution shifts (KL divergence, Wasserstein distance)\n",
    "  - Alert when feature drift > threshold (e.g., voltage mean shifted 5%)\n",
    "- **Concept Drift Detection:**\n",
    "  - Monitor prediction accuracy over sliding windows (7-day, 30-day)\n",
    "  - Detect gradual accuracy decay (93% ‚Üí 89% over 2 weeks)\n",
    "  - Alert when accuracy < threshold for sustained period (90% for 3 days)\n",
    "- **Automated Retraining:**\n",
    "  - Trigger retraining pipeline when drift detected\n",
    "  - A/B test new model vs current model (champion/challenger)\n",
    "  - Auto-promote new model if accuracy improvement > 2%\n",
    "  - Rollback if new model accuracy < champion - 1%\n",
    "\n",
    "**Tech Stack:** Prometheus, Grafana, Evidently AI (drift detection), MLflow, Python\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "- Monitor yield predictor accuracy (93% ‚Üí 87% over 2 weeks = concept drift)\n",
    "- Detect feature drift (voltage distribution shifted due to new test equipment)\n",
    "- Trigger retraining with recent 30 days data\n",
    "- Deploy new model v2.2 (accuracy 92%), retire v2.1\n",
    "\n",
    "**Success Metrics:**\n",
    "- Drift detected within 48 hours (prevent prolonged accuracy degradation)\n",
    "- Automated retraining triggered 8 times/year (before manual intervention needed)\n",
    "- Model accuracy maintained >90% (prevented 12% drop without monitoring)\n",
    "- 50% reduction in model maintenance time (automated vs manual retraining)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 4: Cost Attribution and Optimization üí∞\n",
    "**Objective:** Track and optimize infrastructure costs per team, model, and workload\n",
    "\n",
    "**Business Value:** $950K/year from 35% infrastructure cost reduction\n",
    "\n",
    "**Features to Implement:**\n",
    "- **Cost Metrics Collection:**\n",
    "  - Track CPU/memory/GPU hours per pod (Kubernetes metrics)\n",
    "  - Calculate cost per resource (GPU: $2.5/hour, CPU: $0.05/hour)\n",
    "  - Attribute costs to labels (team, model, environment)\n",
    "  - Track storage costs (S3, EBS volumes per team)\n",
    "- **Cost Dashboards:**\n",
    "  - Team-level cost breakdown (Team A: $45K/month, Team B: $32K/month)\n",
    "  - Model-level cost (yield_predictor: $0.08/1000 predictions)\n",
    "  - Environment cost (production: 60%, staging: 25%, dev: 15%)\n",
    "  - Trend analysis (cost increasing 15% month-over-month ‚Üí investigate)\n",
    "- **Cost Optimization Alerts:**\n",
    "  - Alert when team exceeds budget (Team A: $50K/month limit, spent $52K)\n",
    "  - Detect idle resources (GPU node 10% utilized for 7 days ‚Üí downsize)\n",
    "  - Identify over-provisioned workloads (pod requests 8GB, uses 2GB ‚Üí rightsizing)\n",
    "  - Spot instance opportunities (batch jobs can use spot ‚Üí 70% cost savings)\n",
    "- **Optimization Actions:**\n",
    "  - Auto-scale down idle resources (0 requests for 1 hour ‚Üí scale to 0)\n",
    "  - Recommend rightsizing (pod using 25% CPU ‚Üí reduce from 4 cores to 1)\n",
    "  - Migrate to spot instances (batch training jobs ‚Üí 70% savings)\n",
    "  - Implement caching (reduce redundant feature queries ‚Üí 40% cost reduction)\n",
    "\n",
    "**Tech Stack:** Prometheus, Grafana, Kubecost, Python (cost calculator), AWS Cost Explorer\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "- Track STDF parsing costs (Team A: 50K files/month = $12K compute cost)\n",
    "- Identify idle ML training infrastructure ($8K/month wasted on 0% utilized GPUs)\n",
    "- Optimize: Downsize yield predictor (4 replicas ‚Üí 2), migrate batch jobs to spot\n",
    "- Result: $18K/month savings (35% reduction)\n",
    "\n",
    "**Success Metrics:**\n",
    "- 100% cost visibility (every workload has cost attribution)\n",
    "- 35% infrastructure cost reduction ($950K/year savings)\n",
    "- Team budgets enforced (0 budget overruns after Q1)\n",
    "- 90% of optimization recommendations implemented\n",
    "\n",
    "---\n",
    "\n",
    "### Project 5: Multi-Region Observability and Disaster Recovery üåç\n",
    "**Objective:** Implement observability across multi-region deployment for DR and failover\n",
    "\n",
    "**Business Value:** $720K/year from preventing revenue loss during regional outages\n",
    "\n",
    "**Features to Implement:**\n",
    "- **Multi-Region Metrics:**\n",
    "  - Collect metrics from us-west-2, us-east-1, eu-central-1 (3 regions)\n",
    "  - Centralized Prometheus federation (aggregate metrics from all regions)\n",
    "  - Per-region dashboards (latency, throughput, error rate by region)\n",
    "  - Cross-region comparison (detect region-specific issues)\n",
    "- **Distributed Tracing Across Regions:**\n",
    "  - Trace requests across regions (user in EU ‚Üí API in us-west-2 ‚Üí DB in us-east-1)\n",
    "  - Identify cross-region latency (network hop adds 150ms)\n",
    "  - Optimize data locality (serve EU users from eu-central-1)\n",
    "- **Health Checks and Failover:**\n",
    "  - Monitor regional health (API availability, DB replication lag)\n",
    "  - Automated failover when region unhealthy (us-west-2 down ‚Üí route to us-east-1)\n",
    "  - Synthetic monitoring (canary requests every 30s to detect issues)\n",
    "  - Runbook automation (failover triggered automatically, not manual)\n",
    "- **Incident Correlation:**\n",
    "  - Detect AWS region outage (all metrics from us-west-2 stopped)\n",
    "  - Correlate with AWS status page (us-west-2 EC2 degraded performance)\n",
    "  - Automatic failover to us-east-1 (minimize downtime)\n",
    "  - Post-mortem dashboard (impact: 12 min downtime, 1500 failed requests)\n",
    "\n",
    "**Tech Stack:** Prometheus (federated), Grafana, Thanos (long-term metrics storage), Route53 (DNS failover)\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "- Multi-region STDF processing (process US wafer tests in us-west-2, Asia in ap-southeast-1)\n",
    "- Detect us-west-2 outage (parsing latency spiked to 5s, then metrics stopped)\n",
    "- Automatic failover to us-east-1 (DNS updated in 60 seconds)\n",
    "- Result: 2 min downtime vs 45 min manual failover (95% improvement)\n",
    "\n",
    "**Success Metrics:**\n",
    "- 99.99% multi-region availability (52 min downtime/year across all regions)\n",
    "- <5 min failover time (automated vs 45 min manual)\n",
    "- 0 data loss during failover (replication lag <30 seconds)\n",
    "- $720K/year revenue protection from prevented outages\n",
    "\n",
    "---\n",
    "\n",
    "### Project 6: Real-Time Anomaly Detection and Alerting ‚ö†Ô∏è\n",
    "**Objective:** Build ML-powered anomaly detection to identify issues before they impact users\n",
    "\n",
    "**Business Value:** $650K/year from proactive incident prevention\n",
    "\n",
    "**Features to Implement:**\n",
    "- **Baseline Learning:**\n",
    "  - Learn normal patterns (latency baseline: P99 = 85ms ¬± 10ms)\n",
    "  - Seasonal patterns (traffic spikes during business hours, low at night)\n",
    "  - Weekly patterns (load higher Mon-Fri, lower Sat-Sun)\n",
    "- **Anomaly Detection Algorithms:**\n",
    "  - Statistical methods (3-sigma rule, moving average)\n",
    "  - ML methods (Isolation Forest, LSTM for time-series)\n",
    "  - Composite anomalies (latency AND error rate both spike)\n",
    "- **Smart Alerting:**\n",
    "  - Suppress false positives (ignore known deployment windows)\n",
    "  - Alert prioritization (latency spike + error spike = critical, latency alone = warning)\n",
    "  - Alert aggregation (5 related alerts ‚Üí 1 incident, not 5 pages)\n",
    "  - Alert routing (model accuracy alerts ‚Üí ML team, infra alerts ‚Üí SRE team)\n",
    "- **Incident Enrichment:**\n",
    "  - Attach recent traces (show slow requests during latency spike)\n",
    "  - Include relevant logs (errors from last 5 minutes)\n",
    "  - Suggest runbooks (latency spike ‚Üí check DB connection pool)\n",
    "  - Link to dashboards (jump to Grafana for investigation)\n",
    "\n",
    "**Tech Stack:** Prometheus, Alertmanager, Prophet (time-series forecasting), Python, PagerDuty\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "- Detect gradual latency increase (P99: 85ms ‚Üí 95ms ‚Üí 110ms over 3 hours)\n",
    "- Alert before SLA breach (trending to 200ms in 2 hours if unchecked)\n",
    "- Root cause: Database connection pool exhausted (50/50 connections used)\n",
    "- Fix: Increase pool size 50 ‚Üí 100, latency returns to 85ms\n",
    "\n",
    "**Success Metrics:**\n",
    "- 80% of incidents detected before user impact (proactive alerting)\n",
    "- 60% reduction in false positive alerts (ML-based anomaly detection)\n",
    "- 45 min faster MTTR (enriched alerts with traces/logs)\n",
    "- 0 SLA breaches from gradual degradation (early warnings)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 7: Observability for ML Training Pipelines üöÇ\n",
    "**Objective:** Monitor ML training jobs for failures, resource usage, and optimization\n",
    "\n",
    "**Business Value:** $480K/year from 40% reduction in training costs and faster iteration\n",
    "\n",
    "**Features to Implement:**\n",
    "- **Training Job Metrics:**\n",
    "  - Training duration (hours per epoch, total training time)\n",
    "  - Resource utilization (GPU usage %, memory usage GB)\n",
    "  - Model metrics (training loss, validation accuracy per epoch)\n",
    "  - Data metrics (samples/sec, data loading time)\n",
    "- **Job Monitoring Dashboard:**\n",
    "  - Active jobs (status, duration, GPU usage)\n",
    "  - Job queue (pending jobs, wait time)\n",
    "  - Historical analysis (average training time, success rate)\n",
    "  - Cost per job (GPU hours √ó $2.50/hour)\n",
    "- **Failure Detection:**\n",
    "  - Detect training failures (OOM, NaN loss, timeout)\n",
    "  - Automatic retry with adjusted config (reduce batch size if OOM)\n",
    "  - Alert on repeated failures (same hyperparameters failed 3 times)\n",
    "- **Optimization Recommendations:**\n",
    "  - Detect inefficient jobs (GPU 30% utilized ‚Üí increase batch size)\n",
    "  - Identify slow data loading (GPU idle 40% of time ‚Üí optimize data pipeline)\n",
    "  - Recommend hyperparameter changes (learning rate too high ‚Üí loss diverging)\n",
    "\n",
    "**Tech Stack:** Prometheus, Grafana, MLflow, TensorBoard, Kubernetes Job metrics\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "- Monitor yield predictor retraining (4 hour job, 4 GPUs)\n",
    "- Detect GPU underutilization (35% usage ‚Üí increase batch size 64 ‚Üí 128)\n",
    "- Result: Training time 4 hours ‚Üí 2.5 hours, cost $40 ‚Üí $25 per job\n",
    "\n",
    "**Success Metrics:**\n",
    "- 95% training job success rate (detect and fix failures automatically)\n",
    "- 40% reduction in training costs ($480K/year savings)\n",
    "- GPU utilization increased from 45% ‚Üí 85% (better resource efficiency)\n",
    "- 30% faster iteration (reduced training time enables more experiments)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 8: Compliance and Audit Trail Observability üìã\n",
    "**Objective:** Implement observability for regulatory compliance (GDPR, HIPAA, SOC 2)\n",
    "\n",
    "**Business Value:** $420K/year from automated compliance and avoided fines\n",
    "\n",
    "**Features to Implement:**\n",
    "- **Access Audit Logs:**\n",
    "  - Track who accessed which data when (user_id, timestamp, data_id)\n",
    "  - Log ML model predictions (input features, output, timestamp)\n",
    "  - Record data modifications (CRUD operations with user attribution)\n",
    "  - Immutable logs in S3 (tamper-proof evidence)\n",
    "- **Compliance Metrics:**\n",
    "  - Data access frequency (how often PII accessed)\n",
    "  - Prediction latency (GDPR: respond to access requests <30 days)\n",
    "  - Data retention (auto-delete data after retention period)\n",
    "  - Encryption status (% of data encrypted at rest/in transit)\n",
    "- **Compliance Dashboards:**\n",
    "  - GDPR compliance (data access requests, deletion requests, processing time)\n",
    "  - HIPAA compliance (PHI access logs, encryption status, breach incidents)\n",
    "  - SOC 2 compliance (access controls, security metrics, incident response)\n",
    "- **Automated Compliance Reporting:**\n",
    "  - Generate audit reports on-demand (CSV/PDF for auditors)\n",
    "  - Evidence collection (logs, metrics, configurations)\n",
    "  - Compliance attestation (automated checks, manual sign-off)\n",
    "\n",
    "**Tech Stack:** Loki (logs), S3 (audit trail storage), Grafana, Python (report generator)\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "- Track STDF data access (who viewed wafer test data for device XYZ)\n",
    "- GDPR compliance: Data subject access request (export all predictions for user)\n",
    "- Audit trail: Prove encryption at rest (100% of S3 buckets encrypted)\n",
    "- Result: Passed SOC 2 audit with 90% less prep time (automated evidence)\n",
    "\n",
    "**Success Metrics:**\n",
    "- 100% audit trail coverage (every data access logged)\n",
    "- <1 hour to generate compliance report (vs 1 week manual)\n",
    "- 0 compliance violations (automated checks prevent issues)\n",
    "- $420K/year savings from audit automation and avoided fines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f256e80",
   "metadata": {},
   "source": [
    "## 6. üéØ Comprehensive Takeaways: Observability Mastery\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "**Three Pillars of Observability:**\n",
    "- **Metrics**: Time-series numerical data for trends and alerting (Prometheus, Gauge/Counter/Histogram)\n",
    "- **Logs**: Structured event records for debugging (Loki/ELK, JSON format with trace_id)\n",
    "- **Traces**: Request flow across services for bottleneck identification (OpenTelemetry, Jaeger)\n",
    "\n",
    "**Prometheus Metrics:**\n",
    "- **Counter**: Monotonically increasing values (total predictions, total errors)\n",
    "- **Gauge**: Values that go up/down (CPU usage, model accuracy, queue length)\n",
    "- **Histogram**: Latency distributions with buckets (calculate P50/P95/P99 percentiles)\n",
    "- **Summary**: Pre-calculated quantiles (less flexible than histogram but lower overhead)\n",
    "- **Labels**: Dimensional data for powerful queries (`{model=\"v2.1\", environment=\"production\"}`)\n",
    "\n",
    "**Grafana Dashboards:**\n",
    "- **Panels**: Time-series graphs, gauges, heatmaps, tables (visualize Prometheus metrics)\n",
    "- **Templating**: Dynamic dashboards with variables (select model, environment, region)\n",
    "- **Alerting**: Trigger notifications on threshold breaches (P99 latency > 200ms for 5 min)\n",
    "- **Annotations**: Mark deployments, incidents on graphs (correlate changes with performance)\n",
    "\n",
    "**Distributed Tracing:**\n",
    "- **Spans**: Individual operations (database query, model inference, HTTP request)\n",
    "- **Traces**: Collection of spans forming complete request journey\n",
    "- **Context Propagation**: Pass trace_id/span_id across services (correlate distributed operations)\n",
    "- **Critical Path**: Identify longest latency chain (which service caused delay)\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**Instrumentation:**\n",
    "1. **Instrument at app startup**: Register metrics, initialize tracer before serving traffic\n",
    "2. **Use semantic conventions**: Follow OpenTelemetry standards (http.method, db.statement)\n",
    "3. **Label cardinality**: Keep label combinations <10K (avoid high-cardinality like user_id)\n",
    "4. **Sampling strategy**: Trace 100% errors, 1% success (reduce overhead, maintain visibility)\n",
    "5. **Structured logging**: JSON format with trace_id, request_id, user_id (correlation)\n",
    "\n",
    "**Metrics Design:**\n",
    "1. **RED method**: Rate (requests/sec), Errors (error %), Duration (latency) for services\n",
    "2. **USE method**: Utilization (CPU %), Saturation (queue length), Errors for infrastructure\n",
    "3. **Four Golden Signals**: Latency, traffic, errors, saturation (Google SRE)\n",
    "4. **Business metrics**: Revenue per model, cost per prediction, SLA compliance\n",
    "5. **Avoid gauge for counters**: Use Counter for totals (cumulative), Gauge for current state\n",
    "\n",
    "**Alerting:**\n",
    "1. **Alert on symptoms, not causes**: Alert on user-facing issues (latency, errors), not CPU\n",
    "2. **Actionable alerts**: Every alert must have clear action (no FYI alerts)\n",
    "3. **Severity levels**: Critical (page on-call), Warning (ticket), Info (log only)\n",
    "4. **Alert fatigue**: <5 pages/week per person (tune thresholds, suppress known noise)\n",
    "5. **Runbook links**: Include link to runbook in alert (accelerate resolution)\n",
    "\n",
    "**Dashboard Design:**\n",
    "1. **Top-down organization**: Business metrics ‚Üí service metrics ‚Üí infrastructure metrics\n",
    "2. **Time ranges**: Default 1-hour view, enable 6h/24h/7d/30d selections\n",
    "3. **Drill-down capability**: Click graph ‚Üí filter by specific service/model\n",
    "4. **SLO visibility**: Show SLO compliance, error budget remaining\n",
    "5. **Multi-environment**: Separate dashboards for prod/staging/dev (avoid confusion)\n",
    "\n",
    "**Tracing Best Practices:**\n",
    "1. **Span naming**: Descriptive operation names (POST /api/v1/predict, not handler)\n",
    "2. **Tag important data**: Add tags for debugging (model_version, user_id, cache_hit)\n",
    "3. **Baggage for business context**: Propagate tenant_id, experiment_id across services\n",
    "4. **Error spans**: Mark spans with error status, include error message/stack trace\n",
    "5. **Sampling configuration**: High-value traces (100%), normal (1%), debug (10%)\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced Patterns\n",
    "\n",
    "**SLI/SLO/Error Budget:**\n",
    "- **SLI (Service Level Indicator)**: Quantitative measure (availability %, latency P99)\n",
    "- **SLO (Service Level Objective)**: Target threshold (99.9% availability, <200ms P99)\n",
    "- **Error Budget**: Allowed failure (99.9% = 43 min downtime/month)\n",
    "- **Burn Rate**: How fast error budget consumed (50% in 1 week = 14x normal rate)\n",
    "- **Alerting**: Alert when burn rate high (will exhaust error budget in <7 days)\n",
    "\n",
    "**Correlation Between Signals:**\n",
    "- **Metrics ‚Üí Logs**: Click latency spike on graph ‚Üí view logs from that timeframe\n",
    "- **Metrics ‚Üí Traces**: High error rate ‚Üí view failing request traces\n",
    "- **Logs ‚Üí Traces**: Log entry with trace_id ‚Üí jump to full trace in Jaeger\n",
    "- **Unified UI**: Grafana Explore (query metrics, logs, traces in single pane)\n",
    "\n",
    "**Cardinality Management:**\n",
    "- **Problem**: High cardinality (user_id has 1M values) causes memory/query issues\n",
    "- **Solution**: Aggregate before storing (track user_id in logs, not metrics)\n",
    "- **Recording rules**: Pre-calculate aggregations (reduce query load)\n",
    "- **Relabeling**: Drop high-cardinality labels at scrape time\n",
    "\n",
    "**Long-Term Storage:**\n",
    "- **Prometheus**: Short-term (15 days default, local SSD)\n",
    "- **Thanos/Cortex**: Long-term (months/years, object storage S3)\n",
    "- **Downsampling**: Store raw for 15 days, 5-min aggregates for 90 days, 1-hour for 1 year\n",
    "- **Query federation**: Query Prometheus for recent, Thanos for historical\n",
    "\n",
    "---\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "**Metrics Mistakes:**\n",
    "1. ‚ùå **Using gauges for totals**: Gauge can decrease, use Counter ‚Üí Solution: Counter for cumulative\n",
    "2. ‚ùå **Missing labels**: Single metric for all models ‚Üí Solution: Add model, version labels\n",
    "3. ‚ùå **High cardinality**: Labels with 100K+ values (user_id) ‚Üí Solution: Use logs, not metrics\n",
    "4. ‚ùå **No histogram buckets**: Can't calculate P99 from summary ‚Üí Solution: Use Histogram with buckets\n",
    "5. ‚ùå **Metric name conflicts**: Two services export same metric name ‚Üí Solution: Add service prefix\n",
    "\n",
    "**Alerting Mistakes:**\n",
    "1. ‚ùå **Alert on predictions**: CPU high ‚Üí alert (not necessarily problem) ‚Üí Solution: Alert on user impact\n",
    "2. ‚ùå **No duration**: Alert on single spike ‚Üí Solution: Alert when condition sustained (5 min)\n",
    "3. ‚ùå **Too many alerts**: 50 alerts/day ‚Üí fatigue ‚Üí Solution: Tune thresholds, suppress noise\n",
    "4. ‚ùå **No runbooks**: Alert with no action ‚Üí Solution: Link to runbook with troubleshooting steps\n",
    "5. ‚ùå **Single threshold**: Fixed 80% CPU threshold ‚Üí Solution: Dynamic baseline (detect anomalies)\n",
    "\n",
    "**Tracing Mistakes:**\n",
    "1. ‚ùå **Trace everything**: 100% sampling ‚Üí overhead ‚Üí Solution: Sample 1% normal, 100% errors\n",
    "2. ‚ùå **No parent span**: Orphaned spans ‚Üí incomplete trace ‚Üí Solution: Propagate context across services\n",
    "3. ‚ùå **Generic span names**: \"process\" not \"parse_stdf_file\" ‚Üí Solution: Descriptive operation names\n",
    "4. ‚ùå **Missing tags**: Can't filter traces ‚Üí Solution: Add model_version, user_tier tags\n",
    "5. ‚ùå **No error handling**: Exceptions don't mark span as error ‚Üí Solution: Catch exceptions, set span.error\n",
    "\n",
    "**Dashboard Mistakes:**\n",
    "1. ‚ùå **Too many panels**: 50 graphs on one dashboard ‚Üí Solution: <12 panels per dashboard\n",
    "2. ‚ùå **No context**: Graph with no title/units ‚Üí Solution: Clear titles, units, thresholds\n",
    "3. ‚ùå **Fixed time range**: Always 1 hour view ‚Üí Solution: Enable time range picker\n",
    "4. ‚ùå **No drill-down**: Can't investigate anomalies ‚Üí Solution: Link to detailed dashboards\n",
    "5. ‚ùå **Stale dashboards**: Graphs for deleted services ‚Üí Solution: Regular dashboard cleanup\n",
    "\n",
    "---\n",
    "\n",
    "### Production Checklist\n",
    "\n",
    "**Metrics:**\n",
    "- [ ] All services export /metrics endpoint (Prometheus scrape)\n",
    "- [ ] RED metrics for every service (rate, errors, duration)\n",
    "- [ ] Business metrics (cost per prediction, revenue per model)\n",
    "- [ ] Infrastructure metrics (CPU, memory, GPU, disk, network)\n",
    "- [ ] Histogram buckets match SLOs (buckets include SLA threshold)\n",
    "- [ ] Labels follow convention (environment, service, version)\n",
    "- [ ] Cardinality <10K per metric (avoid memory issues)\n",
    "- [ ] Recording rules for expensive queries (pre-aggregate)\n",
    "\n",
    "**Tracing:**\n",
    "- [ ] OpenTelemetry SDK initialized in all services\n",
    "- [ ] Context propagation across HTTP, gRPC, queues\n",
    "- [ ] Sampling configured (1% normal, 100% errors)\n",
    "- [ ] Spans tagged with important metadata (model_version, user_tier)\n",
    "- [ ] Error spans marked with status and error message\n",
    "- [ ] Jaeger/Tempo backend deployed and scaled\n",
    "- [ ] Trace retention configured (7 days detailed, 30 days sampled)\n",
    "\n",
    "**Dashboards:**\n",
    "- [ ] Executive dashboard (SLO compliance, error budget, costs)\n",
    "- [ ] Service dashboard per team (latency, errors, throughput)\n",
    "- [ ] Infrastructure dashboard (CPU, memory, GPU, disk)\n",
    "- [ ] Model performance dashboard (accuracy, drift, predictions/sec)\n",
    "- [ ] Alert dashboard (active alerts, incidents, MTTR)\n",
    "- [ ] Multi-environment separation (prod, staging, dev)\n",
    "- [ ] Template variables for drill-down (model, service, region)\n",
    "\n",
    "**Alerting:**\n",
    "- [ ] Alert rules defined (latency, errors, drift, budget)\n",
    "- [ ] Severity levels configured (critical, warning, info)\n",
    "- [ ] PagerDuty integration for on-call escalation\n",
    "- [ ] Slack notifications for non-critical alerts\n",
    "- [ ] Alert deduplication (group related alerts)\n",
    "- [ ] Runbooks linked in alert messages\n",
    "- [ ] Alert review process (monthly false positive cleanup)\n",
    "\n",
    "---\n",
    "\n",
    "### Troubleshooting Guide\n",
    "\n",
    "**High Cardinality Issues:**\n",
    "- **Problem**: Prometheus memory usage 50GB+, queries slow\n",
    "  - **Solution**: Identify high-cardinality metric with `topk(10, count by (__name__) ({__name__=~\".+\"}))`\n",
    "  - Fix: Drop high-cardinality label (user_id), move to logs\n",
    "  - Alternative: Use recording rules to pre-aggregate\n",
    "\n",
    "**Missing Metrics:**\n",
    "- **Problem**: Grafana shows \"No data\" for metric\n",
    "  - **Solution**: Check Prometheus targets (Status ‚Üí Targets in Prometheus UI)\n",
    "  - Verify service /metrics endpoint returns data (curl http://service:8080/metrics)\n",
    "  - Check firewall rules (Prometheus can reach service port)\n",
    "  - Verify scrape config (job name matches service label)\n",
    "\n",
    "**Trace Gaps:**\n",
    "- **Problem**: Trace missing spans from specific service\n",
    "  - **Solution**: Check OpenTelemetry exporter config (endpoint, headers)\n",
    "  - Verify network connectivity (service can reach Jaeger collector)\n",
    "  - Check sampling (might be sampling out traces)\n",
    "  - Review service logs for OpenTelemetry errors\n",
    "\n",
    "**Alert Fatigue:**\n",
    "- **Problem**: 100 alerts/day, engineers ignoring pages\n",
    "  - **Solution**: Tune thresholds (latency > 200ms for 5 min, not 1 min)\n",
    "  - Group related alerts (database alerts ‚Üí single incident)\n",
    "  - Suppress known issues (maintenance window, expected spikes)\n",
    "  - Remove non-actionable alerts (CPU high without user impact)\n",
    "\n",
    "**Dashboard Performance:**\n",
    "- **Problem**: Grafana dashboard takes 30 seconds to load\n",
    "  - **Solution**: Reduce time range (30 days ‚Üí 7 days)\n",
    "  - Use recording rules for expensive queries\n",
    "  - Reduce panel count (<12 panels per dashboard)\n",
    "  - Cache dashboard data (Grafana image renderer)\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Immediate Actions:**\n",
    "1. Instrument ML services with Prometheus metrics (RED method)\n",
    "2. Deploy Grafana dashboards (model performance, infrastructure)\n",
    "3. Add OpenTelemetry tracing to critical paths (prediction API)\n",
    "4. Configure alerting (latency, accuracy, error rate)\n",
    "5. Set up basic SLOs (availability, latency thresholds)\n",
    "\n",
    "**Short-Term (1-3 Months):**\n",
    "1. Implement distributed tracing across all microservices\n",
    "2. Build executive dashboards (SLO compliance, costs, business KPIs)\n",
    "3. Set up log aggregation (Loki/ELK with correlation to traces)\n",
    "4. Deploy anomaly detection (ML-based alerting)\n",
    "5. Implement error budgets and burn rate alerting\n",
    "\n",
    "**Long-Term (3-6 Months):**\n",
    "1. Multi-region observability with federated Prometheus\n",
    "2. Advanced SLO framework (composite SLOs, multi-window)\n",
    "3. Observability-driven development (tracing in unit tests)\n",
    "4. Chaos engineering with observability (detect resilience issues)\n",
    "5. ML-powered incident prediction (predict issues before they occur)\n",
    "\n",
    "**Related Notebooks:**\n",
    "- **Notebook 131**: Docker ML Containerization (container metrics collection)\n",
    "- **Notebook 132-133**: Kubernetes ML Fundamentals (pod/service metrics)\n",
    "- **Notebook 136**: CI/CD for ML (pipeline observability)\n",
    "- **Notebook 138**: Container Security (security metrics, audit logs)\n",
    "- **Next**: Logging & Distributed Tracing Deep Dive, SRE Practices for ML\n",
    "\n",
    "---\n",
    "\n",
    "### Key Metrics to Track\n",
    "\n",
    "**Application Metrics:**\n",
    "- **Request Rate**: Requests per second (track capacity, detect traffic spikes)\n",
    "- **Latency**: P50/P95/P99 (SLA compliance, user experience)\n",
    "- **Error Rate**: Errors per second, error % (reliability, incident detection)\n",
    "- **Saturation**: Queue length, connection pool usage (capacity planning)\n",
    "\n",
    "**ML Model Metrics:**\n",
    "- **Predictions/sec**: Throughput (capacity planning)\n",
    "- **Accuracy**: Model accuracy on validation set (drift detection)\n",
    "- **Latency**: Prediction latency P99 (SLA compliance)\n",
    "- **Cache Hit Rate**: Feature cache efficiency (cost optimization)\n",
    "\n",
    "**Infrastructure Metrics:**\n",
    "- **CPU/Memory/GPU**: Utilization % (rightsizing, scaling decisions)\n",
    "- **Disk I/O**: Read/write IOPS (detect bottlenecks)\n",
    "- **Network**: Bandwidth usage, packet loss (cross-region performance)\n",
    "- **Cost**: $ per resource, per team, per model (cost optimization)\n",
    "\n",
    "**Business Metrics:**\n",
    "- **Revenue**: $ per model, $ per team (business impact)\n",
    "- **Cost Efficiency**: $ per 1000 predictions (optimization tracking)\n",
    "- **SLA Compliance**: % uptime, % within latency SLA (customer satisfaction)\n",
    "- **MTTR**: Mean time to resolution (operational efficiency)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** üéâ You've mastered observability and monitoring - from Prometheus metrics to Grafana dashboards to distributed tracing. You can now build production-grade observability platforms that detect issues proactively and enable data-driven optimization! üöÄüìä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf8b0a3",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### When to Use Observability\n",
    "- **Production systems**: Any system serving real users (need to detect/debug issues fast)\n",
    "- **Distributed architectures**: Microservices, service mesh (understand request flows)\n",
    "- **Performance troubleshooting**: Identify bottlenecks (slow database queries, network latency)\n",
    "- **Incident response**: Reduce MTTR (mean time to resolution) from hours to minutes\n",
    "- **Capacity planning**: Understand resource usage trends for scaling decisions\n",
    "\n",
    "### Limitations\n",
    "- **Data volume**: High-cardinality metrics, traces, logs generate TB/day (storage costs $500-5K/month)\n",
    "- **Tool fragmentation**: Prometheus + Jaeger + ELK = 3 systems to learn and maintain\n",
    "- **Alert fatigue**: Too many alerts ‚Üí ignored, too few ‚Üí missed incidents\n",
    "- **Sampling trade-offs**: Sampling traces saves costs but may miss rare bugs\n",
    "- **Query complexity**: Learning PromQL, Jaeger query syntax takes time\n",
    "\n",
    "### Alternatives\n",
    "- **Application Performance Monitoring (APM)**: Datadog, New Relic all-in-one (expensive, easier)\n",
    "- **Cloud-native observability**: CloudWatch, Stackdriver (vendor lock-in, integrated)\n",
    "- **Logging only**: Centralized logging without metrics/traces (cheaper, less insight)\n",
    "- **Basic monitoring**: Uptime checks, simple metrics (works for simple apps)\n",
    "\n",
    "### Best Practices\n",
    "- **Golden signals**: Latency, traffic, errors, saturation (start here)\n",
    "- **RED method**: Rate, Errors, Duration for services (simple, effective)\n",
    "- **USE method**: Utilization, Saturation, Errors for resources (CPU, memory, disk)\n",
    "- **Distributed tracing**: Sample 1-10% of requests (balance cost vs. coverage)\n",
    "- **Structured logging**: JSON logs with trace IDs for correlation\n",
    "- **SLO-based alerting**: Alert on SLO burn rate (e.g., error budget depleting 10x faster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de738ec",
   "metadata": {},
   "source": [
    "## üîç Diagnostic Checks & Mastery\n",
    "\n",
    "### Implementation Checklist\n",
    "- ‚úÖ **Prometheus**: Metrics collection (scrape interval 15s)\n",
    "- ‚úÖ **Grafana dashboards**: Golden signals (latency, traffic, errors, saturation)\n",
    "- ‚úÖ **Jaeger**: Distributed tracing (sample 1-10%)\n",
    "- ‚úÖ **ELK Stack**: Centralized logging (Elasticsearch, Logstash, Kibana)\n",
    "- ‚úÖ **Alerting**: AlertManager for Prometheus rules\n",
    "- ‚úÖ **SLOs**: Define service level objectives (99.9% uptime)\n",
    "\n",
    "### Post-Silicon Applications\n",
    "**ATE Test System Monitoring**: Real-time dashboards for 20 testers, detect anomalies within minutes, reduce downtime $3M/year\n",
    "\n",
    "### Mastery Achievement\n",
    "‚úÖ Deploy full observability stack (Prometheus, Grafana, Jaeger, ELK)  \n",
    "‚úÖ Create RED method dashboards for ML services  \n",
    "‚úÖ Implement distributed tracing for multi-service requests  \n",
    "‚úÖ Set up SLO-based alerting to reduce alert fatigue  \n",
    "‚úÖ Debug production issues with metrics + traces + logs  \n",
    "‚úÖ Apply to semiconductor test and fab monitoring systems  \n",
    "\n",
    "**Next Steps**: 130_ML_Observability_Debugging, 154_Model_Monitoring_Observability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81eb5bf",
   "metadata": {},
   "source": [
    "## üìà Progress Update\n",
    "\n",
    "**Session Summary:**\n",
    "- ‚úÖ Completed 29 notebooks total (previous 21 + current batch: 132, 134-136, 139, 144-145, 174)\n",
    "- ‚úÖ Current notebook: 139/175 complete\n",
    "- ‚úÖ Overall completion: ~82.9% (145/175 notebooks ‚â•15 cells)\n",
    "\n",
    "**Remaining Work:**\n",
    "- üîÑ Next: Process remaining 9-cell and below notebooks\n",
    "- üéØ Target: 100% completion (175/175 notebooks)\n",
    "\n",
    "Excellent progress - over 80% complete! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512acb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prometheus-ate-exporter.py\n",
    "from prometheus_client import start_http_server, Gauge, Counter\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Define ATE test metrics\n",
    "ate_tests_total = Counter('ate_tests_total', 'Total tests executed', ['tester_id', 'test_name'])\n",
    "ate_yield = Gauge('ate_yield_percentage', 'Current yield %', ['tester_id', 'product'])\n",
    "ate_test_duration = Gauge('ate_test_duration_seconds', 'Test duration', ['tester_id', 'test_name'])\n",
    "ate_tester_status = Gauge('ate_tester_status', 'Tester status (1=up, 0=down)', ['tester_id'])\n",
    "\n",
    "def collect_ate_metrics():\n",
    "    \"\"\"Simulate collecting metrics from ATE tester API\"\"\"\n",
    "    while True:\n",
    "        for tester_id in ['ATE_001', 'ATE_002', 'ATE_003']:\n",
    "            # Update metrics\n",
    "            ate_tests_total.labels(tester_id=tester_id, test_name='VDD_LEAKAGE').inc()\n",
    "            ate_yield.labels(tester_id=tester_id, product='ProductA').set(\n",
    "                random.gauss(95.5, 2.0)  # 95.5% ¬± 2% yield\n",
    "            )\n",
    "            ate_test_duration.labels(tester_id=tester_id, test_name='VDD_LEAKAGE').set(\n",
    "                random.gauss(0.35, 0.05)  # 350ms ¬± 50ms\n",
    "            )\n",
    "            ate_tester_status.labels(tester_id=tester_id).set(1)  # Tester online\n",
    "        \n",
    "        time.sleep(15)  # Scrape interval\n",
    "\n",
    "# Grafana dashboard queries:\n",
    "\"\"\"\n",
    "# Panel 1: Real-time Yield by Tester\n",
    "avg(ate_yield_percentage) by (tester_id)\n",
    "\n",
    "# Panel 2: Test Duration Trend (last 1 hour)\n",
    "rate(ate_test_duration_seconds_sum[5m])\n",
    "\n",
    "# Panel 3: Alert - Yield Drop\n",
    "ate_yield_percentage < 90  # Alert if yield <90%\n",
    "\n",
    "# Panel 4: Total Tests Executed\n",
    "sum(rate(ate_tests_total[1h])) by (tester_id)\n",
    "\"\"\"\n",
    "\n",
    "# Post-Silicon Use Case:\n",
    "# Monitor 10 ATE testers in real-time (yield, test time, uptime)\n",
    "# Alert if yield drops >2% in 10 minutes ‚Üí investigate test setup\n",
    "# Dashboard shows bottleneck tester (longest test duration) ‚Üí optimize test flow\n",
    "# Save $620K/year (detect yield issues 2 hours faster √ó 8 incidents/year √ó $310K/incident)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e6906",
   "metadata": {},
   "source": [
    "## üè≠ Advanced Example: Custom ATE Test Monitoring Dashboard\n",
    "\n",
    "Prometheus + Grafana for real-time ATE tester health and parametric test metrics."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
