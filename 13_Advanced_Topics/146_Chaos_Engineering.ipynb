{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 146: Chaos Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8058802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import time\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple, Callable\n",
    "from enum import Enum\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "print(\"\u2705 Chaos Engineering environment ready!\")\n",
    "print(\"\ud83d\udce6 Modules: Fault Injection, Resilience Testing, Circuit Breakers, Game Days\")\n",
    "print(\"\ud83d\udca5 Ready to break things safely and build confidence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5eefb",
   "metadata": {},
   "source": [
    "## 2. \ud83d\udca5 Pod Termination Chaos - Testing Auto-Restart and Load Balancing\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Section?\n",
    "\n",
    "**Purpose:** Test system resilience by randomly terminating pods/instances and measuring recovery time\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Pod termination**: Kubernetes kills pods (crashes, OOM, node failures, spot interruptions) - system must auto-restart\n",
    "- **Load balancing**: Traffic should route to healthy pods only (exclude terminating/unhealthy pods from service)\n",
    "- **Health checks**: Liveness probes detect crashed pods (restart), readiness probes detect slow pods (remove from load balancer)\n",
    "- **Graceful shutdown**: Pods get 30-second SIGTERM to finish requests before SIGKILL (prevents request failures)\n",
    "- **Auto-scaling**: HPA should scale up if pod terminations reduce capacity below target\n",
    "\n",
    "**Pod Failure Scenarios:**\n",
    "- **Random pod kill**: Simulate unexpected crashes (OOM, segfault, kernel panic)\n",
    "- **Node failure**: Entire EC2 instance fails, all pods on that node terminated\n",
    "- **Spot interruption**: AWS reclaims spot instance with 2-minute warning\n",
    "- **Resource exhaustion**: Pod consumes 100% CPU/memory, becomes unresponsive (liveness probe fails)\n",
    "\n",
    "**Kubernetes Resilience Mechanisms:**\n",
    "- **ReplicaSet**: Ensures N pods always running (if 1 dies, start new pod immediately)\n",
    "- **Service + Endpoints**: Load balancer only routes to \"Ready\" pods (excludes terminating/unhealthy)\n",
    "- **PodDisruptionBudget**: Limits voluntary disruptions (max 1 pod down during rolling update)\n",
    "- **Anti-affinity**: Spread pods across nodes/AZs (avoid single point of failure)\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Prevent cascading failures**: Pod terminations shouldn't cause API outages (load balancer routes to healthy pods)\n",
    "- **Validate auto-recovery**: System should self-heal without human intervention (Kubernetes restarts crashed pods)\n",
    "- **Measure MTTR**: Quantify recovery time (pod restart ~30 seconds, new node provision ~2 minutes)\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "- **STDF ETL pipeline**: Kill Spark worker pods during batch processing, validate job retries and checkpoint recovery\n",
    "- **ML inference API**: Terminate prediction service pods under load, ensure zero request failures (load balancer excludes dead pods)\n",
    "- **Database replicas**: Kill PostgreSQL read replica, validate failover to other replicas (read queries continue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pod Termination Chaos Implementation\n",
    "\n",
    "class PodState(Enum):\n",
    "    \"\"\"Pod lifecycle states\"\"\"\n",
    "    PENDING = \"pending\"\n",
    "    RUNNING = \"running\"\n",
    "    TERMINATING = \"terminating\"\n",
    "    FAILED = \"failed\"\n",
    "    SUCCEEDED = \"succeeded\"\n",
    "\n",
    "@dataclass\n",
    "class Pod:\n",
    "    \"\"\"Kubernetes pod simulation\"\"\"\n",
    "    pod_id: str\n",
    "    node_id: str\n",
    "    state: PodState = PodState.PENDING\n",
    "    is_ready: bool = False\n",
    "    start_time: Optional[datetime] = None\n",
    "    requests_handled: int = 0\n",
    "    cpu_usage: float = 0.0\n",
    "    memory_usage: float = 0.0\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Start pod (simulate container startup)\"\"\"\n",
    "        self.state = PodState.RUNNING\n",
    "        self.start_time = datetime.now()\n",
    "        # Simulate startup time (10-30 seconds)\n",
    "        time.sleep(0.01)  # Simulated delay\n",
    "        self.is_ready = True\n",
    "    \n",
    "    def terminate(self):\n",
    "        \"\"\"Terminate pod (graceful shutdown)\"\"\"\n",
    "        self.state = PodState.TERMINATING\n",
    "        self.is_ready = False\n",
    "        # Simulate graceful shutdown (complete in-flight requests)\n",
    "        time.sleep(0.005)\n",
    "        self.state = PodState.SUCCEEDED\n",
    "    \n",
    "    def handle_request(self) -> bool:\n",
    "        \"\"\"Handle incoming request (returns True if successful)\"\"\"\n",
    "        if self.state == PodState.RUNNING and self.is_ready:\n",
    "            self.requests_handled += 1\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "@dataclass\n",
    "class LoadBalancer:\n",
    "    \"\"\"Service load balancer (routes to healthy pods only)\"\"\"\n",
    "    service_name: str\n",
    "    pods: List[Pod] = field(default_factory=list)\n",
    "    total_requests: int = 0\n",
    "    failed_requests: int = 0\n",
    "    \n",
    "    def add_pod(self, pod: Pod):\n",
    "        \"\"\"Add pod to service endpoints\"\"\"\n",
    "        self.pods.append(pod)\n",
    "    \n",
    "    def remove_pod(self, pod: Pod):\n",
    "        \"\"\"Remove pod from service endpoints\"\"\"\n",
    "        if pod in self.pods:\n",
    "            self.pods.remove(pod)\n",
    "    \n",
    "    def get_healthy_pods(self) -> List[Pod]:\n",
    "        \"\"\"Get pods that are ready to serve traffic\"\"\"\n",
    "        return [p for p in self.pods if p.state == PodState.RUNNING and p.is_ready]\n",
    "    \n",
    "    def route_request(self) -> bool:\n",
    "        \"\"\"Route request to healthy pod (round-robin)\"\"\"\n",
    "        self.total_requests += 1\n",
    "        healthy_pods = self.get_healthy_pods()\n",
    "        \n",
    "        if not healthy_pods:\n",
    "            self.failed_requests += 1\n",
    "            return False\n",
    "        \n",
    "        # Round-robin load balancing\n",
    "        pod = healthy_pods[self.total_requests % len(healthy_pods)]\n",
    "        return pod.handle_request()\n",
    "    \n",
    "    def get_error_rate(self) -> float:\n",
    "        \"\"\"Calculate request error rate\"\"\"\n",
    "        if self.total_requests == 0:\n",
    "            return 0.0\n",
    "        return (self.failed_requests / self.total_requests) * 100\n",
    "\n",
    "@dataclass\n",
    "class ReplicaSet:\n",
    "    \"\"\"Kubernetes ReplicaSet (maintains desired pod count)\"\"\"\n",
    "    name: str\n",
    "    desired_replicas: int\n",
    "    pods: List[Pod] = field(default_factory=list)\n",
    "    node_count: int = 3\n",
    "    \n",
    "    def reconcile(self, load_balancer: LoadBalancer):\n",
    "        \"\"\"Reconcile actual vs desired state (create/delete pods)\"\"\"\n",
    "        current_running = len([p for p in self.pods if p.state in [PodState.RUNNING, PodState.PENDING]])\n",
    "        \n",
    "        # Create pods if below desired count\n",
    "        if current_running < self.desired_replicas:\n",
    "            for i in range(self.desired_replicas - current_running):\n",
    "                node_id = f\"node-{random.randint(1, self.node_count)}\"\n",
    "                pod = Pod(pod_id=f\"{self.name}-{random.randint(1000, 9999)}\", node_id=node_id)\n",
    "                pod.start()\n",
    "                self.pods.append(pod)\n",
    "                load_balancer.add_pod(pod)\n",
    "        \n",
    "        # Remove terminated pods\n",
    "        terminated_pods = [p for p in self.pods if p.state in [PodState.FAILED, PodState.SUCCEEDED]]\n",
    "        for pod in terminated_pods:\n",
    "            self.pods.remove(pod)\n",
    "            load_balancer.remove_pod(pod)\n",
    "\n",
    "class ChaosPodKiller:\n",
    "    \"\"\"Chaos experiment: random pod termination\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def kill_random_pod(replica_set: ReplicaSet, load_balancer: LoadBalancer) -> Optional[Pod]:\n",
    "        \"\"\"Kill random running pod\"\"\"\n",
    "        running_pods = [p for p in replica_set.pods if p.state == PodState.RUNNING]\n",
    "        if not running_pods:\n",
    "            return None\n",
    "        \n",
    "        pod = random.choice(running_pods)\n",
    "        print(f\"  \ud83d\udca5 Killing pod: {pod.pod_id} on {pod.node_id}\")\n",
    "        load_balancer.remove_pod(pod)\n",
    "        pod.terminate()\n",
    "        return pod\n",
    "    \n",
    "    @staticmethod\n",
    "    def kill_node(replica_set: ReplicaSet, load_balancer: LoadBalancer, node_id: str) -> int:\n",
    "        \"\"\"Kill all pods on a node (simulate node failure)\"\"\"\n",
    "        pods_on_node = [p for p in replica_set.pods if p.node_id == node_id and p.state == PodState.RUNNING]\n",
    "        print(f\"  \ud83d\udca5 Node failure: {node_id} ({len(pods_on_node)} pods terminating)\")\n",
    "        \n",
    "        for pod in pods_on_node:\n",
    "            load_balancer.remove_pod(pod)\n",
    "            pod.terminate()\n",
    "        \n",
    "        return len(pods_on_node)\n",
    "\n",
    "# Example 1: Pod termination with auto-restart\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CHAOS EXPERIMENT 1: Random Pod Termination\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\ud83d\udccb Hypothesis: System maintains 99% availability when 1 pod killed every 30 seconds\\n\")\n",
    "\n",
    "# Setup: 5 replicas serving traffic\n",
    "replica_set = ReplicaSet(name=\"ml-inference\", desired_replicas=5, node_count=3)\n",
    "load_balancer = LoadBalancer(service_name=\"ml-inference-svc\")\n",
    "\n",
    "# Initial reconciliation (create 5 pods)\n",
    "replica_set.reconcile(load_balancer)\n",
    "\n",
    "print(f\"\u2705 Initial state: {len(replica_set.pods)} pods running\\n\")\n",
    "\n",
    "# Simulate traffic + chaos for 10 time periods\n",
    "print(f\"{'Time':>6} {'Pods':>5} {'Healthy':>8} {'RPS':>6} {'Errors':>7} {'Error %':>8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for t in range(1, 11):\n",
    "    # Simulate 1000 requests per period\n",
    "    for _ in range(1000):\n",
    "        load_balancer.route_request()\n",
    "    \n",
    "    # Kill random pod every 3rd period\n",
    "    if t % 3 == 0:\n",
    "        ChaosPodKiller.kill_random_pod(replica_set, load_balancer)\n",
    "        # ReplicaSet immediately starts new pod\n",
    "        replica_set.reconcile(load_balancer)\n",
    "    \n",
    "    healthy = len(load_balancer.get_healthy_pods())\n",
    "    error_rate = load_balancer.get_error_rate()\n",
    "    \n",
    "    print(f\"{t:>6} {len(replica_set.pods):>5} {healthy:>8} {1000:>6} {load_balancer.failed_requests:>7} {error_rate:>7.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_requests = load_balancer.total_requests\n",
    "failed_requests = load_balancer.failed_requests\n",
    "availability = ((total_requests - failed_requests) / total_requests * 100) if total_requests > 0 else 0\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Experiment Summary:\")\n",
    "print(f\"   Total Requests: {total_requests:,}\")\n",
    "print(f\"   Failed Requests: {failed_requests:,}\")\n",
    "print(f\"   Availability: {availability:.3f}%\")\n",
    "print(f\"   Hypothesis: {'\u2705 PASSED' if availability >= 99.0 else '\u274c FAILED'} (target: 99%)\")\n",
    "\n",
    "print(f\"\\n\ud83d\udca1 Observations:\")\n",
    "print(f\"   \u2022 ReplicaSet auto-restarted terminated pods immediately\")\n",
    "print(f\"   \u2022 Load balancer excluded terminating pods from routing\")\n",
    "print(f\"   \u2022 Zero failed requests during pod termination (graceful)\")\n",
    "print(f\"   \u2022 MTTR: <1 second (pod restart faster than request timeout)\")\n",
    "\n",
    "# Example 2: Node failure simulation\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CHAOS EXPERIMENT 2: Node Failure (All Pods on Node Terminated)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\ud83d\udccb Hypothesis: System recovers from node failure with <5 seconds downtime\\n\")\n",
    "\n",
    "# Reset for new experiment\n",
    "replica_set2 = ReplicaSet(name=\"stdf-processor\", desired_replicas=6, node_count=3)\n",
    "load_balancer2 = LoadBalancer(service_name=\"stdf-processor-svc\")\n",
    "replica_set2.reconcile(load_balancer2)\n",
    "\n",
    "print(f\"\u2705 Initial state: {len(replica_set2.pods)} pods across {replica_set2.node_count} nodes\")\n",
    "\n",
    "# Show pod distribution\n",
    "node_distribution = defaultdict(int)\n",
    "for pod in replica_set2.pods:\n",
    "    node_distribution[pod.node_id] += 1\n",
    "\n",
    "print(f\"\\n\ud83d\udccd Pod distribution:\")\n",
    "for node_id, count in sorted(node_distribution.items()):\n",
    "    print(f\"   {node_id}: {count} pods\")\n",
    "\n",
    "# Simulate node failure\n",
    "failed_node = \"node-2\"\n",
    "print(f\"\\n\ud83d\udca5 Simulating failure of {failed_node}...\")\n",
    "\n",
    "# Send traffic before failure\n",
    "for _ in range(500):\n",
    "    load_balancer2.route_request()\n",
    "\n",
    "baseline_healthy = len(load_balancer2.get_healthy_pods())\n",
    "\n",
    "# Kill node\n",
    "terminated_count = ChaosPodKiller.kill_node(replica_set2, load_balancer2, failed_node)\n",
    "\n",
    "# Send traffic during failure (some requests may fail)\n",
    "for _ in range(500):\n",
    "    load_balancer2.route_request()\n",
    "\n",
    "# ReplicaSet reconciles (creates new pods on other nodes)\n",
    "print(f\"\\n\ud83d\udd04 ReplicaSet reconciling...\")\n",
    "replica_set2.reconcile(load_balancer2)\n",
    "\n",
    "# Send traffic after recovery\n",
    "for _ in range(500):\n",
    "    load_balancer2.route_request()\n",
    "\n",
    "recovered_healthy = len(load_balancer2.get_healthy_pods())\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Node Failure Impact:\")\n",
    "print(f\"   Pods before failure: {baseline_healthy}\")\n",
    "print(f\"   Pods terminated: {terminated_count}\")\n",
    "print(f\"   Pods after recovery: {recovered_healthy}\")\n",
    "print(f\"   Total requests: {load_balancer2.total_requests:,}\")\n",
    "print(f\"   Failed requests: {load_balancer2.failed_requests:,}\")\n",
    "print(f\"   Availability: {((load_balancer2.total_requests - load_balancer2.failed_requests) / load_balancer2.total_requests * 100):.3f}%\")\n",
    "\n",
    "print(f\"\\n\u2705 Recovery validated:\")\n",
    "print(f\"   \u2022 Kubernetes restarted {terminated_count} pods on healthy nodes\")\n",
    "print(f\"   \u2022 Service maintained partial availability during recovery\")\n",
    "print(f\"   \u2022 Full capacity restored after reconciliation\")\n",
    "\n",
    "print(\"\\n\ud83d\udca5 Chaos experiment complete!\")\n",
    "print(f\"\u2705 Pod termination resilience validated\")\n",
    "print(f\"\u2705 Auto-scaling and load balancing working correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874be0b7",
   "metadata": {},
   "source": [
    "## 3. \ud83d\udc0c Network Latency Injection - Testing Circuit Breakers and Timeouts\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Section?\n",
    "\n",
    "**Purpose:** Inject network latency to validate timeout configurations, circuit breakers, and graceful degradation\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Network latency**: Delay in network communication (database queries, API calls, service-to-service) - normal 5-50ms, degraded 200-1000ms\n",
    "- **Timeouts**: Max time to wait for response before giving up (prevent hanging forever on slow dependencies)\n",
    "- **Circuit breakers**: Auto-disable slow/failing dependencies (fail fast vs cascading failures)\n",
    "- **Fallback strategies**: Serve cached data, default values, or degraded functionality when primary dependency fails\n",
    "- **Retry logic**: Retry failed requests with exponential backoff (don't overwhelm already-struggling dependency)\n",
    "\n",
    "**Latency Injection Scenarios:**\n",
    "- **Database latency**: Add 500ms to all database queries (test if application times out gracefully)\n",
    "- **API latency**: External API takes 5 seconds to respond (circuit breaker should open, serve from cache)\n",
    "- **Inter-service latency**: Service A \u2192 Service B call delayed 2 seconds (validate timeout propagation)\n",
    "- **Packet loss**: 10% of packets dropped (TCP retransmits, exponential backoff, may appear as latency)\n",
    "\n",
    "**Circuit Breaker Pattern:**\n",
    "1. **Closed** (normal): All requests pass through, track failures\n",
    "2. **Open** (failing): Requests fail immediately without calling dependency (give it time to recover)\n",
    "3. **Half-Open** (testing): Allow 1 request through to test if dependency recovered\n",
    "4. **Thresholds**: Open after 10 failures in 10 seconds, stay open for 30 seconds, close after 3 consecutive successes\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Prevent cascading failures**: Slow database shouldn't crash entire API (circuit breaker fails fast)\n",
    "- **Improve UX**: Return cached data in 50ms vs waiting 5 seconds for timeout\n",
    "- **Resource protection**: Don't waste connections/threads waiting on slow dependencies\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "- **STDF database queries**: Inject 500ms latency to PostgreSQL, validate circuit breaker opens and serves from Redis cache\n",
    "- **ML model inference**: Backend ML service slow (2s latency), API circuit breaker opens, returns previous prediction\n",
    "- **External test equipment API**: ATE API down, circuit breaker prevents test jobs from hanging (skip flaky equipment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea4edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Latency Injection and Circuit Breaker Implementation\n",
    "\n",
    "class CircuitState(Enum):\n",
    "    \"\"\"Circuit breaker states\"\"\"\n",
    "    CLOSED = \"closed\"  # Normal operation\n",
    "    OPEN = \"open\"      # Failing, reject requests\n",
    "    HALF_OPEN = \"half_open\"  # Testing recovery\n",
    "\n",
    "@dataclass\n",
    "class CircuitBreaker:\n",
    "    \"\"\"Circuit breaker pattern implementation\"\"\"\n",
    "    name: str\n",
    "    failure_threshold: int = 5  # Open after N failures\n",
    "    success_threshold: int = 2  # Close after N successes in half-open\n",
    "    timeout_seconds: float = 30  # Stay open for N seconds\n",
    "    \n",
    "    state: CircuitState = CircuitState.CLOSED\n",
    "    failure_count: int = 0\n",
    "    success_count: int = 0\n",
    "    last_failure_time: Optional[datetime] = None\n",
    "    total_requests: int = 0\n",
    "    rejected_requests: int = 0\n",
    "    \n",
    "    def call(self, func: Callable, *args, **kwargs) -> Tuple[bool, Optional[any]]:\n",
    "        \"\"\"Execute function with circuit breaker protection\"\"\"\n",
    "        self.total_requests += 1\n",
    "        \n",
    "        # Check if circuit should transition to half-open\n",
    "        if self.state == CircuitState.OPEN:\n",
    "            if self.last_failure_time and (datetime.now() - self.last_failure_time).seconds >= self.timeout_seconds:\n",
    "                print(f\"    \ud83d\udd04 Circuit breaker {self.name}: OPEN \u2192 HALF_OPEN (testing recovery)\")\n",
    "                self.state = CircuitState.HALF_OPEN\n",
    "                self.success_count = 0\n",
    "            else:\n",
    "                # Still open, reject request\n",
    "                self.rejected_requests += 1\n",
    "                return False, None\n",
    "        \n",
    "        # Try to execute function\n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            \n",
    "            # Success\n",
    "            if self.state == CircuitState.HALF_OPEN:\n",
    "                self.success_count += 1\n",
    "                if self.success_count >= self.success_threshold:\n",
    "                    print(f\"    \u2705 Circuit breaker {self.name}: HALF_OPEN \u2192 CLOSED (recovered)\")\n",
    "                    self.state = CircuitState.CLOSED\n",
    "                    self.failure_count = 0\n",
    "            elif self.state == CircuitState.CLOSED:\n",
    "                self.failure_count = max(0, self.failure_count - 1)  # Decay failures\n",
    "            \n",
    "            return True, result\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Failure\n",
    "            self.failure_count += 1\n",
    "            self.last_failure_time = datetime.now()\n",
    "            \n",
    "            if self.state == CircuitState.CLOSED and self.failure_count >= self.failure_threshold:\n",
    "                print(f\"    \u26a0\ufe0f Circuit breaker {self.name}: CLOSED \u2192 OPEN ({self.failure_count} failures)\")\n",
    "                self.state = CircuitState.OPEN\n",
    "            elif self.state == CircuitState.HALF_OPEN:\n",
    "                print(f\"    \u274c Circuit breaker {self.name}: HALF_OPEN \u2192 OPEN (test failed)\")\n",
    "                self.state = CircuitState.OPEN\n",
    "                self.success_count = 0\n",
    "            \n",
    "            return False, None\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get circuit breaker statistics\"\"\"\n",
    "        return {\n",
    "            \"state\": self.state.value,\n",
    "            \"total_requests\": self.total_requests,\n",
    "            \"rejected_requests\": self.rejected_requests,\n",
    "            \"rejection_rate\": (self.rejected_requests / self.total_requests * 100) if self.total_requests > 0 else 0,\n",
    "            \"failure_count\": self.failure_count\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class DependencyService:\n",
    "    \"\"\"Simulate external dependency with configurable latency\"\"\"\n",
    "    name: str\n",
    "    normal_latency_ms: float = 10.0\n",
    "    injected_latency_ms: float = 0.0\n",
    "    error_rate: float = 0.0\n",
    "    timeout_ms: float = 1000.0\n",
    "    \n",
    "    def call(self) -> str:\n",
    "        \"\"\"Make service call (may fail or timeout)\"\"\"\n",
    "        # Simulate latency\n",
    "        total_latency = self.normal_latency_ms + self.injected_latency_ms\n",
    "        time.sleep(total_latency / 1000.0 / 100)  # Scaled down for demo\n",
    "        \n",
    "        # Simulate timeout\n",
    "        if total_latency > self.timeout_ms:\n",
    "            raise TimeoutError(f\"{self.name} timeout after {total_latency}ms\")\n",
    "        \n",
    "        # Simulate random errors\n",
    "        if random.random() < self.error_rate:\n",
    "            raise Exception(f\"{self.name} returned 500 error\")\n",
    "        \n",
    "        return f\"{self.name} response (latency: {total_latency}ms)\"\n",
    "\n",
    "@dataclass\n",
    "class APIWithFallback:\n",
    "    \"\"\"API with circuit breaker and cache fallback\"\"\"\n",
    "    name: str\n",
    "    dependency: DependencyService\n",
    "    circuit_breaker: CircuitBreaker\n",
    "    cache: Dict[str, str] = field(default_factory=dict)\n",
    "    cache_hits: int = 0\n",
    "    cache_misses: int = 0\n",
    "    \n",
    "    def get_data(self, key: str) -> Tuple[str, str]:\n",
    "        \"\"\"Fetch data with circuit breaker and cache fallback\"\"\"\n",
    "        # Try primary dependency through circuit breaker\n",
    "        success, result = self.circuit_breaker.call(self.dependency.call)\n",
    "        \n",
    "        if success:\n",
    "            # Cache successful response\n",
    "            self.cache[key] = result\n",
    "            self.cache_misses += 1\n",
    "            return result, \"primary\"\n",
    "        else:\n",
    "            # Fallback to cache\n",
    "            if key in self.cache:\n",
    "                self.cache_hits += 1\n",
    "                return self.cache[key], \"cache\"\n",
    "            else:\n",
    "                self.cache_misses += 1\n",
    "                return \"Service unavailable (no cache)\", \"error\"\n",
    "\n",
    "# Example 1: Latency injection with circuit breaker\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CHAOS EXPERIMENT 3: Network Latency Injection\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\ud83d\udccb Hypothesis: Circuit breaker opens when latency >1s, API serves from cache\\n\")\n",
    "\n",
    "# Setup: API calling database service\n",
    "database = DependencyService(\n",
    "    name=\"PostgreSQL\",\n",
    "    normal_latency_ms=10,\n",
    "    timeout_ms=1000\n",
    ")\n",
    "\n",
    "circuit_breaker = CircuitBreaker(\n",
    "    name=\"database-circuit\",\n",
    "    failure_threshold=5,\n",
    "    success_threshold=2,\n",
    "    timeout_seconds=3  # Short timeout for demo\n",
    ")\n",
    "\n",
    "api = APIWithFallback(\n",
    "    name=\"STDF Query API\",\n",
    "    dependency=database,\n",
    "    circuit_breaker=circuit_breaker\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Baseline: Database latency {database.normal_latency_ms}ms, timeout {database.timeout_ms}ms\\n\")\n",
    "\n",
    "# Phase 1: Normal operation (populate cache)\n",
    "print(\"\ud83d\udcca Phase 1: Normal operation (10ms latency)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(10):\n",
    "    result, source = api.get_data(f\"wafer_{i % 3}\")\n",
    "    print(f\"  Request {i+1}: {source:>8} - {circuit_breaker.state.value:>10}\")\n",
    "\n",
    "# Phase 2: Inject latency (trigger circuit breaker)\n",
    "print(\"\\n\ud83d\udca5 Phase 2: Inject 1500ms latency (exceeds 1000ms timeout)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "database.injected_latency_ms = 1500\n",
    "\n",
    "for i in range(15):\n",
    "    result, source = api.get_data(f\"wafer_{i % 3}\")\n",
    "    print(f\"  Request {i+1}: {source:>8} - {circuit_breaker.state.value:>10}\")\n",
    "\n",
    "# Phase 3: Remove latency (circuit recovery)\n",
    "print(\"\\n\ud83d\udd04 Phase 3: Remove latency (circuit should recover)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "database.injected_latency_ms = 0\n",
    "\n",
    "# Wait for circuit to transition to half-open\n",
    "time.sleep(3.1)\n",
    "\n",
    "for i in range(10):\n",
    "    result, source = api.get_data(f\"wafer_{i % 3}\")\n",
    "    print(f\"  Request {i+1}: {source:>8} - {circuit_breaker.state.value:>10}\")\n",
    "\n",
    "# Results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cb_stats = circuit_breaker.get_stats()\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Circuit Breaker Statistics:\")\n",
    "print(f\"   Total Requests: {cb_stats['total_requests']}\")\n",
    "print(f\"   Rejected Requests: {cb_stats['rejected_requests']} ({cb_stats['rejection_rate']:.1f}%)\")\n",
    "print(f\"   Final State: {cb_stats['state'].upper()}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcbe Cache Statistics:\")\n",
    "print(f\"   Cache Hits: {api.cache_hits}\")\n",
    "print(f\"   Cache Misses: {api.cache_misses}\")\n",
    "print(f\"   Hit Rate: {(api.cache_hits / (api.cache_hits + api.cache_misses) * 100):.1f}%\")\n",
    "\n",
    "print(f\"\\n\ud83d\udca1 Observations:\")\n",
    "print(f\"   \u2022 Circuit breaker opened after {circuit_breaker.failure_threshold} timeout failures\")\n",
    "print(f\"   \u2022 API served from cache while circuit open (zero user-facing failures)\")\n",
    "print(f\"   \u2022 Circuit automatically recovered when latency normalized\")\n",
    "print(f\"   \u2022 MTTR: {circuit_breaker.timeout_seconds} seconds (circuit timeout period)\")\n",
    "\n",
    "# Example 2: Multi-dependency chaos (cascading failures)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CHAOS EXPERIMENT 4: Cascading Failure Prevention\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\ud83d\udccb Hypothesis: Circuit breakers prevent cascading failures across dependencies\\n\")\n",
    "\n",
    "# Setup multiple dependencies\n",
    "db_service = DependencyService(\"Database\", normal_latency_ms=10, timeout_ms=500)\n",
    "ml_service = DependencyService(\"ML Model\", normal_latency_ms=50, timeout_ms=1000)\n",
    "cache_service = DependencyService(\"Redis Cache\", normal_latency_ms=5, timeout_ms=200)\n",
    "\n",
    "db_circuit = CircuitBreaker(\"db-circuit\", failure_threshold=3, timeout_seconds=2)\n",
    "ml_circuit = CircuitBreaker(\"ml-circuit\", failure_threshold=3, timeout_seconds=2)\n",
    "cache_circuit = CircuitBreaker(\"cache-circuit\", failure_threshold=3, timeout_seconds=2)\n",
    "\n",
    "print(f\"\u2705 Setup: 3 dependencies (Database, ML Model, Redis Cache)\\n\")\n",
    "\n",
    "# Inject failures in database (should not affect ML/cache)\n",
    "print(\"\ud83d\udca5 Inject failures in Database service\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "db_service.error_rate = 0.8  # 80% error rate\n",
    "\n",
    "for i in range(10):\n",
    "    db_success, _ = db_circuit.call(db_service.call)\n",
    "    ml_success, _ = ml_circuit.call(ml_service.call)\n",
    "    cache_success, _ = cache_circuit.call(cache_service.call)\n",
    "    \n",
    "    print(f\"  Req {i+1}: DB={db_circuit.state.value:>10}, ML={ml_circuit.state.value:>10}, Cache={cache_circuit.state.value:>10}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Results:\")\n",
    "print(f\"   Database Circuit: {db_circuit.state.value} ({db_circuit.rejected_requests} rejected)\")\n",
    "print(f\"   ML Circuit: {ml_circuit.state.value} ({ml_circuit.rejected_requests} rejected)\")\n",
    "print(f\"   Cache Circuit: {cache_circuit.state.value} ({cache_circuit.rejected_requests} rejected)\")\n",
    "\n",
    "print(f\"\\n\u2705 Cascading failure prevented:\")\n",
    "print(f\"   \u2022 Database circuit opened (failing dependency isolated)\")\n",
    "print(f\"   \u2022 ML and Cache circuits remained closed (unaffected)\")\n",
    "print(f\"   \u2022 System maintained partial functionality\")\n",
    "\n",
    "print(\"\\n\ud83d\udca5 Chaos experiments complete!\")\n",
    "print(f\"\u2705 Circuit breakers validated\")\n",
    "print(f\"\u2705 Cache fallback working correctly\")\n",
    "print(f\"\u2705 Cascading failures prevented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48090af6",
   "metadata": {},
   "source": [
    "## 4. \ud83d\udca3 Resource Exhaustion Chaos - Testing Resource Limits and Auto-Scaling\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Test system behavior when CPU/memory resources are exhausted to validate auto-scaling and graceful degradation.\n",
    "\n",
    "**Key Points:**\n",
    "- **CPU exhaustion**: Simulate CPU-intensive workloads (cryptographic operations, matrix multiplications) that saturate cores\n",
    "- **Memory exhaustion**: Simulate memory leaks or data-intensive operations that consume RAM\n",
    "- **Resource limits**: Kubernetes limits (CPU: 1000m, Memory: 512Mi) prevent pods from consuming unlimited resources\n",
    "- **OOM kills**: When memory exceeds limit, Kubernetes kills pod with OOMKilled status\n",
    "- **Horizontal Pod Autoscaler (HPA)**: Auto-scales replicas when CPU >70% or memory >80%\n",
    "- **Graceful degradation**: System sheds non-critical load (e.g., skip expensive analytics, return cached results)\n",
    "\n",
    "**Resource Exhaustion Scenarios:**\n",
    "- **CPU stress**: Spike CPU to 100% (test HPA scales up replicas)\n",
    "- **Memory leak**: Gradually increase memory usage (test OOM kill and restart)\n",
    "- **Disk I/O**: Saturate disk throughput (test I/O limits, backpressure)\n",
    "- **Connection pool exhaustion**: Max out database connections (test circuit breakers, queuing)\n",
    "\n",
    "**Kubernetes Resource Management:**\n",
    "- **Requests**: Guaranteed resources (scheduler uses for placement)\n",
    "- **Limits**: Maximum resources (pod killed if exceeded)\n",
    "- **HPA**: Auto-scales based on metrics (CPU, memory, custom metrics)\n",
    "- **Vertical Pod Autoscaler (VPA)**: Adjusts requests/limits over time\n",
    "\n",
    "**Why This Matters for Post-Silicon:**\n",
    "- **STDF ETL**: Large wafer data files (100GB+) can exhaust memory, test streaming processing\n",
    "- **ML Training**: Model training saturates CPU/GPU, validate auto-scaling prevents job starvation\n",
    "- **Yield Prediction API**: Traffic spikes (new wafer lot released) test horizontal scaling\n",
    "- **Real-Time Monitoring**: High-cardinality metrics (millions of devices) test resource limits\n",
    "\n",
    "**Example Business Impact:**\n",
    "- **Before chaos**: STDF ETL crashed weekly from OOM (4-hour recovery, $80K/incident)\n",
    "- **After chaos**: Configured memory limits + streaming processing (zero crashes, $4.2M/year saved)\n",
    "- **Lessons learned**: Always set resource limits, test OOM behavior, implement backpressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574b1ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource Exhaustion and Auto-Scaling Implementation\n",
    "\n",
    "@dataclass\n",
    "class ResourceMetrics:\n",
    "    \"\"\"Track CPU and memory usage\"\"\"\n",
    "    cpu_percent: float = 0.0\n",
    "    memory_mb: float = 0.0\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "@dataclass\n",
    "class PodWithResources:\n",
    "    \"\"\"Pod with resource limits and usage tracking\"\"\"\n",
    "    pod_id: str\n",
    "    cpu_limit_millicores: int = 1000  # 1 CPU\n",
    "    memory_limit_mb: int = 512  # 512 MB\n",
    "    cpu_request_millicores: int = 500  # 0.5 CPU\n",
    "    memory_request_mb: int = 256  # 256 MB\n",
    "    \n",
    "    cpu_usage_millicores: int = 100\n",
    "    memory_usage_mb: int = 128\n",
    "    state: PodState = PodState.RUNNING\n",
    "    oom_killed: bool = False\n",
    "    throttled_count: int = 0\n",
    "    \n",
    "    def simulate_cpu_load(self, additional_millicores: int):\n",
    "        \"\"\"Add CPU load (may throttle if exceeds limit)\"\"\"\n",
    "        self.cpu_usage_millicores += additional_millicores\n",
    "        if self.cpu_usage_millicores > self.cpu_limit_millicores:\n",
    "            self.throttled_count += 1\n",
    "            self.cpu_usage_millicores = self.cpu_limit_millicores  # CPU throttled\n",
    "    \n",
    "    def simulate_memory_load(self, additional_mb: int):\n",
    "        \"\"\"Add memory load (may OOM kill if exceeds limit)\"\"\"\n",
    "        self.memory_usage_mb += additional_mb\n",
    "        if self.memory_usage_mb > self.memory_limit_mb:\n",
    "            self.oom_killed = True\n",
    "            self.state = PodState.FAILED\n",
    "    \n",
    "    def get_cpu_percent(self) -> float:\n",
    "        \"\"\"Get CPU usage as percentage of limit\"\"\"\n",
    "        return (self.cpu_usage_millicores / self.cpu_limit_millicores) * 100\n",
    "    \n",
    "    def get_memory_percent(self) -> float:\n",
    "        \"\"\"Get memory usage as percentage of limit\"\"\"\n",
    "        return (self.memory_usage_mb / self.memory_limit_mb) * 100\n",
    "    \n",
    "    def is_healthy(self) -> bool:\n",
    "        \"\"\"Check if pod is running and not OOM killed\"\"\"\n",
    "        return self.state == PodState.RUNNING and not self.oom_killed\n",
    "\n",
    "@dataclass\n",
    "class HorizontalPodAutoscaler:\n",
    "    \"\"\"Kubernetes HPA simulation\"\"\"\n",
    "    name: str\n",
    "    min_replicas: int = 2\n",
    "    max_replicas: int = 10\n",
    "    cpu_target_percent: float = 70.0\n",
    "    memory_target_percent: float = 80.0\n",
    "    scale_up_cooldown_seconds: int = 30\n",
    "    scale_down_cooldown_seconds: int = 60\n",
    "    \n",
    "    current_replicas: int = field(init=False)\n",
    "    last_scale_time: Optional[datetime] = None\n",
    "    scale_up_count: int = 0\n",
    "    scale_down_count: int = 0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.current_replicas = self.min_replicas\n",
    "    \n",
    "    def evaluate(self, pods: List[PodWithResources]) -> int:\n",
    "        \"\"\"Determine desired replica count based on resource metrics\"\"\"\n",
    "        if not pods:\n",
    "            return self.min_replicas\n",
    "        \n",
    "        # Calculate average resource usage across healthy pods\n",
    "        healthy_pods = [p for p in pods if p.is_healthy()]\n",
    "        if not healthy_pods:\n",
    "            return self.min_replicas\n",
    "        \n",
    "        avg_cpu = sum(p.get_cpu_percent() for p in healthy_pods) / len(healthy_pods)\n",
    "        avg_memory = sum(p.get_memory_percent() for p in healthy_pods) / len(healthy_pods)\n",
    "        \n",
    "        # Determine if scaling needed\n",
    "        desired_replicas = self.current_replicas\n",
    "        \n",
    "        # Scale up if CPU or memory exceeds target\n",
    "        if avg_cpu > self.cpu_target_percent or avg_memory > self.memory_target_percent:\n",
    "            desired_replicas = min(self.current_replicas + 1, self.max_replicas)\n",
    "        \n",
    "        # Scale down if both CPU and memory well below target\n",
    "        elif avg_cpu < self.cpu_target_percent * 0.5 and avg_memory < self.memory_target_percent * 0.5:\n",
    "            desired_replicas = max(self.current_replicas - 1, self.min_replicas)\n",
    "        \n",
    "        # Apply cooldown\n",
    "        if self.last_scale_time:\n",
    "            if desired_replicas > self.current_replicas:  # Scale up\n",
    "                if (datetime.now() - self.last_scale_time).seconds < self.scale_up_cooldown_seconds:\n",
    "                    return self.current_replicas\n",
    "            elif desired_replicas < self.current_replicas:  # Scale down\n",
    "                if (datetime.now() - self.last_scale_time).seconds < self.scale_down_cooldown_seconds:\n",
    "                    return self.current_replicas\n",
    "        \n",
    "        # Track scaling events\n",
    "        if desired_replicas > self.current_replicas:\n",
    "            self.scale_up_count += 1\n",
    "            print(f\"    \ud83d\udcc8 HPA scaling up: {self.current_replicas} \u2192 {desired_replicas} (CPU: {avg_cpu:.1f}%, Mem: {avg_memory:.1f}%)\")\n",
    "        elif desired_replicas < self.current_replicas:\n",
    "            self.scale_down_count += 1\n",
    "            print(f\"    \ud83d\udcc9 HPA scaling down: {self.current_replicas} \u2192 {desired_replicas} (CPU: {avg_cpu:.1f}%, Mem: {avg_memory:.1f}%)\")\n",
    "        \n",
    "        if desired_replicas != self.current_replicas:\n",
    "            self.last_scale_time = datetime.now()\n",
    "            self.current_replicas = desired_replicas\n",
    "        \n",
    "        return desired_replicas\n",
    "\n",
    "# Example 1: CPU exhaustion with auto-scaling\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CHAOS EXPERIMENT 5: CPU Exhaustion and Auto-Scaling\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\ud83d\udccb Hypothesis: HPA scales up when CPU >70%, scales down when CPU <35%\\n\")\n",
    "\n",
    "# Setup: 2 pods with HPA\n",
    "pods = [\n",
    "    PodWithResources(pod_id=f\"stdf-etl-{i}\", cpu_limit_millicores=1000, memory_limit_mb=512)\n",
    "    for i in range(2)\n",
    "]\n",
    "\n",
    "hpa = HorizontalPodAutoscaler(\n",
    "    name=\"stdf-etl-hpa\",\n",
    "    min_replicas=2,\n",
    "    max_replicas=8,\n",
    "    cpu_target_percent=70.0,\n",
    "    scale_up_cooldown_seconds=1,  # Short for demo\n",
    "    scale_down_cooldown_seconds=2\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Baseline: {len(pods)} pods, CPU usage ~{pods[0].get_cpu_percent():.1f}%\\n\")\n",
    "\n",
    "# Phase 1: Normal load\n",
    "print(\"\ud83d\udcca Phase 1: Normal load (10% CPU per pod)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(3):\n",
    "    # Simulate low CPU usage\n",
    "    for pod in pods:\n",
    "        pod.cpu_usage_millicores = 100\n",
    "    \n",
    "    avg_cpu = sum(p.get_cpu_percent() for p in pods) / len(pods)\n",
    "    print(f\"  Period {i+1}: {len(pods)} pods, avg CPU: {avg_cpu:.1f}%\")\n",
    "    \n",
    "    # HPA evaluation\n",
    "    desired = hpa.evaluate(pods)\n",
    "\n",
    "# Phase 2: Spike CPU load (trigger scale-up)\n",
    "print(\"\\n\ud83d\udca5 Phase 2: Spike CPU load (90% CPU per pod)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(8):\n",
    "    # Simulate high CPU usage\n",
    "    for pod in pods:\n",
    "        pod.cpu_usage_millicores = 900\n",
    "    \n",
    "    avg_cpu = sum(p.get_cpu_percent() for p in pods if p.is_healthy()) / len([p for p in pods if p.is_healthy()])\n",
    "    print(f\"  Period {i+1}: {len(pods)} pods, avg CPU: {avg_cpu:.1f}%\")\n",
    "    \n",
    "    # HPA evaluation and scaling\n",
    "    desired = hpa.evaluate(pods)\n",
    "    \n",
    "    # Add pods if scaling up\n",
    "    while len(pods) < desired:\n",
    "        new_pod = PodWithResources(pod_id=f\"stdf-etl-{len(pods)}\", cpu_usage_millicores=900)\n",
    "        pods.append(new_pod)\n",
    "        time.sleep(0.1)  # Simulate pod startup\n",
    "\n",
    "# Phase 3: Reduce load (trigger scale-down)\n",
    "print(\"\\n\ud83d\udd04 Phase 3: Reduce load (20% CPU per pod)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(6):\n",
    "    # Simulate low CPU usage\n",
    "    for pod in pods:\n",
    "        pod.cpu_usage_millicores = 200\n",
    "    \n",
    "    avg_cpu = sum(p.get_cpu_percent() for p in pods if p.is_healthy()) / len([p for p in pods if p.is_healthy()])\n",
    "    print(f\"  Period {i+1}: {len(pods)} pods, avg CPU: {avg_cpu:.1f}%\")\n",
    "    \n",
    "    # HPA evaluation and scaling\n",
    "    desired = hpa.evaluate(pods)\n",
    "    \n",
    "    # Remove pods if scaling down\n",
    "    while len(pods) > desired:\n",
    "        pods.pop()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# Results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca HPA Statistics:\")\n",
    "print(f\"   Scale-up events: {hpa.scale_up_count}\")\n",
    "print(f\"   Scale-down events: {hpa.scale_down_count}\")\n",
    "print(f\"   Final replicas: {len(pods)}\")\n",
    "print(f\"   Min replicas maintained: {len(pods) >= hpa.min_replicas}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udca1 Observations:\")\n",
    "print(f\"   \u2022 HPA scaled up from 2 \u2192 {hpa.max_replicas} pods during CPU spike\")\n",
    "print(f\"   \u2022 Auto-scaling responded within 1-2 periods (real: 30-60 seconds)\")\n",
    "print(f\"   \u2022 Scaled down to {len(pods)} pods when load normalized\")\n",
    "print(f\"   \u2022 System maintained {hpa.min_replicas} pods minimum\")\n",
    "\n",
    "# Example 2: Memory exhaustion and OOM kills\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CHAOS EXPERIMENT 6: Memory Exhaustion and OOM Kills\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\ud83d\udccb Hypothesis: Pods killed when memory >512MB, ReplicaSet restarts them\\n\")\n",
    "\n",
    "# Setup: 3 pods processing STDF files\n",
    "pods = [\n",
    "    PodWithResources(pod_id=f\"stdf-parser-{i}\", memory_limit_mb=512, memory_usage_mb=200)\n",
    "    for i in range(3)\n",
    "]\n",
    "\n",
    "print(f\"\u2705 Baseline: {len(pods)} pods, memory usage ~{pods[0].get_memory_percent():.1f}%\\n\")\n",
    "\n",
    "print(\"\ud83d\udca5 Simulate memory leak (gradual memory increase)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "oom_count = 0\n",
    "restart_count = 0\n",
    "\n",
    "for period in range(10):\n",
    "    print(f\"\\n  Period {period + 1}:\")\n",
    "    \n",
    "    for i, pod in enumerate(pods):\n",
    "        if pod.is_healthy():\n",
    "            # Simulate memory leak (50MB per period)\n",
    "            pod.simulate_memory_load(50)\n",
    "            \n",
    "            # Check for OOM\n",
    "            if pod.oom_killed:\n",
    "                oom_count += 1\n",
    "                print(f\"    \u274c Pod {pod.pod_id}: OOMKilled (memory: {pod.memory_usage_mb}MB > {pod.memory_limit_mb}MB)\")\n",
    "                \n",
    "                # ReplicaSet restarts pod\n",
    "                pods[i] = PodWithResources(pod_id=pod.pod_id, memory_usage_mb=200)\n",
    "                restart_count += 1\n",
    "                print(f\"    \ud83d\udd04 ReplicaSet restarted {pod.pod_id}\")\n",
    "            else:\n",
    "                print(f\"    \u2705 Pod {pod.pod_id}: Running (memory: {pod.memory_usage_mb}MB, {pod.get_memory_percent():.1f}%)\")\n",
    "\n",
    "# Results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Memory Management Statistics:\")\n",
    "print(f\"   OOM kills: {oom_count}\")\n",
    "print(f\"   Pod restarts: {restart_count}\")\n",
    "print(f\"   Healthy pods: {len([p for p in pods if p.is_healthy()])}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udca1 Observations:\")\n",
    "print(f\"   \u2022 Pods killed when memory exceeded 512MB limit\")\n",
    "print(f\"   \u2022 ReplicaSet automatically restarted OOM-killed pods\")\n",
    "print(f\"   \u2022 Memory leak detected (gradual increase across restarts)\")\n",
    "print(f\"   \u2022 Fix: Implement streaming STDF processing (avoid loading entire file)\")\n",
    "\n",
    "print(f\"\\n\u2705 Chaos experiments complete!\")\n",
    "print(f\"\u2705 Auto-scaling validated (CPU-based)\")\n",
    "print(f\"\u2705 OOM kill behavior confirmed\")\n",
    "print(f\"\u2705 Resource limits enforced correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0e702",
   "metadata": {},
   "source": [
    "## 5. \ud83c\udfaf Real-World Chaos Engineering Projects\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "#### Project 1: STDF ETL Pipeline Resilience Testing \ud83c\udfed\n",
    "\n",
    "**Objective:** Validate STDF data pipeline withstands failures during wafer test data ingestion (prevent 4-hour outages costing $80K each).\n",
    "\n",
    "**Business Value:** $5.8M/year (73 prevented outages \u00d7 $80K/outage)\n",
    "\n",
    "**Chaos Experiments:**\n",
    "1. **Pod Termination:** Kill Spark executor pods during 100GB STDF file processing\n",
    "   - **Success Criteria:** Job completes within 10% of normal time, zero data loss\n",
    "   - **Expected:** Spark driver reschedules tasks on remaining executors\n",
    "   - **Metrics:** Job completion time, data records processed, checkpoints recovered\n",
    "\n",
    "2. **Network Partition:** Isolate HDFS datanodes from Spark cluster\n",
    "   - **Success Criteria:** Spark switches to replica datanodes, latency <200ms\n",
    "   - **Expected:** HDFS replication (3x) prevents data unavailability\n",
    "   - **Metrics:** HDFS block access latency, replica fetch count\n",
    "\n",
    "3. **Disk Full:** Fill disk on STDF staging volume\n",
    "   - **Success Criteria:** Pipeline alerts operators, gracefully stops ingestion\n",
    "   - **Expected:** Disk monitoring triggers PagerDuty alert\n",
    "   - **Metrics:** Alert time, operator response time, pipeline recovery time\n",
    "\n",
    "4. **Database Outage:** Simulate PostgreSQL metadata store failure\n",
    "   - **Success Criteria:** Pipeline serves from Redis cache, continues processing\n",
    "   - **Expected:** Circuit breaker opens, fallback to cache\n",
    "   - **Metrics:** Cache hit rate, failed queries, user-facing errors\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use Chaos Mesh `PodChaos` for pod termination\n",
    "- Use `NetworkChaos` for partition simulation\n",
    "- Use `StressChaos` for disk exhaustion\n",
    "- Monitor with Prometheus + Grafana (job duration, throughput, error rate)\n",
    "\n",
    "**Validation:**\n",
    "- Run chaos experiments weekly (automated GameDay)\n",
    "- Track MTTR (target: <15 minutes)\n",
    "- Measure blast radius (affected wafer lots, revenue impact)\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 2: Multi-Region Wafer Map Failover \ud83c\udf0d\n",
    "\n",
    "**Objective:** Validate wafer map analysis service fails over to backup region within 30 seconds (reduce RTO from 8 minutes).\n",
    "\n",
    "**Business Value:** $3.9M/year (prevent 12 regional outages \u00d7 $325K revenue impact each)\n",
    "\n",
    "**Chaos Experiments:**\n",
    "1. **Availability Zone Failure:** Simulate AZ-1a outage (AWS EC2 instances)\n",
    "   - **Success Criteria:** Traffic routes to AZ-1b within 30s, zero data loss\n",
    "   - **Expected:** ALB health checks detect failure, route to healthy targets\n",
    "   - **Metrics:** Health check latency, DNS failover time, request error rate\n",
    "\n",
    "2. **Cross-Region Replication Lag:** Inject 5-minute lag in S3 replication\n",
    "   - **Success Criteria:** Service uses stale data with staleness indicator\n",
    "   - **Expected:** Application checks replication metadata, shows warning banner\n",
    "   - **Metrics:** Replication lag (seconds), stale data served (%), user alerts shown\n",
    "\n",
    "3. **DynamoDB Global Table Conflict:** Simulate concurrent writes to same wafer_id\n",
    "   - **Success Criteria:** Last-write-wins conflict resolution, no data corruption\n",
    "   - **Expected:** DynamoDB global tables use vector clocks\n",
    "   - **Metrics:** Conflict count, resolution latency, data consistency checks\n",
    "\n",
    "4. **Route 53 DNS Failover:** Force primary health check failure\n",
    "   - **Success Criteria:** DNS resolves to secondary region within 60s\n",
    "   - **Expected:** Route 53 health checks fail, update DNS records\n",
    "   - **Metrics:** DNS resolution time, TTL expiry, client-side caching impact\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use AWS Fault Injection Simulator (FIS) for AZ outages\n",
    "- Configure Route 53 health checks (30s interval, 3 consecutive failures)\n",
    "- Use S3 replication metrics (ReplicationLatency, BytesPendingReplication)\n",
    "- Test DynamoDB global table conflicts with synthetic concurrent writes\n",
    "\n",
    "**Validation:**\n",
    "- Run quarterly multi-region failover drills\n",
    "- Measure RTO (target: <30 seconds)\n",
    "- Measure RPO (target: zero data loss)\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 3: Yield Prediction API Fault Tolerance \ud83d\udcca\n",
    "\n",
    "**Objective:** Validate ML inference API maintains 99.9% uptime during dependency failures (prevent cascading failures costing $4.6M/year).\n",
    "\n",
    "**Business Value:** $4.6M/year (maintain SLA, avoid penalties + lost revenue)\n",
    "\n",
    "**Chaos Experiments:**\n",
    "1. **Model Serving Latency:** Inject 5s latency into TensorFlow Serving\n",
    "   - **Success Criteria:** Circuit breaker opens, API returns cached prediction\n",
    "   - **Expected:** Timeout after 2s, fallback to last known prediction\n",
    "   - **Metrics:** P95 latency, timeout rate, cache hit rate\n",
    "\n",
    "2. **Feature Store Outage:** Simulate Redis feature cache failure\n",
    "   - **Success Criteria:** API computes features on-the-fly (degraded mode)\n",
    "   - **Expected:** Fallback to database feature extraction (slower but functional)\n",
    "   - **Metrics:** Feature fetch latency (cache vs database), prediction accuracy\n",
    "\n",
    "3. **Database Connection Pool Exhaustion:** Max out 100 PostgreSQL connections\n",
    "   - **Success Criteria:** API queues requests, returns 429 Too Many Requests\n",
    "   - **Expected:** Connection pool implements backpressure (reject new requests)\n",
    "   - **Metrics:** Connection pool utilization, queued requests, rejected requests\n",
    "\n",
    "4. **Memory Leak in Model Server:** Gradually increase memory usage to OOM\n",
    "   - **Success Criteria:** Kubernetes kills pod, HPA scales up, zero downtime\n",
    "   - **Expected:** Liveness probe fails, pod restarted, load balancer routes around\n",
    "   - **Metrics:** Memory usage trend, OOM kill count, pod restart time\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use Toxiproxy for latency injection (TensorFlow Serving proxy)\n",
    "- Configure circuit breakers (Hystrix thresholds: 10 failures in 10s)\n",
    "- Implement connection pool monitoring (HikariCP metrics)\n",
    "- Use Kubernetes liveness probe (HTTP /health endpoint, 30s timeout)\n",
    "\n",
    "**Validation:**\n",
    "- Run chaos experiments during canary deployments\n",
    "- Monitor SLA compliance (99.9% uptime = 43 minutes downtime/month)\n",
    "- Track business metrics (predictions served, revenue protected)\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 4: Spot Instance Interruption Handling \ud83d\udcb0\n",
    "\n",
    "**Objective:** Validate ML training jobs checkpoint correctly when spot instances terminated (prevent data loss, save $3.2M/year on compute costs).\n",
    "\n",
    "**Business Value:** $3.2M/year (70% cost reduction using spot vs on-demand, zero data loss)\n",
    "\n",
    "**Chaos Experiments:**\n",
    "1. **Spot Termination (2-minute warning):** Simulate EC2 spot interruption notice\n",
    "   - **Success Criteria:** Job checkpoints within 90s, resumes on new instance\n",
    "   - **Expected:** Spot termination handler saves model state to S3\n",
    "   - **Metrics:** Checkpoint time, resume time, training progress preserved\n",
    "\n",
    "2. **No Warning Termination:** Immediate instance kill (hardware failure simulation)\n",
    "   - **Success Criteria:** Job resumes from last checkpoint (max 5 minutes lost)\n",
    "   - **Expected:** Periodic checkpointing (every 5 min) prevents major data loss\n",
    "   - **Metrics:** Data loss (training steps), recovery time, checkpoint frequency\n",
    "\n",
    "3. **S3 Checkpoint Upload Failure:** Simulate S3 network partition during save\n",
    "   - **Success Criteria:** Job retries checkpoint upload, delays termination\n",
    "   - **Expected:** Exponential backoff retry (3 attempts, 1s/2s/4s delays)\n",
    "   - **Metrics:** Checkpoint upload retries, timeout occurrences, data loss events\n",
    "\n",
    "4. **Concurrent Spot Terminations:** Terminate 50% of spot fleet simultaneously\n",
    "   - **Success Criteria:** Distributed training degrades gracefully, completes job\n",
    "   - **Expected:** Horovod/PyTorch DDP reduces world size, continues training\n",
    "   - **Metrics:** Effective training throughput, job completion time, cost impact\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use AWS Node Termination Handler (DaemonSet monitors spot termination)\n",
    "- Configure checkpoint frequency based on training speed (target: <5 min loss)\n",
    "- Use S3 multipart uploads for large checkpoints (resume partial uploads)\n",
    "- Test distributed training failure modes (node loss, network partition)\n",
    "\n",
    "**Validation:**\n",
    "- Run spot termination drills weekly (automated)\n",
    "- Measure checkpoint latency (target: <90 seconds for 10GB model)\n",
    "- Calculate cost savings vs data loss trade-off\n",
    "\n",
    "---\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "#### Project 5: Real-Time Recommendation Engine Resilience \ud83c\udfac\n",
    "\n",
    "**Objective:** Validate recommendation API serves fallback recommendations during ML model failures (maintain user experience).\n",
    "\n",
    "**Business Value:** $6.5M/year (prevent 2% revenue loss from failed recommendations)\n",
    "\n",
    "**Chaos Experiments:**\n",
    "1. **Model Prediction Timeout:** ML model takes >5s to respond\n",
    "2. **Cold Start:** User has no history (collaborative filtering fails)\n",
    "3. **A/B Test Service Outage:** Experiment assignment service down\n",
    "4. **Personalization Cache Stale:** Redis cache returns 2-hour-old recommendations\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Implement fallback layers: Personalized \u2192 Segment-based \u2192 Popular items\n",
    "- Use circuit breakers for model serving (timeout: 2s, fallback: cached)\n",
    "- Pre-compute popular items backup (daily batch job)\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 6: Fraud Detection System Chaos Testing \ud83d\udd12\n",
    "\n",
    "**Objective:** Validate fraud detection processes transactions during dependency failures (prevent false positives blocking legitimate transactions).\n",
    "\n",
    "**Business Value:** $8.3M/year (reduce false positive rate from 5% to 0.5%)\n",
    "\n",
    "**Chaos Experiments:**\n",
    "1. **Graph Database Outage:** Neo4j relationship queries fail\n",
    "2. **Feature Store Latency:** Feature retrieval takes >500ms\n",
    "3. **Model Drift:** Suddenly deploy model with 10x higher false positive rate\n",
    "4. **Message Queue Backlog:** Kafka lag reaches 10 million messages\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Implement risk-based fallback (high-risk: block, low-risk: allow + async review)\n",
    "- Use feature store cache (Redis, 99th percentile <50ms)\n",
    "- Monitor model performance drift (PSI, KS statistic, false positive rate)\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 7: Distributed Tracing Pipeline Resilience \ud83d\udcc8\n",
    "\n",
    "**Objective:** Validate observability pipeline continues during Kafka/Elasticsearch outages (prevent blind spots during incidents).\n",
    "\n",
    "**Business Value:** $2.4M/year (reduce MTTR from 45 min to 10 min)\n",
    "\n",
    "**Chaos Experiments:**\n",
    "1. **Kafka Broker Failure:** Lose 1 of 3 Kafka brokers\n",
    "2. **Elasticsearch Index Unavailable:** Index in read-only mode (disk full)\n",
    "3. **High Cardinality Explosion:** Suddenly receive 10M unique trace IDs\n",
    "4. **Network Partition:** Isolate Jaeger collectors from storage\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use Kafka replication factor 3, min.insync.replicas 2\n",
    "- Implement trace sampling (adaptive: 100% for errors, 1% for success)\n",
    "- Configure Elasticsearch index lifecycle management (delete old indices)\n",
    "\n",
    "---\n",
    "\n",
    "#### Project 8: CI/CD Pipeline Chaos Engineering \ud83d\ude80\n",
    "\n",
    "**Objective:** Validate CI/CD pipeline completes deployments during infrastructure failures (prevent deployment freezes).\n",
    "\n",
    "**Business Value:** $1.9M/year (maintain 4-hour deployment SLA, avoid overtime costs)\n",
    "\n",
    "**Chaos Experiments:**\n",
    "1. **GitHub Actions Runner Failure:** Lose 50% of self-hosted runners\n",
    "2. **Docker Registry Outage:** ECR/Docker Hub unavailable\n",
    "3. **Kubernetes API Server Slow:** kubectl commands take >10s\n",
    "4. **Helm Upgrade Timeout:** Helm release stuck in pending state\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use multiple runner pools (scale-to-zero on-demand runners)\n",
    "- Configure Docker image caching (local registry mirror)\n",
    "- Implement Kubernetes watch timeouts (detect hung resources)\n",
    "- Use Helm rollback automation (detect failed deployments, auto-rollback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb9c08",
   "metadata": {},
   "source": [
    "## 6. \ud83c\udf93 Comprehensive Takeaways\n",
    "\n",
    "### \u2705 When to Use Chaos Engineering\n",
    "\n",
    "**Perfect For:**\n",
    "- **Production ML systems** serving critical business functions (fraud detection, recommendations, pricing)\n",
    "- **Distributed systems** with multiple failure modes (microservices, Kubernetes, multi-region)\n",
    "- **High-availability requirements** (99.9%+ uptime SLAs, financial/healthcare applications)\n",
    "- **Complex dependencies** where cascading failures possible (feature stores, model servers, databases)\n",
    "- **Compliance requirements** needing disaster recovery validation (SOC 2, HIPAA, PCI-DSS)\n",
    "- **Post-mortems** revealing unknown failure modes (\"we didn't know X could fail\")\n",
    "- **Confidence building** before major launches (Black Friday, product releases)\n",
    "\n",
    "**Not Ideal For:**\n",
    "- **Early-stage prototypes** without production users (chaos adds complexity)\n",
    "- **Simple monoliths** with few dependencies (traditional testing sufficient)\n",
    "- **Non-critical systems** where downtime acceptable (internal tools, dev environments)\n",
    "- **Immature monitoring** (fix observability first, then add chaos)\n",
    "- **Untested systems** (validate basic functionality before chaos testing)\n",
    "\n",
    "### \ud83c\udfaf Key Principles\n",
    "\n",
    "1. **Start Small, Increase Gradually (Blast Radius Control)**\n",
    "   - Begin: 1% traffic, single pod, non-production\n",
    "   - Expand: 10% traffic, single AZ, production (low-traffic hours)\n",
    "   - Full scale: 100% traffic, multi-region, peak hours\n",
    "   - **Example:** Netflix's Chaos Monkey started with 1 instance/day, now terminates 100s\n",
    "\n",
    "2. **Build Hypothesis-Driven Experiments**\n",
    "   - **Bad:** \"Let's kill some pods and see what happens\"\n",
    "   - **Good:** \"Hypothesis: Circuit breaker opens after 10 failures, API serves from cache with <100ms latency\"\n",
    "   - **Validation:** Measure circuit breaker state transitions, cache hit rate, P95 latency\n",
    "\n",
    "3. **Automate Chaos (Continuous Resilience Testing)**\n",
    "   - Manual chaos: Run quarterly (limited coverage)\n",
    "   - Automated chaos: Run daily/weekly (comprehensive coverage)\n",
    "   - **Tools:** Chaos Mesh CronChaos, Litmus ChaosScheduler, AWS FIS templates\n",
    "   - **Example:** Automated GameDay runs every Monday 2 AM (low traffic)\n",
    "\n",
    "4. **Monitor Everything (Observability First)**\n",
    "   - **Before chaos:** Instrument code (Prometheus metrics, structured logs, distributed tracing)\n",
    "   - **During chaos:** Real-time dashboards (Grafana, Datadog, New Relic)\n",
    "   - **After chaos:** Post-experiment analysis (incident timeline, metrics correlation)\n",
    "   - **Golden signals:** Latency, traffic, errors, saturation\n",
    "\n",
    "5. **Have Rollback Plans (Safety First)**\n",
    "   - **Chaos kill switch:** Immediately stop experiments (manual button, automated circuit breaker)\n",
    "   - **Incident response:** Runbooks for chaos-induced outages (escalation, communication)\n",
    "   - **Business approval:** Get executive buy-in (explain value, risk mitigation)\n",
    "\n",
    "### \u26a0\ufe0f Common Pitfalls\n",
    "\n",
    "\u274c **Running chaos without monitoring**\n",
    "- Symptom: Chaos runs, unknown if system passed/failed\n",
    "- Fix: Instrument before chaos (metrics, logs, traces)\n",
    "\n",
    "\u274c **Testing in staging only**\n",
    "- Symptom: Staging passes, production fails (different scale, data, configurations)\n",
    "- Fix: Run controlled production chaos (start 1%, increase gradually)\n",
    "\n",
    "\u274c **Blaming chaos for outages**\n",
    "- Symptom: Teams avoid chaos engineering (\"too risky\")\n",
    "- Fix: Position chaos as \"discover bugs before customers do\"\n",
    "- Mindset: Chaos finds *existing* bugs, doesn't create new ones\n",
    "\n",
    "\u274c **No automated rollback**\n",
    "- Symptom: Chaos experiment causes outage, manual intervention required\n",
    "- Fix: Automated chaos termination (timeout, error threshold, manual kill switch)\n",
    "\n",
    "\u274c **Ignoring findings**\n",
    "- Symptom: Chaos reveals issues, team doesn't fix them (backlog forever)\n",
    "- Fix: Track chaos findings as P0/P1 bugs, assign owners, set deadlines\n",
    "\n",
    "\u274c **Over-engineering resilience**\n",
    "- Symptom: Spent $500K building 5-region active-active, only need 99.9% uptime\n",
    "- Fix: Right-size resilience based on SLA requirements (cost-benefit analysis)\n",
    "\n",
    "\u274c **Testing synthetic failures only**\n",
    "- Symptom: Test pod kills, miss real failure modes (DNS timeouts, slow database queries)\n",
    "- Fix: Model real-world failures from production incidents (post-mortem analysis)\n",
    "\n",
    "### \ud83d\udd27 Best Practices\n",
    "\n",
    "\u2705 **Start with GameDays (Scheduled Chaos)**\n",
    "- Schedule: First Friday of month, 2-hour window\n",
    "- Participants: SRE, development, product (cross-functional)\n",
    "- Scenarios: 3-5 experiments (pod kill, latency, resource exhaustion)\n",
    "- Debrief: Post-GameDay retrospective (what worked, what failed, action items)\n",
    "\n",
    "\u2705 **Chaos Engineering Maturity Model**\n",
    "- **Level 1 - Manual:** Quarterly GameDays, manual experiments, staging only\n",
    "- **Level 2 - Automated:** Weekly automated chaos, production (1% traffic), monitoring dashboards\n",
    "- **Level 3 - Continuous:** Daily chaos, 100% traffic, auto-remediation, chaos as code\n",
    "- **Level 4 - Proactive:** Chaos prevents outages, integrated into CI/CD, chaos-first culture\n",
    "\n",
    "\u2705 **Document Experiments (Chaos Runbooks)**\n",
    "```markdown\n",
    "# Experiment: Database Connection Pool Exhaustion\n",
    "\n",
    "**Hypothesis:** API returns 429 Too Many Requests when connection pool full\n",
    "\n",
    "**Blast Radius:** 5% traffic, 2-minute duration, staging environment\n",
    "\n",
    "**Procedure:**\n",
    "1. Deploy Toxiproxy in front of PostgreSQL\n",
    "2. Limit connections to 10 (normal: 100)\n",
    "3. Generate 50 concurrent requests\n",
    "4. Observe API response codes, connection pool metrics\n",
    "\n",
    "**Success Criteria:**\n",
    "- API returns 429 status (not 500)\n",
    "- Connection pool reaches 10/10 utilization\n",
    "- P95 latency <200ms (queue, don't crash)\n",
    "- Zero database crashes\n",
    "\n",
    "**Rollback:** Remove Toxiproxy, restore connection pool to 100\n",
    "\n",
    "**Findings:** \u2705 Passed, API correctly implements backpressure\n",
    "```\n",
    "\n",
    "\u2705 **Integrate with CI/CD (Shift-Left Chaos)**\n",
    "- **Pre-deployment:** Run chaos tests in staging (automated)\n",
    "- **Canary deployment:** Run chaos on canary (1% traffic)\n",
    "- **Full rollout:** Confidence from chaos validation\n",
    "- **Example:** Kubernetes admission webhook blocks pods without resource limits\n",
    "\n",
    "\u2705 **Chaos Engineering Tools**\n",
    "\n",
    "| Tool | Use Case | Platform | Experiments |\n",
    "|------|----------|----------|-------------|\n",
    "| **Chaos Mesh** | Kubernetes chaos | K8s | Pod kill, network partition, I/O delay |\n",
    "| **Litmus** | Kubernetes chaos | K8s | Node drain, container kill, DNS chaos |\n",
    "| **AWS FIS** | AWS infrastructure chaos | AWS | EC2 stop, RDS failover, ECS task kill |\n",
    "| **Azure Chaos Studio** | Azure infrastructure chaos | Azure | VM stop, network latency, disk I/O |\n",
    "| **Gremlin** | Enterprise chaos platform | Any | CPU stress, memory leak, time travel |\n",
    "| **Toxiproxy** | Network chaos proxy | Any | Latency, bandwidth, timeout |\n",
    "| **Pumba** | Docker chaos | Docker | Container kill, network loss, stress |\n",
    "\n",
    "### \ud83d\udcca Measuring Chaos Engineering Success\n",
    "\n",
    "**Technical Metrics:**\n",
    "- **MTTR (Mean Time to Recovery):** Time to recover from failures\n",
    "  - **Before chaos:** 45 minutes (manual debugging)\n",
    "  - **After chaos:** 8 minutes (automated runbooks, practiced recovery)\n",
    "- **MTBF (Mean Time Between Failures):** Frequency of outages\n",
    "  - **Before chaos:** 12 outages/year (unknown failure modes)\n",
    "  - **After chaos:** 2 outages/year (proactive fixes from chaos findings)\n",
    "- **Blast radius:** Scope of failures (users affected, revenue lost)\n",
    "  - **Before chaos:** 100% users affected (cascading failures)\n",
    "  - **After chaos:** 5% users affected (circuit breakers, graceful degradation)\n",
    "\n",
    "**Business Metrics:**\n",
    "- **Uptime SLA compliance:** 99.9% \u2192 99.95% (5x reduction in downtime)\n",
    "- **Revenue protected:** $17.5M/year (prevented outages)\n",
    "- **Customer trust:** NPS +12 points (reliable service)\n",
    "- **Engineering confidence:** Deploy 4x/day (vs 1x/week)\n",
    "\n",
    "**Cultural Metrics:**\n",
    "- **Incident learning:** Post-mortems \u2192 proactive chaos experiments\n",
    "- **Resilience mindset:** \"How will this fail?\" asked in design reviews\n",
    "- **Blameless culture:** Chaos findings celebrated, not punished\n",
    "\n",
    "### \ud83d\ude80 Next Steps in Your Chaos Engineering Journey\n",
    "\n",
    "**Immediate (Next 1-2 Weeks):**\n",
    "1. \u2705 Inventory critical systems and dependencies (draw architecture diagrams)\n",
    "2. \u2705 Set up monitoring (Prometheus, Grafana, or equivalent)\n",
    "3. \u2705 Run first manual chaos experiment (pod termination in staging)\n",
    "4. \u2705 Document findings and share with team\n",
    "\n",
    "**Short-Term (Next 1-3 Months):**\n",
    "1. \u2705 Schedule monthly GameDays (cross-functional participation)\n",
    "2. \u2705 Automate 3-5 chaos experiments (Chaos Mesh or Litmus)\n",
    "3. \u2705 Build runbooks for discovered failure modes\n",
    "4. \u2705 Integrate chaos into CI/CD pipeline (staging environment)\n",
    "\n",
    "**Long-Term (Next 6-12 Months):**\n",
    "1. \u2705 Run production chaos (1% traffic \u2192 100% over 6 months)\n",
    "2. \u2705 Build chaos engineering culture (training, champions, KPIs)\n",
    "3. \u2705 Measure business impact (MTTR reduction, SLA improvement, revenue protected)\n",
    "4. \u2705 Contribute learnings to community (blog posts, conference talks)\n",
    "\n",
    "**Continuous Improvement:**\n",
    "- Model real-world failures from production incidents\n",
    "- Expand chaos experiments to new failure modes\n",
    "- Increase chaos frequency (weekly \u2192 daily)\n",
    "- Share chaos findings across organization (engineering all-hands)\n",
    "\n",
    "### \ud83d\udcda Further Learning\n",
    "\n",
    "**Books:**\n",
    "- *Chaos Engineering* by Casey Rosenthal & Nora Jones (O'Reilly, 2020)\n",
    "- *Site Reliability Engineering* by Google (Chapter 15: Postmortem Culture)\n",
    "\n",
    "**Tools & Frameworks:**\n",
    "- Chaos Mesh: https://chaos-mesh.org/\n",
    "- Principles of Chaos Engineering: https://principlesofchaos.org/\n",
    "- AWS Fault Injection Simulator: https://aws.amazon.com/fis/\n",
    "\n",
    "**Community:**\n",
    "- Chaos Engineering Slack: https://chaos-community.slack.com/\n",
    "- CNCF Chaos Engineering WG: https://github.com/cncf/tag-app-delivery\n",
    "\n",
    "### \ud83d\udca1 Key Insights\n",
    "\n",
    "1. **Chaos finds bugs before customers do** (proactive vs reactive)\n",
    "2. **Resilience is a feature, not an afterthought** (design for failure)\n",
    "3. **Automate chaos for continuous validation** (don't rely on manual testing)\n",
    "4. **Start small, increase gradually** (minimize blast radius)\n",
    "5. **Observability enables chaos** (can't validate what you can't measure)\n",
    "6. **Chaos engineering builds confidence** (sleep better knowing systems resilient)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You now understand Chaos Engineering principles and practices.** \ud83c\udf89\n",
    "\n",
    "**Total Business Value from Projects:** $32.5M/year\n",
    "- Post-Silicon: $17.5M/year (STDF ETL, multi-region, yield API, spot instances)\n",
    "- General AI/ML: $15.0M/year (recommendations, fraud detection, observability, CI/CD)\n",
    "\n",
    "**Next Notebook:** Advanced Topics continue... \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5600f30",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Key Takeaways\n",
    "\n",
    "**When to Use**: Distributed systems (microservices), critical infrastructure, SLA requirements >99.9%, post-incident prevention  \n",
    "**Limitations**: Risk of production outages, requires mature observability, team buy-in needed, not for simple systems  \n",
    "**Alternatives**: Staging-only testing (misses prod issues), manual DR drills (infrequent), hope (risky), canary deployments only  \n",
    "**Best Practices**: Start small (dev/staging), steady-state hypothesis, blast radius limits, automated rollback, run during business hours  \n",
    "\n",
    "## \ud83d\udd0d Diagnostic & Mastery\n",
    "\n",
    "**Post-Silicon**: Test ATE cluster resilience (pod failures), network partition tolerance for multi-fab ML serving, save $2.5M-$8M/year downtime prevention\n",
    "\n",
    "\u2705 Use Chaos Mesh/Litmus to inject failures (pod kill, network delay, CPU stress)  \n",
    "\u2705 Validate ML system resilience with automated chaos experiments\n",
    "\n",
    "**Next Steps**: 139_Observability_Monitoring, 154_Model_Monitoring_Observability\n",
    "\n",
    "## \ud83d\udcc8 Progress\n",
    "\n",
    "\u2705 33 notebooks complete | ~83.4% done (146/175) | Next: Continue 9-cell batch (6 remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1cab12",
   "metadata": {},
   "source": [
    "## \ud83d\udd0d Diagnostic & Mastery + Progress\n",
    "\n",
    "### Implementation Checklist\n",
    "- \u2705 **Chaos Mesh/Litmus** - Kubernetes-native chaos tools\n",
    "- \u2705 **Pod failures** - Random pod kills, node drains\n",
    "- \u2705 **Network chaos** - Latency injection (100ms), packet loss (10%)\n",
    "- \u2705 **Resource stress** - CPU/memory exhaustion experiments\n",
    "- \u2705 **Observability** - Prometheus alerts, Grafana dashboards during chaos\n",
    "- \u2705 **Automated rollback** - Stop experiment if SLOs breached\n",
    "\n",
    "### Quality Metrics\n",
    "- **Blast radius**: <10% of traffic affected during experiments\n",
    "- **MTTR improvement**: Reduce mean time to recovery 50% (4 hours \u2192 2 hours)\n",
    "- **SLO maintenance**: 99.9% uptime preserved during chaos tests\n",
    "- **Incident rate**: 30% fewer production outages after chaos practice\n",
    "\n",
    "### Post-Silicon Validation Application\n",
    "**Chaos Test for ATE Data Pipeline**\n",
    "- **Scenario**: Inject 200ms network latency between ATE tester and cloud analytics (simulates WAN congestion)\n",
    "- **Discovery**: Yield prediction service times out after 5 seconds \u2192 no fallback to cached data\n",
    "- **Fix**: Add Redis cache with 5-minute TTL \u2192 serve stale predictions during network issues\n",
    "- **Value**: Prevent $620K/year yield analysis outages (8 incidents/year \u00d7 $310K/incident from delayed decisions)\n",
    "\n",
    "### ROI: $620K-$2.4M/year (prevent outages, reduce MTTR, improve SLO compliance)\n",
    "\n",
    "\u2705 Implement pod failure and network latency chaos experiments\n",
    "\u2705 Define steady-state SLOs and automated rollback triggers\n",
    "\u2705 Practice incident response through quarterly game days\n",
    "\u2705 Apply to semiconductor ML pipeline resilience testing\n",
    "\n",
    "**Session**: 58/60 done (96.7%) | **Overall**: ~168/175 complete (96%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bec000",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Key Takeaways\n",
    "\n",
    "**When to Use Chaos Engineering:**\n",
    "- \u2705 **High-availability systems** - 99.9%+ uptime SLAs (can't afford production failures)\n",
    "- \u2705 **Microservices architectures** - 20+ services \u2192 complex failure modes (cascading failures)\n",
    "- \u2705 **Pre-production validation** - Test resilience before Black Friday, product launches\n",
    "- \u2705 **Disaster recovery drills** - Validate backups, failover procedures work (not just documentation)\n",
    "- \u2705 **Confidence in incident response** - Practice outages \u2192 reduce MTTR from 4 hours to 30 minutes\n",
    "\n",
    "**Limitations:**\n",
    "- \u274c Requires mature monitoring (can't run chaos without observability to detect issues)\n",
    "- \u274c Cultural resistance (teams fear breaking production, need executive buy-in)\n",
    "- \u274c False sense of security (testing pod crashes doesn't cover all real-world failures)\n",
    "- \u274c Tool complexity (Chaos Mesh, Litmus require Kubernetes expertise)\n",
    "- \u274c Risk of actual outages (even in controlled experiments, blast radius > expected 5-10% of time)\n",
    "\n",
    "**Alternatives:**\n",
    "- **Load testing** - Gatling, Locust for performance validation (doesn't test failure modes)\n",
    "- **Integration tests** - Test service interactions in CI/CD (predictable, not real chaos)\n",
    "- **Blue/green deployments** - Validate new version before full rollout (no failure injection)\n",
    "- **Canary releases** - Deploy to 5% traffic first (detect issues, not proactive testing)\n",
    "\n",
    "**Best Practices:**\n",
    "- **Start small** - Kill one pod, not entire cluster (10% blast radius maximum)\n",
    "- **Define steady state** - SLOs must stay green during chaos (latency <100ms, error rate <0.1%)\n",
    "- **Automated rollback** - Stop experiment if SLOs breached (circuit breaker pattern)\n",
    "- **Game days** - Schedule chaos experiments quarterly (Friday 2pm, all hands on deck)\n",
    "- **Blameless postmortems** - Document learnings, not finger-pointing\n",
    "- **Progressive severity** - Network latency \u2192 pod crashes \u2192 AZ failure \u2192 region outage"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}