{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 152: Advanced Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deffcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "from enum import Enum\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# sklearn for models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "print(\"\ud83d\udce6 Imports complete!\")\n",
    "print(\"\\n\ud83d\udd27 Production Model Serving Stack:\")\n",
    "print(\"   - Seldon Core: Kubernetes-native model serving\")\n",
    "print(\"   - BentoML: Model serving framework\")\n",
    "print(\"   - TorchServe: PyTorch model serving\")\n",
    "print(\"   - TensorFlow Serving: TensorFlow model serving\")\n",
    "print(\"   - Ray Serve: Distributed model serving\")\n",
    "print(\"   - KServe: Kubernetes model serving (formerly KFServing)\")\n",
    "print(\"\\n\u2705 Environment ready!\")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afdac8b",
   "metadata": {},
   "source": [
    "## 2. \ud83e\uddea A/B Testing - Statistical Model Comparison\n",
    "\n",
    "**Purpose:** Build A/B testing framework to compare Champion (current production model) vs Challenger (new model) with statistical significance testing.\n",
    "\n",
    "**Key Points:**\n",
    "- **Traffic Splitting**: Random 50/50 split (or 90/10 for safety) between Champion and Challenger\n",
    "- **Metrics Collection**: Track RMSE, MAE, R\u00b2, latency for both models on same traffic\n",
    "- **Statistical Testing**: Use t-test or Mann-Whitney U test to determine if Challenger is significantly better\n",
    "- **Decision Rule**: Promote Challenger if p-value < 0.05 AND mean metric improvement > threshold (e.g., 5% better RMSE)\n",
    "- **Sample Size**: Need sufficient samples (~1000+) for statistical power\n",
    "\n",
    "**Why for Post-Silicon?**\n",
    "- **Prevent Bad Deployments**: Reject Challenger if RMSE=2.3% vs Champion RMSE=1.8% (statistically worse)\n",
    "- **Confidence**: 95% confidence that new model is better before full deployment\n",
    "- **Business Impact**: Avoid $8.3M/year in bad decisions from deploying inferior models\n",
    "- **Audit Trail**: Statistical proof for regulatory compliance (FDA, automotive safety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbefcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B Testing System\n",
    "\n",
    "@dataclass\n",
    "class ModelVariant:\n",
    "    \"\"\"Model variant for A/B testing\"\"\"\n",
    "    name: str\n",
    "    model: Any\n",
    "    predictions: List[float] = field(default_factory=list)\n",
    "    errors: List[float] = field(default_factory=list)\n",
    "    latencies_ms: List[float] = field(default_factory=list)\n",
    "    traffic_share: float = 0.5\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"Make prediction and measure latency\"\"\"\n",
    "        start = time.time()\n",
    "        pred = self.model.predict(X)\n",
    "        latency_ms = (time.time() - start) * 1000\n",
    "        return pred, latency_ms\n",
    "    \n",
    "    def log_prediction(self, y_true: float, y_pred: float, latency_ms: float):\n",
    "        \"\"\"Log prediction result\"\"\"\n",
    "        self.predictions.append(y_pred)\n",
    "        self.errors.append(abs(y_true - y_pred))\n",
    "        self.latencies_ms.append(latency_ms)\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"Compute performance metrics\"\"\"\n",
    "        if not self.errors:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            'mean_error': np.mean(self.errors),\n",
    "            'std_error': np.std(self.errors),\n",
    "            'median_error': np.median(self.errors),\n",
    "            'p95_latency_ms': np.percentile(self.latencies_ms, 95),\n",
    "            'mean_latency_ms': np.mean(self.latencies_ms),\n",
    "            'sample_size': len(self.errors)\n",
    "        }\n",
    "\n",
    "class ABTest:\n",
    "    \"\"\"A/B testing framework (like Optimizely, LaunchDarkly for ML)\"\"\"\n",
    "    \n",
    "    def __init__(self, champion: ModelVariant, challenger: ModelVariant,\n",
    "                 test_name: str = \"ab_test\"):\n",
    "        self.champion = champion\n",
    "        self.challenger = challenger\n",
    "        self.test_name = test_name\n",
    "        self.started_at = datetime.now()\n",
    "        self.completed_at: Optional[datetime] = None\n",
    "        \n",
    "    def route_request(self) -> ModelVariant:\n",
    "        \"\"\"Route request to Champion or Challenger based on traffic split\"\"\"\n",
    "        # Random assignment (in production: use consistent hashing for user stickiness)\n",
    "        if np.random.rand() < self.champion.traffic_share:\n",
    "            return self.champion\n",
    "        else:\n",
    "            return self.challenger\n",
    "    \n",
    "    def serve_request(self, X: np.ndarray, y_true: float):\n",
    "        \"\"\"Serve single request through A/B test\"\"\"\n",
    "        variant = self.route_request()\n",
    "        y_pred, latency_ms = variant.predict(X)\n",
    "        variant.log_prediction(y_true, y_pred[0], latency_ms)\n",
    "        \n",
    "        return {\n",
    "            'variant': variant.name,\n",
    "            'prediction': y_pred[0],\n",
    "            'latency_ms': latency_ms\n",
    "        }\n",
    "    \n",
    "    def statistical_test(self, metric: str = 'mean_error') -> Dict[str, Any]:\n",
    "        \"\"\"Perform statistical significance test (t-test)\"\"\"\n",
    "        champion_metric = self.champion.errors if metric == 'mean_error' else self.champion.latencies_ms\n",
    "        challenger_metric = self.challenger.errors if metric == 'mean_error' else self.challenger.latencies_ms\n",
    "        \n",
    "        if len(champion_metric) < 30 or len(challenger_metric) < 30:\n",
    "            return {\n",
    "                'test': 't-test',\n",
    "                'metric': metric,\n",
    "                'significant': False,\n",
    "                'reason': 'Insufficient sample size (need 30+ per variant)'\n",
    "            }\n",
    "        \n",
    "        # Simple t-test (in production: use scipy.stats.ttest_ind)\n",
    "        champion_mean = np.mean(champion_metric)\n",
    "        challenger_mean = np.mean(challenger_metric)\n",
    "        \n",
    "        champion_std = np.std(champion_metric)\n",
    "        challenger_std = np.std(challenger_metric)\n",
    "        \n",
    "        n1 = len(champion_metric)\n",
    "        n2 = len(challenger_metric)\n",
    "        \n",
    "        # Pooled standard error\n",
    "        se = np.sqrt((champion_std**2 / n1) + (challenger_std**2 / n2))\n",
    "        \n",
    "        # t-statistic\n",
    "        if se > 0:\n",
    "            t_stat = (champion_mean - challenger_mean) / se\n",
    "        else:\n",
    "            t_stat = 0\n",
    "        \n",
    "        # Degrees of freedom (Welch's approximation)\n",
    "        if champion_std > 0 and challenger_std > 0:\n",
    "            df = ((champion_std**2/n1 + challenger_std**2/n2)**2) / \\\n",
    "                 ((champion_std**2/n1)**2/(n1-1) + (challenger_std**2/n2)**2/(n2-1))\n",
    "        else:\n",
    "            df = n1 + n2 - 2\n",
    "        \n",
    "        # Approximate p-value (simplified, use scipy.stats in production)\n",
    "        # For |t| > 2.0, p < 0.05 (rough approximation)\n",
    "        p_value_approx = 0.01 if abs(t_stat) > 2.58 else \\\n",
    "                        0.05 if abs(t_stat) > 1.96 else \\\n",
    "                        0.10 if abs(t_stat) > 1.65 else 0.50\n",
    "        \n",
    "        # Improvement percentage\n",
    "        improvement_pct = ((champion_mean - challenger_mean) / champion_mean) * 100\n",
    "        \n",
    "        # Decision: Significant if p < 0.05 AND improvement > 5%\n",
    "        is_significant = p_value_approx < 0.05\n",
    "        is_better = improvement_pct > 5.0  # Challenger has lower error\n",
    "        \n",
    "        return {\n",
    "            'test': 't-test',\n",
    "            'metric': metric,\n",
    "            'champion_mean': champion_mean,\n",
    "            'challenger_mean': challenger_mean,\n",
    "            'champion_std': champion_std,\n",
    "            'challenger_std': challenger_std,\n",
    "            't_statistic': t_stat,\n",
    "            'df': df,\n",
    "            'p_value_approx': p_value_approx,\n",
    "            'improvement_pct': improvement_pct,\n",
    "            'significant': is_significant,\n",
    "            'better': is_better,\n",
    "            'decision': 'PROMOTE' if (is_significant and is_better) else 'REJECT'\n",
    "        }\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get A/B test summary\"\"\"\n",
    "        champion_metrics = self.champion.get_metrics()\n",
    "        challenger_metrics = self.challenger.get_metrics()\n",
    "        \n",
    "        stat_test = self.statistical_test('mean_error')\n",
    "        \n",
    "        return {\n",
    "            'test_name': self.test_name,\n",
    "            'started_at': self.started_at,\n",
    "            'duration': (datetime.now() - self.started_at).total_seconds(),\n",
    "            'champion': champion_metrics,\n",
    "            'challenger': challenger_metrics,\n",
    "            'statistical_test': stat_test,\n",
    "            'recommendation': stat_test['decision']\n",
    "        }\n",
    "\n",
    "# Example: A/B Test for Yield Prediction Models\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"A/B Testing - Champion vs Challenger Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Generate synthetic wafer test data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "X = np.random.randn(n_samples, 5)\n",
    "X[:, 0] = X[:, 0] * 0.05 + 1.0  # Vdd (around 1.0V)\n",
    "X[:, 1] = X[:, 1] * 0.1 + 0.5   # Idd (around 0.5A)\n",
    "X[:, 2] = X[:, 2] * 50 + 1000   # Frequency (around 1000 MHz)\n",
    "X[:, 3] = X[:, 3] * 5 + 25      # Temperature (around 25\u00b0C)\n",
    "X[:, 4] = (np.random.rand(n_samples) * 150 + 50).astype(int)  # Dies tested\n",
    "\n",
    "# True yield relationship\n",
    "y_true = (50 + 30 * X[:, 0] + 20 * X[:, 1] - 0.1 * X[:, 2] + \n",
    "          2 * X[:, 3] + 0.05 * X[:, 4] + np.random.randn(n_samples) * 1.5)\n",
    "\n",
    "# Train Champion model (current production model - simpler)\n",
    "champion_model = LinearRegression()\n",
    "champion_model.fit(X[:800], y_true[:800])\n",
    "\n",
    "# Train Challenger model (new model - more complex)\n",
    "challenger_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "challenger_model.fit(X[:800], y_true[:800])\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Models Trained:\")\n",
    "print(f\"   Champion: LinearRegression (current production)\")\n",
    "print(f\"   Challenger: RandomForest(n_estimators=100, max_depth=10)\")\n",
    "\n",
    "# Create A/B test\n",
    "champion_variant = ModelVariant(name=\"Champion\", model=champion_model, traffic_share=0.5)\n",
    "challenger_variant = ModelVariant(name=\"Challenger\", model=challenger_model, traffic_share=0.5)\n",
    "\n",
    "ab_test = ABTest(\n",
    "    champion=champion_variant,\n",
    "    challenger=challenger_variant,\n",
    "    test_name=\"YieldPrediction_v2.0_vs_v1.8\"\n",
    ")\n",
    "\n",
    "print(f\"\\n\ud83e\uddea A/B Test Started: {ab_test.test_name}\")\n",
    "print(f\"   Traffic Split: {champion_variant.traffic_share*100:.0f}% Champion / {challenger_variant.traffic_share*100:.0f}% Challenger\")\n",
    "\n",
    "# Serve test traffic (simulate 200 requests)\n",
    "test_requests = 200\n",
    "print(f\"\\n\ud83d\ude80 Serving {test_requests} requests through A/B test...\")\n",
    "\n",
    "for i in range(test_requests):\n",
    "    X_request = X[800 + i].reshape(1, -1)\n",
    "    y_request_true = y_true[800 + i]\n",
    "    \n",
    "    result = ab_test.serve_request(X_request, y_request_true)\n",
    "    \n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"   Progress: {i + 1}/{test_requests} requests served\")\n",
    "\n",
    "print(f\"\u2705 Test traffic completed!\")\n",
    "\n",
    "# Analyze results\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"A/B Test Results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = ab_test.get_summary()\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Champion Metrics:\")\n",
    "for metric, value in summary['champion'].items():\n",
    "    print(f\"   {metric}: {value:.3f}\" if isinstance(value, float) else f\"   {metric}: {value}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Challenger Metrics:\")\n",
    "for metric, value in summary['challenger'].items():\n",
    "    print(f\"   {metric}: {value:.3f}\" if isinstance(value, float) else f\"   {metric}: {value}\")\n",
    "\n",
    "# Statistical test results\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Statistical Significance Test\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "stat_test = summary['statistical_test']\n",
    "\n",
    "print(f\"\\n\ud83e\uddee T-Test Results:\")\n",
    "print(f\"   Champion Mean Error: {stat_test['champion_mean']:.3f}%\")\n",
    "print(f\"   Challenger Mean Error: {stat_test['challenger_mean']:.3f}%\")\n",
    "print(f\"   Improvement: {stat_test['improvement_pct']:.1f}%\")\n",
    "print(f\"   t-statistic: {stat_test['t_statistic']:.3f}\")\n",
    "print(f\"   p-value (approx): {stat_test['p_value_approx']:.3f}\")\n",
    "print(f\"   Degrees of freedom: {stat_test['df']:.1f}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Decision Criteria:\")\n",
    "print(f\"   Statistical significance (p < 0.05): {'\u2705 YES' if stat_test['significant'] else '\u274c NO'}\")\n",
    "print(f\"   Practical significance (>5% improvement): {'\u2705 YES' if stat_test['better'] else '\u274c NO'}\")\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf DECISION: {stat_test['decision']}\")\n",
    "\n",
    "if stat_test['decision'] == 'PROMOTE':\n",
    "    print(f\"   \u2705 Challenger is statistically AND practically better\")\n",
    "    print(f\"   \u2705 Promote Challenger to canary deployment (10% traffic)\")\n",
    "else:\n",
    "    print(f\"   \u274c Challenger does not meet promotion criteria\")\n",
    "    print(f\"   \u274c Keep Champion in production, reject Challenger\")\n",
    "\n",
    "# Business value\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Business Value\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate business impact\n",
    "wafers_per_day = 500\n",
    "days_per_year = 365\n",
    "wafers_per_year = wafers_per_day * days_per_year\n",
    "\n",
    "# Error impact (1% yield error = $50K per wafer)\n",
    "error_cost_per_pct = 50000\n",
    "champion_annual_error_cost = stat_test['champion_mean'] * wafers_per_year * error_cost_per_pct / 100\n",
    "challenger_annual_error_cost = stat_test['challenger_mean'] * wafers_per_year * error_cost_per_pct / 100\n",
    "\n",
    "annual_savings = champion_annual_error_cost - challenger_annual_error_cost\n",
    "\n",
    "print(f\"\\n\ud83d\udcb0 Business Impact:\")\n",
    "print(f\"   Wafers per year: {wafers_per_year:,}\")\n",
    "print(f\"   Error cost: ${error_cost_per_pct:,} per 1% yield error per wafer\")\n",
    "print(f\"\\n   Champion annual error cost: ${champion_annual_error_cost / 1e6:.1f}M\")\n",
    "print(f\"   Challenger annual error cost: ${challenger_annual_error_cost / 1e6:.1f}M\")\n",
    "print(f\"\\n   Annual savings: ${annual_savings / 1e6:.1f}M\")\n",
    "\n",
    "if stat_test['decision'] == 'PROMOTE':\n",
    "    print(f\"\\n   \u2705 Promoting Challenger saves ${annual_savings / 1e6:.1f}M/year\")\n",
    "else:\n",
    "    prevented_loss = abs(annual_savings) if annual_savings < 0 else 0\n",
    "    print(f\"\\n   \u2705 A/B test prevented ${prevented_loss / 1e6:.1f}M/year loss from bad model deployment\")\n",
    "\n",
    "print(f\"\\n\u2705 A/B test validated!\")\n",
    "print(f\"\u2705 {test_requests} requests served ({champion_variant.get_metrics()['sample_size']} Champion, {challenger_variant.get_metrics()['sample_size']} Challenger)\")\n",
    "print(f\"\u2705 Statistical decision: {stat_test['decision']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813da51",
   "metadata": {},
   "source": [
    "## 3. \ud83d\udc24 Canary Deployment - Gradual Traffic Shifting\n",
    "\n",
    "**Purpose:** Gradually shift traffic from old model to new model (10% \u2192 25% \u2192 50% \u2192 100%) with automated rollback if metrics degrade.\n",
    "\n",
    "**Key Points:**\n",
    "- **Gradual Rollout**: Start with 10% traffic to new model, increase if metrics OK\n",
    "- **Health Checks**: Monitor RMSE, latency, error rate at each stage\n",
    "- **Auto-Rollback**: If RMSE increases >20% or latency >2x, rollback to previous model\n",
    "- **Rollback Speed**: <10 seconds to shift 100% traffic back to old model\n",
    "- **Manual Approval**: Optional manual approval before 100% rollout (for critical systems)\n",
    "\n",
    "**Why for Post-Silicon?**\n",
    "- **Risk Mitigation**: Detect issues early (10% of wafers affected vs 100%)\n",
    "- **Fast Recovery**: Rollback in <10 seconds if yield drops (vs hours of manual intervention)\n",
    "- **Business Safety**: Limit blast radius to 10% \u2192 prevent $5.6M/year losses\n",
    "- **Confidence Building**: Gradual validation builds confidence in new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b50874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canary Deployment System\n",
    "\n",
    "class DeploymentStage(Enum):\n",
    "    \"\"\"Canary deployment stages\"\"\"\n",
    "    CANARY_10 = \"10%\"\n",
    "    CANARY_25 = \"25%\"\n",
    "    CANARY_50 = \"50%\"\n",
    "    FULL_100 = \"100%\"\n",
    "    ROLLBACK = \"Rollback\"\n",
    "\n",
    "@dataclass\n",
    "class HealthCheck:\n",
    "    \"\"\"Health check result for canary deployment\"\"\"\n",
    "    stage: DeploymentStage\n",
    "    metrics: Dict[str, float]\n",
    "    baseline_metrics: Dict[str, float]\n",
    "    passed: bool\n",
    "    reason: str\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "class CanaryDeployment:\n",
    "    \"\"\"Canary deployment system (like Flagger, Argo Rollouts)\"\"\"\n",
    "    \n",
    "    def __init__(self, old_model: Any, new_model: Any,\n",
    "                 baseline_rmse: float, baseline_latency_ms: float):\n",
    "        self.old_model = old_model\n",
    "        self.new_model = new_model\n",
    "        self.baseline_rmse = baseline_rmse\n",
    "        self.baseline_latency_ms = baseline_latency_ms\n",
    "        \n",
    "        self.current_stage = DeploymentStage.CANARY_10\n",
    "        self.new_model_traffic = 0.10  # Start at 10%\n",
    "        \n",
    "        self.health_checks: List[HealthCheck] = []\n",
    "        self.new_model_predictions: List[float] = []\n",
    "        self.new_model_errors: List[float] = []\n",
    "        self.new_model_latencies: List[float] = []\n",
    "        \n",
    "        self.deployment_started = datetime.now()\n",
    "        self.deployment_completed: Optional[datetime] = None\n",
    "        self.rollback_executed = False\n",
    "    \n",
    "    def route_request(self) -> str:\n",
    "        \"\"\"Route request to old or new model\"\"\"\n",
    "        if np.random.rand() < self.new_model_traffic:\n",
    "            return \"new\"\n",
    "        return \"old\"\n",
    "    \n",
    "    def serve_request(self, X: np.ndarray, y_true: float) -> Dict[str, Any]:\n",
    "        \"\"\"Serve request through canary deployment\"\"\"\n",
    "        model_version = self.route_request()\n",
    "        \n",
    "        if model_version == \"new\":\n",
    "            start = time.time()\n",
    "            y_pred = self.new_model.predict(X)[0]\n",
    "            latency_ms = (time.time() - start) * 1000\n",
    "            \n",
    "            self.new_model_predictions.append(y_pred)\n",
    "            self.new_model_errors.append(abs(y_true - y_pred))\n",
    "            self.new_model_latencies.append(latency_ms)\n",
    "            \n",
    "            return {\n",
    "                'model': 'new',\n",
    "                'prediction': y_pred,\n",
    "                'latency_ms': latency_ms,\n",
    "                'traffic_share': self.new_model_traffic\n",
    "            }\n",
    "        else:\n",
    "            start = time.time()\n",
    "            y_pred = self.old_model.predict(X)[0]\n",
    "            latency_ms = (time.time() - start) * 1000\n",
    "            \n",
    "            return {\n",
    "                'model': 'old',\n",
    "                'prediction': y_pred,\n",
    "                'latency_ms': latency_ms,\n",
    "                'traffic_share': 1.0 - self.new_model_traffic\n",
    "            }\n",
    "    \n",
    "    def check_health(self, min_samples: int = 20) -> HealthCheck:\n",
    "        \"\"\"Check if new model is healthy at current traffic level\"\"\"\n",
    "        if len(self.new_model_errors) < min_samples:\n",
    "            return HealthCheck(\n",
    "                stage=self.current_stage,\n",
    "                metrics={},\n",
    "                baseline_metrics={},\n",
    "                passed=False,\n",
    "                reason=f\"Insufficient samples ({len(self.new_model_errors)} < {min_samples})\"\n",
    "            )\n",
    "        \n",
    "        # Compute new model metrics\n",
    "        new_rmse = np.sqrt(np.mean(np.array(self.new_model_errors)**2))\n",
    "        new_latency_p95 = np.percentile(self.new_model_latencies, 95)\n",
    "        \n",
    "        metrics = {\n",
    "            'rmse': new_rmse,\n",
    "            'latency_p95_ms': new_latency_p95,\n",
    "            'sample_size': len(self.new_model_errors)\n",
    "        }\n",
    "        \n",
    "        baseline_metrics = {\n",
    "            'rmse': self.baseline_rmse,\n",
    "            'latency_p95_ms': self.baseline_latency_ms\n",
    "        }\n",
    "        \n",
    "        # Health check criteria\n",
    "        rmse_degradation = ((new_rmse - self.baseline_rmse) / self.baseline_rmse) * 100\n",
    "        latency_degradation = ((new_latency_p95 - self.baseline_latency_ms) / self.baseline_latency_ms) * 100\n",
    "        \n",
    "        # Rollback if RMSE >20% worse OR latency >100% worse\n",
    "        if rmse_degradation > 20:\n",
    "            return HealthCheck(\n",
    "                stage=self.current_stage,\n",
    "                metrics=metrics,\n",
    "                baseline_metrics=baseline_metrics,\n",
    "                passed=False,\n",
    "                reason=f\"RMSE degraded by {rmse_degradation:.1f}% (threshold: 20%)\"\n",
    "            )\n",
    "        \n",
    "        if latency_degradation > 100:\n",
    "            return HealthCheck(\n",
    "                stage=self.current_stage,\n",
    "                metrics=metrics,\n",
    "                baseline_metrics=baseline_metrics,\n",
    "                passed=False,\n",
    "                reason=f\"Latency degraded by {latency_degradation:.1f}% (threshold: 100%)\"\n",
    "            )\n",
    "        \n",
    "        return HealthCheck(\n",
    "            stage=self.current_stage,\n",
    "            metrics=metrics,\n",
    "            baseline_metrics=baseline_metrics,\n",
    "            passed=True,\n",
    "            reason=\"All health checks passed\"\n",
    "        )\n",
    "    \n",
    "    def progress_deployment(self) -> bool:\n",
    "        \"\"\"Progress to next deployment stage or rollback\"\"\"\n",
    "        health_check = self.check_health()\n",
    "        self.health_checks.append(health_check)\n",
    "        \n",
    "        if not health_check.passed:\n",
    "            # Rollback!\n",
    "            print(f\"\\n   \u274c Health check FAILED: {health_check.reason}\")\n",
    "            print(f\"   \ud83d\udd04 Rolling back to old model...\")\n",
    "            self.rollback()\n",
    "            return False\n",
    "        \n",
    "        # Health check passed, progress to next stage\n",
    "        print(f\"   \u2705 Health check passed at {self.current_stage.value} traffic\")\n",
    "        \n",
    "        # Progress stages\n",
    "        if self.current_stage == DeploymentStage.CANARY_10:\n",
    "            self.current_stage = DeploymentStage.CANARY_25\n",
    "            self.new_model_traffic = 0.25\n",
    "            print(f\"   \u2b06\ufe0f  Progressing to {self.current_stage.value} traffic\")\n",
    "        elif self.current_stage == DeploymentStage.CANARY_25:\n",
    "            self.current_stage = DeploymentStage.CANARY_50\n",
    "            self.new_model_traffic = 0.50\n",
    "            print(f\"   \u2b06\ufe0f  Progressing to {self.current_stage.value} traffic\")\n",
    "        elif self.current_stage == DeploymentStage.CANARY_50:\n",
    "            self.current_stage = DeploymentStage.FULL_100\n",
    "            self.new_model_traffic = 1.00\n",
    "            print(f\"   \u2b06\ufe0f  Progressing to {self.current_stage.value} traffic\")\n",
    "            print(f\"   \ud83c\udf89 Canary deployment COMPLETE!\")\n",
    "            self.deployment_completed = datetime.now()\n",
    "        \n",
    "        # Reset metrics for next stage\n",
    "        self.new_model_predictions = []\n",
    "        self.new_model_errors = []\n",
    "        self.new_model_latencies = []\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def rollback(self):\n",
    "        \"\"\"Rollback to old model\"\"\"\n",
    "        self.current_stage = DeploymentStage.ROLLBACK\n",
    "        self.new_model_traffic = 0.0\n",
    "        self.rollback_executed = True\n",
    "        print(f\"   \u2705 Rollback complete (100% traffic to old model)\")\n",
    "    \n",
    "    def get_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get deployment status\"\"\"\n",
    "        return {\n",
    "            'stage': self.current_stage.value,\n",
    "            'new_model_traffic': self.new_model_traffic,\n",
    "            'health_checks': len(self.health_checks),\n",
    "            'health_checks_passed': sum(1 for hc in self.health_checks if hc.passed),\n",
    "            'rollback_executed': self.rollback_executed,\n",
    "            'deployment_time': (datetime.now() - self.deployment_started).total_seconds()\n",
    "        }\n",
    "\n",
    "# Example: Canary Deployment for Test Time Optimization Model\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Canary Deployment - Gradual Traffic Shifting\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use models from A/B test (Champion = old, Challenger = new)\n",
    "old_model = champion_model\n",
    "new_model = challenger_model\n",
    "\n",
    "# Baseline metrics from A/B test\n",
    "baseline_rmse = stat_test['champion_mean']\n",
    "baseline_latency_ms = 0.5  # Assume 0.5ms baseline\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Baseline Metrics (Old Model):\")\n",
    "print(f\"   RMSE: {baseline_rmse:.3f}%\")\n",
    "print(f\"   P95 Latency: {baseline_latency_ms:.2f}ms\")\n",
    "\n",
    "# Create canary deployment\n",
    "canary = CanaryDeployment(\n",
    "    old_model=old_model,\n",
    "    new_model=new_model,\n",
    "    baseline_rmse=baseline_rmse,\n",
    "    baseline_latency_ms=baseline_latency_ms\n",
    ")\n",
    "\n",
    "print(f\"\\n\ud83d\udc24 Canary Deployment Started\")\n",
    "print(f\"   Initial stage: {canary.current_stage.value} traffic to new model\")\n",
    "\n",
    "# Simulate deployment stages\n",
    "stages_to_simulate = [\n",
    "    (DeploymentStage.CANARY_10, 50),   # 50 requests at 10%\n",
    "    (DeploymentStage.CANARY_25, 50),   # 50 requests at 25%\n",
    "    (DeploymentStage.CANARY_50, 50),   # 50 requests at 50%\n",
    "    (DeploymentStage.FULL_100, 50)     # 50 requests at 100%\n",
    "]\n",
    "\n",
    "request_idx = 800\n",
    "\n",
    "for stage, num_requests in stages_to_simulate:\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Stage: {stage.value} Traffic to New Model\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\n\ud83d\ude80 Serving {num_requests} requests at {canary.new_model_traffic*100:.0f}% traffic...\")\n",
    "    \n",
    "    for i in range(num_requests):\n",
    "        X_request = X[request_idx].reshape(1, -1)\n",
    "        y_request_true = y_true[request_idx]\n",
    "        \n",
    "        result = canary.serve_request(X_request, y_request_true)\n",
    "        request_idx += 1\n",
    "    \n",
    "    print(f\"   \u2705 {num_requests} requests served\")\n",
    "    print(f\"   New model served: {len(canary.new_model_predictions)} requests\")\n",
    "    \n",
    "    # Health check\n",
    "    print(f\"\\n\ud83c\udfe5 Running health check...\")\n",
    "    \n",
    "    if not canary.progress_deployment():\n",
    "        break  # Rollback executed\n",
    "    \n",
    "    if canary.deployment_completed:\n",
    "        break  # Deployment complete\n",
    "\n",
    "# Deployment summary\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Canary Deployment Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "status = canary.get_status()\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Deployment Status:\")\n",
    "print(f\"   Final stage: {status['stage']}\")\n",
    "print(f\"   New model traffic: {status['new_model_traffic']*100:.0f}%\")\n",
    "print(f\"   Total health checks: {status['health_checks']}\")\n",
    "print(f\"   Health checks passed: {status['health_checks_passed']}\")\n",
    "print(f\"   Rollback executed: {'\u2705 YES' if status['rollback_executed'] else '\u274c NO'}\")\n",
    "print(f\"   Deployment time: {status['deployment_time']:.1f} seconds\")\n",
    "\n",
    "# Health check history\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Health Check History\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n{'Stage':<15} {'RMSE':<10} {'Baseline':<12} {'Degradation':<15} {'Status':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for hc in canary.health_checks:\n",
    "    if hc.metrics:\n",
    "        rmse = hc.metrics['rmse']\n",
    "        baseline = hc.baseline_metrics['rmse']\n",
    "        degradation = ((rmse - baseline) / baseline) * 100\n",
    "        status_icon = \"\u2705 PASS\" if hc.passed else \"\u274c FAIL\"\n",
    "        \n",
    "        print(f\"{hc.stage.value:<15} {rmse:<10.3f} {baseline:<12.3f} {degradation:<15.1f}% {status_icon:<10}\")\n",
    "\n",
    "# Business value\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Business Value\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not canary.rollback_executed:\n",
    "    # Successful deployment\n",
    "    wafers_per_day = 500\n",
    "    test_time_reduction_min = 1.0  # 1 minute saved per wafer\n",
    "    cost_per_minute = 100  # USD (tester time cost)\n",
    "    \n",
    "    daily_savings = wafers_per_day * test_time_reduction_min * cost_per_minute\n",
    "    annual_savings = daily_savings * 365\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcb0 Test Time Optimization Value:\")\n",
    "    print(f\"   Wafers per day: {wafers_per_day}\")\n",
    "    print(f\"   Test time reduction: {test_time_reduction_min} min/wafer\")\n",
    "    print(f\"   Cost per minute: ${cost_per_minute}\")\n",
    "    print(f\"   Daily savings: ${daily_savings:,}\")\n",
    "    print(f\"   Annual savings: ${annual_savings / 1e6:.1f}M\")\n",
    "else:\n",
    "    # Rollback prevented bad deployment\n",
    "    wafers_affected_pct = 0.10  # Only 10% affected (canary stage)\n",
    "    wafers_per_year = 500 * 365\n",
    "    cost_per_bad_wafer = 50000\n",
    "    \n",
    "    prevented_loss = wafers_per_year * wafers_affected_pct * cost_per_bad_wafer\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcb0 Canary Rollback Prevented Loss:\")\n",
    "    print(f\"   Wafers affected: {wafers_affected_pct*100:.0f}% (canary stage)\")\n",
    "    print(f\"   Wafers per year: {wafers_per_year:,}\")\n",
    "    print(f\"   Cost per bad wafer: ${cost_per_bad_wafer:,}\")\n",
    "    print(f\"   Prevented loss: ${prevented_loss / 1e6:.1f}M\")\n",
    "    print(f\"\\n   \u2705 Canary deployment caught bad model early!\")\n",
    "    print(f\"   \u2705 Rollback limited blast radius to 10% of wafers\")\n",
    "\n",
    "print(f\"\\n\u2705 Canary deployment validated!\")\n",
    "print(f\"\u2705 {len(canary.health_checks)} health checks performed\")\n",
    "print(f\"\u2705 Rollback capability tested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bf0a81",
   "metadata": {},
   "source": [
    "## 4. \ud83c\udfb0 Multi-Armed Bandits - Automated Model Selection\n",
    "\n",
    "**Purpose:** Automatically allocate traffic to best-performing model while exploring alternatives, balancing exploitation (use best model) vs exploration (try other models).\n",
    "\n",
    "**Key Points:**\n",
    "- **\u03b5-Greedy**: Exploit best model with probability (1-\u03b5), explore random model with probability \u03b5\n",
    "- **Upper Confidence Bound (UCB)**: Select model with highest upper confidence bound (mean + uncertainty bonus)\n",
    "- **Thompson Sampling**: Bayesian approach, sample from posterior distribution, select model with highest sample\n",
    "- **Regret Minimization**: Goal is to minimize regret (cumulative difference vs always using optimal model)\n",
    "- **Online Learning**: Adapt in real-time as new data arrives (no batch retraining needed)\n",
    "\n",
    "**Why for Post-Silicon?**\n",
    "- **Automated Optimization**: No manual A/B test setup, bandit automatically finds best model per wafer fab\n",
    "- **Continuous Adaptation**: If Fab A patterns change, bandit automatically shifts to better model\n",
    "- **Multi-Context**: Different models excel in different scenarios (Fab A \u2192 XGBoost, Fab B \u2192 Random Forest)\n",
    "- **Business Value**: 15% better accuracy than single-model approach \u2192 $6.8M/year in improved yield predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Armed Bandit System\n",
    "\n",
    "@dataclass\n",
    "class BanditArm:\n",
    "    \"\"\"Model arm in multi-armed bandit\"\"\"\n",
    "    name: str\n",
    "    model: Any\n",
    "    pulls: int = 0  # Number of times selected\n",
    "    total_reward: float = 0.0  # Cumulative reward (negative error)\n",
    "    rewards: List[float] = field(default_factory=list)\n",
    "    \n",
    "    def get_mean_reward(self) -> float:\n",
    "        \"\"\"Get average reward\"\"\"\n",
    "        return self.total_reward / self.pulls if self.pulls > 0 else 0.0\n",
    "    \n",
    "    def update(self, reward: float):\n",
    "        \"\"\"Update arm statistics\"\"\"\n",
    "        self.pulls += 1\n",
    "        self.total_reward += reward\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "class ThompsonSamplingBandit:\n",
    "    \"\"\"Thompson Sampling bandit for model selection (Bayesian approach)\"\"\"\n",
    "    \n",
    "    def __init__(self, arms: List[BanditArm]):\n",
    "        self.arms = arms\n",
    "        self.total_pulls = 0\n",
    "        self.regret_history: List[float] = []\n",
    "        \n",
    "        # Beta distribution parameters for each arm (assume rewards in [0, 1])\n",
    "        self.alpha = [1.0] * len(arms)  # Successes + 1\n",
    "        self.beta = [1.0] * len(arms)   # Failures + 1\n",
    "    \n",
    "    def select_arm(self) -> int:\n",
    "        \"\"\"Select arm using Thompson Sampling\"\"\"\n",
    "        # Sample from Beta distribution for each arm\n",
    "        samples = [np.random.beta(self.alpha[i], self.beta[i]) \n",
    "                  for i in range(len(self.arms))]\n",
    "        \n",
    "        # Select arm with highest sample\n",
    "        return int(np.argmax(samples))\n",
    "    \n",
    "    def update(self, arm_idx: int, reward: float):\n",
    "        \"\"\"Update arm and Beta parameters\"\"\"\n",
    "        # Update arm statistics\n",
    "        self.arms[arm_idx].update(reward)\n",
    "        \n",
    "        # Update Beta parameters (assuming reward in [0, 1])\n",
    "        # reward=1 \u2192 success, reward=0 \u2192 failure\n",
    "        self.alpha[arm_idx] += reward\n",
    "        self.beta[arm_idx] += (1.0 - reward)\n",
    "        \n",
    "        self.total_pulls += 1\n",
    "    \n",
    "    def get_best_arm(self) -> int:\n",
    "        \"\"\"Get arm with highest mean reward\"\"\"\n",
    "        mean_rewards = [arm.get_mean_reward() for arm in self.arms]\n",
    "        return int(np.argmax(mean_rewards))\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get bandit statistics\"\"\"\n",
    "        stats = {\n",
    "            'total_pulls': self.total_pulls,\n",
    "            'arms': []\n",
    "        }\n",
    "        \n",
    "        for i, arm in enumerate(self.arms):\n",
    "            arm_stats = {\n",
    "                'name': arm.name,\n",
    "                'pulls': arm.pulls,\n",
    "                'selection_rate': arm.pulls / self.total_pulls if self.total_pulls > 0 else 0,\n",
    "                'mean_reward': arm.get_mean_reward(),\n",
    "                'total_reward': arm.total_reward\n",
    "            }\n",
    "            stats['arms'].append(arm_stats)\n",
    "        \n",
    "        return stats\n",
    "\n",
    "class UCBBandit:\n",
    "    \"\"\"Upper Confidence Bound bandit for model selection\"\"\"\n",
    "    \n",
    "    def __init__(self, arms: List[BanditArm], c: float = 2.0):\n",
    "        self.arms = arms\n",
    "        self.c = c  # Exploration parameter\n",
    "        self.total_pulls = 0\n",
    "    \n",
    "    def select_arm(self) -> int:\n",
    "        \"\"\"Select arm using UCB\"\"\"\n",
    "        # Explore arms with 0 pulls first\n",
    "        for i, arm in enumerate(self.arms):\n",
    "            if arm.pulls == 0:\n",
    "                return i\n",
    "        \n",
    "        # Compute UCB for each arm\n",
    "        ucb_values = []\n",
    "        for arm in self.arms:\n",
    "            mean_reward = arm.get_mean_reward()\n",
    "            exploration_bonus = self.c * np.sqrt(np.log(self.total_pulls) / arm.pulls)\n",
    "            ucb = mean_reward + exploration_bonus\n",
    "            ucb_values.append(ucb)\n",
    "        \n",
    "        return int(np.argmax(ucb_values))\n",
    "    \n",
    "    def update(self, arm_idx: int, reward: float):\n",
    "        \"\"\"Update arm statistics\"\"\"\n",
    "        self.arms[arm_idx].update(reward)\n",
    "        self.total_pulls += 1\n",
    "    \n",
    "    def get_best_arm(self) -> int:\n",
    "        \"\"\"Get arm with highest mean reward\"\"\"\n",
    "        mean_rewards = [arm.get_mean_reward() for arm in self.arms]\n",
    "        return int(np.argmax(mean_rewards))\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get bandit statistics\"\"\"\n",
    "        stats = {\n",
    "            'total_pulls': self.total_pulls,\n",
    "            'exploration_parameter': self.c,\n",
    "            'arms': []\n",
    "        }\n",
    "        \n",
    "        for arm in self.arms:\n",
    "            arm_stats = {\n",
    "                'name': arm.name,\n",
    "                'pulls': arm.pulls,\n",
    "                'selection_rate': arm.pulls / self.total_pulls if self.total_pulls > 0 else 0,\n",
    "                'mean_reward': arm.get_mean_reward(),\n",
    "                'total_reward': arm.total_reward\n",
    "            }\n",
    "            stats['arms'].append(arm_stats)\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Example: Multi-Armed Bandit for Multi-Model Selection\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Multi-Armed Bandit - Automated Model Selection\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create 3 models with different characteristics\n",
    "model_1 = LinearRegression()\n",
    "model_1.fit(X[:800], y_true[:800])\n",
    "\n",
    "model_2 = RandomForestRegressor(n_estimators=50, max_depth=5, random_state=42)\n",
    "model_2.fit(X[:800], y_true[:800])\n",
    "\n",
    "model_3 = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "model_3.fit(X[:800], y_true[:800])\n",
    "\n",
    "print(f\"\\n\ud83c\udfb0 3 Model Arms Available:\")\n",
    "print(f\"   Model 1: LinearRegression (fast, simple)\")\n",
    "print(f\"   Model 2: RandomForest(n=50, depth=5) (medium complexity)\")\n",
    "print(f\"   Model 3: RandomForest(n=100, depth=10) (high complexity)\")\n",
    "\n",
    "# Create bandit arms\n",
    "arms_thompson = [\n",
    "    BanditArm(name=\"Model_1_Linear\", model=model_1),\n",
    "    BanditArm(name=\"Model_2_RF_Simple\", model=model_2),\n",
    "    BanditArm(name=\"Model_3_RF_Complex\", model=model_3)\n",
    "]\n",
    "\n",
    "arms_ucb = [\n",
    "    BanditArm(name=\"Model_1_Linear\", model=model_1),\n",
    "    BanditArm(name=\"Model_2_RF_Simple\", model=model_2),\n",
    "    BanditArm(name=\"Model_3_RF_Complex\", model=model_3)\n",
    "]\n",
    "\n",
    "# Create bandits\n",
    "thompson_bandit = ThompsonSamplingBandit(arms=arms_thompson)\n",
    "ucb_bandit = UCBBandit(arms=arms_ucb, c=2.0)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Bandit Algorithms:\")\n",
    "print(f\"   1. Thompson Sampling (Bayesian)\")\n",
    "print(f\"   2. UCB (Upper Confidence Bound, c=2.0)\")\n",
    "\n",
    "# Simulate 300 requests\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Thompson Sampling Bandit - 300 Requests\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "n_requests = 300\n",
    "\n",
    "print(f\"\\n\ud83d\ude80 Serving {n_requests} requests...\")\n",
    "\n",
    "for i in range(n_requests):\n",
    "    # Thompson Sampling\n",
    "    arm_idx = thompson_bandit.select_arm()\n",
    "    arm = thompson_bandit.arms[arm_idx]\n",
    "    \n",
    "    # Make prediction\n",
    "    X_request = X[800 + i].reshape(1, -1)\n",
    "    y_request_true = y_true[800 + i]\n",
    "    y_pred = arm.model.predict(X_request)[0]\n",
    "    \n",
    "    # Reward = 1 - normalized_error (higher is better, in [0, 1])\n",
    "    error = abs(y_request_true - y_pred)\n",
    "    normalized_error = min(error / 10.0, 1.0)  # Normalize to [0, 1]\n",
    "    reward = 1.0 - normalized_error\n",
    "    \n",
    "    thompson_bandit.update(arm_idx, reward)\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"   Progress: {i + 1}/{n_requests} requests\")\n",
    "\n",
    "print(f\"\u2705 Thompson Sampling completed!\")\n",
    "\n",
    "# Thompson Sampling Results\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Thompson Sampling Results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "thompson_stats = thompson_bandit.get_statistics()\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Arm Selection Statistics:\")\n",
    "print(f\"\\n{'Arm':<25} {'Pulls':<10} {'Selection %':<15} {'Mean Reward':<15} {'Total Reward':<15}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for arm_stat in thompson_stats['arms']:\n",
    "    print(f\"{arm_stat['name']:<25} {arm_stat['pulls']:<10} {arm_stat['selection_rate']*100:<15.1f} \"\n",
    "          f\"{arm_stat['mean_reward']:<15.3f} {arm_stat['total_reward']:<15.1f}\")\n",
    "\n",
    "best_arm_idx = thompson_bandit.get_best_arm()\n",
    "best_arm_name = thompson_stats['arms'][best_arm_idx]['name']\n",
    "\n",
    "print(f\"\\n\ud83c\udfc6 Best Arm: {best_arm_name}\")\n",
    "print(f\"   Mean Reward: {thompson_stats['arms'][best_arm_idx]['mean_reward']:.3f}\")\n",
    "print(f\"   Selection Rate: {thompson_stats['arms'][best_arm_idx]['selection_rate']*100:.1f}%\")\n",
    "\n",
    "# UCB Bandit\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"UCB Bandit - 300 Requests\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n\ud83d\ude80 Serving {n_requests} requests...\")\n",
    "\n",
    "for i in range(n_requests):\n",
    "    # UCB\n",
    "    arm_idx = ucb_bandit.select_arm()\n",
    "    arm = ucb_bandit.arms[arm_idx]\n",
    "    \n",
    "    # Make prediction\n",
    "    X_request = X[800 + i].reshape(1, -1)\n",
    "    y_request_true = y_true[800 + i]\n",
    "    y_pred = arm.model.predict(X_request)[0]\n",
    "    \n",
    "    # Reward = 1 - normalized_error\n",
    "    error = abs(y_request_true - y_pred)\n",
    "    normalized_error = min(error / 10.0, 1.0)\n",
    "    reward = 1.0 - normalized_error\n",
    "    \n",
    "    ucb_bandit.update(arm_idx, reward)\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"   Progress: {i + 1}/{n_requests} requests\")\n",
    "\n",
    "print(f\"\u2705 UCB completed!\")\n",
    "\n",
    "# UCB Results\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"UCB Results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ucb_stats = ucb_bandit.get_statistics()\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Arm Selection Statistics:\")\n",
    "print(f\"\\n{'Arm':<25} {'Pulls':<10} {'Selection %':<15} {'Mean Reward':<15} {'Total Reward':<15}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for arm_stat in ucb_stats['arms']:\n",
    "    print(f\"{arm_stat['name']:<25} {arm_stat['pulls']:<10} {arm_stat['selection_rate']*100:<15.1f} \"\n",
    "          f\"{arm_stat['mean_reward']:<15.3f} {arm_stat['total_reward']:<15.1f}\")\n",
    "\n",
    "best_arm_idx_ucb = ucb_bandit.get_best_arm()\n",
    "best_arm_name_ucb = ucb_stats['arms'][best_arm_idx_ucb]['name']\n",
    "\n",
    "print(f\"\\n\ud83c\udfc6 Best Arm: {best_arm_name_ucb}\")\n",
    "print(f\"   Mean Reward: {ucb_stats['arms'][best_arm_idx_ucb]['mean_reward']:.3f}\")\n",
    "print(f\"   Selection Rate: {ucb_stats['arms'][best_arm_idx_ucb]['selection_rate']*100:.1f}%\")\n",
    "\n",
    "# Comparison\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Thompson Sampling vs UCB Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "thompson_total_reward = sum(arm_stat['total_reward'] for arm_stat in thompson_stats['arms'])\n",
    "ucb_total_reward = sum(arm_stat['total_reward'] for arm_stat in ucb_stats['arms'])\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Total Cumulative Reward:\")\n",
    "print(f\"   Thompson Sampling: {thompson_total_reward:.1f}\")\n",
    "print(f\"   UCB: {ucb_total_reward:.1f}\")\n",
    "print(f\"   Winner: {'Thompson Sampling' if thompson_total_reward > ucb_total_reward else 'UCB'}\")\n",
    "\n",
    "# Business value\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Business Value\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Multi-model ensemble via bandits\n",
    "baseline_error = 1.8  # Single model baseline RMSE\n",
    "bandit_improvement = 0.15  # 15% better accuracy\n",
    "bandit_error = baseline_error * (1 - bandit_improvement)\n",
    "\n",
    "wafers_per_year = 500 * 365\n",
    "error_cost_per_pct = 50000\n",
    "\n",
    "baseline_annual_cost = baseline_error * wafers_per_year * error_cost_per_pct / 100\n",
    "bandit_annual_cost = bandit_error * wafers_per_year * error_cost_per_pct / 100\n",
    "\n",
    "annual_savings = baseline_annual_cost - bandit_annual_cost\n",
    "\n",
    "print(f\"\\n\ud83d\udcb0 Multi-Armed Bandit Value:\")\n",
    "print(f\"   Baseline single model RMSE: {baseline_error:.2f}%\")\n",
    "print(f\"   Bandit multi-model RMSE: {bandit_error:.2f}% (15% improvement)\")\n",
    "print(f\"   Wafers per year: {wafers_per_year:,}\")\n",
    "print(f\"   Error cost: ${error_cost_per_pct:,} per 1% per wafer\")\n",
    "print(f\"\\n   Baseline annual error cost: ${baseline_annual_cost / 1e6:.1f}M\")\n",
    "print(f\"   Bandit annual error cost: ${bandit_annual_cost / 1e6:.1f}M\")\n",
    "print(f\"\\n   Annual savings: ${annual_savings / 1e6:.1f}M\")\n",
    "\n",
    "print(f\"\\n\u2705 Multi-armed bandit validated!\")\n",
    "print(f\"\u2705 {n_requests} requests served per algorithm\")\n",
    "print(f\"\u2705 Best model auto-selected: {best_arm_name}\")\n",
    "print(f\"\u2705 ${annual_savings / 1e6:.1f}M/year business value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a49e96",
   "metadata": {},
   "source": [
    "## 5. \ud83d\ude80 Real-World Advanced Serving Projects\n",
    "\n",
    "Each project includes clear objectives, business value, and implementation guidance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Post-Silicon Validation Projects** ($24.9M/year total value)\n",
    "\n",
    "#### **Project 1: Multi-Stage Canary Deployment for Binning Models** ($8.3M/year)\n",
    "**Objective:** Build 4-stage canary deployment (10% \u2192 25% \u2192 50% \u2192 100%) for device binning model with automated rollback if binning accuracy drops >2%.\n",
    "\n",
    "**Business Value:** Prevent $8.3M/year revenue loss from incorrect binning (Premium devices binned as Standard) by catching bad models at 10% stage.\n",
    "\n",
    "**Features:**\n",
    "- 4-stage progressive rollout (10% \u2192 25% \u2192 50% \u2192 100%)\n",
    "- Health checks: Binning accuracy, revenue per wafer, Premium bin %\n",
    "- Auto-rollback if accuracy <98% or revenue drops >5%\n",
    "- Manual approval gate before 100% rollout\n",
    "- Rollback in <5 seconds (instant traffic shift)\n",
    "\n",
    "**Tech Stack:** Kubernetes, Istio (traffic splitting), Prometheus (metrics), Grafana (dashboards), PagerDuty (alerts)\n",
    "\n",
    "**Success Metrics:**\n",
    "- Rollback time <5 seconds\n",
    "- 99% of bad models caught at 10-25% stage\n",
    "- Zero revenue loss from bad deployments\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 2: A/B Testing Framework for Test Time Optimization** ($5.6M/year)\n",
    "**Objective:** Build A/B testing platform to compare test time optimization models (skip unnecessary tests) with statistical confidence before deployment.\n",
    "\n",
    "**Business Value:** Reduce test time by 25% (4 min \u2192 3 min per wafer) while maintaining yield confidence, saving $5.6M/year in tester time costs.\n",
    "\n",
    "**Features:**\n",
    "- 50/50 traffic split (Champion vs Challenger)\n",
    "- Metrics: Test time, yield correlation, false negative rate\n",
    "- Statistical test: t-test with 95% confidence, minimum 1000 samples\n",
    "- Decision rule: Promote if test time <75% AND yield correlation >99%\n",
    "- Automated experiment tracking (MLflow integration)\n",
    "\n",
    "**Tech Stack:** Python, MLflow, PostgreSQL, scipy.stats (statistical tests), Kubernetes\n",
    "\n",
    "**Success Metrics:**\n",
    "- Statistical confidence: p-value <0.05\n",
    "- Test time reduction: 25% (1 minute saved per wafer)\n",
    "- Zero yield loss from skipped tests\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 3: Thompson Sampling Bandit for Multi-Fab Model Selection** ($6.8M/year)\n",
    "**Objective:** Deploy Thompson Sampling bandit to automatically select best yield prediction model per wafer fab (5 fabs, 3 models each).\n",
    "\n",
    "**Business Value:** 15% better accuracy than single-model approach by adapting to fab-specific patterns, improving yield predictions worth $6.8M/year.\n",
    "\n",
    "**Features:**\n",
    "- 3 model arms per fab (Linear, Random Forest, Neural Net)\n",
    "- Thompson Sampling with Beta priors (Bayesian approach)\n",
    "- Context-aware selection (fab ID, product type, date)\n",
    "- Real-time adaptation (reacts to drift in <100 requests)\n",
    "- Exploration vs exploitation balance (automatic)\n",
    "\n",
    "**Tech Stack:** Python, Redis (arm statistics), Kubernetes, MLflow, Bayesian libraries\n",
    "\n",
    "**Success Metrics:**\n",
    "- Best model auto-selected per fab within 500 requests\n",
    "- 15% accuracy improvement vs single model\n",
    "- <1ms model selection latency\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 4: Shadow Deployment for Outlier Detection Models** ($4.2M/year)\n",
    "**Objective:** Deploy new outlier detection model in shadow mode (predictions logged, no impact on production) for 1 week validation before promotion.\n",
    "\n",
    "**Business Value:** Validate new model risk-free, preventing $4.2M/year false positive costs (good devices flagged as outliers).\n",
    "\n",
    "**Features:**\n",
    "- Shadow predictions logged for all production traffic\n",
    "- Comparison metrics: Precision, recall, F1, false positive rate\n",
    "- Side-by-side comparison after 1 week (10K+ samples)\n",
    "- Automated promotion if precision >95% AND false positive rate <1%\n",
    "- Zero production impact during validation\n",
    "\n",
    "**Tech Stack:** Python, Kafka (prediction logging), Elasticsearch (log storage), Kibana (visualization), MLflow\n",
    "\n",
    "**Success Metrics:**\n",
    "- 1 week shadow validation (10K+ samples)\n",
    "- Zero production impact\n",
    "- False positive rate <1% before promotion\n",
    "\n",
    "---\n",
    "\n",
    "### **General AI/ML Projects** ($33.6M/year total value)\n",
    "\n",
    "#### **Project 5: Recommendation System A/B Testing with Contextual Bandits** ($9.8M/year)\n",
    "**Objective:** Build A/B testing + contextual bandit hybrid for e-commerce recommendations (10M+ users), testing 5 models with automatic traffic allocation.\n",
    "\n",
    "**Business Value:** 22% conversion rate improvement via contextual bandits (adapt to user segments), driving $9.8M/year additional revenue.\n",
    "\n",
    "**Features:**\n",
    "- Contextual features (user demographics, browsing history, time of day)\n",
    "- 5 recommendation models (collaborative filtering, content-based, hybrid, neural, LLM)\n",
    "- Contextual Thompson Sampling (separate bandits per user segment)\n",
    "- Real-time model selection (<10ms overhead)\n",
    "- A/B test for initial validation, then bandit for optimization\n",
    "\n",
    "**Tech Stack:** Python, Redis (context + arm stats), Ray Serve (model serving), Kafka, PostgreSQL\n",
    "\n",
    "**Success Metrics:**\n",
    "- Contextual model selection <10ms\n",
    "- 22% conversion improvement\n",
    "- 1M+ decisions per day\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 6: Canary Deployment for Fraud Detection Models** ($8.4M/year)\n",
    "**Objective:** 5-stage canary deployment (5% \u2192 10% \u2192 25% \u2192 50% \u2192 100%) for fraud detection model with <1 minute rollback if false positive rate spikes.\n",
    "\n",
    "**Business Value:** Prevent $8.4M/year customer churn from false positives (legitimate transactions blocked) by catching bad models at 5% stage.\n",
    "\n",
    "**Features:**\n",
    "- 5-stage ultra-safe rollout (financial transactions are high stakes)\n",
    "- Metrics: Fraud detection rate, false positive rate, transaction value\n",
    "- Auto-rollback if false positive rate >0.5% OR fraud detection rate <85%\n",
    "- Real-time monitoring (1-minute windows)\n",
    "- Incident response integration (PagerDuty)\n",
    "\n",
    "**Tech Stack:** Kubernetes, Istio, Prometheus, Grafana, PagerDuty, Python\n",
    "\n",
    "**Success Metrics:**\n",
    "- Rollback time <1 minute\n",
    "- False positive rate <0.5%\n",
    "- Fraud detection rate >85%\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 7: Multi-Armed Bandit for Ad Placement Optimization** ($7.2M/year)\n",
    "**Objective:** Deploy UCB bandit to optimize ad placement (10 ad slots, 50 advertisers) with real-time bidding integration.\n",
    "\n",
    "**Business Value:** 35% higher click-through rate via bandit optimization (vs random placement), increasing ad revenue by $7.2M/year.\n",
    "\n",
    "**Features:**\n",
    "- UCB algorithm with exploration parameter c=2.0\n",
    "- Context: User demographics, page content, time of day\n",
    "- Real-time bidding integration (select highest UCB ad per request)\n",
    "- Reward: Click-through rate (CTR) + revenue per click\n",
    "- Automated A/B test every month (bandit vs random baseline)\n",
    "\n",
    "**Tech Stack:** Python, Redis, Ray Serve, Kafka, PostgreSQL, Prometheus\n",
    "\n",
    "**Success Metrics:**\n",
    "- Real-time ad selection <5ms\n",
    "- 35% CTR improvement\n",
    "- $7.2M/year incremental revenue\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 8: Shadow Deployment for Medical Diagnosis Model (HIPAA-Compliant)** ($8.2M/year)\n",
    "**Objective:** Deploy new chest X-ray diagnosis model in shadow mode with encrypted prediction logging, comparing to radiologist diagnoses for 6 months before FDA submission.\n",
    "\n",
    "**Business Value:** Accelerate FDA approval by 6 months via comprehensive shadow validation (10K+ cases), enabling $8.2M/year earlier market entry.\n",
    "\n",
    "**Features:**\n",
    "- HIPAA-compliant shadow predictions (encrypted, access-controlled)\n",
    "- Side-by-side comparison (model vs radiologist)\n",
    "- Metrics: Accuracy, sensitivity, specificity, AUC-ROC\n",
    "- Bias analysis (demographics, imaging equipment)\n",
    "- Regulatory report generation (FDA submission ready)\n",
    "\n",
    "**Tech Stack:** Python, AWS S3 (encrypted), PostgreSQL (encrypted), Vault (secrets), MLflow, CloudWatch\n",
    "\n",
    "**Success Metrics:**\n",
    "- 6 months shadow validation (10K+ cases)\n",
    "- Model accuracy \u2265 radiologist accuracy\n",
    "- HIPAA compliance (zero violations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118dd47",
   "metadata": {},
   "source": [
    "## 6. \ud83c\udfaf Key Takeaways\n",
    "\n",
    "### **Deployment Strategy Selection Guide**\n",
    "\n",
    "**When to Use Each Strategy:**\n",
    "\n",
    "| Strategy | Risk Level | Use When | Rollback Time | Best For |\n",
    "|----------|-----------|----------|---------------|----------|\n",
    "| **Shadow** | Zero | Brand new model, unproven algorithm | N/A (no prod impact) | High-stakes systems (medical, financial) |\n",
    "| **A/B Test** | Low | Need statistical proof of improvement | Instant (traffic shift) | Competing models, regulatory compliance |\n",
    "| **Canary** | Low-Medium | Gradual confidence building needed | <10 seconds | Production deployments, new features |\n",
    "| **Blue-Green** | Medium | Need instant switchover capability | <1 second (atomic) | Zero-downtime requirements |\n",
    "| **Bandit** | Low | Multiple models, need auto-optimization | Continuous adaptation | Multi-context scenarios, personalization |\n",
    "\n",
    "---\n",
    "\n",
    "### **A/B Testing Best Practices**\n",
    "\n",
    "**DO:**\n",
    "- \u2705 **Set minimum sample size** (1000+ samples per variant for statistical power)\n",
    "- \u2705 **Use both statistical AND practical significance** (p<0.05 AND >5% improvement)\n",
    "- \u2705 **Randomize traffic assignment** (avoid selection bias)\n",
    "- \u2705 **Track multiple metrics** (accuracy, latency, throughput, business KPIs)\n",
    "- \u2705 **Run for sufficient duration** (1-2 weeks to capture seasonality)\n",
    "\n",
    "**DON'T:**\n",
    "- \u274c **Peek at results early** (increases false positive rate, wait for planned duration)\n",
    "- \u274c **Ignore latency metrics** (accuracy improvement doesn't matter if latency 10x worse)\n",
    "- \u274c **Use only p-value** (need practical significance too: 0.1% improvement isn't worth deployment)\n",
    "- \u274c **Stop test early if winning** (regression to mean can reverse results)\n",
    "- \u274c **Run multiple tests simultaneously** (interaction effects confound results)\n",
    "\n",
    "**Statistical Testing:**\n",
    "- Use **t-test** for continuous metrics (RMSE, MAE, latency)\n",
    "- Use **Chi-squared test** for categorical metrics (classification accuracy, CTR)\n",
    "- Use **Mann-Whitney U test** if distributions are non-normal\n",
    "- Require **p-value < 0.05** (95% confidence) for production deployment\n",
    "- Consider **Bonferroni correction** if testing multiple metrics (divide \u03b1 by number of tests)\n",
    "\n",
    "---\n",
    "\n",
    "### **Canary Deployment Best Practices**\n",
    "\n",
    "**DO:**\n",
    "- \u2705 **Start small** (5-10% traffic, limit blast radius)\n",
    "- \u2705 **Automate health checks** (every 1-5 minutes, no manual monitoring)\n",
    "- \u2705 **Set clear rollback criteria** (RMSE >20% worse, latency >2x, error rate >1%)\n",
    "- \u2705 **Progress gradually** (10% \u2192 25% \u2192 50% \u2192 100%, validate at each stage)\n",
    "- \u2705 **Test rollback capability** (practice rollbacks in staging environment)\n",
    "\n",
    "**DON'T:**\n",
    "- \u274c **Skip stages** (jumping from 10% to 100% defeats the purpose)\n",
    "- \u274c **Ignore latency spikes** (2x latency can crash production even if accuracy is good)\n",
    "- \u274c **Deploy without rollback plan** (always have instant rollback capability)\n",
    "- \u274c **Rely on manual health checks** (automate or you'll miss issues)\n",
    "- \u274c **Progress too fast** (wait 10-60 minutes per stage to collect sufficient data)\n",
    "\n",
    "**Health Check Criteria:**\n",
    "- **RMSE degradation**: <20% (if RMSE 1.8% \u2192 2.2% is OK, but 1.8% \u2192 2.5% triggers rollback)\n",
    "- **Latency degradation**: <100% (if P95 latency doubles, rollback immediately)\n",
    "- **Error rate**: <1% (if >1% of requests fail, rollback)\n",
    "- **Throughput**: >90% of baseline (if throughput drops >10%, investigate)\n",
    "- **Memory/CPU**: <150% of baseline (if resource usage spikes, potential memory leak)\n",
    "\n",
    "---\n",
    "\n",
    "### **Multi-Armed Bandit Best Practices**\n",
    "\n",
    "**Algorithm Selection:**\n",
    "\n",
    "**\u03b5-Greedy:**\n",
    "- \u2705 Simple to implement, easy to understand\n",
    "- \u2705 Good for stationary environments (reward distributions don't change)\n",
    "- \u274c Explores randomly (wastes traffic on clearly bad arms)\n",
    "- **Use when**: Quick prototyping, simple scenarios\n",
    "\n",
    "**UCB (Upper Confidence Bound):**\n",
    "- \u2705 Optimistic exploration (focuses on uncertain arms)\n",
    "- \u2705 Provable regret bounds (O(log n))\n",
    "- \u274c Assumes stationary rewards\n",
    "- **Use when**: Need theoretical guarantees, non-Bayesian approach preferred\n",
    "\n",
    "**Thompson Sampling:**\n",
    "- \u2705 Bayesian approach (incorporates prior knowledge)\n",
    "- \u2705 Adapts well to non-stationary environments\n",
    "- \u2705 Often best empirical performance\n",
    "- \u274c More complex implementation\n",
    "- **Use when**: Production systems, need best empirical performance\n",
    "\n",
    "**Contextual Bandits:**\n",
    "- \u2705 Personalized decisions (different arms for different users/contexts)\n",
    "- \u2705 Better performance than context-free bandits\n",
    "- \u274c Requires context features\n",
    "- **Use when**: Personalization needed (recommendations, ads, content)\n",
    "\n",
    "**Best Practices:**\n",
    "- \u2705 **Normalize rewards** to [0, 1] (prevents scale issues)\n",
    "- \u2705 **Monitor exploration rate** (ensure not stuck exploiting one arm)\n",
    "- \u2705 **Track regret** (cumulative difference vs optimal arm)\n",
    "- \u2705 **A/B test bandit vs baseline** (prove bandit is better)\n",
    "- \u274c **Don't use bandits if only 2 arms** (A/B test is simpler and sufficient)\n",
    "\n",
    "---\n",
    "\n",
    "### **Shadow Deployment Best Practices**\n",
    "\n",
    "**DO:**\n",
    "- \u2705 **Log all predictions** (encrypted if sensitive data, HIPAA/GDPR compliant)\n",
    "- \u2705 **Compare after sufficient data** (1 week or 10K+ samples minimum)\n",
    "- \u2705 **Automate promotion criteria** (if precision >95%, auto-promote to canary)\n",
    "- \u2705 **Monitor shadow model latency** (ensure it won't slow production if promoted)\n",
    "- \u2705 **Test with production traffic distribution** (use real traffic, not synthetic)\n",
    "\n",
    "**DON'T:**\n",
    "- \u274c **Affect production decisions** (shadow predictions are for logging only)\n",
    "- \u274c **Run shadow indefinitely** (1-2 weeks is sufficient, then decide)\n",
    "- \u274c **Ignore latency** (shadow model that takes 5 seconds can't serve production)\n",
    "- \u274c **Skip data privacy review** (ensure logging complies with regulations)\n",
    "- \u274c **Forget to clean up** (delete shadow infrastructure after validation)\n",
    "\n",
    "**Use Cases:**\n",
    "- **High-stakes systems**: Medical diagnosis, fraud detection (where mistakes are very costly)\n",
    "- **Unproven algorithms**: New architecture or approach (e.g., Transformer replacing CNN)\n",
    "- **Regulatory compliance**: Need extensive validation data for FDA, auditors\n",
    "- **Legacy system replacement**: Validate new system matches old system before switchover\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Pitfalls and Solutions**\n",
    "\n",
    "**Pitfall 1: Insufficient Sample Size**\n",
    "- **Problem**: A/B test with 100 samples per variant \u2192 unreliable results\n",
    "- **Solution**: Use power analysis to determine required sample size (typically 1000+ per variant)\n",
    "- **Tools**: `scipy.stats.ttest_power()`, online power calculators\n",
    "\n",
    "**Pitfall 2: Ignoring Latency in A/B Tests**\n",
    "- **Problem**: Challenger has 2% better accuracy but 10x higher latency \u2192 prod crashes\n",
    "- **Solution**: Track latency, throughput, CPU, memory as part of A/B test metrics\n",
    "- **Criteria**: Latency must be <2x baseline, even if accuracy improves\n",
    "\n",
    "**Pitfall 3: Canary Progression Too Fast**\n",
    "- **Problem**: Progress from 10% \u2192 100% in 10 minutes \u2192 bad model affects 50% of traffic before rollback\n",
    "- **Solution**: Wait 10-60 minutes per stage, collect 100+ samples for health check\n",
    "- **Automation**: Set minimum dwell time per stage (e.g., 30 min at 10%, 30 min at 25%)\n",
    "\n",
    "**Pitfall 4: Bandit Stuck Exploiting Suboptimal Arm**\n",
    "- **Problem**: Early random choices favor bad arm, bandit never explores better arms\n",
    "- **Solution**: Use optimistic initialization (start all arms with high mean reward estimate)\n",
    "- **Alternative**: Decay exploration parameter over time (start \u03b5=0.3, decay to \u03b5=0.05)\n",
    "\n",
    "**Pitfall 5: Shadow Deployment Causing Production Lag**\n",
    "- **Problem**: Shadow model takes 500ms, slows production responses\n",
    "- **Solution**: Run shadow predictions asynchronously (non-blocking), queue for later processing\n",
    "- **Implementation**: Use Kafka/RabbitMQ to queue shadow predictions\n",
    "\n",
    "**Pitfall 6: A/B Test Segment Bias**\n",
    "- **Problem**: Champion gets daytime traffic (easier), Challenger gets nighttime (harder)\n",
    "- **Solution**: Use consistent hashing (user_id \u2192 variant assignment) or fully randomize\n",
    "- **Validation**: Check variant assignment is 50/50 across all hours, days, user segments\n",
    "\n",
    "**Pitfall 7: Forgetting to Remove Shadow Model**\n",
    "- **Problem**: Shadow model runs for 6 months, consuming compute resources unnecessarily\n",
    "- **Solution**: Set expiration date (auto-delete after 2 weeks), alert if still running\n",
    "- **Automation**: Use Kubernetes TTL or cron job to cleanup shadow deployments\n",
    "\n",
    "---\n",
    "\n",
    "### **Production Checklist**\n",
    "\n",
    "**Before A/B Test Deployment:**\n",
    "- [ ] Sample size calculated (power analysis, 1000+ per variant)\n",
    "- [ ] Metrics defined (accuracy, latency, throughput, business KPIs)\n",
    "- [ ] Traffic split configured (50/50 or 90/10)\n",
    "- [ ] Statistical test method chosen (t-test, Mann-Whitney, Chi-squared)\n",
    "- [ ] Test duration set (1-2 weeks, account for seasonality)\n",
    "- [ ] Promotion criteria defined (p<0.05 AND >5% improvement)\n",
    "- [ ] Monitoring dashboards created (Grafana, real-time metrics)\n",
    "\n",
    "**Before Canary Deployment:**\n",
    "- [ ] Rollback plan tested (practice rollback in staging)\n",
    "- [ ] Health check criteria defined (RMSE, latency, error rate thresholds)\n",
    "- [ ] Automated health checks configured (every 1-5 minutes)\n",
    "- [ ] Stages defined (10% \u2192 25% \u2192 50% \u2192 100%)\n",
    "- [ ] Dwell time per stage set (10-60 minutes)\n",
    "- [ ] Manual approval gates configured (if needed before 100%)\n",
    "- [ ] Alerts configured (Slack, PagerDuty on rollback)\n",
    "\n",
    "**Before Multi-Armed Bandit:**\n",
    "- [ ] Algorithm selected (\u03b5-greedy, UCB, Thompson Sampling)\n",
    "- [ ] Reward metric defined (normalized to [0, 1])\n",
    "- [ ] Arm initialization (optimistic start or cold start strategy)\n",
    "- [ ] Context features identified (if using contextual bandit)\n",
    "- [ ] Exploration budget set (minimum pulls per arm before exploitation)\n",
    "- [ ] Monitoring configured (arm selection rates, regret, total reward)\n",
    "- [ ] A/B test planned (bandit vs random baseline validation)\n",
    "\n",
    "**Before Shadow Deployment:**\n",
    "- [ ] Prediction logging configured (encrypted if sensitive)\n",
    "- [ ] Storage provisioned (ElasticSearch, S3 for prediction logs)\n",
    "- [ ] Comparison metrics defined (precision, recall, F1, latency)\n",
    "- [ ] Validation duration set (1-2 weeks, 10K+ samples)\n",
    "- [ ] Promotion criteria defined (precision >95%, false positive <1%)\n",
    "- [ ] Data privacy compliance verified (HIPAA, GDPR if applicable)\n",
    "- [ ] Auto-cleanup configured (delete after validation period)\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Serving Tools & Technologies**\n",
    "\n",
    "**Traffic Splitting:**\n",
    "- **Istio**: Kubernetes service mesh, traffic splitting, circuit breaking\n",
    "- **NGINX**: Reverse proxy, weighted load balancing\n",
    "- **Envoy**: Cloud-native proxy, advanced traffic management\n",
    "- **AWS App Mesh**: Managed service mesh for AWS\n",
    "- **Traefik**: Modern reverse proxy with dynamic config\n",
    "\n",
    "**A/B Testing Platforms:**\n",
    "- **Optimizely**: Commercial A/B testing platform\n",
    "- **LaunchDarkly**: Feature flags + A/B testing\n",
    "- **Split.io**: Feature delivery + experimentation\n",
    "- **Google Optimize**: Free A/B testing (web focused)\n",
    "- **Custom**: Python + statistical libraries (full control)\n",
    "\n",
    "**Canary Deployment:**\n",
    "- **Flagger**: Kubernetes progressive delivery (canary, blue-green, A/B)\n",
    "- **Argo Rollouts**: Kubernetes progressive delivery with analysis\n",
    "- **Spinnaker**: Multi-cloud continuous delivery\n",
    "- **Harness**: Continuous delivery with canary automation\n",
    "- **Jenkins X**: GitOps with progressive delivery\n",
    "\n",
    "**Multi-Armed Bandits:**\n",
    "- **Vowpal Wabbit**: Fast online learning, contextual bandits\n",
    "- **TensorFlow Agents**: Reinforcement learning (including bandits)\n",
    "- **Ray RLlib**: Scalable reinforcement learning\n",
    "- **Microsoft Decision Service**: Contextual bandit platform\n",
    "- **Custom**: Python + numpy (full control, simple algorithms)\n",
    "\n",
    "**Model Serving:**\n",
    "- **Seldon Core**: Kubernetes-native, supports A/B, canary, bandits\n",
    "- **KServe**: Kubernetes model serving (formerly KFServing)\n",
    "- **BentoML**: Model serving framework, supports canary\n",
    "- **Ray Serve**: Distributed model serving, multi-model support\n",
    "- **TorchServe**: PyTorch serving, A/B testing support\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "**Deepen Your Advanced Serving Knowledge:**\n",
    "1. **Notebook 153**: Feature Stores and Real-Time ML (Feast, streaming features, low-latency serving)\n",
    "2. **Notebook 154**: ML Model Explainability and Debugging (SHAP, LIME, debugging techniques)\n",
    "3. **Notebook 155**: Distributed Training and Hyperparameter Tuning (Ray, Optuna, multi-GPU training)\n",
    "\n",
    "**Build a Portfolio Project:**\n",
    "- Start with **Project 2** (A/B Testing Framework) - easy to build, high impact\n",
    "- Then **Project 3** (Thompson Sampling Bandit) - learn online learning\n",
    "- Finally **Project 1** (Multi-Stage Canary) - tie everything together with production deployment\n",
    "\n",
    "**Learn by Doing:**\n",
    "- Implement A/B test for 2 sklearn models (local simulation)\n",
    "- Build canary deployment with Flask + NGINX (traffic splitting)\n",
    "- Code Thompson Sampling bandit from scratch (Python + numpy)\n",
    "- Deploy shadow model with prediction logging (Docker + ElasticSearch)\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "**Advanced serving enables:**\n",
    "- \ud83c\udfaf **Statistical confidence** (prove new model is better before full deployment)\n",
    "- \ud83d\udc24 **Risk mitigation** (gradual rollout with rollback capability)\n",
    "- \ud83c\udfb0 **Automated optimization** (bandits auto-select best model)\n",
    "- \ud83d\udc7b **Risk-free validation** (shadow deployments have zero production impact)\n",
    "- \ud83d\udcb0 **Business value** ($58.5M/year demonstrated in this notebook)\n",
    "\n",
    "**Strategy Selection:**\n",
    "- **Shadow** \u2192 **A/B Test** \u2192 **Canary** \u2192 **Production** (safest path)\n",
    "- **Bandit** for multi-model scenarios (automated optimization)\n",
    "- **Blue-Green** for instant switchover (zero downtime)\n",
    "\n",
    "**Remember:**\n",
    "- Start simple (A/B test 2 models)\n",
    "- Automate everything (health checks, rollback, progression)\n",
    "- Monitor continuously (metrics, alerts, dashboards)\n",
    "- Practice rollbacks (test in staging before production)\n",
    "\n",
    "---\n",
    "\n",
    "\ud83c\udf89 **Congratulations!** You've mastered advanced model serving patterns (A/B testing, canary deployments, multi-armed bandits, shadow deployments). You're ready to deploy models safely and optimize them automatically in production!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92640b43",
   "metadata": {},
   "source": [
    "## \ud83d\udccb Key Takeaways\n",
    "\n",
    "**When to Use Advanced Model Serving:**\n",
    "- \u2705 **High-traffic ML systems** - 1000s of QPS requiring autoscaling\n",
    "- \u2705 **Multi-model deployments** - Serving multiple versions simultaneously\n",
    "- \u2705 **Real-time inference** - <100ms latency requirements (online predictions)\n",
    "- \u2705 **A/B testing needs** - Traffic splitting for model experimentation\n",
    "\n",
    "**Limitations:**\n",
    "- \u26a0\ufe0f **Infrastructure complexity** - Kubernetes, load balancers, monitoring\n",
    "- \u26a0\ufe0f **Cost overhead** - GPU instances, redundancy for HA ($15K-$50K/month)\n",
    "- \u26a0\ufe0f **Debugging difficulty** - Distributed tracing required for multi-service architectures\n",
    "\n",
    "**Alternatives:**\n",
    "- **Batch inference** - Offline predictions for non-real-time use cases (lower cost)\n",
    "- **Serverless** - AWS Lambda, Azure Functions (good for <1 req/sec, cold start issues)\n",
    "- **Edge deployment** - Deploy models on edge devices (IoT, mobile apps)\n",
    "\n",
    "**Best Practices:**\n",
    "1. **Use model versioning** - Immutable model artifacts with semantic versioning\n",
    "2. **Implement canary deployments** - Route 5-10% traffic to new model first\n",
    "3. **Monitor P95/P99 latency** - Not just average (tail latency matters!)\n",
    "4. **Use batching** - Combine requests for GPU efficiency (2-10x throughput)\n",
    "5. **Set up model warmup** - Pre-load models to avoid cold start latency\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd0d Diagnostic Checks & Mastery Achievement\n",
    "\n",
    "### Post-Silicon Validation Applications\n",
    "\n",
    "**Application 1: Real-Time Wafer Binning Service**\n",
    "- **Challenge**: Classify 5000 dies/wafer into 8 bins in <50ms per die\n",
    "- **Solution**: TorchServe with GPU batching (batch size 32), autoscaling 3-10 pods\n",
    "- **Business Value**: Real-time binning enables immediate sorting decisions\n",
    "- **ROI**: $18M/year (reduce scrap by 12% via faster bad die identification)\n",
    "\n",
    "**Application 2: Multi-Model Yield Prediction Platform**\n",
    "- **Challenge**: Serve 8 different yield models (wafer test, final test, package variants)\n",
    "- **Solution**: KServe with model mesh, traffic routing by product family\n",
    "- **Business Value**: Consolidated platform reduces operational complexity\n",
    "- **ROI**: $2.5M/year (infrastructure consolidation, 40% fewer DevOps resources)\n",
    "\n",
    "**Application 3: A/B Testing for Anomaly Detection Models**\n",
    "- **Challenge**: Test new LOF algorithm vs. current Isolation Forest for outlier detection\n",
    "- **Solution**: Istio traffic split (90% old, 10% new), compare false positive rates\n",
    "- **Business Value**: Data-driven model selection reduces false alarms by 25%\n",
    "- **ROI**: $4.2M/year (reduce unnecessary equipment downtime from false alerts)\n",
    "\n",
    "### Mastery Self-Assessment\n",
    "- [ ] Can deploy models with TorchServe/TensorFlow Serving/ONNX Runtime\n",
    "- [ ] Understand autoscaling strategies (HPA, KEDA with custom metrics)\n",
    "- [ ] Implemented canary deployments with traffic splitting\n",
    "- [ ] Know how to optimize GPU batching for throughput vs. latency\n",
    "- [ ] Can set up distributed tracing (Jaeger/Zipkin) for inference debugging\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Progress Update\n",
    "\n",
    "**Session Achievement**: Notebook 152_Advanced_Model_Serving expanded from 9 to 12 cells (80% to target 15 cells)\n",
    "\n",
    "**Overall Progress**: 148 of 175 notebooks complete (84.6% \u2192 100% target)\n",
    "\n",
    "**Current Batch**: 9-cell notebooks - 6 of 10 processed\n",
    "\n",
    "**Estimated Remaining**: 27 notebooks to expand for complete mastery coverage \ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}