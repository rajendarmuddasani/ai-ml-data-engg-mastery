{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 151: MLOps Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7353530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Installation\n",
    "\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import hashlib\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Dict, Set, Optional, Any, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# MLOps simulation (educational implementation)\n",
    "# In production: pip install mlflow wandb dvc\n",
    "\n",
    "print(\"\u2705 MLOps Development Environment Ready\")\n",
    "print(\"\ud83d\udce6 Core libraries loaded\")\n",
    "print(\"\ud83c\udfaf Ready to build production ML pipelines\")\n",
    "print(\"\\n\ud83d\udca1 Production MLOps Stack:\")\n",
    "print(\"   pip install mlflow  # Experiment tracking, model registry\")\n",
    "print(\"   pip install wandb   # Weights & Biases (experiment tracking)\")\n",
    "print(\"   pip install dvc     # Data Version Control\")\n",
    "print(\"   pip install kubeflow  # Kubernetes-native ML workflows\")\n",
    "print(\"   pip install seldon-core  # Model serving on Kubernetes\")\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92eabd6",
   "metadata": {},
   "source": [
    "## 2. \ud83d\udcca Experiment Tracking - Log Hyperparameters, Metrics, and Artifacts\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Build an experiment tracking system to log all ML experiments (hyperparameters, metrics, artifacts) for reproducibility and comparison.\n",
    "\n",
    "**Key Points:**\n",
    "- **Experiment:** Single model training run with specific hyperparameters\n",
    "- **Run Metadata:** Log parameters (n_estimators, max_depth), metrics (RMSE, MAE, R\u00b2)\n",
    "- **Artifacts:** Store model files, feature importance plots, training data snapshots\n",
    "- **Reproducibility:** Exact parameters logged \u2192 can recreate any experiment\n",
    "- **Comparison:** Compare experiments side-by-side (find best hyperparameters)\n",
    "\n",
    "**What to Track:**\n",
    "- **Parameters:** All hyperparameters (learning_rate, batch_size, architecture)\n",
    "- **Metrics:** Training and validation metrics per epoch\n",
    "- **Artifacts:** Model binaries, plots, data samples, predictions\n",
    "- **Environment:** Code version (git SHA), library versions, system info\n",
    "- **Lineage:** Parent models, data sources, feature engineering code\n",
    "\n",
    "**Why This Matters for Post-Silicon:**\n",
    "- **Model Selection:** 20 experiments \u2192 find best yield prediction model\n",
    "- **Audit Trail:** Show regulators exact model used for production decision\n",
    "- **Debugging:** Model accuracy dropped \u2192 compare to previous successful runs\n",
    "- **Knowledge Sharing:** Team sees what's been tried, avoid duplicate work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744a7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Tracking System\n",
    "\n",
    "@dataclass\n",
    "class Experiment:\n",
    "    \"\"\"ML experiment with parameters, metrics, and artifacts\"\"\"\n",
    "    experiment_id: str\n",
    "    name: str\n",
    "    parameters: Dict[str, Any]\n",
    "    metrics: Dict[str, float] = field(default_factory=dict)\n",
    "    artifacts: Dict[str, Any] = field(default_factory=dict)\n",
    "    start_time: datetime = field(default_factory=datetime.now)\n",
    "    end_time: Optional[datetime] = None\n",
    "    status: str = \"running\"  # running, completed, failed\n",
    "    code_version: Optional[str] = None\n",
    "    \n",
    "    def log_metric(self, key: str, value: float):\n",
    "        \"\"\"Log a metric\"\"\"\n",
    "        self.metrics[key] = value\n",
    "    \n",
    "    def log_artifact(self, key: str, value: Any):\n",
    "        \"\"\"Log an artifact (model, plot, data)\"\"\"\n",
    "        self.artifacts[key] = value\n",
    "    \n",
    "    def complete(self):\n",
    "        \"\"\"Mark experiment as completed\"\"\"\n",
    "        self.end_time = datetime.now()\n",
    "        self.status = \"completed\"\n",
    "    \n",
    "    def get_duration(self) -> float:\n",
    "        \"\"\"Get experiment duration in seconds\"\"\"\n",
    "        if self.end_time:\n",
    "            return (self.end_time - self.start_time).total_seconds()\n",
    "        return (datetime.now() - self.start_time).total_seconds()\n",
    "\n",
    "class ExperimentTracker:\n",
    "    \"\"\"Experiment tracking system (like MLflow)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.experiments: Dict[str, Experiment] = {}\n",
    "        self.experiment_counter = 0\n",
    "    \n",
    "    def create_experiment(self, name: str, parameters: Dict[str, Any], \n",
    "                         code_version: Optional[str] = None) -> Experiment:\n",
    "        \"\"\"Create new experiment\"\"\"\n",
    "        self.experiment_counter += 1\n",
    "        experiment_id = f\"exp_{self.experiment_counter:04d}\"\n",
    "        \n",
    "        experiment = Experiment(\n",
    "            experiment_id=experiment_id,\n",
    "            name=name,\n",
    "            parameters=parameters.copy(),\n",
    "            code_version=code_version\n",
    "        )\n",
    "        \n",
    "        self.experiments[experiment_id] = experiment\n",
    "        return experiment\n",
    "    \n",
    "    def get_experiment(self, experiment_id: str) -> Optional[Experiment]:\n",
    "        \"\"\"Get experiment by ID\"\"\"\n",
    "        return self.experiments.get(experiment_id)\n",
    "    \n",
    "    def list_experiments(self, name_filter: Optional[str] = None) -> List[Experiment]:\n",
    "        \"\"\"List all experiments\"\"\"\n",
    "        exps = list(self.experiments.values())\n",
    "        if name_filter:\n",
    "            exps = [e for e in exps if name_filter in e.name]\n",
    "        return sorted(exps, key=lambda e: e.start_time, reverse=True)\n",
    "    \n",
    "    def compare_experiments(self, experiment_ids: List[str], metric: str) -> Dict:\n",
    "        \"\"\"Compare experiments by specific metric\"\"\"\n",
    "        comparison = {}\n",
    "        for exp_id in experiment_ids:\n",
    "            exp = self.experiments.get(exp_id)\n",
    "            if exp and metric in exp.metrics:\n",
    "                comparison[exp_id] = {\n",
    "                    'name': exp.name,\n",
    "                    'value': exp.metrics[metric],\n",
    "                    'parameters': exp.parameters\n",
    "                }\n",
    "        return comparison\n",
    "    \n",
    "    def get_best_experiment(self, metric: str, minimize: bool = True) -> Optional[Experiment]:\n",
    "        \"\"\"Get best experiment by metric\"\"\"\n",
    "        valid_exps = [e for e in self.experiments.values() if metric in e.metrics]\n",
    "        if not valid_exps:\n",
    "            return None\n",
    "        \n",
    "        return min(valid_exps, key=lambda e: e.metrics[metric]) if minimize else \\\n",
    "               max(valid_exps, key=lambda e: e.metrics[metric])\n",
    "\n",
    "# Example: Hyperparameter Tuning with Experiment Tracking\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Experiment Tracking - Hyperparameter Tuning\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Setup\n",
    "tracker = ExperimentTracker()\n",
    "\n",
    "# Generate synthetic wafer test data\n",
    "print(\"\\n\ud83d\udcca Generating synthetic wafer test data...\")\n",
    "n_samples = 1000\n",
    "n_features = 5\n",
    "\n",
    "# Features: Vdd_mean, Idd_mean, Frequency_mean, Temperature, Dies_tested\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "X[:, 0] = X[:, 0] * 0.05 + 1.0  # Vdd around 1.0V\n",
    "X[:, 1] = X[:, 1] * 0.1 + 0.5   # Idd around 0.5A\n",
    "X[:, 2] = X[:, 2] * 50 + 1000   # Frequency around 1000 MHz\n",
    "X[:, 3] = X[:, 3] * 10 + 25     # Temperature around 25\u00b0C\n",
    "X[:, 4] = np.random.randint(50, 200, n_samples)  # Dies tested\n",
    "\n",
    "# Target: Yield percentage (0-100%)\n",
    "# Yield influenced by all parameters\n",
    "y = (85 + \n",
    "     -20 * (X[:, 0] - 1.0) +  # Higher voltage reduces yield\n",
    "     -10 * (X[:, 1] - 0.5) +  # Higher current reduces yield\n",
    "     0.01 * (X[:, 2] - 1000) +  # Higher frequency slightly increases yield\n",
    "     -0.5 * (X[:, 3] - 25) +   # Higher temperature reduces yield\n",
    "     0.05 * X[:, 4] +          # More dies tested, better calibration\n",
    "     np.random.randn(n_samples) * 3)  # Noise\n",
    "\n",
    "y = np.clip(y, 0, 100)  # Yield between 0-100%\n",
    "\n",
    "# Train/test split\n",
    "split = int(0.8 * n_samples)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "print(f\"\u2705 Data generated: {n_samples} samples, {n_features} features\")\n",
    "print(f\"   Training set: {len(X_train)} samples\")\n",
    "print(f\"   Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Run experiments with different hyperparameters\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Running Hyperparameter Tuning Experiments\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "hyperparameter_grid = [\n",
    "    {'n_estimators': 50, 'max_depth': 5},\n",
    "    {'n_estimators': 100, 'max_depth': 5},\n",
    "    {'n_estimators': 100, 'max_depth': 10},\n",
    "    {'n_estimators': 200, 'max_depth': 10},\n",
    "    {'n_estimators': 200, 'max_depth': None}\n",
    "]\n",
    "\n",
    "print(f\"\\n\ud83d\udd2c Training {len(hyperparameter_grid)} Random Forest models...\")\n",
    "print(f\"\\n{'Exp ID':<10} {'n_trees':<10} {'depth':<10} {'RMSE':<10} {'MAE':<10} {'R\u00b2':<10} {'Time (s)':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for params in hyperparameter_grid:\n",
    "    # Create experiment\n",
    "    exp = tracker.create_experiment(\n",
    "        name=\"YieldPrediction_RandomForest\",\n",
    "        parameters=params,\n",
    "        code_version=\"abc123\"  # Git commit SHA\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Train model\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=params['n_estimators'],\n",
    "            max_depth=params['max_depth'],\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Metrics\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "        rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "        r2_test = r2_score(y_test, y_pred_test)\n",
    "        \n",
    "        # Log metrics\n",
    "        exp.log_metric('rmse_train', rmse_train)\n",
    "        exp.log_metric('rmse_test', rmse_test)\n",
    "        exp.log_metric('mae_test', mae_test)\n",
    "        exp.log_metric('r2_test', r2_test)\n",
    "        \n",
    "        # Log artifacts\n",
    "        exp.log_artifact('model', model)\n",
    "        exp.log_artifact('feature_importance', model.feature_importances_)\n",
    "        exp.log_artifact('predictions', y_pred_test[:10])  # Sample predictions\n",
    "        \n",
    "        exp.complete()\n",
    "        \n",
    "        # Print results\n",
    "        depth_str = str(params['max_depth']) if params['max_depth'] else 'None'\n",
    "        print(f\"{exp.experiment_id:<10} {params['n_estimators']:<10} {depth_str:<10} \"\n",
    "              f\"{rmse_test:<10.3f} {mae_test:<10.3f} {r2_test:<10.3f} {exp.get_duration():<10.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        exp.status = \"failed\"\n",
    "        print(f\"{exp.experiment_id:<10} FAILED: {str(e)}\")\n",
    "\n",
    "# Find best model\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Experiment Comparison and Best Model Selection\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_exp = tracker.get_best_experiment('rmse_test', minimize=True)\n",
    "\n",
    "if best_exp:\n",
    "    print(f\"\\n\ud83c\udfc6 Best Model Found:\")\n",
    "    print(f\"   Experiment ID: {best_exp.experiment_id}\")\n",
    "    print(f\"   Parameters: {best_exp.parameters}\")\n",
    "    print(f\"   Test RMSE: {best_exp.metrics['rmse_test']:.3f}%\")\n",
    "    print(f\"   Test MAE: {best_exp.metrics['mae_test']:.3f}%\")\n",
    "    print(f\"   Test R\u00b2: {best_exp.metrics['r2_test']:.3f}\")\n",
    "    print(f\"   Training time: {best_exp.get_duration():.2f} seconds\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_names = ['Vdd_mean', 'Idd_mean', 'Frequency_mean', 'Temperature', 'Dies_tested']\n",
    "    importance = best_exp.artifacts['feature_importance']\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Feature Importance (Top 3):\")\n",
    "    importance_sorted = sorted(zip(feature_names, importance), key=lambda x: x[1], reverse=True)\n",
    "    for i, (name, imp) in enumerate(importance_sorted[:3], 1):\n",
    "        print(f\"   {i}. {name}: {imp:.3f}\")\n",
    "\n",
    "# Compare all experiments\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"All Experiments Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_exps = tracker.list_experiments()\n",
    "completed_exps = [e for e in all_exps if e.status == 'completed']\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Experiment Statistics:\")\n",
    "print(f\"   Total experiments: {len(all_exps)}\")\n",
    "print(f\"   Completed: {len(completed_exps)}\")\n",
    "print(f\"   Failed: {len([e for e in all_exps if e.status == 'failed'])}\")\n",
    "print(f\"   Running: {len([e for e in all_exps if e.status == 'running'])}\")\n",
    "\n",
    "if completed_exps:\n",
    "    rmse_values = [e.metrics['rmse_test'] for e in completed_exps]\n",
    "    print(f\"\\n\ud83d\udcca RMSE Distribution:\")\n",
    "    print(f\"   Best: {min(rmse_values):.3f}%\")\n",
    "    print(f\"   Worst: {max(rmse_values):.3f}%\")\n",
    "    print(f\"   Average: {np.mean(rmse_values):.3f}%\")\n",
    "    print(f\"   Improvement: {(max(rmse_values) - min(rmse_values)) / max(rmse_values) * 100:.1f}% (best vs worst)\")\n",
    "\n",
    "# Business value\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Business Value\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Time savings\n",
    "manual_experiment_time = 2 * 3600  # 2 hours per experiment (manual)\n",
    "automated_experiment_time = best_exp.get_duration()\n",
    "time_saved_per_experiment = manual_experiment_time - automated_experiment_time\n",
    "experiments_per_month = 50\n",
    "monthly_time_savings = time_saved_per_experiment * experiments_per_month / 3600  # hours\n",
    "\n",
    "engineer_cost_per_hour = 150  # USD (fully loaded)\n",
    "monthly_cost_savings = monthly_time_savings * engineer_cost_per_hour\n",
    "annual_savings = monthly_cost_savings * 12\n",
    "\n",
    "print(f\"\\n\ud83d\udcb0 Experiment Tracking Value:\")\n",
    "print(f\"   Manual experiment time: {manual_experiment_time / 3600:.1f} hours\")\n",
    "print(f\"   Automated experiment time: {automated_experiment_time / 60:.1f} minutes\")\n",
    "print(f\"   Time saved per experiment: {time_saved_per_experiment / 3600:.1f} hours\")\n",
    "print(f\"   Experiments per month: {experiments_per_month}\")\n",
    "print(f\"   Monthly time savings: {monthly_time_savings:.0f} hours\")\n",
    "print(f\"   Engineer cost: ${engineer_cost_per_hour}/hour\")\n",
    "print(f\"   Annual cost savings: ${annual_savings / 1e6:.1f}M\")\n",
    "\n",
    "print(f\"\\n\u2705 Experiment tracking validated!\")\n",
    "print(f\"\u2705 {len(completed_exps)} experiments logged and compared\")\n",
    "print(f\"\u2705 Best model identified (RMSE: {best_exp.metrics['rmse_test']:.3f}%)\")\n",
    "print(f\"\u2705 ${annual_savings / 1e6:.1f}M/year business value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a265bc",
   "metadata": {},
   "source": [
    "## 3. \ud83d\uddc2\ufe0f Model Registry - Versioning, Stage Promotion, and Rollback\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Build a model registry to manage model versions, stage promotions (dev \u2192 staging \u2192 production), and enable instant rollback.\n",
    "\n",
    "**Key Points:**\n",
    "- **Model Versioning:** Track every model version (v1, v2, v3) with lineage\n",
    "- **Stage Promotion:** Models progress through stages (None \u2192 Staging \u2192 Production \u2192 Archived)\n",
    "- **Metadata Storage:** Link model to experiment, training data, code version\n",
    "- **Rollback Capability:** Instantly revert to previous production version if issues\n",
    "- **Access Control:** Only authorized users can promote to production\n",
    "\n",
    "**Model Lifecycle Stages:**\n",
    "- **None:** Newly registered, not yet tested\n",
    "- **Staging:** Deployed to staging environment for validation\n",
    "- **Production:** Serving live traffic\n",
    "- **Archived:** Deprecated, kept for audit/compliance\n",
    "\n",
    "**Why This Matters for Post-Silicon:**\n",
    "- **Deployment Safety:** Test models in staging before production (catch bugs early)\n",
    "- **Quick Rollback:** Production model fails \u2192 revert to previous version in seconds\n",
    "- **Audit Trail:** Regulators ask \"which model was used on 2025-12-01?\" \u2192 instant answer\n",
    "- **Multi-Model Management:** Track 50+ production models (yield, test time, binning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4320ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Registry System\n",
    "\n",
    "class ModelStage(Enum):\n",
    "    \"\"\"Model lifecycle stages\"\"\"\n",
    "    NONE = \"None\"\n",
    "    STAGING = \"Staging\"\n",
    "    PRODUCTION = \"Production\"\n",
    "    ARCHIVED = \"Archived\"\n",
    "\n",
    "@dataclass\n",
    "class ModelVersion:\n",
    "    \"\"\"Registered model version\"\"\"\n",
    "    model_name: str\n",
    "    version: int\n",
    "    experiment_id: str\n",
    "    stage: ModelStage = ModelStage.NONE\n",
    "    created_at: datetime = field(default_factory=datetime.now)\n",
    "    promoted_at: Optional[datetime] = None\n",
    "    model_artifact: Optional[Any] = None\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def promote_to_stage(self, new_stage: ModelStage):\n",
    "        \"\"\"Promote model to new stage\"\"\"\n",
    "        self.stage = new_stage\n",
    "        self.promoted_at = datetime.now()\n",
    "    \n",
    "    def get_model_uri(self) -> str:\n",
    "        \"\"\"Get model URI\"\"\"\n",
    "        return f\"models:/{self.model_name}/{self.version}\"\n",
    "\n",
    "class ModelRegistry:\n",
    "    \"\"\"Model registry (like MLflow Model Registry)\"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_tracker: ExperimentTracker):\n",
    "        self.experiment_tracker = experiment_tracker\n",
    "        self.models: Dict[str, List[ModelVersion]] = defaultdict(list)\n",
    "        self.version_counter: Dict[str, int] = defaultdict(int)\n",
    "    \n",
    "    def register_model(self, model_name: str, experiment_id: str, \n",
    "                      model_artifact: Any, metadata: Optional[Dict] = None) -> ModelVersion:\n",
    "        \"\"\"Register new model version\"\"\"\n",
    "        # Verify experiment exists\n",
    "        experiment = self.experiment_tracker.get_experiment(experiment_id)\n",
    "        if not experiment:\n",
    "            raise ValueError(f\"Experiment {experiment_id} not found\")\n",
    "        \n",
    "        # Increment version\n",
    "        self.version_counter[model_name] += 1\n",
    "        version_number = self.version_counter[model_name]\n",
    "        \n",
    "        # Create model version\n",
    "        model_version = ModelVersion(\n",
    "            model_name=model_name,\n",
    "            version=version_number,\n",
    "            experiment_id=experiment_id,\n",
    "            model_artifact=model_artifact,\n",
    "            metadata=metadata or {}\n",
    "        )\n",
    "        \n",
    "        # Add experiment metrics to metadata\n",
    "        model_version.metadata['experiment_metrics'] = experiment.metrics.copy()\n",
    "        model_version.metadata['experiment_parameters'] = experiment.parameters.copy()\n",
    "        \n",
    "        self.models[model_name].append(model_version)\n",
    "        return model_version\n",
    "    \n",
    "    def get_model_version(self, model_name: str, version: int) -> Optional[ModelVersion]:\n",
    "        \"\"\"Get specific model version\"\"\"\n",
    "        for mv in self.models.get(model_name, []):\n",
    "            if mv.version == version:\n",
    "                return mv\n",
    "        return None\n",
    "    \n",
    "    def get_latest_version(self, model_name: str, stage: Optional[ModelStage] = None) -> Optional[ModelVersion]:\n",
    "        \"\"\"Get latest model version (optionally filtered by stage)\"\"\"\n",
    "        versions = self.models.get(model_name, [])\n",
    "        if stage:\n",
    "            versions = [v for v in versions if v.stage == stage]\n",
    "        \n",
    "        if not versions:\n",
    "            return None\n",
    "        \n",
    "        return max(versions, key=lambda v: v.version)\n",
    "    \n",
    "    def promote_model(self, model_name: str, version: int, new_stage: ModelStage):\n",
    "        \"\"\"Promote model to new stage\"\"\"\n",
    "        model_version = self.get_model_version(model_name, version)\n",
    "        if not model_version:\n",
    "            raise ValueError(f\"Model version {model_name}:{version} not found\")\n",
    "        \n",
    "        # If promoting to production, demote current production model to archived\n",
    "        if new_stage == ModelStage.PRODUCTION:\n",
    "            current_prod = self.get_latest_version(model_name, ModelStage.PRODUCTION)\n",
    "            if current_prod:\n",
    "                current_prod.promote_to_stage(ModelStage.ARCHIVED)\n",
    "        \n",
    "        model_version.promote_to_stage(new_stage)\n",
    "    \n",
    "    def list_models(self) -> List[str]:\n",
    "        \"\"\"List all registered model names\"\"\"\n",
    "        return list(self.models.keys())\n",
    "    \n",
    "    def get_model_history(self, model_name: str) -> List[ModelVersion]:\n",
    "        \"\"\"Get all versions of a model\"\"\"\n",
    "        return sorted(self.models.get(model_name, []), key=lambda v: v.version, reverse=True)\n",
    "\n",
    "# Example: Model Registry Workflow\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Model Registry - Versioning and Stage Promotion\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Setup\n",
    "registry = ModelRegistry(tracker)\n",
    "\n",
    "# Register best model from experiments\n",
    "\n",
    "best_exp = tracker.get_best_experiment('rmse_test', minimize=True)\n",
    "if best_exp:\n",
    "    print(f\"\\n\ud83d\udcdd Registering best model to registry...\")\n",
    "    print(f\"   Experiment ID: {best_exp.experiment_id}\")\n",
    "    print(f\"   RMSE: {best_exp.metrics['rmse_test']:.3f}%\")\n",
    "    \n",
    "    model_v1 = registry.register_model(\n",
    "        model_name=\"YieldPredictor\",\n",
    "        experiment_id=best_exp.experiment_id,\n",
    "        model_artifact=best_exp.artifacts['model'],\n",
    "        metadata={\n",
    "            'model_type': 'RandomForest',\n",
    "            'training_samples': len(X_train),\n",
    "            'features': ['Vdd_mean', 'Idd_mean', 'Frequency_mean', 'Temperature', 'Dies_tested']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"\u2705 Model registered: {model_v1.model_name} v{model_v1.version}\")\n",
    "    print(f\"   Stage: {model_v1.stage.value}\")\n",
    "    print(f\"   URI: {model_v1.get_model_uri()}\")\n",
    "\n",
    "# Register another version (simulate improvement)\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Registering Improved Model Version\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find second-best experiment\n",
    "all_completed = [e for e in tracker.list_experiments() if e.status == 'completed']\n",
    "if len(all_completed) >= 2:\n",
    "    second_best = sorted(all_completed, key=lambda e: e.metrics['rmse_test'])[1]\n",
    "    \n",
    "    model_v2 = registry.register_model(\n",
    "        model_name=\"YieldPredictor\",\n",
    "        experiment_id=second_best.experiment_id,\n",
    "        model_artifact=second_best.artifacts['model'],\n",
    "        metadata={\n",
    "            'model_type': 'RandomForest',\n",
    "            'training_samples': len(X_train),\n",
    "            'improvement': 'Increased max_depth for better accuracy'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"\u2705 Model registered: {model_v2.model_name} v{model_v2.version}\")\n",
    "    print(f\"   RMSE: {second_best.metrics['rmse_test']:.3f}%\")\n",
    "    print(f\"   Stage: {model_v2.stage.value}\")\n",
    "\n",
    "# Promote model to staging\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Stage Promotion Workflow\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n1\ufe0f\u20e3 Promote v1 to Staging:\")\n",
    "registry.promote_model(\"YieldPredictor\", version=1, new_stage=ModelStage.STAGING)\n",
    "staging_model = registry.get_latest_version(\"YieldPredictor\", ModelStage.STAGING)\n",
    "print(f\"   \u2705 {staging_model.model_name} v{staging_model.version} \u2192 Staging\")\n",
    "print(f\"   Promoted at: {staging_model.promoted_at.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Test in staging (simulate)\n",
    "print(f\"\\n2\ufe0f\u20e3 Testing in Staging Environment:\")\n",
    "print(f\"   Running validation tests...\")\n",
    "print(f\"   \u2705 Accuracy test passed (RMSE < 2%)\")\n",
    "print(f\"   \u2705 Latency test passed (P95 < 50ms)\")\n",
    "print(f\"   \u2705 Load test passed (1000 QPS)\")\n",
    "\n",
    "# Promote to production\n",
    "print(f\"\\n3\ufe0f\u20e3 Promote to Production:\")\n",
    "registry.promote_model(\"YieldPredictor\", version=1, new_stage=ModelStage.PRODUCTION)\n",
    "prod_model = registry.get_latest_version(\"YieldPredictor\", ModelStage.PRODUCTION)\n",
    "print(f\"   \u2705 {prod_model.model_name} v{prod_model.version} \u2192 Production\")\n",
    "print(f\"   Now serving live traffic\")\n",
    "\n",
    "# Deploy v2 to staging\n",
    "print(f\"\\n4\ufe0f\u20e3 Deploy v2 to Staging (Candidate for Production):\")\n",
    "registry.promote_model(\"YieldPredictor\", version=2, new_stage=ModelStage.STAGING)\n",
    "print(f\"   \u2705 {model_v2.model_name} v{model_v2.version} \u2192 Staging\")\n",
    "print(f\"   Testing new model candidate...\")\n",
    "\n",
    "# Simulate production issue - rollback scenario\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Rollback Scenario - Production Model Fails\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n\ud83d\udea8 Production Incident Detected:\")\n",
    "print(f\"   Model v2 promoted to production\")\n",
    "print(f\"   Accuracy dropped from 90% to 75% (data drift)\")\n",
    "print(f\"   Alert triggered: RMSE >2%\")\n",
    "\n",
    "# Promote v2 to production (simulate bad deployment)\n",
    "registry.promote_model(\"YieldPredictor\", version=2, new_stage=ModelStage.PRODUCTION)\n",
    "\n",
    "print(f\"\\n\u26a0\ufe0f Current Production Model:\")\n",
    "current_prod = registry.get_latest_version(\"YieldPredictor\", ModelStage.PRODUCTION)\n",
    "print(f\"   {current_prod.model_name} v{current_prod.version}\")\n",
    "print(f\"   Status: \u274c FAILING (high RMSE)\")\n",
    "\n",
    "# Rollback to v1\n",
    "print(f\"\\n\ud83d\udd04 Initiating Rollback:\")\n",
    "print(f\"   Demoting v2 to Archived\")\n",
    "registry.promote_model(\"YieldPredictor\", version=2, new_stage=ModelStage.ARCHIVED)\n",
    "\n",
    "print(f\"   Promoting v1 back to Production\")\n",
    "registry.promote_model(\"YieldPredictor\", version=1, new_stage=ModelStage.PRODUCTION)\n",
    "\n",
    "restored_prod = registry.get_latest_version(\"YieldPredictor\", ModelStage.PRODUCTION)\n",
    "print(f\"\\n\u2705 Rollback Complete:\")\n",
    "print(f\"   {restored_prod.model_name} v{restored_prod.version} \u2192 Production\")\n",
    "print(f\"   Production restored in <30 seconds\")\n",
    "print(f\"   RMSE back to normal (< 2%)\")\n",
    "\n",
    "# Model history\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Model Version History\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "history = registry.get_model_history(\"YieldPredictor\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcdc {len(history)} versions of YieldPredictor:\")\n",
    "print(f\"\\n{'Version':<10} {'Stage':<15} {'RMSE':<10} {'Created':<20} {'Experiment':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for mv in history:\n",
    "    rmse = mv.metadata['experiment_metrics'].get('rmse_test', 0)\n",
    "    created = mv.created_at.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f\"v{mv.version:<9} {mv.stage.value:<15} {rmse:<10.3f} {created:<20} {mv.experiment_id:<15}\")\n",
    "\n",
    "# Business value\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Business Value\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Rollback time savings\n",
    "manual_rollback_time = 4 * 3600  # 4 hours (find model, redeploy, test)\n",
    "automated_rollback_time = 30  # 30 seconds\n",
    "rollback_time_saved = manual_rollback_time - automated_rollback_time\n",
    "\n",
    "rollbacks_per_year = 12  # 1 per month\n",
    "downtime_cost_per_hour = 50000  # USD\n",
    "downtime_prevented = (rollback_time_saved * rollbacks_per_year) / 3600  # hours\n",
    "annual_downtime_savings = downtime_prevented * downtime_cost_per_hour\n",
    "\n",
    "# Deployment confidence\n",
    "deployment_incidents_prevented = 50  # per year (caught in staging)\n",
    "incident_cost = 100000  # USD per incident\n",
    "incident_prevention_value = deployment_incidents_prevented * incident_cost\n",
    "\n",
    "total_value = annual_downtime_savings + incident_prevention_value\n",
    "\n",
    "print(f\"\\n\ud83d\udcb0 Model Registry Value:\")\n",
    "print(f\"   Manual rollback time: {manual_rollback_time / 3600:.1f} hours\")\n",
    "print(f\"   Automated rollback time: {automated_rollback_time} seconds\")\n",
    "print(f\"   Time saved per rollback: {rollback_time_saved / 3600:.1f} hours\")\n",
    "print(f\"   Rollbacks per year: {rollbacks_per_year}\")\n",
    "print(f\"   Downtime prevented: {downtime_prevented:.1f} hours/year\")\n",
    "print(f\"   Downtime cost savings: ${annual_downtime_savings / 1e6:.1f}M/year\")\n",
    "print(f\"\\n   Deployment incidents prevented: {deployment_incidents_prevented}/year\")\n",
    "print(f\"   Incident prevention value: ${incident_prevention_value / 1e6:.1f}M/year\")\n",
    "print(f\"\\n   Total annual value: ${total_value / 1e6:.1f}M\")\n",
    "\n",
    "print(f\"\\n\u2705 Model registry validated!\")\n",
    "print(f\"\u2705 {len(history)} model versions tracked\")\n",
    "print(f\"\u2705 Rollback capability tested (30 seconds)\")\n",
    "print(f\"\u2705 ${total_value / 1e6:.1f}M/year business value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe0b976",
   "metadata": {},
   "source": [
    "## 4. \ud83d\udd04 CI/CD Pipeline - Automated Training, Testing, and Deployment\n",
    "\n",
    "**Purpose:** Build CI/CD pipeline for ML that automates training, validation, and deployment when code or data changes.\n",
    "\n",
    "**Key Points:**\n",
    "- **Continuous Integration (CI)**: Automated testing when code changes (unit tests, model tests, data validation)\n",
    "- **Continuous Deployment (CD)**: Automated deployment to staging/production when tests pass\n",
    "- **Pipeline Triggers**: Code commits, new data, schedule (daily retraining), manual trigger\n",
    "- **Pipeline Stages**: Data validation \u2192 Training \u2192 Evaluation \u2192 Model registration \u2192 Deployment \u2192 Smoke tests\n",
    "- **Automated Rollback**: If smoke tests fail, automatically rollback to previous model version\n",
    "\n",
    "**Why for Post-Silicon?**\n",
    "- **Automated Retraining**: When new wafer test data arrives (daily), retrain yield prediction models automatically\n",
    "- **Fast Iteration**: Data scientists commit model improvements \u2192 CI/CD deploys to staging in <10 minutes\n",
    "- **Safety**: Staging tests catch bad models before production (99% of issues caught pre-prod)\n",
    "- **Audit Trail**: Every deployment tracked (who, when, what changed, test results) for regulatory compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b7db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CI/CD Pipeline System\n",
    "\n",
    "class PipelineStage(Enum):\n",
    "    \"\"\"Pipeline stages\"\"\"\n",
    "    DATA_VALIDATION = \"Data Validation\"\n",
    "    TRAINING = \"Training\"\n",
    "    EVALUATION = \"Evaluation\"\n",
    "    MODEL_REGISTRATION = \"Model Registration\"\n",
    "    DEPLOYMENT = \"Deployment\"\n",
    "    SMOKE_TEST = \"Smoke Test\"\n",
    "\n",
    "@dataclass\n",
    "class PipelineRun:\n",
    "    \"\"\"CI/CD pipeline run\"\"\"\n",
    "    run_id: str\n",
    "    trigger: str  # \"code_commit\", \"data_update\", \"schedule\", \"manual\"\n",
    "    stages: Dict[PipelineStage, Dict[str, Any]] = field(default_factory=dict)\n",
    "    status: str = \"running\"  # running, success, failed\n",
    "    started_at: datetime = field(default_factory=datetime.now)\n",
    "    completed_at: Optional[datetime] = None\n",
    "    \n",
    "    def log_stage(self, stage: PipelineStage, status: str, details: Dict[str, Any]):\n",
    "        \"\"\"Log pipeline stage result\"\"\"\n",
    "        self.stages[stage] = {\n",
    "            'status': status,\n",
    "            'details': details,\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def complete(self, status: str):\n",
    "        \"\"\"Complete pipeline run\"\"\"\n",
    "        self.status = status\n",
    "        self.completed_at = datetime.now()\n",
    "    \n",
    "    def get_duration(self) -> Optional[float]:\n",
    "        \"\"\"Get duration in seconds\"\"\"\n",
    "        if self.completed_at:\n",
    "            return (self.completed_at - self.started_at).total_seconds()\n",
    "        return None\n",
    "\n",
    "class MLPipeline:\n",
    "    \"\"\"ML CI/CD Pipeline (like Kubeflow, MLflow Pipelines)\"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_tracker: ExperimentTracker, \n",
    "                 model_registry: ModelRegistry):\n",
    "        self.experiment_tracker = experiment_tracker\n",
    "        self.model_registry = model_registry\n",
    "        self.runs: List[PipelineRun] = []\n",
    "    \n",
    "    def run_pipeline(self, trigger: str, model_name: str, \n",
    "                    hyperparameters: Dict[str, Any],\n",
    "                    X_train: np.ndarray, y_train: np.ndarray,\n",
    "                    X_test: np.ndarray, y_test: np.ndarray,\n",
    "                    min_r2_score: float = 0.7,\n",
    "                    max_rmse: float = 2.0) -> PipelineRun:\n",
    "        \"\"\"Run complete ML pipeline\"\"\"\n",
    "        \n",
    "        run_id = f\"pipeline_{len(self.runs) + 1}\"\n",
    "        pipeline_run = PipelineRun(run_id=run_id, trigger=trigger)\n",
    "        self.runs.append(pipeline_run)\n",
    "        \n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"\ud83d\ude80 Pipeline Run: {run_id}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        print(f\"Trigger: {trigger}\")\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Started: {pipeline_run.started_at.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        try:\n",
    "            # Stage 1: Data Validation\n",
    "            print(f\"\\n1\ufe0f\u20e3 {PipelineStage.DATA_VALIDATION.value}...\")\n",
    "            data_checks = {\n",
    "                'train_samples': len(X_train),\n",
    "                'test_samples': len(X_test),\n",
    "                'features': X_train.shape[1],\n",
    "                'missing_values': np.isnan(X_train).sum() + np.isnan(X_test).sum(),\n",
    "                'target_range': (y_train.min(), y_train.max())\n",
    "            }\n",
    "            \n",
    "            # Validate data quality\n",
    "            if data_checks['missing_values'] > 0:\n",
    "                raise ValueError(f\"Found {data_checks['missing_values']} missing values\")\n",
    "            \n",
    "            if data_checks['train_samples'] < 100:\n",
    "                raise ValueError(f\"Insufficient training samples: {data_checks['train_samples']}\")\n",
    "            \n",
    "            pipeline_run.log_stage(PipelineStage.DATA_VALIDATION, 'passed', data_checks)\n",
    "            print(f\"   \u2705 Data validation passed\")\n",
    "            print(f\"      Train samples: {data_checks['train_samples']}\")\n",
    "            print(f\"      Test samples: {data_checks['test_samples']}\")\n",
    "            print(f\"      Features: {data_checks['features']}\")\n",
    "            \n",
    "            # Stage 2: Training\n",
    "            print(f\"\\n2\ufe0f\u20e3 {PipelineStage.TRAINING.value}...\")\n",
    "            experiment = self.experiment_tracker.create_experiment(\n",
    "                name=f\"{model_name}_pipeline_{run_id}\",\n",
    "                parameters=hyperparameters\n",
    "            )\n",
    "            \n",
    "            model = RandomForestRegressor(**hyperparameters, random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            training_details = {\n",
    "                'experiment_id': experiment.experiment_id,\n",
    "                'hyperparameters': hyperparameters,\n",
    "                'training_time': experiment.get_duration()\n",
    "            }\n",
    "            \n",
    "            pipeline_run.log_stage(PipelineStage.TRAINING, 'passed', training_details)\n",
    "            print(f\"   \u2705 Training completed\")\n",
    "            print(f\"      Experiment ID: {experiment.experiment_id}\")\n",
    "            \n",
    "            # Stage 3: Evaluation\n",
    "            print(f\"\\n3\ufe0f\u20e3 {PipelineStage.EVALUATION.value}...\")\n",
    "            \n",
    "            # Compute metrics\n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            \n",
    "            rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "            rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "            mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "            r2_test = r2_score(y_test, y_pred_test)\n",
    "            \n",
    "            # Log metrics to experiment\n",
    "            experiment.log_metric('rmse_train', rmse_train)\n",
    "            experiment.log_metric('rmse_test', rmse_test)\n",
    "            experiment.log_metric('mae_test', mae_test)\n",
    "            experiment.log_metric('r2_test', r2_test)\n",
    "            experiment.log_artifact('model', model)\n",
    "            experiment.complete()\n",
    "            \n",
    "            evaluation_details = {\n",
    "                'rmse_test': rmse_test,\n",
    "                'mae_test': mae_test,\n",
    "                'r2_test': r2_test,\n",
    "                'metrics_logged': True\n",
    "            }\n",
    "            \n",
    "            # Check if model meets quality thresholds\n",
    "            if r2_test < min_r2_score:\n",
    "                raise ValueError(f\"R\u00b2 score {r2_test:.3f} below threshold {min_r2_score}\")\n",
    "            \n",
    "            if rmse_test > max_rmse:\n",
    "                raise ValueError(f\"RMSE {rmse_test:.3f} above threshold {max_rmse}\")\n",
    "            \n",
    "            pipeline_run.log_stage(PipelineStage.EVALUATION, 'passed', evaluation_details)\n",
    "            print(f\"   \u2705 Evaluation passed\")\n",
    "            print(f\"      RMSE: {rmse_test:.3f}% (threshold: <{max_rmse}%)\")\n",
    "            print(f\"      R\u00b2: {r2_test:.3f} (threshold: >{min_r2_score})\")\n",
    "            \n",
    "            # Stage 4: Model Registration\n",
    "            print(f\"\\n4\ufe0f\u20e3 {PipelineStage.MODEL_REGISTRATION.value}...\")\n",
    "            \n",
    "            model_version = self.model_registry.register_model(\n",
    "                model_name=model_name,\n",
    "                experiment_id=experiment.experiment_id,\n",
    "                model_artifact=model,\n",
    "                metadata={\n",
    "                    'pipeline_run': run_id,\n",
    "                    'trigger': trigger,\n",
    "                    'hyperparameters': hyperparameters\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            registration_details = {\n",
    "                'model_name': model_name,\n",
    "                'version': model_version.version,\n",
    "                'stage': model_version.stage.value\n",
    "            }\n",
    "            \n",
    "            pipeline_run.log_stage(PipelineStage.MODEL_REGISTRATION, 'passed', registration_details)\n",
    "            print(f\"   \u2705 Model registered\")\n",
    "            print(f\"      {model_name} v{model_version.version}\")\n",
    "            print(f\"      URI: {model_version.get_model_uri()}\")\n",
    "            \n",
    "            # Stage 5: Deployment to Staging\n",
    "            print(f\"\\n5\ufe0f\u20e3 {PipelineStage.DEPLOYMENT.value} (Staging)...\")\n",
    "            \n",
    "            self.model_registry.promote_model(\n",
    "                model_name=model_name,\n",
    "                version=model_version.version,\n",
    "                new_stage=ModelStage.STAGING\n",
    "            )\n",
    "            \n",
    "            deployment_details = {\n",
    "                'environment': 'staging',\n",
    "                'model_version': model_version.version,\n",
    "                'deployed_at': datetime.now()\n",
    "            }\n",
    "            \n",
    "            pipeline_run.log_stage(PipelineStage.DEPLOYMENT, 'passed', deployment_details)\n",
    "            print(f\"   \u2705 Deployed to staging\")\n",
    "            print(f\"      {model_name} v{model_version.version} \u2192 Staging\")\n",
    "            \n",
    "            # Stage 6: Smoke Tests\n",
    "            print(f\"\\n6\ufe0f\u20e3 {PipelineStage.SMOKE_TEST.value}...\")\n",
    "            \n",
    "            # Simulate smoke tests\n",
    "            smoke_tests = {\n",
    "                'prediction_test': True,  # Can make predictions\n",
    "                'latency_test': True,     # P95 latency < 50ms\n",
    "                'accuracy_test': r2_test > min_r2_score,\n",
    "                'load_test': True         # Handle 100 QPS\n",
    "            }\n",
    "            \n",
    "            all_passed = all(smoke_tests.values())\n",
    "            \n",
    "            if not all_passed:\n",
    "                # Rollback on failure\n",
    "                print(f\"   \u274c Smoke tests failed!\")\n",
    "                print(f\"      Initiating rollback...\")\n",
    "                self.model_registry.promote_model(\n",
    "                    model_name=model_name,\n",
    "                    version=model_version.version,\n",
    "                    new_stage=ModelStage.ARCHIVED\n",
    "                )\n",
    "                raise ValueError(\"Smoke tests failed, deployment rolled back\")\n",
    "            \n",
    "            pipeline_run.log_stage(PipelineStage.SMOKE_TEST, 'passed', smoke_tests)\n",
    "            print(f\"   \u2705 Smoke tests passed\")\n",
    "            for test_name, result in smoke_tests.items():\n",
    "                print(f\"      {test_name}: {'\u2705' if result else '\u274c'}\")\n",
    "            \n",
    "            # Ready for production promotion (manual approval in real scenario)\n",
    "            print(f\"\\n\u2705 Pipeline completed successfully!\")\n",
    "            print(f\"   Model ready for production promotion\")\n",
    "            \n",
    "            pipeline_run.complete('success')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n\u274c Pipeline failed: {str(e)}\")\n",
    "            pipeline_run.complete('failed')\n",
    "        \n",
    "        return pipeline_run\n",
    "    \n",
    "    def get_pipeline_history(self) -> List[PipelineRun]:\n",
    "        \"\"\"Get all pipeline runs\"\"\"\n",
    "        return sorted(self.runs, key=lambda r: r.started_at, reverse=True)\n",
    "\n",
    "# Example: CI/CD Pipeline Execution\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CI/CD Pipeline - Automated ML Workflow\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = MLPipeline(tracker, registry)\n",
    "\n",
    "# Scenario 1: Code commit trigger (data scientist improved model)\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Scenario 1: Code Commit (Improved Hyperparameters)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "pipeline_run_1 = pipeline.run_pipeline(\n",
    "    trigger=\"code_commit\",\n",
    "    model_name=\"YieldPredictor\",\n",
    "    hyperparameters={\n",
    "        'n_estimators': 300,\n",
    "        'max_depth': 15,\n",
    "        'min_samples_split': 5\n",
    "    },\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    min_r2_score=0.7,\n",
    "    max_rmse=2.0\n",
    ")\n",
    "\n",
    "print(f\"\\nPipeline Duration: {pipeline_run_1.get_duration():.2f} seconds\")\n",
    "print(f\"Status: {pipeline_run_1.status}\")\n",
    "\n",
    "# Scenario 2: Data update trigger (new wafer test data arrived)\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Scenario 2: Data Update (New Wafer Test Data)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simulate new data (slightly different distribution)\n",
    "np.random.seed(100)\n",
    "X_new = np.random.randn(1000, 5)\n",
    "X_new[:, 0] = X_new[:, 0] * 0.05 + 1.0  # Vdd\n",
    "X_new[:, 1] = X_new[:, 1] * 0.1 + 0.5   # Idd\n",
    "X_new[:, 2] = X_new[:, 2] * 50 + 1000   # Frequency\n",
    "X_new[:, 3] = X_new[:, 3] * 5 + 25      # Temperature\n",
    "X_new[:, 4] = (np.random.rand(1000) * 150 + 50).astype(int)  # Dies\n",
    "\n",
    "y_new = 50 + 30 * X_new[:, 0] + 20 * X_new[:, 1] - 0.1 * X_new[:, 2] + \\\n",
    "        2 * X_new[:, 3] + 0.05 * X_new[:, 4] + np.random.randn(1000) * 2\n",
    "\n",
    "X_train_new = X_new[:800]\n",
    "y_train_new = y_new[:800]\n",
    "X_test_new = X_new[800:]\n",
    "y_test_new = y_new[800:]\n",
    "\n",
    "pipeline_run_2 = pipeline.run_pipeline(\n",
    "    trigger=\"data_update\",\n",
    "    model_name=\"YieldPredictor\",\n",
    "    hyperparameters={\n",
    "        'n_estimators': 200,\n",
    "        'max_depth': 10\n",
    "    },\n",
    "    X_train=X_train_new,\n",
    "    y_train=y_train_new,\n",
    "    X_test=X_test_new,\n",
    "    y_test=y_test_new,\n",
    "    min_r2_score=0.7,\n",
    "    max_rmse=2.0\n",
    ")\n",
    "\n",
    "print(f\"\\nPipeline Duration: {pipeline_run_2.get_duration():.2f} seconds\")\n",
    "print(f\"Status: {pipeline_run_2.status}\")\n",
    "\n",
    "# Pipeline history\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Pipeline Run History\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "history = pipeline.get_pipeline_history()\n",
    "\n",
    "print(f\"\\n\ud83d\udcdc {len(history)} pipeline runs:\")\n",
    "print(f\"\\n{'Run ID':<20} {'Trigger':<15} {'Status':<10} {'Duration':<12} {'Stages':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for run in history:\n",
    "    duration = f\"{run.get_duration():.2f}s\" if run.get_duration() else \"running\"\n",
    "    stages_passed = sum(1 for s in run.stages.values() if s['status'] == 'passed')\n",
    "    print(f\"{run.run_id:<20} {run.trigger:<15} {run.status:<10} {duration:<12} {stages_passed}/6\")\n",
    "\n",
    "# Business value\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Business Value\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Time savings\n",
    "manual_deployment_time = 4 * 3600  # 4 hours (manual testing, deployment, validation)\n",
    "automated_deployment_time = pipeline_run_1.get_duration()  # seconds\n",
    "time_saved_per_deployment = manual_deployment_time - automated_deployment_time\n",
    "\n",
    "deployments_per_month = 20  # 1 per business day\n",
    "monthly_time_saved = (time_saved_per_deployment * deployments_per_month) / 3600  # hours\n",
    "engineer_cost = 150  # USD per hour\n",
    "monthly_cost_savings = monthly_time_saved * engineer_cost\n",
    "annual_cost_savings = monthly_cost_savings * 12\n",
    "\n",
    "# Faster iteration\n",
    "time_to_production_manual = 7 * 24 * 3600  # 1 week\n",
    "time_to_production_auto = 10 * 60  # 10 minutes\n",
    "innovation_acceleration = time_to_production_manual / time_to_production_auto\n",
    "\n",
    "# Error prevention\n",
    "deployment_errors_prevented = 50  # per year (caught by automated tests)\n",
    "error_cost = 25000  # USD per deployment error\n",
    "error_prevention_value = deployment_errors_prevented * error_cost\n",
    "\n",
    "total_value = annual_cost_savings + error_prevention_value\n",
    "\n",
    "print(f\"\\n\ud83d\udcb0 CI/CD Pipeline Value:\")\n",
    "print(f\"   Manual deployment time: {manual_deployment_time / 3600:.1f} hours\")\n",
    "print(f\"   Automated deployment time: {automated_deployment_time / 60:.1f} minutes\")\n",
    "print(f\"   Time saved per deployment: {time_saved_per_deployment / 3600:.1f} hours\")\n",
    "print(f\"   Deployments per month: {deployments_per_month}\")\n",
    "print(f\"   Monthly time savings: {monthly_time_saved:.0f} hours\")\n",
    "print(f\"   Annual cost savings: ${annual_cost_savings / 1e6:.1f}M\")\n",
    "print(f\"\\n   Time to production acceleration: {innovation_acceleration:.0f}x faster\")\n",
    "print(f\"   Deployment errors prevented: {deployment_errors_prevented}/year\")\n",
    "print(f\"   Error prevention value: ${error_prevention_value / 1e6:.1f}M/year\")\n",
    "print(f\"\\n   Total annual value: ${total_value / 1e6:.1f}M\")\n",
    "\n",
    "print(f\"\\n\u2705 CI/CD pipeline validated!\")\n",
    "print(f\"\u2705 {len(history)} successful pipeline runs\")\n",
    "print(f\"\u2705 6-stage automated workflow\")\n",
    "print(f\"\u2705 ${total_value / 1e6:.1f}M/year business value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13af7384",
   "metadata": {},
   "source": [
    "## 5. \ud83d\udcc8 Production Monitoring - Drift Detection, Performance Tracking, and Alerts\n",
    "\n",
    "**Purpose:** Monitor production models for data drift, concept drift, performance degradation, and trigger retraining when needed.\n",
    "\n",
    "**Key Points:**\n",
    "- **Data Drift**: Input feature distribution changes (e.g., temperature sensors recalibrated, test parameters changed)\n",
    "- **Concept Drift**: Relationship between features and target changes (e.g., new chip design affects yield patterns)\n",
    "- **Performance Monitoring**: Track prediction accuracy, latency, throughput in production\n",
    "- **Automated Alerts**: Notify when drift detected, accuracy drops, latency spikes\n",
    "- **Retraining Triggers**: Automatically trigger CI/CD pipeline when drift exceeds threshold\n",
    "\n",
    "**Why for Post-Silicon?**\n",
    "- **Early Detection**: Detect when yield prediction models become stale (temperature drift \u2192 15% accuracy drop detected in 1 hour)\n",
    "- **Prevent Bad Decisions**: Alert before model makes incorrect predictions (prevent $500K bad wafer decisions)\n",
    "- **Automated Response**: Auto-retrain when data drift detected (no manual monitoring needed)\n",
    "- **Compliance**: Track model performance for regulatory audits (FDA, automotive safety standards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac0f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Monitoring System\n",
    "\n",
    "class DriftType(Enum):\n",
    "    \"\"\"Types of drift\"\"\"\n",
    "    DATA_DRIFT = \"Data Drift\"\n",
    "    CONCEPT_DRIFT = \"Concept Drift\"\n",
    "    PERFORMANCE_DRIFT = \"Performance Drift\"\n",
    "\n",
    "@dataclass\n",
    "class DriftAlert:\n",
    "    \"\"\"Drift detection alert\"\"\"\n",
    "    alert_id: str\n",
    "    drift_type: DriftType\n",
    "    severity: str  # \"low\", \"medium\", \"high\", \"critical\"\n",
    "    feature_name: Optional[str] = None\n",
    "    metric_name: Optional[str] = None\n",
    "    baseline_value: float = 0.0\n",
    "    current_value: float = 0.0\n",
    "    drift_score: float = 0.0\n",
    "    threshold: float = 0.0\n",
    "    detected_at: datetime = field(default_factory=datetime.now)\n",
    "    recommendation: str = \"\"\n",
    "\n",
    "class ProductionMonitor:\n",
    "    \"\"\"Production model monitoring (like Evidently AI, Fiddler)\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, baseline_data: np.ndarray, \n",
    "                 baseline_predictions: np.ndarray):\n",
    "        self.model_name = model_name\n",
    "        self.baseline_data = baseline_data\n",
    "        self.baseline_predictions = baseline_predictions\n",
    "        \n",
    "        # Compute baseline statistics\n",
    "        self.baseline_mean = baseline_data.mean(axis=0)\n",
    "        self.baseline_std = baseline_data.std(axis=0)\n",
    "        \n",
    "        self.alerts: List[DriftAlert] = []\n",
    "        self.performance_history: List[Dict] = []\n",
    "    \n",
    "    def detect_data_drift(self, current_data: np.ndarray, \n",
    "                         threshold: float = 3.0) -> List[DriftAlert]:\n",
    "        \"\"\"Detect data drift using z-score (Kolmogorov-Smirnov test in production)\"\"\"\n",
    "        drift_alerts = []\n",
    "        \n",
    "        # Check each feature\n",
    "        for i in range(current_data.shape[1]):\n",
    "            current_mean = current_data[:, i].mean()\n",
    "            baseline_mean = self.baseline_mean[i]\n",
    "            baseline_std = self.baseline_std[i]\n",
    "            \n",
    "            # Z-score: how many standard deviations away from baseline\n",
    "            if baseline_std > 0:\n",
    "                z_score = abs(current_mean - baseline_mean) / baseline_std\n",
    "            else:\n",
    "                z_score = 0\n",
    "            \n",
    "            # Alert if drift detected\n",
    "            if z_score > threshold:\n",
    "                severity = \"critical\" if z_score > 5 else \"high\" if z_score > 4 else \"medium\"\n",
    "                \n",
    "                alert = DriftAlert(\n",
    "                    alert_id=f\"drift_{len(self.alerts) + 1}\",\n",
    "                    drift_type=DriftType.DATA_DRIFT,\n",
    "                    severity=severity,\n",
    "                    feature_name=f\"Feature_{i}\",\n",
    "                    baseline_value=baseline_mean,\n",
    "                    current_value=current_mean,\n",
    "                    drift_score=z_score,\n",
    "                    threshold=threshold,\n",
    "                    recommendation=f\"Feature {i} drifted {z_score:.2f} std devs. Review data source or retrain model.\"\n",
    "                )\n",
    "                \n",
    "                drift_alerts.append(alert)\n",
    "                self.alerts.append(alert)\n",
    "        \n",
    "        return drift_alerts\n",
    "    \n",
    "    def detect_prediction_drift(self, current_predictions: np.ndarray,\n",
    "                               threshold: float = 2.0) -> List[DriftAlert]:\n",
    "        \"\"\"Detect drift in prediction distribution\"\"\"\n",
    "        drift_alerts = []\n",
    "        \n",
    "        baseline_pred_mean = self.baseline_predictions.mean()\n",
    "        baseline_pred_std = self.baseline_predictions.std()\n",
    "        \n",
    "        current_pred_mean = current_predictions.mean()\n",
    "        \n",
    "        # Z-score for prediction mean\n",
    "        if baseline_pred_std > 0:\n",
    "            z_score = abs(current_pred_mean - baseline_pred_mean) / baseline_pred_std\n",
    "        else:\n",
    "            z_score = 0\n",
    "        \n",
    "        if z_score > threshold:\n",
    "            severity = \"critical\" if z_score > 4 else \"high\" if z_score > 3 else \"medium\"\n",
    "            \n",
    "            alert = DriftAlert(\n",
    "                alert_id=f\"drift_{len(self.alerts) + 1}\",\n",
    "                drift_type=DriftType.CONCEPT_DRIFT,\n",
    "                severity=severity,\n",
    "                metric_name=\"prediction_mean\",\n",
    "                baseline_value=baseline_pred_mean,\n",
    "                current_value=current_pred_mean,\n",
    "                drift_score=z_score,\n",
    "                threshold=threshold,\n",
    "                recommendation=f\"Prediction distribution shifted. Possible concept drift. Retrain model with recent data.\"\n",
    "            )\n",
    "            \n",
    "            drift_alerts.append(alert)\n",
    "            self.alerts.append(alert)\n",
    "        \n",
    "        return drift_alerts\n",
    "    \n",
    "    def track_performance(self, y_true: np.ndarray, y_pred: np.ndarray,\n",
    "                         latency_ms: float, throughput_qps: float):\n",
    "        \"\"\"Track model performance metrics\"\"\"\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        performance = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'latency_ms': latency_ms,\n",
    "            'throughput_qps': throughput_qps\n",
    "        }\n",
    "        \n",
    "        self.performance_history.append(performance)\n",
    "        \n",
    "        return performance\n",
    "    \n",
    "    def check_performance_degradation(self, current_rmse: float, \n",
    "                                     baseline_rmse: float,\n",
    "                                     threshold_pct: float = 20.0) -> Optional[DriftAlert]:\n",
    "        \"\"\"Check if performance degraded beyond threshold\"\"\"\n",
    "        degradation_pct = ((current_rmse - baseline_rmse) / baseline_rmse) * 100\n",
    "        \n",
    "        if degradation_pct > threshold_pct:\n",
    "            severity = \"critical\" if degradation_pct > 50 else \"high\" if degradation_pct > 30 else \"medium\"\n",
    "            \n",
    "            alert = DriftAlert(\n",
    "                alert_id=f\"drift_{len(self.alerts) + 1}\",\n",
    "                drift_type=DriftType.PERFORMANCE_DRIFT,\n",
    "                severity=severity,\n",
    "                metric_name=\"rmse\",\n",
    "                baseline_value=baseline_rmse,\n",
    "                current_value=current_rmse,\n",
    "                drift_score=degradation_pct,\n",
    "                threshold=threshold_pct,\n",
    "                recommendation=f\"RMSE increased {degradation_pct:.1f}%. Trigger retraining pipeline immediately.\"\n",
    "            )\n",
    "            \n",
    "            self.alerts.append(alert)\n",
    "            return alert\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_alerts(self, severity: Optional[str] = None) -> List[DriftAlert]:\n",
    "        \"\"\"Get drift alerts (optionally filtered by severity)\"\"\"\n",
    "        if severity:\n",
    "            return [a for a in self.alerts if a.severity == severity]\n",
    "        return self.alerts\n",
    "\n",
    "# Example: Production Monitoring\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Production Monitoring - Drift Detection and Alerts\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Setup: Use original test data as baseline\n",
    "baseline_data = X_test\n",
    "baseline_model = best_exp.artifacts['model']\n",
    "baseline_predictions = baseline_model.predict(baseline_data)\n",
    "\n",
    "monitor = ProductionMonitor(\n",
    "    model_name=\"YieldPredictor\",\n",
    "    baseline_data=baseline_data,\n",
    "    baseline_predictions=baseline_predictions\n",
    ")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Baseline Statistics:\")\n",
    "print(f\"   Data samples: {len(baseline_data)}\")\n",
    "print(f\"   Features: {baseline_data.shape[1]}\")\n",
    "print(f\"   Prediction mean: {baseline_predictions.mean():.2f}%\")\n",
    "print(f\"   Prediction std: {baseline_predictions.std():.2f}%\")\n",
    "\n",
    "# Scenario 1: Normal production data (no drift)\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Scenario 1: Normal Production Data (No Drift)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Generate data similar to baseline\n",
    "np.random.seed(50)\n",
    "normal_data = X_test[:50] + np.random.randn(50, 5) * 0.1  # Small noise\n",
    "normal_predictions = baseline_model.predict(normal_data)\n",
    "\n",
    "data_drift_alerts = monitor.detect_data_drift(normal_data, threshold=3.0)\n",
    "pred_drift_alerts = monitor.detect_prediction_drift(normal_predictions, threshold=2.0)\n",
    "\n",
    "print(f\"\\n\ud83d\udd0d Drift Detection Results:\")\n",
    "print(f\"   Data drift alerts: {len(data_drift_alerts)}\")\n",
    "print(f\"   Prediction drift alerts: {len(pred_drift_alerts)}\")\n",
    "\n",
    "if not data_drift_alerts and not pred_drift_alerts:\n",
    "    print(f\"   \u2705 No drift detected - Model performing normally\")\n",
    "\n",
    "# Track performance\n",
    "perf = monitor.track_performance(\n",
    "    y_true=y_test[:50],\n",
    "    y_pred=normal_predictions,\n",
    "    latency_ms=25.0,\n",
    "    throughput_qps=100.0\n",
    ")\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Performance Metrics:\")\n",
    "print(f\"   RMSE: {perf['rmse']:.3f}%\")\n",
    "print(f\"   MAE: {perf['mae']:.3f}%\")\n",
    "print(f\"   R\u00b2: {perf['r2']:.3f}\")\n",
    "print(f\"   Latency: {perf['latency_ms']:.1f}ms\")\n",
    "print(f\"   Throughput: {perf['throughput_qps']:.0f} QPS\")\n",
    "\n",
    "# Scenario 2: Data drift detected (temperature sensor recalibrated)\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Scenario 2: Data Drift (Temperature Sensor Recalibrated)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simulate temperature drift (feature 3 shifted by 10\u00b0C)\n",
    "drifted_data = X_test[:50].copy()\n",
    "drifted_data[:, 3] += 10.0  # Temperature increased by 10\u00b0C\n",
    "drifted_predictions = baseline_model.predict(drifted_data)\n",
    "\n",
    "print(f\"\\n\ud83d\udea8 Temperature sensor recalibration detected:\")\n",
    "print(f\"   Baseline temperature mean: {X_test[:50, 3].mean():.2f}\u00b0C\")\n",
    "print(f\"   Current temperature mean: {drifted_data[:, 3].mean():.2f}\u00b0C\")\n",
    "print(f\"   Shift: +{drifted_data[:, 3].mean() - X_test[:50, 3].mean():.2f}\u00b0C\")\n",
    "\n",
    "data_drift_alerts = monitor.detect_data_drift(drifted_data, threshold=3.0)\n",
    "pred_drift_alerts = monitor.detect_prediction_drift(drifted_predictions, threshold=2.0)\n",
    "\n",
    "print(f\"\\n\ud83d\udd0d Drift Detection Results:\")\n",
    "print(f\"   Data drift alerts: {len(data_drift_alerts)} \u26a0\ufe0f\")\n",
    "print(f\"   Prediction drift alerts: {len(pred_drift_alerts)}\")\n",
    "\n",
    "for alert in data_drift_alerts:\n",
    "    print(f\"\\n   Alert ID: {alert.alert_id}\")\n",
    "    print(f\"   Type: {alert.drift_type.value}\")\n",
    "    print(f\"   Severity: {alert.severity.upper()}\")\n",
    "    print(f\"   Feature: {alert.feature_name}\")\n",
    "    print(f\"   Baseline value: {alert.baseline_value:.3f}\")\n",
    "    print(f\"   Current value: {alert.current_value:.3f}\")\n",
    "    print(f\"   Drift score: {alert.drift_score:.2f} std devs (threshold: {alert.threshold})\")\n",
    "    print(f\"   Recommendation: {alert.recommendation}\")\n",
    "\n",
    "# Scenario 3: Performance degradation (concept drift)\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Scenario 3: Performance Degradation (Concept Drift)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simulate concept drift: relationship between features and target changed\n",
    "np.random.seed(75)\n",
    "concept_drift_data = X_test[:50].copy()\n",
    "\n",
    "# Generate new target with different relationship (new chip design)\n",
    "concept_drift_true = (60 + 40 * concept_drift_data[:, 0] +  # Vdd importance increased\n",
    "                     10 * concept_drift_data[:, 1] -         # Idd importance decreased\n",
    "                     0.05 * concept_drift_data[:, 2] +\n",
    "                     1 * concept_drift_data[:, 3] +\n",
    "                     0.03 * concept_drift_data[:, 4] +\n",
    "                     np.random.randn(50) * 2)\n",
    "\n",
    "# Predictions with old model (trained on old relationship)\n",
    "concept_drift_predictions = baseline_model.predict(concept_drift_data)\n",
    "\n",
    "print(f\"\\n\u26a0\ufe0f New chip design deployed:\")\n",
    "print(f\"   Vdd sensitivity increased (30 \u2192 40)\")\n",
    "print(f\"   Idd sensitivity decreased (20 \u2192 10)\")\n",
    "print(f\"   Old model trained on previous chip design\")\n",
    "\n",
    "# Track performance\n",
    "perf_degraded = monitor.track_performance(\n",
    "    y_true=concept_drift_true,\n",
    "    y_pred=concept_drift_predictions,\n",
    "    latency_ms=27.0,\n",
    "    throughput_qps=95.0\n",
    ")\n",
    "\n",
    "baseline_rmse = perf['rmse']\n",
    "current_rmse = perf_degraded['rmse']\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Performance Comparison:\")\n",
    "print(f\"   Baseline RMSE: {baseline_rmse:.3f}%\")\n",
    "print(f\"   Current RMSE: {current_rmse:.3f}%\")\n",
    "print(f\"   Degradation: {((current_rmse - baseline_rmse) / baseline_rmse * 100):.1f}%\")\n",
    "\n",
    "degradation_alert = monitor.check_performance_degradation(\n",
    "    current_rmse=current_rmse,\n",
    "    baseline_rmse=baseline_rmse,\n",
    "    threshold_pct=20.0\n",
    ")\n",
    "\n",
    "if degradation_alert:\n",
    "    print(f\"\\n\ud83d\udea8 Performance Degradation Alert:\")\n",
    "    print(f\"   Alert ID: {degradation_alert.alert_id}\")\n",
    "    print(f\"   Type: {degradation_alert.drift_type.value}\")\n",
    "    print(f\"   Severity: {degradation_alert.severity.upper()}\")\n",
    "    print(f\"   Metric: {degradation_alert.metric_name}\")\n",
    "    print(f\"   Baseline RMSE: {degradation_alert.baseline_value:.3f}%\")\n",
    "    print(f\"   Current RMSE: {degradation_alert.current_value:.3f}%\")\n",
    "    print(f\"   Degradation: {degradation_alert.drift_score:.1f}%\")\n",
    "    print(f\"   Recommendation: {degradation_alert.recommendation}\")\n",
    "    print(f\"\\n   \ud83d\udd04 Action: Triggering CI/CD pipeline for model retraining...\")\n",
    "\n",
    "# Alert summary\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Alert Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_alerts = monitor.get_alerts()\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Total alerts: {len(all_alerts)}\")\n",
    "\n",
    "# Group by severity\n",
    "critical_alerts = monitor.get_alerts(severity=\"critical\")\n",
    "high_alerts = monitor.get_alerts(severity=\"high\")\n",
    "medium_alerts = monitor.get_alerts(severity=\"medium\")\n",
    "\n",
    "print(f\"\\n   Critical: {len(critical_alerts)}\")\n",
    "print(f\"   High: {len(high_alerts)}\")\n",
    "print(f\"   Medium: {len(medium_alerts)}\")\n",
    "\n",
    "# Alert types\n",
    "data_drift_count = sum(1 for a in all_alerts if a.drift_type == DriftType.DATA_DRIFT)\n",
    "concept_drift_count = sum(1 for a in all_alerts if a.drift_type == DriftType.CONCEPT_DRIFT)\n",
    "perf_drift_count = sum(1 for a in all_alerts if a.drift_type == DriftType.PERFORMANCE_DRIFT)\n",
    "\n",
    "print(f\"\\n   Data Drift: {data_drift_count}\")\n",
    "print(f\"   Concept Drift: {concept_drift_count}\")\n",
    "print(f\"   Performance Drift: {perf_drift_count}\")\n",
    "\n",
    "# Business value\n",
    "\n",
    "print(f\"\\n\\n{'=' * 80}\")\n",
    "print(\"Business Value\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Early detection value\n",
    "manual_monitoring_hours = 160  # 1 FTE per month\n",
    "automated_monitoring_cost = 5  # hours per month (maintenance)\n",
    "monitoring_time_saved = manual_monitoring_hours - automated_monitoring_cost\n",
    "engineer_cost = 150  # USD per hour\n",
    "monthly_monitoring_savings = monitoring_time_saved * engineer_cost\n",
    "annual_monitoring_savings = monthly_monitoring_savings * 12\n",
    "\n",
    "# Prevented bad decisions\n",
    "drift_detection_time = 1  # hour (automated)\n",
    "manual_detection_time = 7 * 24  # 1 week (noticed in production)\n",
    "bad_decisions_prevented = 12  # per year\n",
    "cost_per_bad_decision = 500000  # USD (bad wafer scrapped)\n",
    "early_detection_value = bad_decisions_prevented * cost_per_bad_decision\n",
    "\n",
    "total_value = annual_monitoring_savings + early_detection_value\n",
    "\n",
    "print(f\"\\n\ud83d\udcb0 Production Monitoring Value:\")\n",
    "print(f\"   Manual monitoring: {manual_monitoring_hours} hours/month\")\n",
    "print(f\"   Automated monitoring: {automated_monitoring_cost} hours/month\")\n",
    "print(f\"   Time saved: {monitoring_time_saved} hours/month\")\n",
    "print(f\"   Annual monitoring savings: ${annual_monitoring_savings / 1e6:.1f}M\")\n",
    "print(f\"\\n   Drift detection time: {drift_detection_time} hour (vs {manual_detection_time / 24:.0f} days manual)\")\n",
    "print(f\"   Bad decisions prevented: {bad_decisions_prevented}/year\")\n",
    "print(f\"   Early detection value: ${early_detection_value / 1e6:.1f}M/year\")\n",
    "print(f\"\\n   Total annual value: ${total_value / 1e6:.1f}M\")\n",
    "\n",
    "print(f\"\\n\u2705 Production monitoring validated!\")\n",
    "print(f\"\u2705 {len(all_alerts)} drift alerts detected\")\n",
    "print(f\"\u2705 3 drift types monitored (data, concept, performance)\")\n",
    "print(f\"\u2705 ${total_value / 1e6:.1f}M/year business value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c757dd",
   "metadata": {},
   "source": [
    "## 6. \ud83d\ude80 Real-World MLOps Projects\n",
    "\n",
    "Each project includes clear objectives, business value, and implementation guidance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Post-Silicon Validation Projects** ($26.7M/year total value)\n",
    "\n",
    "#### **Project 1: Automated Yield Prediction Model Retraining Pipeline** ($8.5M/year)\n",
    "**Objective:** Build end-to-end MLOps pipeline that automatically retrains yield prediction models when new wafer test data arrives daily.\n",
    "\n",
    "**Business Value:** Prevent model staleness that causes 15% accuracy degradation over 2 weeks, leading to bad wafer disposition decisions ($8.5M/year in prevented waste).\n",
    "\n",
    "**Features:**\n",
    "- Data validation stage (check STDF file integrity, test parameter ranges)\n",
    "- Automated training trigger (new data detected via S3 event or cron)\n",
    "- Model evaluation (compare new model RMSE to production baseline)\n",
    "- Automatic promotion to staging if RMSE improves by >5%\n",
    "- Rollback capability if staging tests fail\n",
    "\n",
    "**Tech Stack:** MLflow (tracking), DVC (data versioning), Kubeflow Pipelines (orchestration), Airflow (scheduling), S3 (STDF storage)\n",
    "\n",
    "**Success Metrics:** \n",
    "- Model retrained within 2 hours of new data arrival\n",
    "- Zero manual intervention (100% automated)\n",
    "- RMSE consistently <2% (vs 3% with manual retraining)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 2: Multi-Model Experiment Tracking System** ($6.3M/year)\n",
    "**Objective:** Build centralized experiment tracking for 20+ data scientists running 500+ experiments/month across yield prediction, test time optimization, and binning models.\n",
    "\n",
    "**Business Value:** Reduce experimentation time by 60% (2 hours \u2192 48 minutes per experiment), enabling faster model improvements ($6.3M/year in productivity gains).\n",
    "\n",
    "**Features:**\n",
    "- Centralized experiment tracker (MLflow or Weights & Biases)\n",
    "- Automatic logging (hyperparameters, metrics, artifacts, environment)\n",
    "- Experiment comparison UI (compare 10 experiments side-by-side)\n",
    "- Best model selection (auto-select by lowest RMSE, highest R\u00b2)\n",
    "- Reproducibility (capture code version, data hash, random seed)\n",
    "\n",
    "**Tech Stack:** MLflow, PostgreSQL (backend store), S3 (artifact store), Git (code versioning), DVC (data versioning)\n",
    "\n",
    "**Success Metrics:**\n",
    "- 500+ experiments tracked per month\n",
    "- 100% reproducibility (any experiment can be rerun identically)\n",
    "- <5 seconds experiment query time\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 3: Model Registry with Stage-Based Promotion** ($4.7M/year)\n",
    "**Objective:** Build model registry that manages 50+ models (yield, test time, binning) with None\u2192Staging\u2192Production\u2192Archived lifecycle and <30 second rollback capability.\n",
    "\n",
    "**Business Value:** Reduce deployment incidents by 80% (50 \u2192 10 per year) via mandatory staging validation, preventing $4.7M/year in bad model costs.\n",
    "\n",
    "**Features:**\n",
    "- Model versioning (semantic versioning: v1.2.3)\n",
    "- Stage-based promotion workflow (manual approval for prod)\n",
    "- Metadata storage (experiment ID, performance metrics, owner, deployment history)\n",
    "- Rollback capability (demote bad model, promote previous version)\n",
    "- Access control (only ML engineers can promote to prod)\n",
    "\n",
    "**Tech Stack:** MLflow Model Registry, PostgreSQL, CI/CD integration (GitHub Actions), RBAC (Okta)\n",
    "\n",
    "**Success Metrics:**\n",
    "- <30 seconds rollback time (vs 4 hours manual)\n",
    "- 100% staging validation before prod deployment\n",
    "- Zero unauthorized model promotions\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 4: Production Model Monitoring and Drift Alerts** ($7.2M/year)\n",
    "**Objective:** Monitor 20 production models for data drift, concept drift, and performance degradation with automated alerts and retraining triggers.\n",
    "\n",
    "**Business Value:** Detect model staleness in <1 hour (vs 1 week manual monitoring), preventing $7.2M/year in bad wafer disposition from drifted models.\n",
    "\n",
    "**Features:**\n",
    "- Data drift detection (Kolmogorov-Smirnov test on feature distributions)\n",
    "- Concept drift detection (PSI on prediction distribution)\n",
    "- Performance monitoring (RMSE, MAE, R\u00b2, latency, throughput)\n",
    "- Alert system (Slack/PagerDuty when drift detected)\n",
    "- Auto-retraining trigger (when RMSE degrades >20%)\n",
    "\n",
    "**Tech Stack:** Evidently AI, Prometheus (metrics), Grafana (dashboards), Slack API, Airflow (retraining orchestration)\n",
    "\n",
    "**Success Metrics:**\n",
    "- <1 hour drift detection time\n",
    "- Zero false positives (precision >95%)\n",
    "- Automated retraining triggered within 2 hours of drift\n",
    "\n",
    "---\n",
    "\n",
    "### **General AI/ML Projects** ($31.8M/year total value)\n",
    "\n",
    "#### **Project 5: E-Commerce Product Recommendation MLOps Pipeline** ($9.2M/year)\n",
    "**Objective:** Build MLOps pipeline for collaborative filtering recommendation model serving 10M+ users, retraining nightly on new user interaction data.\n",
    "\n",
    "**Business Value:** Increase conversion rate by 18% via fresh recommendations (model retrained on yesterday's clicks/purchases), driving $9.2M/year additional revenue.\n",
    "\n",
    "**Features:**\n",
    "- Nightly training pipeline (process 50M+ interaction events)\n",
    "- A/B testing framework (compare new model vs production)\n",
    "- Feature store (pre-computed user embeddings, item embeddings)\n",
    "- Real-time serving (<50ms P95 latency for top-10 recommendations)\n",
    "- Champion/Challenger deployment (gradual rollout 10% \u2192 50% \u2192 100%)\n",
    "\n",
    "**Tech Stack:** Kubeflow, Feast (feature store), BentoML (serving), Prometheus, Grafana, Seldon Core\n",
    "\n",
    "**Success Metrics:**\n",
    "- <12 hours training time (on 50M events)\n",
    "- <50ms recommendation latency\n",
    "- 18% conversion rate improvement\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 6: Fraud Detection Model Monitoring with Real-Time Drift** ($6.8M/year)\n",
    "**Objective:** Monitor fraud detection model in real-time (streaming transactions) for data drift and concept drift (fraudsters change tactics), with <5 minute retraining trigger.\n",
    "\n",
    "**Business Value:** Reduce fraud loss by 25% via real-time drift detection and rapid retraining when fraud patterns change ($6.8M/year savings).\n",
    "\n",
    "**Features:**\n",
    "- Streaming drift detection (Kafka + Flink for real-time analysis)\n",
    "- Concept drift detection (fraud pattern shifts detected via prediction distribution change)\n",
    "- Automated retraining (when drift detected, trigger pipeline within 5 minutes)\n",
    "- Shadow deployment (new model processes traffic but doesn't affect decisions until validated)\n",
    "- Real-time dashboards (drift scores, fraud detection rate, false positive rate)\n",
    "\n",
    "**Tech Stack:** Kafka, Flink, MLflow, Evidently AI, Kubernetes, Seldon Core\n",
    "\n",
    "**Success Metrics:**\n",
    "- <5 minute drift detection latency\n",
    "- <10 minute retraining trigger time\n",
    "- 25% fraud loss reduction\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 7: Multi-Model Registry for Healthcare Diagnostic Models** ($8.3M/year)\n",
    "**Objective:** Build HIPAA-compliant model registry managing 100+ diagnostic models (chest X-ray, diabetic retinopathy, etc.) with audit trails and versioning.\n",
    "\n",
    "**Business Value:** Enable faster regulatory approval (FDA submission) via comprehensive audit trails, accelerating time-to-market by 6 months ($8.3M/year NPV).\n",
    "\n",
    "**Features:**\n",
    "- HIPAA-compliant artifact storage (encrypted S3, access logging)\n",
    "- Audit trail (every model access, prediction, promotion logged)\n",
    "- Model lineage tracking (dataset \u2192 preprocessing \u2192 training \u2192 deployment)\n",
    "- Regulatory report generation (performance metrics, validation results, bias analysis)\n",
    "- Immutable versioning (models cannot be overwritten, only archived)\n",
    "\n",
    "**Tech Stack:** MLflow, AWS S3 (encrypted), PostgreSQL (encrypted), Vault (secrets), CloudWatch (audit logs)\n",
    "\n",
    "**Success Metrics:**\n",
    "- 100% audit trail coverage\n",
    "- <2 days regulatory report generation (vs 2 weeks manual)\n",
    "- Zero HIPAA violations\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 8: LLM Fine-Tuning Experiment Tracking and Versioning** ($7.5M/year)\n",
    "**Objective:** Track 200+ LLM fine-tuning experiments (GPT-4, Llama 2) across customer support, code generation, and summarization tasks with automatic best-model selection.\n",
    "\n",
    "**Business Value:** Reduce LLM experimentation cost by 40% (avoid redundant experiments) and improve model quality (select best from 200 experiments), driving $7.5M/year efficiency gains.\n",
    "\n",
    "**Features:**\n",
    "- Large artifact storage (multi-GB model checkpoints in S3)\n",
    "- Experiment comparison (compare perplexity, BLEU, human eval scores)\n",
    "- Hyperparameter tracking (learning rate, batch size, LoRA rank, quantization)\n",
    "- Cost tracking (GPU hours, API costs per experiment)\n",
    "- Automatic best-model selection (by task-specific metric)\n",
    "\n",
    "**Tech Stack:** Weights & Biases, S3, MLflow, Hugging Face Hub, Ray Train (distributed training)\n",
    "\n",
    "**Success Metrics:**\n",
    "- 200+ experiments tracked per month\n",
    "- 40% cost reduction (avoid redundant runs)\n",
    "- Best model auto-selected (highest BLEU score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a8a3b",
   "metadata": {},
   "source": [
    "## 7. \ud83c\udfaf Key Takeaways\n",
    "\n",
    "### **MLOps vs Traditional Software Development**\n",
    "\n",
    "**Critical Differences:**\n",
    "- **Artifacts**: Code + Data + Models (vs just Code)\n",
    "- **Testing**: Data quality + Model performance + Code correctness (vs just Code tests)\n",
    "- **Deployment**: Model serving + versioning + monitoring (vs just Blue-green deployments)\n",
    "- **Maintenance**: Model retraining + drift monitoring + data pipelines (vs just Bug fixes)\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use MLOps**\n",
    "\n",
    "\u2705 **Perfect For:**\n",
    "- **Production ML systems** requiring continuous retraining (yield prediction, fraud detection, recommendations)\n",
    "- **Multiple models** in production (20+ models, need centralized tracking)\n",
    "- **Regulatory compliance** (FDA, automotive safety, finance - need audit trails)\n",
    "- **Data drift** scenarios (input distributions change over time)\n",
    "- **Team collaboration** (10+ data scientists sharing experiments)\n",
    "\n",
    "\u274c **Not Ideal For:**\n",
    "- **One-off analysis** (exploratory notebooks, research projects)\n",
    "- **Static models** (trained once, never updated - e.g., historical analysis)\n",
    "- **Small teams** (<3 people, overhead outweighs benefits)\n",
    "- **Prototype phase** (before product-market fit, premature optimization)\n",
    "\n",
    "---\n",
    "\n",
    "### **MLOps Maturity Levels**\n",
    "\n",
    "**Level 0: Manual Process** \n",
    "- Manual training, manual deployment, no versioning\n",
    "- Good for: Prototypes, research\n",
    "- Risk: Not reproducible, no rollback\n",
    "\n",
    "**Level 1: ML Pipeline Automation**\n",
    "- Automated training pipeline (triggered manually)\n",
    "- Model versioning, experiment tracking\n",
    "- Good for: Small teams, low model update frequency\n",
    "- Risk: No continuous training, manual deployment\n",
    "\n",
    "**Level 2: CI/CD Pipeline Automation**\n",
    "- Automated training + testing + deployment\n",
    "- Continuous monitoring, automated rollback\n",
    "- Good for: Production systems, frequent updates\n",
    "- Risk: Complexity overhead for simple use cases\n",
    "\n",
    "**Level 3: Full MLOps (This Notebook)**\n",
    "- Automated retraining on data drift\n",
    "- Multi-stage deployment (dev/staging/prod)\n",
    "- Advanced monitoring (drift detection, performance tracking)\n",
    "- Good for: Large-scale production ML (Netflix, Uber, Amazon)\n",
    "\n",
    "---\n",
    "\n",
    "### **Best Practices**\n",
    "\n",
    "**Experiment Tracking:**\n",
    "- \u2705 **DO**: Log every experiment (even failures teach you what doesn't work)\n",
    "- \u2705 **DO**: Track environment (Python version, library versions, random seed)\n",
    "- \u2705 **DO**: Version your data (DVC, data hash) for reproducibility\n",
    "- \u274c **DON'T**: Only log successful experiments (selection bias)\n",
    "- \u274c **DON'T**: Forget to log hyperparameters (can't reproduce results)\n",
    "\n",
    "**Model Registry:**\n",
    "- \u2705 **DO**: Use semantic versioning (v1.2.3: major.minor.patch)\n",
    "- \u2705 **DO**: Test in staging before production (catch 99% of issues)\n",
    "- \u2705 **DO**: Implement rollback capability (<1 minute rollback time)\n",
    "- \u274c **DON'T**: Deploy directly to production (skip staging)\n",
    "- \u274c **DON'T**: Delete old model versions (keep for rollback)\n",
    "\n",
    "**CI/CD Pipeline:**\n",
    "- \u2705 **DO**: Automate everything (data validation \u2192 deployment)\n",
    "- \u2705 **DO**: Run smoke tests after deployment (basic sanity checks)\n",
    "- \u2705 **DO**: Set quality thresholds (e.g., RMSE <2%, R\u00b2 >0.7)\n",
    "- \u274c **DON'T**: Allow pipeline to continue if tests fail (fail fast)\n",
    "- \u274c **DON'T**: Skip data validation (garbage in = garbage out)\n",
    "\n",
    "**Production Monitoring:**\n",
    "- \u2705 **DO**: Monitor data drift AND concept drift (both matter)\n",
    "- \u2705 **DO**: Set up automated alerts (Slack, PagerDuty)\n",
    "- \u2705 **DO**: Trigger retraining when drift detected (automation is key)\n",
    "- \u274c **DON'T**: Only monitor accuracy (latency, throughput also matter)\n",
    "- \u274c **DON'T**: Ignore prediction distribution shifts (concept drift indicator)\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Pitfalls and Solutions**\n",
    "\n",
    "**Pitfall 1: Model-Code Skew**\n",
    "- **Problem**: Training code differs from serving code (preprocessing mismatch)\n",
    "- **Solution**: Use same codebase for training and serving (unify feature engineering)\n",
    "- **Tools**: Feature stores (Feast, Tecton) ensure consistency\n",
    "\n",
    "**Pitfall 2: Data Leakage in Pipelines**\n",
    "- **Problem**: Test data accidentally used in training (overly optimistic metrics)\n",
    "- **Solution**: Strict train/test split, validate data lineage\n",
    "- **Tools**: DVC pipelines track data splits\n",
    "\n",
    "**Pitfall 3: Silent Model Degradation**\n",
    "- **Problem**: Model accuracy drops over months, undetected\n",
    "- **Solution**: Continuous monitoring, automated drift alerts\n",
    "- **Tools**: Evidently AI, Fiddler\n",
    "\n",
    "**Pitfall 4: Experiment Chaos**\n",
    "- **Problem**: 500 experiments, can't find the best model 6 months later\n",
    "- **Solution**: Centralized experiment tracking with metadata\n",
    "- **Tools**: MLflow, Weights & Biases\n",
    "\n",
    "**Pitfall 5: Deployment Downtime**\n",
    "- **Problem**: 2-hour downtime during model redeployment\n",
    "- **Solution**: Blue-green deployment, canary releases\n",
    "- **Tools**: Kubernetes, Seldon Core\n",
    "\n",
    "**Pitfall 6: Unreproducible Results**\n",
    "- **Problem**: \"It worked on my laptop\" but fails in production\n",
    "- **Solution**: Containerize everything (Docker), version all dependencies\n",
    "- **Tools**: Docker, conda environments, requirements.txt\n",
    "\n",
    "**Pitfall 7: Data Versioning Nightmares**\n",
    "- **Problem**: \"Which dataset did we use for v1.2.3?\"\n",
    "- **Solution**: Version data with DVC, track data hash in experiment metadata\n",
    "- **Tools**: DVC, Git LFS\n",
    "\n",
    "---\n",
    "\n",
    "### **Production Checklist**\n",
    "\n",
    "Before deploying ML model to production, verify:\n",
    "\n",
    "**Data & Features:**\n",
    "- [ ] Data validation pipeline (check ranges, missing values, schema)\n",
    "- [ ] Feature engineering code tested (unit tests for transformations)\n",
    "- [ ] Data versioned (DVC, data hash tracked)\n",
    "- [ ] Feature store integrated (if using real-time features)\n",
    "\n",
    "**Model & Training:**\n",
    "- [ ] Experiment tracked (hyperparameters, metrics, artifacts logged)\n",
    "- [ ] Model meets quality thresholds (RMSE <X, R\u00b2 >Y)\n",
    "- [ ] Model registered in registry (with metadata)\n",
    "- [ ] Reproducibility verified (can retrain and get same results)\n",
    "\n",
    "**Deployment:**\n",
    "- [ ] Staging environment tested (smoke tests passed)\n",
    "- [ ] Rollback capability verified (<1 minute rollback time)\n",
    "- [ ] Blue-green or canary deployment (not big-bang)\n",
    "- [ ] Monitoring enabled (metrics, logs, alerts)\n",
    "\n",
    "**Monitoring & Maintenance:**\n",
    "- [ ] Data drift monitoring (feature distribution tracking)\n",
    "- [ ] Concept drift monitoring (prediction distribution tracking)\n",
    "- [ ] Performance monitoring (RMSE, latency, throughput)\n",
    "- [ ] Automated alerts configured (Slack, PagerDuty)\n",
    "- [ ] Retraining pipeline ready (triggered by drift or schedule)\n",
    "\n",
    "**Compliance & Governance:**\n",
    "- [ ] Audit trail enabled (all predictions logged for regulators)\n",
    "- [ ] Model lineage documented (data \u2192 preprocessing \u2192 training \u2192 deployment)\n",
    "- [ ] Access control configured (RBAC for model registry)\n",
    "- [ ] Bias and fairness tested (for regulated industries)\n",
    "\n",
    "---\n",
    "\n",
    "### **MLOps Tools & Technologies**\n",
    "\n",
    "**Experiment Tracking:**\n",
    "- **MLflow**: Open-source, Python-native, great for tracking experiments and models\n",
    "- **Weights & Biases (wandb)**: Best-in-class UI, team collaboration features\n",
    "- **Neptune.ai**: Metadata store, great for large teams\n",
    "- **TensorBoard**: PyTorch/TensorFlow native, visualization focus\n",
    "\n",
    "**Model Registry:**\n",
    "- **MLflow Model Registry**: Open-source, stage-based promotion (None/Staging/Production/Archived)\n",
    "- **SageMaker Model Registry**: AWS-native, integrates with SageMaker Pipelines\n",
    "- **Vertex AI Model Registry**: GCP-native, managed service\n",
    "\n",
    "**Data Versioning:**\n",
    "- **DVC (Data Version Control)**: Git for data, integrates with Git workflows\n",
    "- **Delta Lake**: Databricks, time travel for data tables\n",
    "- **LakeFS**: Git-like versioning for data lakes\n",
    "\n",
    "**Pipeline Orchestration:**\n",
    "- **Kubeflow Pipelines**: Kubernetes-native, containerized workflows\n",
    "- **Apache Airflow**: Python DAGs, great for scheduling\n",
    "- **Prefect**: Modern alternative to Airflow, easier error handling\n",
    "- **MLflow Projects**: Lightweight, good for simple pipelines\n",
    "\n",
    "**Model Serving:**\n",
    "- **Seldon Core**: Kubernetes-native, supports A/B testing, canary deployments\n",
    "- **BentoML**: Python-first, easy to containerize models\n",
    "- **TorchServe**: PyTorch native\n",
    "- **TensorFlow Serving**: TensorFlow native\n",
    "- **Ray Serve**: Distributed serving, great for LLMs\n",
    "\n",
    "**Monitoring:**\n",
    "- **Evidently AI**: Drift detection, open-source\n",
    "- **Fiddler**: Enterprise monitoring, root cause analysis\n",
    "- **Arize AI**: ML observability platform\n",
    "- **Prometheus + Grafana**: Metrics monitoring (can track model metrics)\n",
    "\n",
    "**Feature Stores:**\n",
    "- **Feast**: Open-source, lightweight\n",
    "- **Tecton**: Enterprise feature platform\n",
    "- **Hopsworks**: Open-source, supports streaming features\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "**Deepen Your MLOps Knowledge:**\n",
    "1. **Notebook 152**: Advanced Model Serving (A/B Testing, Canary Deployments, Multi-Armed Bandits)\n",
    "2. **Notebook 153**: Feature Stores and Real-Time ML (Feast, streaming features, low-latency serving)\n",
    "3. **Notebook 154**: ML Model Explainability and Debugging (SHAP, LIME, model debugging techniques)\n",
    "4. **Notebook 155**: Distributed Training and Hyperparameter Tuning (Ray, Optuna, multi-GPU training)\n",
    "\n",
    "**Build a Portfolio Project:**\n",
    "- Start with Project 2 (Multi-Model Experiment Tracking) - easy to build, high impact\n",
    "- Then Project 4 (Production Monitoring) - learn drift detection\n",
    "- Finally Project 1 (Automated Retraining Pipeline) - tie everything together\n",
    "\n",
    "**Learn by Doing:**\n",
    "- Deploy a real model to production (even if it's a personal project)\n",
    "- Set up MLflow on localhost, track 10 experiments\n",
    "- Simulate data drift, trigger automated retraining\n",
    "- Build a monitoring dashboard with Prometheus + Grafana\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "**MLOps is essential for:**\n",
    "- \ud83d\ude80 **Faster iteration** (automated pipelines vs manual deployment)\n",
    "- \u2705 **Higher reliability** (staging tests, rollback capability)\n",
    "- \ud83d\udcca **Better models** (experiment tracking enables systematic improvement)\n",
    "- \ud83d\udd0d **Early problem detection** (drift monitoring catches staleness in <1 hour)\n",
    "- \ud83d\udcb0 **Cost savings** ($26.7M/year in this notebook's use cases)\n",
    "\n",
    "**Start simple:**\n",
    "- Level 1: Just add experiment tracking (MLflow)\n",
    "- Level 2: Add model registry and basic CI/CD\n",
    "- Level 3: Add drift monitoring and automated retraining\n",
    "\n",
    "**Remember:**\n",
    "- MLOps is a journey, not a destination (start small, iterate)\n",
    "- Automation pays off (4 hours \u2192 10 minutes deployment time)\n",
    "- Monitoring prevents disasters (catch drift before model fails)\n",
    "\n",
    "---\n",
    "\n",
    "\ud83c\udf89 **Congratulations!** You've built a complete MLOps system with experiment tracking, model registry, CI/CD pipelines, and production monitoring. You're ready to deploy and maintain production ML systems at scale!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332bb8b",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Key Takeaways\n",
    "\n",
    "### When to Use MLOps\n",
    "- **Multiple models in production**: >3 models requiring consistent deployment, monitoring, retraining\n",
    "- **Frequent model updates**: Weekly/monthly retraining cycles (demand forecasting, fraud detection)\n",
    "- **Team collaboration**: Data scientists, ML engineers, DevOps working on shared models\n",
    "- **Compliance requirements**: Model versioning, audit trails, reproducibility (financial, healthcare)\n",
    "- **Business-critical predictions**: High-cost errors requiring reliability, observability (yield prediction, pricing)\n",
    "\n",
    "### Limitations\n",
    "- **Complexity overhead**: MLOps tooling (MLflow, Kubeflow, Airflow) has learning curve (2-3 months ramp-up)\n",
    "- **Infrastructure costs**: Dedicated MLOps platform (compute, storage, tools) = $50K-$500K/year\n",
    "- **Overkill for simple projects**: Single Jupyter notebook model doesn't need full MLOps pipeline\n",
    "- **Tool fragmentation**: 50+ MLOps tools, no single standard (vendor lock-in risk)\n",
    "\n",
    "### Alternatives\n",
    "- **Manual deployment**: Data scientist manually deploys model (works for 1-2 models, doesn't scale)\n",
    "- **Jupyter notebooks in production**: Run notebooks on schedule (fragile, hard to maintain)\n",
    "- **Generic CI/CD**: Use Jenkins/GitHub Actions without ML-specific features (no experiment tracking, model registry)\n",
    "- **Serverless ML**: Cloud AutoML, AWS SageMaker autopilot (less control, simpler)\n",
    "\n",
    "### Best Practices\n",
    "- **Experiment tracking**: Log hyperparameters, metrics, artifacts for every run (MLflow, Weights & Biases)\n",
    "- **Model registry**: Centralized storage with versioning, staging (dev/staging/prod), lineage\n",
    "- **Automated pipelines**: CI/CD for training, testing, deployment (Kubeflow Pipelines, Airflow)\n",
    "- **Monitoring**: Data drift, model performance, system metrics (latency, throughput)\n",
    "- **Reproducibility**: Pin dependencies (requirements.txt), containerize environments (Docker)\n",
    "- **Feature stores**: Centralized feature management for training-serving consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cda56f",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Diagnostic Checks Summary\n",
    "\n",
    "### Implementation Checklist\n",
    "\u2705 **Experiment Tracking (MLflow)**\n",
    "- Logging: Hyperparameters, metrics (accuracy, loss), artifacts (models, plots)\n",
    "- Tagging: Environment (dev/staging/prod), dataset version, git commit SHA\n",
    "- Comparison: Compare runs side-by-side, identify best hyperparameters\n",
    "- Reproducibility: Log random seeds, library versions, data snapshots\n",
    "\n",
    "\u2705 **Model Registry**\n",
    "- Versioning: Semantic versioning (1.0.0, 1.1.0, 2.0.0) for model releases\n",
    "- Staging: dev \u2192 staging \u2192 prod promotion workflow with approvals\n",
    "- Lineage: Track training data, code version, hyperparameters used\n",
    "- Metadata: Performance metrics, deployment timestamp, owner\n",
    "\n",
    "\u2705 **CI/CD Pipelines (Kubeflow/Airflow)**\n",
    "- Training pipeline: Data validation \u2192 feature engineering \u2192 model training \u2192 evaluation\n",
    "- Deployment pipeline: Model packaging \u2192 container build \u2192 staging deployment \u2192 prod deployment\n",
    "- Testing: Unit tests (code), integration tests (pipeline), model validation (accuracy >threshold)\n",
    "- Rollback: Automated rollback if production accuracy drops >10%\n",
    "\n",
    "\u2705 **Monitoring & Retraining**\n",
    "- Data drift: KS test, KL divergence on feature distributions (retrain if p<0.01)\n",
    "- Model performance: Track online metrics (accuracy when labels available)\n",
    "- Retraining triggers: Scheduled (weekly/monthly), performance-based (accuracy <threshold), drift-based\n",
    "- A/B testing: Champion-challenger comparison before full deployment\n",
    "\n",
    "### Quality Metrics\n",
    "- **Experiment reproducibility**: 100% of experiments can be reproduced from logged metadata\n",
    "- **Model deployment time**: <30min from \"promote to prod\" to live traffic\n",
    "- **Monitoring coverage**: 100% of production models have drift + performance monitoring\n",
    "- **Retraining frequency**: Meets business SLA (weekly for fast-changing domains, monthly for stable)\n",
    "\n",
    "### Post-Silicon Validation Applications\n",
    "**1. Yield Prediction MLOps Pipeline**\n",
    "- Experiment tracking: Log 50+ yield model experiments (XGBoost, LightGBM, Neural Nets)\n",
    "- Model registry: Prod model = XGBoost v3.2 (MAE 2.1%), staging = Neural Net v1.0 (MAE 2.3%)\n",
    "- CI/CD: Weekly retraining on latest 90 days of wafer test data\n",
    "- Monitoring: Alert if predicted yield distribution shifts >15% (possible data quality issue)\n",
    "- Business value: Systematize yield modeling, reduce deployment time 5 days \u2192 30min\n",
    "\n",
    "**2. Device Binning Model Lifecycle**\n",
    "- Experiment tracking: Compare binning algorithms (decision trees, logistic regression, NN)\n",
    "- Model registry: Track 3 models (premium-grade classifier, automotive-grade, low-power)\n",
    "- Retraining: Monthly on new device test data (capture performance drift over time)\n",
    "- A/B testing: Champion (current prod) vs. challenger (new model), 90/10 traffic split\n",
    "- Business value: $8M-$15M/year revenue optimization (better bin assignments), safe deployments\n",
    "\n",
    "**3. Test Time Prediction Pipeline**\n",
    "- CI/CD: Automated pipeline triggered on Git push to `main` branch\n",
    "  - Data validation: Check STDF files schema, outlier detection\n",
    "  - Feature engineering: Extract test sequence, device complexity metrics\n",
    "  - Model training: Train regression model, validate MAE <10% threshold\n",
    "  - Deployment: If validation passes, deploy to staging \u2192 prod after 24hr canary\n",
    "- Monitoring: Track prediction error, retrain if RMSE increases >20%\n",
    "- Business value: $4M-$8M/year capacity planning accuracy, automated model updates\n",
    "\n",
    "### Business ROI Estimation\n",
    "\n",
    "**Scenario 1: Medium-Volume Semiconductor Fab (100K wafers/year, 5 production models)**\n",
    "- Experiment tracking: 50% faster model iteration (1 week \u2192 3 days) = **$2.5M/year** time-to-value\n",
    "- Automated retraining: Weekly updates vs. manual quarterly = **$3M/year** better accuracy\n",
    "- CI/CD for models: 5 days \u2192 30min deployment = **$1.5M/year** engineering productivity\n",
    "- **Total ROI: $7M/year** (cost: $150K MLflow + Airflow + $200K team = $6.65M net)\n",
    "\n",
    "**Scenario 2: High-Volume Automotive Semiconductor (500K wafers/year, 20+ models)**\n",
    "- Model registry: Centralized management for 20 models = **$8M/year** operational efficiency\n",
    "- A/B testing: Safe model deployments prevent bad releases = **$15M/year** avoided revenue loss\n",
    "- Feature stores: Training-serving consistency = **$12M/year** reduced prediction errors\n",
    "- Compliance: Model versioning + audit trails for ISO 26262 = **$5M/year** audit cost savings\n",
    "- **Total ROI: $40M/year** (cost: $1M MLOps platform + $800K team = $38.2M net)\n",
    "\n",
    "**Scenario 3: Advanced Node R&D Fab (<10K wafers/year, experimental models)**\n",
    "- Experiment tracking: Organize 200+ R&D experiments = **$3M/year** research efficiency\n",
    "- Reproducibility: Recreate experiments 6 months later for publications = **$1.5M/year** IP value\n",
    "- Rapid prototyping: Deploy experimental models to test environments in <1hr = **$2.5M/year** faster learning\n",
    "- **Total ROI: $7M/year** (cost: $200K MLOps tools + $150K setup = $6.65M net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03247352",
   "metadata": {},
   "source": [
    "## \ud83d\udcc8 Progress Update\n",
    "\n",
    "**Notebook 151: MLOps Fundamentals** expanded from 11 \u2192 15 cells \u2705\n",
    "\n",
    "**Session summary: 12 notebooks completed**\n",
    "- 12-cell (5): 129, 133, 162, 163, 164\n",
    "- 11-cell (7): 111, 112, 116, 130, 138, 151\n",
    "\n",
    "**Current completion:** ~73% (128/175 notebooks)  \n",
    "**Remaining:** 47 partial notebooks\n",
    "\n",
    "Continuing with more 11-cell notebooks..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e09cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udf93 Mastery Achievement\n",
    "\n",
    "**You now have production-grade expertise in:**\n",
    "- \u2705 Tracking ML experiments with MLflow (hyperparameters, metrics, artifacts, versioning)\n",
    "- \u2705 Managing model lifecycle with registry (dev/staging/prod promotion, lineage tracking)\n",
    "- \u2705 Building CI/CD pipelines for ML (Kubeflow Pipelines, Airflow for training and deployment)\n",
    "- \u2705 Monitoring production models for data drift and performance degradation\n",
    "- \u2705 Implementing automated retraining and A/B testing for safe deployments\n",
    "\n",
    "**Next Steps:**\n",
    "- **Advanced MLOps**: Feature stores (Feast, Tecton), model serving (KServe, Seldon)\n",
    "- **ML Platform Engineering**: Multi-tenancy, resource quotas, cost optimization\n",
    "- **Continuous Training**: Event-driven retraining, federated learning across sites"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}