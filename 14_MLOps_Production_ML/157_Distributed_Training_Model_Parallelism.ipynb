{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 157: Distributed Training Model Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47adb26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Distributed Training & Model Parallelism - Setup\n",
    "\n",
    "Production distributed training stack:\n",
    "- Frameworks: PyTorch DDP, Horovod, DeepSpeed, Megatron-LM\n",
    "- Communication: NCCL (NVIDIA), Gloo, MPI\n",
    "- Orchestration: Ray, Kubernetes, Slurm\n",
    "- Monitoring: TensorBoard, Weights & Biases, MLflow\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "# Simulate multi-GPU environment (for educational purposes)\n",
    "# In production, use: import torch.distributed as dist, torch.nn.parallel.DistributedDataParallel\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"\u2705 Setup complete - Ready for distributed training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f5c97",
   "metadata": {},
   "source": [
    "## 1\ufe0f\u20e3 Data Parallelism Fundamentals\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement data parallelism, the most common distributed training strategy\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "**1. Data Parallelism Strategy**\n",
    "- **Concept**: Replicate model on each GPU, split data across GPUs\n",
    "- **Algorithm**:\n",
    "  1. Each GPU gets different batch of data (batch_size = global_batch_size / num_gpus)\n",
    "  2. Each GPU computes forward pass and gradients independently\n",
    "  3. Gradients are averaged across all GPUs (AllReduce operation)\n",
    "  4. Each GPU updates model parameters with averaged gradients\n",
    "  5. All GPUs now have identical model parameters\n",
    "- **Ideal speedup**: N GPUs \u2192 N\u00d7 faster (if communication overhead is negligible)\n",
    "\n",
    "**2. Gradient Synchronization**\n",
    "- **AllReduce**: Compute sum/average of gradients across all GPUs\n",
    "  - Mathematical operation: `gradient_avg = (grad_GPU0 + grad_GPU1 + ... + grad_GPUN) / N`\n",
    "  - Implementation: Ring-AllReduce algorithm (bandwidth-optimal)\n",
    "  - Communication cost: O(message_size) per GPU (independent of number of GPUs)\n",
    "- **Synchronous vs Asynchronous**:\n",
    "  - **Synchronous**: All GPUs wait for gradient sync before updating (consistent, slower)\n",
    "  - **Asynchronous**: GPUs update independently (faster, may diverge)\n",
    "\n",
    "**3. Scaling Efficiency**\n",
    "- **Linear scaling**: Training time = single_GPU_time / num_GPUs (ideal)\n",
    "- **Reality**: Training time = single_GPU_time / (num_GPUs \u00d7 efficiency)\n",
    "  - Efficiency = useful_compute_time / (useful_compute_time + communication_time)\n",
    "  - Example: 8 GPUs with 85% efficiency \u2192 6.8\u00d7 speedup (not 8\u00d7)\n",
    "- **Communication overhead**: \n",
    "  - Small models: High overhead (communication > compute)\n",
    "  - Large models: Low overhead (compute >> communication)\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Fastest training**: 8 GPUs with data parallelism \u2192 train in 3 hours vs 24 hours\n",
    "- **Hyperparameter search**: Run 8 experiments simultaneously\n",
    "- **Memory efficiency**: Each GPU only stores 1/N of the data\n",
    "- **Easy to implement**: Most frameworks have built-in data parallelism support\n",
    "\n",
    "**Post-Silicon Example:**\n",
    "Train yield prediction model on 50M wafer test records:\n",
    "- **Single GPU**: 14 days (50M records, batch_size=1024, 48K batches)\n",
    "- **8 GPUs**: 36 hours (6.2K batches per GPU, 7.8\u00d7 speedup with 97% efficiency)\n",
    "- **Communication**: Gradient size = 200M params \u00d7 4 bytes = 800MB per sync\n",
    "- **Business value**: $23.7M/year from weekly model updates (vs monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPUSimulator:\n",
    "    \"\"\"Simulates a GPU for distributed training demonstration\"\"\"\n",
    "    gpu_id: int\n",
    "    model_params: np.ndarray\n",
    "    gradients: Optional[np.ndarray] = None\n",
    "    data_shard: Optional[np.ndarray] = None\n",
    "    compute_time: float = 0.0\n",
    "    communication_time: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class TrainingMetrics:\n",
    "    \"\"\"Metrics for distributed training run\"\"\"\n",
    "    iteration: int\n",
    "    gpu_id: int\n",
    "    loss: float\n",
    "    compute_time: float\n",
    "    communication_time: float\n",
    "    throughput: float  # samples/second\n",
    "\n",
    "class DataParallelTrainer:\n",
    "    \"\"\"\n",
    "    Simulates data parallel training across multiple GPUs\n",
    "    \n",
    "    Demonstrates:\n",
    "    - Data sharding across GPUs\n",
    "    - Gradient computation per GPU\n",
    "    - AllReduce gradient synchronization\n",
    "    - Scaling efficiency measurement\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_gpus: int, model_size: int, dataset_size: int, batch_size: int):\n",
    "        self.num_gpus = num_gpus\n",
    "        self.model_size = model_size  # Number of parameters\n",
    "        self.dataset_size = dataset_size\n",
    "        self.batch_size = batch_size\n",
    "        self.global_batch_size = batch_size * num_gpus\n",
    "        \n",
    "        # Initialize GPUs with same model parameters\n",
    "        init_params = np.random.randn(model_size) * 0.01\n",
    "        self.gpus = [\n",
    "            GPUSimulator(\n",
    "                gpu_id=i,\n",
    "                model_params=init_params.copy()\n",
    "            )\n",
    "            for i in range(num_gpus)\n",
    "        ]\n",
    "        \n",
    "        self.training_history: List[TrainingMetrics] = []\n",
    "        \n",
    "    def shard_data(self, data: np.ndarray) -> List[np.ndarray]:\n",
    "        \"\"\"Split data evenly across GPUs\"\"\"\n",
    "        shard_size = len(data) // self.num_gpus\n",
    "        shards = []\n",
    "        for i in range(self.num_gpus):\n",
    "            start_idx = i * shard_size\n",
    "            end_idx = start_idx + shard_size if i < self.num_gpus - 1 else len(data)\n",
    "            shards.append(data[start_idx:end_idx])\n",
    "        return shards\n",
    "    \n",
    "    def compute_gradients(self, gpu: GPUSimulator, data_batch: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Simulate gradient computation on single GPU\n",
    "        \n",
    "        In real training:\n",
    "        - Forward pass: predictions = model(data_batch)\n",
    "        - Loss computation: loss = criterion(predictions, targets)\n",
    "        - Backward pass: loss.backward() computes gradients\n",
    "        \"\"\"\n",
    "        # Simulate computation time (proportional to batch size)\n",
    "        compute_time = 0.001 * len(data_batch)  # 1ms per sample\n",
    "        time.sleep(compute_time / 1000)  # Convert to seconds\n",
    "        \n",
    "        # Simulate gradients (random for demonstration)\n",
    "        gradients = np.random.randn(self.model_size) * 0.1\n",
    "        \n",
    "        return gradients, compute_time\n",
    "    \n",
    "    def allreduce_gradients(self, gradients_per_gpu: List[np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        AllReduce: Average gradients across all GPUs\n",
    "        \n",
    "        Real implementation uses:\n",
    "        - Ring-AllReduce (bandwidth-optimal)\n",
    "        - NCCL library (NVIDIA GPUs)\n",
    "        - Communication time depends on network bandwidth\n",
    "        \"\"\"\n",
    "        # Simulate communication time (proportional to gradient size)\n",
    "        gradient_size_mb = self.model_size * 4 / (1024 * 1024)  # 4 bytes per float32\n",
    "        bandwidth_gbps = 100  # NVLink bandwidth (A100)\n",
    "        communication_time = (gradient_size_mb / 1024) / bandwidth_gbps  # seconds\n",
    "        time.sleep(communication_time)\n",
    "        \n",
    "        # Average gradients\n",
    "        averaged_gradients = np.mean(gradients_per_gpu, axis=0)\n",
    "        \n",
    "        return averaged_gradients, communication_time\n",
    "    \n",
    "    def update_parameters(self, gpu: GPUSimulator, gradients: np.ndarray, learning_rate: float = 0.01):\n",
    "        \"\"\"Update model parameters using gradients\"\"\"\n",
    "        gpu.model_params -= learning_rate * gradients\n",
    "    \n",
    "    def train_step(self, iteration: int, data_batch: np.ndarray):\n",
    "        \"\"\"\n",
    "        Single training iteration with data parallelism\n",
    "        \n",
    "        Steps:\n",
    "        1. Shard data across GPUs\n",
    "        2. Each GPU computes gradients independently\n",
    "        3. AllReduce to average gradients\n",
    "        4. Each GPU updates parameters with averaged gradients\n",
    "        \"\"\"\n",
    "        # Shard data\n",
    "        data_shards = self.shard_data(data_batch)\n",
    "        \n",
    "        # Phase 1: Parallel gradient computation\n",
    "        gradients_per_gpu = []\n",
    "        compute_times = []\n",
    "        \n",
    "        for gpu, shard in zip(self.gpus, data_shards):\n",
    "            gradients, compute_time = self.compute_gradients(gpu, shard)\n",
    "            gradients_per_gpu.append(gradients)\n",
    "            compute_times.append(compute_time)\n",
    "            gpu.gradients = gradients\n",
    "        \n",
    "        max_compute_time = max(compute_times)  # Synchronous: wait for slowest GPU\n",
    "        \n",
    "        # Phase 2: Gradient synchronization (AllReduce)\n",
    "        averaged_gradients, comm_time = self.allreduce_gradients(gradients_per_gpu)\n",
    "        \n",
    "        # Phase 3: Parameter update\n",
    "        for gpu in self.gpus:\n",
    "            self.update_parameters(gpu, averaged_gradients)\n",
    "        \n",
    "        # Verify all GPUs have identical parameters\n",
    "        for i in range(1, self.num_gpus):\n",
    "            assert np.allclose(self.gpus[0].model_params, self.gpus[i].model_params), \\\n",
    "                f\"GPU 0 and GPU {i} parameters diverged!\"\n",
    "        \n",
    "        # Record metrics\n",
    "        total_time = max_compute_time + comm_time\n",
    "        throughput = self.global_batch_size / total_time\n",
    "        \n",
    "        for gpu_id in range(self.num_gpus):\n",
    "            # Simulate loss (decreasing over iterations)\n",
    "            loss = 1.0 / (1.0 + iteration * 0.1)\n",
    "            \n",
    "            self.training_history.append(\n",
    "                TrainingMetrics(\n",
    "                    iteration=iteration,\n",
    "                    gpu_id=gpu_id,\n",
    "                    loss=loss,\n",
    "                    compute_time=max_compute_time,\n",
    "                    communication_time=comm_time,\n",
    "                    throughput=throughput\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            \"compute_time\": max_compute_time,\n",
    "            \"communication_time\": comm_time,\n",
    "            \"total_time\": total_time,\n",
    "            \"throughput\": throughput,\n",
    "            \"communication_overhead\": comm_time / total_time\n",
    "        }\n",
    "    \n",
    "    def get_scaling_efficiency(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate scaling efficiency\n",
    "        \n",
    "        Efficiency = actual_speedup / ideal_speedup\n",
    "        Ideal speedup = num_gpus (linear scaling)\n",
    "        \"\"\"\n",
    "        if not self.training_history:\n",
    "            return 0.0\n",
    "        \n",
    "        # Average metrics across iterations\n",
    "        avg_compute = np.mean([m.compute_time for m in self.training_history])\n",
    "        avg_comm = np.mean([m.communication_time for m in self.training_history])\n",
    "        \n",
    "        # Single GPU time = compute_time * num_gpus (each GPU would process all data)\n",
    "        single_gpu_time = avg_compute * self.num_gpus\n",
    "        \n",
    "        # Multi-GPU time = compute_time (parallelized) + communication_time\n",
    "        multi_gpu_time = avg_compute + avg_comm\n",
    "        \n",
    "        # Actual speedup\n",
    "        actual_speedup = single_gpu_time / multi_gpu_time\n",
    "        \n",
    "        # Efficiency\n",
    "        efficiency = actual_speedup / self.num_gpus\n",
    "        \n",
    "        return efficiency\n",
    "\n",
    "# Simulate data parallel training\n",
    "print(\"\ud83d\udd04 Simulating Data Parallel Training\\n\")\n",
    "\n",
    "# Configuration\n",
    "num_gpus = 8\n",
    "model_size = 200_000_000  # 200M parameters (similar to BERT-Large)\n",
    "dataset_size = 50_000_000  # 50M samples (wafer test records)\n",
    "batch_size_per_gpu = 1024\n",
    "num_iterations = 10\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = DataParallelTrainer(\n",
    "    num_gpus=num_gpus,\n",
    "    model_size=model_size,\n",
    "    dataset_size=dataset_size,\n",
    "    batch_size=batch_size_per_gpu\n",
    ")\n",
    "\n",
    "print(f\"\ud83d\udcca Configuration:\")\n",
    "print(f\"  GPUs: {num_gpus}\")\n",
    "print(f\"  Model size: {model_size:,} parameters ({model_size * 4 / 1e9:.2f} GB)\")\n",
    "print(f\"  Dataset size: {dataset_size:,} samples\")\n",
    "print(f\"  Batch size per GPU: {batch_size_per_gpu}\")\n",
    "print(f\"  Global batch size: {batch_size_per_gpu * num_gpus:,}\")\n",
    "print(f\"  Total batches: {dataset_size // (batch_size_per_gpu * num_gpus):,}\")\n",
    "print()\n",
    "\n",
    "# Run training iterations\n",
    "print(\"\ud83c\udfc3 Training Progress:\")\n",
    "for iteration in range(num_iterations):\n",
    "    # Generate random batch (simulating data loading)\n",
    "    data_batch = np.random.randn(batch_size_per_gpu * num_gpus, 100)\n",
    "    \n",
    "    # Train step\n",
    "    metrics = trainer.train_step(iteration, data_batch)\n",
    "    \n",
    "    print(f\"  Iteration {iteration + 1}:\")\n",
    "    print(f\"    Compute time: {metrics['compute_time']*1000:.2f}ms\")\n",
    "    print(f\"    Communication time: {metrics['communication_time']*1000:.2f}ms\")\n",
    "    print(f\"    Total time: {metrics['total_time']*1000:.2f}ms\")\n",
    "    print(f\"    Throughput: {metrics['throughput']:.0f} samples/sec\")\n",
    "    print(f\"    Communication overhead: {metrics['communication_overhead']:.1%}\")\n",
    "\n",
    "# Calculate efficiency\n",
    "efficiency = trainer.get_scaling_efficiency()\n",
    "ideal_speedup = num_gpus\n",
    "actual_speedup = ideal_speedup * efficiency\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Scaling Analysis:\")\n",
    "print(f\"  Ideal speedup: {ideal_speedup:.1f}\u00d7 ({num_gpus} GPUs)\")\n",
    "print(f\"  Actual speedup: {actual_speedup:.1f}\u00d7\")\n",
    "print(f\"  Scaling efficiency: {efficiency:.1%}\")\n",
    "print(f\"\\n  \ud83d\udca1 Interpretation:\")\n",
    "print(f\"     - Perfect efficiency would be 100% (linear scaling)\")\n",
    "print(f\"     - Actual {efficiency:.1%} efficiency is excellent for {num_gpus} GPUs\")\n",
    "print(f\"     - Main bottleneck: Communication overhead ({metrics['communication_overhead']:.1%})\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcb0 Business Value (Post-Silicon Wafer Test):\")\n",
    "print(f\"  Single GPU training time: 14 days\")\n",
    "print(f\"  {num_gpus}-GPU training time: {14 / actual_speedup:.1f} days ({14 * 24 / actual_speedup:.1f} hours)\")\n",
    "print(f\"  Time savings: {14 - 14/actual_speedup:.1f} days per training run\")\n",
    "print(f\"  Model update frequency: Weekly (vs monthly with single GPU)\")\n",
    "print(f\"  Annual value: $23.7M/year\")\n",
    "print(f\"    - Faster drift detection: $8.5M/year\")\n",
    "print(f\"    - More frequent model improvements: $12.2M/year\")\n",
    "print(f\"    - Faster experimentation: $3.0M/year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b3d2fe",
   "metadata": {},
   "source": [
    "## 2\ufe0f\u20e3 Model Parallelism for Large Models\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement model parallelism to train models too large for single GPU memory\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "**1. Model Parallelism Strategy**\n",
    "- **Concept**: Split model layers across multiple GPUs (each GPU holds different layers)\n",
    "- **Algorithm**:\n",
    "  1. Layer 1-2 on GPU 0, Layer 3-4 on GPU 1, Layer 5-6 on GPU 2, etc.\n",
    "  2. Forward pass: Data flows GPU 0 \u2192 GPU 1 \u2192 GPU 2 (sequential)\n",
    "  3. Backward pass: Gradients flow GPU 2 \u2192 GPU 1 \u2192 GPU 0 (reverse sequential)\n",
    "  4. Each GPU only stores activations/gradients for its layers\n",
    "- **Memory benefit**: 500M param model \u00f7 4 GPUs = 125M params per GPU (4\u00d7 memory reduction)\n",
    "\n",
    "**2. When to Use Model Parallelism**\n",
    "- **Model too large**: Doesn't fit in single GPU memory (e.g., GPT-3: 175B params = 700GB)\n",
    "- **Deep models**: Many layers but small batches (transformers, very deep CNNs)\n",
    "- **Combine with data parallelism**: Model parallelism within node + data parallelism across nodes\n",
    "\n",
    "**3. Pipeline Parallelism**\n",
    "- **Problem with naive model parallelism**: GPU 0 idle while GPU 1 processes, then GPU 1 idle while GPU 2 processes\n",
    "- **Solution**: Pipeline parallelism (micro-batching)\n",
    "  - Split batch into micro-batches\n",
    "  - While GPU 1 processes micro-batch 1, GPU 0 starts micro-batch 2\n",
    "  - Keeps all GPUs busy simultaneously\n",
    "- **Example**: 4 GPUs, 4 micro-batches \u2192 75% GPU utilization (vs 25% naive)\n",
    "\n",
    "**4. Activation Checkpointing**\n",
    "- **Problem**: Storing all intermediate activations for backward pass requires huge memory\n",
    "- **Solution**: Checkpoint (save) only some activations, recompute others during backward pass\n",
    "- **Trade-off**: 30% slower training, but 50% less memory (enables 2\u00d7 larger models)\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Train massive models**: GPT-3 (175B params), Megatron (530B params), PaLM (540B params)\n",
    "- **Memory efficiency**: Train 300M param model on 4\u00d7 16GB GPUs (vs needing single 64GB GPU)\n",
    "- **Cost savings**: Use smaller, cheaper GPUs instead of expensive A100 80GB\n",
    "- **Flexibility**: Combine model + data parallelism for best of both worlds\n",
    "\n",
    "**Post-Silicon Example:**\n",
    "Multi-modal device characterization (vision + parametric):\n",
    "- **Model**: Vision encoder (150M params) + Feature encoder (50M params) + Fusion layers (100M params) = 300M params total\n",
    "- **Memory requirement**: 300M \u00d7 4 bytes = 1.2GB params + 8GB activations = 9.2GB (exceeds 8GB GPU)\n",
    "- **Model parallelism**: Vision encoder on GPU 0-1, Feature encoder on GPU 2, Fusion on GPU 3\n",
    "- **Result**: Fit on 4\u00d7 16GB GPUs, train wafer map + parametric model\n",
    "- **Business value**: $18.9M/year from 5% yield improvement (spatial + electrical insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa504746",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelLayer:\n",
    "    \"\"\"Represents a single layer in a neural network\"\"\"\n",
    "    layer_id: int\n",
    "    gpu_id: int\n",
    "    num_params: int\n",
    "    input_size: int\n",
    "    output_size: int\n",
    "    \n",
    "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Simulate forward pass through layer\"\"\"\n",
    "        # Simple linear transformation for simulation\n",
    "        weights = np.random.randn(self.input_size, self.output_size) * 0.01\n",
    "        outputs = np.dot(inputs, weights)\n",
    "        return outputs\n",
    "    \n",
    "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Simulate backward pass through layer\"\"\"\n",
    "        # Gradient w.r.t. inputs\n",
    "        weights = np.random.randn(self.output_size, self.input_size) * 0.01\n",
    "        grad_input = np.dot(grad_output, weights)\n",
    "        return grad_input\n",
    "\n",
    "class ModelParallelNetwork:\n",
    "    \"\"\"\n",
    "    Simulates model parallelism across multiple GPUs\n",
    "    \n",
    "    Demonstrates:\n",
    "    - Layer distribution across GPUs\n",
    "    - Sequential forward/backward passes\n",
    "    - Inter-GPU communication for activations/gradients\n",
    "    - Memory savings from model partitioning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_gpus: int, total_layers: int, params_per_layer: int):\n",
    "        self.num_gpus = num_gpus\n",
    "        self.total_layers = total_layers\n",
    "        self.params_per_layer = params_per_layer\n",
    "        \n",
    "        # Distribute layers evenly across GPUs\n",
    "        layers_per_gpu = total_layers // num_gpus\n",
    "        self.layers = []\n",
    "        \n",
    "        for layer_id in range(total_layers):\n",
    "            gpu_id = layer_id // layers_per_gpu\n",
    "            if gpu_id >= num_gpus:\n",
    "                gpu_id = num_gpus - 1  # Last GPU gets remaining layers\n",
    "            \n",
    "            layer = ModelLayer(\n",
    "                layer_id=layer_id,\n",
    "                gpu_id=gpu_id,\n",
    "                num_params=params_per_layer,\n",
    "                input_size=512,\n",
    "                output_size=512\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        self.forward_times = []\n",
    "        self.backward_times = []\n",
    "        self.communication_times = []\n",
    "        \n",
    "    def get_gpu_assignment(self) -> Dict[int, List[int]]:\n",
    "        \"\"\"Return which layers are on which GPU\"\"\"\n",
    "        assignment = defaultdict(list)\n",
    "        for layer in self.layers:\n",
    "            assignment[layer.gpu_id].append(layer.layer_id)\n",
    "        return dict(assignment)\n",
    "    \n",
    "    def simulate_gpu_transfer(self, data_size_mb: float) -> float:\n",
    "        \"\"\"Simulate time to transfer data between GPUs\"\"\"\n",
    "        # PCIe bandwidth: 16 GB/s (PCIe 4.0 x16)\n",
    "        # NVLink bandwidth: 600 GB/s (A100 NVLink)\n",
    "        bandwidth_gbps = 600 / 1000  # 0.6 GB/ms (NVLink)\n",
    "        transfer_time_ms = data_size_mb / (bandwidth_gbps * 1024)\n",
    "        return transfer_time_ms\n",
    "    \n",
    "    def forward_pass(self, batch_size: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Simulate forward pass with model parallelism\n",
    "        \n",
    "        Data flows sequentially through GPUs:\n",
    "        GPU 0 (layers 0-2) \u2192 GPU 1 (layers 3-5) \u2192 GPU 2 (layers 6-8) \u2192 ...\n",
    "        \"\"\"\n",
    "        activation_size_mb = batch_size * 512 * 4 / (1024 * 1024)  # float32\n",
    "        \n",
    "        total_compute_time = 0.0\n",
    "        total_comm_time = 0.0\n",
    "        current_gpu = None\n",
    "        \n",
    "        # Input data\n",
    "        activations = np.random.randn(batch_size, 512)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            # Check if we need to transfer to different GPU\n",
    "            if current_gpu is not None and current_gpu != layer.gpu_id:\n",
    "                # Transfer activations to next GPU\n",
    "                comm_time = self.simulate_gpu_transfer(activation_size_mb)\n",
    "                total_comm_time += comm_time\n",
    "            \n",
    "            # Forward pass through layer (simulate compute)\n",
    "            compute_time = 0.01 * batch_size  # 10\u03bcs per sample\n",
    "            activations = layer.forward(activations)\n",
    "            total_compute_time += compute_time\n",
    "            \n",
    "            current_gpu = layer.gpu_id\n",
    "        \n",
    "        return {\n",
    "            \"compute_time\": total_compute_time,\n",
    "            \"communication_time\": total_comm_time,\n",
    "            \"total_time\": total_compute_time + total_comm_time,\n",
    "            \"final_activations\": activations\n",
    "        }\n",
    "    \n",
    "    def backward_pass(self, batch_size: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Simulate backward pass with model parallelism\n",
    "        \n",
    "        Gradients flow in reverse:\n",
    "        GPU N \u2192 ... \u2192 GPU 2 \u2192 GPU 1 \u2192 GPU 0\n",
    "        \"\"\"\n",
    "        gradient_size_mb = batch_size * 512 * 4 / (1024 * 1024)\n",
    "        \n",
    "        total_compute_time = 0.0\n",
    "        total_comm_time = 0.0\n",
    "        current_gpu = None\n",
    "        \n",
    "        # Output gradients\n",
    "        gradients = np.random.randn(batch_size, 512)\n",
    "        \n",
    "        # Backward through layers in reverse order\n",
    "        for layer in reversed(self.layers):\n",
    "            # Check if we need to transfer to different GPU\n",
    "            if current_gpu is not None and current_gpu != layer.gpu_id:\n",
    "                comm_time = self.simulate_gpu_transfer(gradient_size_mb)\n",
    "                total_comm_time += comm_time\n",
    "            \n",
    "            # Backward pass through layer\n",
    "            compute_time = 0.01 * batch_size\n",
    "            gradients = layer.backward(gradients)\n",
    "            total_compute_time += compute_time\n",
    "            \n",
    "            current_gpu = layer.gpu_id\n",
    "        \n",
    "        return {\n",
    "            \"compute_time\": total_compute_time,\n",
    "            \"communication_time\": total_comm_time,\n",
    "            \"total_time\": total_compute_time + total_comm_time\n",
    "        }\n",
    "    \n",
    "    def get_memory_usage_per_gpu(self) -> Dict[int, float]:\n",
    "        \"\"\"Calculate memory usage per GPU\"\"\"\n",
    "        memory_per_gpu = defaultdict(float)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            # Parameters memory\n",
    "            param_memory_mb = layer.num_params * 4 / (1024 * 1024)  # float32\n",
    "            # Activations memory (approximate)\n",
    "            activation_memory_mb = 512 * 512 * 4 / (1024 * 1024)\n",
    "            \n",
    "            total_memory_mb = param_memory_mb + activation_memory_mb\n",
    "            memory_per_gpu[layer.gpu_id] += total_memory_mb\n",
    "        \n",
    "        return dict(memory_per_gpu)\n",
    "\n",
    "# Simulate model parallelism\n",
    "print(\"\ud83d\udd04 Simulating Model Parallelism\\n\")\n",
    "\n",
    "# Configuration\n",
    "num_gpus = 4\n",
    "total_layers = 12  # 3 layers per GPU\n",
    "params_per_layer = 25_000_000  # 25M params per layer\n",
    "total_params = total_layers * params_per_layer  # 300M total\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize model parallel network\n",
    "model = ModelParallelNetwork(\n",
    "    num_gpus=num_gpus,\n",
    "    total_layers=total_layers,\n",
    "    params_per_layer=params_per_layer\n",
    ")\n",
    "\n",
    "print(f\"\ud83d\udcca Model Configuration:\")\n",
    "print(f\"  Total parameters: {total_params:,} ({total_params * 4 / 1e9:.2f} GB)\")\n",
    "print(f\"  Total layers: {total_layers}\")\n",
    "print(f\"  GPUs: {num_gpus}\")\n",
    "print(f\"  Layers per GPU: ~{total_layers // num_gpus}\")\n",
    "print()\n",
    "\n",
    "# Show GPU assignment\n",
    "assignment = model.get_gpu_assignment()\n",
    "print(f\"\ud83d\udccd Layer Distribution:\")\n",
    "for gpu_id, layer_ids in sorted(assignment.items()):\n",
    "    print(f\"  GPU {gpu_id}: Layers {layer_ids}\")\n",
    "print()\n",
    "\n",
    "# Memory usage per GPU\n",
    "memory_usage = model.get_memory_usage_per_gpu()\n",
    "print(f\"\ud83d\udcbe Memory Usage Per GPU:\")\n",
    "total_memory_single_gpu = sum(memory_usage.values())\n",
    "for gpu_id, memory_mb in sorted(memory_usage.items()):\n",
    "    memory_gb = memory_mb / 1024\n",
    "    print(f\"  GPU {gpu_id}: {memory_gb:.2f} GB\")\n",
    "print(f\"  Total (if on single GPU): {total_memory_single_gpu / 1024:.2f} GB\")\n",
    "print(f\"  Memory reduction: {num_gpus:.1f}\u00d7 (model parallelism benefit)\")\n",
    "print()\n",
    "\n",
    "# Run forward and backward passes\n",
    "print(f\"\ud83c\udfc3 Training Pass (batch_size={batch_size}):\\n\")\n",
    "\n",
    "# Forward pass\n",
    "forward_result = model.forward_pass(batch_size)\n",
    "print(f\"  Forward Pass:\")\n",
    "print(f\"    Compute time: {forward_result['compute_time']:.2f}ms\")\n",
    "print(f\"    Communication time: {forward_result['communication_time']:.2f}ms\")\n",
    "print(f\"    Total time: {forward_result['total_time']:.2f}ms\")\n",
    "print(f\"    Communication overhead: {forward_result['communication_time'] / forward_result['total_time']:.1%}\")\n",
    "\n",
    "# Backward pass\n",
    "backward_result = model.backward_pass(batch_size)\n",
    "print(f\"\\n  Backward Pass:\")\n",
    "print(f\"    Compute time: {backward_result['compute_time']:.2f}ms\")\n",
    "print(f\"    Communication time: {backward_result['communication_time']:.2f}ms\")\n",
    "print(f\"    Total time: {backward_result['total_time']:.2f}ms\")\n",
    "print(f\"    Communication overhead: {backward_result['communication_time'] / backward_result['total_time']:.1%}\")\n",
    "\n",
    "# Total iteration time\n",
    "total_iteration_time = forward_result['total_time'] + backward_result['total_time']\n",
    "print(f\"\\n  Total Iteration Time: {total_iteration_time:.2f}ms\")\n",
    "\n",
    "# GPU utilization analysis\n",
    "print(f\"\\n\ud83d\udcc8 GPU Utilization Analysis:\")\n",
    "print(f\"  Naive model parallelism: ~25% (only 1 GPU active at a time)\")\n",
    "print(f\"  With pipeline parallelism: ~75% (micro-batching keeps GPUs busy)\")\n",
    "print(f\"  \ud83d\udca1 Recommendation: Combine with data parallelism for better efficiency\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcb0 Business Value (Multi-Modal Wafer Characterization):\")\n",
    "print(f\"  Model size: 300M parameters (vision + parametric fusion)\")\n",
    "print(f\"  Single GPU memory: 9.2 GB (exceeds 8GB GPU limit)\")\n",
    "print(f\"  Model parallelism: Fits on 4\u00d7 16GB GPUs ({memory_usage[0]/1024:.1f}GB per GPU)\")\n",
    "print(f\"  Enables: Wafer map image + parametric test data integration\")\n",
    "print(f\"  Yield improvement: 5% (spatial patterns + electrical characteristics)\")\n",
    "print(f\"  Annual value: $18.9M/year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe7a27",
   "metadata": {},
   "source": [
    "## 3\ufe0f\u20e3 Gradient Accumulation & Mixed Precision Training\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement gradient accumulation and mixed precision to optimize memory and speed\n",
    "\n",
    "**Key Techniques:**\n",
    "\n",
    "**1. Gradient Accumulation**\n",
    "- **Problem**: Large batch sizes don't fit in GPU memory (e.g., batch_size=8192 needs 64GB)\n",
    "- **Solution**: Accumulate gradients over multiple small batches, then update once\n",
    "  - Algorithm:\n",
    "    1. Forward pass with mini-batch 1 \u2192 compute gradients \u2192 **don't update yet**\n",
    "    2. Forward pass with mini-batch 2 \u2192 compute gradients \u2192 **accumulate with step 1**\n",
    "    3. Repeat for N mini-batches\n",
    "    4. Average accumulated gradients \u2192 update parameters \u2192 reset gradients\n",
    "- **Effective batch size**: mini_batch_size \u00d7 accumulation_steps\n",
    "- **Memory benefit**: Train with effective batch_size=8192 using only mini_batch_size=256 memory\n",
    "\n",
    "**2. Mixed Precision Training (FP16 + FP32)**\n",
    "- **Concept**: Use 16-bit floats (FP16) for forward/backward, 32-bit (FP32) for parameter updates\n",
    "- **Benefits**:\n",
    "  - **2\u00d7 memory reduction**: FP16 uses half the memory of FP32\n",
    "  - **2-3\u00d7 speedup**: Tensor Cores accelerate FP16 operations\n",
    "  - **Same accuracy**: FP32 master weights prevent precision loss\n",
    "- **Implementation**:\n",
    "  - Forward pass: FP16 (fast computation)\n",
    "  - Loss computation: FP16\n",
    "  - Backward pass: FP16 (fast gradient computation)\n",
    "  - Gradient scaling: Multiply by scale factor (prevent underflow)\n",
    "  - Parameter update: FP32 master weights\n",
    "- **Automatic Mixed Precision (AMP)**: PyTorch/TensorFlow automatically choose FP16/FP32\n",
    "\n",
    "**3. Gradient Clipping**\n",
    "- **Problem**: Exploding gradients in deep networks (gradients \u2192 \u221e)\n",
    "- **Solution**: Clip gradients to maximum norm\n",
    "  - Algorithm: `if ||gradients|| > max_norm: gradients *= max_norm / ||gradients||`\n",
    "  - Typical max_norm: 1.0 for transformers, 5.0 for RNNs\n",
    "- **Benefit**: Training stability, prevents NaN losses\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Memory efficiency**: Train 2\u00d7 larger models or 2\u00d7 larger batches\n",
    "- **Speed**: 2-3\u00d7 faster training on modern GPUs (Tensor Cores)\n",
    "- **Large batch training**: Enables batch_size=8192+ for better convergence\n",
    "- **Cost savings**: Use cheaper GPUs with less memory\n",
    "\n",
    "**Post-Silicon Example:**\n",
    "Hyperparameter tuning for test time optimization:\n",
    "- **Target**: Train 100 RNN models with different hyperparameters\n",
    "- **Challenge**: Each model needs batch_size=2048 for stable training (exceeds 16GB GPU)\n",
    "- **Solution**: Gradient accumulation (mini_batch=256, accum_steps=8) + mixed precision\n",
    "- **Result**: Fit on single 16GB GPU, 2.5\u00d7 faster training\n",
    "- **Business value**: $15.3M/year from 18% test time reduction (120s \u2192 98s per device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a452312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientAccumulationTrainer:\n",
    "    \"\"\"\n",
    "    Simulates gradient accumulation for large effective batch sizes\n",
    "    \n",
    "    Allows training with large batch sizes that don't fit in GPU memory\n",
    "    by accumulating gradients over multiple small batches\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_size: int, mini_batch_size: int, accumulation_steps: int):\n",
    "        self.model_size = model_size\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "        self.effective_batch_size = mini_batch_size * accumulation_steps\n",
    "        \n",
    "        # Model parameters\n",
    "        self.model_params = np.random.randn(model_size) * 0.01\n",
    "        \n",
    "        # Accumulated gradients\n",
    "        self.accumulated_gradients = np.zeros(model_size)\n",
    "        \n",
    "        self.training_history = []\n",
    "        \n",
    "    def forward_backward(self, mini_batch: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Simulate forward and backward pass for mini-batch\"\"\"\n",
    "        # Compute gradients (random for simulation)\n",
    "        gradients = np.random.randn(self.model_size) * 0.1\n",
    "        return gradients\n",
    "    \n",
    "    def train_step_with_accumulation(self, iteration: int, use_fp16: bool = False):\n",
    "        \"\"\"\n",
    "        Training step with gradient accumulation\n",
    "        \n",
    "        Steps:\n",
    "        1. Reset accumulated gradients\n",
    "        2. For each accumulation step:\n",
    "           - Forward/backward on mini-batch\n",
    "           - Accumulate gradients\n",
    "        3. Average accumulated gradients\n",
    "        4. Update parameters\n",
    "        \"\"\"\n",
    "        # Reset accumulated gradients\n",
    "        self.accumulated_gradients = np.zeros(self.model_size)\n",
    "        \n",
    "        total_compute_time = 0.0\n",
    "        memory_usage_mb = 0.0\n",
    "        \n",
    "        # Gradient accumulation loop\n",
    "        for step in range(self.accumulation_steps):\n",
    "            # Generate mini-batch\n",
    "            mini_batch = np.random.randn(self.mini_batch_size, 100)\n",
    "            \n",
    "            # Forward and backward pass\n",
    "            start_time = time.time()\n",
    "            gradients = self.forward_backward(mini_batch)\n",
    "            compute_time = (time.time() - start_time) * 1000  # ms\n",
    "            \n",
    "            # Simulate mixed precision speedup\n",
    "            if use_fp16:\n",
    "                compute_time *= 0.4  # 2.5\u00d7 faster with FP16\n",
    "            \n",
    "            # Accumulate gradients\n",
    "            self.accumulated_gradients += gradients\n",
    "            \n",
    "            total_compute_time += compute_time\n",
    "            \n",
    "            # Memory usage (only need to store one mini-batch at a time)\n",
    "            batch_memory_mb = self.mini_batch_size * 100 * 4 / (1024 * 1024)  # float32\n",
    "            if use_fp16:\n",
    "                batch_memory_mb *= 0.5  # FP16 uses half memory\n",
    "            \n",
    "            memory_usage_mb = max(memory_usage_mb, batch_memory_mb)\n",
    "        \n",
    "        # Average accumulated gradients\n",
    "        averaged_gradients = self.accumulated_gradients / self.accumulation_steps\n",
    "        \n",
    "        # Update parameters (using FP32 for precision)\n",
    "        learning_rate = 0.01\n",
    "        self.model_params -= learning_rate * averaged_gradients\n",
    "        \n",
    "        # Model memory (parameters)\n",
    "        param_memory_mb = self.model_size * 4 / (1024 * 1024)  # Always FP32\n",
    "        if use_fp16:\n",
    "            # FP16 copy of model + FP32 master weights\n",
    "            total_memory_mb = memory_usage_mb + param_memory_mb * 1.5\n",
    "        else:\n",
    "            total_memory_mb = memory_usage_mb + param_memory_mb\n",
    "        \n",
    "        # Record metrics\n",
    "        loss = 1.0 / (1.0 + iteration * 0.1)  # Simulated decreasing loss\n",
    "        \n",
    "        self.training_history.append({\n",
    "            \"iteration\": iteration,\n",
    "            \"loss\": loss,\n",
    "            \"compute_time\": total_compute_time,\n",
    "            \"memory_mb\": total_memory_mb,\n",
    "            \"effective_batch_size\": self.effective_batch_size,\n",
    "            \"use_fp16\": use_fp16\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"compute_time\": total_compute_time,\n",
    "            \"memory_mb\": total_memory_mb\n",
    "        }\n",
    "\n",
    "class MixedPrecisionSimulator:\n",
    "    \"\"\"\n",
    "    Demonstrates memory and speed benefits of mixed precision training\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_precision_modes(model_size: int, batch_size: int) -> Dict[str, Any]:\n",
    "        \"\"\"Compare FP32 vs FP16 training\"\"\"\n",
    "        \n",
    "        # FP32 (full precision)\n",
    "        fp32_param_memory = model_size * 4 / (1024 * 1024)  # MB\n",
    "        fp32_activation_memory = batch_size * 1000 * 4 / (1024 * 1024)  # MB\n",
    "        fp32_total_memory = fp32_param_memory + fp32_activation_memory\n",
    "        fp32_compute_time = 100.0  # ms (baseline)\n",
    "        \n",
    "        # FP16 (mixed precision)\n",
    "        fp16_param_memory = model_size * 4 / (1024 * 1024)  # Master weights still FP32\n",
    "        fp16_model_copy = model_size * 2 / (1024 * 1024)  # FP16 copy for forward/backward\n",
    "        fp16_activation_memory = batch_size * 1000 * 2 / (1024 * 1024)  # FP16 activations\n",
    "        fp16_total_memory = fp16_param_memory + fp16_model_copy + fp16_activation_memory\n",
    "        fp16_compute_time = 100.0 * 0.4  # 2.5\u00d7 speedup with Tensor Cores\n",
    "        \n",
    "        return {\n",
    "            \"fp32\": {\n",
    "                \"memory_mb\": fp32_total_memory,\n",
    "                \"compute_time_ms\": fp32_compute_time,\n",
    "                \"memory_gb\": fp32_total_memory / 1024\n",
    "            },\n",
    "            \"fp16\": {\n",
    "                \"memory_mb\": fp16_total_memory,\n",
    "                \"compute_time_ms\": fp16_compute_time,\n",
    "                \"memory_gb\": fp16_total_memory / 1024,\n",
    "                \"memory_reduction\": fp32_total_memory / fp16_total_memory,\n",
    "                \"speedup\": fp32_compute_time / fp16_compute_time\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Demonstrate gradient accumulation\n",
    "print(\"\ud83d\udd04 Gradient Accumulation Demonstration\\n\")\n",
    "\n",
    "model_size = 100_000_000  # 100M parameters\n",
    "mini_batch_size = 256\n",
    "accumulation_steps = 8\n",
    "effective_batch_size = mini_batch_size * accumulation_steps\n",
    "\n",
    "print(f\"\ud83d\udcca Configuration:\")\n",
    "print(f\"  Model size: {model_size:,} parameters\")\n",
    "print(f\"  Mini-batch size: {mini_batch_size}\")\n",
    "print(f\"  Accumulation steps: {accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {effective_batch_size} (same as training with large batch)\")\n",
    "print()\n",
    "\n",
    "# Compare FP32 vs FP16\n",
    "trainer_fp32 = GradientAccumulationTrainer(model_size, mini_batch_size, accumulation_steps)\n",
    "trainer_fp16 = GradientAccumulationTrainer(model_size, mini_batch_size, accumulation_steps)\n",
    "\n",
    "# Run training iterations\n",
    "num_iterations = 5\n",
    "\n",
    "print(\"\ud83c\udfc3 Training with FP32 (full precision):\")\n",
    "for i in range(num_iterations):\n",
    "    result = trainer_fp32.train_step_with_accumulation(i, use_fp16=False)\n",
    "    print(f\"  Iteration {i+1}: Loss={result['loss']:.4f}, Time={result['compute_time']:.1f}ms, Memory={result['memory_mb']:.1f}MB\")\n",
    "\n",
    "print(\"\\n\ud83c\udfc3 Training with FP16 (mixed precision):\")\n",
    "for i in range(num_iterations):\n",
    "    result = trainer_fp16.train_step_with_accumulation(i, use_fp16=True)\n",
    "    print(f\"  Iteration {i+1}: Loss={result['loss']:.4f}, Time={result['compute_time']:.1f}ms, Memory={result['memory_mb']:.1f}MB\")\n",
    "\n",
    "# Calculate improvements\n",
    "fp32_avg_time = np.mean([h['compute_time'] for h in trainer_fp32.training_history])\n",
    "fp16_avg_time = np.mean([h['compute_time'] for h in trainer_fp16.training_history])\n",
    "fp32_avg_memory = np.mean([h['memory_mb'] for h in trainer_fp32.training_history])\n",
    "fp16_avg_memory = np.mean([h['memory_mb'] for h in trainer_fp16.training_history])\n",
    "\n",
    "speedup = fp32_avg_time / fp16_avg_time\n",
    "memory_reduction = fp32_avg_memory / fp16_avg_memory\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 FP16 Benefits:\")\n",
    "print(f\"  Speedup: {speedup:.2f}\u00d7 faster ({fp32_avg_time:.1f}ms \u2192 {fp16_avg_time:.1f}ms)\")\n",
    "print(f\"  Memory reduction: {memory_reduction:.2f}\u00d7 less ({fp32_avg_memory:.1f}MB \u2192 {fp16_avg_memory:.1f}MB)\")\n",
    "print(f\"  Effective batch size: {effective_batch_size} (same as full batch, but fits in memory)\")\n",
    "\n",
    "# Detailed precision comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Mixed Precision Training: Detailed Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison = MixedPrecisionSimulator.compare_precision_modes(\n",
    "    model_size=200_000_000,  # 200M params (BERT-Large size)\n",
    "    batch_size=2048\n",
    ")\n",
    "\n",
    "print(f\"\\n\ud83d\udcbe Memory Usage:\")\n",
    "print(f\"  FP32: {comparison['fp32']['memory_gb']:.2f} GB\")\n",
    "print(f\"  FP16: {comparison['fp16']['memory_gb']:.2f} GB\")\n",
    "print(f\"  Reduction: {comparison['fp16']['memory_reduction']:.2f}\u00d7 (save {comparison['fp32']['memory_gb'] - comparison['fp16']['memory_gb']:.2f} GB)\")\n",
    "\n",
    "print(f\"\\n\u26a1 Compute Speed:\")\n",
    "print(f\"  FP32: {comparison['fp32']['compute_time_ms']:.1f} ms/iteration\")\n",
    "print(f\"  FP16: {comparison['fp16']['compute_time_ms']:.1f} ms/iteration\")\n",
    "print(f\"  Speedup: {comparison['fp16']['speedup']:.2f}\u00d7 faster\")\n",
    "\n",
    "print(f\"\\n\ud83d\udca1 Key Insights:\")\n",
    "print(f\"  \u2713 FP16 enables 2\u00d7 larger models or 2\u00d7 larger batches\")\n",
    "print(f\"  \u2713 Tensor Cores provide 2-3\u00d7 speedup on modern GPUs (A100, V100)\")\n",
    "print(f\"  \u2713 Master weights in FP32 ensure no accuracy loss\")\n",
    "print(f\"  \u2713 Gradient scaling prevents underflow in FP16\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcb0 Business Value (Test Time Optimization Hyperparameter Tuning):\")\n",
    "print(f\"  Task: Train 100 RNN models with different hyperparameters\")\n",
    "print(f\"  Without FP16: 100 models \u00d7 8 hours = 800 GPU-hours\")\n",
    "print(f\"  With FP16: 100 models \u00d7 3.2 hours = 320 GPU-hours ({speedup:.1f}\u00d7 faster)\")\n",
    "print(f\"  GPU cost savings: 480 GPU-hours \u00d7 $2.50/hour = $1,200 per experiment\")\n",
    "print(f\"  Test time reduction: 18% (120s \u2192 98s per device)\")\n",
    "print(f\"  Annual value: $15.3M/year (98s vs 120s \u00d7 25M devices \u00d7 $2.10/device-min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e48812",
   "metadata": {},
   "source": [
    "## 4\ufe0f\u20e3 Fault Tolerance & Checkpointing\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement fault tolerance mechanisms for long-running distributed training jobs\n",
    "\n",
    "**Key Patterns:**\n",
    "\n",
    "**1. Checkpointing Strategies**\n",
    "- **Regular checkpointing**: Save model + optimizer state every N iterations\n",
    "  - Frequency: Every 1000 iterations or every 1 hour (whichever comes first)\n",
    "  - Saved state: Model parameters, optimizer state (momentum, learning rate schedule), epoch/iteration number\n",
    "  - Storage: S3, GCS, Azure Blob (distributed file systems)\n",
    "- **Best checkpoint**: Keep checkpoint with best validation loss\n",
    "- **Latest checkpoints**: Keep last 3 checkpoints (for recovery if latest is corrupted)\n",
    "\n",
    "**2. Fault Detection & Recovery**\n",
    "- **GPU failure detection**: Monitor GPU health (temperature, memory errors, compute errors)\n",
    "- **Automatic recovery**:\n",
    "  1. Detect failure (GPU hang, OOM, CUDA error)\n",
    "  2. Load last checkpoint\n",
    "  3. Restart training from saved iteration\n",
    "  4. Resume with reduced batch size if OOM\n",
    "- **Graceful degradation**: Continue training with N-1 GPUs if one fails\n",
    "\n",
    "**3. Distributed Checkpointing**\n",
    "- **Challenge**: With model parallelism, each GPU holds different model parts\n",
    "- **Solution**: Distributed checkpoint\n",
    "  - Each GPU saves its model shard independently\n",
    "  - Checkpoint metadata tracks which GPU owns which layers\n",
    "  - Recovery: Each GPU loads its corresponding shard\n",
    "- **Async checkpointing**: Save checkpoint in background while training continues (don't block training)\n",
    "\n",
    "**4. Elastic Training**\n",
    "- **Concept**: Scale up/down number of GPUs during training\n",
    "- **Use cases**:\n",
    "  - Spot instance preemption: GPU reclaimed by cloud provider\n",
    "  - Auto-scaling: Add GPUs during busy hours, remove during idle\n",
    "  - Cost optimization: Use cheap spot instances, fall back to on-demand if interrupted\n",
    "- **Implementation**: Resize data shards, redistribute work, continue training\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Reliability**: 14-day training job \u2192 save 13 days if failure on day 13\n",
    "- **Cost savings**: Use 70% cheaper spot instances with automatic recovery\n",
    "- **Experimentation**: Resume from checkpoint to try different hyperparameters\n",
    "- **Reproducibility**: Checkpoint includes random seeds, exact training state\n",
    "\n",
    "**Real-World Scenarios:**\n",
    "- **GPU failure**: Hardware fault at iteration 45,000/50,000 \u2192 load checkpoint from iteration 44,000 \u2192 resume \u2192 lose only 1,000 iterations (20 minutes vs 13 days)\n",
    "- **OOM error**: Out of memory at batch 10,000 \u2192 automatic recovery with 50% batch size \u2192 continue training\n",
    "- **Spot interruption**: Cloud provider reclaims GPU \u2192 save checkpoint \u2192 restart on new instance \u2192 seamless continuation\n",
    "- **Experiment variation**: Train for 30,000 iterations \u2192 checkpoint \u2192 try different learning rate for next 20,000 iterations \u2192 compare results\n",
    "\n",
    "**Business Impact:**\n",
    "- **Training reliability**: 99.9% job completion rate (vs 85% without fault tolerance)\n",
    "- **Cost savings**: 70% cheaper spot instances (vs on-demand)\n",
    "- **Time savings**: Recover in minutes (vs days to restart from scratch)\n",
    "- **Value**: $12.8M/year from fault-tolerant distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f527e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingCheckpoint:\n",
    "    \"\"\"Represents a training checkpoint\"\"\"\n",
    "    checkpoint_id: str\n",
    "    iteration: int\n",
    "    epoch: int\n",
    "    model_state: np.ndarray\n",
    "    optimizer_state: Dict[str, Any]\n",
    "    loss: float\n",
    "    timestamp: str\n",
    "    gpu_states: Dict[int, Any] = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class FaultEvent:\n",
    "    \"\"\"Records a fault/failure event\"\"\"\n",
    "    event_id: str\n",
    "    event_type: str  # \"gpu_failure\", \"oom\", \"spot_interruption\"\n",
    "    gpu_id: Optional[int]\n",
    "    iteration: int\n",
    "    timestamp: str\n",
    "    recovery_action: str\n",
    "\n",
    "class FaultTolerantTrainer:\n",
    "    \"\"\"\n",
    "    Distributed trainer with fault tolerance and checkpointing\n",
    "    \n",
    "    Features:\n",
    "    - Regular checkpointing\n",
    "    - Automatic failure detection\n",
    "    - Recovery from checkpoints\n",
    "    - Elastic training (handle GPU scaling)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_gpus: int, model_size: int, checkpoint_frequency: int = 1000):\n",
    "        self.num_gpus = num_gpus\n",
    "        self.model_size = model_size\n",
    "        self.checkpoint_frequency = checkpoint_frequency\n",
    "        \n",
    "        # Training state\n",
    "        self.model_params = np.random.randn(model_size) * 0.01\n",
    "        self.optimizer_state = {\"learning_rate\": 0.01, \"momentum\": np.zeros(model_size)}\n",
    "        self.current_iteration = 0\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        # Checkpointing\n",
    "        self.checkpoints: Dict[str, TrainingCheckpoint] = {}\n",
    "        self.best_checkpoint_id: Optional[str] = None\n",
    "        self.best_loss = float('inf')\n",
    "        \n",
    "        # Fault tracking\n",
    "        self.fault_events: List[FaultEvent] = []\n",
    "        self.active_gpus = set(range(num_gpus))\n",
    "        \n",
    "    def save_checkpoint(self, iteration: int, loss: float) -> str:\n",
    "        \"\"\"Save current training state\"\"\"\n",
    "        checkpoint_id = f\"checkpoint_{iteration}_{uuid.uuid4().hex[:8]}\"\n",
    "        \n",
    "        checkpoint = TrainingCheckpoint(\n",
    "            checkpoint_id=checkpoint_id,\n",
    "            iteration=iteration,\n",
    "            epoch=self.current_epoch,\n",
    "            model_state=self.model_params.copy(),\n",
    "            optimizer_state=self.optimizer_state.copy(),\n",
    "            loss=loss,\n",
    "            timestamp=time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            gpu_states={gpu_id: {\"active\": True} for gpu_id in self.active_gpus}\n",
    "        )\n",
    "        \n",
    "        self.checkpoints[checkpoint_id] = checkpoint\n",
    "        \n",
    "        # Update best checkpoint\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.best_checkpoint_id = checkpoint_id\n",
    "        \n",
    "        # Keep only last 3 checkpoints + best checkpoint\n",
    "        if len(self.checkpoints) > 4:\n",
    "            # Sort by iteration\n",
    "            sorted_checkpoints = sorted(self.checkpoints.items(), key=lambda x: x[1].iteration)\n",
    "            # Remove oldest checkpoint (except best)\n",
    "            oldest_id, _ = sorted_checkpoints[0]\n",
    "            if oldest_id != self.best_checkpoint_id:\n",
    "                del self.checkpoints[oldest_id]\n",
    "        \n",
    "        return checkpoint_id\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_id: str) -> bool:\n",
    "        \"\"\"Load training state from checkpoint\"\"\"\n",
    "        if checkpoint_id not in self.checkpoints:\n",
    "            return False\n",
    "        \n",
    "        checkpoint = self.checkpoints[checkpoint_id]\n",
    "        \n",
    "        self.model_params = checkpoint.model_state.copy()\n",
    "        self.optimizer_state = checkpoint.optimizer_state.copy()\n",
    "        self.current_iteration = checkpoint.iteration\n",
    "        self.current_epoch = checkpoint.epoch\n",
    "        \n",
    "        # Restore GPU states\n",
    "        self.active_gpus = set(checkpoint.gpu_states.keys())\n",
    "        \n",
    "        print(f\"  \u2705 Loaded checkpoint from iteration {checkpoint.iteration}\")\n",
    "        return True\n",
    "    \n",
    "    def simulate_fault(self, fault_type: str, iteration: int) -> FaultEvent:\n",
    "        \"\"\"Simulate various failure scenarios\"\"\"\n",
    "        event_id = f\"fault_{uuid.uuid4().hex[:8]}\"\n",
    "        \n",
    "        if fault_type == \"gpu_failure\":\n",
    "            # Simulate GPU hardware failure\n",
    "            if self.active_gpus:\n",
    "                failed_gpu = list(self.active_gpus)[0]\n",
    "                self.active_gpus.remove(failed_gpu)\n",
    "                recovery_action = f\"Removed GPU {failed_gpu}, continue with {len(self.active_gpus)} GPUs\"\n",
    "            else:\n",
    "                recovery_action = \"No GPUs available\"\n",
    "        \n",
    "        elif fault_type == \"oom\":\n",
    "            # Out of memory error\n",
    "            recovery_action = \"Reduce batch size by 50%, reload checkpoint\"\n",
    "        \n",
    "        elif fault_type == \"spot_interruption\":\n",
    "            # Cloud spot instance preemption\n",
    "            recovery_action = \"Save checkpoint, restart on new instance\"\n",
    "        \n",
    "        else:\n",
    "            recovery_action = \"Unknown fault\"\n",
    "        \n",
    "        fault_event = FaultEvent(\n",
    "            event_id=event_id,\n",
    "            event_type=fault_type,\n",
    "            gpu_id=None,\n",
    "            iteration=iteration,\n",
    "            timestamp=time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            recovery_action=recovery_action\n",
    "        )\n",
    "        \n",
    "        self.fault_events.append(fault_event)\n",
    "        return fault_event\n",
    "    \n",
    "    def train_with_fault_tolerance(self, total_iterations: int, simulate_faults: bool = True):\n",
    "        \"\"\"\n",
    "        Train with automatic checkpointing and fault recovery\n",
    "        \n",
    "        Simulates training with periodic checkpoints and random faults\n",
    "        \"\"\"\n",
    "        print(f\"\ud83c\udfc3 Training with Fault Tolerance ({total_iterations} iterations)\\n\")\n",
    "        \n",
    "        recovered_iterations = 0\n",
    "        \n",
    "        while self.current_iteration < total_iterations:\n",
    "            # Simulate training iteration\n",
    "            loss = 1.0 / (1.0 + self.current_iteration * 0.01)  # Decreasing loss\n",
    "            \n",
    "            # Regular checkpointing\n",
    "            if self.current_iteration % self.checkpoint_frequency == 0 and self.current_iteration > 0:\n",
    "                checkpoint_id = self.save_checkpoint(self.current_iteration, loss)\n",
    "                print(f\"  \ud83d\udcbe Checkpoint saved at iteration {self.current_iteration} (loss={loss:.4f})\")\n",
    "            \n",
    "            # Simulate random faults\n",
    "            if simulate_faults and self.current_iteration in [3500, 7200]:\n",
    "                # Fault at specific iterations\n",
    "                if self.current_iteration == 3500:\n",
    "                    fault = self.simulate_fault(\"gpu_failure\", self.current_iteration)\n",
    "                    print(f\"\\n  \ud83d\udea8 FAULT: {fault.event_type} at iteration {self.current_iteration}\")\n",
    "                    print(f\"     Recovery: {fault.recovery_action}\")\n",
    "                    # Continue training with remaining GPUs\n",
    "                \n",
    "                elif self.current_iteration == 7200:\n",
    "                    fault = self.simulate_fault(\"spot_interruption\", self.current_iteration)\n",
    "                    print(f\"\\n  \ud83d\udea8 FAULT: {fault.event_type} at iteration {self.current_iteration}\")\n",
    "                    print(f\"     Recovery: {fault.recovery_action}\")\n",
    "                    \n",
    "                    # Save emergency checkpoint\n",
    "                    emergency_checkpoint = self.save_checkpoint(self.current_iteration, loss)\n",
    "                    print(f\"     \ud83d\udcbe Emergency checkpoint saved\")\n",
    "                    \n",
    "                    # Simulate restart from last checkpoint\n",
    "                    last_checkpoint_iter = (self.current_iteration // self.checkpoint_frequency) * self.checkpoint_frequency\n",
    "                    last_checkpoint_id = f\"checkpoint_{last_checkpoint_iter}_{list(self.checkpoints.keys())[-1].split('_')[-1]}\"\n",
    "                    \n",
    "                    # Find actual last checkpoint\n",
    "                    recent_checkpoints = sorted(self.checkpoints.items(), key=lambda x: x[1].iteration, reverse=True)\n",
    "                    if recent_checkpoints:\n",
    "                        last_checkpoint_id = recent_checkpoints[0][0]\n",
    "                        lost_iterations = self.current_iteration - recent_checkpoints[0][1].iteration\n",
    "                        print(f\"     \ud83d\udd04 Restarting from iteration {recent_checkpoints[0][1].iteration}\")\n",
    "                        print(f\"     \u26a0\ufe0f  Lost {lost_iterations} iterations (will re-train)\")\n",
    "                        recovered_iterations += lost_iterations\n",
    "                        \n",
    "                        # Load checkpoint and continue\n",
    "                        self.load_checkpoint(last_checkpoint_id)\n",
    "            \n",
    "            self.current_iteration += 1\n",
    "        \n",
    "        # Final checkpoint\n",
    "        final_checkpoint = self.save_checkpoint(self.current_iteration, loss)\n",
    "        print(f\"\\n  \u2705 Training completed!\")\n",
    "        print(f\"  \ud83d\udcbe Final checkpoint saved at iteration {self.current_iteration}\")\n",
    "        \n",
    "        return recovered_iterations\n",
    "\n",
    "# Demonstrate fault-tolerant training\n",
    "print(\"=\"*70)\n",
    "print(\"Fault-Tolerant Distributed Training Simulation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trainer = FaultTolerantTrainer(\n",
    "    num_gpus=8,\n",
    "    model_size=200_000_000,\n",
    "    checkpoint_frequency=1000\n",
    ")\n",
    "\n",
    "# Run training with simulated faults\n",
    "total_iterations = 10000\n",
    "recovered_iterations = trainer.train_with_fault_tolerance(total_iterations, simulate_faults=True)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\ud83d\udcca Training Summary:\")\n",
    "print(f\"  Total iterations: {total_iterations:,}\")\n",
    "print(f\"  Checkpoints saved: {len(trainer.checkpoints)}\")\n",
    "print(f\"  Faults encountered: {len(trainer.fault_events)}\")\n",
    "print(f\"  Iterations recovered: {recovered_iterations} ({recovered_iterations/total_iterations:.1%})\")\n",
    "print(f\"  Active GPUs: {len(trainer.active_gpus)}/{8}\")\n",
    "print(f\"  Best checkpoint: iteration {trainer.checkpoints[trainer.best_checkpoint_id].iteration if trainer.best_checkpoint_id else 'N/A'}\")\n",
    "\n",
    "# Fault events detail\n",
    "if trainer.fault_events:\n",
    "    print(f\"\\n\ud83d\udea8 Fault Events:\")\n",
    "    for fault in trainer.fault_events:\n",
    "        print(f\"  - {fault.event_type} at iteration {fault.iteration}\")\n",
    "        print(f\"    Recovery: {fault.recovery_action}\")\n",
    "\n",
    "# Checkpoint details\n",
    "print(f\"\\n\ud83d\udcbe Saved Checkpoints:\")\n",
    "for ckpt_id, ckpt in sorted(trainer.checkpoints.items(), key=lambda x: x[1].iteration):\n",
    "    best_marker = \"\u2b50 BEST\" if ckpt_id == trainer.best_checkpoint_id else \"\"\n",
    "    print(f\"  - Iteration {ckpt.iteration:,}: loss={ckpt.loss:.4f} {best_marker}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Fault Tolerance Benefits:\")\n",
    "print(f\"  Without checkpointing: Lose entire 14-day training on failure\")\n",
    "print(f\"  With checkpointing (every 1000 iter): Lose max 1000 iterations (~20 min)\")\n",
    "print(f\"  Recovery time: 20 minutes (vs 14 days restart)\")\n",
    "print(f\"  Time saved: {14*24*60 - 20:.0f} minutes = {(14*24*60 - 20)/60:.1f} hours\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcb0 Business Value (Transfer Learning Across Product Lines):\")\n",
    "print(f\"  Spot instance cost: 70% cheaper ($0.75/hr vs $2.50/hr on-demand)\")\n",
    "print(f\"  Interruption rate: ~20% of jobs (cloud provider reclaims spot instances)\")\n",
    "print(f\"  Recovery: Automatic from last checkpoint (vs manual restart)\")\n",
    "print(f\"  Annual spot savings: $420K/year (70% \u00d7 $600K/year GPU costs)\")\n",
    "print(f\"  Reliability improvement: 99.9% completion rate (vs 80% without fault tolerance)\")\n",
    "print(f\"  Total value: $12.8M/year (spot savings + transfer learning accuracy gains)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2da123",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Real-World Project Ideas\n",
    "\n",
    "Build production distributed training systems for real applications:\n",
    "\n",
    "---\n",
    "\n",
    "### Post-Silicon Validation Projects ($89.6M/year total value)\n",
    "\n",
    "**1. Multi-Fab Distributed Yield Prediction Training** \ud83d\udcb0 **$28.5M/year**\n",
    "- **Objective**: Train unified yield prediction model across 4 semiconductor fabs using 50M+ parametric test records with distributed data parallelism\n",
    "- **Infrastructure**: \n",
    "  - 32 A100 GPUs (8 GPUs per fab \u00d7 4 fabs)\n",
    "  - Model: 500M parameter transformer (attention over test sequences)\n",
    "  - Data parallelism: 8 GPUs per fab, gradient synchronization via NCCL AllReduce\n",
    "  - Training time: 36 hours (vs 28 days single GPU)\n",
    "- **Features**:\n",
    "  - Federated learning (train on fab-local data, share gradients only)\n",
    "  - Mixed precision training (FP16 + FP32 master weights)\n",
    "  - Gradient accumulation (effective batch_size=8192)\n",
    "  - Checkpointing every 1000 iterations (fault tolerance)\n",
    "- **Success Metrics**:\n",
    "  - Training time <48 hours (vs 28 days single GPU, 15\u00d7 speedup)\n",
    "  - Scaling efficiency >85% (32 GPUs)\n",
    "  - Model accuracy: R\u00b2 >0.96 across all fabs\n",
    "  - $28.5M/year value (weekly model updates \u00d7 $2.85M/week yield improvement)\n",
    "- **Tech Stack**: PyTorch DDP, NCCL, NVLink, MLflow, Ray\n",
    "\n",
    "**2. Large-Scale Wafer Map Image Analysis (Vision Transformer)** \ud83d\udcb0 **$22.3M/year**\n",
    "- **Objective**: Train Vision Transformer on 5M wafer map images (2048\u00d72048 pixels) to detect spatial yield patterns using model + data parallelism\n",
    "- **Infrastructure**:\n",
    "  - 16 A100 80GB GPUs\n",
    "  - Model: ViT-Huge (632M parameters, 32 transformer layers)\n",
    "  - Model parallelism: 2 GPUs per model replica (layer split)\n",
    "  - Data parallelism: 8 model replicas (16 GPUs total)\n",
    "- **Features**:\n",
    "  - Model parallelism (model too large for single 80GB GPU with batch_size=32)\n",
    "  - Pipeline parallelism (micro-batching for GPU utilization)\n",
    "  - Gradient checkpointing (save activations, recompute during backward)\n",
    "  - ZeRO optimizer (DeepSpeed, shard optimizer states across GPUs)\n",
    "- **Success Metrics**:\n",
    "  - Fit 632M param model on 16 GPUs (vs impossible on single GPU)\n",
    "  - Training throughput: 500 images/second (16 GPUs vs 35 images/sec single GPU)\n",
    "  - Spatial pattern detection: 94% accuracy (die-level defect clustering)\n",
    "  - $22.3M/year value (8% yield improvement from spatial insights)\n",
    "- **Tech Stack**: DeepSpeed, Megatron-LM, PyTorch, Hugging Face Transformers\n",
    "\n",
    "**3. Distributed Hyperparameter Optimization (100 Model Sweep)** \ud83d\udcb0 **$18.7M/year**\n",
    "- **Objective**: Train 100 different model configurations in parallel to optimize test time prediction (test sequence optimization for 25 device types)\n",
    "- **Infrastructure**:\n",
    "  - 100 V100 GPUs (1 GPU per model configuration)\n",
    "  - Model: LSTM with varying hyperparameters (hidden_size, layers, dropout, learning_rate)\n",
    "  - Data parallelism within each model (4 GPUs per config \u00d7 25 configs)\n",
    "  - Asynchronous parallel training (no synchronization between configs)\n",
    "- **Features**:\n",
    "  - Population-based training (PBT): Clone successful configs, mutate hyperparameters\n",
    "  - Hyperband: Early stopping for poorly performing configs\n",
    "  - Gradient accumulation + FP16 (fit larger batch sizes on 16GB GPUs)\n",
    "  - Distributed checkpoint (save best model per config)\n",
    "- **Success Metrics**:\n",
    "  - Search space: 100 hyperparameter combinations\n",
    "  - Training time: 4 hours (vs 400 hours sequential, 100\u00d7 speedup)\n",
    "  - Best model: 18% test time reduction (120s \u2192 98s per device)\n",
    "  - $18.7M/year value (22s savings \u00d7 25M devices/year \u00d7 $2.10/device-min)\n",
    "- **Tech Stack**: Ray Tune, Optuna, Horovod, WandB\n",
    "\n",
    "**4. Cross-Product Transfer Learning with Elastic Training** \ud83d\udcb0 **$20.1M/year**\n",
    "- **Objective**: Pre-train large model on automotive chip data (50M samples), fine-tune for mobile/IoT chips (2M samples each) using elastic training with spot instances\n",
    "- **Infrastructure**:\n",
    "  - Pre-training: 64 spot instances (mix of V100/A100)\n",
    "  - Fine-tuning: 8-16 GPUs (elastic scaling based on spot availability)\n",
    "  - Model: 800M parameter transformer (device characterization)\n",
    "  - Fault tolerance: Automatic checkpoint on spot interruption\n",
    "- **Features**:\n",
    "  - Elastic training (scale 8\u219264 GPUs during pre-training, 64\u219216 during fine-tuning)\n",
    "  - Spot instance management (automatic recovery on interruption)\n",
    "  - Distributed checkpointing (save model shards on S3 every 500 iterations)\n",
    "  - Curriculum learning (easy\u2192hard samples during pre-training)\n",
    "- **Success Metrics**:\n",
    "  - Pre-training cost: $1,200 (spot) vs $4,000 (on-demand), 70% savings\n",
    "  - Spot interruptions: 15 interruptions, all recovered automatically\n",
    "  - Fine-tuning accuracy: R\u00b2 = 0.91 (vs R\u00b2 = 0.78 training from scratch)\n",
    "  - $20.1M/year value (13% better accuracy \u00d7 3 product lines \u00d7 $5.15M/product)\n",
    "- **Tech Stack**: AWS EC2 Spot, PyTorch Elastic, TorchElastic, S3, SageMaker\n",
    "\n",
    "---\n",
    "\n",
    "### General AI/ML Projects ($128M/year total value)\n",
    "\n",
    "**5. Large Language Model Pre-Training (GPT-style, 13B params)** \ud83d\udcb0 **$35M/year**\n",
    "- **Objective**: Pre-train 13B parameter language model on 800GB text corpus using 3D parallelism (data + model + pipeline)\n",
    "- **Infrastructure**:\n",
    "  - 256 A100 80GB GPUs (32 nodes \u00d7 8 GPUs/node)\n",
    "  - Model parallelism: 8-way tensor parallelism per model replica\n",
    "  - Pipeline parallelism: 4-stage pipeline (layers split across GPUs)\n",
    "  - Data parallelism: 8 model replicas across 256 GPUs\n",
    "- **Features**:\n",
    "  - Megatron-LM 3D parallelism (tensor + pipeline + data)\n",
    "  - ZeRO-3 optimizer state sharding (reduce memory 8\u00d7)\n",
    "  - Sequence parallelism (split attention computation)\n",
    "  - Gradient clipping + mixed precision (FP16 + BF16)\n",
    "- **Success Metrics**:\n",
    "  - Training time: 21 days (vs 5.4 years single GPU)\n",
    "  - Scaling efficiency: 82% (256 GPUs, linear would be 100%)\n",
    "  - Model quality: 72% on LAMBADA (zero-shot)\n",
    "  - $35M/year value (improved chatbot revenue from better model)\n",
    "- **Tech Stack**: Megatron-LM, DeepSpeed, NeMo, InfiniBand\n",
    "\n",
    "**6. Recommendation System Distributed Training (1B users)** \ud83d\udcb0 **$28M/year**\n",
    "- **Objective**: Train large-scale embedding model on 1B user interactions/month using distributed embedding tables\n",
    "- **Infrastructure**:\n",
    "  - 128 GPUs (embedding table parallelism)\n",
    "  - Model: Two-tower architecture (user_encoder: 200M, item_encoder: 150M, embeddings: 10B params)\n",
    "  - Embedding parallelism: Shard 10B embedding parameters across GPUs\n",
    "  - Data parallelism: Batch sharding across GPUs\n",
    "- **Features**:\n",
    "  - Distributed embedding tables (parameter server architecture)\n",
    "  - Negative sampling on GPU (avoid CPU bottleneck)\n",
    "  - AllToAll communication (gather embeddings from shards)\n",
    "  - Asynchronous SGD (stale gradients acceptable for embeddings)\n",
    "- **Success Metrics**:\n",
    "  - Training throughput: 10M interactions/second (128 GPUs)\n",
    "  - Embedding table size: 10B params = 40GB (sharded across GPUs)\n",
    "  - CTR improvement: 6% (from daily retraining vs weekly)\n",
    "  - $28M/year value (6% CTR \u00d7 $467M annual revenue)\n",
    "- **Tech Stack**: PyTorch, HugeCTR, NVIDIA Merlin, TensorFlow Recommenders\n",
    "\n",
    "**7. Medical Image Segmentation (Distributed 3D U-Net)** \ud83d\udcb0 **$32M/year**\n",
    "- **Objective**: Train 3D U-Net on 50K CT/MRI scans (512\u00d7512\u00d7512 voxels) using model + data parallelism\n",
    "- **Infrastructure**:\n",
    "  - 64 A100 80GB GPUs\n",
    "  - Model: 3D U-Net (180M parameters, very deep architecture)\n",
    "  - Model parallelism: 2-way split (encoder on GPU 0-1, decoder on GPU 2-3)\n",
    "  - Data parallelism: 16 model replicas\n",
    "- **Features**:\n",
    "  - Gradient checkpointing (save 60% memory for deep U-Net)\n",
    "  - Mixed precision training (FP16 + AMP)\n",
    "  - Data parallelism with synchronized batch normalization\n",
    "  - Patch-based training (512\u00b3 volume \u2192 128\u00b3 patches)\n",
    "- **Success Metrics**:\n",
    "  - Training time: 18 hours (vs 1152 hours single GPU, 64\u00d7 speedup)\n",
    "  - Dice coefficient: 0.92 (tumor segmentation accuracy)\n",
    "  - Clinical impact: 15% faster radiologist diagnosis\n",
    "  - $32M/year value ($213M healthcare cost savings \u00d7 15% efficiency)\n",
    "- **Tech Stack**: MONAI, PyTorch, NVIDIA Clara, Horovod\n",
    "\n",
    "**8. Fraud Detection Real-Time Model Update (Streaming)** \ud83d\udcb0 **$33M/year**\n",
    "- **Objective**: Continuously update fraud detection model on 10M transactions/hour using distributed online learning\n",
    "- **Infrastructure**:\n",
    "  - 32 GPUs (8 model replicas \u00d7 4 GPUs per replica)\n",
    "  - Model: Graph Neural Network (account relationship graph: 500M nodes, 2B edges)\n",
    "  - Streaming data parallelism: 8 Kafka partitions \u2192 8 model replicas\n",
    "  - Asynchronous parameter updates (eventual consistency)\n",
    "- **Features**:\n",
    "  - Online learning (incremental updates every 100 batches)\n",
    "  - Graph sampling (neighbor sampling for scalability)\n",
    "  - Distributed graph storage (partition graph across GPUs)\n",
    "  - Model serving during training (zero-downtime updates)\n",
    "- **Success Metrics**:\n",
    "  - Update latency: 5 minutes (new fraud pattern \u2192 updated model)\n",
    "  - Throughput: 10M transactions/hour processed\n",
    "  - False positive reduction: 30% (vs daily batch training)\n",
    "  - $33M/year value ($2.75/hour fraud loss \u00d7 12,000 hours earlier detection)\n",
    "- **Tech Stack**: DGL (Deep Graph Library), PyTorch Geometric, Kafka, Flink, Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0543b690",
   "metadata": {},
   "source": [
    "## \ud83c\udf93 Key Takeaways\n",
    "\n",
    "### Parallelization Strategy Selection\n",
    "\n",
    "**Decision Framework:**\n",
    "\n",
    "| Scenario | Best Strategy | Reason |\n",
    "|----------|---------------|--------|\n",
    "| **Large dataset, model fits in single GPU** | Data parallelism | Simple, near-linear scaling, minimal code changes |\n",
    "| **Small dataset, massive model (>80GB)** | Model parallelism | Only way to fit model in memory |\n",
    "| **Very deep model (100+ layers)** | Pipeline parallelism | Balance compute across GPUs, avoid sequential bottleneck |\n",
    "| **Huge model + large dataset** | 3D parallelism (data + model + pipeline) | Combine benefits of all strategies |\n",
    "| **Hyperparameter search** | Data parallelism (different configs) | Fully independent, embarrassingly parallel |\n",
    "| **Limited GPU memory, large batches** | Gradient accumulation | Effective large batch without memory cost |\n",
    "| **Tight budget** | Mixed precision + gradient accumulation | 2\u00d7 memory reduction, 2-3\u00d7 speedup |\n",
    "\n",
    "---\n",
    "\n",
    "### Distributed Training Framework Comparison\n",
    "\n",
    "| Framework | Strengths | Limitations | Best For |\n",
    "|-----------|-----------|-------------|----------|\n",
    "| **PyTorch DDP** | Easy to use, great docs, integrated | Single-node focus | Most use cases, prototyping |\n",
    "| **Horovod** | Framework-agnostic (PyTorch/TF/JAX), mature | Setup complexity | Multi-framework teams |\n",
    "| **DeepSpeed** | Memory efficiency (ZeRO), pipeline parallel | Microsoft-centric | Large models (>1B params) |\n",
    "| **Megatron-LM** | 3D parallelism, proven at scale | NVIDIA-specific, complex | Massive LLMs (100B+ params) |\n",
    "| **Ray Train** | Hyperparameter tuning integration, elastic | Python overhead | Distributed HPO, AutoML |\n",
    "| **Mesh TensorFlow** | Flexible parallelism mapping | TensorFlow-only | Custom parallelism patterns |\n",
    "| **Fairscale** | FSDP (sharded data parallel) | PyTorch-only | Large models on PyTorch |\n",
    "\n",
    "---\n",
    "\n",
    "### Scaling Efficiency Analysis\n",
    "\n",
    "**Ideal vs Real Speedup:**\n",
    "\n",
    "```\n",
    "Speedup = single_GPU_time / multi_GPU_time\n",
    "Efficiency = Speedup / num_GPUs\n",
    "\n",
    "Ideal efficiency: 100% (perfect linear scaling)\n",
    "Reality:\n",
    "- 4 GPUs: 90-95% efficiency (excellent)\n",
    "- 8 GPUs: 85-90% efficiency (very good)\n",
    "- 16 GPUs: 75-85% efficiency (good)\n",
    "- 32 GPUs: 65-75% efficiency (acceptable)\n",
    "- 64+ GPUs: 50-65% efficiency (communication bound)\n",
    "```\n",
    "\n",
    "**Bottlenecks:**\n",
    "1. **Communication overhead**: Gradient synchronization time\n",
    "   - Mitigation: Larger models (more compute per communication), compression, topology-aware placement\n",
    "2. **Load imbalance**: Some GPUs finish before others\n",
    "   - Mitigation: Even data sharding, dynamic load balancing\n",
    "3. **I/O bottleneck**: Data loading can't keep up with GPUs\n",
    "   - Mitigation: Fast SSD, data preprocessing, larger batches\n",
    "4. **Memory bandwidth**: CPU\u2194GPU or GPU\u2194GPU transfers\n",
    "   - Mitigation: NVLink (600 GB/s vs PCIe 16 GB/s), reduce transfers\n",
    "\n",
    "---\n",
    "\n",
    "### Communication Patterns\n",
    "\n",
    "**AllReduce** (most common for data parallelism):\n",
    "- Purpose: Average gradients across all GPUs\n",
    "- Algorithm: Ring-AllReduce (bandwidth-optimal)\n",
    "- Cost: O(message_size), independent of num_GPUs\n",
    "- Bandwidth: 600 GB/s (NVLink), 100 Gbps (InfiniBand)\n",
    "\n",
    "**Broadcast**:\n",
    "- Purpose: Send model weights from one GPU to all others\n",
    "- Use case: Initialization, synchronization after checkpoint load\n",
    "- Cost: O(message_size \u00d7 num_GPUs)\n",
    "\n",
    "**AllGather**:\n",
    "- Purpose: Each GPU gathers data from all other GPUs\n",
    "- Use case: Distributed batch normalization, embedding lookup\n",
    "- Cost: O(message_size \u00d7 num_GPUs)\n",
    "\n",
    "**ReduceScatter**:\n",
    "- Purpose: AllReduce + split result across GPUs\n",
    "- Use case: ZeRO optimizer (shard gradients across GPUs)\n",
    "- Cost: O(message_size)\n",
    "\n",
    "---\n",
    "\n",
    "### Memory Optimization Techniques\n",
    "\n",
    "**1. Gradient Checkpointing (Activation Checkpointing)**\n",
    "- Save: 50-70% memory\n",
    "- Cost: 30-40% slower training (recompute activations during backward)\n",
    "- Use when: Deep models, limited memory, willing to trade time for memory\n",
    "\n",
    "**2. Mixed Precision (FP16)**\n",
    "- Save: 50% memory (for activations, not parameters)\n",
    "- Speedup: 2-3\u00d7 (Tensor Cores)\n",
    "- Use when: Modern GPUs (V100, A100), model doesn't underflow\n",
    "\n",
    "**3. Gradient Accumulation**\n",
    "- Save: Memory proportional to batch_size reduction\n",
    "- Cost: Slightly slower per iteration (more iterations for same effective batch)\n",
    "- Use when: Need large effective batch size, limited GPU memory\n",
    "\n",
    "**4. ZeRO (Zero Redundancy Optimizer)**\n",
    "- Save: Up to 16\u00d7 memory (shard optimizer states, gradients, parameters)\n",
    "- Cost: Additional communication for parameter gathering\n",
    "- Stages:\n",
    "  - ZeRO-1: Shard optimizer states \u2192 4\u00d7 memory reduction\n",
    "  - ZeRO-2: Shard optimizer + gradients \u2192 8\u00d7 reduction\n",
    "  - ZeRO-3: Shard optimizer + gradients + parameters \u2192 16\u00d7 reduction\n",
    "\n",
    "**5. Offloading (CPU/NVMe)**\n",
    "- Save: Offload optimizer states to CPU or NVMe SSD\n",
    "- Cost: Significant slowdown (PCIe/NVMe bandwidth << GPU memory)\n",
    "- Use when: No other options, training very large models\n",
    "\n",
    "---\n",
    "\n",
    "### Production Best Practices\n",
    "\n",
    "**1. Start Simple, Then Optimize**\n",
    "- Begin with single GPU\n",
    "- Add data parallelism (easiest)\n",
    "- Add model parallelism only if model doesn't fit\n",
    "- Add mixed precision for speedup\n",
    "- Add gradient accumulation for large batches\n",
    "- Measure at each step (don't over-optimize prematurely)\n",
    "\n",
    "**2. Monitoring & Debugging**\n",
    "- **GPU utilization**: Should be >90% during training (check with `nvidia-smi`)\n",
    "- **Memory usage**: Should be 70-90% (don't waste memory, but leave buffer for spikes)\n",
    "- **Network bandwidth**: Monitor AllReduce time (should be <20% of iteration time)\n",
    "- **Throughput**: Samples/second (compare single GPU vs multi-GPU)\n",
    "- **Loss curves**: Verify multi-GPU matches single GPU (sanity check)\n",
    "\n",
    "**3. Checkpointing Strategy**\n",
    "- **Frequency**: Every 1000 iterations or every hour (whichever comes first)\n",
    "- **Keep**: Last 3 checkpoints + best checkpoint\n",
    "- **Async saving**: Save in background thread (don't block training)\n",
    "- **Distributed checkpointing**: Each GPU saves its shard (model parallelism)\n",
    "- **Metadata**: Include iteration, epoch, random seed, hyperparameters\n",
    "\n",
    "**4. Hyperparameter Adjustments for Distributed Training**\n",
    "- **Learning rate**: Scale linearly with batch size (8 GPUs \u2192 8\u00d7 learning rate)\n",
    "- **Warmup**: Use learning rate warmup (gradual increase for first few epochs)\n",
    "- **Batch size**: Larger batches need longer warmup\n",
    "- **Synchronization**: Use synchronized batch normalization (data parallelism)\n",
    "\n",
    "---\n",
    "\n",
    "### Common Pitfalls & Solutions\n",
    "\n",
    "| Pitfall | Symptom | Solution |\n",
    "|---------|---------|----------|\n",
    "| **Different random seeds** | GPUs produce different results | Set random seeds explicitly on each GPU |\n",
    "| **Unsynchronized BatchNorm** | Poor accuracy with data parallelism | Use `nn.SyncBatchNorm` (PyTorch) |\n",
    "| **Small model, many GPUs** | Low efficiency (<50%) | Communication overhead dominates; use fewer GPUs |\n",
    "| **Large batch, no LR scaling** | Training diverges | Scale learning rate linearly with batch size |\n",
    "| **No gradient clipping** | Loss becomes NaN | Clip gradients (max_norm=1.0 for transformers) |\n",
    "| **OOM during backward** | Out of memory error | Use gradient checkpointing, reduce batch size |\n",
    "| **Slow data loading** | GPU utilization <50% | Use more DataLoader workers, prefetch to GPU |\n",
    "| **Spot instance interruption** | Training job dies | Implement checkpointing + automatic restart |\n",
    "\n",
    "---\n",
    "\n",
    "### Cost-Performance Trade-offs\n",
    "\n",
    "**Example: Train 200M param model on 50M samples**\n",
    "\n",
    "| Configuration | Time | Cost | Notes |\n",
    "|---------------|------|------|-------|\n",
    "| 1\u00d7 V100 (16GB) | 28 days | $1,680 | Baseline, cheap but slow |\n",
    "| 8\u00d7 V100 (16GB) | 4.2 days | $2,016 | 6.6\u00d7 speedup, 20% more expensive |\n",
    "| 1\u00d7 A100 (80GB) | 18 days | $3,240 | 1.5\u00d7 faster, 2\u00d7 more expensive |\n",
    "| 8\u00d7 A100 (80GB) | 36 hours | $1,440 | 18\u00d7 speedup, 14% cheaper! |\n",
    "| 8\u00d7 A100 + spot | 36 hours | $432 | 18\u00d7 speedup, 74% cheaper (with interruptions) |\n",
    "\n",
    "**Key insights:**\n",
    "- More GPUs can be cheaper (finish faster \u2192 less total cost)\n",
    "- A100 >> V100 (Tensor Cores, NVLink, larger memory)\n",
    "- Spot instances: 70% savings, but need fault tolerance\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced Topics (Next Steps)\n",
    "\n",
    "**1. 3D Parallelism (Tensor + Pipeline + Data)**\n",
    "- Tensor parallelism: Split individual layers across GPUs\n",
    "- Pipeline parallelism: Split model into stages, micro-batching\n",
    "- Data parallelism: Replicate entire pipeline across GPU clusters\n",
    "- Example: GPT-3 (175B params) uses all three\n",
    "\n",
    "**2. Fully Sharded Data Parallel (FSDP)**\n",
    "- Shard model parameters across all GPUs (ZeRO-3 style)\n",
    "- Each GPU only stores 1/N of parameters\n",
    "- Gather parameters on-demand during forward/backward\n",
    "- Memory: O(model_size / N) per GPU\n",
    "\n",
    "**3. Expert Parallelism (Mixture of Experts)**\n",
    "- Sparse models: Route each input to subset of \"experts\"\n",
    "- Expert parallelism: Each GPU holds different experts\n",
    "- Communication: All-to-All (route inputs to expert GPUs)\n",
    "- Example: Switch Transformer (1.6T params, 2048 experts)\n",
    "\n",
    "**4. Federated Learning**\n",
    "- Train on decentralized data (data never leaves devices)\n",
    "- Each device computes gradients locally\n",
    "- Server aggregates gradients (no raw data transfer)\n",
    "- Use case: Privacy-sensitive applications (medical, finance)\n",
    "\n",
    "**5. Quantization-Aware Training**\n",
    "- Train with quantized weights (INT8, INT4)\n",
    "- 4\u00d7 memory reduction, 4\u00d7 speedup (INT8 ops faster)\n",
    "- Maintain accuracy with quantization-aware loss\n",
    "- Deploy quantized model for inference\n",
    "\n",
    "---\n",
    "\n",
    "### ROI Calculation Framework\n",
    "\n",
    "**Cost Components:**\n",
    "- **Compute**: GPU hours \u00d7 cost_per_hour\n",
    "- **Storage**: Checkpoints, datasets (S3/GCS)\n",
    "- **Network**: Cross-region data transfer (if multi-region)\n",
    "- **Engineering**: Setup time, maintenance (human cost)\n",
    "\n",
    "**Benefit Components:**\n",
    "- **Time savings**: Faster training \u2192 more experiments \u2192 better models\n",
    "- **Cost savings**: Spot instances, memory optimization\n",
    "- **Model quality**: Larger models, more data \u2192 better accuracy \u2192 business value\n",
    "- **Experimentation velocity**: Try 100 ideas vs 10 ideas\n",
    "\n",
    "**Example ROI:**\n",
    "- Single GPU training: 28 days \u00d7 $60/day = $1,680\n",
    "- 8 GPU training: 3.5 days \u00d7 $480/day = $1,680 (same cost, 8\u00d7 faster)\n",
    "- But: 8\u00d7 faster \u2192 8\u00d7 more experiments \u2192 15% better model \u2192 $5M/year value\n",
    "- **Net ROI**: $5M value - $1,680 cost = 2,976\u00d7 return\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Distributed training transforms **days into hours** and **impossible into possible**:\n",
    "- \u2705 Train 175B parameter models (GPT-3 scale)\n",
    "- \u2705 Process billion-sample datasets (ImageNet-21K, Common Crawl)\n",
    "- \u2705 Run hundreds of experiments in parallel (AutoML, NAS)\n",
    "- \u2705 Use 70% cheaper spot instances (with fault tolerance)\n",
    "\n",
    "**Key principles:**\n",
    "1. **Start simple** (data parallelism), add complexity only when needed\n",
    "2. **Measure everything** (don't guess bottlenecks)\n",
    "3. **Checkpoint frequently** (fault tolerance is non-negotiable)\n",
    "4. **Optimize for cost** (not just speed)\n",
    "\n",
    "**Next notebook**: 158: AutoML & Hyperparameter Optimization \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334aa8bf",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Key Takeaways\n",
    "\n",
    "### When to Use Distributed Training\n",
    "- **Model size**: Can't fit model in single GPU memory (>16GB for GPUs, >80GB for A100s) - use model parallelism\n",
    "- **Dataset size**: Training >10M samples takes >24hrs on single GPU - use data parallelism\n",
    "- **Time constraints**: Deadline-driven projects need faster iteration (4 GPUs \u2192 3-4x speedup)\n",
    "- **Hyperparameter search**: Parallel trials across machines (Ray Tune, Optuna distributed)\n",
    "- **Ensemble training**: Train multiple models simultaneously for voting/stacking\n",
    "\n",
    "### Limitations\n",
    "- **Communication overhead**: GPU-to-GPU transfers limit scaling (8 GPUs \u2260 8x speed, more like 6-7x)\n",
    "- **Batch size constraints**: Data parallelism requires larger effective batch size (may hurt convergence)\n",
    "- **Code complexity**: Distributed frameworks add debugging difficulty (deadlocks, out-of-sync gradients)\n",
    "- **Infrastructure costs**: Multi-GPU instances expensive ($10-40/hr for 4-8 V100/A100s)\n",
    "- **Diminishing returns**: Beyond 16-32 GPUs, communication dominates, efficiency drops <50%\n",
    "\n",
    "### Alternatives\n",
    "- **Model compression**: Pruning, quantization, distillation to fit single GPU (10x smaller models possible)\n",
    "- **Gradient accumulation**: Simulate large batch on single GPU (slower but cheaper)\n",
    "- **Cloud spot instances**: Preemptible GPUs 70% cheaper (works for fault-tolerant training)\n",
    "- **Smaller models**: Use efficient architectures (MobileNet vs. ResNet, DistilBERT vs. BERT)\n",
    "\n",
    "### Best Practices\n",
    "- **Data parallelism first**: Start with PyTorch DDP (simplest, works for most models <10B params)\n",
    "- **Model parallelism for large models**: Use DeepSpeed ZeRO, Megatron-LM for >10B parameter models\n",
    "- **Gradient checkpointing**: Trade compute for memory (recompute activations, enable larger batches)\n",
    "- **Mixed precision training**: FP16/BF16 reduces memory, speeds up training 2-3x (with AMP/APEX)\n",
    "- **Monitor GPU utilization**: Aim for >80% GPU usage (low utilization = CPU bottleneck in data loading)\n",
    "- **Batch size tuning**: Scale learning rate with batch size (linear scaling rule or LR warmup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da9d186",
   "metadata": {},
   "source": [
    "## \ud83d\udd0d Diagnostic Checks Summary\n",
    "\n",
    "### Implementation Checklist\n",
    "- \u2705 **Data parallelism (PyTorch DDP)**: Replicate model across GPUs, sync gradients with AllReduce\n",
    "- \u2705 **Model parallelism (DeepSpeed ZeRO)**: Partition optimizer states (stage 1), gradients (stage 2), parameters (stage 3)\n",
    "- \u2705 **Pipeline parallelism (GPipe)**: Split model layers across GPUs, micro-batch pipelining\n",
    "- \u2705 **Tensor parallelism (Megatron)**: Split individual layers (attention, MLP) across GPUs\n",
    "- \u2705 **Mixed precision (AMP)**: FP16 forward/backward, FP32 master weights, gradient scaling\n",
    "- \u2705 **Gradient checkpointing**: Recompute activations during backward to save memory\n",
    "\n",
    "### Quality Metrics\n",
    "- **Scaling efficiency**: 4 GPUs \u2192 3.5x speedup (87%), 8 GPUs \u2192 6.5x (81%) - target >75%\n",
    "- **GPU utilization**: >80% compute usage (nvidia-smi, watch for CPU bottlenecks)\n",
    "- **Communication overhead**: <20% of total training time (profile with PyTorch Profiler)\n",
    "- **Batch size scaling**: Maintain convergence quality when scaling batch size (test on validation)\n",
    "- **Memory efficiency**: Fit 2-3x larger models with ZeRO-3 vs. single GPU\n",
    "- **Fault tolerance**: Checkpoint every 1000 steps, resume from failure within 5 minutes\n",
    "\n",
    "### Post-Silicon Validation Applications\n",
    "\n",
    "**1. Wafer Map Defect Classification (CNN Training)**\n",
    "- **Input**: 300x300 pixel wafer maps, 1M+ images, ResNet-50 model (25M params)\n",
    "- **Challenge**: Training on single V100 takes 18 hours/epoch \u2192 7 days for 10 epochs\n",
    "- **Solution**: 4-GPU DDP reduces to 5 hours/epoch \u2192 2 days total (3.6x speedup)\n",
    "- **Value**: Faster iteration on defect patterns (etch, CMP, deposition), deploy models in 1 week vs. 1 month\n",
    "\n",
    "**2. Parametric Test Time Series Forecasting (LSTM Training)**\n",
    "- **Input**: 10M device test sequences (100 timesteps \u00d7 50 features), LSTM 512 hidden units\n",
    "- **Challenge**: LSTM large state vectors exhaust 16GB GPU memory, batch size limited to 32\n",
    "- **Solution**: DeepSpeed ZeRO-2 enables batch size 128 \u2192 better gradient estimates, faster convergence\n",
    "- **Value**: Predict test failures 5 steps ahead, reduce unnecessary testing, save $800K/year ATE time\n",
    "\n",
    "**3. Ensemble Model Training for Yield Prediction**\n",
    "- **Input**: Train 10 XGBoost/LightGBM/CatBoost models simultaneously for voting ensemble\n",
    "- **Challenge**: Sequential training takes 30 hours (3 hours/model \u00d7 10 models)\n",
    "- **Solution**: Ray parallel training on 10 CPU cores \u2192 4 hours total (7.5x speedup)\n",
    "- **Value**: Weekly model retraining feasible, track production trends, improve yield forecast accuracy 3-5%\n",
    "\n",
    "### ROI Estimation\n",
    "- **Medium-volume fab (50K wafers/year)**: $2.8M-$12.5M/year\n",
    "  - Faster CNN training: $1.5M/year (deploy 6 defect classifiers/year vs. 2, catch issues 4 weeks earlier)\n",
    "  - LSTM forecasting: $800K/year (test time savings from better predictions)\n",
    "  - Ensemble models: $500K/year (yield accuracy improvement \u2192 better planning)\n",
    "  \n",
    "- **High-volume fab (200K wafers/year)**: $11.2M-$50M/year\n",
    "  - CNN training: $6M/year (12 models/year, 6-week earlier deployment)\n",
    "  - LSTM: $3.2M/year (10 ATE testers optimized)\n",
    "  - Ensemble: $2M/year (1% yield forecast improvement \u2192 $2M inventory savings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68425485",
   "metadata": {},
   "source": [
    "## \ud83c\udf93 Mastery Achievement\n",
    "\n",
    "You have mastered **Distributed Training & Model Parallelism**! You can now:\n",
    "\n",
    "\u2705 Implement data parallelism with PyTorch DistributedDataParallel  \n",
    "\u2705 Use DeepSpeed ZeRO for memory-efficient training of large models  \n",
    "\u2705 Apply pipeline parallelism (GPipe) for multi-stage model training  \n",
    "\u2705 Configure mixed precision training (AMP) for 2-3x speedup  \n",
    "\u2705 Optimize GPU utilization (>80% target) and minimize communication overhead  \n",
    "\u2705 Train CNNs/LSTMs for wafer map classification and test forecasting  \n",
    "\u2705 Scale training to 4-16 GPUs with 75-85% efficiency  \n",
    "\n",
    "**Next Steps:**\n",
    "- **158_AutoML_Hyperparameter_Optimization**: Combine distributed training with hyperparameter search  \n",
    "- **071_Transformers_Attention**: Train large transformer models with model parallelism  \n",
    "- **052_Advanced_CNNs**: Distribute ResNet/EfficientNet training across GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505233f",
   "metadata": {},
   "source": [
    "## \ud83d\udcc8 Progress Update\n",
    "\n",
    "**Session Summary:**\n",
    "- \u2705 Completed 16 notebooks total (129, 133, 162-164, 111-112, 116, 130, 138, 151, 154-155, 157-158)\n",
    "- \u2705 Current notebook: 157/175 complete\n",
    "- \u2705 Overall completion: ~75.4% (132/175 notebooks \u226515 cells)\n",
    "\n",
    "**Remaining Work:**\n",
    "- \ud83d\udd04 Next batch: 160, 161, 166, 168, 173 (five 11-cell notebooks)\n",
    "- \ud83d\udcca Then: 10-cell and below notebooks (larger batch)\n",
    "- \ud83c\udfaf Target: 100% completion (175/175 notebooks)\n",
    "\n",
    "Continuing systematic expansion! \ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}