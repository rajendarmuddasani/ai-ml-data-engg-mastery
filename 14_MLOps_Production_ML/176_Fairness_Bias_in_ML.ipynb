{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 176: Fairness Bias in ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af71f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fairness & Bias in ML Environment Setup\n",
    "========================================\n",
    "\n",
    "Purpose: Import libraries for fairness analysis and bias mitigation.\n",
    "\n",
    "Key Libraries:\n",
    "- fairlearn: Microsoft's fairness toolkit\n",
    "- aif360: IBM's AI Fairness 360 toolkit\n",
    "- scikit-learn: ML models and metrics\n",
    "- numpy/pandas: Data manipulation\n",
    "\n",
    "Why This Matters:\n",
    "- Production-ready fairness tools\n",
    "- Industry-standard metrics and mitigation algorithms\n",
    "- Regulatory compliance support\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, classification_report\n",
    ")\n",
    "\n",
    "# Fairness Libraries (optional - will implement from scratch)\n",
    "try:\n",
    "    from fairlearn.metrics import (\n",
    "        demographic_parity_difference,\n",
    "        equalized_odds_difference,\n",
    "        MetricFrame\n",
    "    )\n",
    "    from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "    FAIRLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"\u26a0\ufe0f fairlearn not installed. Using custom implementations.\")\n",
    "    print(\"   Install: pip install fairlearn\")\n",
    "    FAIRLEARN_AVAILABLE = False\n",
    "\n",
    "# Visualization\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\u2705 Fairness & Bias Environment Ready!\")\n",
    "print(\"\\nKey Capabilities:\")\n",
    "print(\"  - Bias detection (demographic parity, equal opportunity)\")\n",
    "print(\"  - Fairness metrics (20+ metrics implemented)\")\n",
    "print(\"  - Debiasing techniques (pre/in/post-processing)\")\n",
    "print(\"  - Fairness-accuracy tradeoff analysis\")\n",
    "print(\"  - Regulatory compliance reporting\")\n",
    "print(f\"\\nFairlearn library: {'\u2705 Available' if FAIRLEARN_AVAILABLE else '\u274c Not installed (using custom)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada71f2",
   "metadata": {},
   "source": [
    "## \ud83e\uddee Mathematical Foundation: Fairness Metrics\n",
    "\n",
    "### **1. Demographic Parity (Statistical Parity)**\n",
    "\n",
    "**Definition:** Positive prediction rates are equal across protected groups.\n",
    "\n",
    "$$\n",
    "P(\\hat{Y} = 1 | A = 0) = P(\\hat{Y} = 1 | A = 1)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{Y}$: Model prediction (0 = negative, 1 = positive)\n",
    "- $A$: Protected attribute (0 = privileged group, 1 = unprivileged group)\n",
    "\n",
    "**Metric:** Demographic Parity Difference (DPD)\n",
    "\n",
    "$$\n",
    "\\text{DPD} = P(\\hat{Y} = 1 | A = 1) - P(\\hat{Y} = 1 | A = 0)\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- $\\text{DPD} = 0$: Perfect demographic parity\n",
    "- $|\\text{DPD}| < 0.1$: Acceptable fairness (industry guideline)\n",
    "- $|\\text{DPD}| > 0.2$: Significant bias\n",
    "\n",
    "**When to Use:**\n",
    "- Loan approvals, hiring decisions, resource allocation\n",
    "- When **equal selection rates** are desired regardless of ground truth\n",
    "\n",
    "**Limitation:**\n",
    "- Ignores prediction accuracy\n",
    "- May conflict with Equal Opportunity\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Equal Opportunity**\n",
    "\n",
    "**Definition:** True positive rates (TPR) are equal across protected groups.\n",
    "\n",
    "$$\n",
    "P(\\hat{Y} = 1 | Y = 1, A = 0) = P(\\hat{Y} = 1 | Y = 1, A = 1)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $Y$: Ground truth label (0 = negative, 1 = positive)\n",
    "\n",
    "**Metric:** Equal Opportunity Difference (EOD)\n",
    "\n",
    "$$\n",
    "\\text{EOD} = \\text{TPR}_{A=1} - \\text{TPR}_{A=0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{TPR}_A = \\frac{\\text{TP}_A}{\\text{TP}_A + \\text{FN}_A}\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- $\\text{EOD} = 0$: Perfect equal opportunity\n",
    "- $|\\text{EOD}| < 0.1$: Acceptable\n",
    "- $|\\text{EOD}| > 0.15$: Concerning bias\n",
    "\n",
    "**When to Use:**\n",
    "- Medical diagnosis, fraud detection, disease screening\n",
    "- When **correctly identifying positive cases** is critical for all groups\n",
    "\n",
    "**Advantage:**\n",
    "- Accounts for ground truth (more meaningful than Demographic Parity)\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Equalized Odds**\n",
    "\n",
    "**Definition:** Both TPR and FPR are equal across protected groups.\n",
    "\n",
    "$$\n",
    "P(\\hat{Y} = 1 | Y = y, A = 0) = P(\\hat{Y} = 1 | Y = y, A = 1) \\\\quad \\\\forall y \\\\in \\\\{0, 1\\\\}\n",
    "$$\n",
    "\n",
    "**Metric:** Equalized Odds Difference (max of TPR and FPR differences)\n",
    "\n",
    "$$\n",
    "\\text{Equalized Odds Diff} = \\\\max(|\\text{TPR}_{A=1} - \\text{TPR}_{A=0}|, |\\text{FPR}_{A=1} - \\text{FPR}_{A=0}|)\n",
    "$$\n",
    "\n",
    "**When to Use:**\n",
    "- Criminal justice, credit scoring\n",
    "- When **both false positives and false negatives** have serious consequences\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Calibration (Predictive Parity)**\n",
    "\n",
    "**Definition:** Predicted probabilities match true positive rates across groups.\n",
    "\n",
    "$$\n",
    "P(Y = 1 | \\\\hat{Y} = s, A = 0) = P(Y = 1 | \\\\hat{Y} = s, A = 1) \\\\quad \\\\forall s\n",
    "$$\n",
    "\n",
    "**Metric:** Calibration Difference (across probability bins)\n",
    "\n",
    "$$\n",
    "\\text{Calibration Diff} = \\\\max_s |P(Y=1|\\\\hat{Y}=s, A=1) - P(Y=1|\\\\hat{Y}=s, A=0)|\n",
    "$$\n",
    "\n",
    "**Example:**\n",
    "- If model predicts 70% default probability for both groups\n",
    "- **Calibrated:** 70% of each group actually defaults\n",
    "- **Not calibrated:** 70% of Group A defaults, 50% of Group B defaults\n",
    "\n",
    "**When to Use:**\n",
    "- Risk assessment where probability interpretation matters\n",
    "- Insurance, credit scoring\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Individual Fairness**\n",
    "\n",
    "**Definition:** Similar individuals receive similar predictions.\n",
    "\n",
    "$$\n",
    "d(\\\\hat{y}_i, \\\\hat{y}_j) \\\\leq L \\\\cdot d(x_i, x_j)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $d(\\\\hat{y}_i, \\\\hat{y}_j)$: Distance between predictions\n",
    "- $d(x_i, x_j)$: Distance between feature vectors\n",
    "- $L$: Lipschitz constant (controls sensitivity)\n",
    "\n",
    "**Challenge:** Defining \\\"similarity\\\" is subjective and domain-dependent\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Impossibility Theorem**\n",
    "\n",
    "**Chouldechova (2017) & Kleinberg et al. (2017):**\n",
    "\n",
    "**Cannot simultaneously satisfy:**\n",
    "1. **Calibration** (predictive parity)\n",
    "2. **Equal Opportunity** (equal TPR)\n",
    "3. **Demographic Parity** (equal selection rate)\n",
    "\n",
    "**Unless:** Base rates are equal across groups: $P(Y=1|A=0) = P(Y=1|A=1)$\n",
    "\n",
    "**Practical Implication:**\n",
    "- Must choose which fairness criterion to prioritize\n",
    "- Trade-offs are inevitable\n",
    "- Context and stakeholder input essential\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Fairness Metrics Summary Table**\n",
    "\n",
    "| **Metric** | **Formula** | **Focus** | **Use Case** | **Threshold** |\n",
    "|------------|-------------|-----------|--------------|---------------|\n",
    "| **Demographic Parity** | $P(\\\\hat{Y}=1\\\\|A=1) - P(\\\\hat{Y}=1\\\\|A=0)$ | Selection rate | Hiring, lending | $\\\\|\\\\text{DPD}\\\\| < 0.1$ |\n",
    "| **Equal Opportunity** | $\\\\text{TPR}_{A=1} - \\\\text{TPR}_{A=0}$ | True positive rate | Medical diagnosis | $\\\\|\\\\text{EOD}\\\\| < 0.1$ |\n",
    "| **Equalized Odds** | $\\\\max(\\\\|\\\\Delta \\\\text{TPR}\\\\|, \\\\|\\\\Delta \\\\text{FPR}\\\\|)$ | TPR & FPR | Criminal justice | $< 0.1$ |\n",
    "| **Calibration** | $\\\\max_s \\\\|P(Y=1\\\\|\\\\hat{Y}=s, A)\\\\|$ | Probability accuracy | Risk assessment | $< 0.05$ |\n",
    "| **Predictive Parity** | $\\\\text{PPV}_{A=1} - \\\\text{PPV}_{A=0}$ | Precision | Credit scoring | $\\\\|\\\\Delta \\\\text{PPV}\\\\| < 0.1$ |\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Debiasing Techniques**\n",
    "\n",
    "#### **Pre-Processing (Data Transformation)**\n",
    "\n",
    "**Reweighting:**\n",
    "\n",
    "$$\n",
    "w_i = \\\\frac{P(A = a_i)}{P(A = a_i, Y = y_i)} \\\\cdot P(Y = y_i)\n",
    "$$\n",
    "\n",
    "Reweight samples to equalize joint distribution of $(A, Y)$.\n",
    "\n",
    "**Sampling:**\n",
    "- **Upsampling:** Duplicate minority group samples\n",
    "- **Downsampling:** Remove majority group samples\n",
    "- **SMOTE:** Synthetic Minority Over-sampling\n",
    "\n",
    "**Disparate Impact Remover:**\n",
    "- Modify features to decorrelate with protected attribute\n",
    "- Preserve predictive power while reducing discrimination\n",
    "\n",
    "---\n",
    "\n",
    "#### **In-Processing (Constrained Optimization)**\n",
    "\n",
    "**Fairness Regularization:**\n",
    "\n",
    "$$\n",
    "\\\\min_\\\\theta \\\\mathcal{L}(\\\\theta) + \\\\lambda \\\\cdot \\\\text{Fairness Penalty}(\\\\theta)\n",
    "$$\n",
    "\n",
    "Example penalties:\n",
    "- Demographic Parity: $\\\\lambda |P(\\\\hat{Y}=1|A=1) - P(\\\\hat{Y}=1|A=0)|$\n",
    "- Equal Opportunity: $\\\\lambda |\\\\text{TPR}_{A=1} - \\\\text{TPR}_{A=0}|$\n",
    "\n",
    "**Adversarial Debiasing:**\n",
    "\n",
    "$$\n",
    "\\\\min_\\\\theta \\\\mathcal{L}_{pred}(\\\\theta) - \\\\lambda \\\\mathcal{L}_{adv}(\\\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\\\mathcal{L}_{pred}$: Prediction loss\n",
    "- $\\\\mathcal{L}_{adv}$: Adversary's ability to predict $A$ from $\\\\hat{Y}$\n",
    "\n",
    "**Adversarial Network:**\n",
    "- Predictor: Maximize prediction accuracy\n",
    "- Adversary: Predict protected attribute from predictions\n",
    "- Goal: Predictions are accurate but uninformative about $A$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Post-Processing (Threshold Adjustment)**\n",
    "\n",
    "**Group-Specific Thresholds:**\n",
    "\n",
    "Find thresholds $\\\\tau_0, \\\\tau_1$ satisfying fairness constraint:\n",
    "\n",
    "$$\n",
    "\\\\text{For Equal Opportunity: } P(\\\\hat{Y}=1|Y=1, A=0) = P(\\\\hat{Y}=1|Y=1, A=1)\n",
    "$$\n",
    "\n",
    "Implementation:\n",
    "- Compute $\\\\tau_0, \\\\tau_1$ such that TPR is equal across groups\n",
    "- Apply: $\\\\hat{Y}_i = \\\\mathbb{1}[p_i > \\\\tau_{A_i}]$\n",
    "\n",
    "**Calibration via Isotonic Regression:**\n",
    "- Fit isotonic regression per group\n",
    "- Map raw scores to calibrated probabilities\n",
    "\n",
    "---\n",
    "\n",
    "### **Toy Example: Demographic Parity Calculation**\n",
    "\n",
    "**Scenario:** Hiring model predicts 100 candidates (50 Group A, 50 Group B)\n",
    "\n",
    "**Predictions:**\n",
    "- Group A: 30 hired (60% selection rate)\n",
    "- Group B: 20 hired (40% selection rate)\n",
    "\n",
    "**Demographic Parity Difference:**\n",
    "\n",
    "$$\n",
    "\\\\text{DPD} = 0.40 - 0.60 = -0.20\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- $|\\\\text{DPD}| = 0.20$: Significant bias (exceeds 0.1 threshold)\n",
    "- Group B is **20% less likely** to be hired\n",
    "- **Action required:** Apply debiasing technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912482f2",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Synthetic Biased Dataset Generation\n",
    "\n",
    "**Purpose:** Create realistic dataset with built-in bias to demonstrate fairness issues.\n",
    "\n",
    "**Dataset Characteristics:**\n",
    "- **Binary classification:** Loan approval (0 = denied, 1 = approved)\n",
    "- **Protected attribute:** Group membership (A: 0 = privileged, 1 = unprivileged)\n",
    "- **Features:** Credit score, income, age, employment years, debt ratio\n",
    "- **Built-in bias:** Unprivileged group has lower approval rate for same qualifications\n",
    "\n",
    "**Bias Injection:**\n",
    "- Base approval probability depends on legitimate features (credit score, income)\n",
    "- Add systematic bias: -15% approval probability for unprivileged group\n",
    "- Result: Demographic Parity violation (~20% DPD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefa639",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate Synthetic Biased Lending Dataset\n",
    "==========================================\n",
    "\n",
    "Purpose: Create realistic loan approval data with systematic bias.\n",
    "\n",
    "Features:\n",
    "- credit_score: 300-850 (FICO score)\n",
    "- income: $20K-$150K (annual)\n",
    "- age: 21-70 years\n",
    "- employment_years: 0-40 years\n",
    "- debt_ratio: 0-0.8 (debt/income)\n",
    "\n",
    "Protected Attribute:\n",
    "- group: 0 (privileged), 1 (unprivileged)\n",
    "\n",
    "Target:\n",
    "- approved: 0 (denied), 1 (approved)\n",
    "\"\"\"\n",
    "\n",
    "def generate_biased_lending_data(n_samples=5000, bias_strength=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic lending dataset with built-in bias.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples\n",
    "        bias_strength: Magnitude of bias (-0.15 = 15% lower approval for unprivileged)\n",
    "        random_state: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        df: DataFrame with features, protected attribute, and biased labels\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate protected attribute (50-50 split)\n",
    "    group = np.random.binomial(1, 0.5, n_samples)\n",
    "    \n",
    "    # Generate legitimate features (correlated with approval)\n",
    "    credit_score = np.random.normal(650, 80, n_samples)\n",
    "    credit_score = np.clip(credit_score, 300, 850)\n",
    "    \n",
    "    income = np.random.lognormal(10.8, 0.5, n_samples)  # Mean ~$60K\n",
    "    income = np.clip(income, 20000, 150000)\n",
    "    \n",
    "    age = np.random.normal(40, 12, n_samples)\n",
    "    age = np.clip(age, 21, 70)\n",
    "    \n",
    "    employment_years = np.random.exponential(8, n_samples)\n",
    "    employment_years = np.clip(employment_years, 0, 40)\n",
    "    \n",
    "    debt_ratio = np.random.beta(2, 5, n_samples) * 0.8\n",
    "    \n",
    "    # Compute base approval probability (legitimate factors)\n",
    "    base_prob = (\n",
    "        0.3 * (credit_score - 300) / 550 +  # Credit score contribution\n",
    "        0.25 * (income - 20000) / 130000 +   # Income contribution\n",
    "        0.15 * (employment_years / 40) +     # Employment contribution\n",
    "        0.15 * (1 - debt_ratio) +            # Low debt is good\n",
    "        0.15 * ((age - 21) / 49)             # Age contribution\n",
    "    )\n",
    "    base_prob = np.clip(base_prob, 0, 1)\n",
    "    \n",
    "    # Inject systematic bias: unprivileged group gets lower approval probability\n",
    "    biased_prob = base_prob.copy()\n",
    "    biased_prob[group == 1] -= bias_strength  # Reduce approval for unprivileged\n",
    "    biased_prob = np.clip(biased_prob, 0, 1)\n",
    "    \n",
    "    # Generate biased labels\n",
    "    approved = np.random.binomial(1, biased_prob)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'credit_score': credit_score,\n",
    "        'income': income,\n",
    "        'age': age,\n",
    "        'employment_years': employment_years,\n",
    "        'debt_ratio': debt_ratio,\n",
    "        'group': group,\n",
    "        'approved': approved\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Generate dataset\n",
    "print(\"Generating biased lending dataset...\")\n",
    "df = generate_biased_lending_data(n_samples=5000, bias_strength=0.15)\n",
    "\n",
    "print(f\"\\n\u2705 Dataset created: {df.shape[0]} samples, {df.shape[1]-1} features\")\n",
    "print(f\"\\nProtected attribute distribution:\")\n",
    "print(df['group'].value_counts())\n",
    "print(f\"  Privileged (group=0): {(df['group'] == 0).sum()}\")\n",
    "print(f\"  Unprivileged (group=1): {(df['group'] == 1).sum()}\")\n",
    "\n",
    "print(f\"\\nApproval rates (showing bias):\")\n",
    "approval_by_group = df.groupby('group')['approved'].agg(['mean', 'count'])\n",
    "print(approval_by_group)\n",
    "print(f\"\\n  Privileged approval rate: {approval_by_group.loc[0, 'mean']:.1%}\")\n",
    "print(f\"  Unprivileged approval rate: {approval_by_group.loc[1, 'mean']:.1%}\")\n",
    "print(f\"  Demographic Parity Difference: {approval_by_group.loc[1, 'mean'] - approval_by_group.loc[0, 'mean']:.3f}\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nSample data:\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Visualize bias\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Chart 1: Approval rates by group\n",
    "ax1 = axes[0]\n",
    "approval_rates = df.groupby('group')['approved'].mean()\n",
    "colors = ['#44a47c', '#e74c3c']\n",
    "bars = ax1.bar(['Privileged\\n(Group 0)', 'Unprivileged\\n(Group 1)'], \n",
    "               approval_rates.values, color=colors, edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "ax1.set_ylabel('Approval Rate', fontsize=12, weight='bold')\n",
    "ax1.set_title('Approval Rates by Protected Group\\n(Systematic Bias Present)', fontsize=13, weight='bold', pad=15)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.axhline(0.5, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, rate in zip(bars, approval_rates.values):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "             f'{rate:.1%}', ha='center', va='bottom', fontsize=11, weight='bold')\n",
    "\n",
    "# Chart 2: Credit score distributions\n",
    "ax2 = axes[1]\n",
    "ax2.hist(df[df['group'] == 0]['credit_score'], bins=30, alpha=0.6, label='Privileged', color='#44a47c')\n",
    "ax2.hist(df[df['group'] == 1]['credit_score'], bins=30, alpha=0.6, label='Unprivileged', color='#e74c3c')\n",
    "ax2.set_xlabel('Credit Score', fontsize=12, weight='bold')\n",
    "ax2.set_ylabel('Count', fontsize=12, weight='bold')\n",
    "ax2.set_title('Credit Score Distribution by Group\\n(Similar Qualifications)', fontsize=13, weight='bold', pad=15)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('biased_dataset_visualization.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca Key Observations:\")\n",
    "print(\"  - Unprivileged group has ~15-20% lower approval rate despite similar credit scores\")\n",
    "print(\"  - This demonstrates systematic bias in the decision-making process\")\n",
    "print(\"  - Fairness intervention is needed to achieve equitable outcomes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d83e09",
   "metadata": {},
   "source": [
    "### \ud83d\udcca Fairness Metrics from Scratch\n",
    "\n",
    "**Purpose:** Implement bias detection metrics to quantify unfairness.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Demographic Parity Difference (DPD)**: $P(\\hat{Y}=1|G=1) - P(\\hat{Y}=1|G=0)$  \n",
    "  Measures difference in positive prediction rates between groups  \n",
    "  **Target:** -0.1 to +0.1 (within 10%)\n",
    "\n",
    "- **Equal Opportunity Difference (EOD)**: $TPR_1 - TPR_0$  \n",
    "  Measures difference in true positive rates (among qualified individuals)  \n",
    "  **Target:** -0.1 to +0.1\n",
    "\n",
    "- **Equalized Odds**: Both TPR and FPR should be equal across groups  \n",
    "  **Target:** Max(|TPR_diff|, |FPR_diff|) < 0.1\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "In device binning, ensure that devices from different wafer lots have equal chance of passing premium bins when they meet quality thresholds (prevent systematic bias against specific fabrication batches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a767ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fairness Metrics Implementation\n",
    "================================\n",
    "\n",
    "Compute bias metrics from first principles.\n",
    "\"\"\"\n",
    "\n",
    "def compute_fairness_metrics(y_true, y_pred, protected_attr):\n",
    "    \"\"\"\n",
    "    Compute comprehensive fairness metrics.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels (0/1)\n",
    "        y_pred: Predicted labels (0/1)\n",
    "        protected_attr: Protected group (0=privileged, 1=unprivileged)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Fairness metrics\n",
    "    \"\"\"\n",
    "    # Separate by group\n",
    "    priv_mask = (protected_attr == 0)\n",
    "    unpriv_mask = (protected_attr == 1)\n",
    "    \n",
    "    # Demographic Parity Difference\n",
    "    priv_positive_rate = y_pred[priv_mask].mean()\n",
    "    unpriv_positive_rate = y_pred[unpriv_mask].mean()\n",
    "    dpd = unpriv_positive_rate - priv_positive_rate\n",
    "    \n",
    "    # Equal Opportunity Difference (TPR among qualified)\n",
    "    # TPR = True Positives / (True Positives + False Negatives)\n",
    "    priv_qualified = (y_true[priv_mask] == 1)\n",
    "    unpriv_qualified = (y_true[unpriv_mask] == 1)\n",
    "    \n",
    "    priv_tpr = y_pred[priv_mask][priv_qualified].mean() if priv_qualified.sum() > 0 else 0\n",
    "    unpriv_tpr = y_pred[unpriv_mask][unpriv_qualified].mean() if unpriv_qualified.sum() > 0 else 0\n",
    "    eod = unpriv_tpr - priv_tpr\n",
    "    \n",
    "    # False Positive Rate (FPR among unqualified)\n",
    "    priv_unqualified = (y_true[priv_mask] == 0)\n",
    "    unpriv_unqualified = (y_true[unpriv_mask] == 0)\n",
    "    \n",
    "    priv_fpr = y_pred[priv_mask][priv_unqualified].mean() if priv_unqualified.sum() > 0 else 0\n",
    "    unpriv_fpr = y_pred[unpriv_mask][unpriv_unqualified].mean() if unpriv_unqualified.sum() > 0 else 0\n",
    "    fpr_diff = unpriv_fpr - priv_fpr\n",
    "    \n",
    "    # Equalized Odds (max of TPR and FPR differences)\n",
    "    equalized_odds = max(abs(eod), abs(fpr_diff))\n",
    "    \n",
    "    # Accuracy by group\n",
    "    priv_accuracy = (y_pred[priv_mask] == y_true[priv_mask]).mean()\n",
    "    unpriv_accuracy = (y_pred[unpriv_mask] == y_true[unpriv_mask]).mean()\n",
    "    \n",
    "    return {\n",
    "        'demographic_parity_diff': dpd,\n",
    "        'equal_opportunity_diff': eod,\n",
    "        'equalized_odds': equalized_odds,\n",
    "        'fpr_diff': fpr_diff,\n",
    "        'priv_positive_rate': priv_positive_rate,\n",
    "        'unpriv_positive_rate': unpriv_positive_rate,\n",
    "        'priv_tpr': priv_tpr,\n",
    "        'unpriv_tpr': unpriv_tpr,\n",
    "        'priv_fpr': priv_fpr,\n",
    "        'unpriv_fpr': unpriv_fpr,\n",
    "        'priv_accuracy': priv_accuracy,\n",
    "        'unpriv_accuracy': unpriv_accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "# Train baseline model (biased)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "X = df[['credit_score', 'income', 'age', 'employment_years', 'debt_ratio']].values\n",
    "y = df['approved'].values\n",
    "group = df['group'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    X, y, group, test_size=0.3, random_state=42, stratify=group\n",
    ")\n",
    "\n",
    "# Train biased model\n",
    "print(\"Training baseline model (on biased data)...\")\n",
    "model_biased = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=8)\n",
    "model_biased.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_biased = model_biased.predict(X_test)\n",
    "\n",
    "# Compute fairness metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE MODEL FAIRNESS METRICS (Biased)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metrics_biased = compute_fairness_metrics(y_test, y_pred_biased, group_test)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Demographic Parity:\")\n",
    "print(f\"  Privileged positive rate: {metrics_biased['priv_positive_rate']:.3f}\")\n",
    "print(f\"  Unprivileged positive rate: {metrics_biased['unpriv_positive_rate']:.3f}\")\n",
    "print(f\"  \u27a1\ufe0f  DPD: {metrics_biased['demographic_parity_diff']:.3f} (target: -0.1 to 0.1)\")\n",
    "print(f\"  \u26a0\ufe0f  Status: {'\u2705 FAIR' if abs(metrics_biased['demographic_parity_diff']) < 0.1 else '\u274c BIASED'}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Equal Opportunity:\")\n",
    "print(f\"  Privileged TPR: {metrics_biased['priv_tpr']:.3f}\")\n",
    "print(f\"  Unprivileged TPR: {metrics_biased['unpriv_tpr']:.3f}\")\n",
    "print(f\"  \u27a1\ufe0f  EOD: {metrics_biased['equal_opportunity_diff']:.3f} (target: -0.1 to 0.1)\")\n",
    "print(f\"  \u26a0\ufe0f  Status: {'\u2705 FAIR' if abs(metrics_biased['equal_opportunity_diff']) < 0.1 else '\u274c BIASED'}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Equalized Odds:\")\n",
    "print(f\"  FPR difference: {metrics_biased['fpr_diff']:.3f}\")\n",
    "print(f\"  \u27a1\ufe0f  Max(TPR_diff, FPR_diff): {metrics_biased['equalized_odds']:.3f}\")\n",
    "print(f\"  \u26a0\ufe0f  Status: {'\u2705 FAIR' if metrics_biased['equalized_odds'] < 0.1 else '\u274c BIASED'}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Accuracy by Group:\")\n",
    "print(f\"  Privileged: {metrics_biased['priv_accuracy']:.3f}\")\n",
    "print(f\"  Unprivileged: {metrics_biased['unpriv_accuracy']:.3f}\")\n",
    "print(f\"  Gap: {abs(metrics_biased['priv_accuracy'] - metrics_biased['unpriv_accuracy']):.3f}\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Interpretation:\")\n",
    "print(\"  - Negative DPD = Unprivileged group has lower approval rate\")\n",
    "print(\"  - Model has learned and amplified the training data bias\")\n",
    "print(\"  - Debiasing techniques are required to achieve fairness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f093c0",
   "metadata": {},
   "source": [
    "### \ud83d\udd27 Pre-Processing Debiasing: Reweighing\n",
    "\n",
    "**Purpose:** Assign weights to training samples to balance group representation.\n",
    "\n",
    "**How It Works:**\n",
    "1. Compute expected and observed counts for each (group, label) combination\n",
    "2. Calculate weights: $W_{g,y} = \\frac{P(Y=y) \\cdot P(G=g)}{P(Y=y, G=g)}$\n",
    "3. Train model with weighted samples\n",
    "\n",
    "**Advantages:**\n",
    "- \u2705 Model-agnostic (works with any classifier)\n",
    "- \u2705 Simple to implement\n",
    "- \u2705 Preserves all training data\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Reweight wafer test data to ensure models don't favor specific fabs or process nodes when predicting device yield. Prevents systematic undervaluation of devices from newer process nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512f2fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pre-Processing Debiasing: Reweighing\n",
    "======================================\n",
    "\n",
    "Rebalance training data using sample weights.\n",
    "\"\"\"\n",
    "\n",
    "def compute_reweighing_weights(y, protected_attr):\n",
    "    \"\"\"\n",
    "    Compute reweighing weights for fairness.\n",
    "    \n",
    "    Formula: W(g,y) = [P(Y=y) * P(G=g)] / P(Y=y, G=g)\n",
    "    \n",
    "    Args:\n",
    "        y: Labels (0/1)\n",
    "        protected_attr: Protected group (0/1)\n",
    "    \n",
    "    Returns:\n",
    "        weights: Sample weights array\n",
    "    \"\"\"\n",
    "    weights = np.ones(len(y))\n",
    "    \n",
    "    # Compute probabilities\n",
    "    n = len(y)\n",
    "    \n",
    "    for g in [0, 1]:  # Groups\n",
    "        for label in [0, 1]:  # Labels\n",
    "            # P(Y=y)\n",
    "            p_y = (y == label).mean()\n",
    "            \n",
    "            # P(G=g)\n",
    "            p_g = (protected_attr == g).mean()\n",
    "            \n",
    "            # P(Y=y, G=g)\n",
    "            mask = (y == label) & (protected_attr == g)\n",
    "            p_y_g = mask.mean()\n",
    "            \n",
    "            # Compute weight\n",
    "            if p_y_g > 0:\n",
    "                weight = (p_y * p_g) / p_y_g\n",
    "                weights[mask] = weight\n",
    "    \n",
    "    return weights\n",
    "\n",
    "\n",
    "# Compute reweighing weights\n",
    "print(\"Computing reweighing weights...\")\n",
    "sample_weights = compute_reweighing_weights(y_train, group_train)\n",
    "\n",
    "print(f\"\\nWeight statistics:\")\n",
    "print(f\"  Mean: {sample_weights.mean():.3f}\")\n",
    "print(f\"  Std: {sample_weights.std():.3f}\")\n",
    "print(f\"  Min: {sample_weights.min():.3f}\")\n",
    "print(f\"  Max: {sample_weights.max():.3f}\")\n",
    "\n",
    "# Show weights for each (group, label) combination\n",
    "for g in [0, 1]:\n",
    "    for label in [0, 1]:\n",
    "        mask = (y_train == label) & (group_train == g)\n",
    "        if mask.sum() > 0:\n",
    "            avg_weight = sample_weights[mask].mean()\n",
    "            print(f\"  Group {g}, Label {label}: weight={avg_weight:.3f} (n={mask.sum()})\")\n",
    "\n",
    "# Train debiased model with reweighing\n",
    "print(\"\\nTraining model with reweighing...\")\n",
    "model_reweighed = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=8)\n",
    "model_reweighed.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Predictions\n",
    "y_pred_reweighed = model_reweighed.predict(X_test)\n",
    "\n",
    "# Compute fairness metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REWEIGHED MODEL FAIRNESS METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metrics_reweighed = compute_fairness_metrics(y_test, y_pred_reweighed, group_test)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Demographic Parity:\")\n",
    "print(f\"  Privileged positive rate: {metrics_reweighed['priv_positive_rate']:.3f}\")\n",
    "print(f\"  Unprivileged positive rate: {metrics_reweighed['unpriv_positive_rate']:.3f}\")\n",
    "print(f\"  \u27a1\ufe0f  DPD: {metrics_reweighed['demographic_parity_diff']:.3f} (target: -0.1 to 0.1)\")\n",
    "print(f\"  \u26a0\ufe0f  Status: {'\u2705 FAIR' if abs(metrics_reweighed['demographic_parity_diff']) < 0.1 else '\u274c BIASED'}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Equal Opportunity:\")\n",
    "print(f\"  Privileged TPR: {metrics_reweighed['priv_tpr']:.3f}\")\n",
    "print(f\"  Unprivileged TPR: {metrics_reweighed['unpriv_tpr']:.3f}\")\n",
    "print(f\"  \u27a1\ufe0f  EOD: {metrics_reweighed['equal_opportunity_diff']:.3f} (target: -0.1 to 0.1)\")\n",
    "print(f\"  \u26a0\ufe0f  Status: {'\u2705 FAIR' if abs(metrics_reweighed['equal_opportunity_diff']) < 0.1 else '\u274c BIASED'}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Accuracy by Group:\")\n",
    "print(f\"  Privileged: {metrics_reweighed['priv_accuracy']:.3f}\")\n",
    "print(f\"  Unprivileged: {metrics_reweighed['unpriv_accuracy']:.3f}\")\n",
    "\n",
    "# Compare with baseline\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPROVEMENT FROM REWEIGHING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Demographic Parity Diff: {metrics_biased['demographic_parity_diff']:.3f} \u2192 {metrics_reweighed['demographic_parity_diff']:.3f}\")\n",
    "print(f\"  Reduction: {abs(metrics_biased['demographic_parity_diff']) - abs(metrics_reweighed['demographic_parity_diff']):.3f}\")\n",
    "print(f\"Equal Opportunity Diff: {metrics_biased['equal_opportunity_diff']:.3f} \u2192 {metrics_reweighed['equal_opportunity_diff']:.3f}\")\n",
    "print(f\"  Reduction: {abs(metrics_biased['equal_opportunity_diff']) - abs(metrics_reweighed['equal_opportunity_diff']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b0803a",
   "metadata": {},
   "source": [
    "### \ud83c\udfaf Post-Processing Debiasing: Threshold Optimization\n",
    "\n",
    "**Purpose:** Adjust decision thresholds per group to achieve fairness.\n",
    "\n",
    "**How It Works:**\n",
    "1. Train standard model (may be biased)\n",
    "2. Get probability predictions\n",
    "3. Find optimal thresholds for each group to satisfy fairness constraint\n",
    "4. Apply group-specific thresholds at inference time\n",
    "\n",
    "**Constraint Options:**\n",
    "- **Demographic Parity**: Same positive rate across groups\n",
    "- **Equal Opportunity**: Same TPR across groups\n",
    "- **Equalized Odds**: Same TPR and FPR across groups\n",
    "\n",
    "**Advantages:**\n",
    "- \u2705 Works with pre-trained models (no retraining needed)\n",
    "- \u2705 Flexible fairness constraints\n",
    "- \u2705 Fast to deploy\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Optimize binning thresholds separately for different wafer fabs to ensure equal opportunity for premium bin assignment when devices meet specifications. Prevents revenue loss from overly conservative binning of specific fab outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c4ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Post-Processing Debiasing: Threshold Optimization\n",
    "==================================================\n",
    "\n",
    "Adjust decision thresholds per group to satisfy fairness constraints.\n",
    "\"\"\"\n",
    "\n",
    "def optimize_thresholds_equal_opportunity(y_true, y_prob, protected_attr):\n",
    "    \"\"\"\n",
    "    Find thresholds that equalize True Positive Rates across groups.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_prob: Predicted probabilities\n",
    "        protected_attr: Protected group (0/1)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Optimal thresholds and predictions\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_curve\n",
    "    \n",
    "    # Separate by group\n",
    "    priv_mask = (protected_attr == 0)\n",
    "    unpriv_mask = (protected_attr == 1)\n",
    "    \n",
    "    # Get ROC curves for each group\n",
    "    fpr_priv, tpr_priv, thresh_priv = roc_curve(y_true[priv_mask], y_prob[priv_mask])\n",
    "    fpr_unpriv, tpr_unpriv, thresh_unpriv = roc_curve(y_true[unpriv_mask], y_prob[unpriv_mask])\n",
    "    \n",
    "    # Find target TPR (use average of achievable TPRs)\n",
    "    target_tpr = 0.75  # Aim for 75% TPR\n",
    "    \n",
    "    # Find thresholds closest to target TPR\n",
    "    idx_priv = np.argmin(np.abs(tpr_priv - target_tpr))\n",
    "    idx_unpriv = np.argmin(np.abs(tpr_unpriv - target_tpr))\n",
    "    \n",
    "    threshold_priv = thresh_priv[idx_priv]\n",
    "    threshold_unpriv = thresh_unpriv[idx_unpriv]\n",
    "    \n",
    "    # Apply group-specific thresholds\n",
    "    y_pred_fair = np.zeros(len(y_true), dtype=int)\n",
    "    y_pred_fair[priv_mask] = (y_prob[priv_mask] >= threshold_priv).astype(int)\n",
    "    y_pred_fair[unpriv_mask] = (y_prob[unpriv_mask] >= threshold_unpriv).astype(int)\n",
    "    \n",
    "    return {\n",
    "        'threshold_priv': threshold_priv,\n",
    "        'threshold_unpriv': threshold_unpriv,\n",
    "        'tpr_priv': tpr_priv[idx_priv],\n",
    "        'tpr_unpriv': tpr_unpriv[idx_unpriv],\n",
    "        'y_pred_fair': y_pred_fair\n",
    "    }\n",
    "\n",
    "\n",
    "# Get probability predictions from biased model\n",
    "print(\"Optimizing thresholds for Equal Opportunity...\")\n",
    "y_prob_test = model_biased.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Optimize thresholds\n",
    "result = optimize_thresholds_equal_opportunity(y_test, y_prob_test, group_test)\n",
    "\n",
    "print(f\"\\n\u2705 Optimal Thresholds Found:\")\n",
    "print(f\"  Privileged group: {result['threshold_priv']:.3f}\")\n",
    "print(f\"  Unprivileged group: {result['threshold_unpriv']:.3f}\")\n",
    "print(f\"  Target TPR achieved: ~{result['tpr_priv']:.3f} (both groups)\")\n",
    "\n",
    "# Compute fairness metrics with optimized thresholds\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"THRESHOLD-OPTIMIZED MODEL FAIRNESS METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metrics_threshold = compute_fairness_metrics(y_test, result['y_pred_fair'], group_test)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Demographic Parity:\")\n",
    "print(f\"  DPD: {metrics_threshold['demographic_parity_diff']:.3f}\")\n",
    "print(f\"  Status: {'\u2705 FAIR' if abs(metrics_threshold['demographic_parity_diff']) < 0.1 else '\u26a0\ufe0f PARTIAL'}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Equal Opportunity:\")\n",
    "print(f\"  Privileged TPR: {metrics_threshold['priv_tpr']:.3f}\")\n",
    "print(f\"  Unprivileged TPR: {metrics_threshold['unpriv_tpr']:.3f}\")\n",
    "print(f\"  \u27a1\ufe0f  EOD: {metrics_threshold['equal_opportunity_diff']:.3f}\")\n",
    "print(f\"  \u26a0\ufe0f  Status: {'\u2705 FAIR' if abs(metrics_threshold['equal_opportunity_diff']) < 0.1 else '\u274c BIASED'}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Accuracy by Group:\")\n",
    "print(f\"  Privileged: {metrics_threshold['priv_accuracy']:.3f}\")\n",
    "print(f\"  Unprivileged: {metrics_threshold['unpriv_accuracy']:.3f}\")\n",
    "\n",
    "# Comparison table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: BASELINE vs REWEIGHED vs THRESHOLD-OPTIMIZED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['DPD', 'EOD', 'Equalized Odds', 'Priv Accuracy', 'Unpriv Accuracy'],\n",
    "    'Baseline (Biased)': [\n",
    "        f\"{metrics_biased['demographic_parity_diff']:.3f}\",\n",
    "        f\"{metrics_biased['equal_opportunity_diff']:.3f}\",\n",
    "        f\"{metrics_biased['equalized_odds']:.3f}\",\n",
    "        f\"{metrics_biased['priv_accuracy']:.3f}\",\n",
    "        f\"{metrics_biased['unpriv_accuracy']:.3f}\"\n",
    "    ],\n",
    "    'Reweighed': [\n",
    "        f\"{metrics_reweighed['demographic_parity_diff']:.3f}\",\n",
    "        f\"{metrics_reweighed['equal_opportunity_diff']:.3f}\",\n",
    "        f\"{metrics_reweighed['equalized_odds']:.3f}\",\n",
    "        f\"{metrics_reweighed['priv_accuracy']:.3f}\",\n",
    "        f\"{metrics_reweighed['unpriv_accuracy']:.3f}\"\n",
    "    ],\n",
    "    'Threshold-Optimized': [\n",
    "        f\"{metrics_threshold['demographic_parity_diff']:.3f}\",\n",
    "        f\"{metrics_threshold['equal_opportunity_diff']:.3f}\",\n",
    "        f\"{metrics_threshold['equalized_odds']:.3f}\",\n",
    "        f\"{metrics_threshold['priv_accuracy']:.3f}\",\n",
    "        f\"{metrics_threshold['unpriv_accuracy']:.3f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Key Observations:\")\n",
    "print(\"  - Threshold optimization achieves near-perfect Equal Opportunity\")\n",
    "print(\"  - Reweighing improves Demographic Parity more effectively\")\n",
    "print(\"  - Choice of debiasing method depends on fairness constraint priority\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bb7831",
   "metadata": {},
   "source": [
    "### \ud83d\udcca Fairness Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bfe788",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize Fairness Improvements\n",
    "================================\n",
    "\"\"\"\n",
    "\n",
    "# Create comprehensive comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Chart 1: Demographic Parity Difference\n",
    "ax1 = axes[0, 0]\n",
    "models = ['Baseline\\n(Biased)', 'Reweighed', 'Threshold\\nOptimized']\n",
    "dpd_values = [\n",
    "    metrics_biased['demographic_parity_diff'],\n",
    "    metrics_reweighed['demographic_parity_diff'],\n",
    "    metrics_threshold['demographic_parity_diff']\n",
    "]\n",
    "colors = ['#e74c3c', '#f39c12', '#44a47c']\n",
    "bars = ax1.bar(models, dpd_values, color=colors, edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "ax1.axhline(0, color='black', linewidth=1)\n",
    "ax1.axhline(0.1, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='Fair threshold')\n",
    "ax1.axhline(-0.1, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax1.set_ylabel('Demographic Parity Difference', fontsize=12, weight='bold')\n",
    "ax1.set_title('Demographic Parity Improvement', fontsize=13, weight='bold', pad=15)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for bar, val in zip(bars, dpd_values):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + (0.01 if height > 0 else -0.02),\n",
    "             f'{val:.3f}', ha='center', va='bottom' if height > 0 else 'top', \n",
    "             fontsize=11, weight='bold')\n",
    "\n",
    "# Chart 2: Equal Opportunity Difference\n",
    "ax2 = axes[0, 1]\n",
    "eod_values = [\n",
    "    metrics_biased['equal_opportunity_diff'],\n",
    "    metrics_reweighed['equal_opportunity_diff'],\n",
    "    metrics_threshold['equal_opportunity_diff']\n",
    "]\n",
    "bars = ax2.bar(models, eod_values, color=colors, edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "ax2.axhline(0, color='black', linewidth=1)\n",
    "ax2.axhline(0.1, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='Fair threshold')\n",
    "ax2.axhline(-0.1, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax2.set_ylabel('Equal Opportunity Difference', fontsize=12, weight='bold')\n",
    "ax2.set_title('Equal Opportunity Improvement', fontsize=13, weight='bold', pad=15)\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "for bar, val in zip(bars, eod_values):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + (0.01 if height > 0 else -0.02),\n",
    "             f'{val:.3f}', ha='center', va='bottom' if height > 0 else 'top',\n",
    "             fontsize=11, weight='bold')\n",
    "\n",
    "# Chart 3: Group-wise Accuracy\n",
    "ax3 = axes[1, 0]\n",
    "x_pos = np.arange(len(models))\n",
    "width = 0.35\n",
    "priv_acc = [metrics_biased['priv_accuracy'], metrics_reweighed['priv_accuracy'], metrics_threshold['priv_accuracy']]\n",
    "unpriv_acc = [metrics_biased['unpriv_accuracy'], metrics_reweighed['unpriv_accuracy'], metrics_threshold['unpriv_accuracy']]\n",
    "\n",
    "bars1 = ax3.bar(x_pos - width/2, priv_acc, width, label='Privileged', color='#44a47c', \n",
    "                edgecolor='black', linewidth=1, alpha=0.8)\n",
    "bars2 = ax3.bar(x_pos + width/2, unpriv_acc, width, label='Unprivileged', color='#3498db',\n",
    "                edgecolor='black', linewidth=1, alpha=0.8)\n",
    "\n",
    "ax3.set_ylabel('Accuracy', fontsize=12, weight='bold')\n",
    "ax3.set_title('Accuracy by Protected Group', fontsize=13, weight='bold', pad=15)\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels(models)\n",
    "ax3.legend()\n",
    "ax3.set_ylim(0.5, 1.0)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Chart 4: Fairness-Accuracy Trade-off\n",
    "ax4 = axes[1, 1]\n",
    "avg_acc = [\n",
    "    (metrics_biased['priv_accuracy'] + metrics_biased['unpriv_accuracy']) / 2,\n",
    "    (metrics_reweighed['priv_accuracy'] + metrics_reweighed['unpriv_accuracy']) / 2,\n",
    "    (metrics_threshold['priv_accuracy'] + metrics_threshold['unpriv_accuracy']) / 2\n",
    "]\n",
    "fairness_score = [\n",
    "    1 - abs(metrics_biased['demographic_parity_diff']),\n",
    "    1 - abs(metrics_reweighed['demographic_parity_diff']),\n",
    "    1 - abs(metrics_threshold['demographic_parity_diff'])\n",
    "]\n",
    "\n",
    "ax4.scatter(fairness_score, avg_acc, s=300, c=colors, edgecolor='black', linewidth=2, alpha=0.8, zorder=3)\n",
    "for i, model in enumerate(models):\n",
    "    ax4.annotate(model.replace('\\n', ' '), \n",
    "                 (fairness_score[i], avg_acc[i]),\n",
    "                 xytext=(10, 10), textcoords='offset points',\n",
    "                 fontsize=10, weight='bold',\n",
    "                 bbox=dict(boxstyle='round,pad=0.5', facecolor='white', edgecolor='black', alpha=0.7))\n",
    "\n",
    "ax4.set_xlabel('Fairness Score (1 - |DPD|)', fontsize=12, weight='bold')\n",
    "ax4.set_ylabel('Average Accuracy', fontsize=12, weight='bold')\n",
    "ax4.set_title('Fairness-Accuracy Trade-off', fontsize=13, weight='bold', pad=15)\n",
    "ax4.grid(alpha=0.3)\n",
    "ax4.set_xlim(0.7, 1.05)\n",
    "ax4.set_ylim(0.65, 0.85)\n",
    "\n",
    "plt.suptitle('Comprehensive Fairness Analysis: Debiasing Techniques Comparison', \n",
    "             fontsize=16, weight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('fairness_comparison.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FAIRNESS DEBIASING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n\u2705 BEST FOR DEMOGRAPHIC PARITY: Reweighed\")\n",
    "print(f\"   - DPD reduced from {metrics_biased['demographic_parity_diff']:.3f} to {metrics_reweighed['demographic_parity_diff']:.3f}\")\n",
    "print(f\"   - Improvement: {abs(metrics_biased['demographic_parity_diff']) - abs(metrics_reweighed['demographic_parity_diff']):.3f}\")\n",
    "\n",
    "print(\"\\n\u2705 BEST FOR EQUAL OPPORTUNITY: Threshold-Optimized\")\n",
    "print(f\"   - EOD reduced from {metrics_biased['equal_opportunity_diff']:.3f} to {metrics_threshold['equal_opportunity_diff']:.3f}\")\n",
    "print(f\"   - Improvement: {abs(metrics_biased['equal_opportunity_diff']) - abs(metrics_threshold['equal_opportunity_diff']):.3f}\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Recommendations:\")\n",
    "print(\"  1. Use Reweighing for group-level fairness (hiring, lending)\")\n",
    "print(\"  2. Use Threshold Optimization for outcome fairness (medical, legal)\")\n",
    "print(\"  3. Monitor fairness-accuracy trade-offs continuously\")\n",
    "print(\"  4. Document fairness constraints in model cards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a30c85",
   "metadata": {},
   "source": [
    "## \ud83c\udfed Real-World Project Ideas\n",
    "\n",
    "Build production-ready fairness systems for impactful applications.\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 1: Fair Device Binning System** \u2b50 Post-Silicon\n",
    "\n",
    "**Objective:** Ensure equitable premium bin assignment across different wafer fabrication plants.\n",
    "\n",
    "**Business Value:** $18.2M/year revenue protection by preventing systematic undervaluation of devices from specific fabs.\n",
    "\n",
    "**Dataset:**\n",
    "- Parametric test data (Vdd, Idd, frequency, temperature)\n",
    "- Wafer metadata (fab_id, process_node, lot_id)\n",
    "- Bin assignments (premium, standard, low-power)\n",
    "\n",
    "**Fairness Constraint:** Equal Opportunity - devices meeting premium specs should have equal probability of premium binning regardless of fab origin.\n",
    "\n",
    "**Implementation:**\n",
    "1. Train multi-class binning classifier\n",
    "2. Detect fab-level bias in premium bin assignments\n",
    "3. Apply threshold optimization per fab\n",
    "4. Validate fairness metrics (EOD < 0.1 across all fabs)\n",
    "5. Monitor fairness drift over time\n",
    "\n",
    "**Success Metrics:**\n",
    "- Demographic Parity Diff < 0.08 across fabs\n",
    "- Equal Opportunity Diff < 0.1 for premium bin\n",
    "- Revenue recovery: $18.2M/year from fair binning\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 2: Equitable ATE Resource Allocation** \u2b50 Post-Silicon\n",
    "\n",
    "**Objective:** Fair distribution of premium Automatic Test Equipment (ATE) across product lines.\n",
    "\n",
    "**Business Value:** $24.6M/year efficiency gain by preventing systematic over-allocation to legacy products.\n",
    "\n",
    "**Dataset:**\n",
    "- Product test requirements (test_time, complexity, volume)\n",
    "- ATE capabilities (speed, accuracy, cost_per_hour)\n",
    "- Historical allocation patterns (product_id, ate_id, utilization)\n",
    "\n",
    "**Fairness Constraint:** Calibrated fairness - allocation probability should match actual test needs, not legacy bias.\n",
    "\n",
    "**Implementation:**\n",
    "1. Build ATE allocation recommendation system\n",
    "2. Compute fairness metrics across product lines\n",
    "3. Apply reweighing to balance historical over-representation\n",
    "4. Implement allocation policy with fairness constraints\n",
    "5. Track allocation equity over quarters\n",
    "\n",
    "**Success Metrics:**\n",
    "- Allocation fairness score > 0.92 across products\n",
    "- Utilization improvement: +18% for under-allocated products\n",
    "- Cost savings: $24.6M/year from optimal allocation\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 3: Bias-Free Root Cause Analysis** \u2b50 Post-Silicon\n",
    "\n",
    "**Objective:** Prevent systematic misattribution of test failures to specific process steps.\n",
    "\n",
    "**Business Value:** $12.8M/year cost avoidance by accurate failure diagnosis (prevent unnecessary process changes).\n",
    "\n",
    "**Dataset:**\n",
    "- Failure signatures (test_name, failure_mode, parametric_values)\n",
    "- Process history (etch_time, deposition_thickness, anneal_temp)\n",
    "- Actual root causes (verified through failure analysis)\n",
    "\n",
    "**Fairness Constraint:** Individual fairness - similar failure patterns should receive similar root cause assignments regardless of process step.\n",
    "\n",
    "**Implementation:**\n",
    "1. Train root cause classifier\n",
    "2. Measure attribution bias across process steps\n",
    "3. Apply adversarial debiasing (remove process_step signal)\n",
    "4. Validate with failure analysis ground truth\n",
    "5. Deploy with fairness monitoring dashboard\n",
    "\n",
    "**Success Metrics:**\n",
    "- Attribution accuracy > 87% (up from 72%)\n",
    "- Demographic Parity Diff < 0.12 across process steps\n",
    "- False root cause reduction: 64% \u2192 18%\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 4: Fair Supplier Performance Benchmarking** \u2b50 Post-Silicon\n",
    "\n",
    "**Objective:** Equitable evaluation of component suppliers to prevent bias against new/smaller vendors.\n",
    "\n",
    "**Business Value:** $8.4M/year cost reduction from competitive supplier ecosystem (prevent monopoly pricing).\n",
    "\n",
    "**Dataset:**\n",
    "- Component quality metrics (defect_rate, reliability, test_yield)\n",
    "- Supplier attributes (size, tenure, geography, price)\n",
    "- Historical evaluation scores\n",
    "\n",
    "**Fairness Constraint:** Equalized Odds - suppliers with similar quality should receive similar scores regardless of size/tenure.\n",
    "\n",
    "**Implementation:**\n",
    "1. Build supplier scoring model\n",
    "2. Detect bias against new/small suppliers\n",
    "3. Apply reweighing to balance historical data\n",
    "4. Implement fair scoring policy\n",
    "5. Track supplier diversity and cost competitiveness\n",
    "\n",
    "**Success Metrics:**\n",
    "- Equalized Odds metric < 0.1 (size/tenure groups)\n",
    "- New supplier adoption: +42% qualified vendors\n",
    "- Cost reduction: $8.4M/year from competitive pricing\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 5: Fair Hiring and Promotion System** \ud83c\udf0d General\n",
    "\n",
    "**Objective:** Build bias-free candidate evaluation system for tech recruiting.\n",
    "\n",
    "**Business Value:** $16M/year productivity gain from diverse talent + legal risk reduction.\n",
    "\n",
    "**Dataset:**\n",
    "- Candidate profiles (education, experience, skills, projects)\n",
    "- Interview scores (technical, behavioral, culture_fit)\n",
    "- Historical hiring decisions and performance data\n",
    "\n",
    "**Fairness Constraint:** Demographic Parity + Equal Opportunity across gender/race.\n",
    "\n",
    "**Implementation:**\n",
    "1. Train candidate ranking model\n",
    "2. Audit for demographic bias\n",
    "3. Apply adversarial debiasing + reweighing\n",
    "4. Implement fair interview panel selection\n",
    "5. Monitor hiring funnel fairness metrics\n",
    "\n",
    "**Success Metrics:**\n",
    "- DPD < 0.08 across protected groups\n",
    "- EOD < 0.1 for high-performer identification\n",
    "- Diversity hiring: +35% underrepresented candidates\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 6: Fair Credit Scoring System** \ud83c\udf0d General\n",
    "\n",
    "**Objective:** Equitable loan approval system that eliminates historical lending bias.\n",
    "\n",
    "**Business Value:** $48M/year market expansion from underserved communities + regulatory compliance.\n",
    "\n",
    "**Dataset:**\n",
    "- Credit history (payment_history, credit_utilization, age_of_credit)\n",
    "- Income and employment data\n",
    "- Loan repayment outcomes\n",
    "\n",
    "**Fairness Constraint:** Equal Opportunity - qualified borrowers should have equal approval rates regardless of demographics.\n",
    "\n",
    "**Implementation:**\n",
    "1. Train credit risk model\n",
    "2. Measure bias across demographic groups\n",
    "3. Apply threshold optimization per group\n",
    "4. Validate with regulatory fairness standards (ECOA compliance)\n",
    "5. Deploy with real-time bias monitoring\n",
    "\n",
    "**Success Metrics:**\n",
    "- Equal Opportunity Diff < 0.1\n",
    "- Default rate maintained < 3.2%\n",
    "- Market expansion: +$48M/year revenue from fair lending\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 7: Fair Medical Diagnosis Assistant** \ud83c\udf0d General\n",
    "\n",
    "**Objective:** Ensure equitable diagnostic accuracy across patient demographics.\n",
    "\n",
    "**Business Value:** $120M/year healthcare cost reduction from early diagnosis + malpractice risk mitigation.\n",
    "\n",
    "**Dataset:**\n",
    "- Patient records (symptoms, lab_results, imaging, demographics)\n",
    "- Diagnostic labels (disease, severity)\n",
    "- Treatment outcomes\n",
    "\n",
    "**Fairness Constraint:** Equalized Odds - same sensitivity/specificity across age/gender/race.\n",
    "\n",
    "**Implementation:**\n",
    "1. Train diagnostic classifier (disease detection)\n",
    "2. Audit for demographic disparities in sensitivity\n",
    "3. Apply group-specific threshold optimization\n",
    "4. Validate on diverse test cohorts\n",
    "5. Deploy with clinical fairness dashboard\n",
    "\n",
    "**Success Metrics:**\n",
    "- Equalized Odds < 0.08 across demographics\n",
    "- Sensitivity improvement: +12% for under-diagnosed groups\n",
    "- Mortality reduction: 18% from equitable early diagnosis\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 8: Fair Criminal Risk Assessment** \ud83c\udf0d General\n",
    "\n",
    "**Objective:** Bias-free recidivism prediction for bail/sentencing decisions.\n",
    "\n",
    "**Business Value:** $280M/year social value (reduced wrongful incarceration + recidivism prevention).\n",
    "\n",
    "**Dataset:**\n",
    "- Criminal history (offense_type, severity, frequency)\n",
    "- Demographics (age, race, socioeconomic_status)\n",
    "- Recidivism outcomes (rearrest, conviction)\n",
    "\n",
    "**Fairness Constraint:** Calibration + Equal Opportunity - risk scores should be calibrated and predictive accuracy equal across groups.\n",
    "\n",
    "**Implementation:**\n",
    "1. Train recidivism prediction model\n",
    "2. Measure calibration bias across demographics\n",
    "3. Apply post-processing calibration + threshold optimization\n",
    "4. Validate with external fairness auditors\n",
    "5. Deploy with judicial oversight and transparency\n",
    "\n",
    "**Success Metrics:**\n",
    "- Calibration error < 0.05 across race\n",
    "- Equal Opportunity Diff < 0.1\n",
    "- Wrongful detention reduction: 34%\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83c\udfaf Project Selection Guide\n",
    "\n",
    "| **Constraint Priority** | **Recommended Technique** | **Best Projects** |\n",
    "|------------------------|---------------------------|-------------------|\n",
    "| Demographic Parity | Reweighing | Hiring (P5), Supplier (P4) |\n",
    "| Equal Opportunity | Threshold Optimization | Credit (P6), Medical (P7), Binning (P1) |\n",
    "| Equalized Odds | Adversarial Debiasing | Criminal Justice (P8), Root Cause (P3) |\n",
    "| Calibration | Post-processing | Medical (P7), ATE Allocation (P2) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5bae53",
   "metadata": {},
   "source": [
    "## \ud83c\udf93 Key Takeaways\n",
    "\n",
    "### \u2705 When to Use Fairness & Bias Mitigation\n",
    "\n",
    "**Use fairness-aware ML when:**\n",
    "- Decisions affect protected groups (hiring, lending, healthcare, criminal justice)\n",
    "- Historical data contains systematic bias\n",
    "- Regulatory compliance required (ECOA, GDPR Article 22, NYC AI Law)\n",
    "- Stakeholder trust and social responsibility are priorities\n",
    "- Post-silicon: Preventing systematic bias in device evaluation across fabs/batches\n",
    "\n",
    "**Skip when:**\n",
    "- No protected attributes involved (weather forecasting, game AI)\n",
    "- Individual predictions don't affect people's opportunities\n",
    "- Exploratory analysis without deployment\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83c\udfaf Choosing the Right Fairness Metric\n",
    "\n",
    "| **Metric** | **Use Case** | **Post-Silicon Example** |\n",
    "|-----------|-------------|--------------------------|\n",
    "| **Demographic Parity** | Group representation | Ensure wafer lots get equal test coverage |\n",
    "| **Equal Opportunity** | Qualified outcome equity | Devices meeting specs get premium bins equally |\n",
    "| **Equalized Odds** | Full predictive parity | Root cause attribution accurate across process steps |\n",
    "| **Calibration** | Risk score reliability | Reliability predictions calibrated across product lines |\n",
    "| **Individual Fairness** | Similar treatment | Similar devices receive similar binning regardless of fab |\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udd27 Debiasing Technique Selection\n",
    "\n",
    "**Pre-Processing (Reweighing, Resampling):**\n",
    "- \u2705 Model-agnostic (works with any classifier)\n",
    "- \u2705 Easy to implement and explain\n",
    "- \u274c May discard valuable data (resampling)\n",
    "- **Best for:** Demographic Parity, when fairness is top priority\n",
    "\n",
    "**In-Processing (Adversarial Debiasing, Fairness Regularization):**\n",
    "- \u2705 Integrated into training (single model)\n",
    "- \u2705 Can optimize fairness-accuracy trade-off\n",
    "- \u274c Requires custom training loops\n",
    "- **Best for:** Complex fairness constraints, neural networks\n",
    "\n",
    "**Post-Processing (Threshold Optimization, Calibration):**\n",
    "- \u2705 Works with pre-trained models (no retraining)\n",
    "- \u2705 Fast to deploy and tune\n",
    "- \u274c May sacrifice overall accuracy\n",
    "- **Best for:** Equal Opportunity, Equalized Odds, when model is frozen\n",
    "\n",
    "---\n",
    "\n",
    "### \u26a0\ufe0f Limitations & Challenges\n",
    "\n",
    "1. **Fairness-Accuracy Trade-off:**  \n",
    "   Stricter fairness constraints often reduce overall accuracy. Document and justify trade-offs.\n",
    "\n",
    "2. **Impossibility Theorem:**  \n",
    "   Cannot simultaneously satisfy Demographic Parity, Equal Opportunity, and Calibration (except in trivial cases). Choose priority metric based on application.\n",
    "\n",
    "3. **Protected Attribute Unavailability:**  \n",
    "   If protected attributes aren't collected (privacy/legal reasons), use proxy detection or fairness-through-unawareness (risky).\n",
    "\n",
    "4. **Fairness Shift Over Time:**  \n",
    "   Data distribution changes can reintroduce bias. Continuous monitoring essential.\n",
    "\n",
    "5. **Definition Disagreement:**  \n",
    "   Stakeholders may disagree on fairness definition. Transparent communication required.\n",
    "\n",
    "6. **Post-Silicon Challenge:**  \n",
    "   Fab-specific process variations may legitimately correlate with yield. Distinguish bias from causal differences.\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\ude80 Best Practices\n",
    "\n",
    "**1. Fairness Auditing Pipeline:**\n",
    "```python\n",
    "# Standard fairness audit workflow\n",
    "1. Identify protected attributes (gender, race, fab_id, etc.)\n",
    "2. Compute baseline bias metrics (DPD, EOD, calibration)\n",
    "3. Apply debiasing technique(s)\n",
    "4. Validate fairness improvement on holdout set\n",
    "5. Document fairness-accuracy trade-offs\n",
    "6. Deploy with continuous monitoring\n",
    "```\n",
    "\n",
    "**2. Model Cards for Fairness:**\n",
    "- Document fairness metrics in model cards\n",
    "- Report performance disaggregated by protected groups\n",
    "- Explain debiasing techniques applied\n",
    "- State limitations and known biases\n",
    "\n",
    "**3. Human-in-the-Loop:**\n",
    "- Fairness-aware ML supports decisions, doesn't replace human judgment\n",
    "- High-stakes applications (bail, medical) require human oversight\n",
    "- Provide explanations alongside fairness-adjusted predictions\n",
    "\n",
    "**4. Continuous Monitoring:**\n",
    "- Track fairness metrics in production (weekly/monthly)\n",
    "- Set alerts for fairness degradation (DPD > 0.15, EOD > 0.12)\n",
    "- Retrain and re-debias when drift detected\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udcca Post-Silicon Specific Insights\n",
    "\n",
    "**1. Fab-Level Fairness:**\n",
    "- Treat fabrication plants as protected groups\n",
    "- Ensure devices from different fabs with similar parametrics receive equal treatment\n",
    "- Revenue impact: $18M/year from fair binning\n",
    "\n",
    "**2. Process Node Equity:**\n",
    "- Newer process nodes may have different yield characteristics (legitimate)\n",
    "- Distinguish between legitimate causal differences and historical bias\n",
    "- Apply fairness constraints only to comparable quality levels\n",
    "\n",
    "**3. Supplier Fairness:**\n",
    "- Prevent systematic bias against smaller/newer component suppliers\n",
    "- Cost savings: $8.4M/year from competitive supplier ecosystem\n",
    "\n",
    "**4. Test Coverage Fairness:**\n",
    "- Ensure premium ATE resources allocated fairly across products\n",
    "- Efficiency gain: $24.6M/year from equitable resource allocation\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udd17 Next Steps in MLOps\n",
    "\n",
    "After mastering fairness & bias mitigation:\n",
    "\n",
    "1. **Model Governance (177):** Implement policies and audit trails for fair ML systems\n",
    "2. **Privacy-Preserving ML (178):** Combine fairness with differential privacy\n",
    "3. **Ethical AI Frameworks (179):** Broader ethical considerations beyond fairness\n",
    "4. **Regulatory Compliance (180):** EU AI Act, NYC AI Law, ECOA compliance\n",
    "\n",
    "**Recommended Reading:**\n",
    "- \"Fairness and Machine Learning\" by Barocas, Hardt, Narayanan\n",
    "- Google's \"ML Fairness Guidelines\"\n",
    "- Microsoft's \"Responsible AI Standard\"\n",
    "- IEEE 7000-2021 (Systems Design for Ethical Concerns)\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udca1 Final Thought\n",
    "\n",
    "**Fairness is not optional** - it's a fundamental requirement for trustworthy AI systems. Whether preventing discriminatory lending, ensuring equitable healthcare, or achieving fair device binning in semiconductors, fairness-aware ML protects both people and business value.\n",
    "\n",
    "**The cost of unfairness:**\n",
    "- Legal liability (class-action lawsuits, regulatory fines)\n",
    "- Reputational damage (PR crises, lost customers)\n",
    "- Social harm (perpetuating inequality)\n",
    "- Lost revenue (market exclusion, monopolistic supplier pricing)\n",
    "\n",
    "**The value of fairness:**\n",
    "- Market expansion ($48M/year from fair credit)\n",
    "- Talent diversity ($16M/year productivity gain)\n",
    "- Regulatory compliance (avoid fines, maintain licensing)\n",
    "- Social responsibility (measurable positive impact)\n",
    "\n",
    "---\n",
    "\n",
    "Let's build fair AI systems that work for everyone! \ud83c\udf0d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315c1b70",
   "metadata": {},
   "source": [
    "## \ud83d\udd11 Key Takeaways\n",
    "\n",
    "**When to Use Fairness/Bias Analysis:**\n",
    "- High-stakes decisions affecting people (hiring, lending, healthcare, criminal justice)\n",
    "- Regulatory requirements (GDPR, ECOA, Fair Housing Act)\n",
    "- Protected attributes in data (race, gender, age, disability)\n",
    "- Public-facing AI systems (brand reputation risk from unfair outcomes)\n",
    "\n",
    "**Limitations:**\n",
    "- Fairness metrics can conflict (demographic parity vs equalized odds - can't optimize both)\n",
    "- Debiasing may reduce overall accuracy (fairness-accuracy trade-off)\n",
    "- Requires domain expertise (what constitutes \"fair\" varies by context)\n",
    "- Can't fix biased data collection (garbage in, garbage out)\n",
    "- Post-hoc fixes don't address root causes (need fair data collection processes)\n",
    "\n",
    "**Alternatives:**\n",
    "- **Fair data collection** (balanced sampling, auditing data sources)\n",
    "- **Causal modeling** (understand causal relationships, not just correlations)\n",
    "- **Human-in-the-loop** (manual review for edge cases)\n",
    "- **Reject option classification** (defer ambiguous cases to humans)\n",
    "\n",
    "**Best Practices:**\n",
    "- Define fairness metric early (align with stakeholders before building)\n",
    "- Measure multiple fairness metrics (demographic parity, equalized odds, calibration)\n",
    "- Use adversarial debiasing or reweighting (not just post-processing)\n",
    "- Monitor fairness in production (drift can reintroduce bias over time)\n",
    "- Document bias mitigation efforts (model cards, fairness reports)\n",
    "- Involve diverse stakeholders (ethicists, domain experts, affected communities)\n",
    "\n",
    "**Next Steps:**\n",
    "- 177: Privacy-Preserving ML (privacy and fairness often aligned)\n",
    "- 178: AI Safety & Alignment (value alignment, robustness to adversaries)\n",
    "- 127: Model Governance (audit trails for fairness compliance)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}