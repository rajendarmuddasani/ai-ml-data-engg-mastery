{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283600a6",
   "metadata": {},
   "source": [
    "# 167: Hierarchical Time Series Forecasting\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** hierarchical time series structures and aggregation constraints\n",
    "- **Implement** bottom-up, top-down, and optimal reconciliation methods\n",
    "- **Build** forecast systems that maintain hierarchical consistency\n",
    "- **Apply** hierarchical forecasting to post-silicon validation (wafer ‚Üí lot ‚Üí fab yield)\n",
    "- **Evaluate** reconciliation quality and forecast coherence metrics\n",
    "\n",
    "## üìö What is Hierarchical Time Series Forecasting?\n",
    "\n",
    "**Hierarchical time series** have natural nested structures where forecasts at different levels must be consistent. For example, total revenue = sum of regional revenues = sum of store revenues.\n",
    "\n",
    "**Key Challenge:** Independent forecasts at each level often violate aggregation constraints (e.g., forecasted total ‚â† sum of forecasted parts).\n",
    "\n",
    "**Reconciliation Methods:**\n",
    "- **Bottom-up:** Forecast lowest level ‚Üí Aggregate upward (always coherent)\n",
    "- **Top-down:** Forecast total ‚Üí Disaggregate downward (proportional allocation)\n",
    "- **Middle-out:** Forecast middle level ‚Üí Aggregate up and disaggregate down\n",
    "- **Optimal reconciliation:** MinTrace/OLS - minimize variance while maintaining coherence\n",
    "\n",
    "**Why Hierarchical Forecasting?**\n",
    "- ‚úÖ **Coherent forecasts:** Aggregations respect hierarchy (total = sum of parts)\n",
    "- ‚úÖ **Better accuracy:** Cross-level information sharing improves forecasts\n",
    "- ‚úÖ **Business alignment:** Forecasts match organizational structure (product/region/store)\n",
    "- ‚úÖ **Reconciliation flexibility:** Choose method based on level reliability\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**1. Wafer ‚Üí Lot ‚Üí Fab Yield Forecasting**\n",
    "- Hierarchy: Fab total yield = Œ£ Lot yields = Œ£ Wafer yields\n",
    "- Input: Historical yield data at wafer/lot/fab levels\n",
    "- Output: Coherent forecasts (fab-level forecast = sum of lot forecasts)\n",
    "- Value: Accurate capacity planning = **$15M-$40M/year**\n",
    "\n",
    "**2. Product Line ‚Üí Device ‚Üí SKU Test Time**\n",
    "- Hierarchy: Product line test time = Œ£ Device test times = Œ£ SKU test times\n",
    "- Input: Test duration history per SKU/device/product line\n",
    "- Output: Reconciled forecasts for capacity allocation\n",
    "- Value: Optimized tester utilization = **$8M-$18M/year**\n",
    "\n",
    "**3. Multi-Site ‚Üí Fab ‚Üí Equipment Downtime**\n",
    "- Hierarchy: Multi-site downtime = Œ£ Fab downtime = Œ£ Equipment downtime\n",
    "- Input: Maintenance logs across equipment/fab/multi-site\n",
    "- Output: Coherent forecasts for spare parts inventory\n",
    "- Value: Reduced emergency procurement = **$5M-$12M/year**\n",
    "\n",
    "**4. Geography ‚Üí Customer ‚Üí Part Demand**\n",
    "- Hierarchy: Global demand = Œ£ Regional demand = Œ£ Customer demand\n",
    "- Input: Order history at customer/region/global levels\n",
    "- Output: Reconciled demand forecasts\n",
    "- Value: Optimized inventory = **$10M-$25M/year**\n",
    "\n",
    "## üîÑ Hierarchical Forecasting Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Hierarchical Data] --> B[Base Forecasts<br/>All Levels]\n",
    "    B --> C{Reconciliation<br/>Method?}\n",
    "    C -->|Bottom-Up| D[Aggregate from<br/>Lowest Level]\n",
    "    C -->|Top-Down| E[Disaggregate from<br/>Top Level]\n",
    "    C -->|Optimal| F[MinTrace/OLS<br/>Reconciliation]\n",
    "    \n",
    "    D --> G[Coherent Forecasts]\n",
    "    E --> G\n",
    "    F --> G\n",
    "    \n",
    "    G --> H[Validate Coherence]\n",
    "    H --> I[Evaluate Accuracy]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style G fill:#e1ffe1\n",
    "    style F fill:#fff4e1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- 114: Time Series Forecasting (ARIMA, seasonal decomposition)\n",
    "- 165: Advanced Time Series (LSTM, Transformers for forecasting)\n",
    "- 166: Probabilistic Time Series (uncertainty quantification)\n",
    "\n",
    "**Next Steps:**\n",
    "- 168: Causal Inference Time Series (causal relationships in hierarchies)\n",
    "- 169: Real-Time Streaming Forecasting (online reconciliation)\n",
    "- 156: ML Pipeline Orchestration (automated hierarchical forecasting workflows)\n",
    "\n",
    "---\n",
    "\n",
    "Let's build hierarchical forecasting systems! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c11980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hierarchical Time Series Forecasting - Production Setup\n",
    "\n",
    "This notebook uses production-grade libraries for hierarchical forecasting:\n",
    "1. Forecast reconciliation: scikit-hts, hierarchicalforecast\n",
    "2. Time series modeling: statsmodels, pmdarima, Prophet\n",
    "3. Optimal reconciliation: Custom implementation (MinTrace, OLS)\n",
    "4. Visualization: matplotlib, seaborn, plotly\n",
    "\n",
    "Install required packages:\n",
    "    pip install scikit-hts hierarchicalforecast statsmodels pmdarima prophet\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import linalg\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Hierarchical forecasting\n",
    "try:\n",
    "    from hts import HTSRegressor\n",
    "    from hts.hierarchy import HierarchyTree\n",
    "    HTS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    HTS_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è scikit-hts not available. Using manual implementation.\")\n",
    "\n",
    "# Time series forecasting\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "try:\n",
    "    from pmdarima import auto_arima\n",
    "    PMDARIMA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PMDARIMA_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è pmdarima not available. Using manual ARIMA.\")\n",
    "\n",
    "# Standard ML utilities\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(47)\n",
    "\n",
    "print(\"‚úÖ Hierarchical forecasting environment ready!\")\n",
    "print(f\"   scikit-hts available: {HTS_AVAILABLE}\")\n",
    "print(f\"   pmdarima available: {PMDARIMA_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca4e92f",
   "metadata": {},
   "source": [
    "### üìù What is Bottom-Up Forecasting?\n",
    "\n",
    "**Bottom-up forecasting** is the simplest hierarchical approach:\n",
    "1. Forecast all **bottom-level (leaf) series** independently\n",
    "2. **Aggregate upward** by summing to get higher-level forecasts\n",
    "3. **Coherence guaranteed** by construction (sums are exact)\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "For a 2-level hierarchy (Total ‚Üí Regions):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Total}_t &= \\text{Region}_A + \\text{Region}_B + \\text{Region}_C \\\\\n",
    "\\hat{y}_{\\text{Total}, t} &= \\hat{y}_{A,t} + \\hat{y}_{B,t} + \\hat{y}_{C,t}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Summing Matrix $S$:**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y_{\\text{Total}} \\\\\n",
    "y_A \\\\\n",
    "y_B \\\\\n",
    "y_C\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "y_A \\\\\n",
    "y_B \\\\\n",
    "y_C\n",
    "\\end{bmatrix}\n",
    "= S \\cdot y_{\\text{bottom}}\n",
    "$$\n",
    "\n",
    "**Bottom-up forecasts:** $\\tilde{y} = S \\cdot \\hat{y}_{\\text{bottom}}$ (coherent by construction)\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ **Simple:** Forecast bottom series, aggregate (no complex math)\n",
    "- ‚úÖ **Coherent:** Automatically satisfies summing constraints\n",
    "- ‚úÖ **Captures detail:** Leverages bottom-level patterns (local trends, seasonality)\n",
    "- ‚úÖ **Interpretable:** Transparent (no black-box reconciliation)\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ùå **Noisy bottom series:** Aggregation propagates errors upward\n",
    "- ‚ùå **Sparse data:** Bottom series may have few observations (high variance)\n",
    "- ‚ùå **Ignores top-level signal:** Doesn't leverage aggregate patterns (e.g., national trends)\n",
    "- ‚ùå **Computational cost:** Forecast N bottom series (large N for deep hierarchies)\n",
    "\n",
    "**When to Use:**\n",
    "- ‚úÖ Bottom series have sufficient data (>100 observations)\n",
    "- ‚úÖ Bottom patterns are informative (local effects dominate)\n",
    "- ‚úÖ Simplicity preferred (no complex reconciliation)\n",
    "\n",
    "**Post-Silicon Application: Multi-Fab Wafer Production**\n",
    "\n",
    "**Scenario:** Forecast daily wafer starts for 5 fabs, aggregate to global capacity planning.\n",
    "\n",
    "**Hierarchy:**\n",
    "```\n",
    "Global Wafer Starts\n",
    "‚îú‚îÄ‚îÄ Fab A (US)\n",
    "‚îú‚îÄ‚îÄ Fab B (Taiwan)\n",
    "‚îú‚îÄ‚îÄ Fab C (Korea)\n",
    "‚îú‚îÄ‚îÄ Fab D (China)\n",
    "‚îî‚îÄ‚îÄ Fab E (Germany)\n",
    "```\n",
    "\n",
    "**Data:** 2 years daily wafer starts (730 days per fab).\n",
    "\n",
    "**Method:**\n",
    "- Forecast each fab using **ETS (Exponential Smoothing)** - captures trend + seasonality\n",
    "- Aggregate: Global = sum of 5 fab forecasts\n",
    "\n",
    "**Business Value:**\n",
    "- **Coherent capacity planning:** Global forecast = sum of fab plans (no impossible allocations)\n",
    "- **Fab-specific patterns:** Taiwan has Lunar New Year shutdown, US has Thanksgiving dips\n",
    "- **Expected MAPE:** 6.8% at fab level, 4.2% at global level (aggregation reduces variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ca910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic multi-fab wafer starts data\n",
    "def generate_hierarchical_wafer_data(n_days=730, n_fabs=5, seed=47):\n",
    "    \"\"\"\n",
    "    Simulate daily wafer starts for 5 global fabs with hierarchy.\n",
    "    Each fab has different capacity, seasonality, and trends.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    days = np.arange(n_days)\n",
    "    fab_names = ['Fab_A_US', 'Fab_B_Taiwan', 'Fab_C_Korea', 'Fab_D_China', 'Fab_E_Germany']\n",
    "    \n",
    "    # Fab-specific parameters\n",
    "    base_capacity = [5000, 8000, 6500, 7000, 4500]  # wafers/day\n",
    "    growth_rates = [0.0005, 0.0008, 0.0006, 0.001, 0.0004]  # daily growth\n",
    "    \n",
    "    fab_data = {}\n",
    "    \n",
    "    for i, fab in enumerate(fab_names):\n",
    "        # Base trend\n",
    "        trend = base_capacity[i] * (1 + growth_rates[i] * days)\n",
    "        \n",
    "        # Weekly seasonality (Mon-Fri high, Sat-Sun low)\n",
    "        weekly = 200 * np.sin(2 * np.pi * days / 7 - np.pi/2)\n",
    "        \n",
    "        # Annual seasonality (Q4 peak for most fabs)\n",
    "        annual = 300 * np.sin(2 * np.pi * days / 365 - np.pi)\n",
    "        \n",
    "        # Fab-specific patterns\n",
    "        if 'Taiwan' in fab:\n",
    "            # Lunar New Year shutdown (around day 45, 410)\n",
    "            lunar_ny_1 = -2000 * np.exp(-((days - 45)**2) / 100)\n",
    "            lunar_ny_2 = -2000 * np.exp(-((days - 410)**2) / 100)\n",
    "            special = lunar_ny_1 + lunar_ny_2\n",
    "        elif 'US' in fab:\n",
    "            # Thanksgiving & Christmas shutdowns\n",
    "            thanksgiving = -1000 * np.exp(-((days - 320)**2) / 50)\n",
    "            christmas = -1500 * np.exp(-((days - 355)**2) / 50)\n",
    "            special = thanksgiving + christmas\n",
    "        elif 'Germany' in fab:\n",
    "            # Summer vacation (days 180-210)\n",
    "            summer_mask = (days >= 180) & (days <= 210)\n",
    "            special = -800 * summer_mask\n",
    "        else:\n",
    "            special = np.zeros(n_days)\n",
    "        \n",
    "        # Noise\n",
    "        noise = np.random.normal(0, 150, n_days)\n",
    "        \n",
    "        # Combine\n",
    "        wafer_starts = trend + weekly + annual + special + noise\n",
    "        wafer_starts = np.clip(wafer_starts, base_capacity[i] * 0.5, base_capacity[i] * 1.5)\n",
    "        \n",
    "        fab_data[fab] = wafer_starts\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(fab_data)\n",
    "    df['day'] = days\n",
    "    df['Global'] = df[fab_names].sum(axis=1)\n",
    "    \n",
    "    return df, fab_names\n",
    "\n",
    "# Generate data\n",
    "df_wafer, fab_names = generate_hierarchical_wafer_data(n_days=730, n_fabs=5)\n",
    "print(f\"üìä Dataset: {len(df_wafer)} days, {len(fab_names)} fabs\")\n",
    "print(f\"üåç Global wafer starts: {df_wafer['Global'].mean():.0f} wafers/day (std: {df_wafer['Global'].std():.0f})\")\n",
    "print(f\"üè≠ Fab breakdown:\")\n",
    "for fab in fab_names:\n",
    "    print(f\"   {fab}: {df_wafer[fab].mean():.0f} wafers/day\")\n",
    "\n",
    "# Train-test split (80-20, time-aware)\n",
    "train_size = int(0.8 * len(df_wafer))\n",
    "train_df = df_wafer.iloc[:train_size].copy()\n",
    "test_df = df_wafer.iloc[train_size:].copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Split: Train={len(train_df)} days, Test={len(test_df)} days\")\n",
    "\n",
    "# Bottom-Up Forecasting: Forecast each fab independently\n",
    "print(\"\\nüîß Bottom-Up Forecasting: Training ETS models per fab...\")\n",
    "\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "fab_forecasts = {}\n",
    "fab_models = {}\n",
    "\n",
    "for fab in fab_names:\n",
    "    # Exponential Smoothing with trend and seasonality\n",
    "    model = ExponentialSmoothing(\n",
    "        train_df[fab],\n",
    "        trend='add',\n",
    "        seasonal='add',\n",
    "        seasonal_periods=7  # Weekly seasonality\n",
    "    )\n",
    "    fitted = model.fit()\n",
    "    fab_models[fab] = fitted\n",
    "    \n",
    "    # Forecast test period\n",
    "    forecast = fitted.forecast(steps=len(test_df))\n",
    "    fab_forecasts[fab] = forecast.values\n",
    "    \n",
    "    print(f\"  ‚úÖ {fab} forecasted\")\n",
    "\n",
    "# Aggregate to Global (Bottom-Up)\n",
    "global_forecast_bu = sum(fab_forecasts[fab] for fab in fab_names)\n",
    "\n",
    "# Evaluate fab-level accuracy\n",
    "print(\"\\nüìä Bottom-Up Forecast Accuracy (Fab Level):\")\n",
    "fab_mapes = {}\n",
    "for fab in fab_names:\n",
    "    mae = mean_absolute_error(test_df[fab], fab_forecasts[fab])\n",
    "    mape = np.mean(np.abs((test_df[fab] - fab_forecasts[fab]) / test_df[fab])) * 100\n",
    "    fab_mapes[fab] = mape\n",
    "    print(f\"   {fab}: MAE={mae:.0f}, MAPE={mape:.2f}%\")\n",
    "\n",
    "# Evaluate global-level accuracy\n",
    "global_mae = mean_absolute_error(test_df['Global'], global_forecast_bu)\n",
    "global_mape = np.mean(np.abs((test_df['Global'] - global_forecast_bu) / test_df['Global'])) * 100\n",
    "print(f\"\\nüìà Bottom-Up Global Forecast: MAE={global_mae:.0f}, MAPE={global_mape:.2f}%\")\n",
    "\n",
    "# Check coherence (should be exact for bottom-up)\n",
    "coherence_error = abs(global_forecast_bu - sum(fab_forecasts[fab] for fab in fab_names)).max()\n",
    "print(f\"‚úÖ Coherence Check: Max error = {coherence_error:.6f} (should be ~0 for bottom-up)\")\n",
    "\n",
    "# Business value calculation\n",
    "# Improved capacity planning from coherent forecasts\n",
    "baseline_waste = 0.12  # 12% capacity waste from incoherent forecasts\n",
    "improved_waste = 0.06  # 6% waste with bottom-up coherence\n",
    "global_capacity_value_per_day = df_wafer['Global'].mean() * 500  # $500 per wafer\n",
    "annual_value = (baseline_waste - improved_waste) * global_capacity_value_per_day * 365\n",
    "print(f\"\\nüí∞ Business Value (Bottom-Up Coherence):\")\n",
    "print(f\"   Capacity waste reduction: {baseline_waste*100:.0f}% ‚Üí {improved_waste*100:.0f}%\")\n",
    "print(f\"   Annual value: ${annual_value/1e6:.1f}M/year\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Hierarchy visualization\n",
    "ax1 = axes[0, 0]\n",
    "test_days = np.arange(len(test_df))\n",
    "ax1.plot(test_days, test_df['Global'], 'o-', color='black', label='Actual Global', markersize=3, linewidth=2, alpha=0.7)\n",
    "ax1.plot(test_days, global_forecast_bu, '--', color='blue', label='Bottom-Up Forecast', linewidth=2)\n",
    "ax1.fill_between(test_days, test_df['Global'], global_forecast_bu, alpha=0.2, color='lightblue')\n",
    "ax1.set_xlabel('Test Day', fontsize=12)\n",
    "ax1.set_ylabel('Global Wafer Starts', fontsize=12)\n",
    "ax1.set_title('Bottom-Up: Global Forecast (Aggregated from Fabs)', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 2. Individual fab forecasts\n",
    "ax2 = axes[0, 1]\n",
    "for i, fab in enumerate(fab_names[:3]):  # Show 3 fabs for clarity\n",
    "    ax2.plot(test_days, test_df[fab], label=f'{fab} Actual', alpha=0.6, linewidth=1.5)\n",
    "    ax2.plot(test_days, fab_forecasts[fab], '--', label=f'{fab} Forecast', alpha=0.8, linewidth=1.5)\n",
    "ax2.set_xlabel('Test Day', fontsize=12)\n",
    "ax2.set_ylabel('Wafer Starts', fontsize=12)\n",
    "ax2.set_title('Bottom-Up: Individual Fab Forecasts', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='upper right', fontsize=8)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# 3. MAPE comparison across fabs\n",
    "ax3 = axes[1, 0]\n",
    "mape_values = [fab_mapes[fab] for fab in fab_names] + [global_mape]\n",
    "labels = fab_names + ['Global']\n",
    "colors = ['coral']*len(fab_names) + ['green']\n",
    "ax3.bar(range(len(labels)), mape_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax3.set_xticks(range(len(labels)))\n",
    "ax3.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax3.set_ylabel('MAPE (%)', fontsize=12)\n",
    "ax3.set_title('Bottom-Up: Forecast Accuracy (MAPE)', fontsize=14, fontweight='bold')\n",
    "ax3.axhline(global_mape, color='green', linestyle='--', linewidth=2, label=f'Global MAPE: {global_mape:.2f}%')\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Aggregation benefit (variance reduction)\n",
    "ax4 = axes[1, 1]\n",
    "fab_errors = np.array([test_df[fab] - fab_forecasts[fab] for fab in fab_names])\n",
    "fab_variance = fab_errors.var(axis=1)\n",
    "global_error = test_df['Global'] - global_forecast_bu\n",
    "global_variance = global_error.var()\n",
    "\n",
    "ax4.bar(fab_names, fab_variance, color='coral', alpha=0.7, label='Fab-level Variance')\n",
    "ax4.axhline(global_variance, color='green', linestyle='--', linewidth=2, label=f'Global Variance: {global_variance:.0f}')\n",
    "ax4.set_ylabel('Forecast Error Variance', fontsize=12)\n",
    "ax4.set_xlabel('Fab', fontsize=12)\n",
    "ax4.set_title('Aggregation Benefit: Variance Reduction', fontsize=14, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(alpha=0.3, axis='y')\n",
    "plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Bottom-Up Forecasting: Multi-fab wafer production complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2afcc3",
   "metadata": {},
   "source": [
    "### üìù What is Top-Down & Optimal Reconciliation?\n",
    "\n",
    "**Top-Down Forecasting:**\n",
    "1. Forecast **top-level aggregate** only\n",
    "2. **Disaggregate downward** using historical proportions or percentages\n",
    "3. Coherence guaranteed (bottom series sum to top by construction)\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "$$\n",
    "\\hat{y}_{\\text{bottom}, t} = P \\cdot \\hat{y}_{\\text{top}, t}\n",
    "$$\n",
    "\n",
    "Where $P$ is a **proportion matrix** (e.g., historical average proportions).\n",
    "\n",
    "**Example:** If Fab A historically represents 18% of global production:\n",
    "$$\n",
    "\\hat{y}_{A,t} = 0.18 \\times \\hat{y}_{\\text{Global}, t}\n",
    "$$\n",
    "\n",
    "**Top-Down Advantages:**\n",
    "- ‚úÖ **Smooth forecasts:** Aggregates have lower variance (less noise)\n",
    "- ‚úÖ **Leverages macro trends:** Captures global patterns\n",
    "- ‚úÖ **Efficient:** Forecast 1 series, disaggregate (fast for large hierarchies)\n",
    "\n",
    "**Top-Down Disadvantages:**\n",
    "- ‚ùå **Loses bottom-level signal:** Ignores fab-specific patterns\n",
    "- ‚ùå **Static proportions:** Assumes proportions don't change over time\n",
    "- ‚ùå **Poor for emerging products:** New products have no historical proportions\n",
    "\n",
    "---\n",
    "\n",
    "**Optimal Reconciliation (MinTrace):**\n",
    "\n",
    "Instead of choosing bottom-up or top-down, **forecast all levels independently**, then **reconcile** to minimize forecast error variance.\n",
    "\n",
    "**Framework:**\n",
    "\n",
    "1. **Base forecasts:** $\\hat{y}$ (all levels, possibly incoherent)\n",
    "2. **Summing matrix:** $S$ (defines aggregation constraints)\n",
    "3. **Reconciled forecasts:** $\\tilde{y} = S \\cdot G \\cdot \\hat{y}$ where $G$ is chosen to minimize trace of error covariance\n",
    "\n",
    "**MinTrace Reconciliation:**\n",
    "\n",
    "$$\n",
    "G = (S^T W_h^{-1} S)^{-1} S^T W_h^{-1}\n",
    "$$\n",
    "\n",
    "Where $W_h$ is the error covariance matrix (estimated from in-sample residuals).\n",
    "\n",
    "**Special Cases:**\n",
    "- **OLS (Ordinary Least Squares):** $W_h = I$ (identity, all errors equally weighted)\n",
    "- **WLS (Weighted Least Squares):** $W_h = \\text{diag}(w_1, ..., w_n)$ (weight by variance)\n",
    "- **MinTrace (Generalized LS):** $W_h = \\hat{\\Sigma}$ (full error covariance)\n",
    "\n",
    "**Why Optimal Reconciliation?**\n",
    "- ‚úÖ **Best of both worlds:** Uses information from all levels\n",
    "- ‚úÖ **Provably optimal:** Minimizes forecast variance under coherence constraints\n",
    "- ‚úÖ **Flexible:** Works with any base forecasting method (ARIMA, LSTM, etc.)\n",
    "- ‚úÖ **Empirically superior:** Typically 10-30% error reduction vs bottom-up/top-down\n",
    "\n",
    "**When to Use:**\n",
    "- ‚úÖ Have forecasts at multiple levels (not just bottom)\n",
    "- ‚úÖ Willing to invest in reconciliation computation (matrix inversion)\n",
    "- ‚úÖ Need maximum accuracy (high-stakes decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db246923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-Down Forecasting\n",
    "print(\"üîß Top-Down Forecasting: Using historical proportions...\")\n",
    "\n",
    "# Calculate historical proportions (from training data)\n",
    "proportions = {}\n",
    "for fab in fab_names:\n",
    "    proportions[fab] = (train_df[fab] / train_df['Global']).mean()\n",
    "\n",
    "print(f\"üìä Historical Proportions:\")\n",
    "for fab in fab_names:\n",
    "    print(f\"   {fab}: {proportions[fab]*100:.1f}%\")\n",
    "\n",
    "# Forecast top level (Global) only\n",
    "global_model_td = ExponentialSmoothing(\n",
    "    train_df['Global'],\n",
    "    trend='add',\n",
    "    seasonal='add',\n",
    "    seasonal_periods=7\n",
    ")\n",
    "global_fitted_td = global_model_td.fit()\n",
    "global_forecast_td = global_fitted_td.forecast(steps=len(test_df)).values\n",
    "\n",
    "# Disaggregate using proportions\n",
    "fab_forecasts_td = {}\n",
    "for fab in fab_names:\n",
    "    fab_forecasts_td[fab] = proportions[fab] * global_forecast_td\n",
    "\n",
    "# Evaluate top-down\n",
    "print(\"\\nüìä Top-Down Forecast Accuracy (Fab Level):\")\n",
    "fab_mapes_td = {}\n",
    "for fab in fab_names:\n",
    "    mae_td = mean_absolute_error(test_df[fab], fab_forecasts_td[fab])\n",
    "    mape_td = np.mean(np.abs((test_df[fab] - fab_forecasts_td[fab]) / test_df[fab])) * 100\n",
    "    fab_mapes_td[fab] = mape_td\n",
    "    print(f\"   {fab}: MAE={mae_td:.0f}, MAPE={mape_td:.2f}%\")\n",
    "\n",
    "global_mae_td = mean_absolute_error(test_df['Global'], global_forecast_td)\n",
    "global_mape_td = np.mean(np.abs((test_df['Global'] - global_forecast_td) / test_df['Global'])) * 100\n",
    "print(f\"\\nüìà Top-Down Global Forecast: MAE={global_mae_td:.0f}, MAPE={global_mape_td:.2f}%\")\n",
    "\n",
    "# Optimal Reconciliation (MinTrace)\n",
    "print(\"\\n\\nüîß Optimal Reconciliation (MinTrace): Combining all forecasts...\")\n",
    "\n",
    "# Step 1: Collect base forecasts (already have bottom-up and top-down)\n",
    "# Use bottom-up fab forecasts + top-level direct forecast\n",
    "\n",
    "# Step 2: Build summing matrix S (maps bottom to all levels)\n",
    "# For this simple 2-level hierarchy: [Global, Fab_A, Fab_B, Fab_C, Fab_D, Fab_E]\n",
    "n_bottom = len(fab_names)\n",
    "n_total = n_bottom + 1  # bottom + top level\n",
    "\n",
    "S = np.vstack([\n",
    "    np.ones(n_bottom),  # Global = sum of all fabs\n",
    "    np.eye(n_bottom)  # Each fab = itself\n",
    "])\n",
    "\n",
    "print(f\"üìê Summing Matrix S: shape {S.shape}\")\n",
    "\n",
    "# Step 3: Estimate error covariance W_h from in-sample residuals\n",
    "# Fit models on training set and get residuals\n",
    "residuals = np.zeros((n_total, len(train_df)))\n",
    "\n",
    "# Global residuals\n",
    "global_train_fitted = global_fitted_td.fittedvalues\n",
    "residuals[0, :] = train_df['Global'] - global_train_fitted\n",
    "\n",
    "# Fab residuals\n",
    "for i, fab in enumerate(fab_names):\n",
    "    fab_train_fitted = fab_models[fab].fittedvalues\n",
    "    residuals[i+1, :] = train_df[fab] - fab_train_fitted\n",
    "\n",
    "# Estimate covariance (use subset to avoid singularity)\n",
    "W_h = np.cov(residuals[:, -200:])  # Use last 200 days\n",
    "W_h += np.eye(n_total) * 1e-6  # Regularization for numerical stability\n",
    "\n",
    "# Step 4: Compute reconciliation matrix G (MinTrace)\n",
    "try:\n",
    "    W_h_inv = np.linalg.inv(W_h)\n",
    "    G = np.linalg.inv(S.T @ W_h_inv @ S) @ S.T @ W_h_inv\n",
    "    print(f\"‚úÖ Reconciliation matrix G computed: shape {G.shape}\")\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"‚ö†Ô∏è Singular matrix, using OLS reconciliation (W_h = I)\")\n",
    "    G = np.linalg.inv(S.T @ S) @ S.T\n",
    "\n",
    "# Step 5: Reconcile forecasts\n",
    "# Base forecasts: [global_forecast_td, fab_forecasts from bottom-up]\n",
    "base_forecasts = np.vstack([\n",
    "    global_forecast_td.reshape(1, -1),\n",
    "    np.array([fab_forecasts[fab] for fab in fab_names])\n",
    "])\n",
    "\n",
    "# Reconciled forecasts\n",
    "reconciled = S @ (G @ base_forecasts)\n",
    "\n",
    "global_forecast_recon = reconciled[0, :]\n",
    "fab_forecasts_recon = {fab: reconciled[i+1, :] for i, fab in enumerate(fab_names)}\n",
    "\n",
    "# Evaluate reconciled forecasts\n",
    "print(\"\\nüìä Optimal Reconciliation Accuracy (Fab Level):\")\n",
    "fab_mapes_recon = {}\n",
    "for fab in fab_names:\n",
    "    mae_recon = mean_absolute_error(test_df[fab], fab_forecasts_recon[fab])\n",
    "    mape_recon = np.mean(np.abs((test_df[fab] - fab_forecasts_recon[fab]) / test_df[fab])) * 100\n",
    "    fab_mapes_recon[fab] = mape_recon\n",
    "    print(f\"   {fab}: MAE={mae_recon:.0f}, MAPE={mape_recon:.2f}%\")\n",
    "\n",
    "global_mae_recon = mean_absolute_error(test_df['Global'], global_forecast_recon)\n",
    "global_mape_recon = np.mean(np.abs((test_df['Global'] - global_forecast_recon) / test_df['Global'])) * 100\n",
    "print(f\"\\nüìà Reconciled Global Forecast: MAE={global_mae_recon:.0f}, MAPE={global_mape_recon:.2f}%\")\n",
    "\n",
    "# Check coherence\n",
    "coherence_error_recon = abs(global_forecast_recon - sum(fab_forecasts_recon[fab] for fab in fab_names)).max()\n",
    "print(f\"‚úÖ Coherence Check: Max error = {coherence_error_recon:.6f}\")\n",
    "\n",
    "# Compare methods\n",
    "print(\"\\n\\nüìä Method Comparison:\")\n",
    "print(f\"{'Method':<25} {'Global MAPE':<15} {'Avg Fab MAPE':<15} {'Coherence Error'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "avg_fab_mape_bu = np.mean(list(fab_mapes.values()))\n",
    "avg_fab_mape_td = np.mean(list(fab_mapes_td.values()))\n",
    "avg_fab_mape_recon = np.mean(list(fab_mapes_recon.values()))\n",
    "\n",
    "print(f\"{'Bottom-Up':<25} {global_mape:<15.2f} {avg_fab_mape_bu:<15.2f} {coherence_error:.6f}\")\n",
    "print(f\"{'Top-Down':<25} {global_mape_td:<15.2f} {avg_fab_mape_td:<15.2f} ~0\")\n",
    "print(f\"{'Optimal (MinTrace)':<25} {global_mape_recon:<15.2f} {avg_fab_mape_recon:<15.2f} {coherence_error_recon:.6f}\")\n",
    "\n",
    "# Business value from optimal reconciliation\n",
    "baseline_mape = avg_fab_mape_bu\n",
    "optimized_mape = avg_fab_mape_recon\n",
    "improvement = (baseline_mape - optimized_mape) / baseline_mape\n",
    "annual_value_improvement = improvement * 342.8e6  # From use case ($342.8M baseline)\n",
    "\n",
    "print(f\"\\nüí∞ Business Value (Optimal Reconciliation):\")\n",
    "print(f\"   MAPE improvement: {baseline_mape:.2f}% ‚Üí {optimized_mape:.2f}% ({improvement*100:.1f}% reduction)\")\n",
    "print(f\"   Annual value gain: ${annual_value_improvement/1e6:.1f}M/year\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Method comparison for one fab\n",
    "ax1 = axes[0, 0]\n",
    "sample_fab = fab_names[1]  # Taiwan fab\n",
    "test_days = np.arange(len(test_df))[:60]  # Show first 60 days\n",
    "ax1.plot(test_days, test_df[sample_fab].values[:60], 'o-', color='black', label='Actual', markersize=4, linewidth=2, alpha=0.7)\n",
    "ax1.plot(test_days, fab_forecasts[sample_fab][:60], '--', color='blue', label='Bottom-Up', linewidth=2)\n",
    "ax1.plot(test_days, fab_forecasts_td[sample_fab][:60], '--', color='orange', label='Top-Down', linewidth=2)\n",
    "ax1.plot(test_days, fab_forecasts_recon[sample_fab][:60], '--', color='green', label='Reconciled', linewidth=2)\n",
    "ax1.set_xlabel('Test Day', fontsize=12)\n",
    "ax1.set_ylabel(f'{sample_fab} Wafer Starts', fontsize=12)\n",
    "ax1.set_title(f'Method Comparison: {sample_fab}', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 2. MAPE comparison across methods\n",
    "ax2 = axes[0, 1]\n",
    "x = np.arange(len(fab_names))\n",
    "width = 0.25\n",
    "ax2.bar(x - width, [fab_mapes[fab] for fab in fab_names], width, label='Bottom-Up', color='blue', alpha=0.7)\n",
    "ax2.bar(x, [fab_mapes_td[fab] for fab in fab_names], width, label='Top-Down', color='orange', alpha=0.7)\n",
    "ax2.bar(x + width, [fab_mapes_recon[fab] for fab in fab_names], width, label='Reconciled', color='green', alpha=0.7)\n",
    "ax2.set_ylabel('MAPE (%)', fontsize=12)\n",
    "ax2.set_xlabel('Fab', fontsize=12)\n",
    "ax2.set_title('Forecast Accuracy by Method & Fab', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([f.split('_')[1] for f in fab_names], rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Global forecast comparison\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(test_days[:60], test_df['Global'].values[:60], 'o-', color='black', label='Actual', markersize=4, linewidth=2, alpha=0.7)\n",
    "ax3.plot(test_days[:60], global_forecast_bu[:60], '--', color='blue', label='Bottom-Up', linewidth=2)\n",
    "ax3.plot(test_days[:60], global_forecast_td[:60], '--', color='orange', label='Top-Down', linewidth=2)\n",
    "ax3.plot(test_days[:60], global_forecast_recon[:60], '--', color='green', label='Reconciled', linewidth=2)\n",
    "ax3.set_xlabel('Test Day', fontsize=12)\n",
    "ax3.set_ylabel('Global Wafer Starts', fontsize=12)\n",
    "ax3.set_title('Global Forecast Comparison', fontsize=14, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. Error variance reduction\n",
    "ax4 = axes[1, 1]\n",
    "methods = ['Bottom-Up', 'Top-Down', 'Reconciled']\n",
    "global_errors = [\n",
    "    test_df['Global'] - global_forecast_bu,\n",
    "    test_df['Global'] - global_forecast_td,\n",
    "    test_df['Global'] - global_forecast_recon\n",
    "]\n",
    "variances = [err.var() for err in global_errors]\n",
    "colors = ['blue', 'orange', 'green']\n",
    "ax4.bar(methods, variances, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax4.set_ylabel('Forecast Error Variance', fontsize=12)\n",
    "ax4.set_title('Variance Reduction from Reconciliation', fontsize=14, fontweight='bold')\n",
    "ax4.grid(alpha=0.3, axis='y')\n",
    "for i, (method, var) in enumerate(zip(methods, variances)):\n",
    "    ax4.text(i, var, f'{var:.0f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Top-Down & Optimal Reconciliation: Multi-fab forecasting complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de03bb53",
   "metadata": {},
   "source": [
    "## üéØ Real-World Hierarchical Forecasting Projects\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "#### **1. Global Semiconductor Supply Chain Hierarchy** ($487.3M/year)\n",
    "\n",
    "**Objective:** Forecast demand across complex product/geography/channel hierarchy with coherence guarantees.\n",
    "\n",
    "**Hierarchy Structure (5 levels):**\n",
    "```\n",
    "Total Demand\n",
    "‚îú‚îÄ‚îÄ Geography (4): North America, Europe, Asia, RoW\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Country (15): US, Canada, Mexico, Germany, UK, China, Japan, Korea, Taiwan, India, ...\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Customer Segment (3): Enterprise, SMB, Consumer\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Product Family (8): DDR4, DDR5, LPDDR4, LPDDR5, GDDR6, HBM2, HBM3, Specialty\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SKU (120): Capacity/Speed variants\n",
    "```\n",
    "\n",
    "**Total Series:** 5,000+ time series (120 leaf SKUs √ó geography √ó segments)\n",
    "\n",
    "**Data:**\n",
    "- 5 years monthly demand (60 months)\n",
    "- Exogenous: GDP growth, semiconductor index, new product launches\n",
    "- Promotional calendar, contract commitments\n",
    "\n",
    "**Method: Hierarchical Reconciliation with Temporal Aggregation**\n",
    "- Base forecasts: Prophet (seasonal decomposition) at all levels\n",
    "- Cross-sectional reconciliation: MinTrace (geography + product hierarchies)\n",
    "- Temporal reconciliation: Monthly ‚Üí Quarterly ‚Üí Yearly coherence\n",
    "- Grouped hierarchy: Both product AND geography groupings (graph structure)\n",
    "\n",
    "**Challenges:**\n",
    "- **Sparsity:** Many SKU/country combinations have zero sales (cold start)\n",
    "- **New products:** DDR5/HBM3 lack historical data ‚Üí use product lifecycle curves\n",
    "- **Promotional effects:** Black Friday, Lunar New Year cause regime shifts\n",
    "- **Currency fluctuations:** Geography forecasts in local currency, reconcile to USD\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "from hierarchicalforecast import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import MinTrace\n",
    "\n",
    "# Define hierarchy (aggregation matrix)\n",
    "S = build_aggregation_matrix(hierarchy_spec)\n",
    "\n",
    "# Base forecasts (all levels)\n",
    "base_forecasts = {}\n",
    "for level in hierarchy_levels:\n",
    "    base_forecasts[level] = prophet_forecast(data[level])\n",
    "\n",
    "# Cross-sectional reconciliation\n",
    "reconciler = MinTrace(method='mint_shrink')  # Shrinkage for sparse data\n",
    "reconciled_forecasts = reconciler.reconcile(S, base_forecasts)\n",
    "\n",
    "# Temporal reconciliation (ensure monthly sum to quarterly)\n",
    "final_forecasts = temporal_reconciliation(reconciled_forecasts, freq=['M', 'Q', 'Y'])\n",
    "```\n",
    "\n",
    "**Business Value:**\n",
    "- Inventory optimization: $284M/year (coherent SKU-level forecasts)\n",
    "- Customer commitment accuracy: $127M/year (99.2% SLA vs 94% baseline)\n",
    "- Production planning: $76M/year (global capacity aligned with regional forecasts)\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Wafer Fab Equipment Utilization Hierarchy** ($298.4M/year)\n",
    "\n",
    "**Objective:** Forecast ATE/photolithography/etch tool hours across fab/product/shift hierarchy.\n",
    "\n",
    "**Hierarchy:**\n",
    "```\n",
    "Total Tool Hours\n",
    "‚îú‚îÄ‚îÄ Fab (5)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Equipment Type (6): ATE, Photo, Etch, Implant, CMP, Metrology\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Tool Generation (3): Legacy, Current, Advanced\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Product Line (8)\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Shift (3): Day, Swing, Night\n",
    "```\n",
    "\n",
    "**Data:**\n",
    "- 2 years hourly utilization (17,520 hours)\n",
    "- Downtime events, PM schedules, yield excursions\n",
    "\n",
    "**Method: Constrained Hierarchical Forecasting**\n",
    "- Hard constraints: Total capacity per fab (e.g., 50K hours/week max)\n",
    "- Bottom-up base: LSTM per shift/product (captures hourly patterns)\n",
    "- Top-down capacity: Exponential smoothing at fab level\n",
    "- Reconciliation: MinTrace with inequality constraints (capacity limits)\n",
    "\n",
    "**Constrained Reconciliation:**\n",
    "```python\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def constrained_reconcile(base_forecasts, S, capacity_limits):\n",
    "    \"\"\"\n",
    "    Minimize: ||reconciled - base||^2\n",
    "    Subject to: S @ reconciled = coherent\n",
    "                total_fab_hours <= capacity_limits\n",
    "    \"\"\"\n",
    "    def objective(x):\n",
    "        return np.sum((x - base_forecasts)**2)\n",
    "    \n",
    "    # Coherence constraints: Ax = b\n",
    "    A_eq = build_coherence_matrix(S)\n",
    "    b_eq = compute_coherence_rhs(S, base_forecasts)\n",
    "    \n",
    "    # Capacity constraints: Cx <= d\n",
    "    A_ub = build_capacity_matrix(hierarchy)\n",
    "    b_ub = capacity_limits\n",
    "    \n",
    "    result = minimize(objective, base_forecasts, \n",
    "                      constraints=[{'type': 'eq', 'A': A_eq, 'b': b_eq},\n",
    "                                   {'type': 'ineq', 'A': -A_ub, 'b': b_ub}])\n",
    "    return result.x\n",
    "```\n",
    "\n",
    "**Value:**\n",
    "- Utilization improvement: 72% ‚Üí 89% ($186M/year from higher throughput)\n",
    "- Overtime reduction: $64M/year (accurate shift-level forecasts)\n",
    "- Capital planning: $48M/year (equipment purchase decisions based on constrained forecasts)\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Bin Distribution Temporal Hierarchy** ($184.7M/year)\n",
    "\n",
    "**Objective:** Forecast device bin percentages (speed grades) across daily/weekly/monthly horizons with compositional coherence.\n",
    "\n",
    "**Compositional Hierarchy:**\n",
    "- Bins must sum to 100% at all time aggregations\n",
    "- Constraint: $\\sum_{i=1}^4 p_i = 1$ where $p_i \\in [0, 1]$\n",
    "\n",
    "**Data:**\n",
    "- 18 months daily bin results (540 days, 2M devices)\n",
    "- 4 bins: Premium (3.5GHz+), Standard (3.0-3.5GHz), Value (2.5-3.0GHz), Scrap (<2.5GHz)\n",
    "\n",
    "**Method: Compositional Temporal Reconciliation**\n",
    "- Base forecasts: Dirichlet regression (compositional data)\n",
    "- Temporal aggregation: Daily ‚Üí Weekly ‚Üí Monthly\n",
    "- Reconciliation: Log-ratio transformation + MinTrace + inverse transform\n",
    "\n",
    "**Compositional Reconciliation:**\n",
    "```python\n",
    "def compositional_reconcile(daily_forecasts, weekly_forecasts, monthly_forecasts):\n",
    "    \"\"\"\n",
    "    Reconcile bin percentages across temporal hierarchy.\n",
    "    Use additive log-ratio (ALR) transformation for unconstrained space.\n",
    "    \"\"\"\n",
    "    # Transform to unconstrained space\n",
    "    daily_alr = alr_transform(daily_forecasts)  # log(p_i / p_4)\n",
    "    weekly_alr = alr_transform(weekly_forecasts)\n",
    "    monthly_alr = alr_transform(monthly_forecasts)\n",
    "    \n",
    "    # Build temporal summing matrix\n",
    "    S_temporal = build_temporal_S(freq=['D', 'W', 'M'])\n",
    "    \n",
    "    # Reconcile in ALR space\n",
    "    reconciled_alr = mintrace_reconcile(S_temporal, [daily_alr, weekly_alr, monthly_alr])\n",
    "    \n",
    "    # Transform back to simplex (ensure sum to 1)\n",
    "    reconciled_probs = inverse_alr_transform(reconciled_alr)\n",
    "    \n",
    "    return reconciled_probs\n",
    "```\n",
    "\n",
    "**Value:**\n",
    "- Pricing optimization: $118M/year (coherent bin forecasts across horizons)\n",
    "- Contract fulfillment: $42M/year (monthly commitments align with daily operations)\n",
    "- Scrap reduction: $25M/year (early bin 4 warnings from daily forecasts)\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Multi-Site Test Floor Capacity Hierarchy** ($236.8M/year)\n",
    "\n",
    "**Objective:** Allocate test capacity across sites/products/programs with cross-site dependencies.\n",
    "\n",
    "**Hierarchy (Grouped):**\n",
    "```\n",
    "Total Test Hours\n",
    "‚îú‚îÄ‚îÄ Site (3): US, Asia-Pacific, Europe\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Product (12)\n",
    "‚îî‚îÄ‚îÄ Test Type (4): Wafer Probe, Final Test, Reliability, Characterization\n",
    "    ‚îî‚îÄ‚îÄ Product (12)\n",
    "```\n",
    "\n",
    "**Note:** This is a **grouped hierarchy** (not strictly nested) - products cross-classified by site AND test type.\n",
    "\n",
    "**Data:**\n",
    "- 3 years weekly test hours (156 weeks)\n",
    "- Cross-site transfers (products move between sites)\n",
    "- Test program versions, handler configurations\n",
    "\n",
    "**Method: Grouped Hierarchy Reconciliation**\n",
    "- Graph structure: Nodes = all series, edges = aggregation relationships\n",
    "- MinTrace on graph: Generalized summing matrix handles cross-classification\n",
    "\n",
    "**Grouped Hierarchy Matrix:**\n",
    "```python\n",
    "def build_grouped_S(hierarchy_graph):\n",
    "    \"\"\"\n",
    "    Build summing matrix for grouped (non-nested) hierarchy.\n",
    "    Uses graph representation of aggregation constraints.\n",
    "    \"\"\"\n",
    "    n_bottom = len(leaf_nodes)\n",
    "    n_total = len(all_nodes)\n",
    "    \n",
    "    S = np.zeros((n_total, n_bottom))\n",
    "    \n",
    "    for i, node in enumerate(all_nodes):\n",
    "        if node in leaf_nodes:\n",
    "            j = leaf_nodes.index(node)\n",
    "            S[i, j] = 1\n",
    "        else:\n",
    "            # Aggregate from descendants\n",
    "            descendants = get_descendants(hierarchy_graph, node)\n",
    "            for desc in descendants:\n",
    "                if desc in leaf_nodes:\n",
    "                    j = leaf_nodes.index(desc)\n",
    "                    S[i, j] = 1\n",
    "    \n",
    "    return S\n",
    "```\n",
    "\n",
    "**Value:**\n",
    "- Cross-site optimization: $142M/year (leverage global capacity)\n",
    "- Load balancing: $58M/year (shift tests to lower-cost sites)\n",
    "- Transfer efficiency: $37M/year (coherent planning reduces shipping delays)\n",
    "\n",
    "---\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "#### **5. Retail Sales Hierarchy (Store/Product/Channel)** ($394.6M/year)\n",
    "\n",
    "**Objective:** Forecast 10,000 SKUs √ó 500 stores √ó 3 channels (online/in-store/wholesale).\n",
    "\n",
    "**Hierarchy:**\n",
    "- 5 levels, 15M leaf series\n",
    "- Temporal: Daily ‚Üí Weekly ‚Üí Monthly ‚Üí Quarterly\n",
    "\n",
    "**Method: Sparse Hierarchical Reconciliation**\n",
    "- Many series are zero (long-tail products)\n",
    "- Use LASSO-regularized MinTrace (sparse covariance estimation)\n",
    "- Incremental reconciliation (only update changed series)\n",
    "\n",
    "**Value:**\n",
    "- Inventory: $218M/year\n",
    "- Markdowns: $124M/year (coherent pricing across channels)\n",
    "- Fulfillment: $53M/year (optimize ship-from-store vs warehouse)\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Energy Load Forecasting (Grid Hierarchy)** ($286.4M/year)\n",
    "\n",
    "**Objective:** Forecast electricity demand across transmission/distribution/substations.\n",
    "\n",
    "**Hierarchy:**\n",
    "- National grid ‚Üí Regional ‚Üí Substation (5,000 nodes)\n",
    "- Renewable integration (solar/wind forecasts feed into hierarchy)\n",
    "\n",
    "**Method: Probabilistic Hierarchical Reconciliation**\n",
    "- Base: Probabilistic forecasts (quantile regression at all levels)\n",
    "- Reconcile quantiles separately (P10, P50, P90 each coherent)\n",
    "\n",
    "**Value:**\n",
    "- Reserve optimization: $168M/year\n",
    "- Renewable curtailment: $84M/year (better integration)\n",
    "- Congestion management: $34M/year\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Hospital Resource Allocation (Bed/Staff/Supplies)** ($198.2M/year)\n",
    "\n",
    "**Objective:** Forecast ICU/general bed demand across hospital network with cross-hospital transfers.\n",
    "\n",
    "**Hierarchy:**\n",
    "- Hospital network (12 hospitals) ‚Üí Department ‚Üí Bed type\n",
    "- Staffing hierarchy: Nurses/Doctors by shift/specialty\n",
    "\n",
    "**Method: Constrained Reconciliation with Transfers**\n",
    "- Capacity constraints (max beds per hospital)\n",
    "- Transfer costs (prefer local admissions, transfer when capacity-constrained)\n",
    "\n",
    "**Value:**\n",
    "- Capacity utilization: $124M/year (89% vs 76%)\n",
    "- Transfer optimization: $48M/year\n",
    "- Staff scheduling: $26M/year\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. Tourism Demand Hierarchy (Country/Region/Attraction)** ($142.8M/year)\n",
    "\n",
    "**Objective:** Forecast visitor arrivals across countries/cities/attractions for hospitality planning.\n",
    "\n",
    "**Hierarchy:**\n",
    "- Global tourism ‚Üí Country (30) ‚Üí City (200) ‚Üí Attraction type (hotel/museum/restaurant)\n",
    "- Temporal: Daily ‚Üí Monthly ‚Üí Yearly\n",
    "\n",
    "**Method: Hierarchical with Events**\n",
    "- Special events (Olympics, conferences) as exogenous variables\n",
    "- Cross-country dependencies (multi-country tours)\n",
    "\n",
    "**Value:**\n",
    "- Hotel pricing: $84M/year (revenue management)\n",
    "- Staffing: $38M/year\n",
    "- Inventory (food/supplies): $21M/year\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Implementation Tips\n",
    "\n",
    "**1. Choose Appropriate Base Forecasts:**\n",
    "- Match complexity to data availability (simple for sparse series)\n",
    "- Use different models at different levels (ARIMA for aggregates, ML for bottom)\n",
    "\n",
    "**2. Reconciliation Scaling:**\n",
    "- Large hierarchies (>10K series): Use sparse methods, approximate covariance\n",
    "- Real-time: Precompute reconciliation matrix G, fast matrix-vector multiply at inference\n",
    "\n",
    "**3. Validation:**\n",
    "- Cross-validation at all levels (not just bottom or top)\n",
    "- Check coherence constraints numerically (floating point errors)\n",
    "- Monitor reconciliation benefit over time (may degrade with distribution shift)\n",
    "\n",
    "**4. Handling New Series:**\n",
    "- Cold start: Use top-down initially (leverage aggregate signal)\n",
    "- Warm start: Switch to bottom-up after collecting sufficient data\n",
    "\n",
    "**5. Probabilistic Extension:**\n",
    "- Reconcile quantiles separately (each quantile forecast must be coherent)\n",
    "- Or reconcile sample paths (for full distribution)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "- ‚ùå **Assuming bottom-up is always best:** Top-down often wins for sparse/noisy data\n",
    "- ‚ùå **Ignoring computational cost:** MinTrace requires matrix inversion (O(n¬≥))\n",
    "- ‚ùå **Static proportions in top-down:** Proportions change over time (use recent averages)\n",
    "- ‚ùå **Forgetting temporal coherence:** Monthly forecasts should sum to quarterly\n",
    "- ‚ùå **Overcomplicated hierarchies:** Keep structure interpretable for stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215fcf5",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways: Hierarchical Time Series Forecasting\n",
    "\n",
    "### **Method Comparison Matrix**\n",
    "\n",
    "| **Method** | **Coherence** | **Accuracy** | **Complexity** | **Computational Cost** | **Best For** |\n",
    "|------------|---------------|--------------|----------------|------------------------|--------------|\n",
    "| **Bottom-Up** | ‚úÖ Guaranteed | Medium-High | Low | O(n) - n bottom forecasts | Detailed data, local patterns |\n",
    "| **Top-Down** | ‚úÖ Guaranteed | Low-Medium | Low | O(1) - 1 top forecast | Sparse bottom, smooth aggregates |\n",
    "| **Middle-Out** | ‚úÖ Guaranteed | Medium | Low | O(m) - m middle forecasts | Balanced data availability |\n",
    "| **Optimal (MinTrace)** | ‚úÖ Guaranteed | Highest | High | O(n¬≥) - matrix inversion | Large scale, need max accuracy |\n",
    "| **OLS Reconciliation** | ‚úÖ Guaranteed | High | Medium | O(n¬≤) - simplified MinTrace | Good compromise |\n",
    "| **Grouped Hierarchy** | ‚úÖ Guaranteed | High | Very High | O(n¬≥) - graph reconciliation | Cross-classifications |\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Which Method?**\n",
    "\n",
    "**Decision Framework:**\n",
    "\n",
    "```\n",
    "1. What's your hierarchy structure?\n",
    "   ‚Üí Strictly nested (tree): Bottom-Up, Top-Down, MinTrace\n",
    "   ‚Üí Cross-classified (graph): Grouped hierarchy reconciliation\n",
    "   ‚Üí Temporal only: Temporal reconciliation\n",
    "\n",
    "2. What's your data availability at bottom level?\n",
    "   ‚Üí Sufficient (>100 obs): Bottom-Up\n",
    "   ‚Üí Sparse (<50 obs): Top-Down\n",
    "   ‚Üí Mixed: MinTrace or Middle-Out\n",
    "\n",
    "3. What's your computational budget?\n",
    "   ‚Üí Limited (real-time): Bottom-Up or Top-Down (O(n) or O(1))\n",
    "   ‚Üí Moderate (batch): OLS reconciliation (O(n¬≤))\n",
    "   ‚Üí High (offline): MinTrace (O(n¬≥))\n",
    "\n",
    "4. What's your accuracy requirement?\n",
    "   ‚Üí Standard: Bottom-Up (5-10% MAPE)\n",
    "   ‚Üí High: MinTrace (3-7% MAPE, 10-30% improvement)\n",
    "   ‚Üí Maximum: Ensemble + MinTrace\n",
    "\n",
    "5. Do you need probabilistic forecasts?\n",
    "   ‚Üí Yes: Reconcile quantiles separately or sample paths\n",
    "   ‚Üí No: Point forecast reconciliation\n",
    "\n",
    "6. Are there capacity constraints?\n",
    "   ‚Üí Yes: Constrained optimization reconciliation\n",
    "   ‚Üí No: Standard MinTrace/OLS\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Best Practices**\n",
    "\n",
    "**1. Hierarchy Design:**\n",
    "```python\n",
    "# Good hierarchy: Clear aggregation rules\n",
    "hierarchy = {\n",
    "    'Total': ['Region_A', 'Region_B', 'Region_C'],\n",
    "    'Region_A': ['Product_1', 'Product_2', 'Product_3'],\n",
    "    'Region_B': ['Product_1', 'Product_2', 'Product_3'],\n",
    "    ...\n",
    "}\n",
    "\n",
    "# Check: Total = sum of all bottom series\n",
    "assert hierarchy_sum_check(hierarchy, data)\n",
    "```\n",
    "\n",
    "**2. Temporal Reconciliation:**\n",
    "```python\n",
    "# Ensure daily forecasts sum to weekly/monthly\n",
    "def temporal_reconcile(daily_fcst, weekly_fcst, monthly_fcst):\n",
    "    # Build temporal summing matrix\n",
    "    S_temporal = build_temporal_S(days_per_week=7, weeks_per_month=4)\n",
    "    \n",
    "    # Stack forecasts\n",
    "    y_hat = np.concatenate([daily_fcst, weekly_fcst, monthly_fcst])\n",
    "    \n",
    "    # Reconcile\n",
    "    y_tilde = S_temporal @ (G_temporal @ y_hat)\n",
    "    \n",
    "    return y_tilde\n",
    "```\n",
    "\n",
    "**3. Cross-Validation for Hierarchies:**\n",
    "```python\n",
    "# Rolling origin cross-validation at all levels\n",
    "def hierarchical_cv(data, hierarchy, h=12, n_splits=5):\n",
    "    errors = {level: [] for level in all_levels(hierarchy)}\n",
    "    \n",
    "    for train, test in rolling_split(data, h, n_splits):\n",
    "        # Forecast and reconcile\n",
    "        base_fcst = forecast_all_levels(train, hierarchy)\n",
    "        reconciled = mintrace_reconcile(hierarchy, base_fcst)\n",
    "        \n",
    "        # Evaluate at each level\n",
    "        for level in all_levels(hierarchy):\n",
    "            errors[level].append(evaluate(test[level], reconciled[level]))\n",
    "    \n",
    "    return errors\n",
    "```\n",
    "\n",
    "**4. Sparse Hierarchy Handling:**\n",
    "```python\n",
    "# Regularized covariance for sparse data\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "def sparse_mintrace(residuals, S):\n",
    "    # Shrinkage estimator for covariance\n",
    "    lw = LedoitWolf()\n",
    "    W_h = lw.fit(residuals.T).covariance_\n",
    "    \n",
    "    # MinTrace with regularized covariance\n",
    "    W_h_inv = np.linalg.inv(W_h + lambda_reg * np.eye(W_h.shape[0]))\n",
    "    G = np.linalg.inv(S.T @ W_h_inv @ S) @ S.T @ W_h_inv\n",
    "    \n",
    "    return G\n",
    "```\n",
    "\n",
    "**5. Production Deployment:**\n",
    "```python\n",
    "class HierarchicalForecaster:\n",
    "    def __init__(self, hierarchy, method='mintrace'):\n",
    "        self.hierarchy = hierarchy\n",
    "        self.method = method\n",
    "        self.S = build_summing_matrix(hierarchy)\n",
    "        self.G = None  # Precomputed reconciliation matrix\n",
    "        \n",
    "    def fit(self, train_data):\n",
    "        # Train base models at all levels\n",
    "        self.base_models = {}\n",
    "        for level in all_levels(self.hierarchy):\n",
    "            self.base_models[level] = fit_model(train_data[level])\n",
    "        \n",
    "        # Compute reconciliation matrix\n",
    "        if self.method == 'mintrace':\n",
    "            residuals = get_residuals(self.base_models, train_data)\n",
    "            self.G = compute_mintrace_G(self.S, residuals)\n",
    "        elif self.method == 'ols':\n",
    "            self.G = compute_ols_G(self.S)\n",
    "    \n",
    "    def predict(self, h):\n",
    "        # Base forecasts\n",
    "        base_fcst = np.array([self.base_models[level].forecast(h) \n",
    "                              for level in all_levels(self.hierarchy)])\n",
    "        \n",
    "        # Reconcile (fast matrix-vector multiply)\n",
    "        reconciled = self.S @ (self.G @ base_fcst)\n",
    "        \n",
    "        return reconciled\n",
    "    \n",
    "    def validate_coherence(self, forecasts):\n",
    "        # Check summing constraints\n",
    "        coherence_errors = []\n",
    "        for parent, children in self.hierarchy.items():\n",
    "            parent_fcst = forecasts[parent]\n",
    "            children_sum = sum(forecasts[child] for child in children)\n",
    "            coherence_errors.append(abs(parent_fcst - children_sum).max())\n",
    "        \n",
    "        return max(coherence_errors)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Metrics**\n",
    "\n",
    "| **Metric** | **Formula** | **Interpretation** | **Level** |\n",
    "|------------|-------------|--------------------|-----------| \n",
    "| **MASE** | $\\frac{\\text{MAE}}{\\text{MAE}_{\\text{naive}}}$ | Mean Absolute Scaled Error | All levels |\n",
    "| **Coherence Error** | $\\max_t |y_{\\text{parent},t} - \\sum y_{\\text{child},t}|$ | Constraint violation | Hierarchy |\n",
    "| **Reconciliation Benefit** | $\\frac{\\text{MAPE}_{\\text{base}} - \\text{MAPE}_{\\text{recon}}}{\\text{MAPE}_{\\text{base}}}$ | % improvement from reconciliation | All levels |\n",
    "| **Weighted MAPE** | $\\sum w_i \\cdot \\text{MAPE}_i$ | Aggregate accuracy (weight by importance) | Hierarchy |\n",
    "\n",
    "**Reconciliation Benefit Formula:**\n",
    "$$\n",
    "\\text{Benefit} = \\frac{\\sum_{i=1}^n (\\text{Error}_{\\text{base}, i} - \\text{Error}_{\\text{reconciled}, i})}{\\sum_{i=1}^n \\text{Error}_{\\text{base}, i}} \\times 100\\%\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Limitations & Challenges**\n",
    "\n",
    "| **Challenge** | **Impact** | **Mitigation** |\n",
    "|---------------|------------|----------------|\n",
    "| **Computational scaling** | MinTrace O(n¬≥) infeasible for n>10K | Use sparse methods, approximate G, OLS instead |\n",
    "| **Covariance estimation** | Requires long history for accurate W_h | Shrinkage estimators, rolling windows, regularization |\n",
    "| **Distribution shift** | Reconciliation matrix G becomes stale | Recompute G periodically (quarterly), monitor benefit |\n",
    "| **New hierarchy nodes** | No historical data for new products/regions | Use top-down initially, switch to bottom-up after warm-up |\n",
    "| **Constraint violations** | Numerical errors in reconciliation | Regularization, constraint projection, iterative refinement |\n",
    "| **Grouped hierarchies** | Non-unique summing matrix S | Graph-based reconciliation, ensure constraints are consistent |\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Topics**\n",
    "\n",
    "**1. Probabilistic Hierarchical Reconciliation:**\n",
    "```python\n",
    "# Reconcile each quantile separately\n",
    "quantiles = [0.1, 0.5, 0.9]\n",
    "reconciled_quantiles = {}\n",
    "\n",
    "for q in quantiles:\n",
    "    base_fcst_q = forecast_all_levels_quantile(data, q)\n",
    "    reconciled_quantiles[q] = mintrace_reconcile(S, base_fcst_q)\n",
    "\n",
    "# Ensure coherence at each quantile\n",
    "assert all(check_coherence(reconciled_quantiles[q], S) for q in quantiles)\n",
    "```\n",
    "\n",
    "**2. Online Reconciliation (Streaming):**\n",
    "```python\n",
    "# Update reconciliation matrix incrementally\n",
    "class OnlineReconciler:\n",
    "    def __init__(self, S, lambda_=0.95):\n",
    "        self.S = S\n",
    "        self.lambda_ = lambda_  # Forgetting factor\n",
    "        self.W_h = None\n",
    "        \n",
    "    def update(self, new_residuals):\n",
    "        if self.W_h is None:\n",
    "            self.W_h = np.outer(new_residuals, new_residuals)\n",
    "        else:\n",
    "            # Exponentially weighted moving covariance\n",
    "            self.W_h = self.lambda_ * self.W_h + (1 - self.lambda_) * np.outer(new_residuals, new_residuals)\n",
    "        \n",
    "        # Recompute G\n",
    "        self.G = compute_mintrace_G(self.S, self.W_h)\n",
    "```\n",
    "\n",
    "**3. Constrained Hierarchical Forecasting:**\n",
    "```python\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def constrained_hierarchical_forecast(base_fcst, S, capacity_constraints):\n",
    "    \"\"\"\n",
    "    Minimize: ||reconciled - base||^2\n",
    "    Subject to: \n",
    "        - Coherence: S @ x_bottom = x_all\n",
    "        - Capacity: C @ x <= capacity_max\n",
    "    \"\"\"\n",
    "    n = len(base_fcst)\n",
    "    \n",
    "    def objective(x):\n",
    "        return np.sum((x - base_fcst)**2)\n",
    "    \n",
    "    # Coherence constraint (equality)\n",
    "    def coherence_constraint(x):\n",
    "        x_bottom = x[-n_bottom:]\n",
    "        x_reconciled = np.concatenate([S @ x_bottom, x_bottom])\n",
    "        return x - x_reconciled\n",
    "    \n",
    "    # Capacity constraint (inequality)\n",
    "    def capacity_constraint(x):\n",
    "        return capacity_max - C @ x\n",
    "    \n",
    "    constraints = [\n",
    "        {'type': 'eq', 'fun': coherence_constraint},\n",
    "        {'type': 'ineq', 'fun': capacity_constraint}\n",
    "    ]\n",
    "    \n",
    "    result = minimize(objective, base_fcst, constraints=constraints)\n",
    "    return result.x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "**After Mastering Hierarchical Forecasting:**\n",
    "\n",
    "1. **Causal Inference for Time Series:**\n",
    "   - üìò **Notebook 168:** Intervention analysis, counterfactual forecasts\n",
    "   - üîó Synthetic control methods for hierarchies\n",
    "   - üîó Causal impact of promotions across hierarchy levels\n",
    "\n",
    "2. **Real-Time Streaming Forecasting:**\n",
    "   - üìò **Notebook 169:** Online learning, incremental reconciliation\n",
    "   - üîó Low-latency hierarchical forecasting (<100ms)\n",
    "   - üîó Streaming reconciliation with Kafka/Flink\n",
    "\n",
    "3. **Demand Sensing:**\n",
    "   - üìò **Notebook 170:** Short-term forecasting with real-time signals\n",
    "   - üîó Incorporate POS data, social media for bottom-level updates\n",
    "   - üîó Reconcile demand sensing with long-term hierarchical forecasts\n",
    "\n",
    "4. **Forecast Value Optimization:**\n",
    "   - üîó Optimize for business metrics (profit, cost) not just accuracy\n",
    "   - üîó Decision-focused learning (train forecasts to improve downstream decisions)\n",
    "   - üîó Economic reconciliation (weight by $ value, not equal)\n",
    "\n",
    "5. **Hierarchical Deep Learning:**\n",
    "   - üîó Neural hierarchical models (DeepAR with hierarchy constraints)\n",
    "   - üîó Graph neural networks for grouped hierarchies\n",
    "   - üîó Transformer-based hierarchical forecasting\n",
    "\n",
    "---\n",
    "\n",
    "### **Resources**\n",
    "\n",
    "**Books:**\n",
    "- üìö *Forecasting: Principles and Practice* - Hyndman & Athanasopoulos (Chapter 11: Hierarchical)\n",
    "- üìö *Optimal Combination Forecasts* - Timmermann & Elliott (theoretical foundations)\n",
    "\n",
    "**Papers:**\n",
    "- üìÑ *Optimal Forecast Reconciliation* - Wickramasuriya et al. (2019, MinTrace)\n",
    "- üìÑ *Hierarchical Probabilistic Forecasting* - Ben Taieb & Koo (2019)\n",
    "- üìÑ *Forecast Reconciliation: A Geometric View* - Panagiotelis et al. (2021)\n",
    "\n",
    "**Courses:**\n",
    "- üéì Monash University: Forecasting with R (hierarchical forecasting module)\n",
    "- üéì Coursera: Practical Time Series Analysis (hierarchical methods)\n",
    "\n",
    "**Libraries:**\n",
    "- üõ†Ô∏è **scikit-hts:** Hierarchical time series in Python (sklearn-style)\n",
    "- üõ†Ô∏è **hierarchicalforecast:** MinTrace, OLS, WLS reconciliation (Nixtla)\n",
    "- üõ†Ô∏è **fable (R):** Comprehensive hierarchical forecasting (tidyverse ecosystem)\n",
    "- üõ†Ô∏è **hts (R):** Original implementation (Hyndman et al.)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ You've Mastered Hierarchical Time Series Forecasting!\n",
    "\n",
    "**What You Can Now Do:**\n",
    "- ‚úÖ **Build hierarchies** for complex product/geography/temporal structures\n",
    "- ‚úÖ **Implement bottom-up** forecasting with coherence guarantees\n",
    "- ‚úÖ **Apply top-down** disaggregation using historical proportions\n",
    "- ‚úÖ **Deploy optimal reconciliation** (MinTrace, OLS) for maximum accuracy\n",
    "- ‚úÖ **Handle grouped hierarchies** with cross-classifications\n",
    "- ‚úÖ **Reconcile temporal hierarchies** (daily ‚Üí weekly ‚Üí monthly coherence)\n",
    "- ‚úÖ **Quantify business value** from coherent forecasts ($1,010M/year post-silicon)\n",
    "\n",
    "**Your Competitive Advantage:**\n",
    "- üíº **Enterprise-critical skill:** 80% of large organizations have hierarchical forecasting needs\n",
    "- üíº **Complexity premium:** Hierarchical reconciliation expertise rare (avg salary: $175K-210K)\n",
    "- üíº **Cross-functional impact:** Aligns finance (top-level budgets) with operations (bottom-level execution)\n",
    "- üíº **Quantifiable ROI:** 10-30% forecast error reduction = $M savings\n",
    "\n",
    "**Career Paths:**\n",
    "- üéØ **Demand Planning Manager:** Supply chain hierarchical forecasting ($145K-185K)\n",
    "- üéØ **ML Scientist (Forecasting):** Advanced reconciliation methods ($170K-220K)\n",
    "- üéØ **Financial Planning & Analysis:** Budget coherence across business units ($135K-175K)\n",
    "- üéØ **Operations Research Specialist:** Hierarchical optimization for capacity planning ($155K-195K)\n",
    "\n",
    "**Keep Building Coherent Forecasting Systems!** üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021256ad",
   "metadata": {},
   "source": [
    "## üìä Diagnostic Checks Summary\n",
    "\n",
    "**Implementation Checklist:**\n",
    "- ‚úÖ Hierarchical data structure (parent-child relationships defined)\n",
    "- ‚úÖ Base forecasting models (ARIMA, ETS, or ML models per series)\n",
    "- ‚úÖ Reconciliation methods (bottom-up, top-down, MinTrace/OLS)\n",
    "- ‚úÖ Coherence validation (aggregation constraints verified)\n",
    "- ‚úÖ Cross-level accuracy metrics (MAPE at each hierarchy level)\n",
    "- ‚úÖ Post-silicon use cases (wafer‚Üílot‚Üífab yield, equipment downtime, demand forecasting)\n",
    "- ‚úÖ Real-world projects with ROI ($38M-$195M/year)\n",
    "\n",
    "**Quality Metrics Achieved:**\n",
    "- Coherence: 100% (reconciliation ensures sum(children) = parent)\n",
    "- Top-level MAPE: 8-12% (improved vs independent forecasts 12-18%)\n",
    "- Bottom-level MAPE: 15-20% (acceptable for high-variance leaf nodes)\n",
    "- Reconciliation time: <5 seconds for 1000-series hierarchy\n",
    "- Business impact: 25% better inventory planning, 18% improved capacity allocation\n",
    "\n",
    "**Post-Silicon Validation Applications:**\n",
    "- **Wafer ‚Üí Lot ‚Üí Fab Yield:** Forecast fab-level yield coherent with lot-level forecasts ‚Üí Accurate capacity planning\n",
    "- **Product ‚Üí Device ‚Üí SKU Test Time:** Reconcile test time forecasts across hierarchy ‚Üí Optimized tester allocation\n",
    "- **Multi-Site ‚Üí Fab ‚Üí Equipment Downtime:** Coherent downtime forecasts ‚Üí Proactive spare parts inventory\n",
    "\n",
    "**Business ROI:**\n",
    "- Accurate capacity planning: 25% better allocation = **$15M-$40M/year**\n",
    "- Optimized tester utilization: 18% improvement = **$8M-$18M/year**\n",
    "- Spare parts inventory: 30% reduction in emergency procurement = **$5M-$12M/year**\n",
    "- Demand forecasting: Coherent multi-level forecasts = **$10M-$25M/year**\n",
    "- **Total value:** $38M-$95M/year per fab (risk-adjusted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f363b5e",
   "metadata": {},
   "source": [
    "## üîë Key Takeaways\n",
    "\n",
    "**When to Use Hierarchical Forecasting:**\n",
    "- Natural hierarchical structure (geography/product/time, organization levels)\n",
    "- Aggregation constraints must hold (total sales = sum of regional sales)\n",
    "- Cross-level information sharing valuable (lower levels inform upper levels)\n",
    "- Forecasts drive decisions at multiple hierarchy levels (corporate + regional + store)\n",
    "\n",
    "**Limitations:**\n",
    "- Computational cost scales with hierarchy depth (100 leaf nodes √ó 10 levels = 1000 series)\n",
    "- Bottom-up sensitive to low-level noise (aggregation amplifies errors)\n",
    "- Top-down loses granular information (proportional split may not reflect reality)\n",
    "- Optimal reconciliation requires covariance estimation (may be unstable with limited data)\n",
    "\n",
    "**Alternatives:**\n",
    "- **Independent forecasts** (ignore hierarchy, faster but incoherent)\n",
    "- **Grouped time series** (cross-sectional constraints without strict hierarchy)\n",
    "- **Aggregate-only forecasting** (forecast top level only, no disaggregation)\n",
    "- **Machine learning reconciliation** (neural networks learn reconciliation weights)\n",
    "\n",
    "**Best Practices:**\n",
    "- Choose reconciliation based on data quality at each level (bottom-up if leaf nodes reliable)\n",
    "- Use probabilistic forecasts for reconciliation (MinTrace-Sample for uncertainty)\n",
    "- Validate coherence explicitly (assert sum of children = parent)\n",
    "- Monitor forecast performance at all levels (not just top or bottom)\n",
    "- Handle missing data carefully (structural zeros vs missing observations)\n",
    "- Apply cross-validation hierarchically (maintain structure in train/test splits)\n",
    "\n",
    "**Next Steps:**\n",
    "- 168: Causal Inference Time Series (understand causal relationships within hierarchy)\n",
    "- 169: Real-Time Streaming Forecasting (online hierarchical reconciliation)\n",
    "- 166: Probabilistic Time Series (probabilistic reconciliation methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb41b7",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "**When to Use Hierarchical Time Series Forecasting:**\n",
    "- ‚úÖ **Multi-level aggregation** - Forecast total sales + regional + store-level (reconcile hierarchies)\n",
    "- ‚úÖ **Cross-series coherence** - Ensure child forecasts sum to parent (bottom-up, top-down, optimal reconciliation)\n",
    "- ‚úÖ **Structural constraints** - Semiconductor: wafer yield = sum of die yields across bins\n",
    "- ‚úÖ **Resource allocation** - Allocate fab capacity across products based on forecasted demand\n",
    "- ‚úÖ **Improved accuracy** - Leverage cross-series correlations (5-15% MAPE improvement vs. independent forecasts)\n",
    "\n",
    "**Limitations:**\n",
    "- ‚ùå Computational complexity (O(n¬≥) for optimal reconciliation with n series, slow for >100 series)\n",
    "- ‚ùå Requires hierarchy definition (mistakes in hierarchy structure propagate errors)\n",
    "- ‚ùå Data quality critical (missing data at child level corrupts parent forecasts)\n",
    "- ‚ùå Difficult to interpret (reconciliation weights not intuitive for business users)\n",
    "- ‚ùå Overfits to historical patterns (assumes hierarchy structure stays constant)\n",
    "\n",
    "**Alternatives:**\n",
    "- **Independent forecasting** - Forecast each series separately (fast, no coherence guarantees)\n",
    "- **Grouped time series** - Aggregate by categories without hierarchy (simpler, less structure)\n",
    "- **Causal models** - Regression with external regressors (better for regime changes)\n",
    "- **Deep learning** - DeepAR, N-BEATS for large-scale forecasting (no hierarchy constraints)\n",
    "\n",
    "**Best Practices:**\n",
    "- **Bottom-up reconciliation** - Sum child forecasts to parent (simple, preserves detailed patterns)\n",
    "- **MinTrace optimal** - Minimize trace of error covariance matrix (best accuracy, computationally expensive)\n",
    "- **Temporal aggregation** - Forecast at multiple horizons (daily, weekly, monthly) and reconcile\n",
    "- **Cross-validation** - Split data respecting hierarchy (don't leak parent info into child training)\n",
    "- **Forecast combinations** - Combine bottom-up + top-down + middle-out (robust to hierarchy changes)\n",
    "- **Monitor coherence** - Alert when child forecasts drift >10% from parent (data quality issue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94406c7f",
   "metadata": {},
   "source": [
    "## üîç Diagnostic & Mastery + Progress\n",
    "\n",
    "### Implementation Checklist\n",
    "- ‚úÖ **Hierarchy definition** - Parent-child relationships (fab ‚Üí products ‚Üí bins)  \n",
    "- ‚úÖ **Bottom-up reconciliation** - Sum child forecasts to parent  \n",
    "- ‚úÖ **Optimal reconciliation** - MinTrace for coherent forecasts  \n",
    "- ‚úÖ **scikit-hts** or **hts** package for Python implementation  \n",
    "- ‚úÖ **Cross-validation** - Respect hierarchy structure in train/test splits  \n",
    "\n",
    "### Quality Metrics\n",
    "- **Coherence**: Child forecasts sum to parent within 5% error  \n",
    "- **MAPE improvement**: 5-15% better than independent forecasts  \n",
    "- **Computational time**: <5 minutes for 100-series hierarchy (MinTrace)  \n",
    "\n",
    "### Post-Silicon Application\n",
    "**Multi-Product Yield Forecasting**  \n",
    "- **Input**: Forecast total wafer yield + per-product yields (5 products) + per-bin yields (3 bins/product)  \n",
    "- **Solution**: Hierarchical forecasting ensures sum(product yields) = total yield, reconcile with bottom-up (preserve bin-level patterns)  \n",
    "- **Value**: Improve fab capacity planning accuracy 10-15% ‚Üí reduce overproduction waste, save $2.1M/year  \n",
    "\n",
    "### ROI: $2.1M-$6.3M/year (medium fab), $8.4M-$25M/year (large fab)  \n",
    "\n",
    "‚úÖ Build hierarchical time series models with coherence constraints  \n",
    "‚úÖ Apply MinTrace optimal reconciliation  \n",
    "‚úÖ Forecast semiconductor yields at multiple aggregation levels  \n",
    "\n",
    "**Session**: 45/60 notebooks done (75%) | **Overall**: ~155/175 complete (88.6%)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
