{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b54f1acd",
   "metadata": {},
   "source": [
    "# 162: Process Mining & Event Log Analysis\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** process mining fundamentals (discovery, conformance, enhancement)\n",
    "- **Implement** directly-follows graph algorithm for process discovery\n",
    "- **Build** conformance checking system to detect process deviations\n",
    "- **Analyze** process performance to identify bottlenecks and resource constraints\n",
    "- **Apply** process mining to semiconductor manufacturing workflows\n",
    "- **Optimize** business processes using data-driven insights\n",
    "\n",
    "## üìö What is Process Mining?\n",
    "\n",
    "**Process Mining** is the bridge between **data mining** and **business process management (BPM)**. It uses event logs (recorded activities) to discover, monitor, and improve real-world business processes.\n",
    "\n",
    "Unlike traditional process modeling (which designs processes top-down), process mining **discovers** actual processes from execution data. This reveals the true process, including deviations, bottlenecks, and inefficiencies.\n",
    "\n",
    "**Three Types of Process Mining:**\n",
    "\n",
    "1. **Process Discovery**: Automatically create process models from event logs\n",
    "   - Input: Event log (case_id, activity, timestamp)\n",
    "   - Output: Process model (graph, Petri net, BPMN)\n",
    "   - Example: \"What is our actual wafer fabrication flow?\"\n",
    "\n",
    "2. **Conformance Checking**: Compare actual process vs expected process\n",
    "   - Input: Event log + Reference model\n",
    "   - Output: Conformance score, violations, deviations\n",
    "   - Example: \"Are we following our quality control procedures?\"\n",
    "\n",
    "3. **Process Enhancement**: Improve existing models with additional information\n",
    "   - Input: Process model + Event log\n",
    "   - Output: Enhanced model (performance metrics, resource utilization)\n",
    "   - Example: \"Where are the bottlenecks in our test flow?\"\n",
    "\n",
    "**Why Process Mining?**\n",
    "- ‚úÖ **Discover reality**: See actual process, not assumed/documented process\n",
    "- ‚úÖ **Data-driven optimization**: Use event data, not opinions or guesses\n",
    "- ‚úÖ **Continuous monitoring**: Track process performance over time\n",
    "- ‚úÖ **Compliance checking**: Ensure processes follow standards/regulations\n",
    "- ‚úÖ **Bottleneck identification**: Find where time/resources are wasted\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**1. Semiconductor Manufacturing Process Flow Analysis**\n",
    "- **Input**: MES event logs (lithography, deposition, etch, implant, test, rework, scrap)\n",
    "- **Output**: Actual fabrication process model with rework loops, bottlenecks\n",
    "- **Value**: Reduce cycle time by 12% (48 hours ‚Üí 42 hours) = **$56.2M/year**\n",
    "- **Method**: Heuristic miner + bottleneck analysis + variant comparison\n",
    "\n",
    "**2. ATE Test Flow Optimization**\n",
    "- **Input**: ATE event logs (power-on, continuity, DC parametric, AC functional, burn-in, binning)\n",
    "- **Output**: Optimized test sequence with parallel execution opportunities\n",
    "- **Value**: 18% test time reduction (45 sec ‚Üí 37 sec) = **$41.7M/year**\n",
    "- **Method**: Process discovery + time analysis + sequence optimization\n",
    "\n",
    "**3. Device Debug & RMA Workflow Analysis**\n",
    "- **Input**: RMA event logs (receive, electrical test, physical FA, root cause, disposition, report)\n",
    "- **Output**: Bottleneck identification, resolution time prediction\n",
    "- **Value**: 35% faster resolution (8 days ‚Üí 5.2 days) = **$38.4M/year**\n",
    "- **Method**: Conformance checking + resource performance analysis\n",
    "\n",
    "**4. Quality Control Process Compliance**\n",
    "- **Input**: QC event logs (wafer inspection, die sort, visual inspection, electrical test, final QA)\n",
    "- **Output**: Violations detected (skipped steps), compliance dashboard\n",
    "- **Value**: 95% compliance (vs 78% baseline), prevent 12 customer escapes/year = **$47.8M/year**\n",
    "- **Method**: Conformance checking + rule violation detection\n",
    "\n",
    "**Total Business Value: $184.1M/year**\n",
    "\n",
    "## üîÑ Process Mining Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Event Logs] --> B[Process Discovery]\n",
    "    B --> C[Process Model]\n",
    "    C --> D[Conformance Checking]\n",
    "    C --> E[Performance Analysis]\n",
    "    D --> F[Deviations & Violations]\n",
    "    E --> G[Bottlenecks & Metrics]\n",
    "    F --> H[Process Improvement]\n",
    "    G --> H\n",
    "    H --> I[Optimized Process]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style C fill:#fff4e1\n",
    "    style I fill:#e1ffe1\n",
    "```\n",
    "\n",
    "**Event Logs** (raw data) ‚Üí **Discovery** (find patterns) ‚Üí **Process Model** (visual representation) ‚Üí **Analysis** (conformance, performance) ‚Üí **Insights** (violations, bottlenecks) ‚Üí **Improvement** (optimize)\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **159_Sequential_Anomaly_Detection**: Time series analysis, pattern recognition\n",
    "- **160_Multi_Variate_Anomaly_Detection**: Correlation analysis, spatial patterns\n",
    "- **161_Root_Cause_Analysis_Explainable_Anomalies**: Explainability methods\n",
    "- **001_DSA_Python_Mastery**: Graph algorithms (DFS, BFS), dynamic programming (edit distance)\n",
    "- **026_KMeans_Clustering**: Grouping similar traces (variant analysis)\n",
    "\n",
    "**Next Steps:**\n",
    "- **163_Business_Process_Optimization**: Combine process mining with optimization algorithms\n",
    "- **154_Model_Deployment_Best_Practices**: Deploy process mining models to production\n",
    "- **155_Production_ML_Infrastructure**: Build real-time process monitoring systems\n",
    "\n",
    "---\n",
    "\n",
    "Let's discover, analyze, and optimize business processes! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b192fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setup: Process Mining & Event Log Analysis\n",
    "\n",
    "Production Stack:\n",
    "- Process Mining: pm4py (Python library for process mining)\n",
    "- Event Processing: pandas, numpy\n",
    "- Visualization: graphviz (process models), matplotlib, seaborn\n",
    "- Graph Analysis: networkx (for process graphs)\n",
    "- Optimization: scipy, pulp (for process optimization)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Set, Optional\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Graph processing\n",
    "import networkx as nx\n",
    "\n",
    "# Visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Process mining concepts (we'll implement core algorithms)\n",
    "# Production: pip install pm4py\n",
    "# import pm4py\n",
    "\n",
    "print(\"‚úÖ Setup complete - Process Mining tools loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fdb617",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Event Log Generation & Basic Analysis\n",
    "\n",
    "### üìù What's Happening in This Section?\n",
    "\n",
    "**Purpose:** Create and analyze event logs - the foundation of process mining.\n",
    "\n",
    "**Event Log Requirements:**\n",
    "Every event must have:\n",
    "- **case_id**: Unique identifier for process instance (e.g., device serial number, order ID)\n",
    "- **activity**: What happened (e.g., \"Fabrication\", \"Test\", \"Approve\")\n",
    "- **timestamp**: When it happened (datetime)\n",
    "- **Optional**: resource (who/what performed it), cost, attributes\n",
    "\n",
    "**Event Log Quality:**\n",
    "- ‚úÖ **Complete**: All activities logged (no gaps)\n",
    "- ‚úÖ **Accurate**: Timestamps correct (watch clock synchronization issues)\n",
    "- ‚úÖ **Consistent**: Activity names standardized (not \"Test\" vs \"Testing\" vs \"test\")\n",
    "- ‚úÖ **Granular**: Right level of detail (not too coarse, not too fine)\n",
    "\n",
    "**Common Data Quality Issues:**\n",
    "1. **Missing events**: Some activities not logged\n",
    "2. **Duplicate events**: Same activity logged twice\n",
    "3. **Out-of-order timestamps**: Clock drift, timezone issues\n",
    "4. **Inconsistent naming**: Multiple names for same activity\n",
    "5. **Incomplete cases**: Process started but never finished\n",
    "\n",
    "**Trace vs Variant:**\n",
    "- **Trace**: Sequence of activities for ONE case\n",
    "  - Example: Case C001 ‚Üí [Order, Fab, Test, Ship]\n",
    "- **Variant**: Unique trace pattern (multiple cases may follow same variant)\n",
    "  - Variant 1 (80% of cases): [Order, Fab, Test, Ship] (happy path)\n",
    "  - Variant 2 (15% of cases): [Order, Fab, Test, Rework, Test, Ship] (rework)\n",
    "  - Variant 3 (5% of cases): [Order, Fab, Ship] (skipped test - violation!)\n",
    "\n",
    "**Process Mining Metrics:**\n",
    "- **Throughput**: Cases completed per time unit\n",
    "- **Cycle time**: Duration from start to end of case\n",
    "- **Activity frequency**: How often each activity occurs\n",
    "- **Variant frequency**: % of cases following each variant\n",
    "- **Rework rate**: % of cases with repeated activities\n",
    "\n",
    "**Why This Matters:**\n",
    "Understanding event log structure is critical for:\n",
    "- Data quality validation before process mining\n",
    "- Identifying which processes can benefit from mining\n",
    "- Scoping the analysis (which activities to include)\n",
    "\n",
    "**Post-Silicon Example:**\n",
    "Device manufacturing event log:\n",
    "- **case_id**: Device serial number (SN123456)\n",
    "- **activity**: Lithography, Etch, Test, Package, etc.\n",
    "- **resource**: Tool ID (Fab_Tool_3, ATE_5)\n",
    "- **cost**: Step cost ($50 for test, $500 for fab)\n",
    "- Typical trace: 15-30 activities per device, 72-hour cycle time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cce48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic event log for semiconductor manufacturing\n",
    "def generate_manufacturing_event_log(n_cases: int = 100, seed: int = 47):\n",
    "    \"\"\"\n",
    "    Generate event log for semiconductor device manufacturing\n",
    "    \n",
    "    Process variants:\n",
    "    1. Happy path (70%): Order ‚Üí Fab ‚Üí Test ‚Üí Package ‚Üí Ship\n",
    "    2. Rework (20%): Order ‚Üí Fab ‚Üí Test ‚Üí Rework ‚Üí Test ‚Üí Package ‚Üí Ship\n",
    "    3. QA skip (5%): Order ‚Üí Fab ‚Üí Package ‚Üí Ship (violation - skipped test)\n",
    "    4. Multi-rework (5%): Order ‚Üí Fab ‚Üí Test ‚Üí Rework ‚Üí Test ‚Üí Rework ‚Üí Test ‚Üí Package ‚Üí Ship\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    events = []\n",
    "    case_id_counter = 1\n",
    "    \n",
    "    # Activity costs and durations (in hours)\n",
    "    activity_info = {\n",
    "        'Receive_Order': {'cost': 0, 'duration_mean': 0.5, 'duration_std': 0.1},\n",
    "        'Fabrication': {'cost': 2500, 'duration_mean': 24, 'duration_std': 4},\n",
    "        'Wafer_Test': {'cost': 800, 'duration_mean': 4, 'duration_std': 0.5},\n",
    "        'Rework': {'cost': 500, 'duration_mean': 8, 'duration_std': 1.5},\n",
    "        'Packaging': {'cost': 300, 'duration_mean': 3, 'duration_std': 0.3},\n",
    "        'Final_Test': {'cost': 400, 'duration_mean': 2, 'duration_std': 0.2},\n",
    "        'Quality_Check': {'cost': 100, 'duration_mean': 1, 'duration_std': 0.1},\n",
    "        'Ship': {'cost': 150, 'duration_mean': 0.5, 'duration_std': 0.1}\n",
    "    }\n",
    "    \n",
    "    # Resources\n",
    "    resources = {\n",
    "        'Receive_Order': ['System'],\n",
    "        'Fabrication': ['Fab_Line_1', 'Fab_Line_2', 'Fab_Line_3'],\n",
    "        'Wafer_Test': ['ATE_1', 'ATE_2', 'ATE_3', 'ATE_4', 'ATE_5'],\n",
    "        'Rework': ['Rework_Station_1', 'Rework_Station_2'],\n",
    "        'Packaging': ['Pack_Line_1', 'Pack_Line_2'],\n",
    "        'Final_Test': ['ATE_6', 'ATE_7', 'ATE_8'],\n",
    "        'Quality_Check': ['QA_Engineer_1', 'QA_Engineer_2'],\n",
    "        'Ship': ['Logistics']\n",
    "    }\n",
    "    \n",
    "    start_date = datetime(2025, 1, 1, 8, 0, 0)\n",
    "    \n",
    "    for i in range(n_cases):\n",
    "        case_id = f\"DEV{case_id_counter:05d}\"\n",
    "        case_id_counter += 1\n",
    "        \n",
    "        # Determine variant\n",
    "        variant_prob = np.random.rand()\n",
    "        if variant_prob < 0.70:\n",
    "            # Happy path\n",
    "            process_flow = ['Receive_Order', 'Fabrication', 'Wafer_Test', \n",
    "                           'Packaging', 'Final_Test', 'Quality_Check', 'Ship']\n",
    "        elif variant_prob < 0.90:\n",
    "            # Single rework\n",
    "            process_flow = ['Receive_Order', 'Fabrication', 'Wafer_Test', 'Rework',\n",
    "                           'Wafer_Test', 'Packaging', 'Final_Test', 'Quality_Check', 'Ship']\n",
    "        elif variant_prob < 0.95:\n",
    "            # QA skip (compliance violation)\n",
    "            process_flow = ['Receive_Order', 'Fabrication', 'Wafer_Test',\n",
    "                           'Packaging', 'Final_Test', 'Ship']  # Skipped Quality_Check\n",
    "        else:\n",
    "            # Multi-rework\n",
    "            process_flow = ['Receive_Order', 'Fabrication', 'Wafer_Test', 'Rework',\n",
    "                           'Wafer_Test', 'Rework', 'Wafer_Test', \n",
    "                           'Packaging', 'Final_Test', 'Quality_Check', 'Ship']\n",
    "        \n",
    "        # Generate events for this case\n",
    "        current_time = start_date + timedelta(hours=np.random.randint(0, 72))  # Random start\n",
    "        \n",
    "        for activity in process_flow:\n",
    "            info = activity_info[activity]\n",
    "            \n",
    "            # Add variability to duration\n",
    "            duration = max(0.1, np.random.normal(info['duration_mean'], info['duration_std']))\n",
    "            \n",
    "            # Select resource\n",
    "            resource = np.random.choice(resources[activity])\n",
    "            \n",
    "            events.append({\n",
    "                'case_id': case_id,\n",
    "                'activity': activity,\n",
    "                'timestamp': current_time,\n",
    "                'resource': resource,\n",
    "                'cost': info['cost']\n",
    "            })\n",
    "            \n",
    "            # Move time forward\n",
    "            current_time += timedelta(hours=duration)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    event_log = pd.DataFrame(events)\n",
    "    event_log = event_log.sort_values(['case_id', 'timestamp']).reset_index(drop=True)\n",
    "    \n",
    "    return event_log\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVENT LOG GENERATION & BASIC ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate event log\n",
    "event_log = generate_manufacturing_event_log(n_cases=100, seed=47)\n",
    "\n",
    "print(f\"\\n‚úÖ Generated event log: {len(event_log)} events, {event_log['case_id'].nunique()} cases\")\n",
    "print(f\"\\nüìä Sample events:\")\n",
    "print(event_log.head(10).to_string(index=False))\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nüìà Event Log Statistics:\")\n",
    "print(f\"   Total events: {len(event_log)}\")\n",
    "print(f\"   Total cases: {event_log['case_id'].nunique()}\")\n",
    "print(f\"   Unique activities: {event_log['activity'].nunique()}\")\n",
    "print(f\"   Time span: {event_log['timestamp'].min()} to {event_log['timestamp'].max()}\")\n",
    "print(f\"   Duration: {(event_log['timestamp'].max() - event_log['timestamp'].min()).days} days\")\n",
    "\n",
    "# Activity frequency\n",
    "print(f\"\\nüî¢ Activity Frequency:\")\n",
    "activity_counts = event_log['activity'].value_counts()\n",
    "for activity, count in activity_counts.items():\n",
    "    pct = (count / event_log['case_id'].nunique()) * 100\n",
    "    print(f\"   {activity:20s}: {count:3d} occurrences ({pct:5.1f}% of cases)\")\n",
    "\n",
    "# Extract traces (sequences) for each case\n",
    "def extract_traces(event_log):\n",
    "    \"\"\"Extract trace (activity sequence) for each case\"\"\"\n",
    "    traces = {}\n",
    "    for case_id, group in event_log.groupby('case_id'):\n",
    "        trace = tuple(group.sort_values('timestamp')['activity'].tolist())\n",
    "        traces[case_id] = trace\n",
    "    return traces\n",
    "\n",
    "traces = extract_traces(event_log)\n",
    "\n",
    "# Count variants\n",
    "from collections import Counter\n",
    "variant_counts = Counter(traces.values())\n",
    "\n",
    "print(f\"\\nüîÑ Process Variants:\")\n",
    "print(f\"   Total unique variants: {len(variant_counts)}\")\n",
    "for i, (variant, count) in enumerate(variant_counts.most_common(5), 1):\n",
    "    pct = (count / len(traces)) * 100\n",
    "    print(f\"\\n   Variant {i} ({count} cases, {pct:.1f}%):\")\n",
    "    print(f\"      {' ‚Üí '.join(variant)}\")\n",
    "\n",
    "# Cycle time analysis\n",
    "def calculate_cycle_times(event_log):\n",
    "    \"\"\"Calculate cycle time for each case\"\"\"\n",
    "    cycle_times = {}\n",
    "    for case_id, group in event_log.groupby('case_id'):\n",
    "        start_time = group['timestamp'].min()\n",
    "        end_time = group['timestamp'].max()\n",
    "        cycle_time_hours = (end_time - start_time).total_seconds() / 3600\n",
    "        cycle_times[case_id] = cycle_time_hours\n",
    "    return cycle_times\n",
    "\n",
    "cycle_times = calculate_cycle_times(event_log)\n",
    "cycle_times_values = list(cycle_times.values())\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Cycle Time Statistics:\")\n",
    "print(f\"   Mean: {np.mean(cycle_times_values):.1f} hours\")\n",
    "print(f\"   Median: {np.median(cycle_times_values):.1f} hours\")\n",
    "print(f\"   Std Dev: {np.std(cycle_times_values):.1f} hours\")\n",
    "print(f\"   Min: {np.min(cycle_times_values):.1f} hours\")\n",
    "print(f\"   Max: {np.max(cycle_times_values):.1f} hours\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Activity frequency\n",
    "ax = axes[0, 0]\n",
    "activity_counts.plot(kind='barh', ax=ax, color='steelblue', alpha=0.7)\n",
    "ax.set_xlabel('Number of Occurrences')\n",
    "ax.set_title('Activity Frequency')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 2: Variant distribution\n",
    "ax = axes[0, 1]\n",
    "top_variants = variant_counts.most_common(5)\n",
    "variant_labels = [f\"V{i}\" for i in range(1, len(top_variants)+1)]\n",
    "variant_values = [count for _, count in top_variants]\n",
    "ax.bar(variant_labels, variant_values, color='coral', alpha=0.7)\n",
    "ax.set_ylabel('Number of Cases')\n",
    "ax.set_title('Top 5 Process Variants')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Cycle time distribution\n",
    "ax = axes[1, 0]\n",
    "ax.hist(cycle_times_values, bins=20, color='green', alpha=0.7, edgecolor='black')\n",
    "ax.axvline(np.mean(cycle_times_values), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "ax.axvline(np.median(cycle_times_values), color='orange', linestyle='--', linewidth=2, label='Median')\n",
    "ax.set_xlabel('Cycle Time (hours)')\n",
    "ax.set_ylabel('Number of Cases')\n",
    "ax.set_title('Cycle Time Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Events over time\n",
    "ax = axes[1, 1]\n",
    "event_log['date'] = event_log['timestamp'].dt.date\n",
    "events_per_day = event_log.groupby('date').size()\n",
    "events_per_day.plot(ax=ax, marker='o', linestyle='-', color='purple')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Number of Events')\n",
    "ax.set_title('Events Over Time')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   - 70% of cases follow happy path (expected)\")\n",
    "print(\"   - ~20% have rework (test failures)\")\n",
    "print(\"   - ~5% skip Quality_Check (compliance violation)\")\n",
    "print(\"   - Mean cycle time ~40 hours (variability from rework)\")\n",
    "print(\"\\nüí∞ Business Value: Foundation for $56.2M/year process optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ed8c9",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Process Discovery (Directly-Follows Graph)\n",
    "\n",
    "### üìù What's Happening in This Method?\n",
    "\n",
    "**Purpose:** Automatically discover process model from event logs - reveal actual process flow.\n",
    "\n",
    "**Directly-Follows Graph (DFG):**\n",
    "- Simplest process discovery method\n",
    "- Shows which activities directly follow each other\n",
    "- **Edge (A ‚Üí B)**: Activity B can directly follow activity A\n",
    "- **Edge weight**: Frequency (how many times A ‚Üí B occurred)\n",
    "\n",
    "**Algorithm:**\n",
    "1. For each case, extract trace (sequence of activities)\n",
    "2. For each consecutive pair (A, B) in trace, add edge A ‚Üí B\n",
    "3. Count frequency of each edge\n",
    "4. Filter low-frequency edges (noise reduction)\n",
    "\n",
    "**Mathematical Representation:**\n",
    "$$\n",
    "\\text{DFG} = (A, E, W)\n",
    "$$\n",
    "Where:\n",
    "- $A$ = Set of activities\n",
    "- $E \\subseteq A \\times A$ = Set of directed edges (follows relationships)\n",
    "- $W: E \\rightarrow \\mathbb{N}$ = Edge weights (frequencies)\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ **Simple**: Easy to understand and implement\n",
    "- ‚úÖ **Fast**: O(n) where n = number of events\n",
    "- ‚úÖ **Handles noise**: Frequency-based filtering\n",
    "- ‚úÖ **Visual**: Clear process visualization\n",
    "\n",
    "**Limitations:**\n",
    "- ‚ùå **No concurrency**: Can't distinguish parallel vs sequential\n",
    "- ‚ùå **All paths shown**: Even rare/exceptional paths\n",
    "- ‚ùå **No loops**: Hard to see if loop is intentional vs rework\n",
    "\n",
    "**More Advanced Algorithms:**\n",
    "1. **Alpha Miner**: Discovers Petri nets, handles concurrency\n",
    "2. **Heuristic Miner**: Noise-tolerant, uses dependency measures\n",
    "3. **Inductive Miner**: Guarantees sound models (no deadlocks)\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "- Discover actual wafer fabrication flow from MES logs\n",
    "- Example findings:\n",
    "  - \"Etch sometimes happens before or after deposition (parallel?)\"\n",
    "  - \"15% of wafers go through Lithography twice (rework)\"\n",
    "  - \"Test ‚Üí Rework ‚Üí Test loop has 85% ‚Üí 15% split\"\n",
    "- Business value: $56.2M/year from cycle time reduction\n",
    "\n",
    "**Interpretation:**\n",
    "- **Thick edges**: Common path (happy path)\n",
    "- **Thin edges**: Rare path (exceptions, rework)\n",
    "- **Cycles**: Rework loops (investigate why)\n",
    "- **Missing edges**: Expected transition not occurring (possible violation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a78b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================\n",
    "# Process Discovery: Directly-Follows Graph (DFG)\n",
    "# ========================================================================================\n",
    "\n",
    "def build_directly_follows_graph(event_log: pd.DataFrame, \n",
    "                                   min_frequency: int = 1) -> Dict[Tuple[str, str], int]:\n",
    "    \"\"\"\n",
    "    Build directly-follows graph from event log.\n",
    "    \n",
    "    Args:\n",
    "        event_log: DataFrame with columns [case_id, activity, timestamp, ...]\n",
    "        min_frequency: Minimum edge frequency to include (noise filtering)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping (activity_A, activity_B) -> frequency\n",
    "        Represents edges A ‚Üí B with counts\n",
    "    \"\"\"\n",
    "    # Extract traces for all cases\n",
    "    traces = extract_traces(event_log)\n",
    "    \n",
    "    # Count directly-follows relationships\n",
    "    dfg = {}\n",
    "    for trace in traces.values():\n",
    "        # For each consecutive pair in trace\n",
    "        for i in range(len(trace) - 1):\n",
    "            activity_from = trace[i]\n",
    "            activity_to = trace[i + 1]\n",
    "            edge = (activity_from, activity_to)\n",
    "            dfg[edge] = dfg.get(edge, 0) + 1\n",
    "    \n",
    "    # Filter edges below minimum frequency (noise reduction)\n",
    "    dfg_filtered = {edge: count for edge, count in dfg.items() \n",
    "                    if count >= min_frequency}\n",
    "    \n",
    "    return dfg_filtered\n",
    "\n",
    "\n",
    "def analyze_dfg(dfg: Dict[Tuple[str, str], int], \n",
    "                total_cases: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze directly-follows graph statistics.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with analysis metrics\n",
    "    \"\"\"\n",
    "    total_edges = len(dfg)\n",
    "    total_transitions = sum(dfg.values())\n",
    "    \n",
    "    # Find most common transitions\n",
    "    sorted_edges = sorted(dfg.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_5_transitions = sorted_edges[:5]\n",
    "    \n",
    "    # Find activities (nodes)\n",
    "    activities = set()\n",
    "    for (from_act, to_act), _ in dfg.items():\n",
    "        activities.add(from_act)\n",
    "        activities.add(to_act)\n",
    "    \n",
    "    # Find start and end activities\n",
    "    # Start: appears as 'from' but not as 'to' (or first in sequences)\n",
    "    # End: appears as 'to' but not as 'from' (or last in sequences)\n",
    "    from_activities = {edge[0] for edge in dfg.keys()}\n",
    "    to_activities = {edge[1] for edge in dfg.keys()}\n",
    "    \n",
    "    start_candidates = from_activities - to_activities\n",
    "    end_candidates = to_activities - from_activities\n",
    "    \n",
    "    # Calculate average transitions per case\n",
    "    avg_transitions = total_transitions / total_cases if total_cases > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total_activities': len(activities),\n",
    "        'total_edges': total_edges,\n",
    "        'total_transitions': total_transitions,\n",
    "        'avg_transitions_per_case': avg_transitions,\n",
    "        'top_5_transitions': top_5_transitions,\n",
    "        'start_activities': start_candidates,\n",
    "        'end_activities': end_candidates,\n",
    "        'activities': activities\n",
    "    }\n",
    "\n",
    "\n",
    "# Build DFG from our event log\n",
    "dfg = build_directly_follows_graph(event_log, min_frequency=2)\n",
    "\n",
    "print(f\"‚úÖ Directly-Follows Graph Built\")\n",
    "print(f\"   Total edges: {len(dfg)}\")\n",
    "print(f\"   Total transitions recorded: {sum(dfg.values())}\\n\")\n",
    "\n",
    "# Analyze DFG\n",
    "analysis = analyze_dfg(dfg, n_cases=100)\n",
    "\n",
    "print(\"üìä DFG Analysis:\")\n",
    "print(f\"   Activities (nodes): {analysis['total_activities']}\")\n",
    "print(f\"   Edges: {analysis['total_edges']}\")\n",
    "print(f\"   Avg transitions/case: {analysis['avg_transitions_per_case']:.1f}\")\n",
    "print(f\"   Start activities: {analysis['start_activities']}\")\n",
    "print(f\"   End activities: {analysis['end_activities']}\\n\")\n",
    "\n",
    "print(\"üîù Top 5 Most Frequent Transitions:\")\n",
    "for (from_act, to_act), count in analysis['top_5_transitions']:\n",
    "    percentage = (count / 100) * 100  # 100 cases\n",
    "    print(f\"   {from_act} ‚Üí {to_act}: {count} times ({percentage:.0f}%)\")\n",
    "\n",
    "# Visualize DFG as network graph\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Left: DFG network visualization using networkx\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add edges with weights\n",
    "for (from_act, to_act), count in dfg.items():\n",
    "    G.add_edge(from_act, to_act, weight=count)\n",
    "\n",
    "# Layout\n",
    "pos = nx.spring_layout(G, k=2, iterations=50, seed=47)\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=3000, node_color='lightblue', \n",
    "                       alpha=0.9, ax=ax1)\n",
    "\n",
    "# Draw edges with varying thickness based on frequency\n",
    "edges = G.edges()\n",
    "weights = [G[u][v]['weight'] for u, v in edges]\n",
    "max_weight = max(weights)\n",
    "\n",
    "# Normalize weights for visualization (thickness)\n",
    "widths = [3 * (w / max_weight) for w in weights]\n",
    "\n",
    "nx.draw_networkx_edges(G, pos, width=widths, alpha=0.6, \n",
    "                       edge_color='gray', arrows=True, \n",
    "                       arrowsize=20, arrowstyle='->', ax=ax1)\n",
    "\n",
    "# Draw labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold', ax=ax1)\n",
    "\n",
    "# Add edge labels (frequencies)\n",
    "edge_labels = {(u, v): G[u][v]['weight'] for u, v in edges}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=7, ax=ax1)\n",
    "\n",
    "ax1.set_title('Directly-Follows Graph (Network View)', fontsize=14, fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Right: Transition frequency heatmap (matrix representation)\n",
    "# Create adjacency matrix\n",
    "activities_list = sorted(list(analysis['activities']))\n",
    "n_activities = len(activities_list)\n",
    "activity_to_idx = {act: i for i, act in enumerate(activities_list)}\n",
    "\n",
    "adjacency_matrix = np.zeros((n_activities, n_activities))\n",
    "for (from_act, to_act), count in dfg.items():\n",
    "    i = activity_to_idx[from_act]\n",
    "    j = activity_to_idx[to_act]\n",
    "    adjacency_matrix[i, j] = count\n",
    "\n",
    "# Plot heatmap\n",
    "sns.heatmap(adjacency_matrix, annot=True, fmt='.0f', cmap='YlOrRd', \n",
    "            xticklabels=[act[:12] for act in activities_list],\n",
    "            yticklabels=[act[:12] for act in activities_list],\n",
    "            cbar_kws={'label': 'Transition Frequency'}, ax=ax2)\n",
    "ax2.set_title('Transition Frequency Matrix', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('To Activity', fontsize=11)\n",
    "ax2.set_ylabel('From Activity', fontsize=11)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   ‚Ä¢ Network graph shows process flow with edge thickness = frequency\")\n",
    "print(\"   ‚Ä¢ Thick edges = common paths (happy path)\")\n",
    "print(\"   ‚Ä¢ Thin edges = rare paths (exceptions, rework)\")\n",
    "print(\"   ‚Ä¢ Cycles visible = rework loops (Wafer_Test ‚Üí Rework ‚Üí Wafer_Test)\")\n",
    "print(\"   ‚Ä¢ Matrix view shows all possible transitions (0 = never occurs)\")\n",
    "print(\"   ‚Ä¢ Foundation for $56.2M/year optimization (identify bottlenecks, eliminate rework)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0ab6f",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Conformance Checking (Process Compliance)\n",
    "\n",
    "### üìù What's Happening in This Method?\n",
    "\n",
    "**Purpose:** Detect deviations between observed process (event log) and expected process (model).\n",
    "\n",
    "**Conformance Checking:**\n",
    "- Compare actual traces vs expected process model\n",
    "- Identify violations, skipped activities, extra activities\n",
    "- Measure conformance score (0-1, where 1 = perfect compliance)\n",
    "\n",
    "**Token-Based Replay Algorithm:**\n",
    "1. Define expected process model (reference trace or rules)\n",
    "2. For each actual trace, try to \"replay\" it on the model\n",
    "3. Count violations:\n",
    "   - **Missing activities**: Expected but not executed\n",
    "   - **Extra activities**: Executed but not expected\n",
    "   - **Wrong order**: Activities in incorrect sequence\n",
    "4. Calculate conformance score\n",
    "\n",
    "**Conformance Metrics:**\n",
    "$$\n",
    "\\text{Conformance Score} = \\frac{\\text{Matching Activities}}{\\text{Total Expected Activities}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Fitness} = \\frac{\\text{Consumed Tokens} + \\text{Produced Tokens}}{\\text{Missing Tokens} + \\text{Remaining Tokens}}\n",
    "$$\n",
    "\n",
    "**Simple Approach (Trace Alignment):**\n",
    "- Compare actual trace to expected trace\n",
    "- Use edit distance (Levenshtein distance)\n",
    "- Lower distance = higher conformance\n",
    "\n",
    "**Why It Matters:**\n",
    "- ‚úÖ **Quality control**: Ensure processes follow standards\n",
    "- ‚úÖ **Compliance**: Detect regulatory violations (e.g., skipped Quality_Check)\n",
    "- ‚úÖ **Audit trail**: Identify who/when/where violations occurred\n",
    "- ‚úÖ **Root cause**: Link violations to outcomes (yield, quality, etc.)\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "- Ensure quality control steps not skipped\n",
    "- Example findings:\n",
    "  - \"5% of devices skipped final QA (compliance violation)\"\n",
    "  - \"12 wafers bypassed contamination check (risk)\"\n",
    "  - \"Device #0048 had rework but no root cause documentation\"\n",
    "- Business value: $47.8M/year from preventing customer escapes\n",
    "\n",
    "**Conformance Categories:**\n",
    "- **Compliant (>95%)**: Process followed correctly\n",
    "- **Minor deviation (80-95%)**: Small variations (investigate)\n",
    "- **Major deviation (<80%)**: Serious violations (immediate action)\n",
    "\n",
    "**Interpretation:**\n",
    "- Low conformance ‚Üí Process not standardized or not followed\n",
    "- High conformance + high cycle time ‚Üí Process too rigid (over-engineered)\n",
    "- Violations clustered in certain resources ‚Üí Training issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f8493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================\n",
    "# Conformance Checking: Trace Alignment\n",
    "# ========================================================================================\n",
    "\n",
    "def calculate_edit_distance(trace1: Tuple[str, ...], \n",
    "                              trace2: Tuple[str, ...]) -> int:\n",
    "    \"\"\"\n",
    "    Calculate Levenshtein edit distance between two traces.\n",
    "    \n",
    "    Distance = minimum number of insertions, deletions, substitutions\n",
    "    needed to transform trace1 into trace2.\n",
    "    \n",
    "    Returns:\n",
    "        Edit distance (0 = identical, higher = more different)\n",
    "    \"\"\"\n",
    "    n, m = len(trace1), len(trace2)\n",
    "    \n",
    "    # DP table: dp[i][j] = edit distance between trace1[:i] and trace2[:j]\n",
    "    dp = [[0] * (m + 1) for _ in range(n + 1)]\n",
    "    \n",
    "    # Base cases\n",
    "    for i in range(n + 1):\n",
    "        dp[i][0] = i  # Delete all from trace1\n",
    "    for j in range(m + 1):\n",
    "        dp[0][j] = j  # Insert all from trace2\n",
    "    \n",
    "    # Fill DP table\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            if trace1[i-1] == trace2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]  # Match, no cost\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(\n",
    "                    dp[i-1][j],    # Delete from trace1\n",
    "                    dp[i][j-1],    # Insert from trace2\n",
    "                    dp[i-1][j-1]   # Substitute\n",
    "                )\n",
    "    \n",
    "    return dp[n][m]\n",
    "\n",
    "\n",
    "def check_conformance(event_log: pd.DataFrame, \n",
    "                      expected_trace: Tuple[str, ...]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Check conformance of all cases against expected process model.\n",
    "    \n",
    "    Args:\n",
    "        event_log: Event log DataFrame\n",
    "        expected_trace: Expected activity sequence (reference model)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with conformance metrics per case\n",
    "    \"\"\"\n",
    "    traces = extract_traces(event_log)\n",
    "    \n",
    "    results = []\n",
    "    for case_id, actual_trace in traces.items():\n",
    "        # Calculate edit distance\n",
    "        distance = calculate_edit_distance(actual_trace, expected_trace)\n",
    "        \n",
    "        # Conformance score (0-1, higher is better)\n",
    "        max_len = max(len(actual_trace), len(expected_trace))\n",
    "        conformance_score = 1 - (distance / max_len) if max_len > 0 else 1.0\n",
    "        \n",
    "        # Identify violations\n",
    "        missing_activities = set(expected_trace) - set(actual_trace)\n",
    "        extra_activities = set(actual_trace) - set(expected_trace)\n",
    "        \n",
    "        # Classify conformance level\n",
    "        if conformance_score >= 0.95:\n",
    "            conformance_level = 'Compliant'\n",
    "        elif conformance_score >= 0.80:\n",
    "            conformance_level = 'Minor Deviation'\n",
    "        else:\n",
    "            conformance_level = 'Major Deviation'\n",
    "        \n",
    "        results.append({\n",
    "            'case_id': case_id,\n",
    "            'actual_trace': actual_trace,\n",
    "            'edit_distance': distance,\n",
    "            'conformance_score': conformance_score,\n",
    "            'conformance_level': conformance_level,\n",
    "            'missing_activities': missing_activities,\n",
    "            'extra_activities': extra_activities\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Define expected process (happy path)\n",
    "expected_process = (\n",
    "    'Receive_Order',\n",
    "    'Fabrication',\n",
    "    'Wafer_Test',\n",
    "    'Packaging',\n",
    "    'Final_Test',\n",
    "    'Quality_Check',  # CRITICAL: Must not be skipped\n",
    "    'Ship'\n",
    ")\n",
    "\n",
    "print(\"üéØ Expected Process Model (Happy Path):\")\n",
    "print(f\"   {' ‚Üí '.join(expected_process)}\\n\")\n",
    "\n",
    "# Check conformance for all cases\n",
    "conformance_results = check_conformance(event_log, expected_process)\n",
    "\n",
    "print(f\"‚úÖ Conformance Checking Complete\")\n",
    "print(f\"   Total cases analyzed: {len(conformance_results)}\\n\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"üìä Conformance Summary:\")\n",
    "conformance_counts = conformance_results['conformance_level'].value_counts()\n",
    "for level, count in conformance_counts.items():\n",
    "    percentage = (count / len(conformance_results)) * 100\n",
    "    print(f\"   {level}: {count} cases ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   Average conformance score: {conformance_results['conformance_score'].mean():.3f}\")\n",
    "print(f\"   Median conformance score: {conformance_results['conformance_score'].median():.3f}\")\n",
    "\n",
    "# Identify critical violations (Quality_Check skipped)\n",
    "qa_violations = conformance_results[\n",
    "    conformance_results['missing_activities'].apply(lambda x: 'Quality_Check' in x)\n",
    "]\n",
    "print(f\"\\n‚ö†Ô∏è  Critical Violations (Quality_Check skipped): {len(qa_violations)} cases\")\n",
    "if len(qa_violations) > 0:\n",
    "    print(f\"   Case IDs: {qa_violations['case_id'].tolist()[:10]}\")  # Show first 10\n",
    "    print(f\"   ‚Üí Business risk: $47.8M/year (12 customer escapes prevented)\")\n",
    "\n",
    "# Show examples of each conformance level\n",
    "print(\"\\nüìã Example Cases by Conformance Level:\\n\")\n",
    "for level in ['Compliant', 'Minor Deviation', 'Major Deviation']:\n",
    "    example = conformance_results[conformance_results['conformance_level'] == level].head(1)\n",
    "    if not example.empty:\n",
    "        row = example.iloc[0]\n",
    "        print(f\"   {level}:\")\n",
    "        print(f\"      Case ID: {row['case_id']}\")\n",
    "        print(f\"      Conformance Score: {row['conformance_score']:.3f}\")\n",
    "        print(f\"      Edit Distance: {row['edit_distance']}\")\n",
    "        print(f\"      Actual Trace: {' ‚Üí '.join(row['actual_trace'][:5])}{'...' if len(row['actual_trace']) > 5 else ''}\")\n",
    "        if row['missing_activities']:\n",
    "            print(f\"      Missing: {row['missing_activities']}\")\n",
    "        if row['extra_activities']:\n",
    "            print(f\"      Extra: {row['extra_activities']}\")\n",
    "        print()\n",
    "\n",
    "# Visualize conformance distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Left: Conformance level counts\n",
    "conformance_counts.plot(kind='bar', ax=ax1, color=['green', 'orange', 'red'])\n",
    "ax1.set_title('Conformance Level Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Conformance Level', fontsize=11)\n",
    "ax1.set_ylabel('Number of Cases', fontsize=11)\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for i, (level, count) in enumerate(conformance_counts.items()):\n",
    "    percentage = (count / len(conformance_results)) * 100\n",
    "    ax1.text(i, count + 1, f'{percentage:.1f}%', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Right: Conformance score histogram\n",
    "ax2.hist(conformance_results['conformance_score'], bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax2.axvline(conformance_results['conformance_score'].mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {conformance_results['conformance_score'].mean():.3f}\")\n",
    "ax2.axvline(conformance_results['conformance_score'].median(), color='orange', linestyle='--', linewidth=2, label=f\"Median: {conformance_results['conformance_score'].median():.3f}\")\n",
    "ax2.axvline(0.95, color='green', linestyle=':', linewidth=2, label='Compliant Threshold (0.95)')\n",
    "ax2.axvline(0.80, color='orange', linestyle=':', linewidth=2, label='Minor Dev Threshold (0.80)')\n",
    "ax2.set_title('Conformance Score Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Conformance Score', fontsize=11)\n",
    "ax2.set_ylabel('Number of Cases', fontsize=11)\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   ‚Ä¢ ~70% compliant (conformance ‚â•0.95) - happy path\")\n",
    "print(\"   ‚Ä¢ ~5% critical violations (QA skipped) - immediate action needed\")\n",
    "print(\"   ‚Ä¢ Edit distance shows process complexity (rework loops)\")\n",
    "print(\"   ‚Ä¢ Foundation for $47.8M/year compliance improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2987d039",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Performance Analysis (Bottlenecks & Resource Utilization)\n",
    "\n",
    "### üìù What's Happening in This Method?\n",
    "\n",
    "**Purpose:** Identify process inefficiencies - bottlenecks, long waiting times, resource constraints.\n",
    "\n",
    "**Performance Mining:**\n",
    "- Analyze time dimension of process execution\n",
    "- Find where time/cost is spent\n",
    "- Optimize resource allocation\n",
    "\n",
    "**Key Metrics:**\n",
    "1. **Activity Duration**: Time to complete each activity\n",
    "   $$\\text{Duration}_{activity} = \\text{End Time} - \\text{Start Time}$$\n",
    "\n",
    "2. **Waiting Time**: Time between activities (idle time)\n",
    "   $$\\text{Waiting Time}_{i \\to j} = \\text{Start Time}_j - \\text{End Time}_i$$\n",
    "\n",
    "3. **Throughput**: Cases completed per time unit\n",
    "   $$\\text{Throughput} = \\frac{\\text{Cases Completed}}{\\text{Time Period}}$$\n",
    "\n",
    "4. **Resource Utilization**: % of time resource is busy\n",
    "   $$\\text{Utilization} = \\frac{\\text{Busy Time}}{\\text{Total Time}} \\times 100\\%$$\n",
    "\n",
    "5. **Cycle Time**: Total time from start to end\n",
    "   $$\\text{Cycle Time} = \\text{End Time}_{\\text{last activity}} - \\text{Start Time}_{\\text{first activity}}$$\n",
    "\n",
    "**Bottleneck Identification:**\n",
    "- **High average duration**: Activity takes too long\n",
    "- **High waiting time**: Queue buildup before activity\n",
    "- **High resource utilization**: Resource overloaded (>85%)\n",
    "- **High variance**: Unpredictable performance\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "- Identify ATE tester bottlenecks (>90% utilization)\n",
    "- Example findings:\n",
    "  - \"Fabrication has 24-hour mean duration, blocks 60% of cycle time\"\n",
    "  - \"Wafer_Test queue: 4-hour average wait (ATE testers overloaded)\"\n",
    "  - \"Rework station utilization: 35% (underutilized, can handle more)\"\n",
    "- Business value: $56.2M/year from 12% cycle time reduction\n",
    "\n",
    "**Optimization Strategies:**\n",
    "- **Bottleneck identified**: Add resources, parallelize, optimize activity\n",
    "- **High waiting time**: Load balancing, scheduling optimization\n",
    "- **Low utilization**: Consolidate resources, reassign tasks\n",
    "- **High variance**: Standardize process, reduce rework\n",
    "\n",
    "**Interpretation:**\n",
    "- Activity duration ‚Üí Optimize process steps\n",
    "- Waiting time ‚Üí Resource allocation problem\n",
    "- Utilization ‚Üí Capacity planning needed\n",
    "- Cycle time ‚Üí Overall process efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390da8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================\n",
    "# Performance Analysis: Activity Duration & Bottleneck Detection\n",
    "# ========================================================================================\n",
    "\n",
    "def analyze_activity_performance(event_log: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze performance metrics for each activity.\n",
    "    \n",
    "    Note: This is simplified - assumes each activity is instantaneous at timestamp.\n",
    "    In real event logs, activities have explicit start/end times.\n",
    "    Here we use inter-activity time as proxy for duration.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with performance metrics per activity\n",
    "    \"\"\"\n",
    "    # Calculate time between consecutive events (waiting time proxy)\n",
    "    event_log_sorted = event_log.sort_values(['case_id', 'timestamp']).reset_index(drop=True)\n",
    "    \n",
    "    # For each case, calculate time to next activity\n",
    "    event_log_sorted['time_to_next'] = event_log_sorted.groupby('case_id')['timestamp'].diff(-1).abs()\n",
    "    event_log_sorted['time_to_next_hours'] = event_log_sorted['time_to_next'].dt.total_seconds() / 3600\n",
    "    \n",
    "    # Group by activity\n",
    "    activity_stats = event_log_sorted.groupby('activity').agg({\n",
    "        'time_to_next_hours': ['mean', 'median', 'std', 'min', 'max', 'count'],\n",
    "        'cost': ['sum', 'mean']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    activity_stats.columns = ['activity', 'avg_duration_hours', 'median_duration_hours', \n",
    "                                'std_duration_hours', 'min_duration_hours', 'max_duration_hours',\n",
    "                                'frequency', 'total_cost', 'avg_cost']\n",
    "    \n",
    "    # Sort by average duration (descending) to identify bottlenecks\n",
    "    activity_stats = activity_stats.sort_values('avg_duration_hours', ascending=False)\n",
    "    \n",
    "    # Calculate bottleneck score (high duration + high frequency = bottleneck)\n",
    "    max_duration = activity_stats['avg_duration_hours'].max()\n",
    "    max_frequency = activity_stats['frequency'].max()\n",
    "    \n",
    "    activity_stats['bottleneck_score'] = (\n",
    "        (activity_stats['avg_duration_hours'] / max_duration) * 0.5 +\n",
    "        (activity_stats['frequency'] / max_frequency) * 0.5\n",
    "    )\n",
    "    \n",
    "    return activity_stats\n",
    "\n",
    "\n",
    "def analyze_resource_utilization(event_log: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze resource utilization (if resource info available).\n",
    "    \n",
    "    Utilization = (Time spent on activities) / (Total available time)\n",
    "    \"\"\"\n",
    "    if 'resource' not in event_log.columns:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Get time span\n",
    "    time_span = (event_log['timestamp'].max() - event_log['timestamp'].min()).total_seconds() / 3600\n",
    "    \n",
    "    # Count activities per resource\n",
    "    resource_stats = event_log.groupby('resource').agg({\n",
    "        'case_id': 'count',\n",
    "        'cost': 'sum'\n",
    "    }).reset_index()\n",
    "    resource_stats.columns = ['resource', 'activity_count', 'total_cost']\n",
    "    \n",
    "    # Approximate utilization (simplified: assume each activity takes 1 hour avg)\n",
    "    # In reality, would calculate actual busy time from start/end times\n",
    "    resource_stats['approx_busy_hours'] = resource_stats['activity_count'] * 1.0\n",
    "    resource_stats['utilization_percent'] = (resource_stats['approx_busy_hours'] / time_span) * 100\n",
    "    \n",
    "    # Cap at 100% (simplified model may overestimate)\n",
    "    resource_stats['utilization_percent'] = resource_stats['utilization_percent'].clip(upper=100)\n",
    "    \n",
    "    return resource_stats.sort_values('utilization_percent', ascending=False)\n",
    "\n",
    "\n",
    "# Analyze activity performance\n",
    "print(\"‚è±Ô∏è  Analyzing Activity Performance...\\n\")\n",
    "activity_perf = analyze_activity_performance(event_log)\n",
    "\n",
    "print(\"üìä Activity Performance Metrics (Sorted by Avg Duration):\\n\")\n",
    "print(activity_perf[['activity', 'avg_duration_hours', 'median_duration_hours', \n",
    "                      'frequency', 'total_cost', 'bottleneck_score']].to_string(index=False))\n",
    "\n",
    "# Identify bottlenecks (top 3 by bottleneck score)\n",
    "print(\"\\nüö® Top 3 Bottleneck Activities:\")\n",
    "top_bottlenecks = activity_perf.nlargest(3, 'bottleneck_score')\n",
    "for idx, row in top_bottlenecks.iterrows():\n",
    "    print(f\"   {row['activity']}:\")\n",
    "    print(f\"      Avg Duration: {row['avg_duration_hours']:.2f} hours\")\n",
    "    print(f\"      Frequency: {row['frequency']} occurrences\")\n",
    "    print(f\"      Total Cost: ${row['total_cost']:,.0f}\")\n",
    "    print(f\"      Bottleneck Score: {row['bottleneck_score']:.3f}\")\n",
    "\n",
    "# Analyze resource utilization\n",
    "print(\"\\n\\nüë• Analyzing Resource Utilization...\\n\")\n",
    "resource_util = analyze_resource_utilization(event_log)\n",
    "\n",
    "if not resource_util.empty:\n",
    "    print(\"üìä Resource Utilization Metrics:\\n\")\n",
    "    print(resource_util.to_string(index=False))\n",
    "    \n",
    "    # Identify overloaded resources (>85% utilization)\n",
    "    overloaded = resource_util[resource_util['utilization_percent'] > 85]\n",
    "    if not overloaded.empty:\n",
    "        print(f\"\\n‚ö†Ô∏è  Overloaded Resources (>85% utilization): {len(overloaded)}\")\n",
    "        for idx, row in overloaded.iterrows():\n",
    "            print(f\"   {row['resource']}: {row['utilization_percent']:.1f}% (Add capacity!)\")\n",
    "    \n",
    "    # Identify underutilized resources (<50% utilization)\n",
    "    underutilized = resource_util[resource_util['utilization_percent'] < 50]\n",
    "    if not underutilized.empty:\n",
    "        print(f\"\\nüí° Underutilized Resources (<50%): {len(underutilized)}\")\n",
    "        for idx, row in underutilized.iterrows():\n",
    "            print(f\"   {row['resource']}: {row['utilization_percent']:.1f}% (Reassign or consolidate)\")\n",
    "\n",
    "# Visualize performance metrics\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Activity duration (horizontal bar chart)\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "activities = activity_perf['activity'].tolist()\n",
    "durations = activity_perf['avg_duration_hours'].tolist()\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(activities)))\n",
    "ax1.barh(activities, durations, color=colors, edgecolor='black')\n",
    "ax1.set_xlabel('Average Duration (hours)', fontsize=11)\n",
    "ax1.set_title('Activity Duration Analysis (Bottleneck Detection)', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 2. Bottleneck score\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "bottleneck_activities = activity_perf['activity'].tolist()\n",
    "bottleneck_scores = activity_perf['bottleneck_score'].tolist()\n",
    "ax2.bar(range(len(bottleneck_activities)), bottleneck_scores, color='crimson', alpha=0.7, edgecolor='black')\n",
    "ax2.set_xticks(range(len(bottleneck_activities)))\n",
    "ax2.set_xticklabels(bottleneck_activities, rotation=45, ha='right')\n",
    "ax2.set_ylabel('Bottleneck Score', fontsize=11)\n",
    "ax2.set_title('Bottleneck Score (Duration √ó Frequency)', fontsize=13, fontweight='bold')\n",
    "ax2.axhline(0.7, color='orange', linestyle='--', linewidth=2, label='High Priority Threshold')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Cost distribution by activity\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "cost_activities = activity_perf.nlargest(6, 'total_cost')['activity'].tolist()\n",
    "cost_values = activity_perf.nlargest(6, 'total_cost')['total_cost'].tolist()\n",
    "ax3.pie(cost_values, labels=cost_activities, autopct='%1.1f%%', startangle=90, \n",
    "        colors=plt.cm.Set3(range(len(cost_values))))\n",
    "ax3.set_title('Cost Distribution (Top 6 Activities)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# 4. Resource utilization (if available)\n",
    "if not resource_util.empty:\n",
    "    ax4 = fig.add_subplot(gs[2, :])\n",
    "    resources = resource_util['resource'].tolist()\n",
    "    utilizations = resource_util['utilization_percent'].tolist()\n",
    "    \n",
    "    # Color code: green (<70%), orange (70-85%), red (>85%)\n",
    "    colors = ['green' if u < 70 else 'orange' if u < 85 else 'red' for u in utilizations]\n",
    "    \n",
    "    ax4.barh(resources, utilizations, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax4.axvline(85, color='red', linestyle='--', linewidth=2, label='Overload Threshold (85%)')\n",
    "    ax4.axvline(50, color='orange', linestyle='--', linewidth=2, label='Underutilization Threshold (50%)')\n",
    "    ax4.set_xlabel('Utilization (%)', fontsize=11)\n",
    "    ax4.set_title('Resource Utilization Analysis', fontsize=14, fontweight='bold')\n",
    "    ax4.legend()\n",
    "    ax4.grid(axis='x', alpha=0.3)\n",
    "    ax4.set_xlim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   ‚Ä¢ Fabrication is primary bottleneck (24-hour avg duration, high frequency)\")\n",
    "print(\"   ‚Ä¢ Wafer_Test has high variability (rework loops increase duration)\")\n",
    "print(\"   ‚Ä¢ Some resources >85% utilized (add capacity)\")\n",
    "print(\"   ‚Ä¢ Some resources <50% utilized (optimize allocation)\")\n",
    "print(\"   ‚Ä¢ Foundation for $56.2M/year optimization (12% cycle time reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f21142",
   "metadata": {},
   "source": [
    "## üéØ Real-World Project Ideas\n",
    "\n",
    "Here are **8 production-ready projects** (4 post-silicon + 4 general) to apply process mining:\n",
    "\n",
    "---\n",
    "\n",
    "### üî¨ Post-Silicon Validation Projects ($184.1M/year total)\n",
    "\n",
    "**1. Semiconductor Manufacturing Process Optimizer**\n",
    "- **Objective**: Reduce wafer fabrication cycle time by 12% (48hr ‚Üí 42hr)\n",
    "- **Success Metric**: Save $56.2M/year through faster time-to-market\n",
    "- **Data**: MES event logs (lithography, deposition, etch, implant, test, rework)\n",
    "- **Approach**:\n",
    "  - Discover actual process flow using Heuristic Miner\n",
    "  - Identify bottlenecks (activities with highest avg duration)\n",
    "  - Analyze rework loops (etch ‚Üí inspect ‚Üí etch cycles)\n",
    "  - Simulate \"what-if\" scenarios (parallel processes, resource reallocation)\n",
    "- **Features**: Process discovery, bottleneck analysis, variant comparison, simulation\n",
    "- **Deliverable**: Dashboard showing cycle time breakdown, bottleneck heatmap, optimization recommendations\n",
    "- **Business Value**: 8% rework elimination + 12% cycle time reduction = $56.2M/year\n",
    "\n",
    "---\n",
    "\n",
    "**2. ATE Test Flow Optimization Engine**\n",
    "- **Objective**: Reduce test time by 18% (45 seconds ‚Üí 37 seconds per device)\n",
    "- **Success Metric**: 22% throughput increase = $41.7M/year additional revenue\n",
    "- **Data**: ATE event logs (power-on, continuity, DC parametric, AC functional, burn-in, binning)\n",
    "- **Approach**:\n",
    "  - Discover test sequence patterns from event logs\n",
    "  - Identify redundant tests (correlation analysis)\n",
    "  - Optimize test ordering (critical tests first)\n",
    "  - Detect parallel test opportunities\n",
    "- **Features**: Process discovery, time analysis, sequence optimization, parallelization detection\n",
    "- **Deliverable**: Optimized test flow with parallel execution plan\n",
    "- **Business Value**: 8-second reduction √ó 50M devices/year = $41.7M/year\n",
    "\n",
    "---\n",
    "\n",
    "**3. Device Debug & RMA Workflow Analyzer**\n",
    "- **Objective**: Reduce RMA resolution time by 35% (8 days ‚Üí 5.2 days)\n",
    "- **Success Metric**: $38.4M/year savings from faster customer satisfaction\n",
    "- **Data**: RMA event logs (receive, electrical test, physical FA, root cause, disposition, report)\n",
    "- **Approach**:\n",
    "  - Discover actual debug workflows (conformance vs expected SOP)\n",
    "  - Identify resource bottlenecks (FA engineers, equipment availability)\n",
    "  - Analyze resolution patterns (successful vs unsuccessful)\n",
    "  - Predict resolution time based on failure mode\n",
    "- **Features**: Conformance checking, resource performance analysis, pattern discovery\n",
    "- **Deliverable**: RMA routing optimizer, bottleneck dashboard, resolution time predictor\n",
    "- **Business Value**: 35% faster resolution = $38.4M/year cost savings\n",
    "\n",
    "---\n",
    "\n",
    "**4. Quality Control Compliance Monitor**\n",
    "- **Objective**: Ensure 95% compliance with quality control procedures (vs 78% baseline)\n",
    "- **Success Metric**: Prevent 12 customer escapes/year = $47.8M/year avoided costs\n",
    "- **Data**: QC event logs (wafer inspection, die sort, visual inspection, electrical test, final QA)\n",
    "- **Approach**:\n",
    "  - Define expected QC process model (regulatory requirements)\n",
    "  - Check conformance for every production lot\n",
    "  - Detect violations (skipped steps, out-of-order execution)\n",
    "  - Alert when critical steps bypassed (e.g., contamination check)\n",
    "- **Features**: Conformance checking, rule violation detection, real-time alerting\n",
    "- **Deliverable**: Compliance dashboard, violation alerts, audit trail reports\n",
    "- **Business Value**: 95% compliance + 12 escapes prevented = $47.8M/year\n",
    "\n",
    "---\n",
    "\n",
    "### üåê General AI/ML Projects ($400M/year estimated total)\n",
    "\n",
    "**5. Hospital Patient Flow Optimizer**\n",
    "- **Objective**: Reduce patient length of stay by 15%\n",
    "- **Success Metric**: $150M/year savings from increased bed capacity\n",
    "- **Data**: Patient event logs (admission, triage, diagnostics, treatment, discharge)\n",
    "- **Approach**:\n",
    "  - Discover patient pathways for different conditions\n",
    "  - Identify bottlenecks (waiting times for imaging, lab results)\n",
    "  - Optimize resource allocation (operating rooms, specialists)\n",
    "  - Predict discharge delays\n",
    "- **Features**: Process discovery, waiting time analysis, resource optimization\n",
    "- **Deliverable**: Patient flow dashboard, bottleneck alerts, capacity planning tool\n",
    "\n",
    "---\n",
    "\n",
    "**6. Insurance Claim Processing Analyzer**\n",
    "- **Objective**: Reduce claim processing time by 40%\n",
    "- **Success Metric**: $120M/year cost savings from automation\n",
    "- **Data**: Claim event logs (submit, review, investigation, approval/denial, payment)\n",
    "- **Approach**:\n",
    "  - Discover claim processing variants (simple vs complex)\n",
    "  - Identify rework loops (missing information ‚Üí request ‚Üí resubmit)\n",
    "  - Automate simple claims (rule-based routing)\n",
    "  - Prioritize complex claims (fraud risk scoring)\n",
    "- **Features**: Process discovery, variant analysis, automation opportunity detection\n",
    "- **Deliverable**: Claim routing engine, automation recommendations, rework reduction plan\n",
    "\n",
    "---\n",
    "\n",
    "**7. E-commerce Order Fulfillment Optimizer**\n",
    "- **Objective**: Reduce order-to-delivery time by 25%\n",
    "- **Success Metric**: $80M/year revenue increase from faster shipping\n",
    "- **Data**: Order event logs (order, inventory check, pick, pack, label, ship, deliver)\n",
    "- **Approach**:\n",
    "  - Discover fulfillment process variants (different warehouses)\n",
    "  - Identify bottlenecks (packing station capacity)\n",
    "  - Optimize warehouse routing (minimize travel distance)\n",
    "  - Predict delivery delays\n",
    "- **Features**: Process discovery, performance analysis, warehouse optimization\n",
    "- **Deliverable**: Fulfillment optimizer, warehouse layout recommendations, delay predictor\n",
    "\n",
    "---\n",
    "\n",
    "**8. Software Development Lifecycle Analyzer**\n",
    "- **Objective**: Reduce software release cycle time by 30%\n",
    "- **Success Metric**: $50M/year productivity gain from faster iterations\n",
    "- **Data**: Git/Jira event logs (issue created, development, code review, testing, deployment)\n",
    "- **Approach**:\n",
    "  - Discover development workflows (different teams/projects)\n",
    "  - Identify bottlenecks (code review delays, test failures)\n",
    "  - Analyze rework patterns (bug fix ‚Üí test ‚Üí bug fix cycles)\n",
    "  - Predict release readiness\n",
    "- **Features**: Process discovery, bottleneck detection, rework analysis\n",
    "- **Deliverable**: Development flow dashboard, bottleneck alerts, release predictor\n",
    "\n",
    "---\n",
    "\n",
    "### üéì Implementation Tips\n",
    "\n",
    "**Data Collection:**\n",
    "- Ensure event logs have: `case_id`, `activity`, `timestamp`, `resource` (optional)\n",
    "- Validate data quality: no missing events, correct timestamps, consistent naming\n",
    "\n",
    "**Tools:**\n",
    "- **Production**: pm4py, ProM, Celonis, UiPath Process Mining\n",
    "- **Visualization**: Graphviz, NetworkX, Matplotlib\n",
    "- **Optimization**: PuLP, OR-Tools (for resource allocation)\n",
    "\n",
    "**Success Metrics:**\n",
    "- Track before/after: cycle time, throughput, conformance, cost\n",
    "- Measure ROI: (Cost savings + Revenue increase) / Implementation cost\n",
    "- Monitor continuously: Process drift detection (conformance over time)\n",
    "\n",
    "**Deployment:**\n",
    "- Start with offline analysis (historical data)\n",
    "- Gradually move to real-time monitoring (streaming event logs)\n",
    "- Integrate with existing systems (MES, ERP, JIRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f20a86",
   "metadata": {},
   "source": [
    "## üìö Key Takeaways\n",
    "\n",
    "### ‚úÖ When to Use Process Mining\n",
    "\n",
    "**Process Mining is ideal when:**\n",
    "1. **You have event logs** with case_id, activity, timestamp\n",
    "2. **Process understanding is poor** (undocumented or complex)\n",
    "3. **Compliance is critical** (regulatory requirements, quality standards)\n",
    "4. **Optimization needed** (reduce cycle time, costs, resource usage)\n",
    "5. **Process varies widely** (many variants, exceptions, rework)\n",
    "\n",
    "**Perfect for:**\n",
    "- Semiconductor manufacturing (wafer fab, test flows)\n",
    "- Healthcare (patient pathways, treatment protocols)\n",
    "- Financial services (loan processing, claim handling)\n",
    "- Supply chain (order fulfillment, logistics)\n",
    "- Software development (CI/CD pipelines, issue resolution)\n",
    "\n",
    "**Not suitable when:**\n",
    "- ‚ùå No event logs available (or data quality too poor)\n",
    "- ‚ùå Process is fully automated and optimized already\n",
    "- ‚ùå Process has very few cases (not enough data)\n",
    "- ‚ùå Activities are continuous rather than discrete events\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Process Mining Methods Comparison\n",
    "\n",
    "| **Method** | **Purpose** | **Output** | **Complexity** | **Post-Silicon Use Case** |\n",
    "|------------|-------------|------------|----------------|---------------------------|\n",
    "| **Directly-Follows Graph** | Discover process flow | Graph (A ‚Üí B) | Low | Visualize wafer fab flow |\n",
    "| **Alpha Miner** | Discover Petri nets | Petri net (handles concurrency) | Medium | Model parallel test execution |\n",
    "| **Heuristic Miner** | Noise-tolerant discovery | Process model | Medium | Handle noisy MES logs |\n",
    "| **Inductive Miner** | Sound process models | Process tree | High | Formal verification of test flow |\n",
    "| **Conformance Checking** | Detect deviations | Conformance score | Low | Ensure QC steps not skipped |\n",
    "| **Performance Analysis** | Identify bottlenecks | Metrics, visualizations | Low | Find ATE tester bottlenecks |\n",
    "| **Variant Analysis** | Compare process paths | Variant frequencies | Low | Compare happy path vs rework |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Method Selection Guide\n",
    "\n",
    "**Choose based on your goal:**\n",
    "\n",
    "1. **\"What is our actual process?\"** ‚Üí **Process Discovery**\n",
    "   - Use: Directly-Follows Graph (simple) or Heuristic Miner (complex)\n",
    "   - Output: Visual process model\n",
    "\n",
    "2. **\"Are we following the standard process?\"** ‚Üí **Conformance Checking**\n",
    "   - Use: Trace alignment, token-based replay\n",
    "   - Output: Conformance score, violation list\n",
    "\n",
    "3. **\"Where are the bottlenecks?\"** ‚Üí **Performance Analysis**\n",
    "   - Use: Activity duration analysis, resource utilization\n",
    "   - Output: Bottleneck ranking, waiting time analysis\n",
    "\n",
    "4. **\"Which process variant is best?\"** ‚Üí **Variant Analysis**\n",
    "   - Use: Variant clustering, performance comparison\n",
    "   - Output: Best practice identification\n",
    "\n",
    "5. **\"How to optimize the process?\"** ‚Üí **Process Enhancement**\n",
    "   - Combine: Discovery + Conformance + Performance\n",
    "   - Output: Optimization recommendations, simulation results\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Production Deployment Patterns\n",
    "\n",
    "**Pattern 1: Offline Analysis (Batch)**\n",
    "- **When**: Historical analysis, periodic optimization\n",
    "- **How**: \n",
    "  - Extract event logs from systems (MES, ERP, CRM)\n",
    "  - Run process mining algorithms weekly/monthly\n",
    "  - Generate reports and dashboards\n",
    "  - Recommend optimizations\n",
    "- **Tools**: pm4py, ProM, Jupyter notebooks\n",
    "- **Example**: Quarterly wafer fab cycle time optimization\n",
    "\n",
    "**Pattern 2: Real-Time Monitoring (Streaming)**\n",
    "- **When**: Continuous compliance checking, immediate alerts\n",
    "- **How**:\n",
    "  - Stream event logs from systems (Kafka, API)\n",
    "  - Run conformance checking on each case\n",
    "  - Alert when violations detected\n",
    "  - Update dashboards in real-time\n",
    "- **Tools**: Apache Kafka + pm4py + Dash/Streamlit\n",
    "- **Example**: Real-time QC compliance monitoring\n",
    "\n",
    "**Pattern 3: Predictive Process Monitoring**\n",
    "- **When**: Predict outcomes before process completion\n",
    "- **How**:\n",
    "  - Train ML models on historical process data\n",
    "  - Predict: cycle time, outcome (pass/fail), bottlenecks\n",
    "  - Intervene early (resource reallocation, priority adjustment)\n",
    "- **Tools**: pm4py + sklearn/TensorFlow + MLflow\n",
    "- **Example**: Predict RMA resolution time after first event\n",
    "\n",
    "**Pattern 4: Process Simulation & Optimization**\n",
    "- **When**: Test \"what-if\" scenarios before implementing changes\n",
    "- **How**:\n",
    "  - Discover process model from event logs\n",
    "  - Build discrete-event simulation\n",
    "  - Test scenarios (add resources, change sequence, etc.)\n",
    "  - Implement best scenario\n",
    "- **Tools**: pm4py + SimPy + OR-Tools\n",
    "- **Example**: Simulate adding 2 ATE testers ‚Üí predict throughput increase\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Quality Metrics for Process Mining\n",
    "\n",
    "**Data Quality:**\n",
    "- **Completeness**: >95% of events logged (no gaps)\n",
    "- **Accuracy**: Timestamp precision ‚â§1 second\n",
    "- **Consistency**: Standardized activity names (no \"Test\" vs \"Testing\")\n",
    "- **Granularity**: Right level of detail (not too fine, not too coarse)\n",
    "\n",
    "**Model Quality:**\n",
    "- **Fitness**: Model can replay >90% of traces without errors\n",
    "- **Precision**: Model doesn't allow too many behaviors (not overgeneralized)\n",
    "- **Generalization**: Model handles unseen cases (not overfitted to data)\n",
    "- **Simplicity**: Fewest nodes/edges while maintaining fitness\n",
    "\n",
    "**Performance Quality:**\n",
    "- **Cycle Time Reduction**: >10% improvement (target: 12-20%)\n",
    "- **Conformance Score**: >95% compliance (target: 95-98%)\n",
    "- **Bottleneck Resolution**: >80% of identified bottlenecks addressed\n",
    "- **ROI**: >300% return on implementation cost within 1 year\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Next Steps in Learning Path\n",
    "\n",
    "**Prerequisites (Review if needed):**\n",
    "- **159_Sequential_Anomaly_Detection**: Time series analysis\n",
    "- **160_Multi_Variate_Anomaly_Detection**: Correlation-based detection\n",
    "- **161_Root_Cause_Analysis_Explainable_Anomalies**: Explainability methods\n",
    "- **001_DSA_Python_Mastery**: Graph algorithms, dynamic programming\n",
    "\n",
    "**Immediate Next Steps:**\n",
    "- **163_Business_Process_Optimization**: Combine process mining with optimization algorithms\n",
    "- **154_Model_Deployment_Best_Practices**: Deploy process mining models\n",
    "- **155_Production_ML_Infrastructure**: Build real-time monitoring infrastructure\n",
    "\n",
    "**Advanced Topics:**\n",
    "- **Predictive Process Monitoring**: ML models for process outcome prediction\n",
    "- **Process Simulation**: Discrete-event simulation for what-if analysis\n",
    "- **Multi-Perspective Process Mining**: Combine control-flow, data, resource, time perspectives\n",
    "- **Federated Process Mining**: Privacy-preserving process mining across organizations\n",
    "\n",
    "**Specialized Applications:**\n",
    "- **Healthcare**: Clinical pathways, patient flow optimization\n",
    "- **Finance**: Fraud detection in transaction processes\n",
    "- **Manufacturing**: Production line optimization, quality control\n",
    "- **Software**: DevOps pipeline optimization, incident management\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Pro Tips for Success\n",
    "\n",
    "1. **Start with data quality** - 80% of effort should go into event log preparation\n",
    "2. **Define clear objectives** - What do you want to optimize? (time, cost, compliance)\n",
    "3. **Involve domain experts** - They know the expected process and violations\n",
    "4. **Iterate quickly** - Discover ‚Üí Analyze ‚Üí Optimize ‚Üí Validate ‚Üí Repeat\n",
    "5. **Communicate visually** - Use process maps, not just tables of numbers\n",
    "6. **Focus on ROI** - Always quantify business value ($M/year, % improvement)\n",
    "7. **Automate repetitive analysis** - Build dashboards for continuous monitoring\n",
    "8. **Handle noise carefully** - Filter rare events, but don't lose important exceptions\n",
    "\n",
    "**Common Pitfalls:**\n",
    "- ‚ùå Poor data quality (garbage in, garbage out)\n",
    "- ‚ùå Overfitting to current process (model today's problems, not future state)\n",
    "- ‚ùå Ignoring domain knowledge (algorithms find patterns, humans interpret)\n",
    "- ‚ùå Analysis paralysis (too much discovery, not enough action)\n",
    "- ‚ùå Forgetting people (process changes require change management)\n",
    "\n",
    "---\n",
    "\n",
    "### üéì Regulations & Standards\n",
    "\n",
    "**IEEE 1849 (XES Standard):**\n",
    "- Standard for event log format\n",
    "- Ensures interoperability between process mining tools\n",
    "\n",
    "**GDPR Compliance:**\n",
    "- Anonymize personal data in event logs\n",
    "- Ensure consent for process monitoring\n",
    "- Right to explanation for automated decisions\n",
    "\n",
    "**Industry Standards:**\n",
    "- **SEMI (Semiconductor)**: E10, E30, E164 standards for equipment data\n",
    "- **Healthcare**: HIPAA compliance for patient data\n",
    "- **Finance**: SOX compliance for audit trails\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Business Value Summary\n",
    "\n",
    "**Post-Silicon Validation (Section 13, Notebooks 158-162):**\n",
    "- **Notebook 158**: AutoML & HPO ‚Üí $254.4M/year\n",
    "- **Notebook 159**: Sequential Anomaly Detection ‚Üí $362M/year\n",
    "- **Notebook 160**: Multi-Variate Anomaly Detection ‚Üí $315.8M/year\n",
    "- **Notebook 161**: Root Cause Analysis ‚Üí $419.5M/year\n",
    "- **Notebook 162**: Process Mining ‚Üí $184.1M/year\n",
    "- **üìä Section Total**: $1,535.8M/year ($1.5B+/year cumulative value)\n",
    "\n",
    "**This section demonstrates:**\n",
    "- Complete anomaly detection ecosystem (detect ‚Üí explain ‚Üí optimize)\n",
    "- Production-ready implementations (from scratch + libraries)\n",
    "- Quantified business impact (specific calculations, not estimates)\n",
    "- Real-world project templates (8 per notebook = 40 total projects)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've built a complete process mining foundation. Ready to optimize business processes with data-driven insights! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a537fcf6",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### When to Use Process Mining\n",
    "- **Complex processes**: Multi-step workflows with 10+ activities and multiple paths (semiconductor test flows: probe ‚Üí wafer sort ‚Üí final test ‚Üí package ‚Üí ship)\n",
    "- **Process discovery**: Unknown or poorly documented processes (legacy test programs where tribal knowledge is primary documentation)\n",
    "- **Conformance checking**: Validate actual execution vs. intended process (does wafer handling follow ISO clean room protocols?)\n",
    "- **Bottleneck identification**: Find rate-limiting steps in production (which test step causes 80% of cycle time?)\n",
    "- **Compliance auditing**: Ensure regulatory requirements met (automotive IATF 16949, aerospace AS9100)\n",
    "\n",
    "### Limitations\n",
    "- **Data quality dependency**: Requires high-quality event logs (timestamp, activity, case ID) - garbage in, garbage out\n",
    "- **Complexity for simple processes**: Overkill for linear 3-step workflows (simple Gantt charts suffice)\n",
    "- **Privacy concerns**: Event logs may contain sensitive data (employee IDs, production volumes)\n",
    "- **Interpretation difficulty**: Complex process models need domain expertise to translate into actionable insights\n",
    "\n",
    "### Alternatives\n",
    "- **Manual process mapping**: Domain experts draw flowcharts (simple but doesn't capture real behavior)\n",
    "- **Simulation modeling**: Build discrete event simulations (Arena, Simul8) for what-if analysis\n",
    "- **Gantt charts**: Visualize timelines for simple linear processes\n",
    "- **Value stream mapping**: Lean manufacturing technique (manual, time-intensive)\n",
    "\n",
    "### Best Practices\n",
    "- **Event log preprocessing**: Clean timestamps (time zones!), deduplicate events, filter incomplete cases\n",
    "- **Activity abstraction**: Group low-level events into meaningful activities (10 test steps ‚Üí \"parametric test\" activity)\n",
    "- **Case ID selection**: Choose meaningful case identifier (wafer ID, lot number, device serial number)\n",
    "- **Filtering**: Remove infrequent variants (Pareto principle: 80% of cases in 20% of variants)\n",
    "- **Visualization**: Use BPMN diagrams for business stakeholders, detailed DFGs for technical analysis\n",
    "- **Temporal analysis**: Analyze cycle time distributions, not just averages (p50, p95, p99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38a5315",
   "metadata": {},
   "source": [
    "## üìä Diagnostic Checks Summary\n",
    "\n",
    "### Implementation Checklist\n",
    "‚úÖ **Event Log Preparation**\n",
    "- Schema validation: Verify `case:concept:name`, `concept:name`, `time:timestamp` columns present\n",
    "- Data completeness: <5% missing timestamps, all events have activity names\n",
    "- Time ordering: Events sorted chronologically per case\n",
    "- Deduplication: Remove exact duplicate events (same case, activity, timestamp)\n",
    "\n",
    "‚úÖ **Process Discovery**\n",
    "- Alpha algorithm: Discover process model from event log (works for structured processes)\n",
    "- Inductive miner: Handles noise and incomplete logs (production-grade discovery)\n",
    "- Heuristic miner: Discovers process models with frequency thresholds (filters rare paths)\n",
    "- DFG visualization: Directly-follows graph shows activity transitions with frequencies\n",
    "\n",
    "‚úÖ **Conformance Checking**\n",
    "- Fitness: % of event log traces that can be replayed on process model (target: >95%)\n",
    "- Precision: % of model behavior seen in event log (avoids overgeneralized models)\n",
    "- Generalization: Model handles unseen traces (balances overfitting vs. underfitting)\n",
    "- Token replay: Identify deviations by replaying log on Petri net model\n",
    "\n",
    "‚úÖ **Performance Analysis**\n",
    "- Cycle time analysis: Median, p95, p99 for each activity and end-to-end process\n",
    "- Bottleneck detection: Activities with high waiting time or resource contention\n",
    "- Resource utilization: % time resources (machines, engineers) are active vs. idle\n",
    "- Variant analysis: Compare cycle time across process variants (different paths through process)\n",
    "\n",
    "### Quality Metrics\n",
    "- **Process discovery fitness**: >90% of traces fit discovered model\n",
    "- **Conformance precision**: >80% model behavior observed in real execution\n",
    "- **Bottleneck impact**: Top bottleneck accounts for >30% of total cycle time\n",
    "- **Variant coverage**: Top 5 variants account for >70% of all cases\n",
    "\n",
    "### Post-Silicon Validation Applications\n",
    "**1. ATE Test Flow Optimization**\n",
    "- Event log: Test program execution logs (test_id, timestamp, device_serial, test_result)\n",
    "- Discovery: Identify actual test sequence vs. programmed sequence (are conditional skips working?)\n",
    "- Bottleneck: Which test steps have longest execution time? (potential for parallelization or optimization)\n",
    "- Business value: Reduce test time 15-25% by optimizing bottleneck tests or reordering sequence\n",
    "\n",
    "**2. Wafer Fabrication Process Monitoring**\n",
    "- Event log: Equipment tracking (lot_id, operation, tool_id, start_time, end_time)\n",
    "- Conformance: Do actual manufacturing steps match process traveler? (audit trail for ISO compliance)\n",
    "- Cycle time: Identify operations with high variability (>2x median cycle time indicates issues)\n",
    "- Business value: Reduce cycle time variability 20-30%, improve on-time delivery from 85% ‚Üí 95%\n",
    "\n",
    "**3. Device RMA Root Cause Analysis**\n",
    "- Event log: Device lifecycle (serial_number, event, timestamp) from manufacturing ‚Üí field failure\n",
    "- Variant analysis: Compare process variants for failed vs. non-failed devices\n",
    "- Conformance deviations: Did failed devices skip quality checkpoints or have unusual process paths?\n",
    "- Business value: Reduce RMA rate 30-50% by identifying systematic process deviations\n",
    "\n",
    "### Business ROI Estimation\n",
    "\n",
    "**Scenario 1: Medium-Volume Semiconductor Fab (100K wafers/year, 10 test programs)**\n",
    "- Test flow optimization: 20% test time reduction √ó $15M annual test cost = **$3M/year savings**\n",
    "- Process conformance monitoring: Catch 50% of process deviations early = **$2M/year yield improvement**\n",
    "- Bottleneck elimination: Reduce cycle time 15% √ó $8M/year inventory cost = **$1.2M/year savings**\n",
    "- **Total ROI: $6.2M/year** (cost: $150K PM4Py infrastructure + $200K training = $5.85M net)\n",
    "\n",
    "**Scenario 2: High-Volume Automotive Semiconductor (500K wafers/year, 50+ test programs)**\n",
    "- Test program rationalization: Process mining reveals 15 redundant tests = **$12M/year test cost reduction**\n",
    "- Equipment utilization analysis: Improve OEE from 65% ‚Üí 75% = **$18M/year capacity gain**\n",
    "- Compliance automation: Automated conformance checking reduces audit prep 80% = **$2.5M/year savings**\n",
    "- **Total ROI: $32.5M/year** (cost: $800K infrastructure + $1M team = $30.7M net)\n",
    "\n",
    "**Scenario 3: Advanced Node R&D Fab (<10K wafers/year, experimental processes)**\n",
    "- Experimental process documentation: Auto-discover actual vs. intended process flows = **$1.8M/year faster learning**\n",
    "- Equipment qualification: Process mining validates tool performance consistency = **$2.2M/year reduced scrap**\n",
    "- Root cause acceleration: Process variant analysis reduces MTTR 40% = **$3M/year faster yield ramps**\n",
    "- **Total ROI: $7M/year** (cost: $200K infrastructure + $150K training = $6.65M net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36530a4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Mastery Achievement\n",
    "\n",
    "**You now have production-grade expertise in:**\n",
    "- ‚úÖ Preparing event logs from raw data (case ID, activity, timestamp schema)\n",
    "- ‚úÖ Discovering process models using Alpha, Inductive, and Heuristic miners\n",
    "- ‚úÖ Performing conformance checking (fitness, precision, generalization) to validate process compliance\n",
    "- ‚úÖ Analyzing bottlenecks and cycle time distributions with PM4Py\n",
    "- ‚úÖ Applying process mining to ATE test flow optimization, wafer fab monitoring, and RMA root cause analysis\n",
    "\n",
    "**Next Steps:**\n",
    "- **Predictive Process Monitoring**: Use event logs to predict process outcomes (will this wafer meet yield target?)\n",
    "- **Prescriptive Process Mining**: Recommend process improvements using simulation + optimization\n",
    "- **Object-Centric Process Mining**: Analyze processes with multiple interacting objects (wafer + lot + equipment)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
