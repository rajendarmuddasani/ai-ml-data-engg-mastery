{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc95fc2",
   "metadata": {},
   "source": [
    "# 094: Data Transformation Pipelines\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** pipeline orchestration concepts (DAGs, task dependencies, scheduling)\n",
    "- **Implement** task-based pipelines with retry logic and error handling\n",
    "- **Build** directed acyclic graphs (DAGs) for complex workflows\n",
    "- **Apply** pipeline patterns to post-silicon test data processing\n",
    "- **Evaluate** execution strategies and optimization techniques\n",
    "\n",
    "## ðŸ“š What are Data Transformation Pipelines?\n",
    "\n",
    "Data transformation pipelines are **orchestrated sequences of tasks** that move and transform data from source to destination. They implement workflows as directed acyclic graphs (DAGs) where tasks execute based on dependencies.\n",
    "\n",
    "**Why Pipelines?**\n",
    "- âœ… **Reliability**: Automatic retries, checkpointing, failure recovery\n",
    "- âœ… **Scalability**: Parallel execution of independent tasks\n",
    "- âœ… **Maintainability**: Modular tasks, clear dependencies, testable components\n",
    "- âœ… **Visibility**: Monitoring, logging, alerting for each task\n",
    "\n",
    "## ðŸ­ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Intel: Daily Test Data Pipeline ($40M Value)**\n",
    "- Input: 2000+ test programs/day generating 50TB STDF files\n",
    "- Pipeline: Extract â†’ Validate â†’ Aggregate â†’ Load warehouse â†’ Generate reports\n",
    "- Value: 99.9% SLA, 4-hour data freshness for fab decisions\n",
    "\n",
    "**NVIDIA: Multi-Stage GPU Validation ($35M Value)**\n",
    "- Input: Wafer test + package test + system test data\n",
    "- Pipeline: Merge test stages â†’ Correlate failures â†’ ML feature engineering â†’ Dashboard\n",
    "- Value: 500 concurrent pipelines, 15-minute end-to-end latency\n",
    "\n",
    "**Qualcomm: Global Test Data Sync ($20M Value)**\n",
    "- Input: 8 fabs worldwide, 24/7 test operations\n",
    "- Pipeline: Incremental extract â†’ Deduplicate â†’ Standardize â†’ Centralize â†’ Analyze\n",
    "- Value: 60% reduced data pipeline maintenance costs\n",
    "\n",
    "**AMD: Adaptive Test Flow ($15M Value)**\n",
    "- Input: Real-time parametric test results\n",
    "- Pipeline: Streaming ingest â†’ Outlier detection â†’ Trigger retest â†’ Update binning â†’ Alert\n",
    "- Value: 10Ã— faster failure detection (10 min â†’ 1 min)\n",
    "\n",
    "## ðŸ”„ Data Pipeline Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Source Data] --> B[Extract Task]\n",
    "    B --> C[Validate Task]\n",
    "    C --> D[Transform Task]\n",
    "    D --> E[Load Task]\n",
    "    E --> F[Target System]\n",
    "    \n",
    "    B -.->|Retry on Fail| B\n",
    "    C -.->|Retry on Fail| C\n",
    "    D -.->|Retry on Fail| D\n",
    "    E -.->|Retry on Fail| E\n",
    "    \n",
    "    G[Scheduler] --> B\n",
    "    G --> C\n",
    "    G --> D\n",
    "    G --> E\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style F fill:#e1ffe1\n",
    "    style G fill:#fff4e1\n",
    "```\n",
    "\n",
    "## ðŸ“Š Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- 091: ETL Fundamentals (extract-transform-load patterns)\n",
    "- 092: Apache Spark & PySpark (distributed processing)\n",
    "- 093: Data Cleaning Advanced (data quality rules)\n",
    "\n",
    "**Next Steps:**\n",
    "- 095: Stream Processing Real-Time (Kafka, Flink, streaming pipelines)\n",
    "- 096: Batch Processing at Scale (massive batch workflows)\n",
    "\n",
    "---\n",
    "\n",
    "Let's build production data pipelines! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dd6e45",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead4f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This notebook demonstrates pipeline concepts\n",
    "# In production, Airflow/Prefect run as separate services\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Callable, Optional\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import json\n",
    "from enum import Enum\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"âœ… Libraries imported\")\n",
    "print(\"\\nðŸ“ Note: This notebook simulates pipeline orchestration\")\n",
    "print(\"   Production: Use Apache Airflow or Prefect as standalone services\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8904ff",
   "metadata": {},
   "source": [
    "### ðŸ“ What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Import libraries for building pipeline orchestration framework\n",
    "\n",
    "**Key Points:**\n",
    "- **Airflow/Prefect**: Not imported here (they run as separate services with web UI)\n",
    "- **Simulation**: We'll build a simplified orchestrator to demonstrate concepts\n",
    "- **Production**: Airflow runs on Kubernetes/Docker with scheduler, workers, metadata DB\n",
    "\n",
    "**Why This Matters:** Understanding orchestration concepts is critical before using Airflow/Prefect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80a368",
   "metadata": {},
   "source": [
    "## 2. Pipeline Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskStatus(Enum):\n",
    "    \"\"\"Task execution status\"\"\"\n",
    "    PENDING = \"pending\"\n",
    "    RUNNING = \"running\"\n",
    "    SUCCESS = \"success\"\n",
    "    FAILED = \"failed\"\n",
    "    SKIPPED = \"skipped\"\n",
    "\n",
    "@dataclass\n",
    "class TaskResult:\n",
    "    \"\"\"Result of task execution\"\"\"\n",
    "    task_id: str\n",
    "    status: TaskStatus\n",
    "    data: Optional[any] = None\n",
    "    error: Optional[str] = None\n",
    "    duration: float = 0.0\n",
    "    start_time: Optional[datetime] = None\n",
    "    end_time: Optional[datetime] = None\n",
    "\n",
    "class Task:\n",
    "    \"\"\"A single task in the pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 task_id: str, \n",
    "                 function: Callable,\n",
    "                 retries: int = 3,\n",
    "                 retry_delay: int = 60):\n",
    "        self.task_id = task_id\n",
    "        self.function = function\n",
    "        self.retries = retries\n",
    "        self.retry_delay = retry_delay\n",
    "        self.dependencies: List['Task'] = []\n",
    "    \n",
    "    def set_upstream(self, task: 'Task'):\n",
    "        \"\"\"Set task dependency (this task depends on upstream task)\"\"\"\n",
    "        self.dependencies.append(task)\n",
    "    \n",
    "    def execute(self, context: Dict) -> TaskResult:\n",
    "        \"\"\"Execute task with retry logic\"\"\"\n",
    "        attempt = 0\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        while attempt < self.retries:\n",
    "            try:\n",
    "                print(f\"ðŸ”„ Executing {self.task_id} (attempt {attempt + 1}/{self.retries})\")\n",
    "                \n",
    "                result_data = self.function(context)\n",
    "                end_time = datetime.now()\n",
    "                \n",
    "                return TaskResult(\n",
    "                    task_id=self.task_id,\n",
    "                    status=TaskStatus.SUCCESS,\n",
    "                    data=result_data,\n",
    "                    duration=(end_time - start_time).total_seconds(),\n",
    "                    start_time=start_time,\n",
    "                    end_time=end_time\n",
    "                )\n",
    "            \n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                if attempt < self.retries:\n",
    "                    print(f\"âš ï¸  Task {self.task_id} failed: {str(e)}. Retrying in {self.retry_delay}s...\")\n",
    "                    time.sleep(self.retry_delay / 60)  # Shortened for demo\n",
    "                else:\n",
    "                    end_time = datetime.now()\n",
    "                    print(f\"âŒ Task {self.task_id} failed after {self.retries} attempts\")\n",
    "                    return TaskResult(\n",
    "                        task_id=self.task_id,\n",
    "                        status=TaskStatus.FAILED,\n",
    "                        error=str(e),\n",
    "                        duration=(end_time - start_time).total_seconds(),\n",
    "                        start_time=start_time,\n",
    "                        end_time=end_time\n",
    "                    )\n",
    "\n",
    "print(\"âœ… Task class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974ab38f",
   "metadata": {},
   "source": [
    "### ðŸ“ What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Define Task class with retry logic and dependency tracking\n",
    "\n",
    "**Key Points:**\n",
    "- **Task**: Wraps a function with metadata (task_id, retries, dependencies)\n",
    "- **Retry Logic**: Automatically retry failed tasks (3Ã— default with 60s delay)\n",
    "- **Dependencies**: `set_upstream()` defines task execution order\n",
    "- **TaskResult**: Captures execution status, data, errors, timing\n",
    "\n",
    "**Retry Strategy:**\n",
    "- **Transient failures**: Network timeouts, API rate limits (retry works)\n",
    "- **Permanent failures**: Logic errors, missing data (retry fails)\n",
    "- **Exponential backoff**: Retry delay increases (60s â†’ 120s â†’ 240s)\n",
    "\n",
    "**Why This Matters:** Retry logic is critical for production pipelines (network failures, API timeouts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9387f9c5",
   "metadata": {},
   "source": [
    "## 3. DAG (Directed Acyclic Graph) Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef392c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAG:\n",
    "    \"\"\"Pipeline DAG (Directed Acyclic Graph)\"\"\"\n",
    "    \n",
    "    def __init__(self, dag_id: str, schedule: str = \"@daily\"):\n",
    "        self.dag_id = dag_id\n",
    "        self.schedule = schedule\n",
    "        self.tasks: Dict[str, Task] = {}\n",
    "    \n",
    "    def add_task(self, task: Task):\n",
    "        \"\"\"Add task to DAG\"\"\"\n",
    "        self.tasks[task.task_id] = task\n",
    "    \n",
    "    def _topological_sort(self) -> List[Task]:\n",
    "        \"\"\"Sort tasks by dependencies (topological order)\"\"\"\n",
    "        # Build adjacency list\n",
    "        in_degree = {task_id: 0 for task_id in self.tasks}\n",
    "        adj_list = {task_id: [] for task_id in self.tasks}\n",
    "        \n",
    "        for task_id, task in self.tasks.items():\n",
    "            for dep in task.dependencies:\n",
    "                adj_list[dep.task_id].append(task_id)\n",
    "                in_degree[task_id] += 1\n",
    "        \n",
    "        # Kahn's algorithm\n",
    "        queue = [task_id for task_id, degree in in_degree.items() if degree == 0]\n",
    "        sorted_tasks = []\n",
    "        \n",
    "        while queue:\n",
    "            task_id = queue.pop(0)\n",
    "            sorted_tasks.append(self.tasks[task_id])\n",
    "            \n",
    "            for neighbor in adj_list[task_id]:\n",
    "                in_degree[neighbor] -= 1\n",
    "                if in_degree[neighbor] == 0:\n",
    "                    queue.append(neighbor)\n",
    "        \n",
    "        if len(sorted_tasks) != len(self.tasks):\n",
    "            raise ValueError(\"Cycle detected in DAG!\")\n",
    "        \n",
    "        return sorted_tasks\n",
    "    \n",
    "    def execute(self, context: Optional[Dict] = None) -> Dict[str, TaskResult]:\n",
    "        \"\"\"Execute all tasks in topological order\"\"\"\n",
    "        if context is None:\n",
    "            context = {}\n",
    "        \n",
    "        context['dag_id'] = self.dag_id\n",
    "        context['execution_date'] = datetime.now()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ðŸš€ Executing DAG: {self.dag_id}\")\n",
    "        print(f\"ðŸ“… Execution Date: {context['execution_date']}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        sorted_tasks = self._topological_sort()\n",
    "        results = {}\n",
    "        \n",
    "        for task in sorted_tasks:\n",
    "            # Check if dependencies succeeded\n",
    "            deps_failed = any(\n",
    "                results[dep.task_id].status == TaskStatus.FAILED \n",
    "                for dep in task.dependencies\n",
    "            )\n",
    "            \n",
    "            if deps_failed:\n",
    "                print(f\"â­ï¸  Skipping {task.task_id} (upstream failed)\")\n",
    "                results[task.task_id] = TaskResult(\n",
    "                    task_id=task.task_id,\n",
    "                    status=TaskStatus.SKIPPED,\n",
    "                    error=\"Upstream task failed\"\n",
    "                )\n",
    "            else:\n",
    "                # Pass upstream results to context\n",
    "                for dep in task.dependencies:\n",
    "                    if results[dep.task_id].data is not None:\n",
    "                        context[dep.task_id] = results[dep.task_id].data\n",
    "                \n",
    "                result = task.execute(context)\n",
    "                results[task.task_id] = result\n",
    "                \n",
    "                if result.status == TaskStatus.SUCCESS:\n",
    "                    print(f\"âœ… {task.task_id} completed in {result.duration:.2f}s\")\n",
    "                else:\n",
    "                    print(f\"âŒ {task.task_id} failed: {result.error}\")\n",
    "        \n",
    "        self._print_summary(results)\n",
    "        return results\n",
    "    \n",
    "    def _print_summary(self, results: Dict[str, TaskResult]):\n",
    "        \"\"\"Print pipeline execution summary\"\"\"\n",
    "        total = len(results)\n",
    "        success = sum(1 for r in results.values() if r.status == TaskStatus.SUCCESS)\n",
    "        failed = sum(1 for r in results.values() if r.status == TaskStatus.FAILED)\n",
    "        skipped = sum(1 for r in results.values() if r.status == TaskStatus.SKIPPED)\n",
    "        total_duration = sum(r.duration for r in results.values())\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ðŸ“Š Pipeline Summary\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total Tasks: {total}\")\n",
    "        print(f\"âœ… Success: {success}\")\n",
    "        print(f\"âŒ Failed: {failed}\")\n",
    "        print(f\"â­ï¸  Skipped: {skipped}\")\n",
    "        print(f\"â±ï¸  Total Duration: {total_duration:.2f}s\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"âœ… DAG class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f9d2d",
   "metadata": {},
   "source": [
    "### ðŸ“ What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Build DAG orchestrator that executes tasks in correct order\n",
    "\n",
    "**Key Points:**\n",
    "- **DAG**: Container for tasks with dependency relationships\n",
    "- **Topological Sort**: Orders tasks so dependencies execute first (Kahn's algorithm)\n",
    "- **Dependency Check**: Skip downstream tasks if upstream fails\n",
    "- **Context**: Shared data structure passed between tasks\n",
    "\n",
    "**Execution Flow:**\n",
    "1. Sort tasks topologically (resolve dependencies)\n",
    "2. Execute each task in order\n",
    "3. If task fails, skip all downstream tasks\n",
    "4. Pass results via context (task1_result â†’ task2_input)\n",
    "\n",
    "**Cycle Detection:**\n",
    "- DAG must be acyclic (no circular dependencies)\n",
    "- Example invalid: Task A â†’ Task B â†’ Task C â†’ Task A (cycle!)\n",
    "\n",
    "**Why This Matters:** Topological sort ensures correct execution order. Intel runs 2000+ DAGs daily with complex dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec04607b",
   "metadata": {},
   "source": [
    "## 4. Example Pipeline: STDF Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3785512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define task functions\n",
    "def extract_stdf_files(context):\n",
    "    \"\"\"Extract STDF files from S3\"\"\"\n",
    "    print(\"  ðŸ“¥ Extracting STDF files from S3...\")\n",
    "    time.sleep(0.5)  # Simulate S3 download\n",
    "    \n",
    "    # Simulate extracted data\n",
    "    data = pd.DataFrame({\n",
    "        'wafer_id': [f'W{i}' for i in range(100)],\n",
    "        'test_value': np.random.normal(1.0, 0.1, 100),\n",
    "        'passed': np.random.choice([True, False], 100, p=[0.95, 0.05])\n",
    "    })\n",
    "    \n",
    "    print(f\"  âœ… Extracted {len(data)} records\")\n",
    "    return data\n",
    "\n",
    "def clean_data(context):\n",
    "    \"\"\"Clean and validate data\"\"\"\n",
    "    print(\"  ðŸ§¹ Cleaning data...\")\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    # Get data from upstream task\n",
    "    data = context['extract_stdf']\n",
    "    \n",
    "    # Remove outliers (Z-score > 3)\n",
    "    z_scores = np.abs((data['test_value'] - data['test_value'].mean()) / data['test_value'].std())\n",
    "    data_clean = data[z_scores < 3]\n",
    "    \n",
    "    print(f\"  âœ… Cleaned: {len(data)} â†’ {len(data_clean)} records (removed {len(data) - len(data_clean)} outliers)\")\n",
    "    return data_clean\n",
    "\n",
    "def aggregate_statistics(context):\n",
    "    \"\"\"Calculate yield statistics\"\"\"\n",
    "    print(\"  ðŸ“Š Aggregating statistics...\")\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "    data = context['clean_data']\n",
    "    \n",
    "    stats = {\n",
    "        'total_wafers': len(data['wafer_id'].unique()),\n",
    "        'total_tests': len(data),\n",
    "        'yield_pct': (data['passed'].sum() / len(data) * 100),\n",
    "        'mean_test_value': data['test_value'].mean(),\n",
    "        'std_test_value': data['test_value'].std()\n",
    "    }\n",
    "    \n",
    "    print(f\"  âœ… Yield: {stats['yield_pct']:.1f}%, Mean: {stats['mean_test_value']:.3f}\")\n",
    "    return stats\n",
    "\n",
    "def load_to_warehouse(context):\n",
    "    \"\"\"Load data to data warehouse\"\"\"\n",
    "    print(\"  ðŸ’¾ Loading to warehouse...\")\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    data = context['clean_data']\n",
    "    stats = context['aggregate_stats']\n",
    "    \n",
    "    # Simulate database write\n",
    "    print(f\"  âœ… Loaded {len(data)} records to warehouse\")\n",
    "    print(f\"  ðŸ“ˆ Yield: {stats['yield_pct']:.1f}%\")\n",
    "    return {\"records_loaded\": len(data)}\n",
    "\n",
    "def send_alert(context):\n",
    "    \"\"\"Send alert if yield is low\"\"\"\n",
    "    print(\"  ðŸ“§ Checking yield threshold...\")\n",
    "    time.sleep(0.2)\n",
    "    \n",
    "    stats = context['aggregate_stats']\n",
    "    \n",
    "    if stats['yield_pct'] < 90:\n",
    "        print(f\"  âš ï¸  ALERT: Low yield detected ({stats['yield_pct']:.1f}%)\")\n",
    "        print(f\"  ðŸ“§ Sent alert to operations team\")\n",
    "    else:\n",
    "        print(f\"  âœ… Yield OK ({stats['yield_pct']:.1f}%)\")\n",
    "    \n",
    "    return {\"alert_sent\": stats['yield_pct'] < 90}\n",
    "\n",
    "# Build DAG\n",
    "dag = DAG(dag_id=\"stdf_processing_pipeline\", schedule=\"@daily\")\n",
    "\n",
    "# Create tasks\n",
    "extract_task = Task(\"extract_stdf\", extract_stdf_files, retries=3)\n",
    "clean_task = Task(\"clean_data\", clean_data, retries=2)\n",
    "aggregate_task = Task(\"aggregate_stats\", aggregate_statistics, retries=1)\n",
    "load_task = Task(\"load_warehouse\", load_to_warehouse, retries=3)\n",
    "alert_task = Task(\"send_alert\", send_alert, retries=1)\n",
    "\n",
    "# Define dependencies\n",
    "clean_task.set_upstream(extract_task)  # clean depends on extract\n",
    "aggregate_task.set_upstream(clean_task)  # aggregate depends on clean\n",
    "load_task.set_upstream(clean_task)  # load depends on clean\n",
    "alert_task.set_upstream(aggregate_task)  # alert depends on aggregate\n",
    "\n",
    "# Add tasks to DAG\n",
    "dag.add_task(extract_task)\n",
    "dag.add_task(clean_task)\n",
    "dag.add_task(aggregate_task)\n",
    "dag.add_task(load_task)\n",
    "dag.add_task(alert_task)\n",
    "\n",
    "print(\"âœ… Pipeline DAG built successfully\")\n",
    "print(f\"\\nðŸ“‹ DAG: {dag.dag_id}\")\n",
    "print(f\"â° Schedule: {dag.schedule}\")\n",
    "print(f\"ðŸ“Š Tasks: {len(dag.tasks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899eacb1",
   "metadata": {},
   "source": [
    "### ðŸ“ What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Build complete STDF processing pipeline with 5 tasks and dependencies\n",
    "\n",
    "**Key Points:**\n",
    "- **Task Functions**: Each function is a pipeline stage (extract, clean, aggregate, load, alert)\n",
    "- **Dependencies**: Clean â†’ Extract, Aggregate â†’ Clean, Load â†’ Clean, Alert â†’ Aggregate\n",
    "- **Context Passing**: Results flow between tasks (extract_stdf â†’ clean_data â†’ aggregate_stats)\n",
    "- **Schedule**: `@daily` means run once per day (Airflow cron syntax)\n",
    "\n",
    "**Dependency Graph:**\n",
    "```\n",
    "extract_stdf\n",
    "     |\n",
    "  clean_data\n",
    "    /    \\\n",
    "aggregate  load_warehouse\n",
    "   |\n",
    "send_alert\n",
    "```\n",
    "\n",
    "**Why This Matters:** Real pipelines have 10-100 tasks with complex dependencies. Intel manages 2000+ daily DAGs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec14d6a",
   "metadata": {},
   "source": [
    "## 5. Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb8f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the pipeline\n",
    "results = dag.execute()\n",
    "\n",
    "# Visualize execution timeline\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "task_names = list(results.keys())\n",
    "task_durations = [results[task].duration for task in task_names]\n",
    "task_colors = [\n",
    "    'green' if results[task].status == TaskStatus.SUCCESS \n",
    "    else 'red' if results[task].status == TaskStatus.FAILED\n",
    "    else 'gray'\n",
    "    for task in task_names\n",
    "]\n",
    "\n",
    "bars = ax.barh(task_names, task_durations, color=task_colors, alpha=0.7)\n",
    "ax.set_xlabel('Duration (seconds)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Task', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Pipeline Execution Timeline', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add duration labels\n",
    "for i, (task, duration) in enumerate(zip(task_names, task_durations)):\n",
    "    ax.text(duration + 0.02, i, f'{duration:.2f}s', \n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='green', alpha=0.7, label='Success'),\n",
    "    Patch(facecolor='red', alpha=0.7, label='Failed'),\n",
    "    Patch(facecolor='gray', alpha=0.7, label='Skipped')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Pipeline execution completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61983035",
   "metadata": {},
   "source": [
    "### ðŸ“ What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Execute pipeline and visualize task execution timeline\n",
    "\n",
    "**Key Points:**\n",
    "- **Execution**: DAG runs tasks in topological order\n",
    "- **Timeline**: Horizontal bar chart shows task durations\n",
    "- **Status Colors**: Green (success), Red (failed), Gray (skipped)\n",
    "- **Bottlenecks**: Longest tasks are optimization opportunities\n",
    "\n",
    "**Production Observability:**\n",
    "- **Airflow UI**: Gantt chart, task logs, retry history\n",
    "- **Metrics**: Grafana dashboards (task duration, failure rate, SLA)\n",
    "- **Alerts**: PagerDuty, Slack (pipeline failures, SLA breaches)\n",
    "- **Logs**: CloudWatch, ELK stack (debug failed tasks)\n",
    "\n",
    "**Why This Matters:** Observability is critical for production pipelines. AMD detects failures 10Ã— faster with monitoring ($15M savings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c35dca",
   "metadata": {},
   "source": [
    "## 6. Real-World Projects & Business Impact\n",
    "\n",
    "### ðŸ­ Post-Silicon Validation Projects\n",
    "\n",
    "**1. Intel Daily Test Data Pipeline ($40M Annual Impact)**\n",
    "- **Objective**: Orchestrate 2000+ daily ETL jobs processing 500TB STDF data\n",
    "- **Data**: Wafer probe + final test from 100+ ATE systems (Oregon, Arizona, Ireland, Israel)\n",
    "- **Architecture**: Airflow (scheduler) â†’ Spark jobs â†’ Delta Lake â†’ BI dashboards\n",
    "- **DAG Structure**:\n",
    "  - 50 parallel extract jobs (one per site/product)\n",
    "  - 20 transform jobs (cleaning, validation, enrichment)\n",
    "  - 10 aggregate jobs (yield, correlation, trends)\n",
    "  - 5 load jobs (Delta Lake, Snowflake, Tableau)\n",
    "  - 5 ML jobs (yield prediction model retraining)\n",
    "- **Optimizations**:\n",
    "  - Task pools (limit concurrency to 100 Spark jobs)\n",
    "  - SLA monitoring (alert if >4 hour latency)\n",
    "  - Dynamic DAG generation (from metadata, not hardcoded)\n",
    "  - Smart retries (exponential backoff, only transient errors)\n",
    "- **Metrics**: 99.9% SLA (2000+ pipelines/day), <4 hour end-to-end latency\n",
    "- **Tech Stack**: Airflow 2.7, Kubernetes, Spark, Delta Lake, Databricks\n",
    "- **Impact**: $40M operational excellence (automated vs manual orchestration)\n",
    "\n",
    "**2. NVIDIA GPU Test Orchestration ($35M Annual Savings)**\n",
    "- **Objective**: Orchestrate 500 concurrent jobs processing 100M GPU test records\n",
    "- **Data**: Voltage, current, frequency, thermal, yield from 10 global test sites\n",
    "- **Architecture**: Prefect (scheduler) â†’ dask (parallel) â†’ InfluxDB â†’ Grafana\n",
    "- **Pipeline Structure**:\n",
    "  - Real-time: 100 streaming jobs (Kafka â†’ Flink â†’ InfluxDB)\n",
    "  - Batch: 200 daily jobs (S3 â†’ Spark â†’ Delta Lake)\n",
    "  - ML: 50 model training jobs (hourly retraining)\n",
    "  - Reporting: 100 dashboard refresh jobs\n",
    "  - Alerting: 50 anomaly detection jobs\n",
    "- **Optimizations**:\n",
    "  - Prefect flow runs (500 concurrent, 10TB/hour throughput)\n",
    "  - Dynamic task mapping (fan-out 1 â†’ 100 parallel tasks)\n",
    "  - Cached results (avoid recomputing expensive tasks)\n",
    "  - Parameterized flows (reuse DAG for multiple products)\n",
    "- **Metrics**: 500 concurrent jobs, 10TB/hour, <1 hour latency\n",
    "- **Tech Stack**: Prefect 2.0, Dask, Kafka, Flink, InfluxDB, Grafana\n",
    "- **Impact**: $35M faster insights (real-time vs daily batch)\n",
    "\n",
    "**3. Qualcomm Multi-Site Data Sync Pipeline ($20M Annual Savings)**\n",
    "- **Objective**: Sync wafer probe + final test data across 10 global sites\n",
    "- **Data**: Probe (Oregon, Austin) + final test (Penang, Shanghai, Taiwan)\n",
    "- **Architecture**: Airflow â†’ S3 (staging) â†’ Spark â†’ Snowflake â†’ Tableau\n",
    "- **Challenges**:\n",
    "  - Time zones: Coordinate jobs across 10 time zones\n",
    "  - Network: Cross-region S3 transfers (optimize with multipart uploads)\n",
    "  - Schema evolution: Handle test program changes (backward compatibility)\n",
    "  - Data quality: Validate before sync (reject bad batches)\n",
    "- **Implementation**:\n",
    "  - Airflow external sensors (wait for upstream site data)\n",
    "  - Cross-region S3 replication (Oregon â†’ Singapore)\n",
    "  - Schema registry (Confluent Schema Registry for compatibility)\n",
    "  - Quality gates (Great Expectations validation before load)\n",
    "- **Metrics**: <4 hour latency (vs 24 hour manual), 99.5% SLA\n",
    "- **Tech Stack**: Airflow 2.7, S3, Spark, Snowflake, Great Expectations\n",
    "- **Impact**: $20M savings (60% reduced maintenance, Airflow vs cron)\n",
    "\n",
    "**4. AMD Yield Model Retraining Pipeline ($15M Annual Savings)**\n",
    "- **Objective**: Automated ML pipeline for yield prediction model retraining\n",
    "- **Data**: Daily wafer test data (100GB), model training (4 hours), deployment\n",
    "- **Architecture**: Airflow â†’ MLflow â†’ Model Registry â†’ Kubernetes deployment\n",
    "- **Pipeline Stages**:\n",
    "  1. Data validation (Great Expectations)\n",
    "  2. Feature engineering (Spark)\n",
    "  3. Model training (XGBoost, LightGBM, CatBoost)\n",
    "  4. Model evaluation (hold-out test set)\n",
    "  5. Model comparison (MLflow, champion/challenger)\n",
    "  6. Model deployment (Kubernetes, blue-green)\n",
    "  7. Model monitoring (Evidently AI, drift detection)\n",
    "- **Optimizations**:\n",
    "  - Branching (train 3 models in parallel)\n",
    "  - Short-circuiting (skip deployment if accuracy < threshold)\n",
    "  - Rollback (revert to previous model if drift detected)\n",
    "  - A/B testing (deploy to 10% traffic first)\n",
    "- **Metrics**: 99% automation (vs 50% manual), 10Ã— faster failure detection\n",
    "- **Tech Stack**: Airflow, MLflow, XGBoost, Kubernetes, Evidently AI\n",
    "- **Impact**: $15M reduced downtime (automated model updates)\n",
    "\n",
    "### ðŸŒ General AI/ML Projects\n",
    "\n",
    "**5. Uber Driver-Rider Matching Pipeline ($100M Revenue Impact)**\n",
    "- **Objective**: Real-time pipeline matching 1M rides/hour globally\n",
    "- **Data**: Driver locations (GPS), rider requests, traffic, pricing\n",
    "- **Architecture**: Prefect â†’ Kafka â†’ Flink â†’ ML models â†’ Redis\n",
    "- **Metrics**: <1s matching latency, 1M rides/hour, 99.99% uptime\n",
    "- **Tech Stack**: Prefect, Kafka, Flink, Redis, ML models (XGBoost)\n",
    "- **Impact**: $100M revenue (optimal matching reduces wait times)\n",
    "\n",
    "**6. Netflix Content Recommendation Pipeline ($80M Revenue Increase)**\n",
    "- **Objective**: Daily pipeline retraining recommendation models on 500M events\n",
    "- **Data**: User viewing history, ratings, clickstream, content metadata\n",
    "- **Architecture**: Airflow â†’ Spark â†’ Feature store â†’ ML training â†’ A/B test\n",
    "- **Metrics**: Daily retraining (500M events), 30% engagement uplift\n",
    "- **Tech Stack**: Airflow, Spark, Feature Store, TensorFlow, Kubernetes\n",
    "- **Impact**: $80M revenue (personalized recommendations drive 80% views)\n",
    "\n",
    "**7. Airbnb Pricing Optimization Pipeline ($60M Revenue Increase)**\n",
    "- **Objective**: Daily pipeline optimizing listing prices based on demand\n",
    "- **Data**: Booking history, search demand, seasonality, events, reviews\n",
    "- **Architecture**: Airflow â†’ Spark â†’ XGBoost â†’ Price API â†’ MySQL\n",
    "- **Metrics**: Daily price updates (7M listings), 15% booking uplift\n",
    "- **Tech Stack**: Airflow, Spark, XGBoost, MySQL, Redis cache\n",
    "- **Impact**: $60M revenue (dynamic pricing optimizes supply/demand)\n",
    "\n",
    "**8. PayPal Fraud Detection Pipeline ($150M Fraud Prevention)**\n",
    "- **Objective**: Real-time pipeline scoring 1B transactions/day for fraud\n",
    "- **Data**: Transaction details, user behavior, merchant risk, device fingerprint\n",
    "- **Architecture**: Airflow â†’ Kafka â†’ Flink â†’ XGBoost â†’ Block API\n",
    "- **Metrics**: <50ms scoring latency, 1B TPS, 95% fraud detection\n",
    "- **Tech Stack**: Airflow, Kafka, Flink, XGBoost, Redis, PostgreSQL\n",
    "- **Impact**: $150M fraud prevented (real-time scoring blocks fraud)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "**Pipeline Orchestration Concepts:**\n",
    "1. **DAG**: Directed Acyclic Graph defines task dependencies\n",
    "2. **Idempotency**: Rerunning pipeline produces same result (critical for retries)\n",
    "3. **Fault Tolerance**: Retry failed tasks, skip downstream if upstream fails\n",
    "4. **Observability**: Logs, metrics, alerts for pipeline health\n",
    "\n",
    "**Business Impact: $500M Total**\n",
    "- **Post-Silicon**: Intel $40M + NVIDIA $35M + Qualcomm $20M + AMD $15M = **$110M**\n",
    "- **General**: Uber $100M + Netflix $80M + Airbnb $60M + PayPal $150M = **$390M**\n",
    "\n",
    "**Airflow vs Prefect:**\n",
    "- **Airflow**: Mature, large ecosystem, Kubernetes-native (Intel, Qualcomm use)\n",
    "- **Prefect**: Modern, Python-native, dynamic DAGs (NVIDIA uses)\n",
    "- **Both**: Production-grade, support retries, SLAs, observability\n",
    "\n",
    "**Best Practices:**\n",
    "- âœ… **Idempotency**: Same input â†’ same output (avoid duplicate data)\n",
    "- âœ… **Small tasks**: 1 task = 1 responsibility (easier debugging)\n",
    "- âœ… **Parameterization**: Reuse DAGs for multiple products/sites\n",
    "- âœ… **SLA monitoring**: Alert if pipeline breaches latency target\n",
    "- âœ… **Dynamic DAGs**: Generate from metadata (avoid hardcoding 2000 DAGs)\n",
    "\n",
    "**Common Pitfalls:**\n",
    "- **Long-running tasks**: 10-hour task blocks executors (split into smaller tasks)\n",
    "- **Too many retries**: 100 retries waste resources (fix root cause)\n",
    "- **Circular dependencies**: Task A â†’ B â†’ C â†’ A (cycle, DAG invalid)\n",
    "- **No observability**: Can't debug failures (add logs, metrics, alerts)\n",
    "\n",
    "**Next Steps:**\n",
    "- **095**: Stream Processing (real-time pipelines with Kafka, Flink)\n",
    "- **097**: Data Lake Architecture (storage layer for pipelines)\n",
    "- **131**: MLOps Fundamentals (ML model deployment pipelines)\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ Congratulations!** You've mastered data pipeline orchestration - from DAG design to fault tolerance to production deployment! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
