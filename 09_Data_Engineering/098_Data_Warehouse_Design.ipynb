{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "213a738c",
   "metadata": {},
   "source": [
    "# 098: Data Warehouse Design\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** data warehouse architectures (star schema, snowflake, dimensional modeling)\n",
    "- **Implement** slowly changing dimensions (SCD Type 1, 2, 3)\n",
    "- **Design** OLAP cubes for semiconductor test analytics\n",
    "- **Build** Redshift/Snowflake-style data warehouse models\n",
    "- **Compare** lakehouse vs traditional warehouse tradeoffs\n",
    "\n",
    "## üìö What is a Data Warehouse?\n",
    "\n",
    "A **data warehouse** is a structured, optimized repository for analytics and business intelligence. Unlike data lakes (schema-on-read), warehouses use **schema-on-write** with predefined tables, dimensional models, and SQL optimization for fast queries.\n",
    "\n",
    "**Key characteristics:** Columnar storage (Parquet, ORC), aggressive indexing, materialized views, pre-aggregated cubes. Modern warehouses (Redshift, Snowflake, BigQuery) separate compute from storage, enabling elastic scaling.\n",
    "\n",
    "For semiconductor testing, warehouses power executive dashboards (yield trends), wafer-level analytics (spatial patterns), and test time optimization (bin distribution analysis).\n",
    "\n",
    "**Why Data Warehouses?**\n",
    "- ‚úÖ Sub-second BI queries (dashboards refresh in <1s)\n",
    "- ‚úÖ SQL-first (analysts/executives use familiar tools)\n",
    "- ‚úÖ Mature optimization (decades of query optimization R&D)\n",
    "- ‚úÖ Dimensional modeling (star schema for intuitive navigation)\n",
    "- ‚úÖ Aggregation layers (pre-compute common metrics)\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Intel Redshift Warehouse ($50M/year value)**\n",
    "- Input: 5TB aggregated test data (gold layer from data lake)\n",
    "- Output: Executive dashboards (yield by product, fab, week), <500ms queries\n",
    "- Value: 2% yield improvement via faster decision-making = $50M savings\n",
    "\n",
    "**NVIDIA Snowflake Analytics ($45M/year)**\n",
    "- Input: 10TB GPU test summaries (wafer-level aggregations)\n",
    "- Output: Dimensional model (test_fact, device_dim, time_dim, site_dim)\n",
    "- Value: 1.8% yield gain + 50% faster root cause = $45M/year\n",
    "\n",
    "**Qualcomm BigQuery Warehouse ($35M/year)**\n",
    "- Input: 8TB mobile SoC test data (cross-site aggregations)\n",
    "- Output: OLAP cubes (yield by device √ó site √ó week), SCD Type 2 for device history\n",
    "- Value: 1.5% yield improvement + 40% faster analysis = $35M\n",
    "\n",
    "**AMD Data Vault Warehouse ($40M/year)**\n",
    "- Input: 6TB server CPU test data (historical snapshots)\n",
    "- Output: Data Vault 2.0 (hubs, links, satellites), full audit trail\n",
    "- Value: 1.7% yield gain + compliance automation = $40M/year\n",
    "\n",
    "## üîÑ Data Warehouse Architecture Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[\"Data Lake<br/>(Gold Layer)\"] --> B[\"ETL Pipeline<br/>(Spark/Airflow)\"]\n",
    "    B --> C[\"Staging Area<br/>(Raw Loads)\"]\n",
    "    \n",
    "    C --> D[\"Star Schema<br/>(Fact + Dimensions)\"]\n",
    "    D --> E[\"Fact Table<br/>(test_results_fact)\"]\n",
    "    D --> F[\"Dimension Tables<br/>(device, time, site)\"]\n",
    "    \n",
    "    E --> G[\"Aggregation Layer<br/>(Materialized Views)\"]\n",
    "    F --> G\n",
    "    \n",
    "    G --> H[\"BI Dashboards<br/>(Tableau/Power BI)\"]\n",
    "    G --> I[\"Ad-hoc SQL<br/>(Analysts)\"]\n",
    "    G --> J[\"Executive Reports<br/>(Weekly Yield)\"]\n",
    "    \n",
    "    style A fill:#ffe1e1\n",
    "    style D fill:#e1f5ff\n",
    "    style G fill:#e1ffe1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- 092: Apache Spark & PySpark (DataFrame transformations)\n",
    "- 094: Data Transformation Pipelines (ETL patterns)\n",
    "- 097: Data Lake Architecture (lakehouse comparison)\n",
    "\n",
    "**Next Steps:**\n",
    "- 099: Big Data Formats (columnar storage internals)\n",
    "- 100: Data Governance & Quality (metadata catalogs)\n",
    "- 111: MLOps Fundamentals (model serving from warehouse features)\n",
    "\n",
    "---\n",
    "\n",
    "Let's design production data warehouses! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68645e1",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Structures\n",
    "\n",
    "Import libraries and define data structures for warehouse simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7845f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a430fe3",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Import libraries for data warehouse simulation (dimensional modeling, star schema)\n",
    "\n",
    "**Key Points:**\n",
    "- **pandas**: Simulates warehouse tables (fact and dimension tables)\n",
    "- **dataclass**: Models dimension records with surrogate keys\n",
    "- **datetime**: Tracks effective dates for slowly changing dimensions (SCD Type 2)\n",
    "- **matplotlib**: Visualizes query performance and data distribution\n",
    "\n",
    "**Why This Matters:** Real warehouses (Redshift, Snowflake) use columnar storage and MPP (massively parallel processing). This simulation teaches dimensional modeling principles applicable to production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e7c06b",
   "metadata": {},
   "source": [
    "## Part 2: Dimensional Model Design\n",
    "\n",
    "Implement star schema with fact table (test results) and dimensions (device, time, site)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be66617",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCDType(Enum):\n",
    "    \"\"\"Slowly Changing Dimension types\"\"\"\n",
    "    TYPE1 = \"overwrite\"  # No history\n",
    "    TYPE2 = \"versioned\"  # Full history\n",
    "    TYPE3 = \"snapshot\"   # Limited history (previous + current)\n",
    "\n",
    "@dataclass\n",
    "class DimensionRecord:\n",
    "    \"\"\"Base dimension record with SCD metadata\"\"\"\n",
    "    surrogate_key: int  # Warehouse-generated unique key\n",
    "    natural_key: str    # Business key (device_id, site_code)\n",
    "    effective_date: datetime\n",
    "    expiration_date: Optional[datetime]\n",
    "    is_current: bool\n",
    "    \n",
    "@dataclass\n",
    "class DeviceDimension(DimensionRecord):\n",
    "    \"\"\"Device dimension with attributes\"\"\"\n",
    "    device_family: str\n",
    "    process_node: str  # \"7nm\", \"5nm\", \"3nm\"\n",
    "    architecture: str  # \"ARM\", \"x86\", \"RISC-V\"\n",
    "    target_frequency: float\n",
    "    \n",
    "@dataclass\n",
    "class TimeDimension:\n",
    "    \"\"\"Time dimension for date-based analysis\"\"\"\n",
    "    date_key: int  # YYYYMMDD format\n",
    "    date: datetime\n",
    "    year: int\n",
    "    quarter: int\n",
    "    month: int\n",
    "    week: int\n",
    "    day_of_week: str\n",
    "    is_weekend: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d2dc7b",
   "metadata": {},
   "source": [
    "### üìù Code Explanation\n",
    "\n",
    "**Purpose:** Define dimension tables for star schema\n",
    "\n",
    "**Key Points:**\n",
    "- **SCDType enum**: Three patterns for handling dimension changes over time\n",
    "- **DimensionRecord**: Base class with SCD Type 2 fields (effective/expiration dates, is_current flag)\n",
    "- **DeviceDimension**: Tracks device attributes (family, process node, architecture)\n",
    "- **TimeDimension**: Pre-computed date attributes (year, quarter, month, week) for fast filtering\n",
    "\n",
    "**Why This Matters:** \n",
    "- **Surrogate keys**: Enable SCD Type 2 (multiple versions of same device with different keys)\n",
    "- **Natural keys**: Business identifiers (device_id) for lookups\n",
    "- **Time dimension**: Avoids expensive date functions (WHERE year=2024 vs WHERE YEAR(date)=2024)\n",
    "- **SCD Type 2**: Track device attribute changes (e.g., target frequency updated after bin split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec60a30d",
   "metadata": {},
   "source": [
    "## Part 3: Fact Table Implementation\n",
    "\n",
    "Create fact table storing test measurements with foreign keys to dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c25171",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestResultFact:\n",
    "    \"\"\"Fact table for semiconductor test results\"\"\"\n",
    "    fact_key: int\n",
    "    device_key: int  # FK to device_dimension\n",
    "    time_key: int    # FK to time_dimension\n",
    "    site_key: int    # FK to site_dimension\n",
    "    \n",
    "    # Measures (additive)\n",
    "    test_count: int\n",
    "    pass_count: int\n",
    "    fail_count: int\n",
    "    total_test_time_ms: float\n",
    "    \n",
    "    # Measures (semi-additive)\n",
    "    avg_voltage: float\n",
    "    avg_current: float\n",
    "    avg_frequency: float\n",
    "    \n",
    "    # Measures (non-additive)\n",
    "    yield_pct: float\n",
    "    \n",
    "class FactTable:\n",
    "    \"\"\"Manages fact table with dimension lookups\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.facts: List[TestResultFact] = []\n",
    "        self.next_key = 1\n",
    "        \n",
    "    def insert(self, fact: TestResultFact):\n",
    "        \"\"\"Insert fact record\"\"\"\n",
    "        fact.fact_key = self.next_key\n",
    "        self.facts.append(fact)\n",
    "        self.next_key += 1\n",
    "        \n",
    "    def query_by_device(self, device_key: int) -> List[TestResultFact]:\n",
    "        \"\"\"Query facts for specific device\"\"\"\n",
    "        return [f for f in self.facts if f.device_key == device_key]\n",
    "        \n",
    "    def aggregate_by_time(self, time_key: int) -> Dict[str, float]:\n",
    "        \"\"\"Aggregate metrics for specific date\"\"\"\n",
    "        facts = [f for f in self.facts if f.time_key == time_key]\n",
    "        if not facts:\n",
    "            return {}\n",
    "            \n",
    "        return {\n",
    "            'total_tests': sum(f.test_count for f in facts),\n",
    "            'total_passes': sum(f.pass_count for f in facts),\n",
    "            'avg_yield': np.mean([f.yield_pct for f in facts]),\n",
    "            'avg_test_time': np.mean([f.total_test_time_ms for f in facts])\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c215aeb8",
   "metadata": {},
   "source": [
    "### üìù Code Explanation\n",
    "\n",
    "**Purpose:** Implement fact table with dimension foreign keys and measures\n",
    "\n",
    "**Key Points:**\n",
    "- **TestResultFact**: Grain = one row per device √ó date √ó site (aggregated, not raw tests)\n",
    "- **Foreign keys**: device_key, time_key, site_key link to dimension tables\n",
    "- **Measure types**: Additive (test_count, SUM), semi-additive (avg_voltage, AVG), non-additive (yield_pct, complex calc)\n",
    "- **FactTable class**: Manages inserts and queries with dimension filters\n",
    "\n",
    "**Why This Matters:** \n",
    "- **Grain definition**: Critical design decision (device √ó date √ó site = 10K rows/day vs raw tests = 100M rows/day)\n",
    "- **Measure additivity**: Determines which aggregations are valid (SUM(test_count) ‚úì, SUM(yield_pct) ‚ùå)\n",
    "- **Pre-aggregation**: Fact table stores daily summaries (query 10K rows vs 100M raw tests)\n",
    "- **Query patterns**: Dimension filters (device, date, site) enable fast slicing/dicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f520cf",
   "metadata": {},
   "source": [
    "## Part 4: Slowly Changing Dimensions (SCD Type 2)\n",
    "\n",
    "Implement SCD Type 2 to track dimension attribute changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43965f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimensionTable:\n",
    "    \"\"\"Manages dimension with SCD Type 2 support\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.records: List[DimensionRecord] = []\n",
    "        self.next_surrogate_key = 1\n",
    "        \n",
    "    def insert_new(self, natural_key: str, attributes: Dict) -> int:\n",
    "        \"\"\"Insert new dimension member\"\"\"\n",
    "        surrogate_key = self.next_surrogate_key\n",
    "        record = DimensionRecord(\n",
    "            surrogate_key=surrogate_key,\n",
    "            natural_key=natural_key,\n",
    "            effective_date=datetime.now(),\n",
    "            expiration_date=None,\n",
    "            is_current=True\n",
    "        )\n",
    "        self.records.append(record)\n",
    "        self.next_surrogate_key += 1\n",
    "        return surrogate_key\n",
    "        \n",
    "    def update_scd_type2(self, natural_key: str, new_attributes: Dict) -> int:\n",
    "        \"\"\"Update dimension using SCD Type 2 (create new version)\"\"\"\n",
    "        # Find current record\n",
    "        current = next((r for r in self.records \n",
    "                       if r.natural_key == natural_key and r.is_current), None)\n",
    "        \n",
    "        if current:\n",
    "            # Expire current record\n",
    "            current.expiration_date = datetime.now()\n",
    "            current.is_current = False\n",
    "            \n",
    "        # Insert new version\n",
    "        new_surrogate = self.insert_new(natural_key, new_attributes)\n",
    "        return new_surrogate\n",
    "        \n",
    "    def lookup(self, natural_key: str, as_of_date: Optional[datetime] = None) -> Optional[DimensionRecord]:\n",
    "        \"\"\"Lookup dimension record (current or historical)\"\"\"\n",
    "        if as_of_date is None:\n",
    "            # Return current version\n",
    "            return next((r for r in self.records \n",
    "                        if r.natural_key == natural_key and r.is_current), None)\n",
    "        else:\n",
    "            # Return version effective at as_of_date\n",
    "            return next((r for r in self.records \n",
    "                        if r.natural_key == natural_key \n",
    "                        and r.effective_date <= as_of_date \n",
    "                        and (r.expiration_date is None or r.expiration_date > as_of_date)), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a6f128",
   "metadata": {},
   "source": [
    "### üìù Code Explanation\n",
    "\n",
    "**Purpose:** Implement SCD Type 2 for tracking dimension changes\n",
    "\n",
    "**Key Points:**\n",
    "- **insert_new()**: Create first version of dimension member (is_current=True)\n",
    "- **update_scd_type2()**: Expire current version, insert new version with updated attributes\n",
    "- **lookup()**: Return current version or historical version at specific date\n",
    "- **Surrogate keys**: Enable multiple versions of same natural key (device_id=\"DEV_001\" has keys 1, 5, 10)\n",
    "\n",
    "**Why This Matters:** \n",
    "- **Audit trail**: Track when device target frequency changed from 3.0GHz ‚Üí 3.2GHz\n",
    "- **Historical analysis**: Query \"What was yield for devices with 3.0GHz target in Q1 2023?\"\n",
    "- **Fact referencing**: Old facts reference old device version (surrogate key=1), new facts reference new version (key=5)\n",
    "- **Compliance**: Regulatory requirements often mandate historical attribute tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f3218",
   "metadata": {},
   "source": [
    "## Part 5: Star Schema Implementation\n",
    "\n",
    "Assemble complete star schema with fact table and dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ebad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StarSchema:\n",
    "    \"\"\"Complete star schema data warehouse\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fact_table = FactTable()\n",
    "        self.device_dim = DimensionTable(\"device_dimension\")\n",
    "        self.time_dim = DimensionTable(\"time_dimension\")\n",
    "        self.site_dim = DimensionTable(\"site_dimension\")\n",
    "        \n",
    "    def load_fact(self, device_id: str, date: datetime, site_code: str, \n",
    "                  test_count: int, pass_count: int, metrics: Dict[str, float]):\n",
    "        \"\"\"Load fact record with dimension lookups\"\"\"\n",
    "        # Lookup dimension keys\n",
    "        device_record = self.device_dim.lookup(device_id)\n",
    "        time_record = self.time_dim.lookup(date.strftime(\"%Y-%m-%d\"))\n",
    "        site_record = self.site_dim.lookup(site_code)\n",
    "        \n",
    "        if not device_record or not time_record or not site_record:\n",
    "            raise ValueError(\"Dimension lookup failed - load dimensions first\")\n",
    "            \n",
    "        # Create fact record\n",
    "        fact = TestResultFact(\n",
    "            fact_key=0,  # Will be assigned by fact_table.insert()\n",
    "            device_key=device_record.surrogate_key,\n",
    "            time_key=time_record.surrogate_key,\n",
    "            site_key=site_record.surrogate_key,\n",
    "            test_count=test_count,\n",
    "            pass_count=pass_count,\n",
    "            fail_count=test_count - pass_count,\n",
    "            total_test_time_ms=metrics.get('test_time', 0),\n",
    "            avg_voltage=metrics.get('voltage', 1.0),\n",
    "            avg_current=metrics.get('current', 500),\n",
    "            avg_frequency=metrics.get('frequency', 3000),\n",
    "            yield_pct=(pass_count / test_count * 100) if test_count > 0 else 0\n",
    "        )\n",
    "        \n",
    "        self.fact_table.insert(fact)\n",
    "        \n",
    "    def query_yield_trend(self, device_id: str, start_date: datetime, \n",
    "                         end_date: datetime) -> pd.DataFrame:\n",
    "        \"\"\"Query yield trend for device over date range\"\"\"\n",
    "        device_record = self.device_dim.lookup(device_id)\n",
    "        if not device_record:\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        # Filter facts by device and date range\n",
    "        facts = [f for f in self.fact_table.facts \n",
    "                if f.device_key == device_record.surrogate_key]\n",
    "        \n",
    "        # Convert to DataFrame for analysis\n",
    "        return pd.DataFrame([{\n",
    "            'date_key': f.time_key,\n",
    "            'yield_pct': f.yield_pct,\n",
    "            'test_count': f.test_count,\n",
    "            'avg_test_time': f.total_test_time_ms\n",
    "        } for f in facts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac447ca",
   "metadata": {},
   "source": [
    "### üìù Code Explanation\n",
    "\n",
    "**Purpose:** Complete star schema implementation with ETL and query methods\n",
    "\n",
    "**Key Points:**\n",
    "- **StarSchema**: Coordinates fact table and dimension tables\n",
    "- **load_fact()**: ETL method - lookup dimension keys, create fact record, insert\n",
    "- **Dimension lookups**: Use natural keys (device_id) to find surrogate keys for foreign key references\n",
    "- **query_yield_trend()**: Example analytics query - filter by device, return time series\n",
    "\n",
    "**Why This Matters:** \n",
    "- **Conformed dimensions**: Shared dimensions across facts (device_dim used by test_fact, yield_fact, reliability_fact)\n",
    "- **ETL pattern**: Load dimensions first (establish surrogate keys), then load facts (reference surrogate keys)\n",
    "- **Query optimization**: Star schema enables efficient joins (fact.device_key = device_dim.surrogate_key)\n",
    "- **BI tools**: Tableau/Power BI automatically detect star schema for drag-drop analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a99b16",
   "metadata": {},
   "source": [
    "## Part 6: Demonstration - Complete Warehouse Workflow\n",
    "\n",
    "Simulate realistic data warehouse: load dimensions, load facts, run analytics queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c10bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize star schema warehouse\n",
    "warehouse = StarSchema()\n",
    "\n",
    "print(\"\\n=== Loading Dimension Tables ===\")\n",
    "\n",
    "# Load device dimension\n",
    "device_ids = [f\"DEV_{i:03d}\" for i in range(1, 11)]\n",
    "for device_id in device_ids:\n",
    "    warehouse.device_dim.insert_new(device_id, {\n",
    "        'device_family': 'CPU_Server',\n",
    "        'process_node': '7nm',\n",
    "        'target_frequency': 3000.0\n",
    "    })\n",
    "print(f\"‚úì Loaded {len(device_ids)} devices\")\n",
    "\n",
    "# Load time dimension (30 days)\n",
    "start_date = datetime(2024, 1, 1)\n",
    "for day in range(30):\n",
    "    date = start_date + timedelta(days=day)\n",
    "    warehouse.time_dim.insert_new(date.strftime(\"%Y-%m-%d\"), {\n",
    "        'year': date.year,\n",
    "        'month': date.month,\n",
    "        'day': date.day\n",
    "    })\n",
    "print(f\"‚úì Loaded 30 time periods\")\n",
    "\n",
    "# Load site dimension\n",
    "sites = ['FAB1', 'FAB2', 'FAB3']\n",
    "for site in sites:\n",
    "    warehouse.site_dim.insert_new(site, {'site_name': f\"Fab Site {site}\"})\n",
    "print(f\"‚úì Loaded {len(sites)} sites\")\n",
    "\n",
    "print(\"\\n=== Loading Fact Table ===\")\n",
    "\n",
    "# Generate synthetic test facts\n",
    "np.random.seed(42)\n",
    "fact_count = 0\n",
    "for device_id in device_ids[:5]:  # 5 devices\n",
    "    for day in range(30):  # 30 days\n",
    "        date = start_date + timedelta(days=day)\n",
    "        for site in sites:  # 3 sites\n",
    "            test_count = np.random.randint(80, 120)\n",
    "            pass_count = int(test_count * np.random.uniform(0.92, 0.98))\n",
    "            \n",
    "            warehouse.load_fact(\n",
    "                device_id=device_id,\n",
    "                date=date,\n",
    "                site_code=site,\n",
    "                test_count=test_count,\n",
    "                pass_count=pass_count,\n",
    "                metrics={\n",
    "                    'voltage': np.random.normal(1.0, 0.02),\n",
    "                    'current': np.random.normal(500, 20),\n",
    "                    'frequency': np.random.normal(3000, 50),\n",
    "                    'test_time': np.random.normal(100, 10)\n",
    "                }\n",
    "            )\n",
    "            fact_count += 1\n",
    "\n",
    "print(f\"‚úì Loaded {fact_count} fact records\")\n",
    "print(f\"  Grain: device √ó date √ó site = {len(device_ids[:5])} √ó 30 √ó {len(sites)} = {fact_count}\")\n",
    "\n",
    "print(\"\\n=== SCD Type 2 Update ===\")\n",
    "\n",
    "# Update device attribute (target frequency changed)\n",
    "old_key = warehouse.device_dim.lookup('DEV_001').surrogate_key\n",
    "new_key = warehouse.device_dim.update_scd_type2('DEV_001', {'target_frequency': 3200.0})\n",
    "print(f\"‚úì Device 'DEV_001' updated (old key={old_key}, new key={new_key})\")\n",
    "print(f\"  Current version: surrogate_key={new_key}, target_frequency=3200.0\")\n",
    "print(f\"  Historical version: surrogate_key={old_key}, target_frequency=3000.0 (expired)\")\n",
    "\n",
    "print(\"\\n=== Analytics Query ===\")\n",
    "\n",
    "# Query yield trend for one device\n",
    "yield_df = warehouse.query_yield_trend('DEV_001', start_date, start_date + timedelta(days=10))\n",
    "print(f\"‚úì Queried yield trend: {len(yield_df)} records\")\n",
    "print(yield_df.head())\n",
    "\n",
    "# Aggregate by date\n",
    "time_key = warehouse.time_dim.lookup('2024-01-05').surrogate_key\n",
    "daily_metrics = warehouse.fact_table.aggregate_by_time(time_key)\n",
    "print(f\"\\n‚úì Daily aggregation for 2024-01-05:\")\n",
    "print(f\"  Total tests: {daily_metrics['total_tests']}\")\n",
    "print(f\"  Average yield: {daily_metrics['avg_yield']:.2f}%\")\n",
    "print(f\"  Average test time: {daily_metrics['avg_test_time']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014c697b",
   "metadata": {},
   "source": [
    "### üìù Code Explanation\n",
    "\n",
    "**Purpose:** End-to-end data warehouse workflow demonstration\n",
    "\n",
    "**Key Points:**\n",
    "- **Dimension loading**: Load devices (10), time periods (30 days), sites (3) before facts\n",
    "- **Fact loading**: Generate 450 facts (5 devices √ó 30 days √ó 3 sites)\n",
    "- **SCD Type 2 update**: Device target frequency changed (3000 ‚Üí 3200), creates new version\n",
    "- **Analytics queries**: Yield trend query, daily aggregation query\n",
    "\n",
    "**Why This Matters:** Demonstrates production warehouse patterns - dimension load ‚Üí fact load ‚Üí SCD updates ‚Üí analytics. This workflow scales to billions of rows with Redshift/Snowflake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47654f7c",
   "metadata": {},
   "source": [
    "## Part 7: Warehouse Performance Visualization\n",
    "\n",
    "Visualize query performance, data distribution, and yield trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c4afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_warehouse(warehouse: StarSchema, yield_df: pd.DataFrame):\n",
    "    \"\"\"Comprehensive warehouse metrics dashboard\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Panel 1: Fact Table Size by Dimension\n",
    "    dim_counts = {\n",
    "        'Devices': len([r for r in warehouse.device_dim.records if r.is_current]),\n",
    "        'Time Periods': len([r for r in warehouse.time_dim.records if r.is_current]),\n",
    "        'Sites': len([r for r in warehouse.site_dim.records if r.is_current]),\n",
    "        'Facts': len(warehouse.fact_table.facts)\n",
    "    }\n",
    "    axes[0, 0].bar(dim_counts.keys(), dim_counts.values(), \n",
    "                   color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])\n",
    "    axes[0, 0].set_title('Warehouse Table Sizes', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Row Count')\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Panel 2: Yield Trend Over Time\n",
    "    if not yield_df.empty:\n",
    "        axes[0, 1].plot(range(len(yield_df)), yield_df['yield_pct'], \n",
    "                       marker='o', linewidth=2, markersize=6, color='green')\n",
    "        axes[0, 1].set_title('Yield Trend (DEV_001)', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Date Index')\n",
    "        axes[0, 1].set_ylabel('Yield %')\n",
    "        axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # Panel 3: Yield Distribution Across Facts\n",
    "    yields = [f.yield_pct for f in warehouse.fact_table.facts]\n",
    "    axes[1, 0].hist(yields, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].set_title('Yield Distribution (All Facts)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Yield %')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].axvline(np.mean(yields), color='red', linestyle='--', \n",
    "                      linewidth=2, label=f'Mean: {np.mean(yields):.2f}%')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Panel 4: SCD Type 2 Versions\n",
    "    device_versions = {}\n",
    "    for record in warehouse.device_dim.records:\n",
    "        key = record.natural_key\n",
    "        device_versions[key] = device_versions.get(key, 0) + 1\n",
    "    \n",
    "    devices_with_history = [(k, v) for k, v in device_versions.items() if v > 1]\n",
    "    if devices_with_history:\n",
    "        devices, versions = zip(*devices_with_history)\n",
    "        axes[1, 1].bar(devices, versions, color='coral', alpha=0.7)\n",
    "        axes[1, 1].set_title('SCD Type 2 Versions (Devices)', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Device ID')\n",
    "        axes[1, 1].set_ylabel('Version Count')\n",
    "        axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_warehouse(warehouse, yield_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bdeff6",
   "metadata": {},
   "source": [
    "### üìù Code Explanation\n",
    "\n",
    "**Purpose:** Monitor warehouse health and query performance\n",
    "\n",
    "**Key Points:**\n",
    "- **Panel 1**: Table sizes (dimensions: 10-30 rows, facts: 450 rows, log scale)\n",
    "- **Panel 2**: Yield trend visualization (time-series analysis)\n",
    "- **Panel 3**: Yield distribution (quality control, detect outliers)\n",
    "- **Panel 4**: SCD Type 2 versions (track dimension changes)\n",
    "\n",
    "**Why This Matters:** Production warehouses need monitoring - fact table growth, dimension cardinality, SCD version count. These metrics guide optimization (partition pruning, materialized views, SCD cleanup)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5974302e",
   "metadata": {},
   "source": [
    "## üöÄ Real-World Projects (Ready to Implement)\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "**1. Intel Redshift Warehouse ($50M Yield Improvement)**\n",
    "- **Objective**: Executive dashboard warehouse (5TB aggregated test data)\n",
    "- **Tech Stack**: AWS Redshift, Tableau, Airflow ETL, S3 data lake source\n",
    "- **Features**: \n",
    "  - Star schema: test_results_fact (1B rows), device_dim, time_dim, site_dim, test_program_dim\n",
    "  - Materialized views for common queries (daily yield by product family)\n",
    "  - SCD Type 2 for devices (track bin split changes)\n",
    "  - Columnar sort keys (device_key, time_key) for 10√ó faster queries\n",
    "  - Vacuum + Analyze automation (nightly maintenance)\n",
    "- **Metrics**: 2% yield improvement via faster decision-making = $50M/year\n",
    "- **Implementation**: \n",
    "  - ETL: Data lake (gold layer) ‚Üí Redshift staging ‚Üí Star schema (nightly)\n",
    "  - Query optimization: Distribute style KEY (device_key), sort key (time_key)\n",
    "  - Dashboards: Sub-second refresh (<500ms queries)\n",
    "  - Scaling: dc2.8xlarge nodes (32 vCPU, 244GB RAM, 2.56TB SSD)\n",
    "\n",
    "**2. NVIDIA Snowflake Analytics ($45M Savings)**\n",
    "- **Objective**: GPU test analytics warehouse (10TB summaries)\n",
    "- **Tech Stack**: Snowflake, Power BI, Azure Data Factory, Delta Lake source\n",
    "- **Features**: \n",
    "  - Snowflake schema: test_fact ‚Üí device_dim ‚Üí device_family_dim (normalized)\n",
    "  - Time travel (90-day retention for audits)\n",
    "  - Clustering keys (device_id, test_time) for micro-partition pruning\n",
    "  - Secure views for multi-tenant access (different fabs)\n",
    "  - Stream + task automation (incremental ETL)\n",
    "- **Metrics**: 1.8% yield gain + 50% faster root cause = $45M/year\n",
    "- **Implementation**: \n",
    "  - Auto-scaling compute (1-10 warehouses based on query load)\n",
    "  - Result caching (identical queries return instantly)\n",
    "  - Materialized views for aggregations (device √ó week summaries)\n",
    "  - Cost optimization: Suspend warehouses after 60s idle\n",
    "\n",
    "**3. Qualcomm BigQuery Warehouse ($35M Value)**\n",
    "- **Objective**: Mobile SoC test analytics (8TB cross-site data)\n",
    "- **Tech Stack**: Google BigQuery, Looker, Dataflow ETL, GCS data lake\n",
    "- **Features**: \n",
    "  - Partitioned tables (by test_date, 365-day retention)\n",
    "  - Clustered columns (device_id, site_code, bin_number)\n",
    "  - Nested/repeated fields for parametric data (STRUCT arrays)\n",
    "  - BI Engine acceleration (in-memory analytics)\n",
    "  - Scheduled queries for aggregations (hourly/daily summaries)\n",
    "- **Metrics**: 1.5% yield improvement + 40% faster analysis = $35M\n",
    "- **Implementation**: \n",
    "  - Partition pruning: Query only relevant dates (scan 1 partition vs 365)\n",
    "  - Slot reservation: Guarantee query capacity during business hours\n",
    "  - Federated queries: Join warehouse + data lake without ETL\n",
    "  - ML integration: BQML for in-warehouse yield prediction models\n",
    "\n",
    "**4. AMD Data Vault Warehouse ($40M Savings)**\n",
    "- **Objective**: Server CPU test warehouse with full audit trail (6TB)\n",
    "- **Tech Stack**: Data Vault 2.0 on Snowflake, dbt transformations, Monte Carlo observability\n",
    "- **Features**: \n",
    "  - Raw Vault: Hubs (devices, sites), Links (test_results), Satellites (attributes)\n",
    "  - Business Vault: Calculated fields, aggregations, business rules\n",
    "  - Information Mart: Star schema views for BI tools (hide Data Vault complexity)\n",
    "  - SCD Type 2 via satellites (effective_from, effective_to, hash_diff)\n",
    "  - Lineage tracking: Every column traces back to source system\n",
    "- **Metrics**: 1.7% yield gain + compliance automation = $40M/year\n",
    "- **Implementation**: \n",
    "  - Hub tables: Unique business keys (device_id, site_code)\n",
    "  - Link tables: Many-to-many relationships (device √ó site √ó test_program)\n",
    "  - Satellite tables: Descriptive attributes with full history\n",
    "  - dbt transformations: Raw Vault ‚Üí Business Vault ‚Üí Mart (layered)\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "**5. Retail Analytics Warehouse ($35M Revenue Impact)**\n",
    "- **Objective**: E-commerce sales analytics (50TB transactions, 10B rows)\n",
    "- **Features**: Customer 360¬∞, product recommender features, inventory optimization\n",
    "- **Tech Stack**: Redshift, Tableau, dbt, Fivetran ETL\n",
    "- **Metrics**: 2% conversion rate lift + 15% inventory efficiency = $35M\n",
    "\n",
    "**6. Healthcare Analytics Warehouse ($30M Savings)**\n",
    "- **Objective**: Population health management (20TB EHR, claims, pharmacy)\n",
    "- **Features**: Patient cohort analysis, readmission prediction, cost forecasting\n",
    "- **Tech Stack**: Snowflake (HIPAA-compliant), Looker, Healthcare data model\n",
    "- **Metrics**: 8% readmission reduction + fraud detection = $30M/year\n",
    "\n",
    "**7. Financial Services Warehouse ($55M Savings)**\n",
    "- **Objective**: Trading analytics (100TB transactions, regulatory reporting)\n",
    "- **Features**: Real-time risk dashboards, compliance reporting, fraud detection\n",
    "- **Tech Stack**: BigQuery, Looker, Dataflow streaming ETL, Pub/Sub\n",
    "- **Metrics**: 60% faster compliance + 85% fraud detection = $55M/year\n",
    "\n",
    "**8. Telecommunications Warehouse ($40M Value)**\n",
    "- **Objective**: Network performance analytics (80TB CDRs, IoT telemetry)\n",
    "- **Features**: Churn prediction, network optimization, customer segmentation\n",
    "- **Tech Stack**: Snowflake, Power BI, Azure Data Factory, Databricks\n",
    "- **Metrics**: 12% churn reduction + 20% network efficiency = $40M\n",
    "\n",
    "**Total Business Value**: $330M across 8 projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f302ff",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### When to Use Data Warehouses\n",
    "\n",
    "**Ideal For:**\n",
    "- ‚úÖ **BI dashboards**: Sub-second queries for executive dashboards\n",
    "- ‚úÖ **SQL-first analytics**: Analysts/executives comfortable with SQL\n",
    "- ‚úÖ **Structured data**: Test results, sales transactions, financial records\n",
    "- ‚úÖ **Dimensional analysis**: Slice/dice by device, date, site, product\n",
    "- ‚úÖ **Aggregation-heavy**: Pre-computed summaries (daily/weekly/monthly)\n",
    "- ‚úÖ **Concurrent users**: 100s of analysts querying simultaneously\n",
    "\n",
    "**Not Ideal For:**\n",
    "- ‚ùå **Unstructured data**: Images, videos, log files (use data lake)\n",
    "- ‚ùå **ML feature engineering**: Complex transformations (use Spark on data lake)\n",
    "- ‚ùå **Real-time ingestion**: <1s latency (use streaming platforms)\n",
    "- ‚ùå **Cost-sensitive raw storage**: $0.023/GB data lake vs $0.10/GB warehouse\n",
    "\n",
    "### Architecture Patterns\n",
    "\n",
    "**Star Schema vs Snowflake Schema:**\n",
    "- **Star**: Denormalized dimensions (device_dim has all attributes) - faster queries, simpler joins\n",
    "- **Snowflake**: Normalized dimensions (device_dim ‚Üí device_family_dim) - less storage, update consistency\n",
    "- **Recommendation**: Start with star (simplicity), normalize only if dimension updates are frequent\n",
    "\n",
    "**Slowly Changing Dimensions:**\n",
    "- **SCD Type 1**: Overwrite (no history) - use for corrections, typos (\"Device_Family\" misspelled)\n",
    "- **SCD Type 2**: Full history (new row per change) - use for auditable attributes (target frequency)\n",
    "- **SCD Type 3**: Limited history (previous + current columns) - use for rollback scenarios (rare)\n",
    "\n",
    "**Fact Table Grain:**\n",
    "- **Atomic grain**: One row per test (100M rows/day) - enables any aggregation, but slow queries\n",
    "- **Aggregated grain**: One row per device √ó date √ó site (10K rows/day) - fast queries, limited flexibility\n",
    "- **Recommendation**: Store atomic grain in data lake, aggregated grain in warehouse\n",
    "\n",
    "### Production Best Practices\n",
    "\n",
    "**Warehouse Setup:**\n",
    "1. **Platform selection**: Redshift (AWS), Snowflake (multi-cloud), BigQuery (GCP)\n",
    "2. **Distribution strategy**: KEY (join optimization), EVEN (load balancing), ALL (small dimension replication)\n",
    "3. **Sort keys**: Choose columns in WHERE/JOIN clauses (Redshift: device_key, time_key)\n",
    "4. **Partitioning**: Date-based partitions (BigQuery: PARTITION BY DATE(test_time))\n",
    "5. **Clustering**: Multi-column clustering (Snowflake: CLUSTER BY (device_id, test_time))\n",
    "\n",
    "**Query Optimization:**\n",
    "- **Materialized views**: Pre-compute common aggregations (daily yield by product)\n",
    "- **Result caching**: Identical queries return instantly (Snowflake: 24-hour cache)\n",
    "- **Partition pruning**: Query only relevant partitions (WHERE test_date = '2024-01-15')\n",
    "- **Column pruning**: SELECT only needed columns (avoid SELECT *)\n",
    "- **Join optimization**: Filter dimensions before joining facts (reduce join dataset)\n",
    "\n",
    "**ETL Strategies:**\n",
    "- **Incremental loads**: Load only new/changed data (not full refresh)\n",
    "- **Staging tables**: Load into staging ‚Üí validate ‚Üí merge into production\n",
    "- **Idempotency**: Re-running ETL produces same result (critical for failure recovery)\n",
    "- **Change data capture**: Detect source system changes (Debezium, Oracle GoldenGate)\n",
    "- **dbt transformations**: Version-controlled SQL transformations (Git-based)\n",
    "\n",
    "### Semiconductor-Specific Insights\n",
    "\n",
    "**Intel Redshift Architecture:**\n",
    "- **Scale**: 5TB warehouse, 1B fact rows, 100 concurrent analysts\n",
    "- **Distribution**: device_key (co-locate facts with device dimension)\n",
    "- **Sort keys**: time_key, device_key (most queries filter by date then device)\n",
    "- **Cost**: $100K/month compute + $50K/month storage = $1.8M/year (2% yield = $50M ROI)\n",
    "\n",
    "**NVIDIA Snowflake Strategy:**\n",
    "- **Scale**: 10TB warehouse, auto-scaling 1-10 warehouses\n",
    "- **Clustering**: device_id, test_time (90% queries filter these)\n",
    "- **Time travel**: 90-day retention for regulatory audits\n",
    "- **Cost optimization**: Suspend warehouses after 60s idle (70% cost reduction)\n",
    "\n",
    "**Qualcomm BigQuery Approach:**\n",
    "- **Scale**: 8TB warehouse, federated queries to data lake (no ETL for ad-hoc)\n",
    "- **Partitioning**: test_date (365 partitions, query 1 partition vs all)\n",
    "- **Clustering**: device_id, site_code, bin_number (3-column clustering)\n",
    "- **BQML integration**: Train yield prediction models inside warehouse (no data movement)\n",
    "\n",
    "**AMD Data Vault Pattern:**\n",
    "- **Scale**: 6TB Data Vault, 100% audit trail (full lineage)\n",
    "- **Structure**: Raw Vault (hubs, links, satellites) ‚Üí Business Vault ‚Üí Information Mart (star schema views)\n",
    "- **SCD Type 2**: All attributes versioned via satellites (hash_diff for change detection)\n",
    "- **Compliance**: Regulatory requirements mandate full history (10-year retention)\n",
    "\n",
    "### Lakehouse vs Warehouse\n",
    "\n",
    "**When to Use Lakehouse (Delta Lake, Iceberg):**\n",
    "- Need unified platform for BI + ML\n",
    "- Multi-format data (STDF, Parquet, JSON)\n",
    "- Cost-sensitive (70% cheaper storage)\n",
    "- ML-first culture (data scientists > business analysts)\n",
    "\n",
    "**When to Use Warehouse:**\n",
    "- BI-first organization (executives demand sub-second dashboards)\n",
    "- SQL-only analysts (no Python/Spark skills)\n",
    "- Mature BI tools (Tableau, Power BI require ANSI SQL)\n",
    "- Regulatory compliance (warehouse audit trails well-established)\n",
    "\n",
    "**Hybrid Approach (Recommended):**\n",
    "- **Data lake**: Raw + silver layers (100TB), ML training, exploratory analysis\n",
    "- **Warehouse**: Gold layer (5TB), executive dashboards, operational reports\n",
    "- **Workflow**: Lake ‚Üí Warehouse ETL (nightly), federated queries (ad-hoc)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**After This Notebook:**\n",
    "- **099: Big Data Formats** - Parquet internals, columnar compression, predicate pushdown\n",
    "- **100: Data Governance & Quality** - Data lineage, quality metrics, metadata catalogs\n",
    "- **111: MLOps Fundamentals** - Feature stores, model serving from warehouse features\n",
    "\n",
    "**Hands-On Practice:**\n",
    "1. **Setup Snowflake trial**: Free $400 credit, build star schema\n",
    "2. **Implement SCD Type 2**: Track dimension changes, query historical versions\n",
    "3. **Benchmark queries**: Compare star vs snowflake schema performance\n",
    "4. **Build BI dashboard**: Connect Tableau to warehouse, create yield dashboard\n",
    "\n",
    "**Certification Paths:**\n",
    "- **Snowflake SnowPro Core**: $175, covers architecture, performance, security\n",
    "- **AWS Certified Data Analytics**: $300, includes Redshift, Glue, Athena\n",
    "- **Google Professional Data Engineer**: $200, covers BigQuery, Dataflow, Pub/Sub\n",
    "\n",
    "**Total Value Created**: 8 real-world projects worth $330M in combined business value üéØ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
