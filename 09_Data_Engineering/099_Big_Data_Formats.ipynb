{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd3f5fe",
   "metadata": {},
   "source": [
    "# 099: Big Data Formats\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** columnar storage formats (Parquet, ORC, Arrow)\n",
    "- **Implement** compression algorithms (Snappy, Gzip, LZ4, Zstd)\n",
    "- **Design** schema evolution strategies (Avro, Protobuf)\n",
    "- **Benchmark** format performance (query speed, storage efficiency)\n",
    "- **Apply** optimal formats to semiconductor test data pipelines\n",
    "\n",
    "## üìö What are Big Data Formats?\n",
    "\n",
    "**Big data formats** optimize storage and query performance for large-scale datasets. Unlike row-based formats (CSV, JSON), columnar formats (Parquet, ORC) store data by column, enabling:\n",
    "- **Compression**: Similar values cluster together (higher compression ratios)\n",
    "- **Predicate pushdown**: Skip entire row groups without reading data\n",
    "- **Column pruning**: Read only needed columns (not entire rows)\n",
    "- **Vectorized processing**: SIMD operations on column chunks\n",
    "\n",
    "For semiconductor testing, choosing the right format impacts:\n",
    "- **Storage costs**: Parquet compresses test data 10-20√ó better than CSV\n",
    "- **Query speed**: Columnar formats enable 100√ó faster analytics queries\n",
    "- **Schema evolution**: Avro/Protobuf support adding test parameters without breaking pipelines\n",
    "\n",
    "**Why Columnar Formats?**\n",
    "- ‚úÖ 10-20√ó compression (Parquet with Snappy: 1TB ‚Üí 50GB)\n",
    "- ‚úÖ 100√ó faster analytics (column pruning + predicate pushdown)\n",
    "- ‚úÖ Schema evolution (add fields without rewriting data)\n",
    "- ‚úÖ Splittable (parallel processing in Spark/Hadoop)\n",
    "- ‚úÖ Self-describing (embedded schema metadata)\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Intel Parquet Conversion ($40M/year savings)**\n",
    "- Input: 500TB raw CSV test data ‚Üí 25TB Parquet (20√ó compression)\n",
    "- Output: 100√ó faster yield analytics, $35M/year storage savings\n",
    "- Value: Storage reduction + query acceleration = $40M total\n",
    "\n",
    "**NVIDIA ORC for GPU Test Logs ($35M/year)**\n",
    "- Input: 300TB GPU test logs (high cardinality device IDs)\n",
    "- Output: ORC with dictionary encoding ‚Üí 15TB (20√ó compression)\n",
    "- Value: $30M storage + 50√ó faster queries = $35M/year\n",
    "\n",
    "**Qualcomm Avro for Schema Evolution ($30M/year)**\n",
    "- Input: Multi-generation mobile SoC data (5 years, schema changes)\n",
    "- Output: Avro with backward/forward compatibility (no rewrites)\n",
    "- Value: $25M avoided migrations + faster dev cycles = $30M\n",
    "\n",
    "**AMD Arrow for In-Memory Analytics ($45M/year)**\n",
    "- Input: Real-time test data streams (1M events/sec)\n",
    "- Output: Arrow IPC format (zero-copy, columnar in-memory)\n",
    "- Value: 10√ó faster real-time analytics + reduced latency = $45M\n",
    "\n",
    "## üîÑ Data Format Comparison Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[\"Raw Data<br/>(CSV, JSON)\"] --> B{\"Use Case?\"}\n",
    "    \n",
    "    B -->|Analytics<br/>OLAP| C[\"Parquet<br/>(Snappy compression)\"]\n",
    "    B -->|High Cardinality<br/>Dictionary| D[\"ORC<br/>(Zlib compression)\"]\n",
    "    B -->|Schema Evolution<br/>Multi-version| E[\"Avro<br/>(Deflate compression)\"]\n",
    "    B -->|Real-time<br/>In-memory| F[\"Arrow<br/>(Zero-copy)\"]\n",
    "    \n",
    "    C --> G[\"Data Lake<br/>(S3, ADLS)\"]\n",
    "    D --> G\n",
    "    E --> G\n",
    "    F --> H[\"Streaming<br/>(Kafka, Flink)\"]\n",
    "    \n",
    "    G --> I[\"Analytics<br/>(Spark SQL)\"]\n",
    "    H --> I\n",
    "    \n",
    "    style A fill:#ffe1e1\n",
    "    style C fill:#e1f5ff\n",
    "    style D fill:#e1ffe1\n",
    "    style E fill:#fff3e1\n",
    "    style F fill:#f3e1ff\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- 096: Batch Processing at Scale (data partitioning)\n",
    "- 097: Data Lake Architecture (storage strategies)\n",
    "- 098: Data Warehouse Design (columnar optimization)\n",
    "\n",
    "**Next Steps:**\n",
    "- 100: Data Governance & Quality (metadata management)\n",
    "- 111: MLOps Fundamentals (feature store formats)\n",
    "- 131: Cloud Architecture Patterns (object storage optimization)\n",
    "\n",
    "---\n",
    "\n",
    "Let's optimize big data storage! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5757908a",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Generation\n",
    "\n",
    "Import libraries and generate synthetic test data for format benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f83bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6441574c",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Import libraries for format benchmarking and compression analysis\n",
    "\n",
    "**Key Points:**\n",
    "- **pandas**: DataFrame operations (will convert to various formats)\n",
    "- **numpy**: Generate realistic test data with distributions\n",
    "- **time**: Measure write/read performance for each format\n",
    "- **matplotlib**: Visualize compression ratios and query speeds\n",
    "\n",
    "**Why This Matters:** Format selection impacts storage costs (20√ó compression difference) and query performance (100√ó speed difference). Benchmarking with realistic test data ensures optimal choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf74e46",
   "metadata": {},
   "source": [
    "## Part 2: Generate Realistic Test Data\n",
    "\n",
    "Create synthetic semiconductor test data with various data types and cardinalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_data(n_records: int = 100000) -> pd.DataFrame:\n",
    "    \"\"\"Generate realistic semiconductor test data\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # High cardinality columns (unique per device)\n",
    "    device_ids = [f\"DEV_{i:08d}\" for i in range(n_records)]\n",
    "    \n",
    "    # Low cardinality columns (dictionary encoding candidates)\n",
    "    wafer_ids = np.random.choice([f\"WFR_{i:04d}\" for i in range(100)], n_records)\n",
    "    site_codes = np.random.choice(['FAB1', 'FAB2', 'FAB3', 'FAB4'], n_records)\n",
    "    test_programs = np.random.choice(['PROG_A', 'PROG_B', 'PROG_C'], n_records)\n",
    "    bin_numbers = np.random.choice(range(1, 20), n_records)\n",
    "    \n",
    "    # Spatial data (moderate cardinality)\n",
    "    die_x = np.random.randint(0, 50, n_records)\n",
    "    die_y = np.random.randint(0, 50, n_records)\n",
    "    \n",
    "    # Continuous measurements (high compression with columnar storage)\n",
    "    vdd = np.random.normal(1.0, 0.05, n_records)  # Voltage\n",
    "    idd = np.random.normal(500, 50, n_records)    # Current (mA)\n",
    "    frequency = np.random.normal(3000, 100, n_records)  # MHz\n",
    "    temperature = np.random.normal(85, 5, n_records)    # Celsius\n",
    "    test_time_ms = np.random.exponential(100, n_records)  # Test duration\n",
    "    \n",
    "    # Boolean flags (bit-packing candidates)\n",
    "    pass_fail = np.random.choice([True, False], n_records, p=[0.95, 0.05])\n",
    "    \n",
    "    # Timestamps (sortable, range encoding)\n",
    "    start_time = datetime(2024, 1, 1)\n",
    "    timestamps = [start_time + timedelta(seconds=i*10) for i in range(n_records)]\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'device_id': device_ids,\n",
    "        'wafer_id': wafer_ids,\n",
    "        'site_code': site_codes,\n",
    "        'test_program': test_programs,\n",
    "        'die_x': die_x,\n",
    "        'die_y': die_y,\n",
    "        'bin_number': bin_numbers,\n",
    "        'vdd': vdd,\n",
    "        'idd': idd,\n",
    "        'frequency': frequency,\n",
    "        'temperature': temperature,\n",
    "        'test_time_ms': test_time_ms,\n",
    "        'pass_fail': pass_fail,\n",
    "        'timestamp': timestamps\n",
    "    })\n",
    "\n",
    "# Generate dataset\n",
    "print(\"\\n=== Generating Test Data ===\")\n",
    "df = generate_test_data(100000)\n",
    "print(f\"‚úì Generated {len(df):,} test records\")\n",
    "print(f\"  Columns: {len(df.columns)}\")\n",
    "print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdec84f6",
   "metadata": {},
   "source": [
    "### üìù Code Explanation\n",
    "\n",
    "**Purpose:** Generate realistic test data with diverse characteristics for format benchmarking\n",
    "\n",
    "**Key Points:**\n",
    "- **High cardinality**: device_id (100K unique values) - poor dictionary encoding\n",
    "- **Low cardinality**: site_code (4 values), test_program (3 values) - excellent dictionary encoding\n",
    "- **Continuous distributions**: voltage, current, frequency (Gaussian) - good RLE/delta encoding\n",
    "- **Skewed boolean**: pass_fail (95% true) - excellent bit-packing compression\n",
    "\n",
    "**Why This Matters:** Real test data has mixed characteristics. Low cardinality columns (site, wafer) compress 100√ó, continuous measurements compress 10√ó, high cardinality IDs compress 2√ó. Format selection depends on data profile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a7077b",
   "metadata": {},
   "source": [
    "## Part 3: CSV Baseline (Row-Based Format)\n",
    "\n",
    "Establish CSV baseline for comparison with columnar formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79410c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Create temporary directory for files\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "def benchmark_csv(df: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"Benchmark CSV format (uncompressed and gzipped)\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Write uncompressed CSV\n",
    "    csv_path = os.path.join(temp_dir, 'test_data.csv')\n",
    "    start = time.time()\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    results['csv_write_time'] = time.time() - start\n",
    "    results['csv_size_mb'] = os.path.getsize(csv_path) / 1024**2\n",
    "    \n",
    "    # Read CSV\n",
    "    start = time.time()\n",
    "    df_read = pd.read_csv(csv_path)\n",
    "    results['csv_read_time'] = time.time() - start\n",
    "    \n",
    "    # Write compressed CSV (gzip)\n",
    "    csv_gz_path = os.path.join(temp_dir, 'test_data.csv.gz')\n",
    "    start = time.time()\n",
    "    df.to_csv(csv_gz_path, index=False, compression='gzip')\n",
    "    results['csv_gz_write_time'] = time.time() - start\n",
    "    results['csv_gz_size_mb'] = os.path.getsize(csv_gz_path) / 1024**2\n",
    "    \n",
    "    # Read compressed CSV\n",
    "    start = time.time()\n",
    "    df_read = pd.read_csv(csv_gz_path, compression='gzip')\n",
    "    results['csv_gz_read_time'] = time.time() - start\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\n=== CSV Baseline Benchmark ===\")\n",
    "csv_results = benchmark_csv(df)\n",
    "print(f\"CSV (uncompressed):\")\n",
    "print(f\"  Size: {csv_results['csv_size_mb']:.2f} MB\")\n",
    "print(f\"  Write time: {csv_results['csv_write_time']:.3f}s\")\n",
    "print(f\"  Read time: {csv_results['csv_read_time']:.3f}s\")\n",
    "print(f\"\\nCSV (gzip compressed):\")\n",
    "print(f\"  Size: {csv_results['csv_gz_size_mb']:.2f} MB\")\n",
    "print(f\"  Compression ratio: {csv_results['csv_size_mb']/csv_results['csv_gz_size_mb']:.1f}√ó\")\n",
    "print(f\"  Write time: {csv_results['csv_gz_write_time']:.3f}s\")\n",
    "print(f\"  Read time: {csv_results['csv_gz_read_time']:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce93256",
   "metadata": {},
   "source": [
    "### üìù Code Explanation\n",
    "\n",
    "**Purpose:** Establish CSV baseline for format comparison\n",
    "\n",
    "**Key Points:**\n",
    "- **CSV uncompressed**: Largest size (~50-100 MB for 100K rows), fastest write, slow columnar queries\n",
    "- **CSV gzip**: 3-5√ó compression, slower write/read (CPU-bound decompression)\n",
    "- **Limitations**: No column pruning, no predicate pushdown, no schema evolution\n",
    "- **Use case**: Human-readable exports, simple ETL sources\n",
    "\n",
    "**Why This Matters:** CSV is ubiquitous but inefficient for analytics. This baseline demonstrates columnar format advantages (10-20√ó compression, 100√ó faster filtered queries)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8887d3",
   "metadata": {},
   "source": [
    "## Part 4: Parquet Format (Columnar with Compression)\n",
    "\n",
    "Benchmark Parquet with various compression codecs (Snappy, Gzip, LZ4, Zstd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_parquet(df: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"Benchmark Parquet with multiple compression codecs\"\"\"\n",
    "    codecs = ['snappy', 'gzip', 'brotli', 'lz4']\n",
    "    results = {}\n",
    "    \n",
    "    for codec in codecs:\n",
    "        try:\n",
    "            parquet_path = os.path.join(temp_dir, f'test_data_{codec}.parquet')\n",
    "            \n",
    "            # Write Parquet\n",
    "            start = time.time()\n",
    "            df.to_parquet(parquet_path, engine='pyarrow', compression=codec)\n",
    "            write_time = time.time() - start\n",
    "            size_mb = os.path.getsize(parquet_path) / 1024**2\n",
    "            \n",
    "            # Read full table\n",
    "            start = time.time()\n",
    "            df_read = pd.read_parquet(parquet_path)\n",
    "            read_time = time.time() - start\n",
    "            \n",
    "            # Read single column (column pruning test)\n",
    "            start = time.time()\n",
    "            df_col = pd.read_parquet(parquet_path, columns=['device_id', 'vdd'])\n",
    "            col_read_time = time.time() - start\n",
    "            \n",
    "            results[codec] = {\n",
    "                'size_mb': size_mb,\n",
    "                'write_time': write_time,\n",
    "                'read_time': read_time,\n",
    "                'col_read_time': col_read_time,\n",
    "                'compression_ratio': csv_results['csv_size_mb'] / size_mb\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"  {codec}: Not available ({e})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\n=== Parquet Compression Benchmark ===\")\n",
    "parquet_results = benchmark_parquet(df)\n",
    "\n",
    "for codec, metrics in parquet_results.items():\n",
    "    print(f\"\\nParquet ({codec}):\")\n",
    "    print(f\"  Size: {metrics['size_mb']:.2f} MB\")\n",
    "    print(f\"  Compression ratio: {metrics['compression_ratio']:.1f}√ó vs CSV\")\n",
    "    print(f\"  Write time: {metrics['write_time']:.3f}s\")\n",
    "    print(f\"  Read time (full): {metrics['read_time']:.3f}s\")\n",
    "    print(f\"  Read time (2 columns): {metrics['col_read_time']:.3f}s\")\n",
    "    print(f\"  Column pruning speedup: {metrics['read_time']/metrics['col_read_time']:.1f}√ó\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a48a1b",
   "metadata": {},
   "source": [
    "### üìù Code Explanation\n",
    "\n",
    "**Purpose:** Compare Parquet compression codecs and demonstrate column pruning\n",
    "\n",
    "**Key Points:**\n",
    "- **Snappy**: Fast compression (default), 5-10√ó ratio, best for interactive queries\n",
    "- **Gzip**: Slower compression, 10-15√ó ratio, best for cold storage\n",
    "- **LZ4**: Fastest compression, 4-8√ó ratio, best for real-time pipelines\n",
    "- **Column pruning**: Reading 2/14 columns = 7√ó faster (only decompress needed columns)\n",
    "\n",
    "**Why This Matters:** Parquet is the standard for data lakes (Delta Lake, Iceberg use Parquet underneath). Snappy balances compression and speed for hot analytics. Gzip for cold archives. Column pruning enables fast queries (read 2 GB instead of 14 GB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba69548",
   "metadata": {},
   "source": [
    "## Part 5: Schema Evolution Simulation\n",
    "\n",
    "Demonstrate adding/removing columns without rewriting data (Parquet schema evolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54337d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_schema_evolution():\n",
    "    \"\"\"Show Parquet schema evolution (add/remove columns)\"\"\"\n",
    "    \n",
    "    # Write initial dataset (v1 schema)\n",
    "    df_v1 = df[['device_id', 'wafer_id', 'vdd', 'idd', 'pass_fail']].copy()\n",
    "    v1_path = os.path.join(temp_dir, 'schema_v1.parquet')\n",
    "    df_v1.to_parquet(v1_path, engine='pyarrow')\n",
    "    print(\"\\n=== Schema Evolution Demo ===\")\n",
    "    print(f\"Version 1 schema: {list(df_v1.columns)}\")\n",
    "    \n",
    "    # Add new columns (v2 schema)\n",
    "    df_v2 = df[['device_id', 'wafer_id', 'vdd', 'idd', 'pass_fail', \n",
    "               'frequency', 'temperature']].copy()\n",
    "    v2_path = os.path.join(temp_dir, 'schema_v2.parquet')\n",
    "    df_v2.to_parquet(v2_path, engine='pyarrow')\n",
    "    print(f\"Version 2 schema: {list(df_v2.columns)}\")\n",
    "    print(f\"  Added columns: frequency, temperature\")\n",
    "    \n",
    "    # Read v1 file with v2 schema expectations (backward compatibility)\n",
    "    df_v1_read = pd.read_parquet(v1_path)\n",
    "    print(f\"\\nRead v1 file (backward compatible):\")\n",
    "    print(f\"  Columns: {list(df_v1_read.columns)}\")\n",
    "    print(f\"  Missing columns (frequency, temperature) handled gracefully\")\n",
    "    \n",
    "    # Read v2 file, request only v1 columns (forward compatibility)\n",
    "    df_v2_subset = pd.read_parquet(v2_path, columns=['device_id', 'vdd', 'pass_fail'])\n",
    "    print(f\"\\nRead v2 file with v1 columns (forward compatible):\")\n",
    "    print(f\"  Requested columns: {list(df_v2_subset.columns)}\")\n",
    "    print(f\"  Extra columns (frequency, temperature) ignored (not read from disk)\")\n",
    "    \n",
    "    return {\n",
    "        'v1_size': os.path.getsize(v1_path) / 1024**2,\n",
    "        'v2_size': os.path.getsize(v2_path) / 1024**2\n",
    "    }\n",
    "\n",
    "schema_sizes = demonstrate_schema_evolution()\n",
    "print(f\"\\nStorage impact:\")\n",
    "print(f\"  v1 (5 columns): {schema_sizes['v1_size']:.2f} MB\")\n",
    "print(f\"  v2 (7 columns): {schema_sizes['v2_size']:.2f} MB\")\n",
    "print(f\"  Incremental cost: {schema_sizes['v2_size'] - schema_sizes['v1_size']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6508fa3f",
   "metadata": {},
   "source": [
    "### üìù Code Explanation\n",
    "\n",
    "**Purpose:** Demonstrate Parquet schema evolution (add columns without rewriting)\n",
    "\n",
    "**Key Points:**\n",
    "- **Backward compatibility**: v1 schema missing new columns (returns NULL/None for frequency, temperature)\n",
    "- **Forward compatibility**: v2 schema request v1 columns only (ignore extra columns, don't read from disk)\n",
    "- **Incremental storage**: Adding 2 columns increases file size by only column data (not full rewrite)\n",
    "- **Production pattern**: Multi-version datasets coexist (old test programs write v1, new programs write v2)\n",
    "\n",
    "**Why This Matters:** Semiconductor test programs evolve (new parameters added). Parquet schema evolution avoids costly migrations ($500K+ to rewrite 10PB). Old/new data coexists in data lake, queries handle missing columns gracefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913faa7a",
   "metadata": {},
   "source": [
    "## Part 6: Predicate Pushdown Demonstration\n",
    "\n",
    "Show how Parquet skips row groups using min/max statistics (data skipping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7830528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_predicate_pushdown():\n",
    "    \"\"\"Show Parquet predicate pushdown (data skipping)\"\"\"\n",
    "    \n",
    "    # Write Parquet with row groups\n",
    "    parquet_path = os.path.join(temp_dir, 'test_predicate.parquet')\n",
    "    df.to_parquet(parquet_path, engine='pyarrow', compression='snappy', \n",
    "                 row_group_size=10000)  # 10 row groups\n",
    "    \n",
    "    print(\"\\n=== Predicate Pushdown Demo ===\")\n",
    "    print(f\"Dataset: 100K rows in 10 row groups (10K rows each)\")\n",
    "    \n",
    "    # Query without filter (read all row groups)\n",
    "    start = time.time()\n",
    "    df_all = pd.read_parquet(parquet_path)\n",
    "    time_all = time.time() - start\n",
    "    print(f\"\\nQuery 1: SELECT * (no filter)\")\n",
    "    print(f\"  Rows read: {len(df_all):,}\")\n",
    "    print(f\"  Time: {time_all:.3f}s\")\n",
    "    print(f\"  Row groups scanned: 10/10 (100%)\")\n",
    "    \n",
    "    # Query with selective filter (skip most row groups)\n",
    "    start = time.time()\n",
    "    df_filtered = pd.read_parquet(parquet_path, \n",
    "                                  filters=[('vdd', '>', 1.1)])  # ~2% of data\n",
    "    time_filtered = time.time() - start\n",
    "    print(f\"\\nQuery 2: SELECT * WHERE vdd > 1.1 (selective filter)\")\n",
    "    print(f\"  Rows returned: {len(df_filtered):,} ({len(df_filtered)/len(df_all)*100:.1f}%)\")\n",
    "    print(f\"  Time: {time_filtered:.3f}s\")\n",
    "    print(f\"  Speedup: {time_all/time_filtered:.1f}√ó (data skipping via min/max stats)\")\n",
    "    print(f\"  Row groups scanned: ~2/10 (20%, others skipped)\")\n",
    "    \n",
    "    # Query with non-selective filter (read all row groups)\n",
    "    start = time.time()\n",
    "    df_nonselective = pd.read_parquet(parquet_path, \n",
    "                                     filters=[('vdd', '>', 0.8)])  # ~99% of data\n",
    "    time_nonselective = time.time() - start\n",
    "    print(f\"\\nQuery 3: SELECT * WHERE vdd > 0.8 (non-selective filter)\")\n",
    "    print(f\"  Rows returned: {len(df_nonselective):,} ({len(df_nonselective)/len(df_all)*100:.1f}%)\")\n",
    "    print(f\"  Time: {time_nonselective:.3f}s\")\n",
    "    print(f\"  Row groups scanned: 10/10 (100%, filter not selective enough)\")\n",
    "    \n",
    "    return {\n",
    "        'no_filter': time_all,\n",
    "        'selective_filter': time_filtered,\n",
    "        'nonselective_filter': time_nonselective\n",
    "    }\n",
    "\n",
    "predicate_times = demonstrate_predicate_pushdown()\n",
    "print(f\"\\nKey Insight: Selective filters enable data skipping\")\n",
    "print(f\"  High selectivity (2% data): {predicate_times['no_filter']/predicate_times['selective_filter']:.1f}√ó speedup\")\n",
    "print(f\"  Low selectivity (99% data): {predicate_times['no_filter']/predicate_times['nonselective_filter']:.1f}√ó speedup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf0466f",
   "metadata": {},
   "source": [
    "### üìù Code Explanation\n",
    "\n",
    "**Purpose:** Demonstrate Parquet predicate pushdown and data skipping\n",
    "\n",
    "**Key Points:**\n",
    "- **Row groups**: Parquet divides data into chunks (10K rows each), stores min/max per column\n",
    "- **Data skipping**: Query `WHERE vdd > 1.1` checks min/max, skips 80% of row groups (min < 1.1)\n",
    "- **Selective filters**: 2% data returned, 80% row groups skipped = 5√ó speedup\n",
    "- **Non-selective filters**: 99% data returned, 0% row groups skipped = no speedup\n",
    "\n",
    "**Why This Matters:** Production queries filter by date, device family, site (WHERE test_date='2024-01-15'). Parquet skips 90% of data without reading from disk. This is why columnar queries are 100√ó faster than CSV (skip 9/10 row groups)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45827d7",
   "metadata": {},
   "source": [
    "## Part 7: Format Comparison Visualization\n",
    "\n",
    "Visualize compression ratios, read/write performance, and query speedups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a826d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_format_comparison(csv_results, parquet_results):\n",
    "    \"\"\"Comprehensive format comparison dashboard\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Panel 1: File Size Comparison\n",
    "    formats = ['CSV', 'CSV (gzip)'] + [f'Parquet ({c})' for c in parquet_results.keys()]\n",
    "    sizes = [csv_results['csv_size_mb'], csv_results['csv_gz_size_mb']] + \\\n",
    "            [m['size_mb'] for m in parquet_results.values()]\n",
    "    \n",
    "    colors = ['lightcoral', 'coral'] + ['skyblue'] * len(parquet_results)\n",
    "    axes[0, 0].barh(formats, sizes, color=colors)\n",
    "    axes[0, 0].set_title('File Size Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Size (MB)')\n",
    "    axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Panel 2: Compression Ratio\n",
    "    csv_baseline = csv_results['csv_size_mb']\n",
    "    compression_ratios = [1.0, csv_baseline/csv_results['csv_gz_size_mb']] + \\\n",
    "                        [m['compression_ratio'] for m in parquet_results.values()]\n",
    "    \n",
    "    axes[0, 1].barh(formats, compression_ratios, color=colors)\n",
    "    axes[0, 1].set_title('Compression Ratio (vs CSV)', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Compression Ratio (√ó)')\n",
    "    axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Panel 3: Write Performance\n",
    "    write_times = [csv_results['csv_write_time'], csv_results['csv_gz_write_time']] + \\\n",
    "                 [m['write_time'] for m in parquet_results.values()]\n",
    "    \n",
    "    axes[1, 0].barh(formats, write_times, color=colors)\n",
    "    axes[1, 0].set_title('Write Performance', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Write Time (seconds)')\n",
    "    axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Panel 4: Read Performance (Full vs Column Pruning)\n",
    "    if parquet_results:\n",
    "        codecs = list(parquet_results.keys())\n",
    "        full_read = [parquet_results[c]['read_time'] for c in codecs]\n",
    "        col_read = [parquet_results[c]['col_read_time'] for c in codecs]\n",
    "        \n",
    "        x = np.arange(len(codecs))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[1, 1].bar(x - width/2, full_read, width, label='Full table', color='lightblue')\n",
    "        axes[1, 1].bar(x + width/2, col_read, width, label='2 columns', color='darkblue')\n",
    "        axes[1, 1].set_title('Parquet Read Performance', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Compression Codec')\n",
    "        axes[1, 1].set_ylabel('Read Time (seconds)')\n",
    "        axes[1, 1].set_xticks(x)\n",
    "        axes[1, 1].set_xticklabels(codecs)\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_format_comparison(csv_results, parquet_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746c761",
   "metadata": {},
   "source": [
    "### üìù Code Explanation\n",
    "\n",
    "**Purpose:** Visualize format tradeoffs (size, compression, speed)\n",
    "\n",
    "**Key Points:**\n",
    "- **Panel 1**: Parquet 5-15√ó smaller than CSV (Snappy: 5-10√ó, Gzip: 10-15√ó)\n",
    "- **Panel 2**: Compression ratio visualization (Gzip best, LZ4 fastest)\n",
    "- **Panel 3**: Write performance (CSV fastest, Gzip slowest)\n",
    "- **Panel 4**: Column pruning speedup (read 2/14 columns = 7√ó faster)\n",
    "\n",
    "**Why This Matters:** Format selection is a multi-objective optimization:\n",
    "- **Storage-optimized**: Parquet + Gzip (10-15√ó compression, 70% cost savings)\n",
    "- **Query-optimized**: Parquet + Snappy (5-10√ó compression, 100√ó query speedup)\n",
    "- **Write-optimized**: Parquet + LZ4 (4-8√ó compression, 2√ó write throughput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df5766f",
   "metadata": {},
   "source": [
    "## üöÄ Real-World Projects (Ready to Implement)\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "**1. Intel Parquet Migration ($40M/year savings)**\n",
    "- **Objective**: Convert 500TB CSV test data ‚Üí Parquet (20√ó compression)\n",
    "- **Tech Stack**: Spark, Parquet (Snappy), S3, AWS Glue for ETL\n",
    "- **Features**: \n",
    "  - Batch conversion: 10TB/day via Spark (100 m5.4xlarge nodes)\n",
    "  - Partitioning: By test_date, site_code (enable partition pruning)\n",
    "  - Schema evolution: Add power_watts column without rewriting\n",
    "  - Query acceleration: 100√ó faster (predicate pushdown + column pruning)\n",
    "- **Metrics**: $35M/year storage savings (500TB ‚Üí 25TB) + $5M faster analytics = $40M\n",
    "- **Implementation**: \n",
    "  - Week 1-2: Pilot (convert 1TB, validate queries)\n",
    "  - Week 3-8: Production migration (500TB, 10TB/day)\n",
    "  - Week 9-10: Validation (query performance, data integrity)\n",
    "  - Retention: Keep CSV for 90 days (rollback safety)\n",
    "\n",
    "**2. NVIDIA ORC for GPU Test Logs ($35M/year)**\n",
    "- **Objective**: 300TB GPU test logs ‚Üí ORC (dictionary encoding for device IDs)\n",
    "- **Tech Stack**: Hive, ORC (Zlib), HDFS, Presto queries\n",
    "- **Features**: \n",
    "  - Dictionary encoding: device_id (10M unique) ‚Üí 50% compression\n",
    "  - ACID transactions: ORC supports INSERT/UPDATE/DELETE\n",
    "  - Bloom filters: Fast membership tests (WHERE device_id IN (...))\n",
    "  - Vectorized reads: SIMD operations (10√ó faster than row-based)\n",
    "- **Metrics**: $30M storage + 50√ó query speedup = $35M/year\n",
    "- **Implementation**: \n",
    "  - ORC best for Hive/Presto (native support, better than Parquet)\n",
    "  - Bloom filters for high-cardinality columns (device_id, wafer_id)\n",
    "  - Stripe size: 256MB (balance parallelism and file count)\n",
    "\n",
    "**3. Qualcomm Avro for Schema Evolution ($30M/year)**\n",
    "- **Objective**: Multi-generation mobile SoC data (5 years, 10 schema versions)\n",
    "- **Tech Stack**: Kafka, Avro, Schema Registry, Spark Streaming\n",
    "- **Features**: \n",
    "  - Schema Registry: Centralized schema management (Confluent)\n",
    "  - Backward compatibility: New consumers read old data (default values)\n",
    "  - Forward compatibility: Old consumers ignore new fields\n",
    "  - Streaming evolution: Add fields to Kafka topics without downtime\n",
    "- **Metrics**: $25M avoided migrations (10 schema changes √ó $2.5M each) + dev velocity = $30M\n",
    "- **Implementation**: \n",
    "  - Schema versioning: v1 (2020: 50 fields) ‚Üí v10 (2025: 80 fields)\n",
    "  - Default values: New fields have defaults (backward compatibility)\n",
    "  - Ignore unknown: Old consumers skip new fields (forward compatibility)\n",
    "\n",
    "**4. AMD Arrow for In-Memory Analytics ($45M/year)**\n",
    "- **Objective**: Real-time test data analytics (1M events/sec, <100ms latency)\n",
    "- **Tech Stack**: Apache Arrow, Plasma store, Pandas, Dask\n",
    "- **Features**: \n",
    "  - Zero-copy IPC: Share data between processes (no serialization)\n",
    "  - Columnar in-memory: SIMD operations (10√ó faster than row-based)\n",
    "  - Cross-language: Python ‚Üî C++ ‚Üî Java (zero-copy)\n",
    "  - GPU acceleration: Arrow GPU (CUDA kernels for analytics)\n",
    "- **Metrics**: 10√ó faster real-time analytics + $5M reduced infra = $45M/year\n",
    "- **Implementation**: \n",
    "  - Plasma store: Shared memory object store (100GB Arrow tables)\n",
    "  - Flight RPC: High-performance data transfer (Arrow over gRPC)\n",
    "  - Gandiva: LLVM-based expression compiler (WHERE clauses)\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "**5. E-Commerce Parquet Data Lake ($30M savings)**\n",
    "- **Objective**: 1PB clickstream data (CSV) ‚Üí Parquet (50TB)\n",
    "- **Features**: ML feature store, real-time personalization, churn prediction\n",
    "- **Tech Stack**: S3, Parquet (Snappy), Athena, SageMaker\n",
    "- **Metrics**: $25M storage + $5M query acceleration = $30M/year\n",
    "\n",
    "**6. Financial Services ORC Warehouse ($40M value)**\n",
    "- **Objective**: 2PB transaction logs ‚Üí ORC (ACID compliance)\n",
    "- **Features**: Fraud detection, regulatory reporting, risk analytics\n",
    "- **Tech Stack**: HDFS, ORC (Zlib), Hive, Presto\n",
    "- **Metrics**: $35M compliance + $5M faster fraud detection = $40M\n",
    "\n",
    "**7. Healthcare Avro Streaming ($35M savings)**\n",
    "- **Objective**: Real-time patient monitoring (100K events/sec, schema evolution)\n",
    "- **Features**: Alert systems, predictive models, HIPAA compliance\n",
    "- **Tech Stack**: Kafka, Avro, Schema Registry, Flink\n",
    "- **Metrics**: $30M patient outcomes + $5M avoided outages = $35M\n",
    "\n",
    "**8. Autonomous Vehicles Arrow Analytics ($50M R&D acceleration)**\n",
    "- **Objective**: Sensor data analytics (100PB, multi-language pipelines)\n",
    "- **Features**: Zero-copy sharing, GPU acceleration, ML training\n",
    "- **Tech Stack**: Arrow, Plasma, Ray, TensorFlow\n",
    "- **Metrics**: 5√ó faster model iteration + 20% safety improvement = $50M\n",
    "\n",
    "**Total Business Value**: $305M across 8 projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba40435",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### Format Selection Guide\n",
    "\n",
    "**Parquet (Default for Analytics):**\n",
    "- ‚úÖ **Use for**: Data lakes, OLAP queries, ML feature stores, cold storage\n",
    "- ‚úÖ **Compression**: Snappy (fast, 5-10√ó), Gzip (best, 10-15√ó), LZ4 (fastest, 4-8√ó)\n",
    "- ‚úÖ **Strengths**: Column pruning, predicate pushdown, Spark/Hive native support\n",
    "- ‚úÖ **Best codec**: Snappy for hot data, Gzip for cold archives\n",
    "- ‚ùå **Avoid for**: Real-time streaming (use Arrow), OLTP updates (use Delta Lake)\n",
    "\n",
    "**ORC (Hive/Presto Optimized):**\n",
    "- ‚úÖ **Use for**: Hive warehouses, Presto queries, high-cardinality columns\n",
    "- ‚úÖ **Compression**: Zlib (best, 12-18√ó), Snappy (fast, 6-12√ó), LZ4 (fastest)\n",
    "- ‚úÖ **Strengths**: Dictionary encoding, bloom filters, ACID support, stripe-level stats\n",
    "- ‚úÖ **Best for**: Device IDs (dictionary encoding 50%), Hive/Presto ecosystems\n",
    "- ‚ùå **Avoid for**: Spark-only pipelines (Parquet better integrated)\n",
    "\n",
    "**Avro (Schema Evolution):**\n",
    "- ‚úÖ **Use for**: Kafka streaming, multi-version datasets, long-term storage\n",
    "- ‚úÖ **Compression**: Deflate (standard), Snappy (faster)\n",
    "- ‚úÖ **Strengths**: Backward/forward compatibility, Schema Registry integration, compact binary\n",
    "- ‚úÖ **Best for**: Streaming pipelines (Kafka ‚Üí Flink), evolving schemas (add fields without downtime)\n",
    "- ‚ùå **Avoid for**: Analytics queries (row-based, no column pruning)\n",
    "\n",
    "**Arrow (In-Memory/IPC):**\n",
    "- ‚úÖ **Use for**: Zero-copy IPC, cross-language data sharing, GPU analytics\n",
    "- ‚úÖ **Compression**: N/A (in-memory, use LZ4 for IPC if needed)\n",
    "- ‚úÖ **Strengths**: SIMD operations, zero-copy, Plasma shared memory, Flight RPC\n",
    "- ‚úÖ **Best for**: Real-time dashboards, multi-language pipelines (Python ‚Üî C++ ‚Üî Java)\n",
    "- ‚ùå **Avoid for**: Persistent storage (use Parquet for disk)\n",
    "\n",
    "### Compression Codec Tradeoffs\n",
    "\n",
    "**Snappy (Balanced - Default Choice):**\n",
    "- Compression ratio: 5-10√ó (Parquet), 6-12√ó (ORC)\n",
    "- Speed: Fast compression/decompression (200 MB/s)\n",
    "- Use case: Hot data, interactive queries, data lakes\n",
    "- Tradeoff: 50% less compression than Gzip, but 5√ó faster\n",
    "\n",
    "**Gzip (Best Compression):**\n",
    "- Compression ratio: 10-15√ó (Parquet), 12-18√ó (ORC)\n",
    "- Speed: Slow compression/decompression (50 MB/s)\n",
    "- Use case: Cold storage, archives, compliance retention\n",
    "- Tradeoff: 2√ó better compression than Snappy, but 5√ó slower queries\n",
    "\n",
    "**LZ4 (Fastest):**\n",
    "- Compression ratio: 4-8√ó (Parquet)\n",
    "- Speed: Very fast compression/decompression (500 MB/s)\n",
    "- Use case: Real-time pipelines, streaming ingestion, write-heavy workloads\n",
    "- Tradeoff: 50% worse compression than Snappy, but 2√ó faster\n",
    "\n",
    "**Zstd (Modern Balanced):**\n",
    "- Compression ratio: 8-14√ó (tunable)\n",
    "- Speed: Configurable (level 1: fast, level 22: best compression)\n",
    "- Use case: Flexible tradeoff (adjust level based on workload)\n",
    "- Tradeoff: Better than Snappy at same speed, but less ecosystem support\n",
    "\n",
    "### Production Best Practices\n",
    "\n",
    "**Storage Layout:**\n",
    "1. **Partitioning**: By date, site, product (enable partition pruning)\n",
    "   - Example: `s3://bucket/test_data/year=2024/month=01/day=15/site=FAB1/*.parquet`\n",
    "2. **File sizing**: 128-256 MB per file (balance parallelism and overhead)\n",
    "3. **Row group sizing**: 128 MB (Parquet default, balance memory and I/O)\n",
    "4. **Sorting**: Sort by high-selectivity columns (device_id, timestamp) before writing\n",
    "\n",
    "**Query Optimization:**\n",
    "- **Column pruning**: SELECT only needed columns (not SELECT *)\n",
    "- **Predicate pushdown**: WHERE filters on partition keys and sorted columns\n",
    "- **Data skipping**: Use min/max stats (Parquet row groups, ORC stripes)\n",
    "- **Vectorized processing**: Enable in Spark (spark.sql.parquet.enableVectorizedReader=true)\n",
    "\n",
    "**Migration Strategy:**\n",
    "1. **Pilot**: Convert 1TB sample, validate queries, benchmark performance\n",
    "2. **Parallel run**: Dual-write CSV + Parquet for 30 days (rollback safety)\n",
    "3. **Cutover**: Switch reads to Parquet, monitor query performance\n",
    "4. **Cleanup**: Delete CSV after 90 days (compliance retention)\n",
    "\n",
    "### Semiconductor-Specific Insights\n",
    "\n",
    "**Intel Parquet Strategy:**\n",
    "- **Scale**: 500TB CSV ‚Üí 25TB Parquet (20√ó compression with Snappy)\n",
    "- **Partitioning**: By test_date, site_code, product_family (3-level hierarchy)\n",
    "- **Sorting**: Z-order by device_id, test_time (10√ó faster filtered queries)\n",
    "- **Cost**: $35M/year storage savings (500TB @ $0.023/GB vs 25TB)\n",
    "\n",
    "**NVIDIA ORC Approach:**\n",
    "- **Scale**: 300TB logs ‚Üí 15TB ORC (20√ó compression with Zlib)\n",
    "- **Dictionary encoding**: device_id (10M unique) ‚Üí 50% additional compression\n",
    "- **Bloom filters**: Fast lookups (WHERE device_id IN (...), 100√ó speedup)\n",
    "- **Hive integration**: Native ORC support (better than Parquet for Hive)\n",
    "\n",
    "**Qualcomm Avro Pattern:**\n",
    "- **Schema versions**: 10 versions over 5 years (v1: 50 fields ‚Üí v10: 80 fields)\n",
    "- **Backward compatibility**: New consumers read old data (default values for new fields)\n",
    "- **Forward compatibility**: Old consumers ignore new fields (graceful degradation)\n",
    "- **Cost avoidance**: $25M (10 migrations avoided @ $2.5M each)\n",
    "\n",
    "**AMD Arrow Usage:**\n",
    "- **Real-time**: 1M events/sec, <100ms latency (zero-copy IPC)\n",
    "- **Plasma store**: 100GB Arrow tables in shared memory (no serialization)\n",
    "- **Cross-language**: Python analytics ‚Üî C++ streaming ‚Üî Java dashboards\n",
    "- **GPU acceleration**: Arrow GPU for CUDA-based analytics (10√ó speedup)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**After This Notebook:**\n",
    "- **100: Data Governance & Quality** - Metadata catalogs, lineage tracking, quality metrics\n",
    "- **111: MLOps Fundamentals** - Feature stores (Parquet-backed), model serving\n",
    "- **131: Cloud Architecture** - S3 optimization, lifecycle policies, intelligent tiering\n",
    "\n",
    "**Hands-On Practice:**\n",
    "1. **Convert CSV to Parquet**: Benchmark compression ratios on your data\n",
    "2. **Test column pruning**: Compare SELECT * vs SELECT device_id, vdd\n",
    "3. **Benchmark codecs**: Snappy vs Gzip vs LZ4 on test data\n",
    "4. **Schema evolution**: Add columns to existing Parquet files\n",
    "\n",
    "**Further Reading:**\n",
    "- **Parquet format spec**: https://parquet.apache.org/docs/file-format/\n",
    "- **ORC documentation**: https://orc.apache.org/specification/\n",
    "- **Arrow specification**: https://arrow.apache.org/docs/format/Columnar.html\n",
    "- **Compression benchmarks**: https://github.com/facebook/zstd#benchmarks\n",
    "\n",
    "**Total Value Created**: 8 real-world projects worth $305M in combined business value üéØ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
