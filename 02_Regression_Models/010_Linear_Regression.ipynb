{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee27d27",
   "metadata": {},
   "source": [
    "# 010: Linear Regression",
    "",
    "Linear regression models the relationship between:",
    "- **Independent variables** (features, X): factors that influence the outcome",
    "- **Dependent variable** (target, y): the value we want to predict",
    "",
    "### \ud83d\udcca Visual Concept",
    "",
    "```mermaid",
    "graph LR",
    "    A[Features X] --> B[Linear Regression Model]",
    "    B --> C[Prediction y]",
    "    D[Training Data] --> B",
    "    style B fill:#4CAF50,stroke:#333,stroke-width:2px,color:#fff",
    "    style C fill:#2196F3,stroke:#333,stroke-width:2px,color:#fff",
    "```",
    "",
    "### The Linear Equation",
    "",
    "For simple linear regression (one feature):",
    "",
    "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$",
    "",
    "Where:",
    "- $y$ = predicted value (target)",
    "- $x$ = feature value",
    "- $\\beta_0$ = intercept (y-value when x=0)",
    "- $\\beta_1$ = slope (change in y per unit change in x)",
    "- $\\epsilon$ = error term (residual)",
    "",
    "For multiple linear regression:",
    "",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon$$",
    "",
    "### \ud83c\udfaf Linear Regression Workflow",
    "",
    "```mermaid",
    "graph TD",
    "    A[Collect Data] --> B[Exploratory Data Analysis]",
    "    B --> C[Feature Selection]",
    "    C --> D[Train-Test Split]",
    "    D --> E[Fit Linear Model]",
    "    E --> F[Make Predictions]",
    "    F --> G[Evaluate Performance]",
    "    G --> H{Good Performance?}",
    "    H -->|Yes| I[Deploy Model]",
    "    H -->|No| J[Feature Engineering]",
    "    J --> E",
    "    style E fill:#FF9800,stroke:#333,stroke-width:2px,color:#fff",
    "    style I fill:#4CAF50,stroke:#333,stroke-width:2px,color:#fff",
    "```",
    "",
    "### When to Use Linear Regression?",
    "",
    "\u2705 **Use when:**",
    "- Relationship between features and target is approximately linear",
    "- You need interpretable results",
    "- Continuous target variable",
    "- Fast training and prediction needed",
    "",
    "\u274c **Don't use when:**",
    "- Relationship is highly non-linear",
    "- Many categorical features with high cardinality",
    "- Target has complex interactions",
    "- Presence of severe outliers (consider robust regression)",
    "",
    "### \ud83c\udfed Real-World Applications",
    "",
    "**Post-Silicon Validation:**",
    "- Predicting device yield from test parameters",
    "- Estimating power consumption from voltage/frequency",
    "- Forecasting test time from complexity metrics",
    "- Correlating performance with process variations",
    "",
    "**General AI/ML:**",
    "- Stock price prediction",
    "- Real estate valuation",
    "- Sales forecasting",
    "- Risk assessment",
    "",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33afa721",
   "metadata": {},
   "source": [
    "## 2. Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c655633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\u2705 Libraries imported successfully\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4d5653",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Import essential libraries and configure the environment\n",
    "\n",
    "**Key Points:**\n",
    "1. **NumPy & Pandas**: Core libraries for numerical computing and data manipulation\n",
    "2. **Matplotlib & Seaborn**: Visualization libraries for plotting graphs and charts\n",
    "3. **Scikit-learn**: Industry-standard ML library providing LinearRegression and evaluation metrics\n",
    "4. **Configuration**: Set random seed for reproducibility and configure plot styling for consistent visuals\n",
    "\n",
    "**Why This Matters:**\n",
    "- Setting random seed ensures your results are reproducible across runs\n",
    "- Warning suppression keeps output clean and focused on results\n",
    "- Pre-configuring plotting style creates professional visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2658b118",
   "metadata": {},
   "source": [
    "### 2.1 Generate Synthetic Dataset\n",
    "\n",
    "Let's create a dataset that simulates **post-silicon validation scenarios** - semiconductor test parameters and device yield.\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Create realistic synthetic data mimicking post-silicon validation scenarios\n",
    "\n",
    "**Key Points:**\n",
    "1. **Feature Generation**: Temperature, voltage, current, and pressure represent typical manufacturing test parameters\n",
    "2. **Linear Relationship**: Yield is calculated as a linear combination of features with added random noise\n",
    "3. **Domain Knowledge**: Coefficients reflect real-world physics (e.g., high current reduces yield, voltage positively affects yield)\n",
    "4. **Data Validation**: Clipping ensures yield stays within realistic bounds (60-100%)\n",
    "\n",
    "**Why This Approach:**\n",
    "- Synthetic data lets us control ground truth and understand model behavior\n",
    "- Mimics real STDF data patterns from semiconductor testing\n",
    "- Allows experimenting without exposing proprietary test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0af31fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_semiconductor_data(n_samples=1000, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Generate synthetic semiconductor test data\n",
    "    \n",
    "    Scenario: Predict chip yield based on manufacturing parameters\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Manufacturing parameters (features)\n",
    "    temperature = np.random.uniform(20, 30, n_samples)  # \u00b0C\n",
    "    voltage = np.random.uniform(3.0, 3.6, n_samples)     # V\n",
    "    current = np.random.uniform(0.4, 0.6, n_samples)     # A\n",
    "    pressure = np.random.uniform(0.95, 1.05, n_samples)  # atm\n",
    "    \n",
    "    # True relationship (linear with some noise)\n",
    "    # yield = f(temperature, voltage, current, pressure)\n",
    "    yield_pct = (\n",
    "        70 +                           # base yield\n",
    "        0.5 * temperature +            # higher temp slightly increases yield\n",
    "        10 * voltage +                 # voltage has strong positive effect\n",
    "        -20 * current +                # high current reduces yield\n",
    "        5 * pressure +                 # pressure has moderate effect\n",
    "        np.random.normal(0, noise_level * 10, n_samples)  # random noise\n",
    "    )\n",
    "    \n",
    "    # Clip yield to realistic range\n",
    "    yield_pct = np.clip(yield_pct, 60, 100)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'temperature': temperature,\n",
    "        'voltage': voltage,\n",
    "        'current': current,\n",
    "        'pressure': pressure,\n",
    "        'yield_pct': yield_pct\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "df = generate_semiconductor_data(n_samples=1000)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e8161",
   "metadata": {},
   "source": [
    "### 2.2 Exploratory Data Analysis (EDA)\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Visualize relationships between features and target variable\n",
    "\n",
    "**Key Points:**\n",
    "1. **Scatter Plots**: Show individual data points to reveal linear/non-linear patterns\n",
    "2. **Trend Lines**: Red dashed lines indicate the best-fit linear relationship for each feature\n",
    "3. **Visual Inspection**: Helps identify which features have strong predictive power\n",
    "4. **Assumption Check**: Verifies linearity assumption before building model\n",
    "\n",
    "**What to Look For:**\n",
    "- Points clustering around trend line = strong linear relationship\n",
    "- Scattered points = weak relationship or non-linearity\n",
    "- Outliers that deviate significantly from the trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c24ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "features = ['temperature', 'voltage', 'current', 'pressure']\n",
    "for idx, feature in enumerate(features):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    ax.scatter(df[feature], df['yield_pct'], alpha=0.5, s=20)\n",
    "    ax.set_xlabel(feature.capitalize(), fontsize=12)\n",
    "    ax.set_ylabel('Yield %', fontsize=12)\n",
    "    ax.set_title(f'Yield vs {feature.capitalize()}', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(df[feature], df['yield_pct'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(df[feature], p(df[feature]), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Scatter plots show relationships between features and yield\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc2090",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Calculate correlations and identify multicollinearity issues\n",
    "\n",
    "**Key Points:**\n",
    "1. **Correlation Matrix**: Heatmap shows linear relationships between all variables (ranges from -1 to +1)\n",
    "2. **Feature Importance Indicator**: High correlation with target = potentially useful feature\n",
    "3. **Multicollinearity Detection**: High correlation between features can cause model instability\n",
    "4. **Color Coding**: Red = positive correlation, Blue = negative correlation, White = no correlation\n",
    "\n",
    "**Interpretation Guide:**\n",
    "- |r| > 0.7: Strong correlation\n",
    "- 0.4 < |r| < 0.7: Moderate correlation  \n",
    "- |r| < 0.4: Weak correlation\n",
    "- r close to 0: No linear relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a681686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca Correlation with yield:\")\n",
    "print(correlation_matrix['yield_pct'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88a343e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Linear Regression from Scratch\n",
    "\n",
    "Let's implement linear regression using the Normal Equation (Ordinary Least Squares).\n",
    "\n",
    "### \ud83e\uddee Mathematical Foundation\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Training Data X, y] --> B[Add Intercept Column]\n",
    "    B --> C[Compute X^T X]\n",
    "    C --> D[Compute Inverse]\n",
    "    D --> E[Compute X^T y]\n",
    "    E --> F[\u03b2 = inv X^T X \u00d7 X^T y]\n",
    "    F --> G[Extract \u03b20 intercept and \u03b21...\u03b2n coefficients]\n",
    "    style F fill:#FF5722,stroke:#333,stroke-width:2px,color:#fff\n",
    "```\n",
    "\n",
    "### The Normal Equation\n",
    "\n",
    "$$\\hat{\\beta} = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{\\beta}$ = estimated coefficients (including intercept)\n",
    "- $X$ = feature matrix (with column of 1s for intercept)\n",
    "- $y$ = target vector\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement linear regression from scratch to understand the mathematics\n",
    "\n",
    "**Key Points:**\n",
    "1. **Normal Equation Method**: Closed-form solution that directly computes optimal coefficients (no iterations needed)\n",
    "2. **Matrix Operations**: Uses linear algebra (transpose, inverse, dot product) to solve the equation\n",
    "3. **Intercept Handling**: Adds column of 1s to feature matrix to learn the bias term\n",
    "4. **Educational Value**: Understanding the math helps debug issues and appreciate sklearn's optimizations\n",
    "\n",
    "**Why From Scratch:**\n",
    "- Builds deep understanding of what's happening \"under the hood\"\n",
    "- Helps recognize when assumptions are violated\n",
    "- Demonstrates that ML isn't magic - it's mathematics\n",
    "- Validates that our implementation matches sklearn (coming next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed12c890",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionScratch:\n",
    "    \"\"\"Linear Regression implemented from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.coefficients = None\n",
    "        self.intercept = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit linear model using Normal Equation\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "        y : array-like, shape (n_samples,)\n",
    "        \"\"\"\n",
    "        # Add column of ones for intercept\n",
    "        X_with_intercept = np.c_[np.ones(X.shape[0]), X]\n",
    "        \n",
    "        # Normal equation: \u03b2 = (X^T X)^-1 X^T y\n",
    "        XtX = X_with_intercept.T @ X_with_intercept\n",
    "        XtX_inv = np.linalg.inv(XtX)\n",
    "        Xty = X_with_intercept.T @ y\n",
    "        \n",
    "        beta = XtX_inv @ Xty\n",
    "        \n",
    "        # Extract intercept and coefficients\n",
    "        self.intercept = beta[0]\n",
    "        self.coefficients = beta[1:]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return X @ self.coefficients + self.intercept\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate R\u00b2 score\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - y.mean()) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Test our implementation\n",
    "X = df[features].values\n",
    "y = df['yield_pct'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train from scratch\n",
    "lr_scratch = LinearRegressionScratch()\n",
    "lr_scratch.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_scratch = lr_scratch.predict(X_test)\n",
    "r2_scratch = lr_scratch.score(X_test, y_test)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LINEAR REGRESSION FROM SCRATCH\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Intercept: {lr_scratch.intercept:.4f}\")\n",
    "print(f\"\\nCoefficients:\")\n",
    "for feat, coef in zip(features, lr_scratch.coefficients):\n",
    "    print(f\"  {feat:12s}: {coef:8.4f}\")\n",
    "print(f\"\\nR\u00b2 Score on test set: {r2_scratch:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73892861",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Linear Regression with Scikit-learn\n",
    "\n",
    "Now let's use the industry-standard scikit-learn implementation.\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Compare our implementation with production-ready sklearn\n",
    "\n",
    "**Key Points:**\n",
    "1. **Industry Standard**: Sklearn's LinearRegression is optimized, tested, and widely used in production\n",
    "2. **API Simplicity**: Consistent fit/predict interface used across all sklearn models\n",
    "3. **Validation**: Comparing coefficients proves our scratch implementation is mathematically correct\n",
    "4. **Production Choice**: In real projects, always prefer sklearn for robustness and performance\n",
    "\n",
    "**Key Takeaway:**\n",
    "- Both implementations produce identical results (validates our understanding)\n",
    "- Sklearn adds: numerical stability, edge case handling, performance optimizations\n",
    "- Understanding the math helps you debug and extend models confidently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f182c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with scikit-learn\n",
    "lr_sklearn = LinearRegression()\n",
    "lr_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_sklearn = lr_sklearn.predict(X_test)\n",
    "\n",
    "# Compare with our implementation\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARISON: Scratch vs Scikit-learn\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<20} {'Scratch':<15} {'Scikit-learn':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Intercept':<20} {lr_scratch.intercept:>14.4f} {lr_sklearn.intercept_:>14.4f}\")\n",
    "\n",
    "for i, feat in enumerate(features):\n",
    "    print(f\"{feat:<20} {lr_scratch.coefficients[i]:>14.4f} {lr_sklearn.coef_[i]:>14.4f}\")\n",
    "\n",
    "r2_sklearn = lr_sklearn.score(X_test, y_test)\n",
    "print(\"-\"*60)\n",
    "print(f\"{'R\u00b2 Score':<20} {r2_scratch:>14.4f} {r2_sklearn:>14.4f}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n\u2705 Both implementations produce identical results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac033b0d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Model Evaluation\n",
    "\n",
    "### \ud83d\udcca Metrics Overview\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Model Predictions] --> B[Evaluation Metrics]\n",
    "    B --> C[MAE - Average Error]\n",
    "    B --> D[RMSE - Penalizes Large Errors]\n",
    "    B --> E[R\u00b2 - Variance Explained]\n",
    "    B --> F[MAPE - Percentage Error]\n",
    "    style B fill:#9C27B0,stroke:#333,stroke-width:2px,color:#fff\n",
    "```\n",
    "\n",
    "### 5.1 Key Regression Metrics\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**: Average absolute difference\n",
    "   $$MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$$\n",
    "\n",
    "2. **Mean Squared Error (MSE)**: Average squared difference\n",
    "   $$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE)**: Square root of MSE\n",
    "   $$RMSE = \\sqrt{MSE}$$\n",
    "\n",
    "4. **R\u00b2 Score (Coefficient of Determination)**: Proportion of variance explained\n",
    "   $$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$$\n",
    "   \n",
    "   - R\u00b2 = 1: Perfect predictions\n",
    "   - R\u00b2 = 0: Model no better than mean\n",
    "   - R\u00b2 < 0: Model worse than mean\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Quantify model performance using multiple metrics\n",
    "\n",
    "**Key Points:**\n",
    "1. **Multiple Metrics**: Each metric reveals different aspects of performance (no single metric tells the full story)\n",
    "2. **MAE**: Easy to interpret (average prediction error in original units)\n",
    "3. **RMSE**: Penalizes large errors more than MAE (sensitive to outliers)\n",
    "4. **R\u00b2**: Shows how much variance your model explains (0% to 100% scale)\n",
    "\n",
    "**Choosing Metrics:**\n",
    "- **MAE**: When all errors are equally important\n",
    "- **RMSE**: When large errors are especially bad (e.g., safety-critical systems)\n",
    "- **R\u00b2**: For comparing models and communicating with non-technical stakeholders\n",
    "- **MAPE**: When relative errors matter more than absolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ea832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics\n",
    "mae = mean_absolute_error(y_test, y_pred_sklearn)\n",
    "mse = mean_squared_error(y_test, y_pred_sklearn)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred_sklearn)\n",
    "\n",
    "# Calculate additional metrics\n",
    "mape = np.mean(np.abs((y_test - y_pred_sklearn) / y_test)) * 100  # Mean Absolute Percentage Error\n",
    "residuals = y_test - y_pred_sklearn\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean Absolute Error (MAE):        {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE):         {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE):   {rmse:.4f}\")\n",
    "print(f\"R\u00b2 Score:                         {r2:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error:   {mape:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\n\ud83d\udcca Interpretation:\")\n",
    "print(f\"  - On average, predictions are off by {mae:.2f} percentage points\")\n",
    "print(f\"  - Model explains {r2*100:.2f}% of variance in yield\")\n",
    "if r2 > 0.9:\n",
    "    print(f\"  - \ud83c\udfaf Excellent fit!\")\n",
    "elif r2 > 0.7:\n",
    "    print(f\"  - \u2705 Good fit\")\n",
    "elif r2 > 0.5:\n",
    "    print(f\"  - \u26a0\ufe0f  Moderate fit - consider feature engineering\")\n",
    "else:\n",
    "    print(f\"  - \u274c Poor fit - linear model may not be appropriate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faacb878",
   "metadata": {},
   "source": [
    "### 5.2 Cross-Validation\n",
    "\n",
    "Cross-validation provides a more robust estimate of model performance.\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Evaluate model on multiple train-test splits for robust performance estimation\n",
    "\n",
    "**Key Points:**\n",
    "1. **5-Fold CV**: Data split into 5 parts; each part serves as test set once while others train the model\n",
    "2. **Reduces Overfitting Risk**: Single train-test split might be lucky/unlucky; CV averages across multiple splits\n",
    "3. **Confidence Intervals**: Standard deviation shows how much performance varies across folds\n",
    "4. **Production Readiness**: Consistent CV scores indicate model will generalize well to new data\n",
    "\n",
    "**When to Use CV:**\n",
    "- Small to medium datasets (where single split might not represent population)\n",
    "- Hyperparameter tuning (coming in notebook 043)\n",
    "- Model selection (comparing different algorithms)\n",
    "- When you need confidence in performance estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f14640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(lr_sklearn, X, y, cv=5, \n",
    "                            scoring='r2')\n",
    "cv_rmse_scores = -cross_val_score(lr_sklearn, X, y, cv=5,\n",
    "                                  scoring='neg_root_mean_squared_error')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CROSS-VALIDATION RESULTS (5-Fold)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"R\u00b2 Scores per fold: {cv_scores}\")\n",
    "print(f\"Mean R\u00b2: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "print(f\"\\nRMSE per fold: {cv_rmse_scores}\")\n",
    "print(f\"Mean RMSE: {cv_rmse_scores.mean():.4f} (+/- {cv_rmse_scores.std() * 2:.4f})\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c6101",
   "metadata": {},
   "source": [
    "### 5.3 Visualization of Results\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Visual diagnostics to understand model behavior and validate assumptions\n",
    "\n",
    "**Key Points:**\n",
    "1. **Predicted vs Actual**: Ideal model has all points on diagonal line; scatter indicates prediction errors\n",
    "2. **Residual Plot**: Random scatter around zero = good; patterns indicate model problems (non-linearity, heteroscedasticity)\n",
    "3. **Residual Distribution**: Should be bell-shaped (normal); skewed = violated assumptions\n",
    "4. **Q-Q Plot**: Points on diagonal = normal distribution; deviations indicate non-normality\n",
    "\n",
    "**Red Flags to Watch:**\n",
    "- Funnel shape in residuals \u2192 heteroscedasticity (variance increases with prediction)\n",
    "- Curved pattern in residuals \u2192 non-linearity (need polynomial features)\n",
    "- Heavy tails in Q-Q plot \u2192 outliers affecting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9552ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Predicted vs Actual\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(y_test, y_pred_sklearn, alpha=0.6, s=50)\n",
    "ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "ax1.set_xlabel('Actual Yield %', fontsize=12)\n",
    "ax1.set_ylabel('Predicted Yield %', fontsize=12)\n",
    "ax1.set_title('Predicted vs Actual', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residuals vs Predicted\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(y_pred_sklearn, residuals, alpha=0.6, s=50)\n",
    "ax2.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "ax2.set_xlabel('Predicted Yield %', fontsize=12)\n",
    "ax2.set_ylabel('Residuals', fontsize=12)\n",
    "ax2.set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residual Distribution\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "ax3.axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "ax3.set_xlabel('Residuals', fontsize=12)\n",
    "ax3.set_ylabel('Frequency', fontsize=12)\n",
    "ax3.set_title('Residual Distribution', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Q-Q Plot (check normality of residuals)\n",
    "ax4 = axes[1, 1]\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=ax4)\n",
    "ax4.set_title('Q-Q Plot (Normality Check)', fontsize=14, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udcca Diagnostic Plots:\")\n",
    "print(\"  1. Predicted vs Actual: Points should cluster around diagonal\")\n",
    "print(\"  2. Residual Plot: Should show random scatter (no patterns)\")\n",
    "print(\"  3. Residual Distribution: Should be approximately normal\")\n",
    "print(\"  4. Q-Q Plot: Points should follow diagonal line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074a97f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Model Assumptions & Diagnostics\n",
    "\n",
    "Linear regression assumes:\n",
    "1. **Linearity**: Relationship between X and y is linear\n",
    "2. **Independence**: Observations are independent\n",
    "3. **Homoscedasticity**: Constant variance of residuals\n",
    "4. **Normality**: Residuals are normally distributed\n",
    "5. **No multicollinearity**: Features are not highly correlated\n",
    "\n",
    "Let's check these assumptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebde742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_assumptions(X, y, y_pred, feature_names):\n",
    "    \"\"\"Comprehensive assumption checking\"\"\"\n",
    "    \n",
    "    residuals = y - y_pred\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ASSUMPTION DIAGNOSTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Linearity (already visualized)\n",
    "    print(\"\\n1. LINEARITY\")\n",
    "    print(\"   \u2713 Check scatter plots above - relationships appear linear\")\n",
    "    \n",
    "    # 2. Independence (Durbin-Watson test)\n",
    "    from statsmodels.stats.stattools import durbin_watson\n",
    "    dw_stat = durbin_watson(residuals)\n",
    "    print(f\"\\n2. INDEPENDENCE (Durbin-Watson)\")\n",
    "    print(f\"   DW Statistic: {dw_stat:.4f}\")\n",
    "    print(f\"   Interpretation: \", end='')\n",
    "    if 1.5 < dw_stat < 2.5:\n",
    "        print(\"\u2713 No significant autocorrelation\")\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f  Possible autocorrelation detected\")\n",
    "    \n",
    "    # 3. Homoscedasticity (Constant Variance)\n",
    "    print(f\"\\n3. HOMOSCEDASTICITY\")\n",
    "    print(f\"   Check residual plot above:\")\n",
    "    print(f\"   \u2713 Residuals should be randomly scattered\")\n",
    "    print(f\"   \u2713 No 'funnel' or 'cone' shape\")\n",
    "    \n",
    "    # 4. Normality of Residuals\n",
    "    from scipy.stats import shapiro\n",
    "    stat, p_value = shapiro(residuals)\n",
    "    print(f\"\\n4. NORMALITY (Shapiro-Wilk Test)\")\n",
    "    print(f\"   Test Statistic: {stat:.4f}\")\n",
    "    print(f\"   P-value: {p_value:.4f}\")\n",
    "    if p_value > 0.05:\n",
    "        print(f\"   \u2713 Residuals appear normally distributed (p > 0.05)\")\n",
    "    else:\n",
    "        print(f\"   \u26a0\ufe0f  Residuals may not be normal (p < 0.05)\")\n",
    "    \n",
    "    # 5. Multicollinearity (VIF)\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    \n",
    "    print(f\"\\n5. MULTICOLLINEARITY (VIF)\")\n",
    "    print(f\"   {'Feature':<15} {'VIF':<10} {'Status'}\")\n",
    "    print(f\"   {'-'*40}\")\n",
    "    \n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = feature_names\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "    \n",
    "    for _, row in vif_data.iterrows():\n",
    "        status = \"\u2713 Good\" if row['VIF'] < 5 else \"\u26a0\ufe0f  High\" if row['VIF'] < 10 else \"\u274c Very High\"\n",
    "        print(f\"   {row['Feature']:<15} {row['VIF']:<10.2f} {status}\")\n",
    "    \n",
    "    print(f\"\\n   VIF < 5: No multicollinearity\")\n",
    "    print(f\"   VIF 5-10: Moderate multicollinearity\")\n",
    "    print(f\"   VIF > 10: High multicollinearity (consider removing features)\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run diagnostics\n",
    "check_assumptions(X_test, y_test, y_pred_sklearn, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c621b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Feature Importance and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features to compare coefficient magnitudes\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train on scaled data\n",
    "lr_scaled = LinearRegression()\n",
    "lr_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Plot feature importance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Original coefficients\n",
    "ax1.barh(features, lr_sklearn.coef_)\n",
    "ax1.set_xlabel('Coefficient Value', fontsize=12)\n",
    "ax1.set_title('Original Coefficients', fontsize=14, fontweight='bold')\n",
    "ax1.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Standardized coefficients (fair comparison)\n",
    "colors = ['green' if c > 0 else 'red' for c in lr_scaled.coef_]\n",
    "ax2.barh(features, np.abs(lr_scaled.coef_), color=colors, alpha=0.7)\n",
    "ax2.set_xlabel('Absolute Coefficient (Standardized)', fontsize=12)\n",
    "ax2.set_title('Feature Importance (Standardized)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE\")\n",
    "print(\"=\"*60)\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient': lr_sklearn.coef_,\n",
    "    'Abs_Std_Coef': np.abs(lr_scaled.coef_)\n",
    "}).sort_values('Abs_Std_Coef', ascending=False)\n",
    "\n",
    "print(importance_df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\ud83d\udcca Interpretation:\")\n",
    "most_important = importance_df.iloc[0]\n",
    "print(f\"  - Most influential: {most_important['Feature']}\")\n",
    "print(f\"  - Effect: {most_important['Coefficient']:.4f} change in yield per unit change\")\n",
    "\n",
    "for _, row in importance_df.iterrows():\n",
    "    direction = \"increases\" if row['Coefficient'] > 0 else \"decreases\"\n",
    "    print(f\"  - {row['Feature']}: {direction} yield\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66910299",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Real-World Project: Post-Silicon Validation Yield Prediction\n",
    "\n",
    "Now let's apply everything to a realistic **post-silicon validation** scenario.\n",
    "\n",
    "### \ud83d\udd2c Post-Silicon Validation Context\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Silicon Chip Manufacturing] --> B[Wafer Testing]\n",
    "    B --> C[Die Level Tests]\n",
    "    C --> D[Electrical Parameters]\n",
    "    C --> E[Environmental Conditions]\n",
    "    C --> F[Position on Wafer]\n",
    "    D --> G[Yield Prediction Model]\n",
    "    E --> G\n",
    "    F --> G\n",
    "    G --> H[Pass/Fail Decision]\n",
    "    H --> I[Yield Optimization]\n",
    "    style G fill:#FF6F00,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style I fill:#4CAF50,stroke:#333,stroke-width:2px,color:#fff\n",
    "```\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Generate realistic post-silicon validation data mimicking real STDF files\n",
    "\n",
    "**Key Points:**\n",
    "1. **Realistic Parameters**: Voltage, current, frequency, temperature from actual semiconductor test specifications\n",
    "2. **Wafer-Level Effects**: Systematic variations across wafers (process differences)\n",
    "3. **Position Effects**: Edge dies have lower yield (thermal and mechanical stress)\n",
    "4. **Complex Interactions**: Yield depends on multiple interacting factors (more realistic than simple linear relationships)\n",
    "\n",
    "**Post-Silicon Validation Scenarios:**\n",
    "- **Electrical Tests**: VDD, VSS, leakage current, operating frequency\n",
    "- **Environmental**: Temperature and humidity during test\n",
    "- **Spatial**: Die position affects yield (edge effects)\n",
    "- **Process**: Wafer-to-wafer variation from manufacturing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b0b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_realistic_stdf_data(n_devices=5000):\n",
    "    \"\"\"\n",
    "    Generate realistic STDF data for semiconductor test\n",
    "    \n",
    "    Real-world scenario: Predict final yield based on early-stage test parameters\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Test parameters (realistic ranges for semiconductor testing)\n",
    "    data = {\n",
    "        # Electrical tests\n",
    "        'vdd_voltage': np.random.normal(3.3, 0.05, n_devices),      # V\n",
    "        'vss_voltage': np.random.normal(0.0, 0.02, n_devices),      # V\n",
    "        'leakage_current': np.random.lognormal(-3, 0.5, n_devices), # \u00b5A\n",
    "        'freq_mhz': np.random.normal(1000, 50, n_devices),          # MHz\n",
    "        \n",
    "        # Environmental conditions\n",
    "        'temp_celsius': np.random.normal(25, 3, n_devices),         # \u00b0C\n",
    "        'humidity_pct': np.random.normal(45, 5, n_devices),         # %\n",
    "        \n",
    "        # Process parameters\n",
    "        'wafer_id': np.random.randint(1, 26, n_devices),            # Wafer 1-25\n",
    "        'die_x': np.random.randint(0, 50, n_devices),               # Position X\n",
    "        'die_y': np.random.randint(0, 50, n_devices),               # Position Y\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Complex yield formula (realistic dependencies)\n",
    "    base_yield = 85\n",
    "    \n",
    "    yield_score = (\n",
    "        base_yield +\n",
    "        50 * (df['vdd_voltage'] - 3.3) +              # Voltage deviation\n",
    "        -10 * df['leakage_current'] +                  # Leakage is bad\n",
    "        0.01 * (df['freq_mhz'] - 1000) +              # Frequency matters\n",
    "        -0.2 * abs(df['temp_celsius'] - 25) +         # Temp deviation\n",
    "        -0.1 * abs(df['humidity_pct'] - 45) +         # Humidity deviation\n",
    "        np.random.normal(0, 2, n_devices)              # Random variation\n",
    "    )\n",
    "    \n",
    "    # Add wafer-level systematic variation\n",
    "    wafer_effects = df['wafer_id'] * 0.1 - 1.25\n",
    "    yield_score += wafer_effects\n",
    "    \n",
    "    # Add position effects (edge dies have lower yield)\n",
    "    edge_penalty = (\n",
    "        np.minimum(df['die_x'], 50 - df['die_x']) +\n",
    "        np.minimum(df['die_y'], 50 - df['die_y'])\n",
    "    ) * -0.05\n",
    "    yield_score += edge_penalty\n",
    "    \n",
    "    # Clip to realistic range\n",
    "    df['yield_score'] = np.clip(yield_score, 60, 100)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate realistic data\n",
    "stdf_df = generate_realistic_stdf_data(5000)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"REALISTIC STDF DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total devices: {len(stdf_df)}\")\n",
    "print(f\"\\nFeatures:\\n{stdf_df.columns.tolist()}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(stdf_df.head(10))\n",
    "print(f\"\\nStatistics:\")\n",
    "print(stdf_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6884558",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Train and evaluate model on realistic post-silicon validation data\n",
    "\n",
    "**Key Points:**\n",
    "1. **Feature Scaling**: StandardScaler normalizes features (different units: voltage, frequency, position) to same scale\n",
    "2. **Train-Test Split**: 80-20 split ensures model evaluated on unseen data\n",
    "3. **Feature Importance**: Identifies which test parameters most strongly predict yield\n",
    "4. **Business Metrics**: R\u00b2, RMSE, MAE translate to actionable insights for manufacturing optimization\n",
    "\n",
    "**Real-World Application:**\n",
    "- Early prediction of yield reduces test time and cost\n",
    "- Identifies problematic process parameters for optimization\n",
    "- Enables data-driven decisions in manufacturing\n",
    "- Can integrate with ATE (Automatic Test Equipment) systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df5221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "feature_cols = ['vdd_voltage', 'vss_voltage', 'leakage_current', 'freq_mhz',\n",
    "                'temp_celsius', 'humidity_pct', 'wafer_id', 'die_x', 'die_y']\n",
    "\n",
    "X_stdf = stdf_df[feature_cols].values\n",
    "y_stdf = stdf_df['yield_score'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train_stdf, X_test_stdf, y_train_stdf, y_test_stdf = train_test_split(\n",
    "    X_stdf, y_stdf, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_stdf = StandardScaler()\n",
    "X_train_stdf_scaled = scaler_stdf.fit_transform(X_train_stdf)\n",
    "X_test_stdf_scaled = scaler_stdf.transform(X_test_stdf)\n",
    "\n",
    "# Train model\n",
    "lr_stdf = LinearRegression()\n",
    "lr_stdf.fit(X_train_stdf_scaled, y_train_stdf)\n",
    "\n",
    "# Predictions\n",
    "y_pred_stdf = lr_stdf.predict(X_test_stdf_scaled)\n",
    "\n",
    "# Evaluate\n",
    "r2_stdf = r2_score(y_test_stdf, y_pred_stdf)\n",
    "rmse_stdf = np.sqrt(mean_squared_error(y_test_stdf, y_pred_stdf))\n",
    "mae_stdf = mean_absolute_error(y_test_stdf, y_pred_stdf)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STDF YIELD PREDICTION MODEL - RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"R\u00b2 Score:  {r2_stdf:.4f}\")\n",
    "print(f\"RMSE:      {rmse_stdf:.4f} yield points\")\n",
    "print(f\"MAE:       {mae_stdf:.4f} yield points\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Feature importance for STDF model\n",
    "importance_stdf = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': np.abs(lr_stdf.coef_)\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "print(importance_stdf.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c41f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize STDF model results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Predicted vs Actual\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(y_test_stdf, y_pred_stdf, alpha=0.4, s=20)\n",
    "ax1.plot([y_test_stdf.min(), y_test_stdf.max()], \n",
    "         [y_test_stdf.min(), y_test_stdf.max()], 'r--', lw=2)\n",
    "ax1.set_xlabel('Actual Yield Score', fontsize=12)\n",
    "ax1.set_ylabel('Predicted Yield Score', fontsize=12)\n",
    "ax1.set_title(f'STDF Prediction Results (R\u00b2={r2_stdf:.3f})', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Feature Importance\n",
    "ax2 = axes[0, 1]\n",
    "ax2.barh(importance_stdf['Feature'], importance_stdf['Importance'])\n",
    "ax2.set_xlabel('Importance (Abs Coefficient)', fontsize=12)\n",
    "ax2.set_title('Feature Importance for Yield Prediction', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Error Distribution\n",
    "ax3 = axes[1, 0]\n",
    "errors = y_test_stdf - y_pred_stdf\n",
    "ax3.hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "ax3.axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "ax3.set_xlabel('Prediction Error', fontsize=12)\n",
    "ax3.set_ylabel('Frequency', fontsize=12)\n",
    "ax3.set_title('Error Distribution', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Residual Plot\n",
    "ax4 = axes[1, 1]\n",
    "ax4.scatter(y_pred_stdf, errors, alpha=0.4, s=20)\n",
    "ax4.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "ax4.set_xlabel('Predicted Yield Score', fontsize=12)\n",
    "ax4.set_ylabel('Residuals', fontsize=12)\n",
    "ax4.set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2705 STDF yield prediction model complete!\")\n",
    "print(\"\\n\ud83d\udca1 Business Impact:\")\n",
    "print(f\"   - Can predict yield within \u00b1{mae_stdf:.2f} points\")\n",
    "print(f\"   - Explains {r2_stdf*100:.1f}% of yield variation\")\n",
    "print(f\"   - Top factors: {', '.join(importance_stdf.head(3)['Feature'].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5d91f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Advanced Topics\n",
    "\n",
    "### \ud83d\ude80 Extending Linear Regression\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Basic Linear Regression] --> B[Polynomial Features]\n",
    "    A --> C[Regularization]\n",
    "    A --> D[Feature Engineering]\n",
    "    B --> E[Capture Non-linearity]\n",
    "    C --> F[Ridge - L2]\n",
    "    C --> G[Lasso - L1]\n",
    "    C --> H[ElasticNet]\n",
    "    D --> I[Interactions]\n",
    "    D --> J[Transformations]\n",
    "    style A fill:#2196F3,stroke:#333,stroke-width:2px,color:#fff\n",
    "```\n",
    "\n",
    "### 9.1 Polynomial Features (Handling Non-linearity)\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Extend linear regression to capture non-linear relationships\n",
    "\n",
    "**Key Points:**\n",
    "1. **Polynomial Features**: Creates squared terms (x\u00b2) and interaction terms (x\u2081 \u00d7 x\u2082) from original features\n",
    "2. **Still Linear Model**: Despite polynomial features, it's still linear in coefficients (linear regression can fit it)\n",
    "3. **Curse of Dimensionality**: 9 features \u2192 54 polynomial features (combinations grow quickly)\n",
    "4. **Trade-off**: Better fit on training data but risk of overfitting on test data\n",
    "\n",
    "**When to Use:**\n",
    "- Clear non-linear patterns in scatter plots\n",
    "- Domain knowledge suggests quadratic/interaction effects (e.g., voltage \u00d7 current = power)\n",
    "- Linear model underperforming despite good data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create polynomial features (degree 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_stdf_scaled)\n",
    "X_test_poly = poly.transform(X_test_stdf_scaled)\n",
    "\n",
    "print(f\"Original features: {X_train_stdf_scaled.shape[1]}\")\n",
    "print(f\"Polynomial features: {X_train_poly.shape[1]}\")\n",
    "print(f\"(Includes interactions and squares)\")\n",
    "\n",
    "# Train polynomial model\n",
    "lr_poly = LinearRegression()\n",
    "lr_poly.fit(X_train_poly, y_train_stdf)\n",
    "y_pred_poly = lr_poly.predict(X_test_poly)\n",
    "\n",
    "# Compare\n",
    "r2_poly = r2_score(y_test_stdf, y_pred_poly)\n",
    "rmse_poly = np.sqrt(mean_squared_error(y_test_stdf, y_pred_poly))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POLYNOMIAL FEATURES COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<20} {'Linear':<15} {'Polynomial':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'R\u00b2 Score':<20} {r2_stdf:<15.4f} {r2_poly:<15.4f}\")\n",
    "print(f\"{'RMSE':<20} {rmse_stdf:<15.4f} {rmse_poly:<15.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if r2_poly > r2_stdf:\n",
    "    print(\"\u2705 Polynomial features improved performance!\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Linear model is sufficient for this data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33425bb0",
   "metadata": {},
   "source": [
    "### 9.2 Regularization Preview (Ridge & Lasso)\n",
    "\n",
    "When you have many features, regularization helps prevent overfitting.\n",
    "We'll cover this in detail in **012_Ridge_Lasso_ElasticNet.ipynb**.\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Introduce regularization techniques for preventing overfitting\n",
    "\n",
    "**Key Points:**\n",
    "1. **Ridge (L2)**: Shrinks coefficients toward zero but keeps all features (adds penalty: \u03b1\u2211\u03b2\u00b2)\n",
    "2. **Lasso (L1)**: Shrinks some coefficients exactly to zero = automatic feature selection (adds penalty: \u03b1\u2211|\u03b2|)\n",
    "3. **Alpha Parameter**: Controls regularization strength (higher \u03b1 = more regularization)\n",
    "4. **When Needed**: Many features, multicollinearity, or polynomial features (54 features here!)\n",
    "\n",
    "**Preview of Notebook 012:**\n",
    "- Detailed math behind regularization\n",
    "- Cross-validated alpha selection\n",
    "- Feature selection with Lasso\n",
    "- ElasticNet (combination of Ridge + Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988ce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# Ridge regression (L2 regularization)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_poly, y_train_stdf)\n",
    "y_pred_ridge = ridge.predict(X_test_poly)\n",
    "r2_ridge = r2_score(y_test_stdf, y_pred_ridge)\n",
    "\n",
    "# Lasso regression (L1 regularization)\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_poly, y_train_stdf)\n",
    "y_pred_lasso = lasso.predict(X_test_poly)\n",
    "r2_lasso = r2_score(y_test_stdf, y_pred_lasso)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"REGULARIZATION COMPARISON (Preview)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'R\u00b2 Score':<15} {'Non-zero Coefs'}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Linear':<20} {r2_stdf:<15.4f} {len(feature_cols)}\")\n",
    "print(f\"{'Polynomial':<20} {r2_poly:<15.4f} {X_train_poly.shape[1]}\")\n",
    "print(f\"{'Ridge':<20} {r2_ridge:<15.4f} {X_train_poly.shape[1]}\")\n",
    "print(f\"{'Lasso':<20} {r2_lasso:<15.4f} {np.sum(lasso.coef_ != 0)}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n\ud83d\udca1 Lasso performs feature selection by setting some coefficients to zero\")\n",
    "print(\"   \u2192 More in notebook 012!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e4cb12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Key Takeaways\n",
    "\n",
    "### \u2705 When Linear Regression Works Well:\n",
    "- Linear relationships between features and target\n",
    "- Continuous target variable\n",
    "- Need for interpretability\n",
    "- Fast training/prediction required\n",
    "- Features are not highly correlated\n",
    "\n",
    "### \u26a0\ufe0f Limitations:\n",
    "- Cannot capture non-linear relationships (use polynomial features or other models)\n",
    "- Sensitive to outliers (consider robust regression)\n",
    "- Assumes linear additive effects\n",
    "- Multicollinearity causes unstable coefficients\n",
    "\n",
    "### \ud83c\udfaf Best Practices:\n",
    "1. Always visualize relationships first (EDA)\n",
    "2. Check model assumptions\n",
    "3. Use cross-validation for robust evaluation\n",
    "4. Scale features for fair coefficient comparison\n",
    "5. Test for multicollinearity (VIF)\n",
    "6. Examine residual plots\n",
    "7. Consider regularization for many features\n",
    "\n",
    "### \ud83d\udcc8 Next Models to Learn:\n",
    "When linear regression isn't enough, progress to:\n",
    "- **011_Polynomial_Regression.ipynb** - Handle non-linear relationships\n",
    "- **012_Ridge_Lasso_ElasticNet.ipynb** - Regularization techniques\n",
    "- **013_Logistic_Regression.ipynb** - Classification problems\n",
    "- **016_Decision_Trees.ipynb** - Complex non-linear patterns\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Real-World Projects\n",
    "\n",
    "### \ud83d\udd2c Post-Silicon Validation Projects\n",
    "\n",
    "#### Project 1: Device Power Consumption Predictor\n",
    "**Objective:** Predict power consumption from voltage, frequency, and temperature\n",
    "- Load STDF power test data\n",
    "- Engineer features: voltage \u00d7 current, frequency bins\n",
    "- Build linear regression model\n",
    "- Validate against specifications\n",
    "- **Business Value:** Early detection of high-power devices, yield optimization\n",
    "\n",
    "#### Project 2: Test Time Estimator\n",
    "**Objective:** Estimate test execution time from test complexity metrics\n",
    "- Features: number of test patterns, vector count, clock frequency\n",
    "- Handle time-series aspects (sequential tests)\n",
    "- Predict total test time per device\n",
    "- **Business Value:** ATE scheduling optimization, capacity planning\n",
    "\n",
    "#### Project 3: Parametric Yield Prediction\n",
    "**Objective:** Predict final yield based on early parametric test results\n",
    "- Features: electrical parameters (Vdd, Vss, leakage, frequency)\n",
    "- Wafer-level and die-level spatial features\n",
    "- Environmental conditions (temp, humidity)\n",
    "- **Business Value:** Early yield prediction, process optimization feedback\n",
    "\n",
    "#### Project 4: Voltage-Frequency Characterization\n",
    "**Objective:** Model voltage-frequency operating curves (V-F curves)\n",
    "- Non-linear relationship (may need polynomial features)\n",
    "- Device-to-device variation modeling\n",
    "- Guardband prediction for production test\n",
    "- **Business Value:** Optimized test limits, reduced guardbands, higher yield\n",
    "\n",
    "### \ud83d\udca1 General AI/ML Projects\n",
    "\n",
    "#### Project 5: Sales Forecasting\n",
    "**Objective:** Predict monthly sales from marketing spend and seasonality\n",
    "- Features: advertising budget, month, previous sales\n",
    "- Time series considerations\n",
    "- Interpretable coefficients for business decisions\n",
    "\n",
    "#### Project 6: Real Estate Price Prediction\n",
    "**Objective:** Estimate house prices from features\n",
    "- Features: square footage, bedrooms, location, age\n",
    "- Feature engineering: price per sq ft, neighborhood clusters\n",
    "- Compare linear vs non-linear models\n",
    "\n",
    "#### Project 7: Customer Lifetime Value (CLV)\n",
    "**Objective:** Predict customer value from behavior metrics\n",
    "- Features: purchase frequency, average order value, tenure\n",
    "- Interaction effects (frequency \u00d7 value)\n",
    "- Segmentation for targeted marketing\n",
    "\n",
    "#### Project 8: Energy Consumption Forecasting\n",
    "**Objective:** Predict building energy usage\n",
    "- Features: temperature, occupancy, time of day, day of week\n",
    "- Seasonal patterns and trends\n",
    "- Optimization recommendations\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83c\udfaf Project Implementation Template\n",
    "\n",
    "```python\n",
    "# 1. Load and explore data\n",
    "df = pd.read_csv('your_data.csv')\n",
    "# or for STDF: df = parse_stdf_file('test_results.stdf')\n",
    "\n",
    "# 2. EDA and visualization\n",
    "# - Scatter plots\n",
    "# - Correlation matrix\n",
    "# - Distribution checks\n",
    "\n",
    "# 3. Feature engineering\n",
    "# - Create interactions\n",
    "# - Transform variables\n",
    "# - Handle missing data\n",
    "\n",
    "# 4. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 5. Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 6. Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 7. Evaluate\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(f\"R\u00b2: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\n",
    "\n",
    "# 8. Interpret and deploy\n",
    "# - Feature importance\n",
    "# - Diagnostic plots\n",
    "# - Save model for production\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! \ud83c\udf89**\n",
    "\n",
    "You've mastered linear regression - the foundation of machine learning!\n",
    "\n",
    "**Next Steps:**\n",
    "- \u2192 **011_Polynomial_Regression.ipynb** for non-linear relationships\n",
    "- \u2192 **041_Feature_Engineering_Masterclass.ipynb** for advanced feature creation\n",
    "- \u2192 **042_Model_Evaluation_Metrics.ipynb** for deeper metric understanding\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Complete!** \u2705"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}