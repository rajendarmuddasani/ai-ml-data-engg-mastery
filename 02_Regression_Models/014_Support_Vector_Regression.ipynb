{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49302820",
   "metadata": {},
   "source": [
    "# 014: Support Vector Regression (SVR)\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** SVR's epsilon-insensitive loss and kernel trick\n",
    "- **Implement** linear and non-linear SVR with RBF/polynomial kernels\n",
    "- **Master** hyperparameter tuning (C, epsilon, gamma) for optimal performance\n",
    "- **Apply** SVR to robust regression with outliers in test data\n",
    "- **Build** accurate predictors resilient to measurement noise and anomalies\n",
    "\n",
    "## üìö What is Support Vector Regression?\n",
    "\n",
    "SVR extends support vector machines to regression by finding a function within an epsilon tube around the data. It's robust to outliers and handles non-linear relationships via kernels.\n",
    "\n",
    "**Why SVR?**\n",
    "- ‚úÖ Robust to outliers (measurement errors, equipment glitches)\n",
    "- ‚úÖ Non-linear modeling via kernels (voltage-frequency curves)\n",
    "- ‚úÖ Sparse solution (only support vectors matter)\n",
    "- ‚úÖ Handles high-dimensional data (100+ test parameters)\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Robust Parametric Prediction**\n",
    "- Input: Test data with 5-10% outliers (sensor spikes, ESD events)\n",
    "- Output: SVR model ignoring outliers, accurate predictions\n",
    "- Value: 88% accuracy vs 72% OLS with outliers, save $4M/year\n",
    "\n",
    "**Non-Linear V-F Characterization**\n",
    "- Input: Voltage-frequency pairs (device speed vs power)\n",
    "- Output: RBF kernel SVR fitting complex non-linear relationship\n",
    "- Value: Precise speed binning, optimize performance 15%\n",
    "\n",
    "**Multi-Site Correlation Analysis**\n",
    "- Input: Parallel test site measurements (site 1-8 readings)\n",
    "- Output: SVR predicting site 1 from sites 2-8 (detect drifts)\n",
    "- Value: Early calibration issues, prevent 2-3% yield loss\n",
    "\n",
    "**Yield Forecasting Under Uncertainty**\n",
    "- Input: Noisy historical yield data, process variations\n",
    "- Output: Robust SVR prediction with confidence intervals\n",
    "- Value: Better capacity planning, reduce inventory costs 20%\n",
    "\n",
    "---\n",
    "\n",
    "Let's master Support Vector Regression! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c947762",
   "metadata": {},
   "source": [
    "# 014: Support Vector Regression (SVR)\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** epsilon-insensitive loss and support vectors\n",
    "- **Master** kernel trick (linear, RBF, polynomial) for non-linear patterns\n",
    "- **Implement** SVR from scratch and with sklearn\n",
    "- **Apply** robust regression to noisy semiconductor test data\n",
    "- **Tune** hyperparameters (C, epsilon, gamma) for optimal performance\n",
    "\n",
    "## üìö What is Support Vector Regression?\n",
    "\n",
    "**Support Vector Regression (SVR)** is a robust regression algorithm that fits a tube of width epsilon (Œµ) around the predicted function, ignoring errors within the tube. Only points outside the tube (support vectors) contribute to the model.\n",
    "\n",
    "Key equation (epsilon-insensitive loss):\n",
    "$$L_\\varepsilon(y, \\hat{y}) = \\begin{cases} 0 & \\text{if } |y - \\hat{y}| \\leq \\varepsilon \\\\ |y - \\hat{y}| - \\varepsilon & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "**Why SVR?**\n",
    "- ‚úÖ Robust to outliers (epsilon tube ignores small errors)\n",
    "- ‚úÖ Kernel trick handles non-linear relationships\n",
    "- ‚úÖ Sparse solution (only support vectors matter)\n",
    "- ‚úÖ Effective in high-dimensional spaces (p > n)\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Outlier-Robust Yield Prediction**\n",
    "- Input: Parametric test data with measurement noise and outliers\n",
    "- Output: Robust yield estimates ignoring transient noise\n",
    "- Value: Stable predictions for capacity planning ($5-10M)\n",
    "\n",
    "**Noise-Resistant Test Time Modeling**\n",
    "- Input: Test time measurements with ATE jitter and variability\n",
    "- Output: SVR model that filters out noise patterns\n",
    "- Value: Accurate test time forecasting (15-25% optimization)\n",
    "\n",
    "**Non-Linear V-F Characterization**\n",
    "- Input: Voltage-frequency sweep data (non-linear relationships)\n",
    "- Output: RBF kernel SVR capturing complex V-F curves\n",
    "- Value: Precise power-performance modeling ($3-8M)\n",
    "\n",
    "**Extreme Condition Performance**\n",
    "- Input: Stress test results (temperature, voltage corners)\n",
    "- Output: SVR predicting behavior at untested corners\n",
    "- Value: Reduced characterization cost (30-40% fewer tests)\n",
    "\n",
    "## üîÑ SVR Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Training Data] --> B[Kernel Transformation]\n",
    "    B --> C[Epsilon Tube Fitting]\n",
    "    C --> D[Support Vector Selection]\n",
    "    D --> E[Sparse Model]\n",
    "    E --> F[Robust Predictions]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style F fill:#e1ffe1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- 010: Linear Regression (regression fundamentals)\n",
    "- 012: Ridge/Lasso (regularization concepts)\n",
    "\n",
    "**Next Steps:**\n",
    "- 024: Support Vector Machines (SVC for classification)\n",
    "- 015: Quantile Regression (another robust approach)\n",
    "\n",
    "---\n",
    "\n",
    "Let's master robust regression with SVR! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5978f330",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc55a3",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Import essential libraries for SVR implementation, visualization, and evaluation.\n",
    "\n",
    "**Key Points:**\n",
    "- **NumPy**: Core mathematical operations for from-scratch SVR implementation\n",
    "- **Pandas**: Data manipulation for post-silicon STDF datasets\n",
    "- **Matplotlib/Seaborn**: Visualize epsilon tubes, support vectors, kernel effects\n",
    "- **Scikit-learn**: Production-ready SVR, StandardScaler (SVR requires scaled features), metrics\n",
    "- **Warnings**: Suppress convergence warnings during hyperparameter search\n",
    "\n",
    "**Why This Matters:** SVR is sensitive to feature scaling (unlike tree-based models). StandardScaler is ESSENTIAL for SVR to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.datasets import make_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acebca5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Part 1: SVR from Scratch (Educational)\n",
    "\n",
    "We'll implement a **simplified Linear SVR** using gradient descent to understand the mechanics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18fac3",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement Linear SVR from scratch using subgradient descent on epsilon-insensitive loss.\n",
    "\n",
    "**Key Points:**\n",
    "- **Epsilon-insensitive loss**: Computes penalty only for errors > Œµ\n",
    "- **Subgradient descent**: Used because epsilon-insensitive loss is non-differentiable at boundaries\n",
    "- **Regularization term**: $\\frac{1}{2}||w||^2$ prevents overfitting\n",
    "- **Support vectors**: Data points with non-zero gradients (outside epsilon tube)\n",
    "- **C parameter**: Balances fit quality vs. model complexity (similar to Ridge/Lasso alpha)\n",
    "\n",
    "**Why This Matters:** Understanding the optimization mechanics reveals why SVR is robust to outliers (outliers contribute limited penalty due to epsilon tube)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVRScratch:\n",
    "    \"\"\"\n",
    "    Linear Support Vector Regression from scratch.\n",
    "    Uses subgradient descent on epsilon-insensitive loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C=1.0, epsilon=0.1, learning_rate=0.01, n_iterations=1000):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        C : float\n",
    "            Penalty parameter (cost of violations)\n",
    "        epsilon : float\n",
    "            Width of epsilon tube (tolerance)\n",
    "        learning_rate : float\n",
    "            Step size for gradient descent\n",
    "        n_iterations : int\n",
    "            Number of training iterations\n",
    "        \"\"\"\n",
    "        self.C = C\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def _compute_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute epsilon-insensitive loss.\n",
    "        \"\"\"\n",
    "        predictions = X @ self.w + self.b\n",
    "        errors = np.abs(y - predictions)\n",
    "        \n",
    "        # Epsilon-insensitive: max(0, |error| - epsilon)\n",
    "        epsilon_loss = np.maximum(0, errors - self.epsilon)\n",
    "        \n",
    "        # Total loss: regularization + C * epsilon_loss\n",
    "        loss = 0.5 * np.dot(self.w, self.w) + self.C * np.sum(epsilon_loss)\n",
    "        return loss\n",
    "    \n",
    "    def _compute_gradients(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute subgradients for weights and bias.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        predictions = X @ self.w + self.b\n",
    "        errors = y - predictions\n",
    "        \n",
    "        # Initialize gradients\n",
    "        grad_w = self.w.copy()  # Regularization term\n",
    "        grad_b = 0\n",
    "        \n",
    "        # Add epsilon-insensitive loss gradients\n",
    "        for i in range(n_samples):\n",
    "            abs_error = np.abs(errors[i])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Code Continuation (2/2)\n",
    "\n",
    "Continuing implementation...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            if abs_error > self.epsilon:\n",
    "                # Outside epsilon tube\n",
    "                sign = np.sign(errors[i])\n",
    "                grad_w -= self.C * sign * X[i]\n",
    "                grad_b -= self.C * sign\n",
    "        \n",
    "        return grad_w, grad_b\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train SVR using subgradient descent.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for iteration in range(self.n_iterations):\n",
    "            # Compute gradients\n",
    "            grad_w, grad_b = self._compute_gradients(X, y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.w -= self.learning_rate * grad_w\n",
    "            self.b -= self.learning_rate * grad_b\n",
    "            \n",
    "            # Track loss\n",
    "            if iteration % 100 == 0:\n",
    "                loss = self._compute_loss(X, y)\n",
    "                self.loss_history.append(loss)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \"\"\"\n",
    "        return X @ self.w + self.b\n",
    "    \n",
    "    def get_support_vectors(self, X, y):\n",
    "        \"\"\"\n",
    "        Identify support vectors (points outside epsilon tube).\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        errors = np.abs(y - predictions)\n",
    "        return errors > self.epsilon\n",
    "\n",
    "print(\"‚úÖ LinearSVRScratch class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c90dec6",
   "metadata": {},
   "source": [
    "### Test From-Scratch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c805d0",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Validate our from-scratch SVR implementation on synthetic data with outliers.\n",
    "\n",
    "**Key Points:**\n",
    "- **Synthetic data**: Linear relationship + noise + outliers to test robustness\n",
    "- **Outlier injection**: 10% of data points have extreme values (¬±3 standard deviations)\n",
    "- **Feature scaling**: StandardScaler applied (critical for SVR performance)\n",
    "- **Epsilon tube visualization**: Shows which points are support vectors (outside tube)\n",
    "- **Performance metrics**: RMSE and R¬≤ to quantify prediction quality\n",
    "\n",
    "**Why This Matters:** Demonstrates SVR's core advantage‚Äîoutliers have limited impact on the regression line due to epsilon-insensitive loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7546a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with outliers\n",
    "np.random.seed(42)\n",
    "X_synthetic = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_synthetic = 2 * X_synthetic.ravel() + 1 + np.random.normal(0, 1, 100)\n",
    "\n",
    "# Add outliers (10% of data)\n",
    "outlier_indices = np.random.choice(100, size=10, replace=False)\n",
    "y_synthetic[outlier_indices] += np.random.choice([-1, 1], size=10) * np.random.uniform(5, 10, size=10)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_synthetic, y_synthetic, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features (CRITICAL for SVR)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train from-scratch SVR\n",
    "svr_scratch = LinearSVRScratch(C=1.0, epsilon=0.5, learning_rate=0.01, n_iterations=1000)\n",
    "svr_scratch.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_scratch = svr_scratch.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "rmse_scratch = np.sqrt(mean_squared_error(y_test, y_pred_scratch))\n",
    "r2_scratch = r2_score(y_test, y_pred_scratch)\n",
    "\n",
    "print(\"\\nüîß From-Scratch SVR Performance:\")\n",
    "print(f\"RMSE: {rmse_scratch:.4f}\")\n",
    "print(f\"R¬≤: {r2_scratch:.4f}\")\n",
    "\n",
    "# Identify support vectors\n",
    "support_vectors = svr_scratch.get_support_vectors(X_train_scaled, y_train)\n",
    "print(f\"\\nSupport Vectors: {np.sum(support_vectors)} / {len(y_train)} ({100*np.mean(support_vectors):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca27db",
   "metadata": {},
   "source": [
    "### Visualize Epsilon Tube and Support Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3162e9bf",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Visualize the epsilon tube concept and identify support vectors (points driving the model).\n",
    "\n",
    "**Key Points:**\n",
    "- **Epsilon tube**: Gray shaded region (¬±Œµ around regression line) where errors are ignored\n",
    "- **Support vectors (red)**: Points outside the tube that contribute to the loss function\n",
    "- **Regular points (blue)**: Points inside tube with zero loss\n",
    "- **Robust regression**: SVR line ignores outliers better than OLS would\n",
    "- **Sparse model**: Only support vectors matter for predictions (efficiency advantage)\n",
    "\n",
    "**Why This Matters:** Visual understanding of why SVR is robust‚Äîmost data points (inside tube) don't influence the model at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e8a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot epsilon tube\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Generate smooth prediction line\n",
    "X_plot = np.linspace(X_train_scaled.min(), X_train_scaled.max(), 100).reshape(-1, 1)\n",
    "y_plot = svr_scratch.predict(X_plot)\n",
    "\n",
    "# Identify support vectors on training data\n",
    "y_train_pred = svr_scratch.predict(X_train_scaled)\n",
    "train_errors = np.abs(y_train - y_train_pred)\n",
    "support_vector_mask = train_errors > svr_scratch.epsilon\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X_train_scaled[~support_vector_mask], y_train[~support_vector_mask], \n",
    "           c='blue', alpha=0.5, label='Regular Points (inside tube)', s=50)\n",
    "plt.scatter(X_train_scaled[support_vector_mask], y_train[support_vector_mask], \n",
    "           c='red', marker='x', s=100, label='Support Vectors (outside tube)', linewidths=2)\n",
    "plt.plot(X_plot, y_plot, 'g-', linewidth=2, label='SVR Prediction')\n",
    "plt.fill_between(X_plot.ravel(), \n",
    "                 y_plot - svr_scratch.epsilon, \n",
    "                 y_plot + svr_scratch.epsilon, \n",
    "                 alpha=0.2, color='gray', label=f'Epsilon Tube (Œµ={svr_scratch.epsilon})')\n",
    "\n",
    "plt.xlabel('Feature (scaled)', fontsize=12)\n",
    "plt.ylabel('Target', fontsize=12)\n",
    "plt.title('SVR: Epsilon Tube and Support Vectors', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Interpretation:\")\n",
    "print(f\"‚Ä¢ Red X markers: {np.sum(support_vector_mask)} support vectors (contribute to loss)\")\n",
    "print(f\"‚Ä¢ Blue circles: {np.sum(~support_vector_mask)} regular points (zero loss, ignored)\")\n",
    "print(f\"‚Ä¢ Gray band: Epsilon tube (¬±{svr_scratch.epsilon}) where errors are tolerated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef53924",
   "metadata": {},
   "source": [
    "### Loss Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7c0518",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Verify that our from-scratch SVR implementation converges during training.\n",
    "\n",
    "**Key Points:**\n",
    "- **Loss function**: Combination of regularization term ($\\frac{1}{2}||w||^2$) and epsilon-insensitive loss\n",
    "- **Convergence pattern**: Should decrease rapidly initially, then stabilize\n",
    "- **Non-smooth curve**: Expected due to subgradient descent (not true gradient)\n",
    "- **Learning rate impact**: If loss diverges, learning rate is too high\n",
    "- **Validation**: Confirms our implementation is optimizing correctly\n",
    "\n",
    "**Why This Matters:** Loss convergence is essential validation‚Äîif loss doesn't decrease, the implementation has bugs or hyperparameters are wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea72fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss convergence\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(svr_scratch.loss_history, linewidth=2)\n",
    "plt.xlabel('Iteration (x100)', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('SVR Training Loss Convergence', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìâ Loss Analysis:\")\n",
    "print(f\"‚Ä¢ Initial loss: {svr_scratch.loss_history[0]:.4f}\")\n",
    "print(f\"‚Ä¢ Final loss: {svr_scratch.loss_history[-1]:.4f}\")\n",
    "print(f\"‚Ä¢ Loss reduction: {100*(1 - svr_scratch.loss_history[-1]/svr_scratch.loss_history[0]):.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
