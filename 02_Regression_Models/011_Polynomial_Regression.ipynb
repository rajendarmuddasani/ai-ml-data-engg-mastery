{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e8c771",
   "metadata": {},
   "source": [
    "# 011: Polynomial Regression",
    "",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d21993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, validation_curve\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('\u2705 Libraries imported successfully')\n",
    "print(f'NumPy version: {np.__version__}')\n",
    "print(f'Pandas version: {pd.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e40ca",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Import essential libraries and configure the environment for polynomial regression\n",
    "\n",
    "**Key Points:**\n",
    "- **PolynomialFeatures**: Scikit-learn's transformer for creating polynomial and interaction features\n",
    "- **Pipeline**: Chains preprocessing (polynomial transform) and modeling steps for cleaner code\n",
    "- **validation_curve**: Tool for analyzing how model performance changes with polynomial degree\n",
    "- **Configuration**: Reproducibility and visualization setup matching workspace standards\n",
    "\n",
    "**Why This Matters:**\n",
    "- Pipeline prevents data leakage by ensuring train/test splits happen before feature engineering\n",
    "- validation_curve is critical for degree selection to avoid overfitting\n",
    "- Consistent setup allows fair comparison with linear regression baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5631515",
   "metadata": {},
   "source": [
    "### 2.1 Generate Non-Linear Synthetic Dataset\n",
    "\n",
    "Create data with **known polynomial relationships** to validate our implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101fd30f",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Generate synthetic data with quadratic and cubic relationships for educational validation\n",
    "\n",
    "**Key Points:**\n",
    "- **Temperature-Performance**: Quadratic relationship mimics thermal effects (peak performance at optimal temp)\n",
    "- **Voltage-Frequency**: Cubic relationship models complex V-F curves from semiconductor physics\n",
    "- **Controlled Noise**: Added to simulate real measurement variance while maintaining ground truth\n",
    "- **Domain Realism**: Coefficients chosen to reflect realistic semiconductor parameter ranges\n",
    "\n",
    "**Why This Approach:**\n",
    "- Knowing true polynomial degree lets us validate model selection methods\n",
    "- Mimics real STDF patterns where temperature and voltage affect device performance non-linearly\n",
    "- Allows comparison between different polynomial degrees to demonstrate overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcaaa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_polynomial_data(n_samples=200, noise_level=5.0):\n",
    "    \"\"\"\n",
    "    Generate synthetic data with polynomial relationships\n",
    "    Simulates post-silicon validation scenarios\n",
    "    \n",
    "    Returns:\n",
    "        X: Features (temperature, voltage)\n",
    "        y: Target (device performance score)\n",
    "    \"\"\"\n",
    "    # Temperature feature (Celsius): 20-100\u00b0C\n",
    "    temperature = np.random.uniform(20, 100, n_samples)\n",
    "    \n",
    "    # Voltage feature (Volts): 0.8-1.2V\n",
    "    voltage = np.random.uniform(0.8, 1.2, n_samples)\n",
    "    \n",
    "    # Ground truth: Quadratic relationship with temperature (optimal at 60\u00b0C)\n",
    "    # Performance degrades at extreme temperatures\n",
    "    temp_effect = 100 - 0.02 * (temperature - 60)**2\n",
    "    \n",
    "    # Ground truth: Cubic relationship with voltage (complex V-F curve)\n",
    "    voltage_effect = 50 * voltage**3 - 30 * voltage**2 + 20 * voltage\n",
    "    \n",
    "    # Combined performance score with noise\n",
    "    y = temp_effect + voltage_effect + np.random.normal(0, noise_level, n_samples)\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X = np.column_stack([temperature, voltage])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate dataset\n",
    "X, y = generate_polynomial_data(n_samples=200, noise_level=3.0)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "df = pd.DataFrame(X, columns=['Temperature_C', 'Voltage_V'])\n",
    "df['Performance_Score'] = y\n",
    "\n",
    "print('\u2705 Synthetic polynomial dataset generated')\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print('\\nFirst 5 samples:')\n",
    "print(df.head())\n",
    "print('\\nDataset statistics:')\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c18fb",
   "metadata": {},
   "source": [
    "### 2.2 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfffd579",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Visualize relationships to identify non-linearity before modeling\n",
    "\n",
    "**Key Points:**\n",
    "- **Scatter plots**: Reveal curved patterns that linear regression cannot capture\n",
    "- **Temperature plot**: Shows inverted U-shape (quadratic) with peak performance at mid-range\n",
    "- **Voltage plot**: Shows cubic curve with complex non-monotonic behavior\n",
    "- **Visual evidence**: Justifies using polynomial regression over linear baseline\n",
    "\n",
    "**Why This Matters:**\n",
    "- Visual inspection is first step in determining polynomial degree\n",
    "- Helps set expectations for model performance\n",
    "- Documents data characteristics for stakeholders (e.g., \"performance peaks at 60\u00b0C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598de9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize relationships\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Temperature vs Performance\n",
    "axes[0].scatter(df['Temperature_C'], df['Performance_Score'], alpha=0.6, edgecolor='k')\n",
    "axes[0].set_xlabel('Temperature (\u00b0C)', fontsize=12)\n",
    "axes[0].set_ylabel('Performance Score', fontsize=12)\n",
    "axes[0].set_title('Temperature vs Performance\\n(Non-linear Quadratic Relationship)', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Voltage vs Performance\n",
    "axes[1].scatter(df['Voltage_V'], df['Performance_Score'], alpha=0.6, edgecolor='k', color='coral')\n",
    "axes[1].set_xlabel('Voltage (V)', fontsize=12)\n",
    "axes[1].set_ylabel('Performance Score', fontsize=12)\n",
    "axes[1].set_title('Voltage vs Performance\\n(Non-linear Cubic Relationship)', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\ud83d\udcca Visual inspection confirms non-linear relationships')\n",
    "print('   \u2192 Temperature shows inverted U-shape (quadratic)')\n",
    "print('   \u2192 Voltage shows complex curve (cubic)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02de4c9f",
   "metadata": {},
   "source": [
    "### 2.3 Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00318a7f",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Split data for unbiased model evaluation\n",
    "\n",
    "**Key Points:**\n",
    "- **80-20 split**: Standard ratio balancing training data quantity and test reliability\n",
    "- **Random state**: Ensures reproducible splits for consistent experimentation\n",
    "- **Stratification not needed**: Continuous target variable (unlike classification)\n",
    "- **Validation importance**: Critical for detecting overfitting in polynomial models\n",
    "\n",
    "**Why This Matters:**\n",
    "- Polynomial models are prone to overfitting - test set reveals this\n",
    "- Without proper validation, high-degree polynomials appear artificially good\n",
    "- Simulates production scenario where model faces unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddeff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f'\u2705 Data split completed')\n",
    "print(f'Training samples: {X_train.shape[0]}')\n",
    "print(f'Test samples: {X_test.shape[0]}')\n",
    "print(f'Feature count: {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13acfc55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Mathematical Foundation\n",
    "\n",
    "### 3.1 Polynomial Feature Transformation\n",
    "\n",
    "Given features $x_1, x_2$, polynomial features of degree $d=2$ include:\n",
    "\n",
    "$$\\text{Original: } [x_1, x_2]$$\n",
    "\n",
    "$$\\text{Degree 2: } [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]$$\n",
    "\n",
    "For $n$ features and degree $d$, number of polynomial features:\n",
    "\n",
    "$$\\text{Number of features} = \\binom{n+d}{d} = \\frac{(n+d)!}{n! \\cdot d!}$$\n",
    "\n",
    "**Example:** 2 features, degree 3 \u2192 $\\binom{2+3}{3} = \\binom{5}{3} = 10$ features\n",
    "\n",
    "### 3.2 Model Fitting\n",
    "\n",
    "After transformation, we fit standard linear regression:\n",
    "\n",
    "$$\\hat{y} = \\mathbf{X}_{\\text{poly}} \\boldsymbol{\\beta}$$\n",
    "\n",
    "Where $\\mathbf{X}_{\\text{poly}}$ contains all polynomial terms.\n",
    "\n",
    "### 3.3 The Bias-Variance Tradeoff\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Low Degree<br/>High Bias<br/>Underfitting] --> B[Optimal Degree<br/>Balanced]\n",
    "    B --> C[High Degree<br/>High Variance<br/>Overfitting]\n",
    "    style B fill:#4CAF50,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style A fill:#FF9800,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style C fill:#f44336,stroke:#333,stroke-width:2px,color:#fff\n",
    "```\n",
    "\n",
    "- **Underfitting (degree too low)**: Model too simple, misses true relationship\n",
    "- **Optimal**: Captures true pattern without memorizing noise\n",
    "- **Overfitting (degree too high)**: Model memorizes training noise, poor generalization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8839ac2",
   "metadata": {},
   "source": [
    "## 4. Implementation from Scratch\n",
    "\n",
    "Build polynomial regression manually to understand the mechanics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7d0f3c",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement polynomial regression from scratch for educational understanding\n",
    "\n",
    "**Key Points:**\n",
    "- **Manual feature generation**: Creates polynomial terms using nested loops over features and degrees\n",
    "- **Normal equation**: Computes optimal coefficients $\\boldsymbol{\\beta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$\n",
    "- **Interaction terms**: Includes cross-products like $x_1 x_2$ for feature interactions\n",
    "- **Validation**: Compare against sklearn to ensure correctness\n",
    "\n",
    "**Why This Matters:**\n",
    "- Demystifies polynomial regression as \"linear regression on transformed features\"\n",
    "- Shows how feature explosion happens with high degrees\n",
    "- Understanding internals helps debug production issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de29930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialRegressionScratch:\n",
    "    \"\"\"\n",
    "    Polynomial Regression implemented from scratch\n",
    "    \"\"\"\n",
    "    def __init__(self, degree=2):\n",
    "        self.degree = degree\n",
    "        self.coefficients = None\n",
    "        self.intercept = None\n",
    "        \n",
    "    def _create_polynomial_features(self, X):\n",
    "        \"\"\"\n",
    "        Transform features to polynomial features\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Start with bias term (column of ones)\n",
    "        X_poly = [np.ones(n_samples)]\n",
    "        \n",
    "        # Add polynomial terms\n",
    "        for d in range(1, self.degree + 1):\n",
    "            for i in range(n_features):\n",
    "                X_poly.append(X[:, i] ** d)\n",
    "                \n",
    "                # Add interaction terms for degree > 1\n",
    "                if d > 1:\n",
    "                    for j in range(i + 1, n_features):\n",
    "                        X_poly.append((X[:, i] ** (d-1)) * X[:, j])\n",
    "        \n",
    "        return np.column_stack(X_poly)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit polynomial regression using normal equation\n",
    "        \"\"\"\n",
    "        # Transform to polynomial features\n",
    "        X_poly = self._create_polynomial_features(X)\n",
    "        \n",
    "        # Normal equation: \u03b2 = (X^T X)^(-1) X^T y\n",
    "        X_transpose = X_poly.T\n",
    "        \n",
    "        try:\n",
    "            # Compute coefficients\n",
    "            coefficients = np.linalg.inv(X_transpose @ X_poly) @ X_transpose @ y\n",
    "            \n",
    "            self.intercept = coefficients[0]\n",
    "            self.coefficients = coefficients[1:]\n",
    "            \n",
    "            return self\n",
    "            \n",
    "        except np.linalg.LinAlgError:\n",
    "            raise ValueError(\"Matrix is singular. Try regularization or reduce degree.\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        \"\"\"\n",
    "        X_poly = self._create_polynomial_features(X)\n",
    "        return X_poly @ np.concatenate([[self.intercept], self.coefficients])\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate R\u00b2 score\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Train from-scratch model (degree 2)\n",
    "model_scratch = PolynomialRegressionScratch(degree=2)\n",
    "model_scratch.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_r2_scratch = model_scratch.score(X_train, y_train)\n",
    "test_r2_scratch = model_scratch.score(X_test, y_test)\n",
    "y_pred_scratch = model_scratch.predict(X_test)\n",
    "rmse_scratch = np.sqrt(mean_squared_error(y_test, y_pred_scratch))\n",
    "\n",
    "print('\u2705 From-Scratch Polynomial Regression (Degree 2)')\n",
    "print(f'Training R\u00b2: {train_r2_scratch:.4f}')\n",
    "print(f'Test R\u00b2: {test_r2_scratch:.4f}')\n",
    "print(f'Test RMSE: {rmse_scratch:.4f}')\n",
    "print(f'\\nNumber of coefficients: {len(model_scratch.coefficients) + 1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e582ba1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Production Implementation with Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d83f44a",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Use production-ready sklearn pipeline for polynomial regression\n",
    "\n",
    "**Key Points:**\n",
    "- **Pipeline**: Chains PolynomialFeatures \u2192 StandardScaler \u2192 LinearRegression in one object\n",
    "- **StandardScaler**: Essential for polynomial features to prevent numerical instability\n",
    "- **include_bias=False**: Pipeline's LinearRegression adds intercept, avoid duplication\n",
    "- **Degree sweep**: Test degrees 1-5 to find optimal complexity\n",
    "\n",
    "**Why This Matters:**\n",
    "- Pipeline ensures consistent preprocessing in train/test/production\n",
    "- StandardScaler prevents overflow with high-degree polynomial terms\n",
    "- Systematic degree evaluation prevents arbitrary choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76851be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple polynomial degrees\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "results = []\n",
    "\n",
    "for degree in degrees:\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('poly_features', PolynomialFeatures(degree=degree, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Train\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_r2 = pipeline.score(X_train, y_train)\n",
    "    test_r2 = pipeline.score(X_test, y_test)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, \n",
    "                                 scoring='r2')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    \n",
    "    results.append({\n",
    "        'Degree': degree,\n",
    "        'Train_R2': train_r2,\n",
    "        'Test_R2': test_r2,\n",
    "        'CV_R2_Mean': cv_mean,\n",
    "        'Test_RMSE': rmse,\n",
    "        'Test_MAE': mae,\n",
    "        'Overfitting_Gap': train_r2 - test_r2\n",
    "    })\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print('\ud83d\udcca Polynomial Regression Results (Multiple Degrees)\\n')\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Identify best degree\n",
    "best_degree = results_df.loc[results_df['Test_R2'].idxmax(), 'Degree']\n",
    "print(f'\\n\ud83c\udfaf Best polynomial degree: {int(best_degree)} (highest Test R\u00b2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de22ebe4",
   "metadata": {},
   "source": [
    "### 5.1 Visualize Degree Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4256ca81",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Visualize bias-variance tradeoff to select optimal polynomial degree\n",
    "\n",
    "**Key Points:**\n",
    "- **Training R\u00b2 trend**: Monotonically increases with degree (models memorize training data)\n",
    "- **Test R\u00b2 trend**: Increases then decreases (overfitting after optimal point)\n",
    "- **Overfitting gap**: Widening gap between train/test indicates overfitting\n",
    "- **Sweet spot**: Degree where test R\u00b2 peaks before declining\n",
    "\n",
    "**Why This Matters:**\n",
    "- Visual proof of overfitting - crucial for explaining to stakeholders\n",
    "- Objective criterion for degree selection (not guesswork)\n",
    "- Documents model selection process for regulatory compliance (e.g., FDA, ISO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95826ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance vs degree\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# R\u00b2 scores\n",
    "axes[0].plot(results_df['Degree'], results_df['Train_R2'], marker='o', \n",
    "             label='Training R\u00b2', linewidth=2)\n",
    "axes[0].plot(results_df['Degree'], results_df['Test_R2'], marker='s', \n",
    "             label='Test R\u00b2', linewidth=2)\n",
    "axes[0].plot(results_df['Degree'], results_df['CV_R2_Mean'], marker='^', \n",
    "             label='CV R\u00b2 (mean)', linewidth=2, linestyle='--')\n",
    "axes[0].axvline(best_degree, color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'Best Degree ({int(best_degree)})')\n",
    "axes[0].set_xlabel('Polynomial Degree', fontsize=12)\n",
    "axes[0].set_ylabel('R\u00b2 Score', fontsize=12)\n",
    "axes[0].set_title('Model Performance vs Polynomial Degree', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xticks(degrees)\n",
    "\n",
    "# Overfitting gap\n",
    "axes[1].bar(results_df['Degree'], results_df['Overfitting_Gap'], \n",
    "            color=['green' if d == best_degree else 'coral' for d in results_df['Degree']],\n",
    "            edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Polynomial Degree', fontsize=12)\n",
    "axes[1].set_ylabel('Train R\u00b2 - Test R\u00b2 (Overfitting Gap)', fontsize=12)\n",
    "axes[1].set_title('Overfitting Detection', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].set_xticks(degrees)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\ud83d\udcc8 Interpretation:')\n",
    "print(f'   \u2192 Degree {int(best_degree)} provides best generalization')\n",
    "print(f'   \u2192 Higher degrees show increasing overfitting gap')\n",
    "print(f'   \u2192 Training R\u00b2 keeps improving, but test R\u00b2 degrades')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567281ca",
   "metadata": {},
   "source": [
    "### 5.2 Train Final Model with Optimal Degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180e609",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Train production model with validated optimal degree\n",
    "\n",
    "**Key Points:**\n",
    "- **Best degree selection**: Based on empirical test R\u00b2 performance, not assumptions\n",
    "- **Final retraining**: Use optimal hyperparameter for production deployment\n",
    "- **Feature count**: Track polynomial feature explosion for memory planning\n",
    "- **Model persistence**: Production model ready for serialization (pickle/joblib)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Systematic hyperparameter selection builds confidence in model\n",
    "- Knowing feature count helps capacity planning for production inference\n",
    "- Clear validation path supports model governance and auditing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca314916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best degree\n",
    "final_model = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree=int(best_degree), include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = final_model.predict(X_train)\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "print(f'\\n\ud83c\udfaf Final Model (Polynomial Degree {int(best_degree)}) Performance:')\n",
    "print('='*60)\n",
    "print(f'Training R\u00b2:   {r2_score(y_train, y_train_pred):.4f}')\n",
    "print(f'Test R\u00b2:       {r2_score(y_test, y_test_pred):.4f}')\n",
    "print(f'Test RMSE:     {np.sqrt(mean_squared_error(y_test, y_test_pred)):.4f}')\n",
    "print(f'Test MAE:      {mean_absolute_error(y_test, y_test_pred):.4f}')\n",
    "\n",
    "# Feature count\n",
    "poly_features = final_model.named_steps['poly_features']\n",
    "n_poly_features = poly_features.transform(X_train[:1]).shape[1]\n",
    "print(f'\\nPolynomial features generated: {n_poly_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ce970",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Model Diagnostics and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895e9489",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Validate assumptions and detect potential issues through residual analysis\n",
    "\n",
    "**Key Points:**\n",
    "- **Predicted vs Actual**: Scatter around 45\u00b0 line indicates good fit\n",
    "- **Residual plot**: Random scatter (no patterns) confirms model captures relationship\n",
    "- **Residual distribution**: Should be approximately normal (Gaussian) with mean \u2248 0\n",
    "- **QQ plot**: Points on diagonal line confirm normality assumption\n",
    "\n",
    "**Why This Matters:**\n",
    "- Patterns in residuals indicate model misspecification (need higher degree or different approach)\n",
    "- Non-normal residuals invalidate confidence intervals and p-values\n",
    "- Stakeholders need visual proof that model is trustworthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b295e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Predicted vs Actual\n",
    "axes[0, 0].scatter(y_test, y_test_pred, alpha=0.6, edgecolor='k')\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0, 0].set_xlabel('Actual Performance Score', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Predicted Performance Score', fontsize=12)\n",
    "axes[0, 0].set_title('Predicted vs Actual (Test Set)', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residual Plot\n",
    "residuals = y_test - y_test_pred\n",
    "axes[0, 1].scatter(y_test_pred, residuals, alpha=0.6, edgecolor='k')\n",
    "axes[0, 1].axhline(0, color='r', linestyle='--', lw=2)\n",
    "axes[0, 1].set_xlabel('Predicted Performance Score', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Residuals', fontsize=12)\n",
    "axes[0, 1].set_title('Residual Plot (Test Set)', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residual Distribution\n",
    "axes[1, 0].hist(residuals, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(residuals.mean(), color='r', linestyle='--', lw=2, \n",
    "                   label=f'Mean: {residuals.mean():.2f}')\n",
    "axes[1, 0].set_xlabel('Residuals', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 0].set_title('Residual Distribution', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Q-Q Plot\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot (Normality Check)', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\ud83d\udd0d Diagnostic Analysis:')\n",
    "print(f'   \u2192 Residual mean: {residuals.mean():.4f} (should be \u2248 0)')\n",
    "print(f'   \u2192 Residual std: {residuals.std():.4f}')\n",
    "print(f'   \u2192 Min residual: {residuals.min():.4f}')\n",
    "print(f'   \u2192 Max residual: {residuals.max():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e5fef",
   "metadata": {},
   "source": [
    "### 6.1 Feature Importance (Polynomial Coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f2ea6",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Interpret which polynomial terms contribute most to predictions\n",
    "\n",
    "**Key Points:**\n",
    "- **Coefficient magnitudes**: Larger absolute values indicate stronger feature influence\n",
    "- **Feature names**: Generated automatically by PolynomialFeatures (e.g., 'x0^2', 'x0 x1')\n",
    "- **Scaling context**: StandardScaler normalizes features, so coefficients are comparable\n",
    "- **Interaction terms**: Identifies important feature interactions (e.g., temp \u00d7 voltage)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Explains model behavior to domain experts (e.g., \"quadratic temperature term dominates\")\n",
    "- Identifies physics relationships (e.g., voltage squared term confirms power law)\n",
    "- Supports feature engineering decisions for future models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61faf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients\n",
    "regressor = final_model.named_steps['regressor']\n",
    "poly_features = final_model.named_steps['poly_features']\n",
    "\n",
    "# Get feature names\n",
    "feature_names = poly_features.get_feature_names_out(['Temperature', 'Voltage'])\n",
    "coefficients = regressor.coef_\n",
    "\n",
    "# Create DataFrame\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients,\n",
    "    'Abs_Coefficient': np.abs(coefficients)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print('\ud83d\udcca Top 10 Most Important Polynomial Features:\\n')\n",
    "print(coef_df.head(10).to_string(index=False))\n",
    "\n",
    "# Visualize top features\n",
    "top_features = coef_df.head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['green' if c >= 0 else 'red' for c in top_features['Coefficient']]\n",
    "plt.barh(top_features['Feature'], top_features['Coefficient'], color=colors, \n",
    "         edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Coefficient Value', fontsize=12)\n",
    "plt.ylabel('Polynomial Feature', fontsize=12)\n",
    "plt.title('Top 10 Most Important Features (After Scaling)', fontsize=13, fontweight='bold')\n",
    "plt.axvline(0, color='black', linewidth=0.8)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7e58d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Comparison: Linear vs Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09a803a",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Quantitatively demonstrate value of polynomial regression over linear baseline\n",
    "\n",
    "**Key Points:**\n",
    "- **Linear baseline**: Simple model (degree 1) for comparison\n",
    "- **Performance gain**: Polynomial model's improvement over linear in R\u00b2 and RMSE\n",
    "- **Overfitting check**: Ensure polynomial doesn't sacrifice test performance for training fit\n",
    "- **Visualization**: Side-by-side predictions show polynomial captures curvature\n",
    "\n",
    "**Why This Matters:**\n",
    "- Justifies model complexity to stakeholders (\"25% R\u00b2 improvement\")\n",
    "- Documents modeling decision for audit trails\n",
    "- Validates domain intuition about non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d42360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline linear model\n",
    "linear_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_linear = linear_model.predict(X_test)\n",
    "\n",
    "# Compare metrics\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', f'Polynomial (Degree {int(best_degree)})'],\n",
    "    'Test_R2': [\n",
    "        r2_score(y_test, y_test_pred_linear),\n",
    "        r2_score(y_test, y_test_pred)\n",
    "    ],\n",
    "    'Test_RMSE': [\n",
    "        np.sqrt(mean_squared_error(y_test, y_test_pred_linear)),\n",
    "        np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    ],\n",
    "    'Test_MAE': [\n",
    "        mean_absolute_error(y_test, y_test_pred_linear),\n",
    "        mean_absolute_error(y_test, y_test_pred)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print('\ud83d\udcca Model Comparison: Linear vs Polynomial\\n')\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Calculate improvements\n",
    "r2_improvement = ((comparison.loc[1, 'Test_R2'] - comparison.loc[0, 'Test_R2']) / \n",
    "                  comparison.loc[0, 'Test_R2'] * 100)\n",
    "rmse_improvement = ((comparison.loc[0, 'Test_RMSE'] - comparison.loc[1, 'Test_RMSE']) / \n",
    "                    comparison.loc[0, 'Test_RMSE'] * 100)\n",
    "\n",
    "print(f'\\n\ud83c\udfaf Improvements:')\n",
    "print(f'   \u2192 R\u00b2 improved by {r2_improvement:.1f}%')\n",
    "print(f'   \u2192 RMSE reduced by {rmse_improvement:.1f}%')\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Linear predictions\n",
    "axes[0].scatter(y_test, y_test_pred_linear, alpha=0.6, edgecolor='k')\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted', fontsize=12)\n",
    "axes[0].set_title(f'Linear Regression\\nTest R\u00b2: {comparison.loc[0, \"Test_R2\"]:.4f}', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Polynomial predictions\n",
    "axes[1].scatter(y_test, y_test_pred, alpha=0.6, edgecolor='k', color='coral')\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted', fontsize=12)\n",
    "axes[1].set_title(f'Polynomial Regression (Degree {int(best_degree)})\\nTest R\u00b2: {comparison.loc[1, \"Test_R2\"]:.4f}', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2fde00",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Real-World Projects\n",
    "\n",
    "Apply polynomial regression to practical scenarios with implementation guidance.\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udd2c Post-Silicon Validation Projects\n",
    "\n",
    "#### **Project 1: Temperature-Performance Curve Characterization**\n",
    "\n",
    "**Objective:** Model device performance across temperature range to identify optimal operating conditions and thermal limits.\n",
    "\n",
    "**Business Value:**\n",
    "- Optimize thermal design of products\n",
    "- Set temperature specifications for datasheets\n",
    "- Predict field performance under varying conditions\n",
    "- Reduce thermal testing time by 40%\n",
    "\n",
    "**Dataset Features:**\n",
    "- Temperature (\u00b0C): Junction temperature measurements\n",
    "- Performance metrics: Speed, power, error rate\n",
    "- Environmental: Ambient temp, cooling method\n",
    "- Device ID: Track individual unit variations\n",
    "\n",
    "**Implementation Tips:**\n",
    "- Start with degree 2 (quadratic) - most thermal curves are parabolic\n",
    "- Include interaction term: temperature \u00d7 voltage\n",
    "- Validate at extreme temperatures (corner cases)\n",
    "- Use physically meaningful features (Kelvin scale for reactions)\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- Identify optimal temperature for peak performance\n",
    "- Quantify performance degradation at extremes\n",
    "- R\u00b2 > 0.85 indicates reliable characterization\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 2: Voltage-Frequency (V-F) Curve Modeling**\n",
    "\n",
    "**Objective:** Predict maximum operating frequency at different voltage levels for power-performance optimization.\n",
    "\n",
    "**Business Value:**\n",
    "- Enable dynamic voltage-frequency scaling (DVFS)\n",
    "- Optimize power vs performance tradeoffs\n",
    "- Support low-power product variants\n",
    "- Improve battery life by 25%\n",
    "\n",
    "**Dataset Features:**\n",
    "- Supply voltage (V): Core voltage levels\n",
    "- Max frequency (MHz): Measured at each voltage\n",
    "- Process corner: Fast, typical, slow\n",
    "- Temperature: Operating temperature during test\n",
    "\n",
    "**Implementation Tips:**\n",
    "- Polynomial degree 2-3 typical for V-F curves\n",
    "- Physical constraint: Frequency must increase with voltage\n",
    "- Consider separate models per process corner\n",
    "- Validate against silicon measurements at 5+ voltage points\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- Accurate frequency prediction within 5%\n",
    "- Identify voltage scaling limits\n",
    "- Support DVFS algorithms in firmware\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 3: Device Aging and Degradation Modeling**\n",
    "\n",
    "**Objective:** Predict long-term parameter drift due to aging effects (NBTI, HCI, EM) for reliability engineering.\n",
    "\n",
    "**Business Value:**\n",
    "- Set product lifetime warranties\n",
    "- Design guardbands for end-of-life performance\n",
    "- Predict field failure rates\n",
    "- Reduce qualification testing by 30%\n",
    "\n",
    "**Dataset Features:**\n",
    "- Time (hours): Stress test duration\n",
    "- Parameter drift: Threshold voltage shift, speed degradation\n",
    "- Stress conditions: Voltage, temperature, duty cycle\n",
    "- Initial value: Baseline parameter measurement\n",
    "\n",
    "**Implementation Tips:**\n",
    "- Aging often follows power-law: $\\Delta V_{th} \\propto t^n$ (n \u2248 0.25-0.5)\n",
    "- Transform time feature: $\\log(t)$ or $t^{0.5}$\n",
    "- Polynomial degree 2 after transformation\n",
    "- Include temperature acceleration factor\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- Predict 10-year drift from 1000-hour stress test\n",
    "- R\u00b2 > 0.90 needed for reliability predictions\n",
    "- Validate against industry models (e.g., Black's equation)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 4: Non-Linear Parametric Test Correlation**\n",
    "\n",
    "**Objective:** Find non-linear relationships between parametric test results to optimize test flow and reduce test time.\n",
    "\n",
    "**Business Value:**\n",
    "- Eliminate redundant tests\n",
    "- Reduce ATE test time by 20%\n",
    "- Predict expensive tests from cheap ones\n",
    "- Lower cost per device tested\n",
    "\n",
    "**Dataset Features:**\n",
    "- Fast tests: Digital patterns, basic DC tests\n",
    "- Slow tests: Detailed AC characterization, high-precision measurements\n",
    "- Correlations: Non-linear relationships between parameters\n",
    "- Device binning: Pass/fail categories\n",
    "\n",
    "**Implementation Tips:**\n",
    "- Use polynomial degree 2-3 for test correlations\n",
    "- Create interaction terms between related tests\n",
    "- Cross-validate on different lots to avoid overfitting\n",
    "- Threshold predictions for pass/fail decisions\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- Predict slow test results with 95% accuracy\n",
    "- Skip 15-20% of expensive tests\n",
    "- Maintain quality (yield/defect level)\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udcca General AI/ML Projects\n",
    "\n",
    "#### **Project 5: Marketing Response Curve Optimization**\n",
    "\n",
    "**Objective:** Model diminishing returns in marketing spend to optimize budget allocation across channels.\n",
    "\n",
    "**Business Value:**\n",
    "- Maximize ROI on marketing spend\n",
    "- Identify saturation points per channel\n",
    "- Optimize budget allocation\n",
    "- Improve efficiency by 30%\n",
    "\n",
    "**Dataset Features:**\n",
    "- Marketing spend ($): Investment per channel\n",
    "- Conversions: Sales, leads, sign-ups\n",
    "- Channel: Email, social, paid search\n",
    "- Time: Campaign duration\n",
    "\n",
    "**Implementation Tips:**\n",
    "- Degree 2 polynomial captures diminishing returns\n",
    "- Separate models per channel\n",
    "- Include interaction: spend \u00d7 time\n",
    "- Constrain: Response must increase with spend (enforce monotonicity)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 6: Growth Trajectory Forecasting (S-Curves)**\n",
    "\n",
    "**Objective:** Predict user/revenue growth following S-curve patterns (slow start, rapid growth, saturation).\n",
    "\n",
    "**Business Value:**\n",
    "- Forecast revenue for planning\n",
    "- Identify growth stage (early, rapid, mature)\n",
    "- Set realistic targets\n",
    "- Support investor presentations\n",
    "\n",
    "**Dataset Features:**\n",
    "- Time: Days/months since launch\n",
    "- Growth metric: Users, revenue, engagement\n",
    "- Marketing events: Campaigns, launches\n",
    "- External factors: Seasonality, competition\n",
    "\n",
    "**Implementation Tips:**\n",
    "- Logistic transformation: $\\log(\\frac{y}{K-y})$ where K = saturation\n",
    "- Polynomial degree 2-3 on transformed target\n",
    "- Validate saturation point estimate with domain experts\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 7: Price Elasticity Modeling (Non-Linear Demand)**\n",
    "\n",
    "**Objective:** Model non-linear relationship between price and demand to optimize pricing strategy.\n",
    "\n",
    "**Business Value:**\n",
    "- Maximize revenue (price \u00d7 volume)\n",
    "- Identify optimal price point\n",
    "- Understand customer price sensitivity\n",
    "- Increase profit margins by 15%\n",
    "\n",
    "**Dataset Features:**\n",
    "- Price: Product price points tested\n",
    "- Demand: Units sold\n",
    "- Competitor prices: Market context\n",
    "- Customer segment: Premium, budget\n",
    "\n",
    "**Implementation Tips:**\n",
    "- Degree 2-3 polynomial captures elasticity curves\n",
    "- Log-transform features for multiplicative effects\n",
    "- Include interaction: price \u00d7 competitor_price\n",
    "- Validate against economic theory (downward sloping demand)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 8: Environmental Trend Analysis (Climate Data)**\n",
    "\n",
    "**Objective:** Model non-linear environmental trends (temperature, pollution) for policy and forecasting.\n",
    "\n",
    "**Business Value:**\n",
    "- Long-term climate predictions\n",
    "- Policy impact assessment\n",
    "- Risk management\n",
    "- Support sustainability goals\n",
    "\n",
    "**Dataset Features:**\n",
    "- Time: Years/decades\n",
    "- Environmental metric: Temperature, CO2, air quality\n",
    "- Location: Geographic coordinates\n",
    "- Human activity: Emissions, industrial output\n",
    "\n",
    "**Implementation Tips:**\n",
    "- High-degree polynomials (3-5) for long-term trends\n",
    "- Careful validation - avoid extrapolation beyond data range\n",
    "- Include seasonal components (Fourier terms)\n",
    "- Cross-validate across geographic regions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d16f40e",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "### \u2705 When to Use Polynomial Regression\n",
    "\n",
    "1. **Visual evidence** of curvature in scatter plots\n",
    "2. **Domain knowledge** suggests polynomial relationships\n",
    "3. **Residual patterns** from linear regression show systematic errors\n",
    "4. **Moderate data size** (risk of overfitting with small datasets)\n",
    "\n",
    "### \u26a0\ufe0f Limitations and Alternatives\n",
    "\n",
    "**Limitations:**\n",
    "- **Feature explosion**: $O(n^d)$ features grow rapidly with degree\n",
    "- **Extrapolation risk**: Poor predictions outside training range\n",
    "- **Overfitting prone**: Requires careful validation\n",
    "- **Interpretability loss**: High-degree coefficients hard to explain\n",
    "\n",
    "**Better Alternatives:**\n",
    "- **Splines/GAMs**: For complex curves with many local changes\n",
    "- **Tree-based models**: For arbitrary non-linearity with minimal tuning\n",
    "- **Neural networks**: For very complex multi-dimensional non-linearity\n",
    "- **Domain transformations**: Log, sqrt, exp transformations may linearize relationship\n",
    "\n",
    "### \ud83c\udfaf Best Practices\n",
    "\n",
    "1. **Start simple**: Try linear first, add complexity if justified\n",
    "2. **Cross-validate**: Always use validation set for degree selection\n",
    "3. **Visualize**: Plot training and test performance vs degree\n",
    "4. **Scale features**: StandardScaler essential for polynomial features\n",
    "5. **Monitor overfitting**: Watch gap between train and test R\u00b2\n",
    "6. **Use pipelines**: Prevent data leakage in preprocessing\n",
    "7. **Physical constraints**: Enforce monotonicity or bounds if domain requires\n",
    "8. **Regularization**: Consider Ridge/Lasso for high-degree polynomials\n",
    "\n",
    "### \ud83d\udcda Next Learning Steps\n",
    "\n",
    "After mastering polynomial regression, explore:\n",
    "\n",
    "1. **`012_Ridge_Lasso_ElasticNet.ipynb`** - Regularization for polynomial models\n",
    "2. **`016_Decision_Trees.ipynb`** - Non-parametric non-linearity\n",
    "3. **Splines** - Piecewise polynomials for smoother curves\n",
    "4. **Kernel methods** - Implicit infinite-degree polynomials (SVM)\n",
    "\n",
    "### \ud83d\udd11 Core Concepts Mastered\n",
    "\n",
    "\u2705 Polynomial feature transformation  \n",
    "\u2705 Bias-variance tradeoff in practice  \n",
    "\u2705 Degree selection via validation curves  \n",
    "\u2705 Overfitting detection and prevention  \n",
    "\u2705 Pipeline usage for clean ML workflows  \n",
    "\u2705 Domain-specific applications (post-silicon + general)  \n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now understand how to capture non-linear relationships while avoiding overfitting. This is a critical skill for real-world ML where relationships are rarely perfectly linear.\n",
    "\n",
    "Continue to **012_Ridge_Lasso_ElasticNet** to learn regularization techniques that stabilize polynomial models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d8125",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Mathematical Derivation\n",
    "\n",
    "### Feature Expansion Example\n",
    "\n",
    "For input $\\mathbf{x} = [x_1, x_2]$ and degree $d=2$:\n",
    "\n",
    "$$\\phi(\\mathbf{x}) = [1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]$$\n",
    "\n",
    "Model becomes:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_1 x_2 + \\beta_5 x_2^2$$\n",
    "\n",
    "This is **still linear in parameters** $\\boldsymbol{\\beta}$, so normal equation applies:\n",
    "\n",
    "$$\\boldsymbol{\\beta} = (\\mathbf{X}_{\\text{poly}}^T \\mathbf{X}_{\\text{poly}})^{-1} \\mathbf{X}_{\\text{poly}}^T \\mathbf{y}$$\n",
    "\n",
    "### Overfitting Illustration\n",
    "\n",
    "As degree increases:\n",
    "\n",
    "- **Degree 1**: Underfits (high bias, low variance)\n",
    "- **Degree 2-3**: Optimal (balanced bias-variance)\n",
    "- **Degree 10+**: Overfits (low bias, high variance)\n",
    "\n",
    "Training error always decreases, but test error U-shaped:\n",
    "\n",
    "$$\\text{Test Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
    "\n",
    "Optimal degree minimizes total test error.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}