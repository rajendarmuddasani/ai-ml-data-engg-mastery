# 02 - Regression Models

**Purpose:** Master supervised learning for continuous target prediction

## Notebooks

- **010_Linear_Regression.ipynb** - OLS, feature engineering, assumptions, diagnostics
- **011_Polynomial_Regression.ipynb** - Non-linear relationships, degree selection, bias-variance tradeoff
- **012_Ridge_Lasso_ElasticNet.ipynb** - L1/L2 regularization, feature selection, multicollinearity
- **013_Logistic_Regression.ipynb** - Binary/multi-class classification, sigmoid, softmax, ROC/AUC
- **014_Support_Vector_Regression.ipynb** - Epsilon-insensitive loss, kernel trick, robust regression
- **015_Quantile_Regression.ipynb** - Conditional quantiles, prediction intervals, VaR/CVaR

## Key Learning Outcomes

- Implement regression from scratch (NumPy only)
- Master scikit-learn production APIs
- Apply to semiconductor test data (yield, test time, parametric prediction)
- Handle high-dimensional data with regularization
- Build probabilistic classifiers with calibrated probabilities

## Prerequisites

- **01_Foundations** (Python, NumPy basics)
- Linear algebra (matrix operations, eigenvalues)
- Basic statistics (mean, variance, distributions)

## Next Steps

Move to **03_Tree_Based_Models** for non-linear, interpretable models.
