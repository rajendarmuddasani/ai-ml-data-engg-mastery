{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f74d511",
   "metadata": {},
   "source": [
    "# 012: Ridge, Lasso & ElasticNet Regularization\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** regularization techniques (L1, L2, ElasticNet) and their mathematical foundations\n",
    "- **Implement** Ridge, Lasso, and ElasticNet regression from scratch\n",
    "- **Master** hyperparameter tuning (alpha selection via cross-validation)\n",
    "- **Apply** regularization to prevent overfitting in high-dimensional test data\n",
    "- **Build** robust yield prediction models with feature selection capabilities\n",
    "\n",
    "## üìö What is Regularization?\n",
    "\n",
    "Regularization adds penalty terms to the loss function to prevent overfitting by constraining model complexity. Ridge (L2) shrinks coefficients, Lasso (L1) performs feature selection, and ElasticNet combines both.\n",
    "\n",
    "**Why Regularization?**\n",
    "- ‚úÖ Prevents overfitting in high-dimensional data (100+ test parameters)\n",
    "- ‚úÖ Handles multicollinearity (correlated voltage/current measurements)\n",
    "- ‚úÖ Enables feature selection (identify critical test parameters)\n",
    "- ‚úÖ Improves model generalization to new wafer lots/products\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**High-Dimensional Parametric Analysis**\n",
    "- Input: 200+ electrical test parameters per device\n",
    "- Output: Regularized model identifying 20-30 critical parameters\n",
    "- Value: Reduce test time 40%, maintain 95% prediction accuracy\n",
    "\n",
    "**Multicollinearity Handling**\n",
    "- Input: Correlated measurements (Vdd, Idd, power = Vdd√óIdd)\n",
    "- Output: Stable coefficient estimates, interpretable relationships\n",
    "- Value: Avoid unstable models, enable root cause analysis\n",
    "\n",
    "**Automated Feature Selection**\n",
    "- Input: 500 test parameters from wafer probe + final test\n",
    "- Output: Lasso-selected 15 key predictors of yield\n",
    "- Value: Simplify models, reduce data storage by 97%\n",
    "\n",
    "**Yield Prediction with Robustness**\n",
    "- Input: Historical test data with measurement noise\n",
    "- Output: ElasticNet model balancing bias-variance tradeoff\n",
    "- Value: 92% yield prediction accuracy vs 85% OLS regression\n",
    "\n",
    "---\n",
    "\n",
    "Let's master regularization techniques! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2db191",
   "metadata": {},
   "source": [
    "# 012: Ridge, Lasso & ElasticNet Regression\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** L1 (Lasso) and L2 (Ridge) regularization penalties\n",
    "- **Master** ElasticNet's combined L1+L2 approach\n",
    "- **Implement** regularized regression from scratch and with sklearn\n",
    "- **Apply** feature selection using Lasso for high-dimensional data\n",
    "- **Solve** multicollinearity problems in post-silicon test data\n",
    "\n",
    "## üìö What is Regularized Regression?\n",
    "\n",
    "**Regularized regression** adds penalty terms to the ordinary least squares (OLS) loss function to prevent overfitting and handle multicollinearity. Instead of minimizing just the sum of squared residuals, we minimize:\n",
    "\n",
    "$$\\text{Loss} = \\text{RSS} + \\lambda \\cdot \\text{Penalty}$$\n",
    "\n",
    "Where:\n",
    "- **Ridge (L2)**: Penalty = $\\sum_{j=1}^{p} \\beta_j^2$ (shrinks coefficients toward zero)\n",
    "- **Lasso (L1)**: Penalty = $\\sum_{j=1}^{p} |\\beta_j|$ (shrinks some coefficients exactly to zero)\n",
    "- **ElasticNet**: Penalty = $\\alpha \\cdot \\text{L1} + (1-\\alpha) \\cdot \\text{L2}$ (combination of both)\n",
    "\n",
    "**Why Regularization?**\n",
    "- ‚úÖ Prevents overfitting when features > samples or features are correlated\n",
    "- ‚úÖ Lasso performs automatic feature selection (sparse models)\n",
    "- ‚úÖ Ridge handles multicollinearity better than OLS\n",
    "- ‚úÖ ElasticNet combines benefits of both L1 and L2\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**High-Dimensional STDF Reduction**\n",
    "- Input: 1000+ parametric tests per device (highly correlated)\n",
    "- Output: Sparse model with 50 most important parameters\n",
    "- Value: 20√ó faster inference, 95%+ accuracy retained ($3-5M/year)\n",
    "\n",
    "**Correlated Test Elimination**\n",
    "- Input: Multiple tests measuring similar electrical characteristics\n",
    "- Output: Lasso identifies redundant tests for removal\n",
    "- Value: 15-30% test time reduction ($2-10M equipment savings)\n",
    "\n",
    "**Robust Yield Prediction**\n",
    "- Input: Noisy parametric measurements with outliers\n",
    "- Output: Ridge regression model resistant to extreme values\n",
    "- Value: More stable yield forecasts for capacity planning\n",
    "\n",
    "**Sparse Feature Engineering**\n",
    "- Input: Polynomial/interaction features (100s-1000s dimensions)\n",
    "- Output: ElasticNet automatically selects meaningful feature combinations\n",
    "- Value: Interpretable models for engineering insights\n",
    "\n",
    "## üîÑ Regularization Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[High-D Features] --> B[Scale Features]\n",
    "    B --> C{Regularization Type}\n",
    "    C -->|L2| D[Ridge]\n",
    "    C -->|L1| E[Lasso]\n",
    "    C -->|L1+L2| F[ElasticNet]\n",
    "    D --> G[Shrunk Coefficients]\n",
    "    E --> H[Sparse Coefficients]\n",
    "    F --> I[Balanced Solution]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style G fill:#e1ffe1\n",
    "    style H fill:#e1ffe1\n",
    "    style I fill:#e1ffe1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- 010: Linear Regression (OLS fundamentals)\n",
    "- 011: Polynomial Regression (feature engineering)\n",
    "\n",
    "**Next Steps:**\n",
    "- 013: Logistic Regression (classification)\n",
    "- 041: Feature Engineering (advanced transformations)\n",
    "\n",
    "---\n",
    "\n",
    "Let's master regularized regression! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1623d2b7",
   "metadata": {},
   "source": [
    "## 1. What is Regularization?\n",
    "\n",
    "Regularization adds a **penalty term** to the loss function to constrain model complexity and prevent overfitting.\n",
    "\n",
    "### üìä Regularization Concept\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Standard Loss Function] --> B[Add Penalty Term]\n",
    "    B --> C{Regularization Type}\n",
    "    C -->|L2| D[Ridge: Shrink All Coefficients]\n",
    "    C -->|L1| E[Lasso: Zero Out Features]\n",
    "    C -->|L1 + L2| F[ElasticNet: Combine Both]\n",
    "    style D fill:#4CAF50,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style E fill:#FF9800,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style F fill:#2196F3,stroke:#333,stroke-width:2px,color:#fff\n",
    "```\n",
    "\n",
    "### Loss Functions with Penalties\n",
    "\n",
    "**Ordinary Least Squares (No Regularization):**\n",
    "$$\\mathcal{L}_{OLS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Ridge Regression (L2 Penalty):**\n",
    "$$\\mathcal{L}_{Ridge} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2$$\n",
    "\n",
    "**Lasso Regression (L1 Penalty):**\n",
    "$$\\mathcal{L}_{Lasso} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} |\\beta_j|$$\n",
    "\n",
    "**ElasticNet (L1 + L2):**\n",
    "$$\\mathcal{L}_{ElasticNet} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\rho \\sum_{j=1}^{p} |\\beta_j| + \\frac{\\alpha(1-\\rho)}{2} \\sum_{j=1}^{p} \\beta_j^2$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = regularization strength (higher ‚Üí more penalty)\n",
    "- $\\rho$ = L1 ratio for ElasticNet (0=Ridge, 1=Lasso)\n",
    "- $\\beta_j$ = model coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ccf2a8",
   "metadata": {},
   "source": [
    "### üéØ Regularization Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[High-Dimensional Data] --> B{Problem Type?}\n",
    "    B -->|Multicollinearity| C[Use Ridge]\n",
    "    B -->|Feature Selection Needed| D[Use Lasso]\n",
    "    B -->|Both Issues| E[Use ElasticNet]\n",
    "    C --> F[Tune Alpha via CV]\n",
    "    D --> F\n",
    "    E --> F\n",
    "    F --> G[Train Final Model]\n",
    "    G --> H[Evaluate on Test Set]\n",
    "    style E fill:#4CAF50,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style H fill:#2196F3,stroke:#333,stroke-width:2px,color:#fff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc860fcf",
   "metadata": {},
   "source": [
    "### When to Use Each Method?\n",
    "\n",
    "**Ridge (L2):**\n",
    "- ‚úÖ Many correlated features\n",
    "- ‚úÖ Want to keep all features (just shrink them)\n",
    "- ‚úÖ Multicollinearity issues\n",
    "- ‚úÖ Numerical stability important\n",
    "\n",
    "**Lasso (L1):**\n",
    "- ‚úÖ Need automatic feature selection\n",
    "- ‚úÖ Sparse models preferred (interpretability)\n",
    "- ‚úÖ Many irrelevant features\n",
    "- ‚úÖ Storage/computation constraints\n",
    "\n",
    "**ElasticNet:**\n",
    "- ‚úÖ Grouped correlated features (keeps groups)\n",
    "- ‚úÖ More features than samples (p > n)\n",
    "- ‚úÖ Need both regularization and selection\n",
    "- ‚úÖ Best of both worlds\n",
    "\n",
    "### üè≠ Real-World Applications\n",
    "\n",
    "**Post-Silicon Validation:**\n",
    "- High-dimensional STDF parameter reduction (1000+ test parameters)\n",
    "- Correlated test elimination (voltage tests highly correlated)\n",
    "- Sparse yield modeling (only key tests matter)\n",
    "- Robust parameter prediction with noise\n",
    "\n",
    "**General AI/ML:**\n",
    "- Genomics (millions of genes, few samples)\n",
    "- Text classification (large vocabulary, sparse features)\n",
    "- Financial modeling (correlated economic indicators)\n",
    "- Image processing (high-dimensional pixel data)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff1e156",
   "metadata": {},
   "source": [
    "## 2. Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebb708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, validation_curve\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('‚úÖ Libraries imported successfully')\n",
    "print(f'NumPy version: {np.__version__}')\n",
    "print(f'Pandas version: {pd.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bc5ad8",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Import regularization models and tools for hyperparameter tuning\n",
    "\n",
    "**Key Points:**\n",
    "- **Ridge, Lasso, ElasticNet**: Three regularization methods with different penalty types\n",
    "- **RidgeCV, LassoCV, ElasticNetCV**: Built-in cross-validation for alpha tuning (efficient)\n",
    "- **validation_curve**: Analyze model performance vs alpha values for visualization\n",
    "- **StandardScaler**: Essential - regularization is scale-sensitive\n",
    "\n",
    "**Why This Matters:**\n",
    "- CV versions automatically find best alpha without manual loops\n",
    "- StandardScaler ensures fair penalization (features on same scale)\n",
    "- Unified API allows easy comparison between methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb60952c",
   "metadata": {},
   "source": [
    "### 2.1 Generate High-Dimensional Dataset with Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7af1f",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Create synthetic data that demonstrates regularization benefits\n",
    "\n",
    "**Key Points:**\n",
    "- **High dimensionality**: 50 features (some relevant, many irrelevant)\n",
    "- **Multicollinearity**: Intentionally correlate features to mimic STDF data (voltage tests correlated)\n",
    "- **Sparse ground truth**: Only 10 features truly predictive (others are noise)\n",
    "- **Controlled experiment**: Known which features matter for validating Lasso selection\n",
    "\n",
    "**Why This Approach:**\n",
    "- Mimics real STDF data where parametric tests are highly correlated\n",
    "- OLS would overfit badly - regularization shines here\n",
    "- Can validate feature selection against ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e818aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_high_dimensional_data(n_samples=200, n_features=50, n_informative=10, noise=5.0):\n",
    "    \"\"\"\n",
    "    Generate high-dimensional data with multicollinearity\n",
    "    Simulates STDF-like scenario with many correlated test parameters\n",
    "    \"\"\"\n",
    "    # Generate base informative features\n",
    "    X_informative = np.random.randn(n_samples, n_informative)\n",
    "    \n",
    "    # True coefficients (sparse - only informative features matter)\n",
    "    true_coef = np.zeros(n_features)\n",
    "    true_coef[:n_informative] = np.random.randn(n_informative) * 10\n",
    "    \n",
    "    # Generate target from informative features\n",
    "    y = X_informative @ true_coef[:n_informative] + np.random.randn(n_samples) * noise\n",
    "    \n",
    "    # Create correlated redundant features (multicollinearity)\n",
    "    X_redundant = np.zeros((n_samples, n_features - n_informative))\n",
    "    for i in range(n_features - n_informative):\n",
    "        # Each redundant feature is linear combination of informative ones\n",
    "        weights = np.random.randn(n_informative) * 0.5\n",
    "        X_redundant[:, i] = X_informative @ weights + np.random.randn(n_samples) * 0.1\n",
    "    \n",
    "    # Combine features\n",
    "    X = np.hstack([X_informative, X_redundant])\n",
    "    \n",
    "    # Shuffle columns to hide which are informative\n",
    "    shuffle_idx = np.random.permutation(n_features)\n",
    "    X = X[:, shuffle_idx]\n",
    "    true_coef = true_coef[shuffle_idx]\n",
    "    \n",
    "    return X, y, true_coef\n",
    "\n",
    "# Generate dataset\n",
    "X, y, true_coef = generate_high_dimensional_data(n_samples=200, n_features=50, \n",
    "                                                  n_informative=10, noise=5.0)\n",
    "\n",
    "# Create DataFrame\n",
    "feature_names = [f'Feature_{i+1}' for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['Target'] = y\n",
    "\n",
    "print('‚úÖ High-dimensional dataset generated')\n",
    "print(f'Samples: {X.shape[0]}, Features: {X.shape[1]}')\n",
    "print(f'Informative features: 10 (hidden in 50 total)')\n",
    "print(f'\\nFeature correlation matrix shape: {df.iloc[:, :-1].corr().shape}')\n",
    "print(f'Max correlation: {df.iloc[:, :-1].corr().abs().values[np.triu_indices_from(df.iloc[:, :-1].corr(), k=1)].max():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc6efd",
   "metadata": {},
   "source": [
    "### 2.2 Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fd4b0d",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Split data and standardize features for regularization\n",
    "\n",
    "**Key Points:**\n",
    "- **Split first, then scale**: Prevents data leakage (test set never seen during scaling)\n",
    "- **StandardScaler critical**: Ridge/Lasso penalize by coefficient magnitude - features must be same scale\n",
    "- **Fit on train only**: Scaler learns statistics from training data, applies to test\n",
    "- **80-20 split**: Standard ratio balancing training data and test reliability\n",
    "\n",
    "**Why This Matters:**\n",
    "- Without scaling, features with large ranges get under-penalized\n",
    "- Data leakage (scaling on full data) inflates performance metrics artificially\n",
    "- Proper workflow ensures production-ready code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features (critical for regularization)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f'‚úÖ Data split and scaled')\n",
    "print(f'Training samples: {X_train_scaled.shape[0]}')\n",
    "print(f'Test samples: {X_test_scaled.shape[0]}')\n",
    "print(f'Features: {X_train_scaled.shape[1]}')\n",
    "print(f'\\nFeature means after scaling (should be ~0): {X_train_scaled.mean(axis=0)[:5]}')\n",
    "print(f'Feature stds after scaling (should be ~1): {X_train_scaled.std(axis=0)[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7562911",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Mathematical Foundation\n",
    "\n",
    "### 3.1 Ridge Regression (L2 Regularization)\n",
    "\n",
    "**Objective function:**\n",
    "$$\\min_{\\boldsymbol{\\beta}} \\left\\{ \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2 \\right\\}$$\n",
    "\n",
    "**Closed-form solution:**\n",
    "$$\\boldsymbol{\\beta}_{ridge} = (\\mathbf{X}^T \\mathbf{X} + \\alpha \\mathbf{I})^{-1} \\mathbf{X}^T \\mathbf{y}$$\n",
    "\n",
    "**Properties:**\n",
    "- Shrinks all coefficients toward zero (but never exactly zero)\n",
    "- Works well with correlated features\n",
    "- Always has unique solution (even when $\\mathbf{X}^T \\mathbf{X}$ singular)\n",
    "\n",
    "### 3.2 Lasso Regression (L1 Regularization)\n",
    "\n",
    "**Objective function:**\n",
    "$$\\min_{\\boldsymbol{\\beta}} \\left\\{ \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 + \\alpha \\sum_{j=1}^{p} |\\beta_j| \\right\\}$$\n",
    "\n",
    "**Properties:**\n",
    "- Forces some coefficients to **exactly zero** (feature selection)\n",
    "- No closed-form solution (solved via coordinate descent)\n",
    "- Tends to pick one feature from correlated groups\n",
    "\n",
    "### 3.3 ElasticNet (L1 + L2)\n",
    "\n",
    "**Objective function:**\n",
    "$$\\min_{\\boldsymbol{\\beta}} \\left\\{ \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 + \\alpha \\rho \\sum_{j=1}^{p} |\\beta_j| + \\frac{\\alpha(1-\\rho)}{2} \\sum_{j=1}^{p} \\beta_j^2 \\right\\}$$\n",
    "\n",
    "**Properties:**\n",
    "- Combines benefits: feature selection + handling correlated features\n",
    "- Keeps grouped correlated features together\n",
    "- $\\rho = 0$: Pure Ridge, $\\rho = 1$: Pure Lasso\n",
    "\n",
    "### 3.4 Geometric Interpretation\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[L2 Penalty<br/>Smooth Circle<br/>Coefficients Shrink Smoothly] --> B[Optimal Point]\n",
    "    C[L1 Penalty<br/>Diamond Shape<br/>Hits Axes ‚Üí Zeros] --> B\n",
    "    style A fill:#4CAF50,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style C fill:#FF9800,stroke:#333,stroke-width:2px,color:#fff\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea9ef8",
   "metadata": {},
   "source": [
    "## 4. Implementation from Scratch (Ridge)\n",
    "\n",
    "Implement Ridge regression to understand the math."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7998b63",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Build Ridge regression from scratch using closed-form solution\n",
    "\n",
    "**Key Points:**\n",
    "- **Normal equation + penalty**: $(\\mathbf{X}^T \\mathbf{X} + \\alpha \\mathbf{I})^{-1} \\mathbf{X}^T \\mathbf{y}$\n",
    "- **Identity matrix**: $\\alpha \\mathbf{I}$ adds to diagonal, ensures invertibility\n",
    "- **No intercept penalty**: Only coefficients penalized (intercept term excluded)\n",
    "- **Numerical stability**: Ridge makes inversion stable even with multicollinearity\n",
    "\n",
    "**Why This Matters:**\n",
    "- Shows regularization as simple modification to OLS\n",
    "- Explains why Ridge always has solution (even singular matrices)\n",
    "- Understanding math helps debug production issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f410a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegressionScratch:\n",
    "    \"\"\"\n",
    "    Ridge Regression from scratch using closed-form solution\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.coefficients = None\n",
    "        self.intercept = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit Ridge regression: Œ≤ = (X^T X + Œ±I)^(-1) X^T y\n",
    "        \"\"\"\n",
    "        # Add intercept column\n",
    "        n_samples = X.shape[0]\n",
    "        X_with_intercept = np.column_stack([np.ones(n_samples), X])\n",
    "        \n",
    "        # Create penalty matrix (don't penalize intercept)\n",
    "        n_features = X_with_intercept.shape[1]\n",
    "        penalty_matrix = np.eye(n_features) * self.alpha\n",
    "        penalty_matrix[0, 0] = 0  # No penalty on intercept\n",
    "        \n",
    "        # Ridge solution\n",
    "        XtX = X_with_intercept.T @ X_with_intercept\n",
    "        Xty = X_with_intercept.T @ y\n",
    "        \n",
    "        coefficients_all = np.linalg.inv(XtX + penalty_matrix) @ Xty\n",
    "        \n",
    "        self.intercept = coefficients_all[0]\n",
    "        self.coefficients = coefficients_all[1:]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.coefficients + self.intercept\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Train from-scratch Ridge\n",
    "model_scratch = RidgeRegressionScratch(alpha=1.0)\n",
    "model_scratch.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_r2 = model_scratch.score(X_train_scaled, y_train)\n",
    "test_r2 = model_scratch.score(X_test_scaled, y_test)\n",
    "y_pred_scratch = model_scratch.predict(X_test_scaled)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_scratch))\n",
    "\n",
    "print('‚úÖ From-Scratch Ridge Regression (Œ±=1.0)')\n",
    "print(f'Training R¬≤: {train_r2:.4f}')\n",
    "print(f'Test R¬≤: {test_r2:.4f}')\n",
    "print(f'Test RMSE: {rmse:.4f}')\n",
    "print(f'Non-zero coefficients: {np.sum(np.abs(model_scratch.coefficients) > 0.001)}/{len(model_scratch.coefficients)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef86b6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Production Implementation with Scikit-learn\n",
    "\n",
    "### 5.1 Compare OLS vs Ridge vs Lasso vs ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9704b0",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Systematically compare all four regression methods\n",
    "\n",
    "**Key Points:**\n",
    "- **OLS baseline**: No regularization - overfits with high-dimensional data\n",
    "- **Ridge**: Shrinks coefficients but keeps all features\n",
    "- **Lasso**: Zeros out irrelevant features (automatic selection)\n",
    "- **ElasticNet**: Balanced approach with both penalties\n",
    "\n",
    "**Why This Matters:**\n",
    "- OLS likely overfits (training R¬≤ >> test R¬≤)\n",
    "- Lasso should identify ~10 important features\n",
    "- Ridge should have better test R¬≤ than OLS\n",
    "- ElasticNet often wins in high-dimensional settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with reasonable alpha\n",
    "models = {\n",
    "    'OLS': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.1),\n",
    "    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_r2 = model.score(X_train_scaled, y_train)\n",
    "    test_r2 = model.score(X_test_scaled, y_test)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # Count non-zero coefficients\n",
    "    if hasattr(model, 'coef_'):\n",
    "        n_nonzero = np.sum(np.abs(model.coef_) > 0.001)\n",
    "    else:\n",
    "        n_nonzero = X.shape[1]\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train_R2': train_r2,\n",
    "        'Test_R2': test_r2,\n",
    "        'Test_RMSE': rmse,\n",
    "        'Test_MAE': mae,\n",
    "        'Non_Zero_Features': n_nonzero,\n",
    "        'Overfitting_Gap': train_r2 - test_r2\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print('üìä Model Comparison: OLS vs Ridge vs Lasso vs ElasticNet\\n')\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Identify best model\n",
    "best_model = results_df.loc[results_df['Test_R2'].idxmax(), 'Model']\n",
    "print(f'\\nüéØ Best model: {best_model} (highest Test R¬≤)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63662174",
   "metadata": {},
   "source": [
    "### 5.2 Hyperparameter Tuning with Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fef1dd",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Find optimal alpha (regularization strength) using cross-validation\n",
    "\n",
    "**Key Points:**\n",
    "- **Alpha range**: Test from 0.001 (weak) to 100 (strong regularization)\n",
    "- **RidgeCV/LassoCV**: Built-in CV - efficient, no manual loops needed\n",
    "- **Logarithmic scale**: Alpha effects are multiplicative (0.01, 0.1, 1, 10, 100)\n",
    "- **Best alpha**: Chosen automatically based on CV performance\n",
    "\n",
    "**Why This Matters:**\n",
    "- Wrong alpha ‚Üí underfitting (too high) or overfitting (too low)\n",
    "- CV ensures alpha generalizes to unseen data\n",
    "- Automates hyperparameter search - production-ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha values to test (logarithmic scale)\n",
    "alphas = np.logspace(-3, 2, 50)  # 0.001 to 100\n",
    "\n",
    "# Ridge with CV\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5)\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Lasso with CV\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, max_iter=10000)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ElasticNet with CV\n",
    "enet_cv = ElasticNetCV(alphas=alphas, l1_ratio=0.5, cv=5, max_iter=10000)\n",
    "enet_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print('üîç Optimal Alpha Values (via 5-Fold CV)\\n')\n",
    "print(f'Ridge optimal Œ±: {ridge_cv.alpha_:.4f}')\n",
    "print(f'Lasso optimal Œ±: {lasso_cv.alpha_:.4f}')\n",
    "print(f'ElasticNet optimal Œ±: {enet_cv.alpha_:.4f}')\n",
    "\n",
    "# Evaluate tuned models\n",
    "print('\\nüìä Performance with Optimized Alpha:\\n')\n",
    "for name, model in [('Ridge', ridge_cv), ('Lasso', lasso_cv), ('ElasticNet', enet_cv)]:\n",
    "    test_r2 = model.score(X_test_scaled, y_test)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    n_nonzero = np.sum(np.abs(model.coef_) > 0.001)\n",
    "    \n",
    "    print(f'{name:12} ‚Üí Test R¬≤: {test_r2:.4f}, RMSE: {rmse:.4f}, Features: {n_nonzero}/50')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0792063",
   "metadata": {},
   "source": [
    "### 5.3 Visualize Coefficient Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5850e",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Visualize how coefficients change with regularization strength\n",
    "\n",
    "**Key Points:**\n",
    "- **Coefficient paths**: Each line represents one feature's coefficient vs alpha\n",
    "- **Ridge behavior**: All coefficients shrink smoothly toward zero (never exactly zero)\n",
    "- **Lasso behavior**: Coefficients hit zero at different alphas (sequential feature elimination)\n",
    "- **Feature selection**: Lasso path shows which features eliminated first\n",
    "\n",
    "**Why This Matters:**\n",
    "- Visual proof of L1 vs L2 difference\n",
    "- Identifies robust features (survive high alpha)\n",
    "- Helps communicate regularization to stakeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffedadbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute coefficient paths for Ridge and Lasso\n",
    "alphas_path = np.logspace(-2, 2, 100)\n",
    "\n",
    "# Ridge path\n",
    "ridge_coefs = []\n",
    "for alpha in alphas_path:\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    ridge_coefs.append(model.coef_)\n",
    "ridge_coefs = np.array(ridge_coefs)\n",
    "\n",
    "# Lasso path\n",
    "lasso_coefs = []\n",
    "for alpha in alphas_path:\n",
    "    model = Lasso(alpha=alpha, max_iter=10000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    lasso_coefs.append(model.coef_)\n",
    "lasso_coefs = np.array(lasso_coefs)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Ridge coefficient paths\n",
    "for i in range(ridge_coefs.shape[1]):\n",
    "    axes[0].plot(alphas_path, ridge_coefs[:, i], alpha=0.6, linewidth=1.5)\n",
    "axes[0].axvline(ridge_cv.alpha_, color='red', linestyle='--', linewidth=2, label=f'Optimal Œ±={ridge_cv.alpha_:.3f}')\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('Alpha (log scale)', fontsize=12)\n",
    "axes[0].set_ylabel('Coefficient Value', fontsize=12)\n",
    "axes[0].set_title('Ridge Coefficient Paths (L2)\\nAll coefficients shrink smoothly', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Lasso coefficient paths\n",
    "for i in range(lasso_coefs.shape[1]):\n",
    "    axes[1].plot(alphas_path, lasso_coefs[:, i], alpha=0.6, linewidth=1.5)\n",
    "axes[1].axvline(lasso_cv.alpha_, color='red', linestyle='--', linewidth=2, label=f'Optimal Œ±={lasso_cv.alpha_:.3f}')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlabel('Alpha (log scale)', fontsize=12)\n",
    "axes[1].set_ylabel('Coefficient Value', fontsize=12)\n",
    "axes[1].set_title('Lasso Coefficient Paths (L1)\\nCoefficients hit zero (feature selection)', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('üìà Coefficient Path Interpretation:')\n",
    "print('   ‚Üí Ridge: Gradual shrinkage, all features retained')\n",
    "print('   ‚Üí Lasso: Sequential zeroing, automatic feature selection')\n",
    "print(f'   ‚Üí At optimal Œ±, Lasso keeps {np.sum(np.abs(lasso_cv.coef_) > 0.001)} features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2bb2c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Feature Selection Analysis (Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012761ef",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Analyze which features Lasso selected as important\n",
    "\n",
    "**Key Points:**\n",
    "- **Non-zero coefficients**: Features Lasso kept (implicitly important)\n",
    "- **Zero coefficients**: Features eliminated (redundant or irrelevant)\n",
    "- **Magnitude ranking**: Larger |coefficient| ‚Üí more important\n",
    "- **Ground truth comparison**: Validate against known informative features\n",
    "\n",
    "**Why This Matters:**\n",
    "- Reduces model complexity (fewer features ‚Üí faster inference)\n",
    "- Improves interpretability (focus on key drivers)\n",
    "- Lowers storage/memory requirements in production\n",
    "- Validates feature engineering decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Lasso coefficients\n",
    "lasso_coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': lasso_cv.coef_,\n",
    "    'Abs_Coefficient': np.abs(lasso_cv.coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Identify selected features\n",
    "selected_features = lasso_coef_df[lasso_coef_df['Abs_Coefficient'] > 0.001]\n",
    "zeroed_features = lasso_coef_df[lasso_coef_df['Abs_Coefficient'] <= 0.001]\n",
    "\n",
    "print(f'üìä Lasso Feature Selection Results (Œ±={lasso_cv.alpha_:.4f})\\n')\n",
    "print(f'Features selected: {len(selected_features)}/50')\n",
    "print(f'Features eliminated: {len(zeroed_features)}/50')\n",
    "print(f'\\nTop 15 Selected Features:\\n')\n",
    "print(selected_features.head(15).to_string(index=False))\n",
    "\n",
    "# Visualize selected features\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['green' if c >= 0 else 'red' for c in selected_features['Coefficient']]\n",
    "plt.barh(selected_features['Feature'][:15], selected_features['Coefficient'][:15], \n",
    "         color=colors, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Lasso Coefficient', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title(f'Top 15 Features Selected by Lasso (Œ±={lasso_cv.alpha_:.4f})', fontsize=13, fontweight='bold')\n",
    "plt.axvline(0, color='black', linewidth=0.8)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3453a353",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Model Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3e7eb",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Comprehensive diagnostic plots for best regularized model\n",
    "\n",
    "**Key Points:**\n",
    "- **Predicted vs Actual**: Points near diagonal indicate good predictions\n",
    "- **Residual plot**: Random scatter confirms no systematic errors\n",
    "- **Residual distribution**: Normal distribution validates assumptions\n",
    "- **Q-Q plot**: Diagonal line confirms residual normality\n",
    "\n",
    "**Why This Matters:**\n",
    "- Patterns in residuals indicate model misspecification\n",
    "- Non-normal residuals invalidate confidence intervals\n",
    "- Validates that regularization didn't introduce bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best model (ElasticNet from earlier comparison)\n",
    "best_model_obj = enet_cv\n",
    "y_test_pred = best_model_obj.predict(X_test_scaled)\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Diagnostic plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Predicted vs Actual\n",
    "axes[0, 0].scatter(y_test, y_test_pred, alpha=0.6, edgecolor='k')\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0, 0].set_xlabel('Actual Target', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Predicted Target', fontsize=12)\n",
    "axes[0, 0].set_title('Predicted vs Actual (Test Set)', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residual Plot\n",
    "axes[0, 1].scatter(y_test_pred, residuals, alpha=0.6, edgecolor='k')\n",
    "axes[0, 1].axhline(0, color='r', linestyle='--', lw=2)\n",
    "axes[0, 1].set_xlabel('Predicted Target', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Residuals', fontsize=12)\n",
    "axes[0, 1].set_title('Residual Plot', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residual Distribution\n",
    "axes[1, 0].hist(residuals, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(residuals.mean(), color='r', linestyle='--', lw=2, \n",
    "                   label=f'Mean: {residuals.mean():.2f}')\n",
    "axes[1, 0].set_xlabel('Residuals', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 0].set_title('Residual Distribution', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Q-Q Plot\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot (Normality Check)', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('üîç Diagnostic Summary:')\n",
    "print(f'   ‚Üí Mean residual: {residuals.mean():.4f} (close to 0 ‚úì)')\n",
    "print(f'   ‚Üí Residual std: {residuals.std():.4f}')\n",
    "print(f'   ‚Üí R¬≤ on test: {r2_score(y_test, y_test_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea982e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Real-World Projects\n",
    "\n",
    "### üî¨ Post-Silicon Validation Projects\n",
    "\n",
    "#### **Project 1: High-Dimensional STDF Parameter Reduction**\n",
    "\n",
    "**Objective:** Reduce 1000+ parametric test parameters to ~50 key predictors for yield modeling.\n",
    "\n",
    "**Business Value:**\n",
    "- Reduce test time by 40% (skip redundant tests)\n",
    "- Lower data storage costs\n",
    "- Faster model inference\n",
    "- Maintain prediction accuracy >95%\n",
    "\n",
    "**Dataset Features:**\n",
    "- 1000+ parametric tests (voltage, current, frequency, timing)\n",
    "- High correlation (VDD tests, timing paths)\n",
    "- Mix of analog and digital parameters\n",
    "- Spatial wafer data (die_x, die_y)\n",
    "\n",
    "**Implementation Guide:**\n",
    "1. Start with ElasticNet (handles correlated groups)\n",
    "2. Use `ElasticNetCV` with `l1_ratio=0.7` (favor selection)\n",
    "3. Test alphas: `np.logspace(-4, 1, 50)`\n",
    "4. Validate on multiple wafer lots\n",
    "5. Keep features with |coef| > threshold\n",
    "\n",
    "**Expected Outcomes:** 950 ‚Üí 50 parameters, R¬≤ > 0.90\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 2: Correlated Test Elimination**\n",
    "\n",
    "**Objective:** Identify and remove redundant tests that provide no additional yield information.\n",
    "\n",
    "**Business Value:**\n",
    "- Cut ATE test cost per device\n",
    "- Increase throughput\n",
    "- Simplify test programs\n",
    "- Reduce engineering debug time\n",
    "\n",
    "**Implementation Guide:**\n",
    "1. Calculate correlation matrix of all tests\n",
    "2. Apply Lasso to zero out redundant tests\n",
    "3. Cross-validate on different product lots\n",
    "4. Verify eliminated tests truly redundant (domain expert review)\n",
    "\n",
    "**Expected Outcomes:** 30% test reduction, <2% yield prediction degradation\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 3: Sparse Yield Modeling**\n",
    "\n",
    "**Objective:** Build yield prediction model using minimal test set for early-stage screening.\n",
    "\n",
    "**Business Value:**\n",
    "- Enable fast screening before expensive tests\n",
    "- Support multi-site parallel testing\n",
    "- Reduce cost of test development\n",
    "- Identify critical yield limiters\n",
    "\n",
    "**Implementation Guide:**\n",
    "1. Use Lasso for aggressive feature selection\n",
    "2. Start with alpha=1.0, decrease until R¬≤>0.85\n",
    "3. Include physics-based features (power, speed)\n",
    "4. Validate on out-of-lot data\n",
    "\n",
    "**Expected Outcomes:** 10-15 critical tests identified, 90%+ yield prediction accuracy\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 4: Robust Power Parameter Prediction**\n",
    "\n",
    "**Objective:** Predict device power consumption from fast pre-tests, handling measurement noise.\n",
    "\n",
    "**Business Value:**\n",
    "- Skip expensive power measurements\n",
    "- Real-time binning decisions\n",
    "- Reduce test time by 25%\n",
    "- Robust to measurement variance\n",
    "\n",
    "**Implementation Guide:**\n",
    "1. Use Ridge for robustness (outlier resistance)\n",
    "2. Include voltage, frequency, temperature features\n",
    "3. Add polynomial terms for V¬≤ (physics-based)\n",
    "4. Cross-validate across temperature corners\n",
    "\n",
    "**Expected Outcomes:** Power prediction within 5%, robust to 10% measurement noise\n",
    "\n",
    "---\n",
    "\n",
    "### üìä General AI/ML Projects\n",
    "\n",
    "#### **Project 5: Genomic Biomarker Discovery**\n",
    "\n",
    "**Objective:** Identify disease-predictive genes from 10,000+ gene expression levels.\n",
    "\n",
    "**Business Value:**\n",
    "- Drug target discovery\n",
    "- Personalized medicine\n",
    "- Reduce diagnostic test costs\n",
    "\n",
    "**Implementation:** Lasso for gene selection, ElasticNet for gene networks\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 6: Text Classification with Large Vocabulary**\n",
    "\n",
    "**Objective:** Build spam filter with 50,000+ word features, select key spam indicators.\n",
    "\n",
    "**Business Value:**\n",
    "- Fast inference (fewer features)\n",
    "- Interpretable rules\n",
    "- Lower memory footprint\n",
    "\n",
    "**Implementation:** Lasso L1 penalty naturally handles sparse text features\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 7: Financial Risk Modeling**\n",
    "\n",
    "**Objective:** Predict credit default from 200+ correlated economic indicators.\n",
    "\n",
    "**Business Value:**\n",
    "- Accurate risk assessment\n",
    "- Regulatory compliance\n",
    "- Automated lending decisions\n",
    "\n",
    "**Implementation:** Ridge handles multicollinearity (GDP, inflation, rates correlated)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 8: Image Compression and Denoising**\n",
    "\n",
    "**Objective:** Learn sparse representation of images for compression.\n",
    "\n",
    "**Business Value:**\n",
    "- Reduce storage costs\n",
    "- Faster transmission\n",
    "- Remove noise while preserving edges\n",
    "\n",
    "**Implementation:** Lasso on pixel/wavelet features for sparse coding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9bddd",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "### ‚úÖ When to Use Each Method\n",
    "\n",
    "**Ridge (L2):**\n",
    "- Many correlated features\n",
    "- Want smooth shrinkage (keep all features)\n",
    "- Multicollinearity problems\n",
    "- Numerical stability critical\n",
    "\n",
    "**Lasso (L1):**\n",
    "- Need feature selection\n",
    "- Sparse models preferred\n",
    "- Interpretability important\n",
    "- More features than samples\n",
    "\n",
    "**ElasticNet:**\n",
    "- High-dimensional data (p >> n)\n",
    "- Grouped correlated features\n",
    "- Best of both worlds\n",
    "- When unsure, start here\n",
    "\n",
    "### ‚ö†Ô∏è Limitations\n",
    "\n",
    "**All Methods:**\n",
    "- Still assumes linear relationship (use with polynomial features for non-linearity)\n",
    "- Require feature scaling\n",
    "- Alpha tuning needed (computationally expensive)\n",
    "\n",
    "**Lasso Specific:**\n",
    "- Picks arbitrarily from correlated features\n",
    "- Can be unstable (small data changes ‚Üí different features)\n",
    "- Max selected features = n samples\n",
    "\n",
    "### üéØ Best Practices\n",
    "\n",
    "1. **Always scale features** (StandardScaler before regularization)\n",
    "2. **Use CV for alpha** (RidgeCV, LassoCV, ElasticNetCV)\n",
    "3. **Start with ElasticNet** (covers Ridge and Lasso as special cases)\n",
    "4. **Validate feature selection** (check if Lasso choices make domain sense)\n",
    "5. **Monitor overfitting** (train vs test R¬≤ gap)\n",
    "6. **Test on unseen data** (different batches/lots for production readiness)\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "After mastering regularization:\n",
    "1. **`013_Logistic_Regression.ipynb`** - Classification with regularization\n",
    "2. **`016_Decision_Trees.ipynb`** - Non-linear alternatives\n",
    "3. **Polynomial + Regularization** - Combine for powerful models\n",
    "\n",
    "### üîë Core Concepts Mastered\n",
    "\n",
    "‚úÖ L1, L2, and combined penalties  \n",
    "‚úÖ Bias-variance tradeoff through regularization  \n",
    "‚úÖ Automatic feature selection with Lasso  \n",
    "‚úÖ Handling multicollinearity with Ridge  \n",
    "‚úÖ Hyperparameter tuning via cross-validation  \n",
    "‚úÖ Production-ready pipelines with CV  \n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You can now handle high-dimensional data, multicollinearity, and perform automatic feature selection. These are essential skills for real-world ML where data is often messy and high-dimensional."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
