{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1623d2b7",
   "metadata": {},
   "source": [
    "# 012: Ridge, Lasso & ElasticNet Regression\n",
    "\n",
    "Regularization adds a **penalty term** to the loss function to constrain model complexity and prevent overfitting.\n",
    "\n",
    "### \ud83d\udcca Regularization Concept\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Standard Loss Function] --> B[Add Penalty Term]\n",
    "    B --> C{Regularization Type}\n",
    "    C -->|L2| D[Ridge: Shrink All Coefficients]\n",
    "    C -->|L1| E[Lasso: Zero Out Features]\n",
    "    C -->|L1 + L2| F[ElasticNet: Combine Both]\n",
    "    style D fill:#4CAF50,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style E fill:#FF9800,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style F fill:#2196F3,stroke:#333,stroke-width:2px,color:#fff\n",
    "```\n",
    "\n",
    "### Loss Functions with Penalties\n",
    "\n",
    "**Ordinary Least Squares (No Regularization):**\n",
    "$$\\mathcal{L}_{OLS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Ridge Regression (L2 Penalty):**\n",
    "$$\\mathcal{L}_{Ridge} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2$$\n",
    "\n",
    "**Lasso Regression (L1 Penalty):**\n",
    "$$\\mathcal{L}_{Lasso} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} |\\beta_j|$$\n",
    "\n",
    "**ElasticNet (L1 + L2):**\n",
    "$$\\mathcal{L}_{ElasticNet} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\rho \\sum_{j=1}^{p} |\\beta_j| + \\frac{\\alpha(1-\\rho)}{2} \\sum_{j=1}^{p} \\beta_j^2$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = regularization strength (higher \u2192 more penalty)\n",
    "- $\\rho$ = L1 ratio for ElasticNet (0=Ridge, 1=Lasso)\n",
    "- $\\beta_j$ = model coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ccf2a8",
   "metadata": {},
   "source": [
    "### \ud83c\udfaf Regularization Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[High-Dimensional Data] --> B{Problem Type?}\n",
    "    B -->|Multicollinearity| C[Use Ridge]\n",
    "    B -->|Feature Selection Needed| D[Use Lasso]\n",
    "    B -->|Both Issues| E[Use ElasticNet]\n",
    "    C --> F[Tune Alpha via CV]\n",
    "    D --> F\n",
    "    E --> F\n",
    "    F --> G[Train Final Model]\n",
    "    G --> H[Evaluate on Test Set]\n",
    "    style E fill:#4CAF50,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style H fill:#2196F3,stroke:#333,stroke-width:2px,color:#fff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc860fcf",
   "metadata": {},
   "source": [
    "### When to Use Each Method?\n",
    "\n",
    "**Ridge (L2):**\n",
    "- \u2705 Many correlated features\n",
    "- \u2705 Want to keep all features (just shrink them)\n",
    "- \u2705 Multicollinearity issues\n",
    "- \u2705 Numerical stability important\n",
    "\n",
    "**Lasso (L1):**\n",
    "- \u2705 Need automatic feature selection\n",
    "- \u2705 Sparse models preferred (interpretability)\n",
    "- \u2705 Many irrelevant features\n",
    "- \u2705 Storage/computation constraints\n",
    "\n",
    "**ElasticNet:**\n",
    "- \u2705 Grouped correlated features (keeps groups)\n",
    "- \u2705 More features than samples (p > n)\n",
    "- \u2705 Need both regularization and selection\n",
    "- \u2705 Best of both worlds\n",
    "\n",
    "### \ud83c\udfed Real-World Applications\n",
    "\n",
    "**Post-Silicon Validation:**\n",
    "- High-dimensional STDF parameter reduction (1000+ test parameters)\n",
    "- Correlated test elimination (voltage tests highly correlated)\n",
    "- Sparse yield modeling (only key tests matter)\n",
    "- Robust parameter prediction with noise\n",
    "\n",
    "**General AI/ML:**\n",
    "- Genomics (millions of genes, few samples)\n",
    "- Text classification (large vocabulary, sparse features)\n",
    "- Financial modeling (correlated economic indicators)\n",
    "- Image processing (high-dimensional pixel data)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff1e156",
   "metadata": {},
   "source": [
    "## 2. Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebb708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, validation_curve\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('\u2705 Libraries imported successfully')\n",
    "print(f'NumPy version: {np.__version__}')\n",
    "print(f'Pandas version: {pd.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bc5ad8",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Import regularization models and tools for hyperparameter tuning\n",
    "\n",
    "**Key Points:**\n",
    "- **Ridge, Lasso, ElasticNet**: Three regularization methods with different penalty types\n",
    "- **RidgeCV, LassoCV, ElasticNetCV**: Built-in cross-validation for alpha tuning (efficient)\n",
    "- **validation_curve**: Analyze model performance vs alpha values for visualization\n",
    "- **StandardScaler**: Essential - regularization is scale-sensitive\n",
    "\n",
    "**Why This Matters:**\n",
    "- CV versions automatically find best alpha without manual loops\n",
    "- StandardScaler ensures fair penalization (features on same scale)\n",
    "- Unified API allows easy comparison between methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb60952c",
   "metadata": {},
   "source": [
    "### 2.1 Generate High-Dimensional Dataset with Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7af1f",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Create synthetic data that demonstrates regularization benefits\n",
    "\n",
    "**Key Points:**\n",
    "- **High dimensionality**: 50 features (some relevant, many irrelevant)\n",
    "- **Multicollinearity**: Intentionally correlate features to mimic STDF data (voltage tests correlated)\n",
    "- **Sparse ground truth**: Only 10 features truly predictive (others are noise)\n",
    "- **Controlled experiment**: Known which features matter for validating Lasso selection\n",
    "\n",
    "**Why This Approach:**\n",
    "- Mimics real STDF data where parametric tests are highly correlated\n",
    "- OLS would overfit badly - regularization shines here\n",
    "- Can validate feature selection against ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e818aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_high_dimensional_data(n_samples=200, n_features=50, n_informative=10, noise=5.0):\n",
    "    \"\"\"\n",
    "    Generate high-dimensional data with multicollinearity\n",
    "    Simulates STDF-like scenario with many correlated test parameters\n",
    "    \"\"\"\n",
    "    # Generate base informative features\n",
    "    X_informative = np.random.randn(n_samples, n_informative)\n",
    "    \n",
    "    # True coefficients (sparse - only informative features matter)\n",
    "    true_coef = np.zeros(n_features)\n",
    "    true_coef[:n_informative] = np.random.randn(n_informative) * 10\n",
    "    \n",
    "    # Generate target from informative features\n",
    "    y = X_informative @ true_coef[:n_informative] + np.random.randn(n_samples) * noise\n",
    "    \n",
    "    # Create correlated redundant features (multicollinearity)\n",
    "    X_redundant = np.zeros((n_samples, n_features - n_informative))\n",
    "    for i in range(n_features - n_informative):\n",
    "        # Each redundant feature is linear combination of informative ones\n",
    "        weights = np.random.randn(n_informative) * 0.5\n",
    "        X_redundant[:, i] = X_informative @ weights + np.random.randn(n_samples) * 0.1\n",
    "    \n",
    "    # Combine features\n",
    "    X = np.hstack([X_informative, X_redundant])\n",
    "    \n",
    "    # Shuffle columns to hide which are informative\n",
    "    shuffle_idx = np.random.permutation(n_features)\n",
    "    X = X[:, shuffle_idx]\n",
    "    true_coef = true_coef[shuffle_idx]\n",
    "    \n",
    "    return X, y, true_coef\n",
    "\n",
    "# Generate dataset\n",
    "X, y, true_coef = generate_high_dimensional_data(n_samples=200, n_features=50, \n",
    "                                                  n_informative=10, noise=5.0)\n",
    "\n",
    "# Create DataFrame\n",
    "feature_names = [f'Feature_{i+1}' for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['Target'] = y\n",
    "\n",
    "print('\u2705 High-dimensional dataset generated')\n",
    "print(f'Samples: {X.shape[0]}, Features: {X.shape[1]}')\n",
    "print(f'Informative features: 10 (hidden in 50 total)')\n",
    "print(f'\\nFeature correlation matrix shape: {df.iloc[:, :-1].corr().shape}')\n",
    "print(f'Max correlation: {df.iloc[:, :-1].corr().abs().values[np.triu_indices_from(df.iloc[:, :-1].corr(), k=1)].max():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc6efd",
   "metadata": {},
   "source": [
    "### 2.2 Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fd4b0d",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Split data and standardize features for regularization\n",
    "\n",
    "**Key Points:**\n",
    "- **Split first, then scale**: Prevents data leakage (test set never seen during scaling)\n",
    "- **StandardScaler critical**: Ridge/Lasso penalize by coefficient magnitude - features must be same scale\n",
    "- **Fit on train only**: Scaler learns statistics from training data, applies to test\n",
    "- **80-20 split**: Standard ratio balancing training data and test reliability\n",
    "\n",
    "**Why This Matters:**\n",
    "- Without scaling, features with large ranges get under-penalized\n",
    "- Data leakage (scaling on full data) inflates performance metrics artificially\n",
    "- Proper workflow ensures production-ready code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features (critical for regularization)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f'\u2705 Data split and scaled')\n",
    "print(f'Training samples: {X_train_scaled.shape[0]}')\n",
    "print(f'Test samples: {X_test_scaled.shape[0]}')\n",
    "print(f'Features: {X_train_scaled.shape[1]}')\n",
    "print(f'\\nFeature means after scaling (should be ~0): {X_train_scaled.mean(axis=0)[:5]}')\n",
    "print(f'Feature stds after scaling (should be ~1): {X_train_scaled.std(axis=0)[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7562911",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Mathematical Foundation\n",
    "\n",
    "### 3.1 Ridge Regression (L2 Regularization)\n",
    "\n",
    "**Objective function:**\n",
    "$$\\min_{\\boldsymbol{\\beta}} \\left\\{ \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2 \\right\\}$$\n",
    "\n",
    "**Closed-form solution:**\n",
    "$$\\boldsymbol{\\beta}_{ridge} = (\\mathbf{X}^T \\mathbf{X} + \\alpha \\mathbf{I})^{-1} \\mathbf{X}^T \\mathbf{y}$$\n",
    "\n",
    "**Properties:**\n",
    "- Shrinks all coefficients toward zero (but never exactly zero)\n",
    "- Works well with correlated features\n",
    "- Always has unique solution (even when $\\mathbf{X}^T \\mathbf{X}$ singular)\n",
    "\n",
    "### 3.2 Lasso Regression (L1 Regularization)\n",
    "\n",
    "**Objective function:**\n",
    "$$\\min_{\\boldsymbol{\\beta}} \\left\\{ \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 + \\alpha \\sum_{j=1}^{p} |\\beta_j| \\right\\}$$\n",
    "\n",
    "**Properties:**\n",
    "- Forces some coefficients to **exactly zero** (feature selection)\n",
    "- No closed-form solution (solved via coordinate descent)\n",
    "- Tends to pick one feature from correlated groups\n",
    "\n",
    "### 3.3 ElasticNet (L1 + L2)\n",
    "\n",
    "**Objective function:**\n",
    "$$\\min_{\\boldsymbol{\\beta}} \\left\\{ \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 + \\alpha \\rho \\sum_{j=1}^{p} |\\beta_j| + \\frac{\\alpha(1-\\rho)}{2} \\sum_{j=1}^{p} \\beta_j^2 \\right\\}$$\n",
    "\n",
    "**Properties:**\n",
    "- Combines benefits: feature selection + handling correlated features\n",
    "- Keeps grouped correlated features together\n",
    "- $\\rho = 0$: Pure Ridge, $\\rho = 1$: Pure Lasso\n",
    "\n",
    "### 3.4 Geometric Interpretation\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[L2 Penalty<br/>Smooth Circle<br/>Coefficients Shrink Smoothly] --> B[Optimal Point]\n",
    "    C[L1 Penalty<br/>Diamond Shape<br/>Hits Axes \u2192 Zeros] --> B\n",
    "    style A fill:#4CAF50,stroke:#333,stroke-width:2px,color:#fff\n",
    "    style C fill:#FF9800,stroke:#333,stroke-width:2px,color:#fff\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea9ef8",
   "metadata": {},
   "source": [
    "## 4. Implementation from Scratch (Ridge)\n",
    "\n",
    "Implement Ridge regression to understand the math."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7998b63",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Build Ridge regression from scratch using closed-form solution\n",
    "\n",
    "**Key Points:**\n",
    "- **Normal equation + penalty**: $(\\mathbf{X}^T \\mathbf{X} + \\alpha \\mathbf{I})^{-1} \\mathbf{X}^T \\mathbf{y}$\n",
    "- **Identity matrix**: $\\alpha \\mathbf{I}$ adds to diagonal, ensures invertibility\n",
    "- **No intercept penalty**: Only coefficients penalized (intercept term excluded)\n",
    "- **Numerical stability**: Ridge makes inversion stable even with multicollinearity\n",
    "\n",
    "**Why This Matters:**\n",
    "- Shows regularization as simple modification to OLS\n",
    "- Explains why Ridge always has solution (even singular matrices)\n",
    "- Understanding math helps debug production issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f410a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegressionScratch:\n",
    "    \"\"\"\n",
    "    Ridge Regression from scratch using closed-form solution\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.coefficients = None\n",
    "        self.intercept = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit Ridge regression: \u03b2 = (X^T X + \u03b1I)^(-1) X^T y\n",
    "        \"\"\"\n",
    "        # Add intercept column\n",
    "        n_samples = X.shape[0]\n",
    "        X_with_intercept = np.column_stack([np.ones(n_samples), X])\n",
    "        \n",
    "        # Create penalty matrix (don't penalize intercept)\n",
    "        n_features = X_with_intercept.shape[1]\n",
    "        penalty_matrix = np.eye(n_features) * self.alpha\n",
    "        penalty_matrix[0, 0] = 0  # No penalty on intercept\n",
    "        \n",
    "        # Ridge solution\n",
    "        XtX = X_with_intercept.T @ X_with_intercept\n",
    "        Xty = X_with_intercept.T @ y\n",
    "        \n",
    "        coefficients_all = np.linalg.inv(XtX + penalty_matrix) @ Xty\n",
    "        \n",
    "        self.intercept = coefficients_all[0]\n",
    "        self.coefficients = coefficients_all[1:]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.coefficients + self.intercept\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Train from-scratch Ridge\n",
    "model_scratch = RidgeRegressionScratch(alpha=1.0)\n",
    "model_scratch.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_r2 = model_scratch.score(X_train_scaled, y_train)\n",
    "test_r2 = model_scratch.score(X_test_scaled, y_test)\n",
    "y_pred_scratch = model_scratch.predict(X_test_scaled)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_scratch))\n",
    "\n",
    "print('\u2705 From-Scratch Ridge Regression (\u03b1=1.0)')\n",
    "print(f'Training R\u00b2: {train_r2:.4f}')\n",
    "print(f'Test R\u00b2: {test_r2:.4f}')\n",
    "print(f'Test RMSE: {rmse:.4f}')\n",
    "print(f'Non-zero coefficients: {np.sum(np.abs(model_scratch.coefficients) > 0.001)}/{len(model_scratch.coefficients)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef86b6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Production Implementation with Scikit-learn\n",
    "\n",
    "### 5.1 Compare OLS vs Ridge vs Lasso vs ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9704b0",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Systematically compare all four regression methods\n",
    "\n",
    "**Key Points:**\n",
    "- **OLS baseline**: No regularization - overfits with high-dimensional data\n",
    "- **Ridge**: Shrinks coefficients but keeps all features\n",
    "- **Lasso**: Zeros out irrelevant features (automatic selection)\n",
    "- **ElasticNet**: Balanced approach with both penalties\n",
    "\n",
    "**Why This Matters:**\n",
    "- OLS likely overfits (training R\u00b2 >> test R\u00b2)\n",
    "- Lasso should identify ~10 important features\n",
    "- Ridge should have better test R\u00b2 than OLS\n",
    "- ElasticNet often wins in high-dimensional settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with reasonable alpha\n",
    "models = {\n",
    "    'OLS': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.1),\n",
    "    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_r2 = model.score(X_train_scaled, y_train)\n",
    "    test_r2 = model.score(X_test_scaled, y_test)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # Count non-zero coefficients\n",
    "    if hasattr(model, 'coef_'):\n",
    "        n_nonzero = np.sum(np.abs(model.coef_) > 0.001)\n",
    "    else:\n",
    "        n_nonzero = X.shape[1]\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train_R2': train_r2,\n",
    "        'Test_R2': test_r2,\n",
    "        'Test_RMSE': rmse,\n",
    "        'Test_MAE': mae,\n",
    "        'Non_Zero_Features': n_nonzero,\n",
    "        'Overfitting_Gap': train_r2 - test_r2\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print('\ud83d\udcca Model Comparison: OLS vs Ridge vs Lasso vs ElasticNet\\n')\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Identify best model\n",
    "best_model = results_df.loc[results_df['Test_R2'].idxmax(), 'Model']\n",
    "print(f'\\n\ud83c\udfaf Best model: {best_model} (highest Test R\u00b2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63662174",
   "metadata": {},
   "source": [
    "### 5.2 Hyperparameter Tuning with Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fef1dd",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Find optimal alpha (regularization strength) using cross-validation\n",
    "\n",
    "**Key Points:**\n",
    "- **Alpha range**: Test from 0.001 (weak) to 100 (strong regularization)\n",
    "- **RidgeCV/LassoCV**: Built-in CV - efficient, no manual loops needed\n",
    "- **Logarithmic scale**: Alpha effects are multiplicative (0.01, 0.1, 1, 10, 100)\n",
    "- **Best alpha**: Chosen automatically based on CV performance\n",
    "\n",
    "**Why This Matters:**\n",
    "- Wrong alpha \u2192 underfitting (too high) or overfitting (too low)\n",
    "- CV ensures alpha generalizes to unseen data\n",
    "- Automates hyperparameter search - production-ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha values to test (logarithmic scale)\n",
    "alphas = np.logspace(-3, 2, 50)  # 0.001 to 100\n",
    "\n",
    "# Ridge with CV\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5)\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Lasso with CV\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, max_iter=10000)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ElasticNet with CV\n",
    "enet_cv = ElasticNetCV(alphas=alphas, l1_ratio=0.5, cv=5, max_iter=10000)\n",
    "enet_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print('\ud83d\udd0d Optimal Alpha Values (via 5-Fold CV)\\n')\n",
    "print(f'Ridge optimal \u03b1: {ridge_cv.alpha_:.4f}')\n",
    "print(f'Lasso optimal \u03b1: {lasso_cv.alpha_:.4f}')\n",
    "print(f'ElasticNet optimal \u03b1: {enet_cv.alpha_:.4f}')\n",
    "\n",
    "# Evaluate tuned models\n",
    "print('\\n\ud83d\udcca Performance with Optimized Alpha:\\n')\n",
    "for name, model in [('Ridge', ridge_cv), ('Lasso', lasso_cv), ('ElasticNet', enet_cv)]:\n",
    "    test_r2 = model.score(X_test_scaled, y_test)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    n_nonzero = np.sum(np.abs(model.coef_) > 0.001)\n",
    "    \n",
    "    print(f'{name:12} \u2192 Test R\u00b2: {test_r2:.4f}, RMSE: {rmse:.4f}, Features: {n_nonzero}/50')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0792063",
   "metadata": {},
   "source": [
    "### 5.3 Visualize Coefficient Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c5850e",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Visualize how coefficients change with regularization strength\n",
    "\n",
    "**Key Points:**\n",
    "- **Coefficient paths**: Each line represents one feature's coefficient vs alpha\n",
    "- **Ridge behavior**: All coefficients shrink smoothly toward zero (never exactly zero)\n",
    "- **Lasso behavior**: Coefficients hit zero at different alphas (sequential feature elimination)\n",
    "- **Feature selection**: Lasso path shows which features eliminated first\n",
    "\n",
    "**Why This Matters:**\n",
    "- Visual proof of L1 vs L2 difference\n",
    "- Identifies robust features (survive high alpha)\n",
    "- Helps communicate regularization to stakeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffedadbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute coefficient paths for Ridge and Lasso\n",
    "alphas_path = np.logspace(-2, 2, 100)\n",
    "\n",
    "# Ridge path\n",
    "ridge_coefs = []\n",
    "for alpha in alphas_path:\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    ridge_coefs.append(model.coef_)\n",
    "ridge_coefs = np.array(ridge_coefs)\n",
    "\n",
    "# Lasso path\n",
    "lasso_coefs = []\n",
    "for alpha in alphas_path:\n",
    "    model = Lasso(alpha=alpha, max_iter=10000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    lasso_coefs.append(model.coef_)\n",
    "lasso_coefs = np.array(lasso_coefs)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Ridge coefficient paths\n",
    "for i in range(ridge_coefs.shape[1]):\n",
    "    axes[0].plot(alphas_path, ridge_coefs[:, i], alpha=0.6, linewidth=1.5)\n",
    "axes[0].axvline(ridge_cv.alpha_, color='red', linestyle='--', linewidth=2, label=f'Optimal \u03b1={ridge_cv.alpha_:.3f}')\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('Alpha (log scale)', fontsize=12)\n",
    "axes[0].set_ylabel('Coefficient Value', fontsize=12)\n",
    "axes[0].set_title('Ridge Coefficient Paths (L2)\\nAll coefficients shrink smoothly', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Lasso coefficient paths\n",
    "for i in range(lasso_coefs.shape[1]):\n",
    "    axes[1].plot(alphas_path, lasso_coefs[:, i], alpha=0.6, linewidth=1.5)\n",
    "axes[1].axvline(lasso_cv.alpha_, color='red', linestyle='--', linewidth=2, label=f'Optimal \u03b1={lasso_cv.alpha_:.3f}')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlabel('Alpha (log scale)', fontsize=12)\n",
    "axes[1].set_ylabel('Coefficient Value', fontsize=12)\n",
    "axes[1].set_title('Lasso Coefficient Paths (L1)\\nCoefficients hit zero (feature selection)', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\ud83d\udcc8 Coefficient Path Interpretation:')\n",
    "print('   \u2192 Ridge: Gradual shrinkage, all features retained')\n",
    "print('   \u2192 Lasso: Sequential zeroing, automatic feature selection')\n",
    "print(f'   \u2192 At optimal \u03b1, Lasso keeps {np.sum(np.abs(lasso_cv.coef_) > 0.001)} features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2bb2c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Feature Selection Analysis (Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012761ef",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Analyze which features Lasso selected as important\n",
    "\n",
    "**Key Points:**\n",
    "- **Non-zero coefficients**: Features Lasso kept (implicitly important)\n",
    "- **Zero coefficients**: Features eliminated (redundant or irrelevant)\n",
    "- **Magnitude ranking**: Larger |coefficient| \u2192 more important\n",
    "- **Ground truth comparison**: Validate against known informative features\n",
    "\n",
    "**Why This Matters:**\n",
    "- Reduces model complexity (fewer features \u2192 faster inference)\n",
    "- Improves interpretability (focus on key drivers)\n",
    "- Lowers storage/memory requirements in production\n",
    "- Validates feature engineering decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Lasso coefficients\n",
    "lasso_coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': lasso_cv.coef_,\n",
    "    'Abs_Coefficient': np.abs(lasso_cv.coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Identify selected features\n",
    "selected_features = lasso_coef_df[lasso_coef_df['Abs_Coefficient'] > 0.001]\n",
    "zeroed_features = lasso_coef_df[lasso_coef_df['Abs_Coefficient'] <= 0.001]\n",
    "\n",
    "print(f'\ud83d\udcca Lasso Feature Selection Results (\u03b1={lasso_cv.alpha_:.4f})\\n')\n",
    "print(f'Features selected: {len(selected_features)}/50')\n",
    "print(f'Features eliminated: {len(zeroed_features)}/50')\n",
    "print(f'\\nTop 15 Selected Features:\\n')\n",
    "print(selected_features.head(15).to_string(index=False))\n",
    "\n",
    "# Visualize selected features\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['green' if c >= 0 else 'red' for c in selected_features['Coefficient']]\n",
    "plt.barh(selected_features['Feature'][:15], selected_features['Coefficient'][:15], \n",
    "         color=colors, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Lasso Coefficient', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title(f'Top 15 Features Selected by Lasso (\u03b1={lasso_cv.alpha_:.4f})', fontsize=13, fontweight='bold')\n",
    "plt.axvline(0, color='black', linewidth=0.8)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3453a353",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Model Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3e7eb",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Comprehensive diagnostic plots for best regularized model\n",
    "\n",
    "**Key Points:**\n",
    "- **Predicted vs Actual**: Points near diagonal indicate good predictions\n",
    "- **Residual plot**: Random scatter confirms no systematic errors\n",
    "- **Residual distribution**: Normal distribution validates assumptions\n",
    "- **Q-Q plot**: Diagonal line confirms residual normality\n",
    "\n",
    "**Why This Matters:**\n",
    "- Patterns in residuals indicate model misspecification\n",
    "- Non-normal residuals invalidate confidence intervals\n",
    "- Validates that regularization didn't introduce bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best model (ElasticNet from earlier comparison)\n",
    "best_model_obj = enet_cv\n",
    "y_test_pred = best_model_obj.predict(X_test_scaled)\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Diagnostic plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Predicted vs Actual\n",
    "axes[0, 0].scatter(y_test, y_test_pred, alpha=0.6, edgecolor='k')\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0, 0].set_xlabel('Actual Target', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Predicted Target', fontsize=12)\n",
    "axes[0, 0].set_title('Predicted vs Actual (Test Set)', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residual Plot\n",
    "axes[0, 1].scatter(y_test_pred, residuals, alpha=0.6, edgecolor='k')\n",
    "axes[0, 1].axhline(0, color='r', linestyle='--', lw=2)\n",
    "axes[0, 1].set_xlabel('Predicted Target', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Residuals', fontsize=12)\n",
    "axes[0, 1].set_title('Residual Plot', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residual Distribution\n",
    "axes[1, 0].hist(residuals, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(residuals.mean(), color='r', linestyle='--', lw=2, \n",
    "                   label=f'Mean: {residuals.mean():.2f}')\n",
    "axes[1, 0].set_xlabel('Residuals', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 0].set_title('Residual Distribution', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Q-Q Plot\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot (Normality Check)', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\ud83d\udd0d Diagnostic Summary:')\n",
    "print(f'   \u2192 Mean residual: {residuals.mean():.4f} (close to 0 \u2713)')\n",
    "print(f'   \u2192 Residual std: {residuals.std():.4f}')\n",
    "print(f'   \u2192 R\u00b2 on test: {r2_score(y_test, y_test_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea982e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Real-World Projects\n",
    "\n",
    "### \ud83d\udd2c Post-Silicon Validation Projects\n",
    "\n",
    "#### **Project 1: High-Dimensional STDF Parameter Reduction**\n",
    "\n",
    "**Objective:** Reduce 1000+ parametric test parameters to ~50 key predictors for yield modeling.\n",
    "\n",
    "**Business Value:**\n",
    "- Reduce test time by 40% (skip redundant tests)\n",
    "- Lower data storage costs\n",
    "- Faster model inference\n",
    "- Maintain prediction accuracy >95%\n",
    "\n",
    "**Dataset Features:**\n",
    "- 1000+ parametric tests (voltage, current, frequency, timing)\n",
    "- High correlation (VDD tests, timing paths)\n",
    "- Mix of analog and digital parameters\n",
    "- Spatial wafer data (die_x, die_y)\n",
    "\n",
    "**Implementation Guide:**\n",
    "1. Start with ElasticNet (handles correlated groups)\n",
    "2. Use `ElasticNetCV` with `l1_ratio=0.7` (favor selection)\n",
    "3. Test alphas: `np.logspace(-4, 1, 50)`\n",
    "4. Validate on multiple wafer lots\n",
    "5. Keep features with |coef| > threshold\n",
    "\n",
    "**Expected Outcomes:** 950 \u2192 50 parameters, R\u00b2 > 0.90\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 2: Correlated Test Elimination**\n",
    "\n",
    "**Objective:** Identify and remove redundant tests that provide no additional yield information.\n",
    "\n",
    "**Business Value:**\n",
    "- Cut ATE test cost per device\n",
    "- Increase throughput\n",
    "- Simplify test programs\n",
    "- Reduce engineering debug time\n",
    "\n",
    "**Implementation Guide:**\n",
    "1. Calculate correlation matrix of all tests\n",
    "2. Apply Lasso to zero out redundant tests\n",
    "3. Cross-validate on different product lots\n",
    "4. Verify eliminated tests truly redundant (domain expert review)\n",
    "\n",
    "**Expected Outcomes:** 30% test reduction, <2% yield prediction degradation\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 3: Sparse Yield Modeling**\n",
    "\n",
    "**Objective:** Build yield prediction model using minimal test set for early-stage screening.\n",
    "\n",
    "**Business Value:**\n",
    "- Enable fast screening before expensive tests\n",
    "- Support multi-site parallel testing\n",
    "- Reduce cost of test development\n",
    "- Identify critical yield limiters\n",
    "\n",
    "**Implementation Guide:**\n",
    "1. Use Lasso for aggressive feature selection\n",
    "2. Start with alpha=1.0, decrease until R\u00b2>0.85\n",
    "3. Include physics-based features (power, speed)\n",
    "4. Validate on out-of-lot data\n",
    "\n",
    "**Expected Outcomes:** 10-15 critical tests identified, 90%+ yield prediction accuracy\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 4: Robust Power Parameter Prediction**\n",
    "\n",
    "**Objective:** Predict device power consumption from fast pre-tests, handling measurement noise.\n",
    "\n",
    "**Business Value:**\n",
    "- Skip expensive power measurements\n",
    "- Real-time binning decisions\n",
    "- Reduce test time by 25%\n",
    "- Robust to measurement variance\n",
    "\n",
    "**Implementation Guide:**\n",
    "1. Use Ridge for robustness (outlier resistance)\n",
    "2. Include voltage, frequency, temperature features\n",
    "3. Add polynomial terms for V\u00b2 (physics-based)\n",
    "4. Cross-validate across temperature corners\n",
    "\n",
    "**Expected Outcomes:** Power prediction within 5%, robust to 10% measurement noise\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udcca General AI/ML Projects\n",
    "\n",
    "#### **Project 5: Genomic Biomarker Discovery**\n",
    "\n",
    "**Objective:** Identify disease-predictive genes from 10,000+ gene expression levels.\n",
    "\n",
    "**Business Value:**\n",
    "- Drug target discovery\n",
    "- Personalized medicine\n",
    "- Reduce diagnostic test costs\n",
    "\n",
    "**Implementation:** Lasso for gene selection, ElasticNet for gene networks\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 6: Text Classification with Large Vocabulary**\n",
    "\n",
    "**Objective:** Build spam filter with 50,000+ word features, select key spam indicators.\n",
    "\n",
    "**Business Value:**\n",
    "- Fast inference (fewer features)\n",
    "- Interpretable rules\n",
    "- Lower memory footprint\n",
    "\n",
    "**Implementation:** Lasso L1 penalty naturally handles sparse text features\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 7: Financial Risk Modeling**\n",
    "\n",
    "**Objective:** Predict credit default from 200+ correlated economic indicators.\n",
    "\n",
    "**Business Value:**\n",
    "- Accurate risk assessment\n",
    "- Regulatory compliance\n",
    "- Automated lending decisions\n",
    "\n",
    "**Implementation:** Ridge handles multicollinearity (GDP, inflation, rates correlated)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 8: Image Compression and Denoising**\n",
    "\n",
    "**Objective:** Learn sparse representation of images for compression.\n",
    "\n",
    "**Business Value:**\n",
    "- Reduce storage costs\n",
    "- Faster transmission\n",
    "- Remove noise while preserving edges\n",
    "\n",
    "**Implementation:** Lasso on pixel/wavelet features for sparse coding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9bddd",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "### \u2705 When to Use Each Method\n",
    "\n",
    "**Ridge (L2):**\n",
    "- Many correlated features\n",
    "- Want smooth shrinkage (keep all features)\n",
    "- Multicollinearity problems\n",
    "- Numerical stability critical\n",
    "\n",
    "**Lasso (L1):**\n",
    "- Need feature selection\n",
    "- Sparse models preferred\n",
    "- Interpretability important\n",
    "- More features than samples\n",
    "\n",
    "**ElasticNet:**\n",
    "- High-dimensional data (p >> n)\n",
    "- Grouped correlated features\n",
    "- Best of both worlds\n",
    "- When unsure, start here\n",
    "\n",
    "### \u26a0\ufe0f Limitations\n",
    "\n",
    "**All Methods:**\n",
    "- Still assumes linear relationship (use with polynomial features for non-linearity)\n",
    "- Require feature scaling\n",
    "- Alpha tuning needed (computationally expensive)\n",
    "\n",
    "**Lasso Specific:**\n",
    "- Picks arbitrarily from correlated features\n",
    "- Can be unstable (small data changes \u2192 different features)\n",
    "- Max selected features = n samples\n",
    "\n",
    "### \ud83c\udfaf Best Practices\n",
    "\n",
    "1. **Always scale features** (StandardScaler before regularization)\n",
    "2. **Use CV for alpha** (RidgeCV, LassoCV, ElasticNetCV)\n",
    "3. **Start with ElasticNet** (covers Ridge and Lasso as special cases)\n",
    "4. **Validate feature selection** (check if Lasso choices make domain sense)\n",
    "5. **Monitor overfitting** (train vs test R\u00b2 gap)\n",
    "6. **Test on unseen data** (different batches/lots for production readiness)\n",
    "\n",
    "### \ud83d\udcda Next Steps\n",
    "\n",
    "After mastering regularization:\n",
    "1. **`013_Logistic_Regression.ipynb`** - Classification with regularization\n",
    "2. **`016_Decision_Trees.ipynb`** - Non-linear alternatives\n",
    "3. **Polynomial + Regularization** - Combine for powerful models\n",
    "\n",
    "### \ud83d\udd11 Core Concepts Mastered\n",
    "\n",
    "\u2705 L1, L2, and combined penalties  \n",
    "\u2705 Bias-variance tradeoff through regularization  \n",
    "\u2705 Automatic feature selection with Lasso  \n",
    "\u2705 Handling multicollinearity with Ridge  \n",
    "\u2705 Hyperparameter tuning via cross-validation  \n",
    "\u2705 Production-ready pipelines with CV  \n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You can now handle high-dimensional data, multicollinearity, and perform automatic feature selection. These are essential skills for real-world ML where data is often messy and high-dimensional."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}