{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 022 - Voting & Stacking Ensembles: Meta-Learning for Superior Performance\n",
        "\n",
        "## \ud83d\udcd8 Introduction\n",
        "\n",
        "**Ensemble methods** combine multiple models to achieve better performance than any single model. While Random Forest and Gradient Boosting are ensemble methods themselves, **meta-ensembles** take this further by combining *different types* of models.\n",
        "\n",
        "### Types of Meta-Ensembles\n",
        "\n",
        "1. **Voting** - Average/majority vote across models\n",
        "2. **Stacking** - Train meta-model on base model predictions\n",
        "3. **Blending** - Simpler stacking with hold-out validation set\n",
        "\n",
        "### Why Meta-Ensembles?\n",
        "\n",
        "**The wisdom of crowds:** Combining diverse models reduces errors because:\n",
        "- **Reduces variance**: Different models make different errors\n",
        "- **Reduces bias**: Weak models complement each other\n",
        "- **Captures different patterns**: Linear + tree + neural approaches\n",
        "- **Kaggle dominance**: Top solutions are almost always ensembles\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "**Diversity is critical:**\n",
        "- Combining 5 identical models \u2192 no benefit\n",
        "- Combining 5 diverse models \u2192 significant improvement\n",
        "- Use different algorithms: Linear, Tree, Boosting, Neural\n",
        "- Use different feature sets, hyperparameters, or data subsets\n",
        "\n",
        "**When to Use Ensembles:**\n",
        "\n",
        "\u2705 **Competitions** (Kaggle, etc.) - Every 0.01% accuracy matters  \n",
        "\u2705 **High-stakes predictions** (medical, financial) - Need maximum reliability  \n",
        "\u2705 **Production systems** - Robust to model degradation  \n",
        "\u2705 **Diverse data** - Different models capture different patterns  \n",
        "\u2705 **Have compute budget** - Can train multiple models  \n",
        "\n",
        "\u274c **Avoid when:**\n",
        "- Need fast inference (<10ms)\n",
        "- Limited training resources\n",
        "- Interpretability critical (ensemble = black box)\n",
        "- Single model already achieves target accuracy\n",
        "\n",
        "### Comparison: Voting vs Stacking vs Blending\n",
        "\n",
        "| Aspect | Voting | Stacking | Blending |\n",
        "|--------|--------|----------|----------|\n",
        "| **Complexity** | Simple | Complex | Medium |\n",
        "| **Training** | Parallel | Sequential (2-level) | Sequential |\n",
        "| **Overfitting risk** | Low | **Higher** | Medium |\n",
        "| **Performance** | Good | **Best** | Good |\n",
        "| **Interpretability** | Easy | Hard | Medium |\n",
        "| **Use case** | Quick ensemble | **Competition winning** | Production balanced |\n",
        "\n",
        "### Learning Path Context\n",
        "\n",
        "- **016_Decision_Trees** - Single tree foundations\n",
        "- **017_Random_Forest** - Bagging ensemble (parallel)\n",
        "- **018-021_Gradient_Boosting** - Sequential ensembles\n",
        "- **022_Voting_Stacking (this)** - Meta-ensembles (combining different models)\n",
        "- **023_Hyperparameter_Optimization** (next) - Systematic tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd04 Meta-Ensemble Workflows\n",
        "\n",
        "### Voting Ensemble\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[Training Data] --> B1[Model 1: LogisticReg]\n",
        "    A --> B2[Model 2: Random Forest]\n",
        "    A --> B3[Model 3: XGBoost]\n",
        "    \n",
        "    B1 --> C[Voting]\n",
        "    B2 --> C\n",
        "    B3 --> C\n",
        "    \n",
        "    C --> D{Voting Type}\n",
        "    D -->|Hard| E[Majority Vote]\n",
        "    D -->|Soft| F[Average Probabilities]\n",
        "    E --> G[Final Prediction]\n",
        "    F --> G\n",
        "    \n",
        "    style C fill:#e1f5ff\n",
        "    style G fill:#e1ffe1\n",
        "```\n",
        "\n",
        "### Stacking Ensemble\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[Training Data] --> B{K-Fold Split}\n",
        "    B --> C1[Fold 1]\n",
        "    B --> C2[Fold 2]\n",
        "    B --> C3[Fold K]\n",
        "    \n",
        "    C1 --> D1[Base Models on Fold 1]\n",
        "    C2 --> D2[Base Models on Fold 2]\n",
        "    C3 --> D3[Base Models on Fold K]\n",
        "    \n",
        "    D1 --> E[Out-of-Fold Predictions]\n",
        "    D2 --> E\n",
        "    D3 --> E\n",
        "    \n",
        "    E --> F[Meta-Features]\n",
        "    F --> G[Meta-Model: LogisticReg]\n",
        "    G --> H[Final Prediction]\n",
        "    \n",
        "    style E fill:#fff4e1\n",
        "    style F fill:#f0e1ff\n",
        "    style H fill:#e1ffe1\n",
        "```\n",
        "\n",
        "**Key Differences:**\n",
        "- **Voting**: Simple average/majority, no meta-model\n",
        "- **Stacking**: Meta-model learns optimal weights from out-of-fold predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcd0 Mathematical Foundation\n",
        "\n",
        "### 1. Voting Ensembles\n",
        "\n",
        "**Hard Voting (Classification):**\n",
        "$$\\hat{y} = \\text{mode}(h_1(x), h_2(x), ..., h_M(x))$$\n",
        "\n",
        "Where $h_m(x)$ is the prediction from model $m$, and mode is the most frequent class.\n",
        "\n",
        "**Example:** 3 models predict [0, 1, 1] \u2192 Hard vote = 1 (majority)\n",
        "\n",
        "**Soft Voting (Classification with probabilities):**\n",
        "$$\\hat{y} = \\arg\\max_c \\frac{1}{M} \\sum_{m=1}^M P_m(y = c | x)$$\n",
        "\n",
        "Where $P_m(y = c | x)$ is model $m$'s predicted probability for class $c$.\n",
        "\n",
        "**Example:**\n",
        "- Model 1: P(class=1) = 0.6\n",
        "- Model 2: P(class=1) = 0.55\n",
        "- Model 3: P(class=1) = 0.9\n",
        "- Average: (0.6 + 0.55 + 0.9) / 3 = 0.683 \u2192 Predict class 1\n",
        "\n",
        "**Weighted Voting:**\n",
        "$$\\hat{y} = \\arg\\max_c \\sum_{m=1}^M w_m \\cdot P_m(y = c | x)$$\n",
        "\n",
        "Where $w_m$ is the weight for model $m$ (e.g., based on validation accuracy), and $\\sum w_m = 1$.\n",
        "\n",
        "**Voting for Regression:**\n",
        "$$\\hat{y} = \\frac{1}{M} \\sum_{m=1}^M h_m(x)$$\n",
        "\n",
        "Simple average of predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Stacking (Stacked Generalization)\n",
        "\n",
        "**Two-level architecture:**\n",
        "\n",
        "**Level 0 (Base models):**\n",
        "$$h_m(x) = f_m(x; \\theta_m), \\quad m = 1, ..., M$$\n",
        "\n",
        "Train $M$ diverse base models on training data.\n",
        "\n",
        "**Level 1 (Meta-model):**\n",
        "$$\\hat{y} = g([h_1(x), h_2(x), ..., h_M(x)]; \\phi)$$\n",
        "\n",
        "Where:\n",
        "- $g$ is the meta-model (often LogisticRegression, Ridge, or LightGBM)\n",
        "- $[h_1(x), ..., h_M(x)]$ are the meta-features (base model predictions)\n",
        "- $\\phi$ are meta-model parameters\n",
        "\n",
        "**Critical: Out-of-Fold Predictions**\n",
        "\n",
        "To prevent overfitting, base models must predict on data they haven't seen:\n",
        "\n",
        "1. Split training data into K folds\n",
        "2. For each fold $k$:\n",
        "   - Train base model on folds $\\neq k$\n",
        "   - Predict on fold $k$ (out-of-fold predictions)\n",
        "3. Concatenate all out-of-fold predictions \u2192 meta-features\n",
        "4. Train meta-model on meta-features\n",
        "\n",
        "**Mathematical formulation:**\n",
        "$$\\text{Meta-features}_{\\text{train}} = [h_1^{\\text{OOF}}, h_2^{\\text{OOF}}, ..., h_M^{\\text{OOF}}]$$\n",
        "$$\\text{Meta-features}_{\\text{test}} = [h_1(x_{\\text{test}}), h_2(x_{\\text{test}}), ..., h_M(x_{\\text{test}})]$$\n",
        "\n",
        "Where $h_m^{\\text{OOF}}$ are out-of-fold predictions from model $m$.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Blending\n",
        "\n",
        "Simpler alternative to stacking:\n",
        "\n",
        "1. Split training data: 80% train, 20% hold-out\n",
        "2. Train base models on 80% train\n",
        "3. Predict on 20% hold-out \u2192 meta-features\n",
        "4. Train meta-model on hold-out meta-features\n",
        "\n",
        "**Advantage:** Simpler, faster (no K-fold cross-validation)  \n",
        "**Disadvantage:** Uses less data for base models (80% vs 100% in stacking)\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Why Ensembles Work: Bias-Variance Decomposition\n",
        "\n",
        "**Expected error of a model:**\n",
        "$$E[(y - \\hat{y})^2] = \\text{Bias}^2 + \\text{Variance} + \\sigma^2$$\n",
        "\n",
        "**For ensemble of M uncorrelated models:**\n",
        "$$\\text{Variance}_{\\text{ensemble}} = \\frac{1}{M} \\cdot \\text{Variance}_{\\text{individual}}$$\n",
        "\n",
        "Variance reduces by factor of $M$!\n",
        "\n",
        "**For correlated models:**\n",
        "$$\\text{Variance}_{\\text{ensemble}} = \\rho \\cdot \\text{Variance}_{\\text{individual}} + \\frac{1-\\rho}{M} \\cdot \\text{Variance}_{\\text{individual}}$$\n",
        "\n",
        "Where $\\rho$ is correlation between models.\n",
        "\n",
        "**Key insight:** Diversity (low $\\rho$) is critical for ensemble performance!\n",
        "\n",
        "---\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "**Voting:**\n",
        "- `voting='hard'` or `'soft'`\n",
        "- `weights=[w1, w2, ...]` for weighted voting\n",
        "\n",
        "**Stacking:**\n",
        "- `cv=5` (number of folds for out-of-fold predictions)\n",
        "- `final_estimator` (meta-model, default LogisticRegression)\n",
        "- `passthrough=False` (include original features in meta-model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd27 Setup and Imports\n",
        "\n",
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Import libraries for ensemble methods and diverse base models.\n",
        "\n",
        "**Key Points:**\n",
        "- **VotingClassifier/Regressor**: Simple averaging ensemble\n",
        "- **StackingClassifier/Regressor**: Two-level meta-learning\n",
        "- **Diverse base models**: Linear, Tree, Boosting for maximum diversity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    roc_auc_score, roc_curve, mean_squared_error, r2_score\n",
        ")\n",
        "\n",
        "# Ensemble methods\n",
        "from sklearn.ensemble import (\n",
        "    VotingClassifier, VotingRegressor,\n",
        "    StackingClassifier, StackingRegressor,\n",
        "    RandomForestClassifier, RandomForestRegressor,\n",
        "    GradientBoostingClassifier, GradientBoostingRegressor\n",
        ")\n",
        "\n",
        "# Base models for diversity\n",
        "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Modern boosting\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "print(\"\u2705 Libraries loaded successfully\")\n",
        "print(\"   Ready to build meta-ensembles!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\uddf3\ufe0f Voting Ensemble: Simple Yet Effective\n",
        "\n",
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Combine diverse models using hard and soft voting for classification.\n",
        "\n",
        "**Key Points:**\n",
        "- **Base models**: LogisticRegression (linear), RandomForest (tree), XGBoost (boosting)\n",
        "- **Hard voting**: Majority class wins (e.g., 2/3 predict class 1 \u2192 output class 1)\n",
        "- **Soft voting**: Average probabilities (better when models are calibrated)\n",
        "- **Diversity**: Linear + tree + boosting approaches capture different patterns\n",
        "\n",
        "**Why This Matters:** Voting ensembles are simple to implement, parallelize easily, and often outperform single models by 2-5%.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate realistic classification dataset\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "print(\"\ud83d\udcca Generating classification dataset...\\n\")\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=5000,\n",
        "    n_features=20,\n",
        "    n_informative=15,\n",
        "    n_redundant=3,\n",
        "    n_clusters_per_class=2,\n",
        "    class_sep=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\u2705 Dataset generated:\")\n",
        "print(f\"   Training samples: {len(X_train)}\")\n",
        "print(f\"   Test samples: {len(X_test)}\")\n",
        "print(f\"   Features: {X.shape[1]}\")\n",
        "print(f\"   Classes: {len(np.unique(y))}\")\n",
        "print(f\"   Class distribution: {np.bincount(y_train) / len(y_train) * 100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83d\udd28 Training individual base models...\\n\")\n",
        "\n",
        "# Model 1: Logistic Regression (linear)\n",
        "print(\"1\ufe0f\u20e3 Logistic Regression (Linear)\")\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr.fit(X_train, y_train)\n",
        "lr_pred = lr.predict(X_test)\n",
        "lr_acc = accuracy_score(y_test, lr_pred)\n",
        "lr_auc = roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1])\n",
        "print(f\"   Accuracy: {lr_acc:.4f}, AUC: {lr_auc:.4f}\")\n",
        "\n",
        "# Model 2: Random Forest (tree ensemble)\n",
        "print(f\"\\n2\ufe0f\u20e3 Random Forest (Tree Ensemble)\")\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, rf_pred)\n",
        "rf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
        "print(f\"   Accuracy: {rf_acc:.4f}, AUC: {rf_auc:.4f}\")\n",
        "\n",
        "# Model 3: XGBoost (gradient boosting)\n",
        "print(f\"\\n3\ufe0f\u20e3 XGBoost (Gradient Boosting)\")\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "xgb_pred = xgb_clf.predict(X_test)\n",
        "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
        "xgb_auc = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1])\n",
        "print(f\"   Accuracy: {xgb_acc:.4f}, AUC: {xgb_auc:.4f}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Individual Model Performance Summary:\")\n",
        "print(f\"   {'Model':<25} {'Accuracy':<12} {'AUC':<10}\")\n",
        "print(f\"   {'-'*47}\")\n",
        "print(f\"   {'Logistic Regression':<25} {lr_acc:<12.4f} {lr_auc:<10.4f}\")\n",
        "print(f\"   {'Random Forest':<25} {rf_acc:<12.4f} {rf_auc:<10.4f}\")\n",
        "print(f\"   {'XGBoost':<25} {xgb_acc:<12.4f} {xgb_auc:<10.4f}\")\n",
        "print(f\"\\n   Best single model: {max([('LR', lr_acc), ('RF', rf_acc), ('XGB', xgb_acc)], key=lambda x: x[1])[0]} ({max(lr_acc, rf_acc, xgb_acc):.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83d\uddf3\ufe0f Building Voting Ensembles...\\n\")\n",
        "\n",
        "# Hard Voting\n",
        "print(\"1\ufe0f\u20e3 Hard Voting (Majority Class)\")\n",
        "voting_hard = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', lr),\n",
        "        ('rf', rf),\n",
        "        ('xgb', xgb_clf)\n",
        "    ],\n",
        "    voting='hard'\n",
        ")\n",
        "voting_hard.fit(X_train, y_train)\n",
        "voting_hard_pred = voting_hard.predict(X_test)\n",
        "voting_hard_acc = accuracy_score(y_test, voting_hard_pred)\n",
        "print(f\"   Accuracy: {voting_hard_acc:.4f}\")\n",
        "\n",
        "# Soft Voting\n",
        "print(f\"\\n2\ufe0f\u20e3 Soft Voting (Average Probabilities)\")\n",
        "voting_soft = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', lr),\n",
        "        ('rf', rf),\n",
        "        ('xgb', xgb_clf)\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "voting_soft.fit(X_train, y_train)\n",
        "voting_soft_pred = voting_soft.predict(X_test)\n",
        "voting_soft_acc = accuracy_score(y_test, voting_soft_pred)\n",
        "voting_soft_auc = roc_auc_score(y_test, voting_soft.predict_proba(X_test)[:, 1])\n",
        "print(f\"   Accuracy: {voting_soft_acc:.4f}\")\n",
        "print(f\"   AUC: {voting_soft_auc:.4f}\")\n",
        "\n",
        "# Weighted Soft Voting (weight by individual performance)\n",
        "print(f\"\\n3\ufe0f\u20e3 Weighted Soft Voting (Performance-Based Weights)\")\n",
        "weights = [lr_auc, rf_auc, xgb_auc]  # Weight by AUC\n",
        "voting_weighted = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', lr),\n",
        "        ('rf', rf),\n",
        "        ('xgb', xgb_clf)\n",
        "    ],\n",
        "    voting='soft',\n",
        "    weights=weights\n",
        ")\n",
        "voting_weighted.fit(X_train, y_train)\n",
        "voting_weighted_pred = voting_weighted.predict(X_test)\n",
        "voting_weighted_acc = accuracy_score(y_test, voting_weighted_pred)\n",
        "voting_weighted_auc = roc_auc_score(y_test, voting_weighted.predict_proba(X_test)[:, 1])\n",
        "print(f\"   Weights: {[f'{w:.3f}' for w in weights]}\")\n",
        "print(f\"   Accuracy: {voting_weighted_acc:.4f}\")\n",
        "print(f\"   AUC: {voting_weighted_auc:.4f}\")\n",
        "\n",
        "# Comparison\n",
        "print(f\"\\n\ud83d\udcca Voting Ensemble Comparison:\")\n",
        "print(f\"   {'Ensemble Type':<30} {'Accuracy':<12} {'AUC':<10}\")\n",
        "print(f\"   {'-'*52}\")\n",
        "print(f\"   {'Best Single Model':<30} {max(lr_acc, rf_acc, xgb_acc):<12.4f} {max(lr_auc, rf_auc, xgb_auc):<10.4f}\")\n",
        "print(f\"   {'Hard Voting':<30} {voting_hard_acc:<12.4f} {'N/A':<10}\")\n",
        "print(f\"   {'Soft Voting':<30} {voting_soft_acc:<12.4f} {voting_soft_auc:<10.4f}\")\n",
        "print(f\"   {'Weighted Soft Voting':<30} {voting_weighted_acc:<12.4f} {voting_weighted_auc:<10.4f}\")\n",
        "\n",
        "improvement = (voting_soft_acc - max(lr_acc, rf_acc, xgb_acc)) / max(lr_acc, rf_acc, xgb_acc) * 100\n",
        "print(f\"\\n\ud83d\udca1 Ensemble Improvement: {improvement:.2f}% over best single model\")\n",
        "print(f\"   Soft voting typically outperforms hard voting by using probability information\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udccb Batch 1 Summary: Voting Ensembles Complete\n",
        "\n",
        "### \u2705 What We've Covered\n",
        "\n",
        "1. **Meta-ensemble concepts** - Combining different model types\n",
        "2. **Voting ensembles** - Hard, soft, and weighted voting\n",
        "3. **Diverse base models** - Linear + Tree + Boosting for maximum diversity\n",
        "4. **Performance comparison** - Ensemble typically 2-5% better than single models\n",
        "\n",
        "### \ud83c\udfaf Key Insights\n",
        "\n",
        "- **Diversity is critical**: Use different algorithm families (linear, tree, boosting)\n",
        "- **Soft voting > hard voting**: Probability averaging uses more information\n",
        "- **Weighted voting**: Give stronger models more influence\n",
        "- **Simple to implement**: No complex training procedure\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\ude80 Coming in Batch 2\n",
        "\n",
        "- **Stacking ensembles** - Meta-model learns optimal combination\n",
        "- **Out-of-fold predictions** - Prevent overfitting in stacking\n",
        "- **Post-silicon application** - Multi-model yield prediction\n",
        "- **Blending** - Simpler alternative to stacking\n",
        "- **8 Real-world projects** - Competition winning and production systems\n",
        "- **Best practices** - When to use voting vs stacking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcda Stacking: Meta-Learning with Out-of-Fold Predictions\n",
        "\n",
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Build two-level stacking ensemble where meta-model learns optimal combination of base models.\n",
        "\n",
        "**Key Points:**\n",
        "- **Level 0 (Base models)**: Diverse models trained on original features\n",
        "- **Out-of-fold predictions**: Each sample predicted by models that didn't see it during training\n",
        "- **Level 1 (Meta-model)**: Learns to combine base model predictions optimally\n",
        "- **Critical**: Must use out-of-fold predictions to prevent overfitting\n",
        "\n",
        "**Why This Matters:** Stacking typically outperforms voting by 1-3% because the meta-model learns optimal weights rather than simple averaging. Dominant in Kaggle competitions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83d\udcda Building Stacking Ensemble...\\n\")\n",
        "\n",
        "# Base models (diverse algorithm families)\n",
        "base_models = [\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)),\n",
        "    ('xgb', xgb.XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1,\n",
        "                              random_state=42, use_label_encoder=False, eval_metric='logloss')),\n",
        "    ('lgb', lgb.LGBMClassifier(n_estimators=100, num_leaves=31, learning_rate=0.1,\n",
        "                               random_state=42, verbose=-1))\n",
        "]\n",
        "\n",
        "# Meta-model (simple linear model to learn optimal weights)\n",
        "meta_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Stacking Classifier with 5-fold out-of-fold predictions\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=base_models,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5,  # 5-fold cross-validation for out-of-fold predictions\n",
        "    stack_method='predict_proba',  # Use probabilities instead of hard predictions\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\ud83d\udd28 Training stacking ensemble (this takes longer due to cross-validation)...\")\n",
        "start_time = time.time()\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "stacking_time = time.time() - start_time\n",
        "\n",
        "# Predictions\n",
        "stacking_pred = stacking_clf.predict(X_test)\n",
        "stacking_proba = stacking_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Metrics\n",
        "stacking_acc = accuracy_score(y_test, stacking_pred)\n",
        "stacking_auc = roc_auc_score(y_test, stacking_proba)\n",
        "\n",
        "print(f\"\\n\u2705 Stacking complete ({stacking_time:.2f}s)\")\n",
        "print(f\"   Base models: 4 (LR, RF, XGB, LGB)\")\n",
        "print(f\"   Meta-model: LogisticRegression\")\n",
        "print(f\"   Cross-validation: 5-fold\")\n",
        "print(f\"   Accuracy: {stacking_acc:.4f}\")\n",
        "print(f\"   AUC: {stacking_auc:.4f}\")\n",
        "\n",
        "# Compare with voting\n",
        "print(f\"\\n\ud83d\udcca Stacking vs Voting vs Single Models:\")\n",
        "print(f\"   {'Method':<30} {'Accuracy':<12} {'AUC':<10}\")\n",
        "print(f\"   {'-'*52}\")\n",
        "print(f\"   {'Best Single Model':<30} {max(lr_acc, rf_acc, xgb_acc):<12.4f} {max(lr_auc, rf_auc, xgb_auc):<10.4f}\")\n",
        "print(f\"   {'Soft Voting':<30} {voting_soft_acc:<12.4f} {voting_soft_auc:<10.4f}\")\n",
        "print(f\"   {'Weighted Soft Voting':<30} {voting_weighted_acc:<12.4f} {voting_weighted_auc:<10.4f}\")\n",
        "print(f\"   {'Stacking':<30} {stacking_acc:<12.4f} {stacking_auc:<10.4f}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udca1 Key Insights:\")\n",
        "stacking_improvement = (stacking_acc - max(lr_acc, rf_acc, xgb_acc)) / max(lr_acc, rf_acc, xgb_acc) * 100\n",
        "print(f\"   \u2022 Stacking improvement over best single: {stacking_improvement:.2f}%\")\n",
        "print(f\"   \u2022 Meta-model learns optimal weights from out-of-fold predictions\")\n",
        "print(f\"   \u2022 Typically 1-3% better than voting (worth the extra complexity)\")\n",
        "print(f\"   \u2022 Training time: {stacking_time / max([lr, rf, xgb_clf], key=lambda m: 1):.1f}x longer (due to cross-validation)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze meta-model learned weights\n",
        "print(\"\ud83d\udd0d Meta-Model Analysis\\n\")\n",
        "\n",
        "# Get meta-model coefficients (weights for each base model's predictions)\n",
        "meta_coef = stacking_clf.final_estimator_.coef_[0]\n",
        "base_model_names = [name for name, _ in base_models]\n",
        "\n",
        "# For predict_proba, we get 2 predictions per model (prob class 0, prob class 1)\n",
        "# Extract weights for class 1 probabilities\n",
        "num_base_models = len(base_models)\n",
        "class_1_coefs = meta_coef[num_base_models:2*num_base_models]  # Second set of coefficients\n",
        "\n",
        "print(\"\ud83d\udcca Meta-Model Learned Weights (for class 1 probabilities):\\n\")\n",
        "for name, coef in zip(base_model_names, class_1_coefs):\n",
        "    print(f\"   {name:<10} \u2192 {coef:>8.4f}\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(base_model_names, class_1_coefs)\n",
        "plt.xlabel('Base Model', fontsize=12)\n",
        "plt.ylabel('Meta-Model Weight', fontsize=12)\n",
        "plt.title('Stacking Meta-Model Learned Weights', fontsize=14, fontweight='bold')\n",
        "plt.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n\ud83d\udca1 Interpretation:\")\n",
        "print(f\"   \u2022 Positive weights: Model's predictions positively influence final prediction\")\n",
        "print(f\"   \u2022 Larger magnitude: More influence in ensemble\")\n",
        "print(f\"   \u2022 Meta-model automatically learns optimal combination\")\n",
        "print(f\"   \u2022 Different from equal-weight voting (all weights = 1/{len(base_models)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd2c Post-Silicon Application: Multi-Model Yield Prediction\n",
        "\n",
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Build production-grade stacking ensemble for semiconductor yield prediction.\n",
        "\n",
        "**Key Points:**\n",
        "- **Scale**: 50K devices with 25 parametric tests + categorical features\n",
        "- **Base models**: 5 diverse models (Linear, Tree, RF, XGB, LGB)\n",
        "- **Meta-model**: LightGBM (faster than LogisticRegression for large data)\n",
        "- **Business value**: Combine strengths of multiple models for robust predictions\n",
        "\n",
        "**Why This Matters:** In production, model performance can degrade over time. Ensembles are more robust to drift because failure of one model doesn't break the system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate realistic post-silicon dataset\n",
        "print(\"\ud83c\udfed Generating 50K device post-silicon dataset...\\n\")\n",
        "\n",
        "np.random.seed(42)\n",
        "n_devices = 50000\n",
        "\n",
        "# Categorical features\n",
        "equipment_ids = [f'EQ_{i:03d}' for i in range(50)]\n",
        "lot_ids = [f'LOT_{i:04d}' for i in range(100)]\n",
        "\n",
        "equipment_id = np.random.choice(equipment_ids, n_devices)\n",
        "lot_id = np.random.choice(lot_ids, n_devices)\n",
        "\n",
        "# Equipment/lot effects\n",
        "equipment_effects = {eq: np.random.normal(0, 2) for eq in equipment_ids}\n",
        "lot_effects = {lot: np.random.normal(0, 3) for lot in lot_ids}\n",
        "\n",
        "# Parametric tests (25 features)\n",
        "voltage = np.random.normal(1.8, 0.04, n_devices)\n",
        "current = np.random.normal(150, 18, n_devices)\n",
        "frequency = np.random.normal(2000, 90, n_devices)\n",
        "temperature = np.random.uniform(25, 85, n_devices)\n",
        "power = voltage * current\n",
        "leakage = np.random.exponential(9, n_devices)\n",
        "delay = np.random.normal(500, 45, n_devices)\n",
        "jitter = np.random.exponential(18, n_devices)\n",
        "noise_margin = np.random.normal(0.3, 0.055, n_devices)\n",
        "skew = np.random.normal(0, 16, n_devices)\n",
        "\n",
        "# Additional tests\n",
        "additional_tests = {f'test_{i}': np.random.normal(100, 10, n_devices) for i in range(11, 26)}\n",
        "\n",
        "# Spatial\n",
        "die_x = np.random.randint(0, 50, n_devices)\n",
        "die_y = np.random.randint(0, 50, n_devices)\n",
        "\n",
        "# Complex yield model\n",
        "yield_score = (\n",
        "    100 +\n",
        "    0.5 * (frequency - 2000) / 90 +\n",
        "    -0.35 * (temperature - 25) / 10 +\n",
        "    -1.1 * leakage / 9 +\n",
        "    -0.06 * delay / 45 +\n",
        "    -0.25 * jitter / 18 +\n",
        "    11 * noise_margin +\n",
        "    np.array([equipment_effects[eq] for eq in equipment_id]) +\n",
        "    np.array([lot_effects[lot] for lot in lot_id]) +\n",
        "    np.random.normal(0, 5.5, n_devices)\n",
        ")\n",
        "\n",
        "yield_binary = (yield_score > 95).astype(int)\n",
        "\n",
        "# Create DataFrame\n",
        "df_ps = pd.DataFrame({\n",
        "    'Equipment_ID': equipment_id,\n",
        "    'Lot_ID': lot_id,\n",
        "    'Voltage_V': voltage,\n",
        "    'Current_mA': current,\n",
        "    'Frequency_MHz': frequency,\n",
        "    'Temperature_C': temperature,\n",
        "    'Power_mW': power,\n",
        "    'Leakage_uA': leakage,\n",
        "    'Delay_ps': delay,\n",
        "    'Jitter_ps': jitter,\n",
        "    'Noise_Margin': noise_margin,\n",
        "    'Skew_ps': skew,\n",
        "    'Die_X': die_x,\n",
        "    'Die_Y': die_y,\n",
        "    'Yield': yield_binary\n",
        "})\n",
        "\n",
        "for name, values in additional_tests.items():\n",
        "    df_ps[name] = values\n",
        "\n",
        "# Encode categoricals for sklearn models\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le_eq = LabelEncoder()\n",
        "le_lot = LabelEncoder()\n",
        "df_ps['Equipment_ID_encoded'] = le_eq.fit_transform(df_ps['Equipment_ID'])\n",
        "df_ps['Lot_ID_encoded'] = le_lot.fit_transform(df_ps['Lot_ID'])\n",
        "\n",
        "print(f\"\u2705 Dataset Generated:\")\n",
        "print(f\"   Devices: {n_devices:,}\")\n",
        "print(f\"   Features: {df_ps.shape[1] - 1} (2 categorical + 25 parametric + 2 spatial)\")\n",
        "print(f\"   Yield rate: {yield_binary.mean():.1%}\")\n",
        "print(f\"\\n{df_ps.head()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Prepare data (use encoded categoricals)\n",
        "feature_cols = [col for col in df_ps.columns if col not in ['Yield', 'Equipment_ID', 'Lot_ID']]\n",
        "X_ps = df_ps[feature_cols].values\n",
        "y_ps = df_ps['Yield'].values\n",
        "\n",
        "X_train_ps, X_test_ps, y_train_ps, y_test_ps = train_test_split(\n",
        "    X_ps, y_ps, test_size=0.2, random_state=42, stratify=y_ps\n",
        ")\n",
        "\n",
        "print(\"\ud83d\ude80 Building Production Stacking Ensemble...\\n\")\n",
        "\n",
        "# Diverse base models\n",
        "base_models_ps = [\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n",
        "    ('dt', DecisionTreeClassifier(max_depth=10, random_state=42)),\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, max_depth=12, random_state=42, n_jobs=-1)),\n",
        "    ('xgb', xgb.XGBClassifier(n_estimators=150, max_depth=6, learning_rate=0.05,\n",
        "                              random_state=42, use_label_encoder=False, eval_metric='logloss', n_jobs=-1)),\n",
        "    ('lgb', lgb.LGBMClassifier(n_estimators=150, num_leaves=31, learning_rate=0.05,\n",
        "                               random_state=42, verbose=-1, n_jobs=-1))\n",
        "]\n",
        "\n",
        "# Meta-model: LightGBM (faster than LogReg for large data)\n",
        "meta_model_ps = lgb.LGBMClassifier(n_estimators=50, num_leaves=15, learning_rate=0.1,\n",
        "                                   random_state=42, verbose=-1)\n",
        "\n",
        "print(\"1\ufe0f\u20e3 Training base models individually (for comparison)...\")\n",
        "base_results = {}\n",
        "for name, model in base_models_ps:\n",
        "    model.fit(X_train_ps, y_train_ps)\n",
        "    pred = model.predict(X_test_ps)\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        proba = model.predict_proba(X_test_ps)[:, 1]\n",
        "        auc = roc_auc_score(y_test_ps, proba)\n",
        "    else:\n",
        "        auc = None\n",
        "    acc = accuracy_score(y_test_ps, pred)\n",
        "    base_results[name] = {'accuracy': acc, 'auc': auc}\n",
        "    print(f\"   {name:<6} Accuracy: {acc:.4f}, AUC: {auc:.4f if auc else 'N/A'}\")\n",
        "\n",
        "# Stacking ensemble\n",
        "print(f\"\\n2\ufe0f\u20e3 Training stacking ensemble (5-fold CV)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "stacking_ps = StackingClassifier(\n",
        "    estimators=base_models_ps,\n",
        "    final_estimator=meta_model_ps,\n",
        "    cv=5,\n",
        "    stack_method='predict_proba',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "stacking_ps.fit(X_train_ps, y_train_ps)\n",
        "stacking_ps_time = time.time() - start_time\n",
        "\n",
        "# Predictions\n",
        "stacking_ps_pred = stacking_ps.predict(X_test_ps)\n",
        "stacking_ps_proba = stacking_ps.predict_proba(X_test_ps)[:, 1]\n",
        "\n",
        "# Metrics\n",
        "stacking_ps_acc = accuracy_score(y_test_ps, stacking_ps_pred)\n",
        "stacking_ps_auc = roc_auc_score(y_test_ps, stacking_ps_proba)\n",
        "stacking_ps_cm = confusion_matrix(y_test_ps, stacking_ps_pred)\n",
        "\n",
        "print(f\"\\n\u2705 Stacking ensemble complete ({stacking_ps_time:.2f}s)\")\n",
        "print(f\"   Accuracy: {stacking_ps_acc:.4f}\")\n",
        "print(f\"   AUC: {stacking_ps_auc:.4f}\")\n",
        "\n",
        "# Comparison\n",
        "print(f\"\\n\ud83d\udcca Model Comparison on 50K Device Dataset:\")\n",
        "print(f\"   {'Model':<20} {'Accuracy':<12} {'AUC':<10}\")\n",
        "print(f\"   {'-'*42}\")\n",
        "for name, results in base_results.items():\n",
        "    auc_str = f\"{results['auc']:.4f}\" if results['auc'] else 'N/A'\n",
        "    print(f\"   {name.upper():<20} {results['accuracy']:<12.4f} {auc_str:<10}\")\n",
        "print(f\"   {'-'*42}\")\n",
        "print(f\"   {'STACKING ENSEMBLE':<20} {stacking_ps_acc:<12.4f} {stacking_ps_auc:<10.4f}\")\n",
        "\n",
        "best_base_acc = max(r['accuracy'] for r in base_results.values())\n",
        "improvement = (stacking_ps_acc - best_base_acc) / best_base_acc * 100\n",
        "print(f\"\\n\ud83d\udca1 Stacking improvement: {improvement:.2f}% over best single model\")\n",
        "\n",
        "# Confusion matrix\n",
        "print(f\"\\n\ud83d\udccb Confusion Matrix:\")\n",
        "print(f\"   True Neg:  {stacking_ps_cm[0,0]:7,}  |  False Pos: {stacking_ps_cm[0,1]:6,}\")\n",
        "print(f\"   False Neg: {stacking_ps_cm[1,0]:7,}  |  True Pos:  {stacking_ps_cm[1,1]:6,}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udccb Classification Report:\")\n",
        "print(classification_report(y_test_ps, stacking_ps_pred, target_names=['Fail', 'Pass']))\n",
        "\n",
        "# Business impact\n",
        "false_negatives = stacking_ps_cm[1, 0]\n",
        "false_positives = stacking_ps_cm[0, 1]\n",
        "true_negatives = stacking_ps_cm[0, 0]\n",
        "\n",
        "print(f\"\ud83d\udcb0 Business Impact (50K device analysis):\")\n",
        "print(f\"   Correctly caught failures: {true_negatives:,}\")\n",
        "print(f\"   Cost avoided: ${true_negatives * 1.0:,.0f}\")\n",
        "print(f\"   Missed failures: {false_negatives:,}\")\n",
        "print(f\"   Cost of misses: ${false_negatives * 5.0:,.0f}\")\n",
        "print(f\"   Net benefit: ${(true_negatives * 1.0 - false_negatives * 5.0 - false_positives * 3.0):,.0f}\")\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf Production Advantages:\")\n",
        "print(f\"   \u2022 Robust to model degradation (single model failure doesn't break system)\")\n",
        "print(f\"   \u2022 Combines strengths: Linear captures trends, trees capture non-linearities\")\n",
        "print(f\"   \u2022 Meta-model adapts weights as data distribution changes\")\n",
        "print(f\"   \u2022 Can add/remove base models without retraining from scratch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf Real-World Ensemble Projects\n",
        "\n",
        "### \ud83d\udd2c Post-Silicon Validation Projects (4)\n",
        "\n",
        "### **1. Multi-Model Test Flow Optimizer**\n",
        "**Objective:** Ensemble predicts test time from multiple model perspectives\n",
        "\n",
        "**Features:**\n",
        "- Base models: LinearRegression (trends), RandomForest (non-linear), XGBoost (interactions)\n",
        "- Stacking meta-model learns optimal combination\n",
        "- 15 parametric tests, equipment_id, lot_id categorical features\n",
        "- Predict total test time for adaptive scheduling\n",
        "\n",
        "**Success Metrics:**\n",
        "- Ensemble MAE <5ms (better than single models by 15-20%)\n",
        "- Robust to equipment changes (ensemble doesn't break)\n",
        "- Test time reduction: 20-30% via dynamic scheduling\n",
        "- Production uptime: 99.9% (voting fallback if one model fails)\n",
        "\n",
        "**Business Value:** $100K-300K annual savings per production line\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Robust Failure Mode Classification**\n",
        "**Objective:** Multi-class ensemble for identifying 10+ failure modes\n",
        "\n",
        "**Features:**\n",
        "- Base models: SVM (boundaries), RandomForest (non-linear), XGBoost (boosting)\n",
        "- 20 parametric tests + spatial features (die_x, die_y)\n",
        "- Failure modes: leakage, delay, power, frequency, voltage, mixed (10 classes)\n",
        "- Voting ensemble (hard voting for interpretability)\n",
        "\n",
        "**Success Metrics:**\n",
        "- Multi-class accuracy >75% (vs 65% single model)\n",
        "- Confusion between similar modes <10%\n",
        "- Root cause identification time: 2 hours \u2192 15 minutes\n",
        "- Ensemble robustness: Works even if one model degrades\n",
        "\n",
        "**Business Value:** Reduce debug time 60% \u2192 $500K-2M annual savings\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Production Drift-Resistant Yield Predictor**\n",
        "**Objective:** Stacking ensemble adapts to process changes over time\n",
        "\n",
        "**Features:**\n",
        "- Base models: 6 diverse models (Linear, Trees, Boosting, SVM)\n",
        "- Monthly retraining: Meta-model learns new weights as process drifts\n",
        "- Categorical: equipment_id (100+), lot_id (1000s), supplier_id\n",
        "- Temporal features: days_since_calibration, cumulative_devices\n",
        "\n",
        "**Success Metrics:**\n",
        "- AUC degradation <2% over 6 months (vs 8% single model)\n",
        "- Automatic weight adjustment detects model degradation\n",
        "- Alert when base model contribution drops >20%\n",
        "- Production uptime: 99.95% (ensemble never fails completely)\n",
        "\n",
        "**Business Value:** Consistent quality \u2192 $2-5M avoided rework costs annually\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Kaggle-Style Competition: Semiconductor Yield Prediction**\n",
        "**Objective:** Win internal data science competition using ensemble techniques\n",
        "\n",
        "**Features:**\n",
        "- Stacking: 10+ diverse base models (Linear, Tree, Boosting, Neural)\n",
        "- Feature engineering: Polynomial features, interactions, aggregations\n",
        "- Hyperparameter tuning: Optuna for each base model\n",
        "- Blending: Multiple stacking ensembles averaged\n",
        "\n",
        "**Success Metrics:**\n",
        "- Top 3 finish (AUC >0.95)\n",
        "- Beat baseline by >5%\n",
        "- Reproducible: Cross-validation AUC within 0.5% of leaderboard\n",
        "- Deploy winner to production\n",
        "\n",
        "**Business Value:** Prestige + production model worth $1-3M annually\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83c\udf10 General AI/ML Projects (4)\n",
        "\n",
        "### **5. Healthcare Multi-Model Readmission Predictor**\n",
        "**Objective:** Ensemble combines clinical, demographic, and behavioral models\n",
        "\n",
        "**Features:**\n",
        "- Base models: LogisticReg (demographics), RandomForest (clinical), XGBoost (behavioral)\n",
        "- Each model specializes in different feature subsets\n",
        "- Stacking meta-model learns patient-specific weights\n",
        "- 100+ features: diagnosis_codes, procedures, medications, social determinants\n",
        "\n",
        "**Success Metrics:**\n",
        "- AUC >0.78 (vs 0.72 single model)\n",
        "- Precision >75% (minimize false positives)\n",
        "- Recall >85% (catch most readmissions)\n",
        "- Interpretable: Can explain which sub-model contributed most\n",
        "\n",
        "**Business Value:** Reduce readmissions 18-25% \u2192 $8-15M per hospital\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Financial Fraud Detection Ensemble**\n",
        "**Objective:** Real-time fraud scoring with fallback redundancy\n",
        "\n",
        "**Features:**\n",
        "- Base models: IsolationForest (anomaly), XGBoost (patterns), LSTM (sequence)\n",
        "- Voting ensemble for <10ms latency (parallel inference)\n",
        "- Features: transaction_amount, velocity, merchant_risk, location_anomaly\n",
        "- Fallback: If LSTM fails, other models still work\n",
        "\n",
        "**Success Metrics:**\n",
        "- Precision >85% (minimize false declines)\n",
        "- Recall >92% (catch most fraud)\n",
        "- Latency <10ms (real-time authorization)\n",
        "- Uptime: 99.99% (ensemble never fully fails)\n",
        "\n",
        "**Business Value:** Block $20-50M fraud annually, reduce false declines 30%\n",
        "\n",
        "---\n",
        "\n",
        "### **7. E-Commerce Click-Through Rate Meta-Ensemble**\n",
        "**Objective:** Stacking for ad CTR prediction across multiple platforms\n",
        "\n",
        "**Features:**\n",
        "- Base models: FM (interactions), GBDT (non-linear), DeepFM (neural)\n",
        "- Each model trained on different feature representations\n",
        "- Meta-model: Lightweight LR for <5ms inference\n",
        "- 1M+ categorical features (user_id, ad_id, context)\n",
        "\n",
        "**Success Metrics:**\n",
        "- AUC >0.78 (vs 0.74 single model)\n",
        "- CTR prediction error <5%\n",
        "- Revenue lift: 12-18% from better targeting\n",
        "- A/B test: Ensemble beats single models by 15%\n",
        "\n",
        "**Business Value:** $10-30M additional revenue per quarter\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Kaggle Competition Framework (Tabular Data)**\n",
        "**Objective:** Reusable stacking pipeline for top 1% finishes\n",
        "\n",
        "**Features:**\n",
        "- Automated ensemble: 15+ base models with hyperparameter tuning\n",
        "- Multi-level stacking: Level 1 (10 models) \u2192 Level 2 (5 models) \u2192 Level 3 (meta)\n",
        "- Blending: Average 3 stacking ensembles for robustness\n",
        "- Feature engineering: Automated polynomial, interactions, aggregations\n",
        "\n",
        "**Success Metrics:**\n",
        "- Top 1% finish on 5+ Kaggle competitions\n",
        "- Reproducible: Cross-validation within 1% of leaderboard\n",
        "- Fast iteration: End-to-end pipeline in <6 hours\n",
        "- Open-source: Share framework for community\n",
        "\n",
        "**Business Value:** Career advancement, consulting opportunities, reputation\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u2705 Key Takeaways: Meta-Ensemble Mastery\n",
        "\n",
        "### \ud83c\udfaf When to Use Each Ensemble Type\n",
        "\n",
        "**Voting Ensembles:**\n",
        "- \u2705 Quick ensemble (parallel training)\n",
        "- \u2705 Interpretable (simple average/majority)\n",
        "- \u2705 Production systems with <100ms latency requirements\n",
        "- \u2705 Failover redundancy (voting continues if one model fails)\n",
        "- \u274c Not optimal (equal or manual weights)\n",
        "\n",
        "**Stacking Ensembles:**\n",
        "- \u2705 Competition winning (Kaggle, data science contests)\n",
        "- \u2705 Maximum accuracy (1-3% better than voting)\n",
        "- \u2705 Adaptive to data changes (meta-model learns optimal weights)\n",
        "- \u2705 Feature-aware meta-learning (passthrough=True option)\n",
        "- \u274c Slower training (cross-validation required)\n",
        "- \u274c More complex (two-level architecture)\n",
        "\n",
        "**Blending:**\n",
        "- \u2705 Simpler than stacking (no cross-validation)\n",
        "- \u2705 Faster training (single hold-out set)\n",
        "- \u2705 Good balance for production\n",
        "- \u274c Uses less data (80% train vs 100% in stacking)\n",
        "- \u274c Slightly worse than stacking\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd11 Critical Success Factors\n",
        "\n",
        "1. **Diversity is everything**\n",
        "   - Use different algorithm families: Linear, Tree, Boosting, Neural\n",
        "   - Different feature sets (e.g., one model on raw, another on engineered)\n",
        "   - Different hyperparameters or data subsets\n",
        "   - Rule of thumb: Base model correlation <0.8\n",
        "\n",
        "2. **Out-of-fold predictions for stacking**\n",
        "   - **Critical**: Never train meta-model on in-sample predictions\n",
        "   - Use cv=5 or cv=10 for robust out-of-fold predictions\n",
        "   - Prevents catastrophic overfitting\n",
        "\n",
        "3. **Meta-model simplicity**\n",
        "   - Prefer simple meta-models: LogisticRegression, Ridge, Lasso\n",
        "   - Complex meta-models (XGBoost) can overfit\n",
        "   - Exception: Large datasets (>100K) can use LightGBM meta-model\n",
        "\n",
        "4. **Computational trade-offs**\n",
        "   - Voting: 1x training time (parallel)\n",
        "   - Stacking: 5-10x training time (due to cross-validation)\n",
        "   - Inference: Both ~Mx prediction time (M = number of base models)\n",
        "   - Use voting if latency <10ms required\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udcca Performance Expectations\n",
        "\n",
        "| Dataset Size | Single Model | Voting | Stacking |\n",
        "|--------------|--------------|--------|----------|\n",
        "| Small (<10K) | Baseline | +1-2% | +2-4% |\n",
        "| Medium (10K-100K) | Baseline | +2-3% | +3-5% |\n",
        "| Large (>100K) | Baseline | +1-2% | +2-3% |\n",
        "\n",
        "*Improvements depend on base model diversity and dataset complexity*\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd27 Best Practices\n",
        "\n",
        "1. **Start with voting**\n",
        "   - Quick baseline ensemble\n",
        "   - Identifies if ensemble approach is promising\n",
        "   - Only move to stacking if voting shows benefit\n",
        "\n",
        "2. **Validate base model diversity**\n",
        "   ```python\n",
        "   # Check correlation between base model predictions\n",
        "   predictions_df = pd.DataFrame({\n",
        "       'model1': model1.predict_proba(X)[:, 1],\n",
        "       'model2': model2.predict_proba(X)[:, 1]\n",
        "   })\n",
        "   print(predictions_df.corr())\n",
        "   # Aim for correlation <0.8\n",
        "   ```\n",
        "\n",
        "3. **Monitor base model contributions**\n",
        "   - Track meta-model coefficients over time\n",
        "   - Remove base models with near-zero weights\n",
        "   - Alert when previously strong model degrades\n",
        "\n",
        "4. **Use cross-validation consistently**\n",
        "   - Same cv folds for all base models in stacking\n",
        "   - Stratified folds for classification\n",
        "   - Reproducible seeds\n",
        "\n",
        "5. **Production deployment patterns**\n",
        "   - Voting: Parallel microservices (failover redundancy)\n",
        "   - Stacking: Sequential pipeline (base models \u2192 meta-model)\n",
        "   - Cache base model predictions to speed up inference\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\ude80 Next Steps\n",
        "\n",
        "1. **023_Hyperparameter_Optimization.ipynb** - Systematic tuning with Optuna\n",
        "2. **024_Model_Interpretation.ipynb** - SHAP, LIME, feature interactions\n",
        "3. **025_Imbalanced_Learning.ipynb** - Class weights, SMOTE, custom loss\n",
        "4. **026_K_Means_Clustering.ipynb** - Unsupervised learning begins\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83c\udf93 What You've Mastered\n",
        "\n",
        "\u2705 **Voting ensembles** - Hard, soft, weighted voting  \n",
        "\u2705 **Stacking ensembles** - Two-level meta-learning  \n",
        "\u2705 **Out-of-fold predictions** - Prevent overfitting in stacking  \n",
        "\u2705 **Blending** - Simpler alternative to stacking  \n",
        "\u2705 **Diversity principles** - Why different models matter  \n",
        "\u2705 **Production deployment** - Robust, failover-ready systems  \n",
        "\u2705 **Meta-model selection** - Simple models for meta-learning  \n",
        "\u2705 **Business applications** - $100K-30M impact across domains  \n",
        "\n",
        "You now understand how to combine multiple models for superior performance and production robustness! \ud83c\udf89\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcda References and Further Reading\n",
        "\n",
        "### Original Papers\n",
        "\n",
        "1. **Stacked Generalization** (1992)  \n",
        "   Wolpert, Neural Networks 1992  \n",
        "   Original stacking paper introducing meta-learning concept\n",
        "\n",
        "2. **Ensemble Methods in Machine Learning** (2000)  \n",
        "   Dietterich, Multiple Classifier Systems 2000  \n",
        "   Comprehensive survey of ensemble techniques\n",
        "\n",
        "3. **Netflix Prize Winning Solution** (2009)  \n",
        "   Koren, Bellkor's Pragmatic Chaos  \n",
        "   100+ model ensemble, demonstrates stacking at scale\n",
        "\n",
        "### Official Documentation\n",
        "\n",
        "4. **Sklearn Ensemble Module**  \n",
        "   https://scikit-learn.org/stable/modules/ensemble.html\n",
        "\n",
        "5. **VotingClassifier/Regressor**  \n",
        "   https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
        "\n",
        "6. **StackingClassifier/Regressor**  \n",
        "   https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html\n",
        "\n",
        "### Kaggle Resources\n",
        "\n",
        "7. **Kaggle Ensembling Guide**  \n",
        "   https://mlwave.com/kaggle-ensembling-guide/\n",
        "\n",
        "8. **Stacking Made Easy**  \n",
        "   https://github.com/vecxoz/vecstack - Python library for stacking\n",
        "\n",
        "### Related Notebooks\n",
        "\n",
        "- **017_Random_Forest.ipynb** - Bagging ensemble (parallel trees)\n",
        "- **018_Gradient_Boosting.ipynb** - Sequential ensemble foundations\n",
        "- **019_XGBoost.ipynb** - Regularized boosting\n",
        "- **020_LightGBM.ipynb** - Histogram-based boosting\n",
        "- **021_CatBoost.ipynb** - Ordered boosting with categoricals\n",
        "- **023_Hyperparameter_Optimization.ipynb** (next) - Tune base models systematically\n",
        "\n",
        "---\n",
        "\n",
        "**Notebook Complete!** \u2705  \n",
        "**Next:** 023_Hyperparameter_Optimization.ipynb - Systematic tuning with Optuna/Hyperopt\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}