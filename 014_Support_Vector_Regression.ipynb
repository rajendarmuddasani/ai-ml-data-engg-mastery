{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5978f330",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc55a3",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Import essential libraries for SVR implementation, visualization, and evaluation.\n",
    "\n",
    "**Key Points:**\n",
    "- **NumPy**: Core mathematical operations for from-scratch SVR implementation\n",
    "- **Pandas**: Data manipulation for post-silicon STDF datasets\n",
    "- **Matplotlib/Seaborn**: Visualize epsilon tubes, support vectors, kernel effects\n",
    "- **Scikit-learn**: Production-ready SVR, StandardScaler (SVR requires scaled features), metrics\n",
    "- **Warnings**: Suppress convergence warnings during hyperparameter search\n",
    "\n",
    "**Why This Matters:** SVR is sensitive to feature scaling (unlike tree-based models). StandardScaler is ESSENTIAL for SVR to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.datasets import make_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acebca5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Part 1: SVR from Scratch (Educational)\n",
    "\n",
    "We'll implement a **simplified Linear SVR** using gradient descent to understand the mechanics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18fac3",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement Linear SVR from scratch using subgradient descent on epsilon-insensitive loss.\n",
    "\n",
    "**Key Points:**\n",
    "- **Epsilon-insensitive loss**: Computes penalty only for errors > Œµ\n",
    "- **Subgradient descent**: Used because epsilon-insensitive loss is non-differentiable at boundaries\n",
    "- **Regularization term**: $\\frac{1}{2}||w||^2$ prevents overfitting\n",
    "- **Support vectors**: Data points with non-zero gradients (outside epsilon tube)\n",
    "- **C parameter**: Balances fit quality vs. model complexity (similar to Ridge/Lasso alpha)\n",
    "\n",
    "**Why This Matters:** Understanding the optimization mechanics reveals why SVR is robust to outliers (outliers contribute limited penalty due to epsilon tube)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVRScratch:\n",
    "    \"\"\"\n",
    "    Linear Support Vector Regression from scratch.\n",
    "    Uses subgradient descent on epsilon-insensitive loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C=1.0, epsilon=0.1, learning_rate=0.01, n_iterations=1000):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        C : float\n",
    "            Penalty parameter (cost of violations)\n",
    "        epsilon : float\n",
    "            Width of epsilon tube (tolerance)\n",
    "        learning_rate : float\n",
    "            Step size for gradient descent\n",
    "        n_iterations : int\n",
    "            Number of training iterations\n",
    "        \"\"\"\n",
    "        self.C = C\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def _compute_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute epsilon-insensitive loss.\n",
    "        \"\"\"\n",
    "        predictions = X @ self.w + self.b\n",
    "        errors = np.abs(y - predictions)\n",
    "        \n",
    "        # Epsilon-insensitive: max(0, |error| - epsilon)\n",
    "        epsilon_loss = np.maximum(0, errors - self.epsilon)\n",
    "        \n",
    "        # Total loss: regularization + C * epsilon_loss\n",
    "        loss = 0.5 * np.dot(self.w, self.w) + self.C * np.sum(epsilon_loss)\n",
    "        return loss\n",
    "    \n",
    "    def _compute_gradients(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute subgradients for weights and bias.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        predictions = X @ self.w + self.b\n",
    "        errors = y - predictions\n",
    "        \n",
    "        # Initialize gradients\n",
    "        grad_w = self.w.copy()  # Regularization term\n",
    "        grad_b = 0\n",
    "        \n",
    "        # Add epsilon-insensitive loss gradients\n",
    "        for i in range(n_samples):\n",
    "            abs_error = np.abs(errors[i])\n",
    "            \n",
    "            if abs_error > self.epsilon:\n",
    "                # Outside epsilon tube\n",
    "                sign = np.sign(errors[i])\n",
    "                grad_w -= self.C * sign * X[i]\n",
    "                grad_b -= self.C * sign\n",
    "        \n",
    "        return grad_w, grad_b\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train SVR using subgradient descent.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for iteration in range(self.n_iterations):\n",
    "            # Compute gradients\n",
    "            grad_w, grad_b = self._compute_gradients(X, y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.w -= self.learning_rate * grad_w\n",
    "            self.b -= self.learning_rate * grad_b\n",
    "            \n",
    "            # Track loss\n",
    "            if iteration % 100 == 0:\n",
    "                loss = self._compute_loss(X, y)\n",
    "                self.loss_history.append(loss)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \"\"\"\n",
    "        return X @ self.w + self.b\n",
    "    \n",
    "    def get_support_vectors(self, X, y):\n",
    "        \"\"\"\n",
    "        Identify support vectors (points outside epsilon tube).\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        errors = np.abs(y - predictions)\n",
    "        return errors > self.epsilon\n",
    "\n",
    "print(\"‚úÖ LinearSVRScratch class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c90dec6",
   "metadata": {},
   "source": [
    "### Test From-Scratch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c805d0",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Validate our from-scratch SVR implementation on synthetic data with outliers.\n",
    "\n",
    "**Key Points:**\n",
    "- **Synthetic data**: Linear relationship + noise + outliers to test robustness\n",
    "- **Outlier injection**: 10% of data points have extreme values (¬±3 standard deviations)\n",
    "- **Feature scaling**: StandardScaler applied (critical for SVR performance)\n",
    "- **Epsilon tube visualization**: Shows which points are support vectors (outside tube)\n",
    "- **Performance metrics**: RMSE and R¬≤ to quantify prediction quality\n",
    "\n",
    "**Why This Matters:** Demonstrates SVR's core advantage‚Äîoutliers have limited impact on the regression line due to epsilon-insensitive loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7546a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with outliers\n",
    "np.random.seed(42)\n",
    "X_synthetic = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_synthetic = 2 * X_synthetic.ravel() + 1 + np.random.normal(0, 1, 100)\n",
    "\n",
    "# Add outliers (10% of data)\n",
    "outlier_indices = np.random.choice(100, size=10, replace=False)\n",
    "y_synthetic[outlier_indices] += np.random.choice([-1, 1], size=10) * np.random.uniform(5, 10, size=10)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_synthetic, y_synthetic, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features (CRITICAL for SVR)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train from-scratch SVR\n",
    "svr_scratch = LinearSVRScratch(C=1.0, epsilon=0.5, learning_rate=0.01, n_iterations=1000)\n",
    "svr_scratch.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_scratch = svr_scratch.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "rmse_scratch = np.sqrt(mean_squared_error(y_test, y_pred_scratch))\n",
    "r2_scratch = r2_score(y_test, y_pred_scratch)\n",
    "\n",
    "print(\"\\nüîß From-Scratch SVR Performance:\")\n",
    "print(f\"RMSE: {rmse_scratch:.4f}\")\n",
    "print(f\"R¬≤: {r2_scratch:.4f}\")\n",
    "\n",
    "# Identify support vectors\n",
    "support_vectors = svr_scratch.get_support_vectors(X_train_scaled, y_train)\n",
    "print(f\"\\nSupport Vectors: {np.sum(support_vectors)} / {len(y_train)} ({100*np.mean(support_vectors):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca27db",
   "metadata": {},
   "source": [
    "### Visualize Epsilon Tube and Support Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3162e9bf",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Visualize the epsilon tube concept and identify support vectors (points driving the model).\n",
    "\n",
    "**Key Points:**\n",
    "- **Epsilon tube**: Gray shaded region (¬±Œµ around regression line) where errors are ignored\n",
    "- **Support vectors (red)**: Points outside the tube that contribute to the loss function\n",
    "- **Regular points (blue)**: Points inside tube with zero loss\n",
    "- **Robust regression**: SVR line ignores outliers better than OLS would\n",
    "- **Sparse model**: Only support vectors matter for predictions (efficiency advantage)\n",
    "\n",
    "**Why This Matters:** Visual understanding of why SVR is robust‚Äîmost data points (inside tube) don't influence the model at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e8a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot epsilon tube\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Generate smooth prediction line\n",
    "X_plot = np.linspace(X_train_scaled.min(), X_train_scaled.max(), 100).reshape(-1, 1)\n",
    "y_plot = svr_scratch.predict(X_plot)\n",
    "\n",
    "# Identify support vectors on training data\n",
    "y_train_pred = svr_scratch.predict(X_train_scaled)\n",
    "train_errors = np.abs(y_train - y_train_pred)\n",
    "support_vector_mask = train_errors > svr_scratch.epsilon\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X_train_scaled[~support_vector_mask], y_train[~support_vector_mask], \n",
    "           c='blue', alpha=0.5, label='Regular Points (inside tube)', s=50)\n",
    "plt.scatter(X_train_scaled[support_vector_mask], y_train[support_vector_mask], \n",
    "           c='red', marker='x', s=100, label='Support Vectors (outside tube)', linewidths=2)\n",
    "plt.plot(X_plot, y_plot, 'g-', linewidth=2, label='SVR Prediction')\n",
    "plt.fill_between(X_plot.ravel(), \n",
    "                 y_plot - svr_scratch.epsilon, \n",
    "                 y_plot + svr_scratch.epsilon, \n",
    "                 alpha=0.2, color='gray', label=f'Epsilon Tube (Œµ={svr_scratch.epsilon})')\n",
    "\n",
    "plt.xlabel('Feature (scaled)', fontsize=12)\n",
    "plt.ylabel('Target', fontsize=12)\n",
    "plt.title('SVR: Epsilon Tube and Support Vectors', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Interpretation:\")\n",
    "print(f\"‚Ä¢ Red X markers: {np.sum(support_vector_mask)} support vectors (contribute to loss)\")\n",
    "print(f\"‚Ä¢ Blue circles: {np.sum(~support_vector_mask)} regular points (zero loss, ignored)\")\n",
    "print(f\"‚Ä¢ Gray band: Epsilon tube (¬±{svr_scratch.epsilon}) where errors are tolerated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef53924",
   "metadata": {},
   "source": [
    "### Loss Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7c0518",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Verify that our from-scratch SVR implementation converges during training.\n",
    "\n",
    "**Key Points:**\n",
    "- **Loss function**: Combination of regularization term ($\\frac{1}{2}||w||^2$) and epsilon-insensitive loss\n",
    "- **Convergence pattern**: Should decrease rapidly initially, then stabilize\n",
    "- **Non-smooth curve**: Expected due to subgradient descent (not true gradient)\n",
    "- **Learning rate impact**: If loss diverges, learning rate is too high\n",
    "- **Validation**: Confirms our implementation is optimizing correctly\n",
    "\n",
    "**Why This Matters:** Loss convergence is essential validation‚Äîif loss doesn't decrease, the implementation has bugs or hyperparameters are wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea72fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss convergence\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(svr_scratch.loss_history, linewidth=2)\n",
    "plt.xlabel('Iteration (x100)', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('SVR Training Loss Convergence', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìâ Loss Analysis:\")\n",
    "print(f\"‚Ä¢ Initial loss: {svr_scratch.loss_history[0]:.4f}\")\n",
    "print(f\"‚Ä¢ Final loss: {svr_scratch.loss_history[-1]:.4f}\")\n",
    "print(f\"‚Ä¢ Loss reduction: {100*(1 - svr_scratch.loss_history[-1]/svr_scratch.loss_history[0]):.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
