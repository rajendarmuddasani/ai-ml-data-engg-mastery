{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "359407c7",
   "metadata": {},
   "source": [
    "# 067: Neural Architecture Search (NAS)",
    "",
    "## \ud83d\udcda Introduction",
    "",
    "Welcome to **Neural Architecture Search (NAS)** - the technology that automates the most challenging part of deep learning: designing optimal network architectures. This notebook explores how AI designs AI, eliminating months of manual experimentation and achieving superhuman performance.",
    "",
    "---",
    "",
    "### **\ud83d\ude80 Why Neural Architecture Search Matters**",
    "",
    "**The Manual Architecture Design Problem:**",
    "- ResNet (2015): 2 years research, 34-152 layers, countless failed experiments",
    "- Transformer (2017): 6 months iteration, 6 encoder/decoder layers, 8 attention heads",
    "- EfficientNet (2019): 9 months compound scaling exploration, multiple design iterations",
    "- **Total cost:** 10-50 engineer-years per breakthrough architecture",
    "",
    "**Before NAS (Manual Design, Pre-2016):**",
    "- Expert intuition: \"Let's try 5 conv layers \u2192 3 FC layers \u2192 See what happens\"",
    "- Trial & error: Test 100-1000 architectures manually (6-12 months)",
    "- Hyperparameter tuning: Grid search over depth, width, kernel sizes (weeks)",
    "- Result: Suboptimal architectures (human bias, limited search space exploration)",
    "",
    "**After NAS (Automated Design, 2016+):**",
    "- Algorithmic search: Explore 10,000-1,000,000 architectures automatically",
    "- Time: 1-7 days on 100-500 GPUs (vs 6-12 months manual)",
    "- Result: Superhuman architectures (NASNet beats human-designed ResNet)",
    "- Cost: $5K-$50K compute (vs $500K-$2M in researcher salaries)",
    "",
    "**The Breakthrough Moment:**",
    "- **2016:** Google AutoML (Zoph & Le) - First NAS using reinforcement learning",
    "  - NASNet: 82.7% ImageNet accuracy (beats ResNet-50's 76.5%)",
    "  - But: 22,400 GPU-days ($500K+ compute cost) \ud83d\udcb8",
    "- **2017:** ENAS (Efficient NAS) - Parameter sharing reduces cost 1000\u00d7",
    "  - Same accuracy as NASNet, 1000\u00d7 faster (16 GPU-hours vs 22,400 GPU-days)",
    "  - Breakthrough: Reuse weights across architectures (no training from scratch)",
    "- **2018:** DARTS (Differentiable NAS) - Continuous relaxation enables gradient descent",
    "  - Search in 1 GPU-day (vs 22,400 GPU-days)",
    "  - Differentiable: Optimize architecture via backprop (like training weights)",
    "- **2019:** EfficientNet (Compound Scaling + NAS)",
    "  - 84.3% ImageNet accuracy, 8.4\u00d7 smaller, 6.1\u00d7 faster than GPT-2 vision model",
    "  - AutoML + compound scaling (depth + width + resolution)",
    "- **2020-2025:** NAS becomes mainstream",
    "  - Google Cloud AutoML: No-code NAS for non-experts",
    "  - TensorFlow/PyTorch AutoML: Open-source NAS libraries",
    "  - Production: Amazon, Facebook, Netflix use NAS for recommendation systems",
    "",
    "---",
    "",
    "### **\ud83d\udcb0 Business Value: Why NAS Matters to Qualcomm/AMD**",
    "",
    "Neural Architecture Search unlocks **$30M-$80M/year** across multiple semiconductor and AI deployment scenarios:",
    "",
    "#### **Use Case 1: Chip Design Verification AI ($20M-$40M/year)**",
    "**Problem:** Verify 10M+ logic gates per chip design (functional correctness, timing, power)",
    "- Current: ResNet-50 CNN (78% defect detection, 2.2M defects missed/year)",
    "- NAS-discovered architecture: 91% detection (+13%), 900K missed/year",
    "- Training time: Manual (6 months) vs NAS (3 days)",
    "",
    "**Business Impact:**",
    "- Catch 1.3M more defects \u2192 Prevent 50-100 bad tapeouts \u2192 **Save $15M-$30M/year** ($300K/tapeout)",
    "- Time-to-market: 6 months faster design cycles \u2192 **$5M-$10M/year** revenue acceleration",
    "- Architecture reuse: One NAS run \u2192 Deploy to 20 chip families \u2192 **Amortize $50K compute over 20 projects**",
    "",
    "**Implementation:**",
    "```python",
    "# NAS for chip verification (defect detection)",
    "from naszilla import DARTS",
    "from chip_verification_dataset import ChipDefectDataset",
    "",
    "# Define search space (operations: conv, attention, residual, etc.)",
    "search_space = {",
    "    'operations': ['conv3x3', 'conv5x5', 'attention', 'residual', 'dilated_conv'],",
    "    'num_layers': (5, 20),",
    "    'channels': (32, 512)",
    "}",
    "",
    "# Run DARTS (1 GPU-day)",
    "nas = DARTS(search_space, dataset=ChipDefectDataset())",
    "best_architecture = nas.search(epochs=50, gpu_hours=24)",
    "",
    "# Train discovered architecture (2-3 days)",
    "model = best_architecture.build()",
    "model.train(ChipDefectDataset(), epochs=100)",
    "",
    "# Result: 91% detection (vs 78% baseline), $20M-$40M/year value",
    "```",
    "",
    "**Qualcomm Impact:** 20 chip families/year \u00d7 $2M/family = **$40M/year**",
    "",
    "#### **Use Case 2: On-Device AI Optimization ($10M-$20M/year)**",
    "**Problem:** Deploy AI to mobile chips (Snapdragon) with strict constraints",
    "- Latency: <50ms per inference (user experience)",
    "- Memory: <100MB model size (limited RAM)",
    "- Power: <500mW (battery life)",
    "- Accuracy: \u226595% (don't sacrifice quality)",
    "",
    "**Manual Approach:**",
    "- Try MobileNet, EfficientNet, SqueezeNet variations (3-6 months)",
    "- Iterate hyperparameters (depth, width, kernel sizes)",
    "- Result: 93% accuracy, 75ms latency (misses targets)",
    "",
    "**NAS Approach:**",
    "- Define multi-objective search: Optimize accuracy AND latency AND power",
    "- Search space: 100,000 architectures (mobile-optimized operations)",
    "- Result: 96% accuracy, 45ms latency, 400mW power \u2705",
    "",
    "**Business Impact:**",
    "- Superior user experience: 45ms vs 75ms (30% faster) \u2192 **Competitive advantage**",
    "- Longer battery life: 400mW vs 700mW (43% savings) \u2192 **Product differentiation**",
    "- Time-to-market: 3 days vs 6 months \u2192 **Launch 5 months earlier**",
    "- Market share: +2-3% (premium AI features) \u2192 **$10M-$20M/year revenue**",
    "",
    "**AMD Impact (GPU inference optimization):** **$15M-$25M/year** (similar multi-objective NAS for RDNA architectures)",
    "",
    "#### **Use Case 3: Wafer Inspection AutoML ($5M-$15M/year)**",
    "**Problem:** Each fab has unique defect patterns (different equipment, processes)",
    "- Current: One-size-fits-all model (ResNet-50) \u2192 88% recall",
    "- Desired: Custom model per fab \u2192 95%+ recall",
    "",
    "**Manual Customization:**",
    "- Hire ML engineer per fab ($200K/year \u00d7 5 fabs = $1M/year)",
    "- Tune architecture manually (3-6 months per fab)",
    "- Result: 92% recall (marginal improvement)",
    "",
    "**NAS Customization:**",
    "- Run DARTS per fab (1 GPU-day \u00d7 5 fabs = 5 GPU-days = $500 compute)",
    "- Discover optimal architecture for each fab's defect distribution",
    "- Result: 95%+ recall (7% improvement)",
    "",
    "**Business Impact:**",
    "- Better recall: 88% \u2192 95% \u2192 Catch 7K more defects/year \u2192 **Save $5M-$10M/year** ($700/defect)",
    "- Lower cost: $500 compute vs $1M engineers \u2192 **Save $1M/year**",
    "- Faster deployment: 1 day vs 6 months \u2192 **Launch immediately**",
    "",
    "**Intel Impact (15 fabs):** $5M/fab \u00d7 15 = **$75M/year**",
    "",
    "---",
    "",
    "### **\ud83c\udfaf What We'll Build**",
    "",
    "By the end of this notebook, you'll implement 3 NAS algorithms and deploy them to real-world scenarios:",
    "",
    "1. **Reinforcement Learning NAS (Google AutoML, 2016):**",
    "   - Controller RNN generates architectures",
    "   - Train each architecture, reward = validation accuracy",
    "   - Policy gradient optimization (REINFORCE)",
    "   - Result: NASNet architecture (82.7% ImageNet)",
    "",
    "2. **ENAS (Efficient NAS, 2017):**",
    "   - Parameter sharing: One supernet contains all architectures",
    "   - Train supernet weights (shared across architectures)",
    "   - Search with controller RNN (cheap: no retraining)",
    "   - Result: 1000\u00d7 faster than NASNet (16 GPU-hours)",
    "",
    "3. **DARTS (Differentiable NAS, 2018):**",
    "   - Continuous relaxation: Architecture becomes continuous variable \u03b1",
    "   - Bi-level optimization: \u2207_\u03b1 L_val, \u2207_w L_train",
    "   - Gradient descent on architecture parameters",
    "   - Result: 1 GPU-day, 97.0% CIFAR-10 accuracy",
    "",
    "4. **Multi-Objective NAS:**",
    "   - Optimize accuracy + latency + power simultaneously",
    "   - Pareto frontier: Trade-off exploration (95% acc @ 40ms vs 97% acc @ 80ms)",
    "   - Use case: Mobile deployment (Snapdragon), edge AI",
    "",
    "5. **AutoML for Chip Verification:**",
    "   - Custom search space: Conv + attention + residual blocks",
    "   - Domain-specific constraints: Receptive field \u2265 128\u00d7128 (chip layout size)",
    "   - Transfer learning: Pretrain on synthetic data, fine-tune on real chips",
    "   - Result: 91% defect detection (vs 78% baseline), $20M-$40M/year",
    "",
    "---",
    "",
    "### **\ud83d\udcca Learning Roadmap**",
    "",
    "```mermaid",
    "graph TB",
    "    A[Neural Architecture Search] --> B[RL-Based NAS]",
    "    A --> C[One-Shot NAS]",
    "    A --> D[Gradient-Based NAS]",
    "    A --> E[Multi-Objective NAS]",
    "    ",
    "    B --> F[NASNet 2016<br/>22400 GPU-days]",
    "    C --> G[ENAS 2017<br/>16 GPU-hours]",
    "    D --> H[DARTS 2018<br/>1 GPU-day]",
    "    E --> I[Pareto Frontier]",
    "    ",
    "    F --> J[Chip Verification<br/>$20M-$40M/year]",
    "    G --> J",
    "    H --> K[On-Device AI<br/>$10M-$20M/year]",
    "    I --> K",
    "    ",
    "    style A fill:#4A90E2,stroke:#2E5C8A,stroke-width:3px,color:#fff",
    "    style J fill:#7ED321,stroke:#5FA319,stroke-width:2px",
    "    style K fill:#7ED321,stroke:#5FA319,stroke-width:2px",
    "```",
    "",
    "**Learning Path:**",
    "1. **Foundations** (2-3 hours): NAS problem formulation, search space, search strategy, evaluation",
    "2. **RL-Based NAS** (3-4 hours): Controller RNN, policy gradient, NASNet architecture",
    "3. **ENAS** (3-4 hours): Parameter sharing, supernet training, controller search",
    "4. **DARTS** (4-5 hours): Continuous relaxation, bi-level optimization, gradient descent",
    "5. **Multi-Objective** (3-4 hours): Pareto frontier, latency prediction, power modeling",
    "6. **Applications** (5-10 hours): Chip verification, on-device AI, wafer inspection",
    "",
    "**Total Time:** 20-30 hours (4-6 days intensive, or 3-4 weeks part-time)",
    "",
    "---",
    "",
    "### **\ud83c\udf93 Learning Objectives**",
    "",
    "By completing this notebook, you will:",
    "",
    "1. \u2705 **Understand NAS problem formulation:** Search space, search strategy, evaluation strategy",
    "2. \u2705 **Master RL-based NAS:** Controller RNN, policy gradient (REINFORCE), NASNet",
    "3. \u2705 **Implement ENAS:** Parameter sharing, supernet training, 1000\u00d7 speedup vs NASNet",
    "4. \u2705 **Implement DARTS:** Continuous relaxation, bi-level optimization, gradient-based search",
    "5. \u2705 **Multi-objective optimization:** Accuracy + latency + power trade-offs",
    "6. \u2705 **Deploy to chip verification:** 91% defect detection (vs 78% baseline), $20M-$40M/year",
    "7. \u2705 **Optimize for mobile:** 96% accuracy @ 45ms latency (vs 93% @ 75ms), $10M-$20M/year",
    "8. \u2705 **Quantify business value:** ROI analysis, cost-benefit for Qualcomm/AMD/Intel",
    "",
    "---",
    "",
    "### **\ud83d\udd11 Key Concepts Preview**",
    "",
    "Before diving into the algorithms, here's the intuition behind NAS:",
    "",
    "#### **1. The NAS Problem (Three Components)**",
    "```",
    "1. Search Space: What architectures can we explore?",
    "   - Operations: conv3x3, conv5x5, max_pool, attention, residual",
    "   - Connections: How layers connect (skip connections, dense, etc.)",
    "   - Hyperparameters: Depth, width, kernel sizes",
    "",
    "2. Search Strategy: How do we navigate the search space?",
    "   - Reinforcement Learning: Controller RNN learns to generate good architectures",
    "   - Evolutionary: Mutate + crossover architectures, select fittest",
    "   - Gradient-Based: Optimize architecture via backpropagation (DARTS)",
    "",
    "3. Evaluation Strategy: How do we measure architecture quality?",
    "   - Train from scratch: Accurate but slow (days per architecture)",
    "   - Weight sharing: Fast but biased (minutes per architecture)",
    "   - Early stopping: Compromise (hours per architecture)",
    "```",
    "",
    "#### **2. Search Space Example (MobileNet-like)**",
    "```python",
    "search_space = {",
    "    'num_layers': 7,  # Fixed depth",
    "    'layer_i_operation': ['conv3x3', 'conv5x5', 'conv7x7', 'max_pool3x3', 'avg_pool3x3', 'identity'],",
    "    'layer_i_channels': [16, 24, 32, 48, 64, 96, 128],",
    "    'layer_i_kernel_size': [3, 5, 7],",
    "    'layer_i_stride': [1, 2]",
    "}",
    "",
    "# Total architectures: 6^7 \u00d7 7^7 \u00d7 3^7 \u00d7 2^7 \u2248 10^14 (100 trillion!)",
    "# Exploration challenge: Can't try all, need smart search strategy",
    "```",
    "",
    "#### **3. NASNet Architecture (Discovered by RL-NAS)**",
    "```",
    "NASNet-A Cell (Discovered, not hand-designed):",
    "1. Input1 \u2192 SeparableConv5x5 \u2192 Identity \u2192 Add",
    "2. Input2 \u2192 SeparableConv3x3 \u2192 DepthwiseConv3x3 \u2192 Add",
    "3. Result1 + Result2 \u2192 Output",
    "",
    "Why it's good:",
    "- Depthwise separable convolutions: 8-9\u00d7 fewer parameters than standard conv",
    "- Multiple paths: Ensemble effect (like multi-head attention)",
    "- Skip connections: Gradient flow (like ResNet)",
    "",
    "Human intuition: Would NOT have designed this specific combination",
    "NAS discovered: Through 20,000 architecture trials",
    "```",
    "",
    "#### **4. DARTS Key Innovation (Continuous Relaxation)**",
    "```",
    "Discrete (original):",
    "  operation = one_of(['conv3x3', 'conv5x5', 'max_pool'])  # Discrete choice",
    "",
    "Continuous (DARTS):",
    "  output = \u03b11 \u00d7 conv3x3(x) + \u03b12 \u00d7 conv5x5(x) + \u03b13 \u00d7 max_pool(x)",
    "  where \u03b1 = softmax([\u03b11, \u03b12, \u03b13])  # Continuous weights",
    "",
    "Benefit:",
    "- Can compute gradient: \u2202Loss/\u2202\u03b1 (optimize via backprop!)",
    "- Fast search: 1 GPU-day (vs 22,400 GPU-days for discrete RL-NAS)",
    "```",
    "",
    "---",
    "",
    "### **\u2705 Success Criteria**",
    "",
    "You'll know you've mastered NAS when you can:",
    "",
    "- [ ] Explain the NAS problem (search space, strategy, evaluation) in 3 sentences",
    "- [ ] Implement controller RNN for RL-based NAS (<100 lines PyTorch)",
    "- [ ] Train NAS controller with REINFORCE (policy gradient)",
    "- [ ] Implement ENAS with parameter sharing (1000\u00d7 speedup over NASNet)",
    "- [ ] Implement DARTS with continuous relaxation (<200 lines)",
    "- [ ] Run DARTS on CIFAR-10 (97%+ accuracy in 1 GPU-day)",
    "- [ ] Explain bi-level optimization (architecture vs weight updates)",
    "- [ ] Implement multi-objective NAS (accuracy + latency trade-off)",
    "- [ ] Deploy to chip verification (91% defect detection vs 78% baseline)",
    "- [ ] Quantify ROI: $XM-$YM/year for your application",
    "",
    "---",
    "",
    "### **\ud83d\udd70\ufe0f Historical Context: The AutoML Revolution**",
    "",
    "Understanding the timeline helps appreciate why NAS transformed deep learning:",
    "",
    "**2012-2015: Manual Architecture Engineering**",
    "- AlexNet (2012): 8 layers, hand-designed (won ImageNet by 10% margin)",
    "- VGG (2014): 16-19 layers, simple pattern (3\u00d73 conv repeated)",
    "- ResNet (2015): 34-152 layers, skip connections (2 years research at Microsoft)",
    "",
    "**2016: Birth of Neural Architecture Search**",
    "- Zoph & Le (Google Brain): \"Neural Architecture Search with Reinforcement Learning\"",
    "- Controller RNN generates architectures \u2192 Train each \u2192 Reward = accuracy",
    "- NASNet: 82.7% ImageNet (beats ResNet-50's 76.5%) \u2705",
    "- But: 22,400 GPU-days ($500K compute) \u274c",
    "",
    "**2017: Efficiency Breakthrough (ENAS)**",
    "- Pham et al. (Google): \"Efficient Neural Architecture Search via Parameter Sharing\"",
    "- Key insight: Share weights across architectures (no retraining from scratch)",
    "- Result: Same accuracy, **1000\u00d7 faster** (16 GPU-hours vs 22,400 GPU-days)",
    "- Cost: $50 vs $500K (democratized NAS for academia)",
    "",
    "**2018: Gradient-Based NAS (DARTS)**",
    "- Liu et al. (CMU): \"DARTS: Differentiable Architecture Search\"",
    "- Continuous relaxation: Architecture becomes differentiable",
    "- Bi-level optimization: Alternate between architecture and weight updates",
    "- Result: 1 GPU-day, 97.0% CIFAR-10 accuracy",
    "- Impact: NAS accessible to anyone with 1 GPU",
    "",
    "**2019: Production-Scale NAS**",
    "- EfficientNet (Google): Compound scaling + NAS \u2192 84.3% ImageNet, 8.4\u00d7 smaller",
    "- AmoebaNet, MnasNet: Mobile-optimized architectures (latency-aware NAS)",
    "- Google Cloud AutoML: No-code NAS for non-experts ($20/hour)",
    "",
    "**2020-2022: Transformers Meet NAS**",
    "- AutoFormer, DeiT, AutoViT: NAS for Vision Transformers",
    "- HAT (Hardware-Aware Transformers): Optimize for specific hardware (V100 vs A100)",
    "- Result: 85.5% ImageNet with 50% fewer parameters than ViT",
    "",
    "**2023-2025: Foundation Model NAS**",
    "- LLaMA-NAS: Search architecture for 70B parameter models",
    "- Mixture-of-Experts NAS: Optimize routing for MoE models (Mixtral, GPT-4)",
    "- Multi-modal NAS: Joint optimization for vision + language (GPT-4V, Gemini)",
    "",
    "**Key Insight:** NAS went from $500K (2016) \u2192 $50 (2017) \u2192 $20 (2018) \u2192 Mainstream (2025)",
    "",
    "---",
    "",
    "### **\ud83c\udfaf When to Use NAS (Decision Framework)**",
    "",
    "| Scenario | Use NAS? | Alternative | Rationale |",
    "|----------|----------|-------------|-----------|",
    "| **New domain** (chip verification, medical imaging) | \u2705 Yes | Pretrained ResNet-50 | NAS discovers domain-specific patterns |",
    "| **Strict constraints** (latency <50ms, memory <100MB) | \u2705 Yes | Manual architecture tuning | Multi-objective NAS optimizes trade-offs |",
    "| **Large dataset** (1M+ samples) | \u2705 Yes | N/A | NAS needs data to differentiate architectures |",
    "| **Limited compute** (<10 GPU-days) | \u2705 Yes | DARTS, ENAS | Efficient NAS methods (1-16 GPU-days) |",
    "| **Standard task** (ImageNet classification) | \u274c No | EfficientNet, ResNet | Pretrained models already optimal |",
    "| **Small dataset** (<10K samples) | \u274c No | Transfer learning | NAS overfits, transfer better |",
    "| **Interpretability required** | \u274c Maybe | Manual design | NAS architectures less interpretable |",
    "",
    "---",
    "",
    "### **\ud83d\udd2c What Makes NAS Special?**",
    "",
    "Three key properties distinguish NAS from manual architecture design:",
    "",
    "#### **1. Superhuman Performance**",
    "- **Manual:** Human intuition limited by cognitive biases (prefer simple patterns)",
    "- **NAS:** Explores 10,000-1,000,000 architectures, no bias",
    "- **Example:** NASNet uses depthwise separable conv + dilated conv (human wouldn't combine)",
    "- **Result:** 82.7% ImageNet (NASNet) vs 76.5% (ResNet-50, human-designed)",
    "",
    "#### **2. Domain Adaptation**",
    "- **Manual:** One-size-fits-all (ResNet for everything)",
    "- **NAS:** Custom architecture per domain (chip verification vs medical imaging)",
    "- **Example:** Chip verification needs large receptive field (128\u00d7128), medical imaging needs multi-scale",
    "- **Result:** 91% chip defect detection (NAS) vs 78% (ResNet-50)",
    "",
    "#### **3. Multi-Objective Optimization**",
    "- **Manual:** Optimize accuracy, then compress (two-stage, suboptimal)",
    "- **NAS:** Jointly optimize accuracy + latency + power (Pareto frontier)",
    "- **Example:** 96% acc @ 45ms (NAS) vs 93% acc @ 75ms (manual)",
    "- **Result:** Better trade-offs (no post-hoc compression artifacts)",
    "",
    "---",
    "",
    "### **\ud83d\udca1 Intuition: NAS as Architecture Evolution**",
    "",
    "The best analogy for understanding NAS:",
    "",
    "**Biological Evolution:**",
    "```",
    "1. Population: 100 organisms (architectures)",
    "2. Fitness: Survival rate (validation accuracy)",
    "3. Selection: Top 20 organisms reproduce",
    "4. Mutation: Randomly change genes (layers, connections)",
    "5. Crossover: Combine genes from 2 parents",
    "6. Repeat for 50 generations",
    "7. Result: Optimized organism (architecture)",
    "```",
    "",
    "**Neural Architecture Search (Evolutionary):**",
    "```python",
    "population = [random_architecture() for _ in range(100)]",
    "",
    "for generation in range(50):",
    "    # Evaluate fitness (validation accuracy)",
    "    fitness = [train(arch).accuracy for arch in population]",
    "    ",
    "    # Selection (top 20)",
    "    parents = select_top_k(population, fitness, k=20)",
    "    ",
    "    # Mutation + Crossover",
    "    offspring = []",
    "    for _ in range(80):",
    "        parent1, parent2 = random.sample(parents, 2)",
    "        child = crossover(parent1, parent2)",
    "        child = mutate(child, prob=0.1)",
    "        offspring.append(child)",
    "    ",
    "    # New population",
    "    population = parents + offspring",
    "",
    "best_architecture = max(population, key=lambda arch: train(arch).accuracy)",
    "```",
    "",
    "**Why This Works:**",
    "- Exploration: Mutation explores new architectures",
    "- Exploitation: Selection keeps good architectures",
    "- Diversity: Crossover combines strengths from multiple architectures",
    "- Convergence: After 50 generations, population converges to optimal architecture",
    "",
    "---",
    "",
    "### **\ud83c\udfaf This Notebook's Structure**",
    "",
    "**Part 1: NAS Foundations (Cells 1-2)**",
    "- Problem formulation: Search space, strategy, evaluation",
    "- RL-based NAS: Controller RNN, policy gradient (REINFORCE)",
    "- NASNet architecture: What was discovered, why it's good",
    "",
    "**Part 2: Efficient NAS (Cells 3-4)**",
    "- ENAS: Parameter sharing, supernet training, 1000\u00d7 speedup",
    "- Weight sharing bias: Why it works, when it fails",
    "- Controller search: How to sample architectures efficiently",
    "",
    "**Part 3: Differentiable NAS (Cells 5-6)**",
    "- DARTS: Continuous relaxation, bi-level optimization",
    "- Gradient-based search: \u2207_\u03b1 L_val (architecture gradients)",
    "- Discretization: How to convert continuous \u03b1 to final architecture",
    "",
    "**Part 4: Real-World Applications (Cells 7-8)**",
    "- Chip verification: NAS for defect detection (91% vs 78%)",
    "- On-device AI: Multi-objective NAS (accuracy + latency + power)",
    "- Wafer inspection: AutoML per fab (95% vs 88% recall)",
    "- ROI analysis: $30M-$80M/year across Qualcomm/AMD/Intel",
    "",
    "---",
    "",
    "### **\ud83d\ude80 Ready to Begin?**",
    "",
    "You're about to learn the technology that powers:",
    "- Google AutoML (millions of users, $20/hour cloud service)",
    "- EfficientNet (84.3% ImageNet, 8.4\u00d7 smaller than GPT-2 vision)",
    "- Mobile AI (Snapdragon, Apple Neural Engine optimization)",
    "- Chip design verification ($20M-$40M/year defect detection)",
    "",
    "**Business value:** $30M-$80M/year for semiconductor applications (chip verification + on-device AI + wafer inspection)",
    "",
    "**Next:** Dive into NAS problem formulation and RL-based search! \ud83c\udfaf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39275d",
   "metadata": {},
   "source": [
    "# \ud83d\udcd0 Mathematical Foundations & Algorithms\n",
    "\n",
    "## \ud83c\udfaf NAS Problem Formulation\n",
    "\n",
    "Neural Architecture Search optimizes three interconnected components. Understanding each is critical for implementing effective NAS systems.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Search Space (What Architectures Can We Explore?)**\n",
    "\n",
    "The search space defines all possible architectures that NAS can discover. Design too narrow \u2192 miss optimal architecture. Design too broad \u2192 search takes forever.\n",
    "\n",
    "#### **Search Space Dimensions**\n",
    "\n",
    "**Global Search Space (Early NAS, 2016-2017):**\n",
    "```python\n",
    "# Entire network structure is searched\n",
    "search_space = {\n",
    "    'num_layers': (10, 100),          # Total depth\n",
    "    'layer_type': ['conv', 'fc', 'pool'],  # Operations\n",
    "    'layer_connections': 'any',       # How layers connect\n",
    "    'channels': (16, 512),            # Width\n",
    "    'kernel_sizes': [1, 3, 5, 7]      # Receptive field\n",
    "}\n",
    "\n",
    "# Total architectures: Astronomical (10^50+)\n",
    "# Problem: Intractable to search (years of compute)\n",
    "```\n",
    "\n",
    "**Cell-Based Search Space (Modern NAS, 2017+):**\n",
    "```python\n",
    "# Search for repeating \"cell\" (motif), stack to build network\n",
    "class NASCell:\n",
    "    \"\"\"\n",
    "    Cell: Small module (5-7 operations) that repeats\n",
    "    Network: Stack cell 12-20 times\n",
    "    \n",
    "    Benefit: Smaller search space (10^4-10^6 vs 10^50)\n",
    "    Transferability: Cell discovered on CIFAR-10 \u2192 Works on ImageNet\n",
    "    \"\"\"\n",
    "    def __init__(self, num_nodes=7):\n",
    "        self.num_nodes = num_nodes  # 7 intermediate nodes\n",
    "        self.operations = ['conv3x3', 'conv5x5', 'max_pool3x3', \n",
    "                           'avg_pool3x3', 'identity', 'sep_conv3x3', \n",
    "                           'sep_conv5x5', 'dil_conv3x3']  # 8 operations\n",
    "        \n",
    "        # Each node: Choose 2 input nodes + operation for each\n",
    "        # Node i can connect to nodes 0, 1, ..., i-1\n",
    "        # Total: Choose 2 from i predecessors \u00d7 8 operations \u00d7 2\n",
    "        \n",
    "    def search_space_size(self):\n",
    "        \"\"\"\n",
    "        Node 2: C(2,2) \u00d7 8^2 = 1 \u00d7 64 = 64\n",
    "        Node 3: C(3,2) \u00d7 8^2 = 3 \u00d7 64 = 192\n",
    "        ...\n",
    "        Node 7: C(7,2) \u00d7 8^2 = 21 \u00d7 64 = 1,344\n",
    "        \n",
    "        Total: ~10^6 architectures (searchable!)\n",
    "        \"\"\"\n",
    "        total = 1\n",
    "        for i in range(2, self.num_nodes + 2):\n",
    "            total *= comb(i, 2) * len(self.operations)**2\n",
    "        return total\n",
    "\n",
    "# Example: NASNet search space = 7 nodes \u2192 ~10^6 architectures\n",
    "```\n",
    "\n",
    "**Why Cell-Based Works:**\n",
    "- **Reduced complexity:** 10^6 vs 10^50 (searchable in days vs years)\n",
    "- **Transferability:** CIFAR-10 cell \u2192 ImageNet cell (same structure, different scale)\n",
    "- **Modularity:** Swap cells for different tasks (classification vs segmentation)\n",
    "\n",
    "#### **Operations in Search Space**\n",
    "\n",
    "**Standard Operations (Most NAS systems):**\n",
    "```python\n",
    "operations = {\n",
    "    # Convolutions\n",
    "    'conv3x3': lambda C_in, C_out: nn.Conv2d(C_in, C_out, 3, padding=1),\n",
    "    'conv5x5': lambda C_in, C_out: nn.Conv2d(C_in, C_out, 5, padding=2),\n",
    "    'conv7x7': lambda C_in, C_out: nn.Conv2d(C_in, C_out, 7, padding=3),\n",
    "    \n",
    "    # Separable convolutions (MobileNet-style, 8-9\u00d7 fewer params)\n",
    "    'sep_conv3x3': lambda C_in, C_out: SeparableConv2d(C_in, C_out, 3),\n",
    "    'sep_conv5x5': lambda C_in, C_out: SeparableConv2d(C_in, C_out, 5),\n",
    "    \n",
    "    # Dilated convolutions (larger receptive field, same compute)\n",
    "    'dil_conv3x3': lambda C_in, C_out: nn.Conv2d(C_in, C_out, 3, dilation=2, padding=2),\n",
    "    'dil_conv5x5': lambda C_in, C_out: nn.Conv2d(C_in, C_out, 5, dilation=2, padding=4),\n",
    "    \n",
    "    # Pooling\n",
    "    'max_pool3x3': lambda C_in, C_out: nn.MaxPool2d(3, stride=1, padding=1),\n",
    "    'avg_pool3x3': lambda C_in, C_out: nn.AvgPool2d(3, stride=1, padding=1),\n",
    "    \n",
    "    # Special\n",
    "    'identity': lambda C_in, C_out: Identity() if C_in == C_out else FactorizedReduce(C_in, C_out),\n",
    "    'zero': lambda C_in, C_out: Zero()  # No connection (pruning)\n",
    "}\n",
    "```\n",
    "\n",
    "**Domain-Specific Operations (Chip Verification Example):**\n",
    "```python\n",
    "# Custom operations for chip layout analysis\n",
    "chip_operations = {\n",
    "    'conv3x3': Standard2DConv,\n",
    "    'conv5x5': Standard2DConv,\n",
    "    \n",
    "    # Attention: Capture long-range dependencies (critical for chip layout)\n",
    "    'spatial_attention': lambda C: SpatialAttention(C),\n",
    "    'channel_attention': lambda C: ChannelAttention(C),\n",
    "    \n",
    "    # Graph convolutions: Model circuit connectivity\n",
    "    'graph_conv': lambda C: GraphConv(C),\n",
    "    \n",
    "    # Multi-scale: Detect defects at multiple resolutions\n",
    "    'multi_scale': lambda C: MultiScaleFusion([C, 2*C, 4*C]),\n",
    "    \n",
    "    # Residual: Gradient flow for deep networks\n",
    "    'residual': lambda C: ResidualBlock(C)\n",
    "}\n",
    "```\n",
    "\n",
    "#### **Constraints in Search Space**\n",
    "\n",
    "Real-world NAS requires constraints (can't search ALL architectures):\n",
    "\n",
    "**Computational Constraints:**\n",
    "```python\n",
    "def check_latency_constraint(architecture, max_latency_ms=50):\n",
    "    \"\"\"\n",
    "    Mobile deployment: Must run in <50ms\n",
    "    \"\"\"\n",
    "    latency = measure_latency(architecture, input_size=(1, 3, 224, 224))\n",
    "    return latency <= max_latency_ms\n",
    "\n",
    "def check_memory_constraint(architecture, max_memory_mb=100):\n",
    "    \"\"\"\n",
    "    Mobile deployment: Model size <100MB\n",
    "    \"\"\"\n",
    "    memory = sum(p.numel() * 4 for p in architecture.parameters()) / 1e6  # MB\n",
    "    return memory <= max_memory_mb\n",
    "\n",
    "def check_power_constraint(architecture, max_power_mw=500):\n",
    "    \"\"\"\n",
    "    Battery life: Power consumption <500mW\n",
    "    \"\"\"\n",
    "    power = estimate_power(architecture)  # Platform-specific model\n",
    "    return power <= max_power_mw\n",
    "```\n",
    "\n",
    "**Domain Constraints (Chip Verification):**\n",
    "```python\n",
    "def check_receptive_field_constraint(architecture, min_rf=128):\n",
    "    \"\"\"\n",
    "    Chip layouts are 128\u00d7128 pixels minimum\n",
    "    Architecture must have receptive field \u2265128\u00d7128\n",
    "    \"\"\"\n",
    "    receptive_field = compute_receptive_field(architecture)\n",
    "    return receptive_field >= min_rf\n",
    "\n",
    "def check_rotation_invariance(architecture):\n",
    "    \"\"\"\n",
    "    Chip defects can appear at any rotation\n",
    "    Architecture should be rotation-invariant (or use data augmentation)\n",
    "    \"\"\"\n",
    "    # Test: Feed rotated image, check if features are rotation-equivariant\n",
    "    return test_rotation_equivariance(architecture)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Search Strategy (How to Navigate Search Space?)**\n",
    "\n",
    "Given 10^6 possible architectures, how do we find the best one without trying all?\n",
    "\n",
    "#### **Reinforcement Learning Search (NASNet, 2016)**\n",
    "\n",
    "**Intuition:** Train a controller to generate architectures, reward = validation accuracy.\n",
    "\n",
    "**Algorithm:**\n",
    "```\n",
    "1. Controller RNN generates architecture A (sequence of operations)\n",
    "2. Train A from scratch for N epochs\n",
    "3. Measure accuracy: acc_val(A)\n",
    "4. Reward: R = acc_val(A) - baseline\n",
    "5. Update controller with policy gradient: \u2207_\u03b8 E[R]\n",
    "6. Repeat for 20,000 iterations\n",
    "```\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "**Controller:** RNN that outputs architecture tokens\n",
    "```\n",
    "Hidden state: h_t = LSTM(h_{t-1}, action_{t-1})\n",
    "Action distribution: \u03c0(action_t | h_t) = softmax(W_h h_t + b)\n",
    "\n",
    "Architecture A = [action_1, action_2, ..., action_T]\n",
    "Example: ['conv3x3', 'node1\u2192node2', 'sep_conv5x5', 'node0\u2192node3', ...]\n",
    "```\n",
    "\n",
    "**Policy Gradient (REINFORCE):**\n",
    "```\n",
    "Objective: Maximize expected reward J(\u03b8) = E_{A~\u03c0_\u03b8}[R(A)]\n",
    "\n",
    "Gradient:\n",
    "\u2207_\u03b8 J(\u03b8) = E_{A~\u03c0_\u03b8}[\u2207_\u03b8 log \u03c0_\u03b8(A) \u00b7 R(A)]\n",
    "         \u2248 1/B \u03a3_{i=1}^B \u2207_\u03b8 log \u03c0_\u03b8(A_i) \u00b7 (R(A_i) - baseline)\n",
    "\n",
    "where:\n",
    "- B = batch size (100 architectures per iteration)\n",
    "- R(A_i) = validation accuracy of architecture A_i\n",
    "- baseline = moving average of rewards (reduces variance)\n",
    "\n",
    "Update rule:\n",
    "\u03b8 \u2190 \u03b8 + \u03b1 \u00b7 \u2207_\u03b8 J(\u03b8)\n",
    "```\n",
    "\n",
    "**Why Baseline Matters:**\n",
    "```python\n",
    "# Without baseline (high variance)\n",
    "R = [0.85, 0.87, 0.86, 0.88]  # Validation accuracies\n",
    "gradients = [log_prob(A) * R for A, R in zip(architectures, R)]\n",
    "# Problem: All R > 0 \u2192 All gradients same sign \u2192 Poor differentiation\n",
    "\n",
    "# With baseline (lower variance)\n",
    "baseline = 0.865  # Moving average\n",
    "R_centered = [0.85 - 0.865, 0.87 - 0.865, 0.86 - 0.865, 0.88 - 0.865]\n",
    "            = [-0.015, +0.005, -0.005, +0.015]\n",
    "gradients = [log_prob(A) * (R - baseline) for A, R in zip(architectures, R_centered)]\n",
    "# Now: Positive R \u2192 Increase prob, Negative R \u2192 Decrease prob \u2705\n",
    "```\n",
    "\n",
    "**NASNet Results:**\n",
    "- **Search cost:** 22,400 GPU-days (450 GPUs \u00d7 50 days)\n",
    "- **Architectures tried:** ~20,000\n",
    "- **Best architecture:** NASNet-A (82.7% ImageNet, beats ResNet-50's 76.5%)\n",
    "- **Cost:** $450K compute (450 \u00d7 $1/GPU-day \u00d7 50 days + electricity)\n",
    "\n",
    "**Why So Expensive?**\n",
    "```python\n",
    "# Each architecture trial:\n",
    "1. Sample architecture from controller: 1 second\n",
    "2. Train architecture from scratch: 4 GPU-days (CIFAR-10) to 50 GPU-days (ImageNet)\n",
    "3. Evaluate on validation set: 1 hour\n",
    "4. Total per trial: ~50 GPU-days\n",
    "\n",
    "# Total cost:\n",
    "20,000 trials \u00d7 50 GPU-days = 1,000,000 GPU-days\n",
    "# But: Parallelization on 450 GPUs \u2192 1,000,000 / 450 \u2248 2,222 days... wait, paper says 50 days?\n",
    "\n",
    "# Trick: Early stopping + proxy dataset\n",
    "- Train on CIFAR-10 (4 GPU-days) instead of ImageNet (50 GPU-days)\n",
    "- Early stopping: Stop at 20 epochs instead of 600 (12\u00d7 speedup)\n",
    "- Final cost: 20,000 \u00d7 4 / 12 \u00f7 450 \u2248 50 days \u2705\n",
    "```\n",
    "\n",
    "#### **Evolutionary Search (AmoebaNet, 2018)**\n",
    "\n",
    "**Intuition:** Mimic biological evolution - mutate architectures, select fittest.\n",
    "\n",
    "**Algorithm:**\n",
    "```python\n",
    "def evolutionary_search(population_size=100, generations=50):\n",
    "    # Initialize population with random architectures\n",
    "    population = [random_architecture() for _ in range(population_size)]\n",
    "    \n",
    "    for gen in range(generations):\n",
    "        # Evaluate fitness (validation accuracy)\n",
    "        fitness = [train_and_eval(arch) for arch in population]\n",
    "        \n",
    "        # Selection: Keep top 20%\n",
    "        top_k = int(0.2 * population_size)\n",
    "        parents = [population[i] for i in np.argsort(fitness)[-top_k:]]\n",
    "        \n",
    "        # Mutation + Crossover\n",
    "        offspring = []\n",
    "        for _ in range(population_size - top_k):\n",
    "            if random.random() < 0.5:\n",
    "                # Mutation: Randomly change one operation\n",
    "                parent = random.choice(parents)\n",
    "                child = mutate(parent, prob=0.1)\n",
    "            else:\n",
    "                # Crossover: Combine two parents\n",
    "                parent1, parent2 = random.sample(parents, 2)\n",
    "                child = crossover(parent1, parent2)\n",
    "            offspring.append(child)\n",
    "        \n",
    "        # New generation\n",
    "        population = parents + offspring\n",
    "    \n",
    "    # Return best architecture\n",
    "    return max(population, key=lambda arch: train_and_eval(arch))\n",
    "\n",
    "def mutate(architecture, prob=0.1):\n",
    "    \"\"\"\n",
    "    Randomly change operations with probability `prob`\n",
    "    \"\"\"\n",
    "    new_arch = copy.deepcopy(architecture)\n",
    "    for i in range(len(new_arch.operations)):\n",
    "        if random.random() < prob:\n",
    "            new_arch.operations[i] = random.choice(OPERATIONS)\n",
    "    return new_arch\n",
    "\n",
    "def crossover(arch1, arch2):\n",
    "    \"\"\"\n",
    "    Single-point crossover: Split at random point, combine halves\n",
    "    \"\"\"\n",
    "    split = random.randint(1, len(arch1.operations) - 1)\n",
    "    child = copy.deepcopy(arch1)\n",
    "    child.operations[split:] = arch2.operations[split:]\n",
    "    return child\n",
    "```\n",
    "\n",
    "**AmoebaNet Results:**\n",
    "- **Search cost:** 3,150 GPU-days (75 GPUs \u00d7 42 days)\n",
    "- **Architectures tried:** 5,000 (vs 20,000 for NASNet)\n",
    "- **Best architecture:** AmoebaNet-A (82.8% ImageNet, slightly better than NASNet)\n",
    "- **Benefit:** More diverse architectures (mutation explores radical changes)\n",
    "\n",
    "#### **Gradient-Based Search (DARTS, 2018)**\n",
    "\n",
    "**Key Innovation:** Make architecture continuous \u2192 Optimize via gradient descent!\n",
    "\n",
    "**Discrete \u2192 Continuous Relaxation:**\n",
    "```python\n",
    "# Discrete (original): Choose ONE operation\n",
    "operation = one_of(['conv3x3', 'conv5x5', 'max_pool'])  # Categorical choice\n",
    "\n",
    "# Continuous (DARTS): Weighted sum over ALL operations\n",
    "output = \u03a3_i softmax(\u03b1_i) \u00d7 operation_i(x)\n",
    "       = (e^\u03b11 / \u03a3e^\u03b1) \u00d7 conv3x3(x) + (e^\u03b12 / \u03a3e^\u03b1) \u00d7 conv5x5(x) + (e^\u03b13 / \u03a3e^\u03b1) \u00d7 max_pool(x)\n",
    "\n",
    "where \u03b1 = [\u03b11, \u03b12, \u03b13] are continuous architecture parameters\n",
    "```\n",
    "\n",
    "**Bi-Level Optimization:**\n",
    "\n",
    "DARTS optimizes TWO sets of parameters:\n",
    "1. **Architecture parameters \u03b1:** Which operations to use\n",
    "2. **Network weights w:** How to perform operations\n",
    "\n",
    "**Objective:**\n",
    "```\n",
    "min_\u03b1  L_val(w*(\u03b1), \u03b1)\n",
    "s.t.   w*(\u03b1) = argmin_w L_train(w, \u03b1)\n",
    "\n",
    "In words:\n",
    "- Inner optimization: Train weights w to minimize training loss (given architecture \u03b1)\n",
    "- Outer optimization: Adjust architecture \u03b1 to minimize validation loss (given optimal weights w*)\n",
    "```\n",
    "\n",
    "**Algorithm (Simplified):**\n",
    "```python\n",
    "# Initialize\n",
    "\u03b1 = random_init()  # Architecture parameters\n",
    "w = random_init()  # Network weights\n",
    "\n",
    "for epoch in range(50):\n",
    "    # Phase 1: Update w (inner optimization)\n",
    "    for batch in train_loader:\n",
    "        loss_train = compute_loss(batch, w, \u03b1)\n",
    "        w = w - lr_w \u00d7 \u2207_w loss_train\n",
    "    \n",
    "    # Phase 2: Update \u03b1 (outer optimization)\n",
    "    for batch in val_loader:\n",
    "        loss_val = compute_loss(batch, w, \u03b1)\n",
    "        \u03b1 = \u03b1 - lr_\u03b1 \u00d7 \u2207_\u03b1 loss_val\n",
    "\n",
    "# Derive discrete architecture\n",
    "final_architecture = discretize(\u03b1)  # Keep top-2 operations per edge\n",
    "```\n",
    "\n",
    "**Why This Works:**\n",
    "```\n",
    "Intuition: As training progresses, softmax(\u03b1) concentrates on good operations\n",
    "- Initially: \u03b1 = [0.1, 0.05, -0.03] \u2192 softmax = [0.37, 0.35, 0.28] (uniform)\n",
    "- After training: \u03b1 = [2.5, 0.3, -1.8] \u2192 softmax = [0.85, 0.13, 0.02] (peaked)\n",
    "- Interpretation: conv3x3 is best (\u03b11 = 2.5), max_pool is worst (\u03b13 = -1.8)\n",
    "```\n",
    "\n",
    "**DARTS Results:**\n",
    "- **Search cost:** 1 GPU-day (4 GPUs \u00d7 6 hours)\n",
    "- **Speedup vs NASNet:** 22,400 GPU-days \u2192 1 GPU-day = **22,400\u00d7 faster** \ud83d\ude80\n",
    "- **Accuracy:** 97.0% CIFAR-10 (comparable to NASNet's 97.4%)\n",
    "- **Cost:** $24 (4 GPUs \u00d7 6 hours \u00d7 $1/GPU-hour)\n",
    "\n",
    "#### **Why DARTS is Revolutionary:**\n",
    "\n",
    "**Comparison Table:**\n",
    "\n",
    "| Method | Search Space | Search Strategy | Cost | Accuracy (CIFAR-10) |\n",
    "|--------|--------------|-----------------|------|---------------------|\n",
    "| NASNet (RL) | 10^6 cells | Reinforcement learning | 22,400 GPU-days | 97.4% |\n",
    "| AmoebaNet (Evolutionary) | 10^6 cells | Evolution (mutation, crossover) | 3,150 GPU-days | 97.5% |\n",
    "| ENAS (Weight sharing) | 10^6 cells | RL + weight sharing | 0.67 GPU-days | 97.3% |\n",
    "| **DARTS (Gradient)** | **10^6 cells** | **Continuous relaxation + gradient descent** | **1 GPU-day** | **97.0%** |\n",
    "\n",
    "**Key Insight:** Gradient descent is 22,400\u00d7 faster than RL for NAS! (Same insight as deep learning revolution: gradient descent > genetic algorithms)\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Evaluation Strategy (How to Measure Architecture Quality?)**\n",
    "\n",
    "Given an architecture, how do we measure its quality without spending 50 GPU-days training?\n",
    "\n",
    "#### **Strategy 1: Train from Scratch (Accurate but Slow)**\n",
    "\n",
    "```python\n",
    "def evaluate_architecture(architecture, dataset='CIFAR-10', epochs=600):\n",
    "    \"\"\"\n",
    "    Gold standard: Train to convergence\n",
    "    \n",
    "    Cost: 4-50 GPU-days per architecture (CIFAR-10 to ImageNet)\n",
    "    Accuracy: Perfect (no bias)\n",
    "    Use case: Final evaluation only (not during search)\n",
    "    \"\"\"\n",
    "    model = build_model(architecture)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer)\n",
    "        val_acc = evaluate(model, val_loader)\n",
    "    \n",
    "    return val_acc  # Final validation accuracy after 600 epochs\n",
    "```\n",
    "\n",
    "**Problem:** 20,000 architectures \u00d7 4 GPU-days = 80,000 GPU-days (infeasible)\n",
    "\n",
    "#### **Strategy 2: Early Stopping (Fast but Noisy)**\n",
    "\n",
    "```python\n",
    "def evaluate_architecture_early_stopping(architecture, epochs=20):\n",
    "    \"\"\"\n",
    "    Stop training early, use validation accuracy as proxy\n",
    "    \n",
    "    Cost: 0.1-0.5 GPU-days per architecture\n",
    "    Accuracy: Noisy (architectures that converge slowly are penalized)\n",
    "    Use case: RL-based NAS (NASNet), Evolutionary (AmoebaNet)\n",
    "    \"\"\"\n",
    "    model = build_model(architecture)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "    \n",
    "    for epoch in range(epochs):  # Only 20 epochs instead of 600\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer)\n",
    "    \n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    return val_acc\n",
    "\n",
    "# Problem: Correlation between 20-epoch accuracy and 600-epoch accuracy is ~0.7\n",
    "# Some architectures (e.g., ResNet) start slow, converge high \u2192 Underestimated by early stopping\n",
    "```\n",
    "\n",
    "#### **Strategy 3: Weight Sharing (Fast but Biased - ENAS)**\n",
    "\n",
    "**Key Insight:** All architectures share the same weights (supernet) \u2192 No training per architecture!\n",
    "\n",
    "```python\n",
    "class Supernet(nn.Module):\n",
    "    \"\"\"\n",
    "    Supernet: Contains ALL possible operations\n",
    "    Each architecture = Subset of supernet\n",
    "    \"\"\"\n",
    "    def __init__(self, num_nodes=7, num_ops=8):\n",
    "        super().__init__()\n",
    "        # Create ALL operations (shared across architectures)\n",
    "        self.ops = nn.ModuleList([\n",
    "            nn.ModuleList([Operation(op) for op in OPERATIONS])\n",
    "            for _ in range(num_nodes)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, architecture):\n",
    "        \"\"\"\n",
    "        Forward pass for a specific architecture\n",
    "        architecture: List of operation indices\n",
    "        \"\"\"\n",
    "        for node_idx, op_idx in enumerate(architecture):\n",
    "            x = self.ops[node_idx][op_idx](x)\n",
    "        return x\n",
    "\n",
    "# Training: Sample architectures, update supernet weights\n",
    "supernet = Supernet()\n",
    "optimizer = torch.optim.SGD(supernet.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for batch in train_loader:\n",
    "        # Sample random architecture\n",
    "        architecture = [random.randint(0, 7) for _ in range(7)]\n",
    "        \n",
    "        # Forward pass with this architecture\n",
    "        output = supernet(batch, architecture)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # Backward pass (update supernet weights)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation: Instantiate architecture, use supernet weights (NO training!)\n",
    "def evaluate_architecture_weight_sharing(supernet, architecture):\n",
    "    \"\"\"\n",
    "    Cost: 0 GPU-days (just forward pass!)\n",
    "    Accuracy: Biased (supernet weights optimized for ALL architectures, not this specific one)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        val_acc = evaluate(supernet, val_loader, architecture)\n",
    "    return val_acc\n",
    "```\n",
    "\n",
    "**Why Weight Sharing Works:**\n",
    "- **Speed:** 0.67 GPU-days total (train supernet once, evaluate 20,000 architectures instantly)\n",
    "- **Correlation:** Accuracy ranking is preserved (~0.8 correlation with train-from-scratch)\n",
    "\n",
    "**Why Weight Sharing Fails:**\n",
    "- **Bias:** Supernet weights are a compromise (good for average architecture, suboptimal for specific one)\n",
    "- **Example:** Architecture A (95% acc if trained from scratch) might get 93% with shared weights\n",
    "- **Impact:** May miss best architecture if ranking is incorrect\n",
    "\n",
    "#### **Strategy 4: Performance Prediction (Data-Efficient)**\n",
    "\n",
    "**Intuition:** Train a predictor that maps architecture \u2192 accuracy (skip training entirely!)\n",
    "\n",
    "```python\n",
    "# Step 1: Train predictor on 500 architectures\n",
    "predictor_train_data = []\n",
    "for _ in range(500):\n",
    "    arch = random_architecture()\n",
    "    acc = train_from_scratch(arch, epochs=20)  # Expensive: 500 \u00d7 0.1 GPU-days = 50 GPU-days\n",
    "    predictor_train_data.append((arch, acc))\n",
    "\n",
    "# Step 2: Train neural network predictor\n",
    "predictor = AccuracyPredictor()  # Maps architecture encoding \u2192 accuracy\n",
    "predictor.train(predictor_train_data)\n",
    "\n",
    "# Step 3: Evaluate 10,000 architectures using predictor (FREE!)\n",
    "predicted_accuracies = []\n",
    "for _ in range(10000):\n",
    "    arch = random_architecture()\n",
    "    pred_acc = predictor(arch)  # Instant! No training\n",
    "    predicted_accuracies.append((arch, pred_acc))\n",
    "\n",
    "# Step 4: Select top 10, train from scratch to verify\n",
    "top_10 = sorted(predicted_accuracies, key=lambda x: x[1], reverse=True)[:10]\n",
    "final_accuracies = [train_from_scratch(arch, epochs=600) for arch, _ in top_10]\n",
    "```\n",
    "\n",
    "**Cost Analysis:**\n",
    "- Train predictor: 50 GPU-days (one-time)\n",
    "- Evaluate 10,000 architectures: 0 GPU-days (predictor inference)\n",
    "- Final verification: 10 \u00d7 4 GPU-days = 40 GPU-days\n",
    "- **Total: 90 GPU-days** (vs 40,000 GPU-days if training all from scratch)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Algorithm Deep Dive\n",
    "\n",
    "Now that we understand the components, let's dive into the three major NAS algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "### **Algorithm 1: NASNet (RL-Based NAS, 2016)**\n",
    "\n",
    "**Paper:** \"Neural Architecture Search with Reinforcement Learning\" (Zoph & Le, Google Brain)\n",
    "\n",
    "#### **Architecture Encoding**\n",
    "\n",
    "NASNet searches for a cell (motif) that repeats in the network.\n",
    "\n",
    "**Cell Structure:**\n",
    "```\n",
    "Cell has 7 nodes (5 intermediate + 2 input nodes)\n",
    "Each node i (i=2..7):\n",
    "  1. Select 2 input nodes: from {0, 1, ..., i-1}\n",
    "  2. Select operation for each input: from {conv3x3, conv5x5, max_pool, ...}\n",
    "  3. Combine: output_i = operation1(input1) + operation2(input2)\n",
    "\n",
    "Controller RNN generates:\n",
    "[input1_node2, op1_node2, input2_node2, op2_node2,   # Node 2\n",
    " input1_node3, op1_node3, input2_node3, op2_node3,   # Node 3\n",
    " ...\n",
    " input1_node7, op1_node7, input2_node7, op2_node7]   # Node 7\n",
    "\n",
    "Total: 24 decisions (4 per node \u00d7 6 nodes)\n",
    "```\n",
    "\n",
    "**Example Architecture:**\n",
    "```python\n",
    "# NASNet-A cell (discovered by RL-NAS)\n",
    "node2: input=0 (prev cell), op=sep_conv5x5 | input=1 (prev-prev cell), op=identity\n",
    "node3: input=0, op=sep_conv5x5 | input=2, op=sep_conv3x3\n",
    "node4: input=0, op=avg_pool3x3 | input=2, op=identity\n",
    "node5: input=0, op=sep_conv3x3 | input=3, op=avg_pool3x3\n",
    "node6: input=2, op=max_pool3x3 | input=5, op=sep_conv5x5\n",
    "node7: input=0, op=avg_pool3x3 | input=4, op=max_pool3x3\n",
    "\n",
    "# Final cell output: Concatenate outputs of nodes 2, 3, 4, 5, 6, 7\n",
    "```\n",
    "\n",
    "#### **Controller RNN**\n",
    "\n",
    "```python\n",
    "class ControllerRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN that generates architecture decisions\n",
    "    \"\"\"\n",
    "    def __init__(self, num_operations=8, num_nodes=7, hidden_size=100):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTMCell(hidden_size, hidden_size)\n",
    "        \n",
    "        # Embedding for previous decisions\n",
    "        self.embedding = nn.Embedding(num_operations + num_nodes, hidden_size)\n",
    "        \n",
    "        # Output heads\n",
    "        self.input_selector = nn.Linear(hidden_size, num_nodes)  # Which input node?\n",
    "        self.operation_selector = nn.Linear(hidden_size, num_operations)  # Which operation?\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Generate architecture by sampling from distributions\n",
    "        \"\"\"\n",
    "        h, c = self.init_hidden()\n",
    "        architecture = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for node_idx in range(2, 2 + self.num_nodes):\n",
    "            # Generate 2 inputs + 2 operations for this node\n",
    "            for _ in range(2):\n",
    "                # Select input node\n",
    "                h, c = self.lstm(h, c)\n",
    "                input_logits = self.input_selector(h)\n",
    "                input_logits = input_logits[:node_idx]  # Can only select from previous nodes\n",
    "                input_dist = Categorical(logits=input_logits)\n",
    "                input_node = input_dist.sample()\n",
    "                architecture.append(input_node.item())\n",
    "                log_probs.append(input_dist.log_prob(input_node))\n",
    "                \n",
    "                # Select operation\n",
    "                h_input = self.embedding(input_node)  # Embed previous decision\n",
    "                h, c = self.lstm(h_input, c)\n",
    "                op_logits = self.operation_selector(h)\n",
    "                op_dist = Categorical(logits=op_logits)\n",
    "                operation = op_dist.sample()\n",
    "                architecture.append(operation.item())\n",
    "                log_probs.append(op_dist.log_prob(operation))\n",
    "        \n",
    "        return architecture, torch.stack(log_probs)\n",
    "```\n",
    "\n",
    "#### **Training Algorithm (REINFORCE)**\n",
    "\n",
    "```python\n",
    "def train_nas_controller(controller, num_iterations=20000, batch_size=100):\n",
    "    \"\"\"\n",
    "    Train controller with policy gradient (REINFORCE)\n",
    "    \"\"\"\n",
    "    baseline = None  # Moving average of rewards\n",
    "    optimizer = torch.optim.Adam(controller.parameters(), lr=0.001)\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Sample batch of architectures\n",
    "        architectures = []\n",
    "        log_probs_batch = []\n",
    "        rewards = []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            # Generate architecture\n",
    "            architecture, log_probs = controller()\n",
    "            architectures.append(architecture)\n",
    "            log_probs_batch.append(log_probs)\n",
    "            \n",
    "            # Evaluate architecture (train from scratch)\n",
    "            child_model = build_model_from_architecture(architecture)\n",
    "            accuracy = train_and_evaluate(child_model, epochs=20)  # Early stopping\n",
    "            \n",
    "            rewards.append(accuracy)\n",
    "        \n",
    "        # Compute baseline (exponential moving average)\n",
    "        if baseline is None:\n",
    "            baseline = np.mean(rewards)\n",
    "        else:\n",
    "            baseline = 0.9 * baseline + 0.1 * np.mean(rewards)\n",
    "        \n",
    "        # Policy gradient update\n",
    "        policy_loss = 0\n",
    "        for log_probs, reward in zip(log_probs_batch, rewards):\n",
    "            # Advantage: How much better than average?\n",
    "            advantage = reward - baseline\n",
    "            \n",
    "            # Policy gradient: Increase prob of good architectures, decrease prob of bad\n",
    "            policy_loss -= (log_probs.sum() * advantage)\n",
    "        \n",
    "        policy_loss /= batch_size\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iteration {iteration}, Avg Reward: {np.mean(rewards):.4f}, Baseline: {baseline:.4f}\")\n",
    "    \n",
    "    # After 20,000 iterations, sample best architecture\n",
    "    best_architecture, _ = controller()\n",
    "    return best_architecture\n",
    "```\n",
    "\n",
    "**Why REINFORCE Works:**\n",
    "```\n",
    "Policy gradient theorem:\n",
    "\u2207_\u03b8 J(\u03b8) = E_{A~\u03c0_\u03b8}[\u2207_\u03b8 log \u03c0_\u03b8(A) \u00b7 (R(A) - baseline)]\n",
    "\n",
    "Interpretation:\n",
    "- If R(A) > baseline (better than average): Increase prob of A (positive gradient)\n",
    "- If R(A) < baseline (worse than average): Decrease prob of A (negative gradient)\n",
    "- Magnitude: Proportional to advantage (R - baseline)\n",
    "\n",
    "Example:\n",
    "Architecture A: 95% accuracy, baseline: 90%\n",
    "\u2192 Advantage = +5% \u2192 INCREASE prob of A\n",
    "\n",
    "Architecture B: 85% accuracy, baseline: 90%\n",
    "\u2192 Advantage = -5% \u2192 DECREASE prob of B\n",
    "```\n",
    "\n",
    "**NASNet Results:**\n",
    "- **Best cell:** NASNet-A (see architecture above)\n",
    "- **ImageNet accuracy:** 82.7% (vs 76.5% for ResNet-50, +6.2%)\n",
    "- **Parameters:** 88M (vs 25M for ResNet-50, larger but more accurate)\n",
    "- **Search cost:** 22,400 GPU-days (2017 prices: $450K)\n",
    "\n",
    "---\n",
    "\n",
    "### **Algorithm 2: ENAS (Efficient NAS, 2017)**\n",
    "\n",
    "**Paper:** \"Efficient Neural Architecture Search via Parameter Sharing\" (Pham et al., Google Brain)\n",
    "\n",
    "**Key Innovation:** Share weights across ALL architectures \u2192 No retraining per architecture \u2192 1000\u00d7 speedup\n",
    "\n",
    "#### **Supernet (One Network to Rule Them All)**\n",
    "\n",
    "```python\n",
    "class ENASSupernet(nn.Module):\n",
    "    \"\"\"\n",
    "    Supernet contains ALL possible operations\n",
    "    Each architecture = Path through supernet\n",
    "    \"\"\"\n",
    "    def __init__(self, num_nodes=7, operations=['conv3x3', 'conv5x5', 'max_pool', 'avg_pool', 'identity']):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_ops = len(operations)\n",
    "        \n",
    "        # Create all operations (shared across architectures)\n",
    "        self.ops = nn.ModuleList([\n",
    "            nn.ModuleList([self._create_operation(op, channels=64) for op in operations])\n",
    "            for _ in range(num_nodes)\n",
    "        ])\n",
    "    \n",
    "    def _create_operation(self, op_name, channels):\n",
    "        if op_name == 'conv3x3':\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(channels, channels, 3, padding=1),\n",
    "                nn.BatchNorm2d(channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        elif op_name == 'conv5x5':\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(channels, channels, 5, padding=2),\n",
    "                nn.BatchNorm2d(channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        # ... other operations\n",
    "    \n",
    "    def forward(self, x, architecture):\n",
    "        \"\"\"\n",
    "        Forward pass for a specific architecture\n",
    "        \n",
    "        architecture: List of (input_node, operation_idx) tuples\n",
    "        Example: [(0, 2), (1, 3), (0, 1), ...]\n",
    "                  Node 2 takes input from node 0 with operation 2\n",
    "                  Node 3 takes input from node 1 with operation 3\n",
    "        \"\"\"\n",
    "        nodes = [x, x]  # Initial inputs (from previous cells)\n",
    "        \n",
    "        for node_idx in range(2, 2 + self.num_nodes):\n",
    "            # Get inputs and operations for this node\n",
    "            input1_idx, op1_idx = architecture[2 * (node_idx - 2)]\n",
    "            input2_idx, op2_idx = architecture[2 * (node_idx - 2) + 1]\n",
    "            \n",
    "            # Apply operations\n",
    "            output1 = self.ops[node_idx - 2][op1_idx](nodes[input1_idx])\n",
    "            output2 = self.ops[node_idx - 2][op2_idx](nodes[input2_idx])\n",
    "            \n",
    "            # Combine (element-wise addition)\n",
    "            nodes.append(output1 + output2)\n",
    "        \n",
    "        # Concatenate all intermediate nodes\n",
    "        return torch.cat(nodes[2:], dim=1)\n",
    "```\n",
    "\n",
    "#### **Training Algorithm (Two-Phase)**\n",
    "\n",
    "**Phase 1: Train Supernet Weights (w)**\n",
    "\n",
    "```python\n",
    "def train_supernet(supernet, controller, num_epochs=100):\n",
    "    \"\"\"\n",
    "    Train supernet weights by sampling architectures\n",
    "    \"\"\"\n",
    "    optimizer_supernet = torch.optim.SGD(supernet.parameters(), lr=0.1, momentum=0.9)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Sample architecture from controller\n",
    "            architecture, _ = controller()  # Controller outputs architecture\n",
    "            \n",
    "            # Forward pass with this architecture\n",
    "            output = supernet(data, architecture)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            \n",
    "            # Backward pass (update supernet weights only)\n",
    "            optimizer_supernet.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_supernet.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch}, Supernet trained on {len(train_loader)} batches\")\n",
    "```\n",
    "\n",
    "**Phase 2: Train Controller (\u03b8) with REINFORCE**\n",
    "\n",
    "```python\n",
    "def train_controller(controller, supernet, num_iterations=2000):\n",
    "    \"\"\"\n",
    "    Train controller to generate good architectures\n",
    "    Uses supernet for evaluation (NO retraining!)\n",
    "    \"\"\"\n",
    "    optimizer_controller = torch.optim.Adam(controller.parameters(), lr=0.001)\n",
    "    baseline = None\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Sample batch of architectures\n",
    "        architectures = []\n",
    "        log_probs_batch = []\n",
    "        rewards = []\n",
    "        \n",
    "        for _ in range(10):  # Batch size 10\n",
    "            architecture, log_probs = controller()\n",
    "            architectures.append(architecture)\n",
    "            log_probs_batch.append(log_probs)\n",
    "            \n",
    "            # Evaluate architecture using supernet (FAST!)\n",
    "            with torch.no_grad():\n",
    "                accuracy = evaluate_with_supernet(supernet, architecture, val_loader)\n",
    "            rewards.append(accuracy)\n",
    "        \n",
    "        # Update baseline\n",
    "        if baseline is None:\n",
    "            baseline = np.mean(rewards)\n",
    "        else:\n",
    "            baseline = 0.9 * baseline + 0.1 * np.mean(rewards)\n",
    "        \n",
    "        # Policy gradient\n",
    "        policy_loss = 0\n",
    "        for log_probs, reward in zip(log_probs_batch, rewards):\n",
    "            advantage = reward - baseline\n",
    "            policy_loss -= (log_probs.sum() * advantage)\n",
    "        policy_loss /= len(rewards)\n",
    "        \n",
    "        # Update controller\n",
    "        optimizer_controller.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer_controller.step()\n",
    "    \n",
    "    # Return best architecture\n",
    "    best_architecture, _ = controller()\n",
    "    return best_architecture\n",
    "```\n",
    "\n",
    "**Alternating Training (Complete ENAS):**\n",
    "\n",
    "```python\n",
    "def train_enas(supernet, controller, num_cycles=10):\n",
    "    \"\"\"\n",
    "    Alternate between training supernet and controller\n",
    "    \"\"\"\n",
    "    for cycle in range(num_cycles):\n",
    "        print(f\"\\n=== Cycle {cycle+1}/{num_cycles} ===\")\n",
    "        \n",
    "        # Phase 1: Train supernet (10 epochs)\n",
    "        print(\"Training supernet...\")\n",
    "        train_supernet(supernet, controller, num_epochs=10)\n",
    "        \n",
    "        # Phase 2: Train controller (200 iterations)\n",
    "        print(\"Training controller...\")\n",
    "        train_controller(controller, supernet, num_iterations=200)\n",
    "    \n",
    "    # Final: Sample best architecture, train from scratch\n",
    "    best_architecture, _ = controller()\n",
    "    print(f\"Best architecture: {best_architecture}\")\n",
    "    \n",
    "    final_model = build_model_from_architecture(best_architecture)\n",
    "    train_from_scratch(final_model, epochs=600)\n",
    "    return final_model\n",
    "```\n",
    "\n",
    "**ENAS Results:**\n",
    "- **Search cost:** 0.67 GPU-days (16 GPU-hours)\n",
    "- **Speedup vs NASNet:** 22,400 / 0.67 \u2248 **33,000\u00d7 faster** \ud83d\ude80\n",
    "- **CIFAR-10 accuracy:** 97.3% (vs 97.4% for NASNet, -0.1% only!)\n",
    "- **Why it works:** Weight sharing preserves architecture ranking (correlation \u22480.8)\n",
    "\n",
    "---\n",
    "\n",
    "### **Algorithm 3: DARTS (Differentiable NAS, 2018)**\n",
    "\n",
    "**Paper:** \"DARTS: Differentiable Architecture Search\" (Liu et al., CMU)\n",
    "\n",
    "**Key Innovation:** Continuous relaxation \u2192 Architecture parameters differentiable \u2192 Gradient descent!\n",
    "\n",
    "#### **Continuous Relaxation**\n",
    "\n",
    "**Discrete (original):**\n",
    "```python\n",
    "# Choose ONE operation from list\n",
    "def mixed_op_discrete(x, operations, choice):\n",
    "    return operations[choice](x)\n",
    "\n",
    "# Example: choice=2 \u2192 max_pool(x)\n",
    "```\n",
    "\n",
    "**Continuous (DARTS):**\n",
    "```python\n",
    "def mixed_op_continuous(x, operations, alpha):\n",
    "    \"\"\"\n",
    "    Weighted sum over ALL operations\n",
    "    \n",
    "    alpha: [\u03b11, \u03b12, ..., \u03b1_k] (continuous parameters)\n",
    "    output = \u03a3 softmax(\u03b1_i) \u00d7 operation_i(x)\n",
    "    \"\"\"\n",
    "    weights = F.softmax(alpha, dim=0)  # Normalize to probabilities\n",
    "    return sum(w * op(x) for w, op in zip(weights, operations))\n",
    "\n",
    "# Example: alpha = [2.5, 0.3, -1.8]\n",
    "# \u2192 softmax = [0.85, 0.13, 0.02]\n",
    "# \u2192 output = 0.85 \u00d7 conv3x3(x) + 0.13 \u00d7 conv5x5(x) + 0.02 \u00d7 max_pool(x)\n",
    "```\n",
    "\n",
    "**Why This Enables Gradient Descent:**\n",
    "```python\n",
    "# Forward pass\n",
    "output = mixed_op_continuous(x, operations, alpha)\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# Backward pass: Compute \u2202loss/\u2202alpha\n",
    "alpha.grad = torch.autograd.grad(loss, alpha)\n",
    "\n",
    "# Gradient descent on architecture!\n",
    "alpha = alpha - lr \u00d7 alpha.grad\n",
    "```\n",
    "\n",
    "#### **DARTS Architecture**\n",
    "\n",
    "```python\n",
    "class DARTSCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Differentiable cell with continuous relaxation\n",
    "    \"\"\"\n",
    "    def __init__(self, num_nodes=4, operations=['conv3x3', 'conv5x5', 'max_pool', 'avg_pool', 'identity', 'zero']):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.operations = operations\n",
    "        \n",
    "        # Architecture parameters \u03b1 (one per edge per operation)\n",
    "        # Edge (i, j): Connection from node i to node j\n",
    "        self.alphas = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(i + 2, len(operations)))  # Node j can connect to nodes 0..j-1\n",
    "            for i in range(num_nodes)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, s0, s1):\n",
    "        \"\"\"\n",
    "        s0, s1: Inputs from previous 2 cells\n",
    "        \"\"\"\n",
    "        states = [s0, s1]\n",
    "        \n",
    "        for node_idx in range(self.num_nodes):\n",
    "            # Collect inputs from all previous nodes\n",
    "            node_inputs = []\n",
    "            for prev_idx in range(len(states)):\n",
    "                # Weighted sum over operations (continuous relaxation)\n",
    "                alpha = self.alphas[node_idx][prev_idx]  # Architecture parameters for this edge\n",
    "                weights = F.softmax(alpha, dim=0)\n",
    "                \n",
    "                mixed_output = sum(\n",
    "                    w * op(states[prev_idx])\n",
    "                    for w, op in zip(weights, self.operations)\n",
    "                )\n",
    "                node_inputs.append(mixed_output)\n",
    "            \n",
    "            # Sum all inputs to this node\n",
    "            states.append(sum(node_inputs))\n",
    "        \n",
    "        # Concatenate all intermediate nodes\n",
    "        return torch.cat(states[2:], dim=1)\n",
    "```\n",
    "\n",
    "#### **Bi-Level Optimization**\n",
    "\n",
    "**Objective:**\n",
    "```\n",
    "min_\u03b1  L_val(w*(\u03b1), \u03b1)\n",
    "s.t.   w*(\u03b1) = argmin_w L_train(w, \u03b1)\n",
    "```\n",
    "\n",
    "**Exact Second-Order Method (Expensive):**\n",
    "```python\n",
    "# Compute w*(\u03b1) by fully training network (expensive!)\n",
    "w_star = train_to_convergence(architecture=alpha)\n",
    "\n",
    "# Then compute gradient \u2207_\u03b1 L_val(w_star, \u03b1)\n",
    "grad_alpha = compute_val_gradient(w_star, alpha)\n",
    "\n",
    "# Update \u03b1\n",
    "alpha = alpha - lr_alpha * grad_alpha\n",
    "```\n",
    "\n",
    "**First-Order Approximation (DARTS, Practical):**\n",
    "```python\n",
    "def darts_bilevel_optimization(model, train_loader, val_loader, epochs=50):\n",
    "    \"\"\"\n",
    "    Approximate bi-level optimization\n",
    "    \"\"\"\n",
    "    # Optimizers\n",
    "    optimizer_weights = torch.optim.SGD(model.weights(), lr=0.025, momentum=0.9)\n",
    "    optimizer_arch = torch.optim.Adam(model.alphas(), lr=3e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Alternate between weight and architecture updates\n",
    "        for step, (train_batch, val_batch) in enumerate(zip(train_loader, val_loader)):\n",
    "            train_x, train_y = train_batch\n",
    "            val_x, val_y = val_batch\n",
    "            \n",
    "            # ===== Phase 1: Update weights w (inner optimization) =====\n",
    "            optimizer_weights.zero_grad()\n",
    "            train_logits = model(train_x)\n",
    "            train_loss = F.cross_entropy(train_logits, train_y)\n",
    "            train_loss.backward()\n",
    "            optimizer_weights.step()\n",
    "            \n",
    "            # ===== Phase 2: Update architecture \u03b1 (outer optimization) =====\n",
    "            optimizer_arch.zero_grad()\n",
    "            val_logits = model(val_x)\n",
    "            val_loss = F.cross_entropy(val_logits, val_y)\n",
    "            val_loss.backward()  # Compute \u2207_\u03b1 L_val (backprop through \u03b1!)\n",
    "            optimizer_arch.step()\n",
    "        \n",
    "        # Log progress\n",
    "        print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return model.alphas()\n",
    "```\n",
    "\n",
    "**Why First-Order Approximation Works:**\n",
    "```\n",
    "Exact gradient (expensive):\n",
    "\u2207_\u03b1 L_val(w*, \u03b1) = \u2202L_val/\u2202\u03b1 + \u2202L_val/\u2202w \u00d7 \u2202w*/\u2202\u03b1\n",
    "                   \u2191            \u2191\n",
    "                   Direct       Indirect (requires computing \u2202w*/\u2202\u03b1, very expensive!)\n",
    "\n",
    "First-order approximation (DARTS):\n",
    "\u2207_\u03b1 L_val(w, \u03b1) \u2248 \u2202L_val/\u2202\u03b1\n",
    "                  \u2191\n",
    "                  Direct only (ignore indirect term)\n",
    "\n",
    "Justification:\n",
    "- w is updated to minimize L_train \u2192 \u2202L_train/\u2202w \u2248 0 (optimality condition)\n",
    "- If L_train and L_val are similar, then \u2202L_val/\u2202w \u2248 0 too\n",
    "- Therefore, \u2202L_val/\u2202w \u00d7 \u2202w*/\u2202\u03b1 \u2248 0 (indirect term is small)\n",
    "- Empirically: Works well in practice! (97% CIFAR-10)\n",
    "```\n",
    "\n",
    "#### **Architecture Discretization**\n",
    "\n",
    "After search, convert continuous \u03b1 to discrete architecture:\n",
    "\n",
    "```python\n",
    "def discretize_architecture(model, k=2):\n",
    "    \"\"\"\n",
    "    Convert continuous architecture parameters to discrete\n",
    "    \n",
    "    Strategy: Keep top-k operations per edge (k=2 typical)\n",
    "    \"\"\"\n",
    "    final_architecture = []\n",
    "    \n",
    "    for node_idx in range(model.num_nodes):\n",
    "        # Get architecture parameters for this node\n",
    "        alphas_node = model.alphas[node_idx]  # Shape: (num_prev_nodes, num_ops)\n",
    "        \n",
    "        # For each previous node (edge), select top-k operations\n",
    "        node_edges = []\n",
    "        for prev_idx in range(len(alphas_node)):\n",
    "            alpha_edge = alphas_node[prev_idx]  # Shape: (num_ops,)\n",
    "            \n",
    "            # Get top-k operation indices\n",
    "            top_k_ops = torch.topk(alpha_edge, k).indices\n",
    "            node_edges.append((prev_idx, top_k_ops.tolist()))\n",
    "        \n",
    "        final_architecture.append(node_edges)\n",
    "    \n",
    "    return final_architecture\n",
    "\n",
    "# Example output:\n",
    "# Node 2: [(0, [conv3x3, conv5x5]), (1, [max_pool, identity])]\n",
    "#         \u2191     \u2191\n",
    "#         Input from node 0, use conv3x3 OR conv5x5 (keep both, ensemble effect)\n",
    "```\n",
    "\n",
    "#### **Complete DARTS Algorithm**\n",
    "\n",
    "```python\n",
    "def run_darts(num_nodes=4, operations=['conv3x3', 'conv5x5', 'max_pool', 'avg_pool', 'identity', 'zero']):\n",
    "    \"\"\"\n",
    "    Complete DARTS: Search \u2192 Discretize \u2192 Retrain\n",
    "    \"\"\"\n",
    "    # ===== Phase 1: Architecture Search =====\n",
    "    print(\"Phase 1: Searching for architecture...\")\n",
    "    \n",
    "    model = DARTSNetwork(num_nodes=num_nodes, operations=operations)\n",
    "    darts_bilevel_optimization(model, train_loader, val_loader, epochs=50)\n",
    "    \n",
    "    # Discretize architecture\n",
    "    best_architecture = discretize_architecture(model, k=2)\n",
    "    print(f\"Best architecture found: {best_architecture}\")\n",
    "    \n",
    "    # ===== Phase 2: Retrain from Scratch =====\n",
    "    print(\"Phase 2: Retraining discovered architecture...\")\n",
    "    \n",
    "    final_model = build_model_from_architecture(best_architecture)\n",
    "    optimizer = torch.optim.SGD(final_model.parameters(), lr=0.025, momentum=0.9, weight_decay=3e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=600)\n",
    "    \n",
    "    for epoch in range(600):  # Full training\n",
    "        train_loss = train_one_epoch(final_model, train_loader, optimizer)\n",
    "        val_acc = evaluate(final_model, val_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Acc: {val_acc:.2%}\")\n",
    "    \n",
    "    final_acc = evaluate(final_model, test_loader)\n",
    "    print(f\"Final Test Accuracy: {final_acc:.2%}\")\n",
    "    \n",
    "    return final_model, best_architecture\n",
    "```\n",
    "\n",
    "**DARTS Results:**\n",
    "- **Search cost:** 1 GPU-day (4 GPUs \u00d7 6 hours)\n",
    "- **CIFAR-10 accuracy:** 97.00% (vs 97.4% NASNet, 97.3% ENAS)\n",
    "- **ImageNet accuracy:** 73.3% (with discovered cell, not state-of-art but respectable)\n",
    "- **Key benefit:** Fast, simple, gradient-based (no RL complexity)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Algorithm Comparison Summary\n",
    "\n",
    "| Algorithm | Search Strategy | Search Space | Evaluation | Cost | CIFAR-10 Acc | Key Innovation |\n",
    "|-----------|----------------|--------------|------------|------|--------------|----------------|\n",
    "| **NASNet** | Reinforcement Learning (REINFORCE) | 10^6 cells | Train from scratch (20 epochs) | 22,400 GPU-days | 97.4% | First RL-based NAS |\n",
    "| **ENAS** | RL + Weight Sharing | 10^6 cells | Supernet (no retraining) | 0.67 GPU-days | 97.3% | 33,000\u00d7 speedup via weight sharing |\n",
    "| **DARTS** | Gradient Descent | Continuous relaxation | Bi-level optimization | 1 GPU-day | 97.0% | Differentiable architecture |\n",
    "| **AmoebaNet** | Evolutionary | 10^6 cells | Train from scratch (20 epochs) | 3,150 GPU-days | 97.5% | Mutation + crossover |\n",
    "\n",
    "**When to Use Each:**\n",
    "\n",
    "1. **NASNet (RL):** When you need state-of-art accuracy and have 20,000+ GPU-days (large companies only)\n",
    "2. **ENAS:** When you want fast search (1 GPU-day) and don't mind weight sharing bias\n",
    "3. **DARTS:** When you want fastest search + simplicity (gradient descent, no RL complexity)\n",
    "4. **AmoebaNet:** When you want diverse architectures (evolutionary exploration)\n",
    "\n",
    "**Modern Recommendation (2025):** Start with DARTS (1 GPU-day), if accuracy insufficient, try ENAS (0.67 GPU-days), if still insufficient, use NASNet (but expect 20,000+ GPU-days)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udca1 Key Insights from Theory\n",
    "\n",
    "**Insight 1: Weight Sharing \u2260 True Performance**\n",
    "- ENAS supernet weights are biased (optimized for ALL architectures)\n",
    "- Correlation with train-from-scratch: ~0.8 (good but not perfect)\n",
    "- Risk: May miss best architecture if ranking is incorrect\n",
    "\n",
    "**Insight 2: Continuous Relaxation Enables Gradient Descent**\n",
    "- DARTS: Discrete choice \u2192 Continuous weights \u2192 Backpropagation\n",
    "- 22,400\u00d7 faster than RL-based NAS (gradient descent > policy gradient)\n",
    "\n",
    "**Insight 3: Bi-Level Optimization is Approximate**\n",
    "- Exact: Requires computing \u2202w*/\u2202\u03b1 (expensive!)\n",
    "- DARTS: Ignores indirect term (works empirically)\n",
    "\n",
    "**Insight 4: Search Space Design is Critical**\n",
    "- Too broad (10^50 architectures): Intractable\n",
    "- Too narrow (10^2 architectures): Miss optimal\n",
    "- Sweet spot: Cell-based (10^6), discoverable + transferable\n",
    "\n",
    "**Insight 5: Evaluation Strategy Trade-off**\n",
    "- Train from scratch: Accurate but slow (4-50 GPU-days)\n",
    "- Early stopping: Fast but noisy (correlation ~0.7)\n",
    "- Weight sharing: Fastest but biased (correlation ~0.8)\n",
    "- Predictor: Data-efficient (train once, predict 10,000\u00d7)\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Implementation of DARTS, ENAS, and NAS for chip verification! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23bd43a",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# NEURAL ARCHITECTURE SEARCH\n",
    "# Complete Implementation\n",
    "# ===========================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import time\n",
    "# ===========================\n",
    "# 1. OPERATIONS (Building Blocks)\n",
    "# ===========================\n",
    "class SeparableConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Depthwise separable convolution (MobileNet-style)\n",
    "    \n",
    "    Standard conv: H \u00d7 W \u00d7 C_in \u00d7 C_out \u00d7 K \u00d7 K = O(H\u00d7W\u00d7C_in\u00d7C_out\u00d7K\u00b2)\n",
    "    Separable: Depthwise O(H\u00d7W\u00d7C_in\u00d7K\u00b2) + Pointwise O(H\u00d7W\u00d7C_in\u00d7C_out)\n",
    "             = O(H\u00d7W\u00d7K\u00b2\u00d7C_in + H\u00d7W\u00d7C_in\u00d7C_out)\n",
    "             \u2248 O(H\u00d7W\u00d7C_in\u00d7C_out\u00d7K\u00b2) / K\u00b2 (assuming C_out >> K\u00b2)\n",
    "    \n",
    "    For K=3, C_out=128: 8-9\u00d7 fewer parameters\n",
    "    \"\"\"\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(C_in, C_in, kernel_size, stride=stride, \n",
    "                                    padding=padding, groups=C_in, bias=False)\n",
    "        self.pointwise = nn.Conv2d(C_in, C_out, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(C_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "class DilatedConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Dilated (atrous) convolution\n",
    "    \n",
    "    Receptive field: r = 1 + (K-1) \u00d7 dilation\n",
    "    Standard 3\u00d73: r = 3\n",
    "    Dilated 3\u00d73 (dilation=2): r = 1 + 2\u00d72 = 5\n",
    "    \n",
    "    Benefit: Larger receptive field without extra parameters\n",
    "    Use case: Chip layout analysis (need to see 128\u00d7128 region)\n",
    "    \"\"\"\n",
    "    def __init__(self, C_in, C_out, kernel_size, stride=1, dilation=2):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size + (kernel_size - 1) * (dilation - 1)) // 2\n",
    "        self.conv = nn.Conv2d(C_in, C_out, kernel_size, stride=stride, \n",
    "                              padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(C_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.bn(self.conv(x))\n",
    "class Identity(nn.Module):\n",
    "    \"\"\"\n",
    "    Identity operation (skip connection)\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebd9186",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Class: Zero\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b42d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zero(nn.Module):\n",
    "    \"\"\"\n",
    "    Zero operation (no connection, for pruning)\n",
    "    \"\"\"\n",
    "    def __init__(self, stride=1):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.stride == 1:\n",
    "            return x * 0.0\n",
    "        else:\n",
    "            # Stride > 1: Downsample then zero\n",
    "            return x[:, :, ::self.stride, ::self.stride] * 0.0\n",
    "class FactorizedReduce(nn.Module):\n",
    "    \"\"\"\n",
    "    Reduce spatial dimensions by 2\u00d7 (for stride=2 connections)\n",
    "    \"\"\"\n",
    "    def __init__(self, C_in, C_out):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, bias=False)\n",
    "        self.conv2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(C_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.conv1(x)\n",
    "        out2 = self.conv2(x[:, :, 1:, 1:])  # Shift by 1 pixel\n",
    "        out = torch.cat([out1, out2], dim=1)\n",
    "        return self.bn(out)\n",
    "# Define operation factory\n",
    "OPERATIONS = {\n",
    "    'conv3x3': lambda C, stride: nn.Sequential(\n",
    "        nn.Conv2d(C, C, 3, stride=stride, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(C),\n",
    "        nn.ReLU(inplace=False)\n",
    "    ),\n",
    "    'conv5x5': lambda C, stride: nn.Sequential(\n",
    "        nn.Conv2d(C, C, 5, stride=stride, padding=2, bias=False),\n",
    "        nn.BatchNorm2d(C),\n",
    "        nn.ReLU(inplace=False)\n",
    "    ),\n",
    "    'sep_conv3x3': lambda C, stride: nn.Sequential(\n",
    "        SeparableConv2d(C, C, 3, stride=stride, padding=1),\n",
    "        nn.ReLU(inplace=False)\n",
    "    ),\n",
    "    'sep_conv5x5': lambda C, stride: nn.Sequential(\n",
    "        SeparableConv2d(C, C, 5, stride=stride, padding=2),\n",
    "        nn.ReLU(inplace=False)\n",
    "    ),\n",
    "    'dil_conv3x3': lambda C, stride: nn.Sequential(\n",
    "        DilatedConv2d(C, C, 3, stride=stride, dilation=2),\n",
    "        nn.ReLU(inplace=False)\n",
    "    ),\n",
    "    'max_pool3x3': lambda C, stride: nn.MaxPool2d(3, stride=stride, padding=1),\n",
    "    'avg_pool3x3': lambda C, stride: nn.AvgPool2d(3, stride=stride, padding=1),\n",
    "    'identity': lambda C, stride: Identity() if stride == 1 else FactorizedReduce(C, C),\n",
    "    'zero': lambda C, stride: Zero(stride=stride)\n",
    "}\n",
    "OPERATION_NAMES = list(OPERATIONS.keys())\n",
    "# ===========================\n",
    "# 2. DARTS MIXED OPERATION\n",
    "# ===========================\n",
    "class MixedOp(nn.Module):\n",
    "    \"\"\"\n",
    "    Continuous relaxation: Weighted sum over ALL operations\n",
    "    \n",
    "    output = \u03a3_i softmax(\u03b1_i) \u00d7 operation_i(x)\n",
    "    \n",
    "    During search: Use all operations (memory expensive)\n",
    "    After search: Keep top-k operations\n",
    "    \"\"\"\n",
    "    def __init__(self, C, stride, operations=OPERATION_NAMES):\n",
    "        super().__init__()\n",
    "        self.ops = nn.ModuleList([\n",
    "            OPERATIONS[op_name](C, stride) for op_name in operations\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, weights):\n",
    "        \"\"\"\n",
    "        weights: softmax(\u03b1) for this edge\n",
    "        \"\"\"\n",
    "        return sum(w * op(x) for w, op in zip(weights, self.ops))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6beeae",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62304fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 3. DARTS CELL\n",
    "# ===========================\n",
    "class DARTSCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Differentiable cell with continuous relaxation\n",
    "    \n",
    "    Cell structure:\n",
    "    - num_nodes intermediate nodes (typically 4)\n",
    "    - Each node receives inputs from ALL previous nodes\n",
    "    - Each edge: MixedOp (weighted sum of operations)\n",
    "    - Final output: Concatenate all intermediate nodes\n",
    "    \"\"\"\n",
    "    def __init__(self, num_nodes, C_prev_prev, C_prev, C, reduction, operations=OPERATION_NAMES):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_ops = len(operations)\n",
    "        \n",
    "        # Preprocessing: Match channel dimensions\n",
    "        if reduction:\n",
    "            self.preprocess0 = FactorizedReduce(C_prev_prev, C)\n",
    "            self.preprocess1 = FactorizedReduce(C_prev, C)\n",
    "        else:\n",
    "            self.preprocess0 = nn.Sequential(\n",
    "                nn.Conv2d(C_prev_prev, C, 1, bias=False),\n",
    "                nn.BatchNorm2d(C)\n",
    "            )\n",
    "            self.preprocess1 = nn.Sequential(\n",
    "                nn.Conv2d(C_prev, C, 1, bias=False),\n",
    "                nn.BatchNorm2d(C)\n",
    "            )\n",
    "        \n",
    "        # Create edges: Each node connects to all previous nodes\n",
    "        self.edges = nn.ModuleList()\n",
    "        for i in range(num_nodes):\n",
    "            # Node i can connect to: input0, input1, node0, ..., node(i-1)\n",
    "            # Total: 2 + i predecessors\n",
    "            for j in range(2 + i):\n",
    "                stride = 2 if reduction and j < 2 else 1\n",
    "                op = MixedOp(C, stride, operations)\n",
    "                self.edges.append(op)\n",
    "        \n",
    "    def forward(self, s0, s1, alphas):\n",
    "        \"\"\"\n",
    "        s0, s1: Inputs from previous 2 cells\n",
    "        alphas: Architecture parameters (continuous weights)\n",
    "        \"\"\"\n",
    "        s0 = self.preprocess0(s0)\n",
    "        s1 = self.preprocess1(s1)\n",
    "        \n",
    "        states = [s0, s1]\n",
    "        offset = 0\n",
    "        \n",
    "        for i in range(self.num_nodes):\n",
    "            # Collect inputs from all previous nodes\n",
    "            node_inputs = []\n",
    "            for j in range(2 + i):\n",
    "                # Get architecture weights for this edge\n",
    "                edge_idx = offset + j\n",
    "                edge_alphas = alphas[edge_idx]  # Shape: (num_ops,)\n",
    "                weights = F.softmax(edge_alphas, dim=0)\n",
    "                \n",
    "                # Mixed operation\n",
    "                node_inputs.append(self.edges[edge_idx](states[j], weights))\n",
    "            \n",
    "            # Sum all inputs to this node\n",
    "            states.append(sum(node_inputs))\n",
    "            offset += (2 + i)\n",
    "        \n",
    "        # Concatenate all intermediate nodes\n",
    "        return torch.cat(states[2:], dim=1)\n",
    "    \n",
    "    def num_edges(self):\n",
    "        \"\"\"\n",
    "        Total number of edges in cell\n",
    "        \n",
    "        Node 0: 2 predecessors (input0, input1)\n",
    "        Node 1: 3 predecessors (input0, input1, node0)\n",
    "        Node 2: 4 predecessors\n",
    "        Node 3: 5 predecessors\n",
    "        Total: 2 + 3 + 4 + 5 = 14 edges (for num_nodes=4)\n",
    "        \"\"\"\n",
    "        return sum(2 + i for i in range(self.num_nodes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2528469d",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8150b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 4. DARTS NETWORK\n",
    "# ===========================\n",
    "class DARTSNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete DARTS searchable network\n",
    "    \n",
    "    Structure:\n",
    "    - Stem: Initial convolution\n",
    "    - Cells: Stack of normal + reduction cells\n",
    "    - Head: Global pooling + classifier\n",
    "    \n",
    "    Two types of cells:\n",
    "    - Normal: Keep spatial dimensions (stride=1)\n",
    "    - Reduction: Downsample 2\u00d7 (stride=2)\n",
    "    \"\"\"\n",
    "    def __init__(self, C=16, num_classes=10, num_layers=8, num_nodes=4, operations=OPERATION_NAMES):\n",
    "        super().__init__()\n",
    "        self.C = C\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_ops = len(operations)\n",
    "        \n",
    "        # Stem\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, C, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(C)\n",
    "        )\n",
    "        \n",
    "        # Build cells\n",
    "        self.cells = nn.ModuleList()\n",
    "        C_prev_prev, C_prev, C_curr = C, C, C\n",
    "        reduction_layers = [num_layers // 3, 2 * num_layers // 3]\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            reduction = i in reduction_layers\n",
    "            if reduction:\n",
    "                C_curr *= 2\n",
    "            \n",
    "            cell = DARTSCell(num_nodes, C_prev_prev, C_prev, C_curr, reduction, operations)\n",
    "            self.cells.append(cell)\n",
    "            \n",
    "            C_prev_prev, C_prev = C_prev, num_nodes * C_curr\n",
    "        \n",
    "        # Head\n",
    "        self.global_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Linear(C_prev, num_classes)\n",
    "        \n",
    "        # Architecture parameters (learnable!)\n",
    "        num_edges_normal = self.cells[0].num_edges()\n",
    "        num_edges_reduce = self.cells[0].num_edges()\n",
    "        \n",
    "        self.alphas_normal = nn.Parameter(torch.randn(num_edges_normal, self.num_ops))\n",
    "        self.alphas_reduce = nn.Parameter(torch.randn(num_edges_reduce, self.num_ops))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        s0 = s1 = self.stem(x)\n",
    "        \n",
    "        for i, cell in enumerate(self.cells):\n",
    "            alphas = self.alphas_reduce if cell.reduction else self.alphas_normal\n",
    "            s0, s1 = s1, cell(s0, s1, alphas)\n",
    "        \n",
    "        out = self.global_pooling(s1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        logits = self.classifier(out)\n",
    "        return logits\n",
    "    \n",
    "    def arch_parameters(self):\n",
    "        \"\"\"\n",
    "        Return architecture parameters for optimizer\n",
    "        \"\"\"\n",
    "        return [self.alphas_normal, self.alphas_reduce]\n",
    "    \n",
    "    def weight_parameters(self):\n",
    "        \"\"\"\n",
    "        Return network weights (excluding architecture parameters)\n",
    "        \"\"\"\n",
    "        return [p for name, p in self.named_parameters() \n",
    "                if 'alpha' not in name]\n",
    "    \n",
    "    def discretize(self, k=2):\n",
    "        \"\"\"\n",
    "        Convert continuous architecture to discrete\n",
    "        \n",
    "        Strategy: Keep top-k operations per edge\n",
    "        \"\"\"\n",
    "        def parse_alphas(alphas):\n",
    "            gene = []\n",
    "            for i in range(self.num_nodes):\n",
    "                edges = []\n",
    "                # Get edges for this node\n",
    "                start = sum(2 + j for j in range(i))\n",
    "                end = start + (2 + i)\n",
    "                \n",
    "                for j in range(start, end):\n",
    "                    # Get top-k operations for this edge\n",
    "                    edge_alphas = alphas[j]\n",
    "                    topk_ops = torch.topk(edge_alphas, k).indices.tolist()\n",
    "                    edges.append((j - start, [OPERATION_NAMES[idx] for idx in topk_ops]))\n",
    "                \n",
    "                gene.append(edges)\n",
    "            return gene\n",
    "        \n",
    "        gene_normal = parse_alphas(self.alphas_normal)\n",
    "        gene_reduce = parse_alphas(self.alphas_reduce)\n",
    "        \n",
    "        return {\n",
    "            'normal': gene_normal,\n",
    "            'reduce': gene_reduce\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d60a6f",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 5\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7c542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 5. DARTS TRAINING\n",
    "# ===========================\n",
    "def train_darts(model, train_loader, val_loader, epochs=50, lr_w=0.025, lr_alpha=3e-4):\n",
    "    \"\"\"\n",
    "    Bi-level optimization for DARTS\n",
    "    \n",
    "    Phase 1: Update weights w on training set\n",
    "    Phase 2: Update architecture \u03b1 on validation set\n",
    "    \"\"\"\n",
    "    # Optimizers\n",
    "    optimizer_w = torch.optim.SGD(\n",
    "        model.weight_parameters(), \n",
    "        lr=lr_w, \n",
    "        momentum=0.9, \n",
    "        weight_decay=3e-4\n",
    "    )\n",
    "    optimizer_alpha = torch.optim.Adam(\n",
    "        model.arch_parameters(), \n",
    "        lr=lr_alpha, \n",
    "        betas=(0.5, 0.999), \n",
    "        weight_decay=1e-3\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_w, T_max=epochs)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Training DARTS on {device}\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "    print(f\"Architecture parameters: {sum(p.numel() for p in model.arch_parameters())}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        # Create iterators\n",
    "        train_iter = iter(train_loader)\n",
    "        val_iter = iter(val_loader)\n",
    "        \n",
    "        for step in range(len(train_loader)):\n",
    "            # Get training batch\n",
    "            try:\n",
    "                train_x, train_y = next(train_iter)\n",
    "            except StopIteration:\n",
    "                train_iter = iter(train_loader)\n",
    "                train_x, train_y = next(train_iter)\n",
    "            \n",
    "            train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "            \n",
    "            # ===== Phase 1: Update weights w =====\n",
    "            optimizer_w.zero_grad()\n",
    "            logits = model(train_x)\n",
    "            loss = F.cross_entropy(logits, train_y)\n",
    "            loss.backward()\n",
    "            optimizer_w.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            train_total += train_y.size(0)\n",
    "            train_correct += predicted.eq(train_y).sum().item()\n",
    "            \n",
    "            # ===== Phase 2: Update architecture \u03b1 =====\n",
    "            if step % 2 == 0:  # Update architecture every 2 steps\n",
    "                try:\n",
    "                    val_x, val_y = next(val_iter)\n",
    "                except StopIteration:\n",
    "                    val_iter = iter(val_loader)\n",
    "                    val_x, val_y = next(val_iter)\n",
    "                \n",
    "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "                \n",
    "                optimizer_alpha.zero_grad()\n",
    "                logits = model(val_x)\n",
    "                loss = F.cross_entropy(logits, val_y)\n",
    "                loss.backward()\n",
    "                optimizer_alpha.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_x, val_y in val_loader:\n",
    "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "                logits = model(val_x)\n",
    "                loss = F.cross_entropy(logits, val_y)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = logits.max(1)\n",
    "                val_total += val_y.size(0)\n",
    "                val_correct += predicted.eq(val_y).sum().item()\n",
    "        \n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Print architecture every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\"\\nCurrent Architecture:\")\n",
    "            print(\"Normal cell alphas (top-3 ops per edge):\")\n",
    "            for i in range(min(3, model.alphas_normal.size(0))):\n",
    "                alphas = F.softmax(model.alphas_normal[i], dim=0)\n",
    "                top3 = torch.topk(alphas, 3)\n",
    "                ops = [OPERATION_NAMES[idx] for idx in top3.indices]\n",
    "                weights = top3.values.tolist()\n",
    "                print(f\"  Edge {i}: {list(zip(ops, [f'{w:.3f}' for w in weights]))}\")\n",
    "            print()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24988e73",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 6\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b353b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 6. CHIP VERIFICATION NAS\n",
    "# ===========================\n",
    "class ChipDefectDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Simulated chip defect dataset for demonstration\n",
    "    \n",
    "    Real data: STDF files with parametric test results + wafer maps\n",
    "    \n",
    "    Defect types:\n",
    "    1. Scratch: Linear patterns (manufacturing damage)\n",
    "    2. Particle: Circular contamination\n",
    "    3. Pattern defect: Repeating structures (lithography issues)\n",
    "    4. Normal: No defects\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples=5000, image_size=128, split='train'):\n",
    "        self.num_samples = num_samples\n",
    "        self.image_size = image_size\n",
    "        self.split = split\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Generate synthetic chip layout image\n",
    "        img = torch.randn(3, self.image_size, self.image_size)\n",
    "        \n",
    "        # Add defect patterns\n",
    "        defect_type = np.random.randint(0, 4)\n",
    "        \n",
    "        if defect_type == 0:  # Scratch (linear)\n",
    "            start = np.random.randint(0, self.image_size)\n",
    "            img[:, start:start+2, :] += 2.0\n",
    "        elif defect_type == 1:  # Particle (circular)\n",
    "            center = np.random.randint(20, self.image_size-20, size=2)\n",
    "            y, x = np.ogrid[:self.image_size, :self.image_size]\n",
    "            mask = (x - center[1])**2 + (y - center[0])**2 <= 10**2\n",
    "            img[:, mask] += 1.5\n",
    "        elif defect_type == 2:  # Pattern defect (repeating)\n",
    "            for i in range(0, self.image_size, 16):\n",
    "                img[:, i:i+2, :] += 1.0\n",
    "        # defect_type == 3: Normal (no modification)\n",
    "        \n",
    "        # Normalize\n",
    "        img = (img - img.mean()) / (img.std() + 1e-8)\n",
    "        \n",
    "        return img, defect_type\n",
    "def train_chip_verification_nas():\n",
    "    \"\"\"\n",
    "    NAS for chip defect detection\n",
    "    \n",
    "    Goal: Discover architecture optimized for chip layouts\n",
    "    \n",
    "    Custom search space:\n",
    "    - Dilated convolutions (large receptive field for 128\u00d7128 layouts)\n",
    "    - Multi-scale operations (defects at multiple resolutions)\n",
    "    - Attention (long-range dependencies in circuits)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"NEURAL ARCHITECTURE SEARCH FOR CHIP VERIFICATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Custom operations for chip verification\n",
    "    chip_operations = [\n",
    "        'conv3x3', 'conv5x5', \n",
    "        'sep_conv3x3', 'sep_conv5x5',\n",
    "        'dil_conv3x3',  # Large receptive field\n",
    "        'max_pool3x3', 'avg_pool3x3',\n",
    "        'identity'\n",
    "    ]\n",
    "    \n",
    "    # Dataset\n",
    "    train_dataset = ChipDefectDataset(num_samples=3000, image_size=128, split='train')\n",
    "    val_dataset = ChipDefectDataset(num_samples=500, image_size=128, split='val')\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Model\n",
    "    model = DARTSNetwork(\n",
    "        C=16,  # Initial channels\n",
    "        num_classes=4,  # 4 defect types\n",
    "        num_layers=6,  # Fewer layers (small dataset)\n",
    "        num_nodes=4,  # 4 intermediate nodes per cell\n",
    "        operations=chip_operations\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nPhase 1: Architecture Search (Simulated - 1 epoch for demo)\")\n",
    "    model = train_darts(model, train_loader, val_loader, epochs=1, lr_w=0.025, lr_alpha=3e-4)\n",
    "    \n",
    "    # Discretize\n",
    "    print(\"\\nPhase 2: Discretizing Architecture\")\n",
    "    architecture = model.discretize(k=2)\n",
    "    \n",
    "    print(\"\\nDiscovered Architecture:\")\n",
    "    print(\"Normal Cell:\")\n",
    "    for i, edges in enumerate(architecture['normal']):\n",
    "        print(f\"  Node {i}: {edges}\")\n",
    "    print(\"\\nReduction Cell:\")\n",
    "    for i, edges in enumerate(architecture['reduce']):\n",
    "        print(f\"  Node {i}: {edges}\")\n",
    "    \n",
    "    # Business value\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BUSINESS VALUE PROJECTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    baseline_accuracy = 78.0  # ResNet-50 baseline\n",
    "    nas_accuracy = 91.0  # NAS-discovered architecture (projected)\n",
    "    \n",
    "    defects_per_chip = 100\n",
    "    chips_per_year = 1_000_000\n",
    "    cost_per_missed_defect = 50  # Dollars\n",
    "    \n",
    "    missed_defects_baseline = defects_per_chip * chips_per_year * (1 - baseline_accuracy / 100)\n",
    "    missed_defects_nas = defects_per_chip * chips_per_year * (1 - nas_accuracy / 100)\n",
    "    \n",
    "    savings = (missed_defects_baseline - missed_defects_nas) * cost_per_missed_defect\n",
    "    \n",
    "    print(f\"Baseline (ResNet-50): {baseline_accuracy}% detection\")\n",
    "    print(f\"  Missed defects: {missed_defects_baseline:,.0f}/year\")\n",
    "    print(f\"  Cost: ${missed_defects_baseline * cost_per_missed_defect:,.0f}/year\")\n",
    "    \n",
    "    print(f\"\\nNAS Architecture: {nas_accuracy}% detection (+{nas_accuracy - baseline_accuracy}%)\")\n",
    "    print(f\"  Missed defects: {missed_defects_nas:,.0f}/year\")\n",
    "    print(f\"  Cost: ${missed_defects_nas * cost_per_missed_defect:,.0f}/year\")\n",
    "    \n",
    "    print(f\"\\n\u2705 Annual Savings: ${savings:,.0f}\")\n",
    "    print(f\"   ROI: ${savings:,.0f} / $24 (search cost) = {savings/24:.0f}\u00d7 return\")\n",
    "    \n",
    "    return model, architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cdd040",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 7\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5ff65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 7. EXAMPLE: CIFAR-10 DEMO\n",
    "# ===========================\n",
    "def run_cifar10_demo(epochs=5):\n",
    "    \"\"\"\n",
    "    Demonstration: DARTS on CIFAR-10 (simplified)\n",
    "    \n",
    "    Full DARTS: 50 epochs search + 600 epochs retrain = 1 GPU-day\n",
    "    This demo: 5 epochs for illustration purposes\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DARTS on CIFAR-10 (Demo)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                             download=True, transform=transform)\n",
    "    \n",
    "    # Split train into train + val for bi-level optimization\n",
    "    train_size = int(0.8 * len(trainset))\n",
    "    val_size = len(trainset) - train_size\n",
    "    train_subset, val_subset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=64, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_subset, batch_size=64, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Model\n",
    "    model = DARTSNetwork(C=16, num_classes=10, num_layers=8, num_nodes=4)\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nSearching architecture for {epochs} epochs (demo)...\")\n",
    "    model = train_darts(model, train_loader, val_loader, epochs=epochs)\n",
    "    \n",
    "    # Discretize\n",
    "    architecture = model.discretize(k=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DISCOVERED ARCHITECTURE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nNormal Cell:\")\n",
    "    for i, edges in enumerate(architecture['normal']):\n",
    "        print(f\"  Node {i}: {edges}\")\n",
    "    \n",
    "    print(\"\\nReduction Cell:\")\n",
    "    for i, edges in enumerate(architecture['reduce']):\n",
    "        print(f\"  Node {i}: {edges}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"KEY INSIGHTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\u2705 DARTS uses continuous relaxation (gradient descent on architecture)\")\n",
    "    print(\"\u2705 Bi-level optimization: Alternate between weights (w) and architecture (\u03b1)\")\n",
    "    print(\"\u2705 Search cost: ~1 GPU-day for full CIFAR-10 (50 epochs)\")\n",
    "    print(\"\u2705 Final accuracy: 97.0% (after retraining discovered architecture)\")\n",
    "    print(\"\\nComparison:\")\n",
    "    print(\"  NASNet (RL): 22,400 GPU-days \u2192 97.4% accuracy\")\n",
    "    print(\"  ENAS (Weight sharing): 0.67 GPU-days \u2192 97.3% accuracy\")\n",
    "    print(\"  DARTS (Gradient): 1 GPU-day \u2192 97.0% accuracy\")\n",
    "    print(\"\\n\ud83d\ude80 DARTS is 22,400\u00d7 faster than NASNet!\")\n",
    "    \n",
    "    return model, architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadae53c",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 8\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a95fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# MAIN EXECUTION\n",
    "# ===========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"NEURAL ARCHITECTURE SEARCH - IMPLEMENTATION SHOWCASE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nThis notebook implements:\")\n",
    "    print(\"  1. DARTS (Differentiable Architecture Search)\")\n",
    "    print(\"  2. Custom operations (SeparableConv, DilatedConv, etc.)\")\n",
    "    print(\"  3. Bi-level optimization (weights + architecture)\")\n",
    "    print(\"  4. Application to chip verification ($20M-$40M/year)\")\n",
    "    print(\"\\nExecution:\")\n",
    "    print(\"  - CIFAR-10 demo: Uncomment run_cifar10_demo(epochs=5)\")\n",
    "    print(\"  - Chip verification: Uncomment train_chip_verification_nas()\")\n",
    "    print(\"  - Full DARTS: Set epochs=50 (requires 1 GPU-day)\")\n",
    "    \n",
    "    # Uncomment to run:\n",
    "    # model, architecture = run_cifar10_demo(epochs=5)\n",
    "    # model, architecture = train_chip_verification_nas()\n",
    "    \n",
    "    print(\"\\n\u2705 Implementation complete!\")\n",
    "    print(\"   Next: Run the functions above to see NAS in action.\")\n",
    "    print(\"   Expected results:\")\n",
    "    print(\"   - CIFAR-10: 80-85% accuracy (5 epochs demo)\")\n",
    "    print(\"   - Full training: 97%+ accuracy (50 epochs search + 600 epochs retrain)\")\n",
    "    print(\"   - Chip verification: 91% detection vs 78% baseline\")\n",
    "    print(\"   - ROI: $20M-$40M/year savings per chip family\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9be450",
   "metadata": {},
   "source": [
    "# \ud83c\udfaf Real-World Projects & Key Takeaways\n",
    "\n",
    "## \ud83d\ude80 Production-Ready NAS Projects\n",
    "\n",
    "Here are 8 real-world NAS applications with clear objectives, expected outcomes, and business value for semiconductor and general AI domains.\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 1: Automated Test Pattern Optimization NAS** \u2699\ufe0f\n",
    "\n",
    "**Domain:** Post-Silicon Validation (Qualcomm, AMD, Intel)\n",
    "\n",
    "**Problem:**\n",
    "Test patterns for chip verification are manually designed (6-12 months per chip family). Each pattern tests specific functionality (arithmetic, memory, power management). Current approach:\n",
    "- 1,000-5,000 test patterns per chip\n",
    "- Manual design: Engineers craft patterns based on experience\n",
    "- Coverage: 85-90% (miss edge cases, corner conditions)\n",
    "- Time: 6-12 months per chip generation\n",
    "\n",
    "**NAS Solution:**\n",
    "Use neural architecture search to discover optimal CNN architectures for test pattern generation and fault prediction.\n",
    "\n",
    "**Objectives:**\n",
    "1. **Automate test pattern generation:** NAS discovers architecture that generates test patterns automatically\n",
    "2. **Improve coverage:** 95%+ fault coverage (vs 85-90% manual)\n",
    "3. **Reduce time:** 6 months \u2192 2 weeks (180\u00d7 faster)\n",
    "4. **Generalize across chip families:** Architecture discovered for one chip \u2192 Transfer to others\n",
    "\n",
    "**Dataset:**\n",
    "- **Training:** Historical test patterns (10 chip generations \u00d7 5,000 patterns = 50K samples)\n",
    "- **Labels:** Fault coverage metrics (% of bugs detected per pattern)\n",
    "- **Features:** Test pattern encoding (instruction sequences, register states, memory access patterns)\n",
    "- **Validation:** Current-generation chip (test discovered patterns)\n",
    "\n",
    "**NAS Configuration:**\n",
    "```python\n",
    "search_space = {\n",
    "    'operations': [\n",
    "        'conv3x3', 'conv5x5',  # Spatial pattern extraction\n",
    "        'attention',  # Long-range dependencies (test sequence relationships)\n",
    "        'graph_conv',  # Circuit topology awareness\n",
    "        'lstm',  # Sequential test pattern generation\n",
    "        'transformer_block'  # Parallel sequence processing\n",
    "    ],\n",
    "    'num_layers': (5, 15),\n",
    "    'channels': (64, 512),\n",
    "    'constraints': {\n",
    "        'max_latency_ms': 100,  # Real-time pattern generation\n",
    "        'max_memory_mb': 500,  # On-tester deployment\n",
    "        'min_coverage': 95.0  # Quality threshold\n",
    "    }\n",
    "}\n",
    "\n",
    "# Multi-objective optimization\n",
    "objectives = {\n",
    "    'coverage': maximize,  # Primary: Fault coverage\n",
    "    'time': minimize,  # Secondary: Pattern generation time\n",
    "    'patterns': minimize  # Tertiary: Fewer patterns = faster test\n",
    "}\n",
    "```\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- **Coverage:** 85% \u2192 95% (+10%, catch 150K more bugs/year)\n",
    "- **Time-to-market:** 6 months \u2192 2 weeks (launch products faster)\n",
    "- **Cost savings:** $500K/engineer \u00d7 5 engineers \u00d7 5 months = **$12.5M/year saved**\n",
    "- **Quality improvement:** 95% coverage \u2192 50K fewer field failures/year \u2192 **$25M/year** ($500/failure)\n",
    "\n",
    "**Business Value:** **$35M-$50M/year** (cost savings + quality improvement)\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. **Data preparation:** Collect 50K historical test patterns + coverage metrics\n",
    "2. **Search space design:** Custom operations (graph conv for circuits, attention for sequences)\n",
    "3. **Multi-objective NAS:** Optimize coverage + time + pattern count simultaneously\n",
    "4. **Transfer learning:** Pretrain on old chip families, fine-tune on new\n",
    "5. **Deployment:** Integrate with test infrastructure (tester software, STDF logging)\n",
    "6. **Validation:** Compare NAS-generated patterns vs manual (A/B test on 1000 chips)\n",
    "\n",
    "**Success Metrics:**\n",
    "- \u2705 NAS search completes in 3-7 days (vs 6 months manual)\n",
    "- \u2705 95%+ coverage on validation chips\n",
    "- \u2705 Transfer to 3+ chip families with <5% accuracy drop\n",
    "- \u2705 Deployment latency <100ms per pattern\n",
    "- \u2705 ROI >10\u00d7 (value / search cost)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 2: On-Device AI Optimization for Snapdragon** \ud83d\udcf1\n",
    "\n",
    "**Domain:** Mobile AI (Qualcomm Snapdragon, Apple Neural Engine)\n",
    "\n",
    "**Problem:**\n",
    "Deploy AI models to mobile chips with strict constraints:\n",
    "- **Latency:** <50ms per inference (user experience)\n",
    "- **Memory:** <100MB model size (limited RAM on mobile devices)\n",
    "- **Power:** <500mW (battery life, thermal management)\n",
    "- **Accuracy:** \u226595% (don't sacrifice quality for efficiency)\n",
    "\n",
    "Manual approach: Try MobileNet, EfficientNet variants (3-6 months), often miss targets.\n",
    "\n",
    "**NAS Solution:**\n",
    "Multi-objective NAS to discover architectures optimized for Snapdragon DSP/NPU hardware.\n",
    "\n",
    "**Objectives:**\n",
    "1. **Meet all constraints:** Latency <50ms, memory <100MB, power <500mW\n",
    "2. **Maximize accuracy:** \u226596% on target task (image classification, object detection)\n",
    "3. **Hardware-aware:** Optimize for Snapdragon architecture (quantization-friendly, DSP-optimized ops)\n",
    "4. **Fast search:** Complete in 2-3 days (not 6 months)\n",
    "\n",
    "**Dataset:**\n",
    "- **Task:** Image classification (ImageNet-1K, 1.2M images, 1000 classes)\n",
    "- **Hardware:** Qualcomm Snapdragon 8 Gen 3 (Hexagon NPU, Adreno GPU)\n",
    "- **Validation:** Real device (measure actual latency, not FLOPs estimate)\n",
    "\n",
    "**NAS Configuration:**\n",
    "```python\n",
    "# Hardware-aware operations (Snapdragon-optimized)\n",
    "operations = [\n",
    "    'depthwise_conv',  # Efficient on mobile GPUs (8-9\u00d7 fewer params)\n",
    "    'inverted_residual',  # MobileNet-style (channel expansion \u2192 depthwise \u2192 squeeze)\n",
    "    'squeeze_excite',  # Channel attention (minimal overhead)\n",
    "    'quantized_conv',  # INT8 operations (4\u00d7 faster on NPU)\n",
    "    'identity',  # Skip connections\n",
    "]\n",
    "\n",
    "# Multi-objective search\n",
    "objectives = {\n",
    "    'accuracy': maximize,  # Primary goal\n",
    "    'latency': minimize,  # <50ms constraint\n",
    "    'power': minimize,  # <500mW constraint\n",
    "    'memory': minimize  # <100MB constraint\n",
    "}\n",
    "\n",
    "# Hardware-aware cost model\n",
    "def estimate_latency(architecture, hardware='snapdragon8gen3'):\n",
    "    \"\"\"\n",
    "    Predict latency on Snapdragon hardware\n",
    "    \n",
    "    Uses lookup table from profiling 10K architectures on real device\n",
    "    \"\"\"\n",
    "    return hardware_cost_model.predict(architecture)\n",
    "```\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- **Accuracy:** 96.5% ImageNet (vs 95% baseline MobileNetV3)\n",
    "- **Latency:** 42ms (vs 65ms baseline, 35% faster) \u2705\n",
    "- **Power:** 450mW (vs 700mW baseline, 36% savings) \u2705\n",
    "- **Memory:** 85MB (vs 120MB baseline, 29% smaller) \u2705\n",
    "- **Battery life:** +20% (lower power consumption)\n",
    "\n",
    "**Business Value:**\n",
    "- **Market differentiation:** \"50% faster AI\" (vs competition) \u2192 +2-3% market share \u2192 **$15M-$25M/year revenue**\n",
    "- **User satisfaction:** Better experience (faster, longer battery) \u2192 Higher retention\n",
    "- **Cost savings:** No 6-month manual tuning \u2192 **$2M/year** (10 engineers \u00d7 $200K)\n",
    "\n",
    "**Total Value:** **$17M-$27M/year**\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. **Hardware profiling:** Measure latency/power for 10K architectures on Snapdragon\n",
    "2. **Cost model training:** Neural network predicts latency from architecture encoding\n",
    "3. **Multi-objective NAS:** DARTS + Pareto frontier optimization\n",
    "4. **Quantization-aware search:** Search for INT8-friendly architectures\n",
    "5. **Device deployment:** Export to ONNX \u2192 Compile with Snapdragon Neural Processing SDK\n",
    "6. **A/B testing:** Compare NAS model vs MobileNetV3 in production (100K users)\n",
    "\n",
    "**Success Metrics:**\n",
    "- \u2705 All constraints met (<50ms, <100MB, <500mW)\n",
    "- \u2705 96%+ accuracy (better than baseline)\n",
    "- \u2705 Latency verified on real Snapdragon device\n",
    "- \u2705 Deployed to 10M+ devices (production scale)\n",
    "- \u2705 User satisfaction: +5% (measured via app ratings)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 3: Wafer Defect Detection AutoML** \ud83d\udd2c\n",
    "\n",
    "**Domain:** Semiconductor Manufacturing (Intel, TSMC, Samsung foundries)\n",
    "\n",
    "**Problem:**\n",
    "Each fab has unique defect patterns (different equipment, processes, materials):\n",
    "- **Fab A:** Mostly scratch defects (CMP tool issues)\n",
    "- **Fab B:** Particle contamination (cleanroom problems)\n",
    "- **Fab C:** Pattern defects (lithography alignment errors)\n",
    "\n",
    "Current approach: One-size-fits-all model (ResNet-50) \u2192 88% recall (suboptimal for each fab)\n",
    "\n",
    "**NAS Solution:**\n",
    "Run AutoML per fab to discover custom architectures optimized for that fab's defect distribution.\n",
    "\n",
    "**Objectives:**\n",
    "1. **Custom model per fab:** Optimize for each fab's unique defect patterns\n",
    "2. **Improve recall:** 88% \u2192 95%+ (catch 7% more defects)\n",
    "3. **Fast deployment:** 1 GPU-day search (not 6 months manual tuning)\n",
    "4. **Cost-effective:** $50 compute per fab (vs $200K/engineer \u00d7 6 months)\n",
    "\n",
    "**Dataset:**\n",
    "- **Per-fab data:** 10K wafer images, 128\u00d7128 pixels, 4 classes (scratch, particle, pattern, normal)\n",
    "- **Real STDF data:** Test results + die coordinates (x, y) + defect labels\n",
    "- **Validation:** Hold out 20% per fab for architecture evaluation\n",
    "\n",
    "**NAS Configuration:**\n",
    "```python\n",
    "# Domain-specific operations\n",
    "operations = [\n",
    "    'conv3x3', 'conv5x5',\n",
    "    'sep_conv3x3',  # Efficient (mobile deployment)\n",
    "    'dil_conv3x3',  # Large receptive field (see full 128\u00d7128 layout)\n",
    "    'attention',  # Spatial attention (defect localization)\n",
    "    'max_pool', 'avg_pool',\n",
    "    'identity'\n",
    "]\n",
    "\n",
    "# Per-fab search\n",
    "for fab_id in ['fab_a', 'fab_b', 'fab_c', 'fab_d', 'fab_e']:\n",
    "    print(f\"Searching architecture for {fab_id}...\")\n",
    "    \n",
    "    dataset = load_stdf_data(fab_id)  # Real wafer test data\n",
    "    \n",
    "    model = DARTSNetwork(\n",
    "        C=16, \n",
    "        num_classes=4,  # Defect types\n",
    "        num_layers=6,\n",
    "        operations=operations\n",
    "    )\n",
    "    \n",
    "    # Search (1 GPU-day)\n",
    "    train_darts(model, train_loader, val_loader, epochs=50)\n",
    "    \n",
    "    # Evaluate\n",
    "    architecture = model.discretize()\n",
    "    final_model = build_and_train(architecture, epochs=600)\n",
    "    \n",
    "    recall = evaluate(final_model, test_loader)\n",
    "    print(f\"{fab_id}: {recall:.1%} recall (vs 88% baseline)\")\n",
    "    \n",
    "    # Deploy\n",
    "    deploy_to_fab(final_model, fab_id)\n",
    "```\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- **Recall improvement:** 88% \u2192 95% (+7% per fab)\n",
    "- **Fab A:** 96% recall (optimized for scratches)\n",
    "- **Fab B:** 97% recall (optimized for particles)\n",
    "- **Fab C:** 94% recall (optimized for patterns)\n",
    "- **Cost:** 5 fabs \u00d7 $50/fab = **$250 total** (vs $1M for engineers)\n",
    "\n",
    "**Business Value (per fab):**\n",
    "- **Defects caught:** 88% \u2192 95% \u2192 7K more defects/year\n",
    "- **Cost per defect:** $700 (scrap, rework, customer returns)\n",
    "- **Annual savings:** 7K \u00d7 $700 = **$4.9M/year per fab**\n",
    "- **Total (5 fabs):** **$24.5M/year**\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. **STDF data collection:** Extract wafer images + labels from real fab data\n",
    "2. **Per-fab training:** Run DARTS separately for each fab (parallelizable)\n",
    "3. **Architecture analysis:** Compare discovered architectures across fabs (insights into defect patterns)\n",
    "4. **Transfer learning:** Test if architecture from Fab A works on Fab B (cross-fab generalization)\n",
    "5. **Deployment:** Integrate with inspection tools (SEM, optical inspection)\n",
    "6. **Continuous learning:** Retrain quarterly as fab processes evolve\n",
    "\n",
    "**Success Metrics:**\n",
    "- \u2705 95%+ recall on all 5 fabs\n",
    "- \u2705 Search completes in <2 days per fab\n",
    "- \u2705 Architectures differ across fabs (confirms customization)\n",
    "- \u2705 Deployed to production (real-time wafer inspection)\n",
    "- \u2705 ROI: $24.5M/year / $250 = **98,000\u00d7 return**\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 4: LLM Architecture Search (GPT-Style Models)** \ud83e\udd16\n",
    "\n",
    "**Domain:** Large Language Models (General AI)\n",
    "\n",
    "**Problem:**\n",
    "GPT/LLaMA architectures are hand-designed:\n",
    "- **Layers:** 32-80 layers (GPT-3: 96 layers)\n",
    "- **Attention heads:** 16-128 heads (GPT-3: 96 heads)\n",
    "- **Hidden dimensions:** 4096-12288 (GPT-3: 12288)\n",
    "- **FFN ratio:** 4\u00d7 (hidden \u2192 4\u00d7hidden \u2192 hidden)\n",
    "\n",
    "Can NAS discover better architectures than human designers?\n",
    "\n",
    "**NAS Solution:**\n",
    "Search for optimal Transformer architecture (# layers, # heads, hidden dim, FFN ratio).\n",
    "\n",
    "**Objectives:**\n",
    "1. **Match GPT-3 performance:** \u2265GPT-3 accuracy on standard benchmarks\n",
    "2. **Reduce parameters:** 175B \u2192 <100B (43% smaller, cheaper to train/deploy)\n",
    "3. **Improve efficiency:** Faster inference (lower latency, higher throughput)\n",
    "4. **Fast search:** Complete in 100-500 GPU-days (vs 1M GPU-days for GPT-3 training)\n",
    "\n",
    "**Dataset:**\n",
    "- **Training:** Pile dataset (825GB text, diverse domains)\n",
    "- **Validation:** LAMBADA, HellaSwag, MMLU benchmarks\n",
    "- **Compute:** 100-500 A100 GPUs \u00d7 1-5 days\n",
    "\n",
    "**NAS Configuration:**\n",
    "```python\n",
    "# Search space: Transformer architecture\n",
    "search_space = {\n",
    "    'num_layers': (24, 96),  # Depth\n",
    "    'num_heads': (8, 128),  # Attention heads\n",
    "    'hidden_dim': (2048, 16384),  # Width\n",
    "    'ffn_ratio': (2, 8),  # Feedforward expansion\n",
    "    'attention_type': ['full', 'sparse', 'local', 'global'],  # Attention pattern\n",
    "    'layer_type': ['standard', 'moe', 'mixture']  # Mixture-of-experts\n",
    "}\n",
    "\n",
    "# Objectives\n",
    "objectives = {\n",
    "    'accuracy': maximize,  # Benchmark performance\n",
    "    'params': minimize,  # Model size\n",
    "    'latency': minimize,  # Inference speed\n",
    "    'training_cost': minimize  # GPU-hours to train\n",
    "}\n",
    "\n",
    "# Efficient search strategy\n",
    "# 1. Train small models (1B params) for 10K steps \u2192 Rank architectures\n",
    "# 2. Scale up top-10 to full size (100B params) \u2192 Train to convergence\n",
    "# 3. Select best based on benchmarks\n",
    "```\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- **Accuracy:** Match GPT-3 on LAMBADA (75%+), HellaSwag (80%+), MMLU (50%+)\n",
    "- **Parameters:** 175B \u2192 98B (44% reduction)\n",
    "- **Inference:** 30% faster (architectural efficiency)\n",
    "- **Training cost:** $4.6M (GPT-3) \u2192 $2.5M (NAS model, 46% cheaper)\n",
    "\n",
    "**Business Value:**\n",
    "- **Training savings:** $4.6M - $2.5M = **$2.1M per training run**\n",
    "- **Inference savings:** 30% faster \u2192 30% lower cloud costs \u2192 **$500K/year** (at scale)\n",
    "- **Competitive advantage:** Better model than GPT-3 \u2192 Market differentiation\n",
    "- **Open-source impact:** Democratize LLM research (smaller models accessible to academia)\n",
    "\n",
    "**Total Value:** **$2.6M/year** (one-time training + ongoing inference)\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. **Small-scale search:** Train 1B-param models for 10K steps each (100 architectures)\n",
    "2. **Ranking:** Evaluate on validation set, select top-10\n",
    "3. **Scaling:** Train top-10 to full size (100B params, 1M steps each)\n",
    "4. **Benchmarking:** Evaluate on LAMBADA, HellaSwag, MMLU, BIG-bench\n",
    "5. **Analysis:** Ablation studies (which architectural choices matter most?)\n",
    "6. **Open-source:** Release architecture + weights for research community\n",
    "\n",
    "**Success Metrics:**\n",
    "- \u2705 Match or exceed GPT-3 on 3+ benchmarks\n",
    "- \u2705 <100B parameters (smaller than GPT-3)\n",
    "- \u2705 20-30% faster inference\n",
    "- \u2705 Search completes in 500 GPU-days (vs 1M for GPT-3 training)\n",
    "- \u2705 1000+ citations (research impact)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 5: Neural Accelerator Architecture Search** \ud83d\udd27\n",
    "\n",
    "**Domain:** Hardware Design (Qualcomm NPU, Google TPU, Apple Neural Engine)\n",
    "\n",
    "**Problem:**\n",
    "Design optimal neural accelerator hardware:\n",
    "- **Operations:** Matrix multiply, activation functions, pooling\n",
    "- **Memory hierarchy:** L1 cache, L2 cache, DRAM bandwidth\n",
    "- **Parallelism:** # of MACs (multiply-accumulate units), pipelining\n",
    "- **Power:** Watts per operation\n",
    "\n",
    "Current approach: Manual design (2-3 years per generation), suboptimal trade-offs.\n",
    "\n",
    "**NAS Solution:**\n",
    "Co-design NAS: Simultaneously optimize neural network architecture AND hardware architecture.\n",
    "\n",
    "**Objectives:**\n",
    "1. **Maximize throughput:** TOPS (tera-operations per second)\n",
    "2. **Minimize power:** Watts per inference\n",
    "3. **Minimize area:** mm\u00b2 silicon (cost per chip)\n",
    "4. **Meet latency targets:** <10ms per inference (real-time applications)\n",
    "\n",
    "**Search Space:**\n",
    "```python\n",
    "# Neural network architecture\n",
    "nn_search_space = {\n",
    "    'operations': ['conv', 'depthwise_conv', 'matmul', 'attention'],\n",
    "    'num_layers': (5, 50),\n",
    "    'channels': (16, 512)\n",
    "}\n",
    "\n",
    "# Hardware architecture\n",
    "hw_search_space = {\n",
    "    'num_macs': (128, 4096),  # Multiply-accumulate units\n",
    "    'l1_cache_kb': (16, 256),\n",
    "    'l2_cache_kb': (256, 8192),\n",
    "    'dram_bandwidth_gbps': (50, 500),\n",
    "    'clock_mhz': (500, 2000),\n",
    "    'bit_width': [8, 16, 32]  # Quantization\n",
    "}\n",
    "\n",
    "# Co-optimization\n",
    "objectives = {\n",
    "    'throughput': maximize,  # TOPS\n",
    "    'power': minimize,  # Watts\n",
    "    'area': minimize,  # mm\u00b2\n",
    "    'latency': minimize  # ms\n",
    "}\n",
    "```\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- **Throughput:** 50 TOPS (vs 35 TOPS baseline, +43%)\n",
    "- **Power:** 5W (vs 8W baseline, -37%)\n",
    "- **Area:** 45 mm\u00b2 (vs 60 mm\u00b2, -25% cost)\n",
    "- **Latency:** 8ms (vs 12ms, 33% faster) \u2705\n",
    "\n",
    "**Business Value:**\n",
    "- **Performance advantage:** 50 TOPS vs competition's 35 TOPS \u2192 Marketing edge\n",
    "- **Cost reduction:** 25% smaller die \u2192 **$50/chip savings** \u00d7 10M chips/year = **$500M/year**\n",
    "- **Power efficiency:** 37% lower power \u2192 Longer battery life \u2192 Product differentiation\n",
    "\n",
    "**Total Value:** **$500M+/year** (cost reduction for high-volume chips)\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. **Hardware simulator:** Build cycle-accurate simulator for accelerator (estimate latency, power, area)\n",
    "2. **Co-design NAS:** Jointly optimize NN + HW architectures\n",
    "3. **Pareto frontier:** Generate multiple designs (high-performance vs low-power vs low-cost)\n",
    "4. **RTL generation:** Convert discovered HW architecture to Verilog\n",
    "5. **Fabrication:** Tape out prototype chip (6-12 months)\n",
    "6. **Validation:** Measure real chip (throughput, power, latency)\n",
    "\n",
    "**Success Metrics:**\n",
    "- \u2705 40+ TOPS throughput (state-of-art)\n",
    "- \u2705 <6W power consumption\n",
    "- \u2705 <50 mm\u00b2 area (manufacturable)\n",
    "- \u2705 Tapeout successful (chip works on first silicon)\n",
    "- \u2705 Production deployment (10M+ chips/year)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 6: Recommender System Architecture Search (Netflix, Amazon)** \ud83c\udfac\n",
    "\n",
    "**Domain:** Recommendation Systems\n",
    "\n",
    "**Problem:**\n",
    "Design optimal neural network for recommendations:\n",
    "- **Input:** User history (watch history, ratings), item features (genre, actors, etc.)\n",
    "- **Output:** Top-K recommendations (personalized)\n",
    "- **Scale:** 200M users, 10K items, real-time inference (<50ms)\n",
    "\n",
    "Manual architecture: Multi-layer perceptron (MLP) \u2192 85% accuracy\n",
    "\n",
    "**NAS Solution:**\n",
    "Search for architecture optimized for recommendation task (handle sparse features, capture user-item interactions).\n",
    "\n",
    "**Objectives:**\n",
    "1. **Improve accuracy:** 85% \u2192 90%+ (better recommendations \u2192 higher engagement)\n",
    "2. **Reduce latency:** <50ms (real-time personalization)\n",
    "3. **Handle sparsity:** User history is sparse (most users watch <100 movies out of 10K)\n",
    "4. **Scalability:** Deploy to 200M users (production scale)\n",
    "\n",
    "**Dataset:**\n",
    "- **Netflix Prize:** 100M ratings, 480K users, 17K movies\n",
    "- **Features:** User demographics, movie genre, actors, directors, watch history\n",
    "- **Validation:** Hold out 10% for architecture evaluation\n",
    "\n",
    "**NAS Configuration:**\n",
    "```python\n",
    "# Operations for recommender systems\n",
    "operations = [\n",
    "    'embedding',  # Categorical features (user_id, movie_id)\n",
    "    'mlp',  # Feedforward layers\n",
    "    'attention',  # User-item attention (which movies are most relevant?)\n",
    "    'factorization_machine',  # Capture 2nd-order interactions\n",
    "    'cross_product',  # Explicit feature crosses\n",
    "    'lstm',  # Sequential history (watch order matters)\n",
    "]\n",
    "\n",
    "search_space = {\n",
    "    'embedding_dim': (32, 512),\n",
    "    'num_layers': (3, 10),\n",
    "    'hidden_dim': (128, 2048),\n",
    "    'interaction_type': ['dot', 'cosine', 'mlp']  # User-item scoring\n",
    "}\n",
    "\n",
    "objectives = {\n",
    "    'accuracy': maximize,  # Recommendation accuracy\n",
    "    'latency': minimize,  # Inference time\n",
    "    'memory': minimize  # Model size (for caching)\n",
    "}\n",
    "```\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- **Accuracy:** 85% \u2192 92% (+7%, better recommendations)\n",
    "- **Latency:** 45ms (vs 60ms baseline, 25% faster)\n",
    "- **Engagement:** +5% watch time (users watch more recommended content)\n",
    "\n",
    "**Business Value:**\n",
    "- **Engagement:** 5% more watch time \u2192 5% more ad revenue \u2192 **$50M-$100M/year** (Netflix scale)\n",
    "- **Retention:** Better recommendations \u2192 Lower churn \u2192 **$20M-$40M/year** (saved subscribers)\n",
    "- **Compute savings:** 25% faster \u2192 25% fewer servers \u2192 **$5M-$10M/year** (AWS costs)\n",
    "\n",
    "**Total Value:** **$75M-$150M/year**\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 7: Medical Imaging AutoML (Radiology, Pathology)** \ud83c\udfe5\n",
    "\n",
    "**Domain:** Medical AI\n",
    "\n",
    "**Problem:**\n",
    "Each medical imaging modality has unique characteristics:\n",
    "- **X-ray:** 2D, bone/tissue contrast\n",
    "- **CT:** 3D, cross-sectional slices\n",
    "- **MRI:** 3D, soft tissue detail\n",
    "- **Pathology:** Microscopy, cellular structures\n",
    "\n",
    "One-size-fits-all models (ResNet) achieve 85-90% accuracy. Can NAS do better?\n",
    "\n",
    "**NAS Solution:**\n",
    "AutoML per modality to discover custom architectures.\n",
    "\n",
    "**Objectives:**\n",
    "1. **Improve accuracy:** 85% \u2192 95%+ (catch more diseases)\n",
    "2. **Reduce false positives:** 20% \u2192 5% (fewer unnecessary biopsies)\n",
    "3. **Fast deployment:** 1-2 days search per modality\n",
    "4. **Regulatory compliance:** Explainability (FDA approval requires interpretability)\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- **Accuracy:** 85% \u2192 96% (+11%)\n",
    "- **Lives saved:** 1000+ per year (earlier diagnosis)\n",
    "- **Cost savings:** $10K/false positive \u00d7 15% reduction \u2192 **$150M/year** (US healthcare)\n",
    "\n",
    "**Business Value:** **$150M+/year** (healthcare system savings)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 8: Autonomous Driving Perception NAS** \ud83d\ude97\n",
    "\n",
    "**Domain:** Self-Driving Cars (Tesla, Waymo, Cruise)\n",
    "\n",
    "**Problem:**\n",
    "Perception systems for autonomous driving:\n",
    "- **Inputs:** Camera (8\u00d7 1080p), LiDAR (64-128 channels), Radar\n",
    "- **Output:** Object detection, segmentation, tracking\n",
    "- **Requirements:** <50ms latency, 99.99% accuracy (safety-critical)\n",
    "\n",
    "Manual architectures: BEVFormer, PointPillars \u2192 95% accuracy\n",
    "\n",
    "**NAS Solution:**\n",
    "Multi-modal NAS to fuse camera + LiDAR + radar optimally.\n",
    "\n",
    "**Objectives:**\n",
    "1. **Improve accuracy:** 95% \u2192 99%+ (safety)\n",
    "2. **Meet latency:** <50ms (real-time perception)\n",
    "3. **Optimize for hardware:** Deploy to Tesla FSD Computer (72 TOPS)\n",
    "4. **Multi-modal fusion:** Learn optimal way to combine sensors\n",
    "\n",
    "**Expected Outcomes:**\n",
    "- **Accuracy:** 95% \u2192 99.2% (+4.2%)\n",
    "- **Latency:** 38ms (vs 55ms, 31% faster)\n",
    "- **Safety:** 5\u00d7 fewer accidents (higher perception accuracy)\n",
    "\n",
    "**Business Value:**\n",
    "- **Safety:** Prevent 1000+ accidents/year \u2192 **Priceless** (lives saved)\n",
    "- **Regulatory:** 99%+ accuracy \u2192 Faster regulatory approval\n",
    "- **Market:** First to market \u2192 **$1B+ revenue** (autonomous taxi service)\n",
    "\n",
    "**Total Value:** **$1B+** (market advantage + safety)\n",
    "\n",
    "---\n",
    "\n",
    "## \u2705 Key Takeaways: When and How to Use NAS\n",
    "\n",
    "### **What You've Mastered**\n",
    "\n",
    "By completing this notebook, you now understand:\n",
    "\n",
    "1. \u2705 **NAS Problem Formulation:** Search space (what architectures?), search strategy (how to find?), evaluation (how to measure?)\n",
    "2. \u2705 **Three Major Algorithms:**\n",
    "   - **NASNet (RL):** Policy gradient, 22,400 GPU-days, 82.7% ImageNet\n",
    "   - **ENAS (Weight sharing):** Supernet, 1000\u00d7 speedup, 97.3% CIFAR-10\n",
    "   - **DARTS (Gradient-based):** Continuous relaxation, 1 GPU-day, 97.0% CIFAR-10\n",
    "3. \u2705 **Implementation:** Complete DARTS code from scratch (<500 lines)\n",
    "4. \u2705 **Applications:** Chip verification ($20M-$40M/year), mobile AI ($10M-$20M/year), wafer inspection ($5M-$15M/year)\n",
    "5. \u2705 **Business Value:** How to quantify ROI for NAS projects\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83c\udfaf When to Use NAS (Decision Framework)**\n",
    "\n",
    "| Scenario | Use NAS? | Rationale | Alternative |\n",
    "|----------|----------|-----------|-------------|\n",
    "| **New domain** (chip verification, medical imaging) | \u2705 **Yes** | NAS discovers domain-specific patterns (manual design may miss) | Pretrained ResNet-50 + fine-tuning |\n",
    "| **Strict constraints** (latency <50ms, memory <100MB, power <500mW) | \u2705 **Yes** | Multi-objective NAS optimizes trade-offs | Manual architecture tuning (3-6 months) |\n",
    "| **Large dataset** (100K+ samples) | \u2705 **Yes** | NAS needs data to differentiate architectures | N/A |\n",
    "| **Compute budget** (100-1000 GPU-days available) | \u2705 **Yes** | DARTS (1 day), ENAS (0.67 days), NASNet (22K days) | N/A |\n",
    "| **Production deployment** (millions of users) | \u2705 **Yes** | Even 1% improvement \u2192 Huge business value | N/A |\n",
    "| **Standard task** (ImageNet classification) | \u274c **No** | Pretrained models already optimal (EfficientNet, ResNet) | EfficientNet-B7 (84.3% ImageNet) |\n",
    "| **Small dataset** (<10K samples) | \u274c **No** | NAS overfits, transfer learning better | Pretrained model + fine-tuning |\n",
    "| **Limited compute** (<10 GPU-days) | \u26a0\ufe0f **Maybe** | Use DARTS (1 day) or ENAS (0.67 days), skip NASNet | Manual design |\n",
    "| **Interpretability required** (healthcare, finance) | \u26a0\ufe0f **Maybe** | NAS architectures less interpretable (may fail regulatory review) | Manual design + explainability |\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83d\ude80 NAS Implementation Workflow**\n",
    "\n",
    "**Step 1: Problem Definition (1-2 days)**\n",
    "```python\n",
    "# Define objectives\n",
    "objectives = {\n",
    "    'accuracy': maximize,\n",
    "    'latency': minimize,  # <50ms constraint\n",
    "    'memory': minimize,  # <100MB constraint\n",
    "    'power': minimize  # <500mW constraint\n",
    "}\n",
    "\n",
    "# Define constraints\n",
    "constraints = {\n",
    "    'max_latency_ms': 50,\n",
    "    'max_memory_mb': 100,\n",
    "    'max_power_mw': 500,\n",
    "    'min_accuracy': 95.0\n",
    "}\n",
    "```\n",
    "\n",
    "**Step 2: Search Space Design (2-3 days)**\n",
    "```python\n",
    "# Domain-specific operations\n",
    "if domain == 'chip_verification':\n",
    "    operations = ['conv3x3', 'conv5x5', 'dil_conv3x3', 'attention', 'graph_conv']\n",
    "elif domain == 'mobile':\n",
    "    operations = ['depthwise_conv', 'inverted_residual', 'squeeze_excite']\n",
    "elif domain == 'llm':\n",
    "    operations = ['full_attention', 'sparse_attention', 'moe', 'standard_ffn']\n",
    "```\n",
    "\n",
    "**Step 3: Algorithm Selection (1 day)**\n",
    "```python\n",
    "if compute_budget > 10000:\n",
    "    algorithm = 'NASNet'  # Best accuracy, expensive\n",
    "elif compute_budget > 100:\n",
    "    algorithm = 'ENAS'  # Good accuracy, efficient\n",
    "else:\n",
    "    algorithm = 'DARTS'  # Fast, gradient-based\n",
    "```\n",
    "\n",
    "**Step 4: Search (0.67-22,400 GPU-days)**\n",
    "```python\n",
    "model = DARTSNetwork(C=16, num_classes=num_classes, operations=operations)\n",
    "train_darts(model, train_loader, val_loader, epochs=50)\n",
    "architecture = model.discretize(k=2)\n",
    "```\n",
    "\n",
    "**Step 5: Retraining (1-7 days)**\n",
    "```python\n",
    "final_model = build_from_architecture(architecture)\n",
    "train_from_scratch(final_model, epochs=600)\n",
    "```\n",
    "\n",
    "**Step 6: Validation (1-2 days)**\n",
    "```python\n",
    "test_accuracy = evaluate(final_model, test_loader)\n",
    "latency = measure_latency(final_model, hardware='snapdragon8gen3')\n",
    "memory = model.size() / 1e6  # MB\n",
    "power = estimate_power(final_model)\n",
    "\n",
    "assert test_accuracy >= constraints['min_accuracy']\n",
    "assert latency <= constraints['max_latency_ms']\n",
    "assert memory <= constraints['max_memory_mb']\n",
    "assert power <= constraints['max_power_mw']\n",
    "```\n",
    "\n",
    "**Step 7: Deployment (1-4 weeks)**\n",
    "```python\n",
    "# Export to production format\n",
    "torch.onnx.export(final_model, 'model.onnx')\n",
    "\n",
    "# Quantize for mobile deployment\n",
    "quantized_model = torch.quantization.quantize_dynamic(final_model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "# Deploy to device\n",
    "deploy_to_snapdragon(quantized_model)\n",
    "```\n",
    "\n",
    "**Step 8: Monitoring (Ongoing)**\n",
    "```python\n",
    "# Track metrics in production\n",
    "metrics = {\n",
    "    'accuracy': 96.5%,  # A/B test vs baseline\n",
    "    'latency': 42ms,  # P99 latency\n",
    "    'error_rate': 0.05%,  # Production errors\n",
    "    'user_satisfaction': 4.7/5.0  # App ratings\n",
    "}\n",
    "\n",
    "# Retrain quarterly as data distribution shifts\n",
    "if metrics['accuracy'] < 95.0:\n",
    "    retrain(model, new_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **\u26a0\ufe0f Common Pitfalls and How to Avoid**\n",
    "\n",
    "**1. Search-Evaluation Gap**\n",
    "- **Problem:** Architecture performs well during search but poorly when retrained from scratch\n",
    "- **Cause:** Weight sharing bias (ENAS), early stopping noise (NASNet)\n",
    "- **Solution:** Use multiple evaluation strategies (weight sharing + early stopping + train from scratch for top-10)\n",
    "\n",
    "**2. Overfitting to Validation Set**\n",
    "- **Problem:** Architecture optimized for validation set, poor test accuracy\n",
    "- **Cause:** Search uses validation set for architecture updates (data leakage)\n",
    "- **Solution:** Use separate search validation set + final test set (never seen during search)\n",
    "\n",
    "**3. Expensive Search Space**\n",
    "- **Problem:** 10^50 architectures, intractable to search\n",
    "- **Cause:** Global search space (entire network structure)\n",
    "- **Solution:** Use cell-based search space (10^6 architectures, transferable)\n",
    "\n",
    "**4. Ignoring Hardware Constraints**\n",
    "- **Problem:** Discovered architecture has 200ms latency (vs <50ms target)\n",
    "- **Cause:** Search optimizes accuracy only, ignores latency/memory/power\n",
    "- **Solution:** Multi-objective NAS (Pareto frontier optimization)\n",
    "\n",
    "**5. Poor Transfer Learning**\n",
    "- **Problem:** Architecture discovered on CIFAR-10 fails on ImageNet\n",
    "- **Cause:** Dataset size mismatch, resolution difference\n",
    "- **Solution:** Search on proxy task, validate on target task, adjust if needed\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83c\udf93 Advanced Topics (Beyond This Notebook)**\n",
    "\n",
    "**1. Efficient Attention for NAS**\n",
    "- **Problem:** Quadratic complexity O(n\u00b2) for self-attention\n",
    "- **Solutions:** \n",
    "  - **Sparse attention** (Longformer): O(n log n)\n",
    "  - **Low-rank attention** (Linformer): O(n k), k << n\n",
    "  - **Kernelized attention** (Performer): O(n d)\n",
    "- **Use case:** LLM architecture search (GPT-style models)\n",
    "\n",
    "**2. Hardware-Aware NAS**\n",
    "- **Problem:** FLOPs \u2260 latency (different hardware has different bottlenecks)\n",
    "- **Solution:** Build latency predictor from profiling 10K architectures on target hardware\n",
    "- **Tools:** TensorRT (NVIDIA), ONNX Runtime (Microsoft), Snapdragon NPE (Qualcomm)\n",
    "\n",
    "**3. Once-for-All Networks (OFA)**\n",
    "- **Problem:** Need different architectures for different devices (phone vs tablet vs laptop)\n",
    "- **Solution:** Train one supernet, deploy sub-networks for each device\n",
    "- **Benefit:** Train once, deploy anywhere (no per-device NAS search)\n",
    "\n",
    "**4. Neural Architecture Transfer**\n",
    "- **Problem:** Search on CIFAR-10 (cheap), deploy to ImageNet (expensive)\n",
    "- **Solution:** Search cell on small dataset, transfer to large dataset\n",
    "- **Validation:** NASNet cell (CIFAR-10) \u2192 82.7% ImageNet (transferred successfully)\n",
    "\n",
    "**5. Multi-Objective NAS (Pareto Frontier)**\n",
    "- **Problem:** Optimize accuracy + latency + power simultaneously (conflicting objectives)\n",
    "- **Solution:** NSGA-II (evolutionary algorithm for multi-objective optimization)\n",
    "- **Output:** Pareto frontier (multiple optimal architectures, user picks trade-off)\n",
    "\n",
    "**6. Predictor-Based NAS**\n",
    "- **Problem:** Evaluating 20,000 architectures is expensive (even with weight sharing)\n",
    "- **Solution:** Train neural network predictor (architecture \u2192 accuracy), search in predictor space\n",
    "- **Benefit:** Evaluate 10,000 architectures instantly (predictor inference)\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83d\udcda Learning Path: Next Steps**\n",
    "\n",
    "**Week 1-2: Implement DARTS on Your Dataset**\n",
    "```python\n",
    "# 1. Choose your dataset (CIFAR-10, ImageNet, or domain-specific)\n",
    "# 2. Define search space (operations, constraints)\n",
    "# 3. Run DARTS (1 GPU-day)\n",
    "# 4. Discretize architecture\n",
    "# 5. Retrain from scratch\n",
    "# 6. Compare vs baseline (ResNet-50, MobileNet)\n",
    "\n",
    "model = DARTSNetwork(C=16, num_classes=10)\n",
    "train_darts(model, train_loader, val_loader, epochs=50)\n",
    "architecture = model.discretize()\n",
    "```\n",
    "\n",
    "**Week 3-4: Multi-Objective NAS**\n",
    "```python\n",
    "# 1. Define objectives (accuracy + latency + memory)\n",
    "# 2. Hardware profiling (measure latency for 1000 architectures)\n",
    "# 3. Train cost predictor (architecture \u2192 latency)\n",
    "# 4. Multi-objective search (NSGA-II or weighted sum)\n",
    "# 5. Pareto frontier analysis\n",
    "\n",
    "objectives = {\n",
    "    'accuracy': maximize,\n",
    "    'latency': minimize,\n",
    "    'memory': minimize\n",
    "}\n",
    "\n",
    "pareto_frontier = multi_objective_nas(objectives)\n",
    "```\n",
    "\n",
    "**Week 5-6: Deploy to Production**\n",
    "```python\n",
    "# 1. Export to ONNX (interoperability)\n",
    "# 2. Quantize to INT8 (4\u00d7 speedup)\n",
    "# 3. Compile for target hardware (TensorRT, Snapdragon NPE)\n",
    "# 4. A/B test vs baseline (measure real metrics)\n",
    "# 5. Monitor in production (latency, accuracy, error rate)\n",
    "\n",
    "torch.onnx.export(model, 'model.onnx')\n",
    "quantized = quantize_int8(model)\n",
    "deploy_to_device(quantized)\n",
    "```\n",
    "\n",
    "**Month 2: Domain-Specific NAS**\n",
    "```python\n",
    "# Apply to your domain:\n",
    "# - Chip verification: Graph conv + attention for circuits\n",
    "# - Mobile: Depthwise conv + squeeze-excite for efficiency\n",
    "# - LLM: Sparse attention + MoE for scalability\n",
    "# - Medical: 3D conv for CT/MRI, attention for pathology\n",
    "# - Autonomous: Multi-modal fusion for camera + LiDAR\n",
    "```\n",
    "\n",
    "**Month 3: Research Contributions**\n",
    "```python\n",
    "# Push the field forward:\n",
    "# 1. Novel search space (new operations, constraints)\n",
    "# 2. Faster search strategy (predictor-based, zero-cost proxies)\n",
    "# 3. Better evaluation (correlation studies, transferability)\n",
    "# 4. Real-world deployment (measure business value, ROI)\n",
    "# 5. Open-source release (reproducible research)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83c\udfaf Success Criteria: You've Mastered NAS When...**\n",
    "\n",
    "- [ ] You can explain NAS in 3 sentences to a non-expert\n",
    "- [ ] You've implemented DARTS from scratch (<500 lines PyTorch)\n",
    "- [ ] You've run NAS on your own dataset (1 GPU-day search)\n",
    "- [ ] You've discovered an architecture better than baseline (even +1% is success!)\n",
    "- [ ] You've deployed NAS model to production (real users, real metrics)\n",
    "- [ ] You can quantify business value ($XM/year ROI)\n",
    "- [ ] You understand trade-offs (NASNet vs ENAS vs DARTS)\n",
    "- [ ] You know when NOT to use NAS (small dataset, standard task)\n",
    "- [ ] You've read 3+ NAS papers (NASNet, ENAS, DARTS minimum)\n",
    "- [ ] You can design custom search space for your domain\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83d\udcd6 Essential Resources**\n",
    "\n",
    "**Foundational Papers:**\n",
    "1. **NASNet (2016):** \"Neural Architecture Search with Reinforcement Learning\" - Zoph & Le\n",
    "2. **ENAS (2017):** \"Efficient Neural Architecture Search via Parameter Sharing\" - Pham et al.\n",
    "3. **DARTS (2018):** \"DARTS: Differentiable Architecture Search\" - Liu et al.\n",
    "4. **AmoebaNet (2018):** \"Regularized Evolution for Image Classifier Architecture Search\" - Real et al.\n",
    "5. **EfficientNet (2019):** \"EfficientNet: Rethinking Model Scaling for CNNs\" - Tan & Le\n",
    "\n",
    "**Advanced Papers:**\n",
    "6. **Once-for-All (2020):** \"Once-for-All: Train One Network and Specialize it for Efficient Deployment\"\n",
    "7. **AutoFormer (2021):** \"Searching the Search Space of Vision Transformer\"\n",
    "8. **HAT (2022):** \"Hardware-Aware Transformers for Efficient Natural Language Processing\"\n",
    "9. **NAS-Bench-101 (2019):** \"NAS-Bench-101: Towards Reproducible Neural Architecture Search\"\n",
    "\n",
    "**Tutorials & Code:**\n",
    "- **PyTorch NAS Tutorial:** https://pytorch.org/tutorials/intermediate/neural_architecture_search.html\n",
    "- **NASLib (open-source):** https://github.com/automl/NASLib\n",
    "- **DARTS GitHub:** https://github.com/quark0/darts\n",
    "- **Once-for-All GitHub:** https://github.com/mit-han-lab/once-for-all\n",
    "\n",
    "**Courses:**\n",
    "- **CS224N (Stanford):** Week on AutoML and NAS\n",
    "- **CS285 (Berkeley):** Deep RL for NAS\n",
    "- **Fast.ai:** Practical NAS for practitioners\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83d\udcb0 Business Value Summary**\n",
    "\n",
    "| Application | Annual Value | ROI | Key Metric |\n",
    "|-------------|--------------|-----|------------|\n",
    "| **Chip Verification** | $20M-$40M/year | 1,000,000\u00d7 | 91% detection vs 78% |\n",
    "| **Mobile AI** | $10M-$20M/year | 500,000\u00d7 | 45ms latency vs 75ms |\n",
    "| **Wafer Inspection** | $5M-$15M/year | 98,000\u00d7 | 95% recall vs 88% |\n",
    "| **LLM Architecture** | $2.6M/year | 1,000\u00d7 | 98B params vs 175B |\n",
    "| **Neural Accelerator** | $500M/year | 10,000\u00d7 | 50 TOPS vs 35 TOPS |\n",
    "| **Recommender System** | $75M-$150M/year | 5,000\u00d7 | 92% accuracy vs 85% |\n",
    "| **Medical Imaging** | $150M/year | 100,000\u00d7 | 96% accuracy vs 85% |\n",
    "| **Autonomous Driving** | $1B+/year | \u221e | 99.2% accuracy (safety) |\n",
    "\n",
    "**Total Potential:** **$750M-$1.9B/year** across all applications\n",
    "\n",
    "**Key Insight:** NAS ROI is 1000-1,000,000\u00d7 because:\n",
    "- Search cost: $24-$50K (one-time)\n",
    "- Business value: $5M-$500M/year (ongoing)\n",
    "- Deployment scale: Millions of users/devices\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83c\udf93 Final Thoughts**\n",
    "\n",
    "Neural Architecture Search represents a fundamental shift in how we design AI systems:\n",
    "\n",
    "**Before NAS (Pre-2016):**\n",
    "- Human intuition (AlexNet, ResNet, Transformer)\n",
    "- 6-12 months per breakthrough\n",
    "- Suboptimal (limited by human creativity)\n",
    "\n",
    "**After NAS (2016+):**\n",
    "- Algorithmic search (NASNet, EfficientNet, discovered architectures)\n",
    "- 1-7 days per architecture\n",
    "- Superhuman (explores 10,000-1,000,000 architectures)\n",
    "\n",
    "**The Future (2025+):**\n",
    "- **Foundation Model NAS:** Search architectures for GPT-5, Gemini, Claude\n",
    "- **Hardware Co-Design:** Jointly optimize NN + chip architecture\n",
    "- **Continuous NAS:** Architectures evolve as data distribution shifts\n",
    "- **Multi-Modal NAS:** Discover optimal fusion of vision + language + audio\n",
    "- **Neuromorphic NAS:** Search architectures for spiking neural networks (brain-inspired)\n",
    "\n",
    "**Your Opportunity:**\n",
    "You now have the knowledge to apply NAS to YOUR domain and unlock $XM-$YM/year business value. The limiting factor is no longer search algorithms (DARTS solves that), but identifying high-value applications.\n",
    "\n",
    "**Go build something amazing!** \ud83d\ude80\n",
    "\n",
    "---\n",
    "\n",
    "**Next Notebook:** 068_Model_Compression_and_Quantization.ipynb\n",
    "- Pruning (remove 90% of weights, keep 98% accuracy)\n",
    "- Quantization (INT8, 4\u00d7 speedup)\n",
    "- Knowledge distillation (compress GPT-3 \u2192 1/10 size)\n",
    "- Deployment optimization (TensorRT, ONNX, CoreML)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}