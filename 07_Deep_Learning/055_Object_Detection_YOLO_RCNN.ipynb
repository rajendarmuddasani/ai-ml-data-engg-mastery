{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14bb5c14",
   "metadata": {},
   "source": [
    "# 055: Object Detection (YOLO, R-CNN)",
    "",
    "## \ud83d\udcda Learning Objectives",
    "",
    "By the end of this notebook, you will master:",
    "",
    "1. **Object Detection Fundamentals** - Bounding boxes, IoU, anchor boxes, multi-task learning",
    "2. **Two-Stage Detectors** - R-CNN evolution (R-CNN \u2192 Fast R-CNN \u2192 Faster R-CNN \u2192 Mask R-CNN)",
    "3. **Single-Stage Detectors** - YOLO architecture (YOLOv3, YOLOv5, YOLOv8), SSD, RetinaNet",
    "4. **Loss Functions** - Localization loss (L1, IoU, GIoU, DIoU), classification loss, focal loss",
    "5. **Non-Maximum Suppression (NMS)** - Post-processing, soft-NMS, DIoU-NMS",
    "6. **Evaluation Metrics** - mAP (mean Average Precision), IoU thresholds, COCO metrics",
    "7. **Real-Time Detection** - FPS optimization, model quantization, edge deployment",
    "8. **Semiconductor Applications** - PCB defect localization, die-level component detection, wafer map spatial analysis",
    "",
    "---",
    "",
    "## \ud83c\udfaf Why Object Detection Matters",
    "",
    "### Classification vs Detection vs Segmentation",
    "",
    "```",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
    "\u2502 IMAGE UNDERSTANDING TASKS                                    \u2502",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524",
    "\u2502                                                              \u2502",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502",
    "\u2502  \u2502 CLASSIFICATION   \u2502  \u2502 DETECTION        \u2502  \u2502 SEGMENTATION\u2502",
    "\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502",
    "\u2502  \u2502 What?            \u2502  \u2502 What + Where?    \u2502  \u2502 Pixel-wise\u2502 \u2502",
    "\u2502  \u2502                  \u2502  \u2502                  \u2502  \u2502 masks     \u2502 \u2502",
    "\u2502  \u2502 [Dog]            \u2502  \u2502 [Dog] @ (x,y,w,h)\u2502  \u2502 [Outline] \u2502 \u2502",
    "\u2502  \u2502                  \u2502  \u2502 [Cat] @ (x,y,w,h)\u2502  \u2502 [Exact]   \u2502 \u2502",
    "\u2502  \u2502 Single label     \u2502  \u2502 Multiple objects \u2502  \u2502 Boundaries\u2502 \u2502",
    "\u2502  \u2502                  \u2502  \u2502 + Bounding boxes \u2502  \u2502 per pixel \u2502 \u2502",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502",
    "\u2502                                                              \u2502",
    "\u2502  Notebook 053          Notebook 055 (THIS)  Notebook 058   \u2502",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
    "```",
    "",
    "**Object Detection = Classification + Localization**",
    "- **Input:** Image (H \u00d7 W \u00d7 3)",
    "- **Output:** ",
    "  - N bounding boxes: $(x, y, w, h)$ coordinates",
    "  - N class labels: $c \\in \\{1, 2, \\ldots, C\\}$",
    "  - N confidence scores: $p \\in [0, 1]$",
    "",
    "---",
    "",
    "## \ud83d\udcbc Business Value for Semiconductor Industry",
    "",
    "### Use Case 1: PCB Defect Localization (vs Classification)",
    "",
    "**Previous approach (Notebook 053):** Classify entire image as \"defective\" or \"normal\"",
    "- **Problem:** Can't pinpoint defect location \u2192 Manual inspection still required",
    "",
    "**Object detection solution:** Detect and localize specific defects on PCB",
    "- **Output:** \"Scratch @ (x=245, y=180, w=50, h=10), confidence=0.95\"",
    "- **Business Impact:** $5M-$20M/year from automated defect localization",
    "- **Time Saved:** 80% reduction in manual inspection (from 30 sec/PCB \u2192 6 sec)",
    "",
    "**Defect Types Detected:**",
    "1. Solder bridges (shorts between pins)",
    "2. Missing components (resistors, capacitors)",
    "3. Misaligned ICs",
    "4. Scratches on traces",
    "5. Contamination particles",
    "6. Cold solder joints",
    "",
    "---",
    "",
    "### Use Case 2: Die-Level Component Detection for Test Coverage",
    "",
    "**Problem:** Verify test probe placement on die (1000+ test points, 5mm \u00d7 5mm die)",
    "",
    "**Solution:** YOLOv8 detects all 1000+ probe pads in <50ms",
    "- **Input:** High-resolution die image (4096\u00d74096 \u2192 640\u00d7640 resize)",
    "- **Output:** Bounding boxes around each probe pad + classification (analog/digital/power)",
    "- **Business Impact:** $10M-$30M/year from reduced test escapes + faster test program validation",
    "- **Accuracy:** mAP@0.5 \u2265 0.98 (99%+ probe pads detected)",
    "",
    "---",
    "",
    "### Use Case 3: Wafer Map Spatial Defect Analysis",
    "",
    "**Previous approach (Notebooks 053-054):** Classify wafer map pattern (e.g., \"center cluster\")",
    "- **Limitation:** Can't identify multiple simultaneous defect clusters",
    "",
    "**Object detection solution:** Detect multiple defect regions on single wafer map",
    "- **Example:** \"Cluster 1 @ center (30 dies), Cluster 2 @ edge (15 dies), Scratch @ (x=200, y=400)\"",
    "- **Business Impact:** $20M-$80M/year from precise root-cause analysis",
    "- **Semiconductor benefit:** Correlate defect locations with process steps (lithography zones, etching patterns)",
    "",
    "---",
    "",
    "## \ud83c\udfd7\ufe0f What We'll Build",
    "",
    "### 1. **PCB Defect Detector (YOLOv8)**",
    "- **Task:** Detect 6 defect types on PCB images",
    "- **Dataset:** Synthetic PCB images with labeled defects (2000 images)",
    "- **Model:** YOLOv8-Medium (25M params, 40 FPS on GPU)",
    "- **Metrics:** mAP@0.5 \u2265 0.85, mAP@0.5:0.95 \u2265 0.60",
    "",
    "### 2. **Die Component Detector (Faster R-CNN)**",
    "- **Task:** Localize 50+ IC components on die photograph",
    "- **Approach:** Two-stage detection (region proposals \u2192 classification)",
    "- **Model:** Faster R-CNN with ResNet-50 backbone",
    "- **Metrics:** mAP@0.5 \u2265 0.92 (high precision needed for test validation)",
    "",
    "### 3. **Real-Time Inference Pipeline**",
    "- **Optimization:** TensorRT quantization (FP32 \u2192 INT8)",
    "- **Target:** 30 FPS on NVIDIA Jetson Nano (edge device)",
    "- **Deployment:** ONNX export \u2192 TensorRT engine \u2192 C++ inference",
    "",
    "---",
    "",
    "## \ud83d\udcca Object Detection Architecture Evolution",
    "",
    "```mermaid",
    "graph TD",
    "    A[Object Detection History] --> B[Two-Stage Detectors]",
    "    A --> C[Single-Stage Detectors]",
    "    ",
    "    B --> B1[R-CNN 2014<br/>Selective Search + CNN<br/>mAP: 53.3%<br/>Speed: 47 sec/image]",
    "    B1 --> B2[Fast R-CNN 2015<br/>RoI Pooling<br/>mAP: 66.9%<br/>Speed: 0.3 sec/image]",
    "    B2 --> B3[Faster R-CNN 2015<br/>Region Proposal Network<br/>mAP: 73.2%<br/>Speed: 0.2 sec/image]",
    "    B3 --> B4[Mask R-CNN 2017<br/>Instance Segmentation<br/>mAP: 37.1 mask<br/>Speed: 0.2 sec/image]",
    "    ",
    "    C --> C1[YOLO v1 2015<br/>Single pass<br/>mAP: 63.4%<br/>Speed: 45 FPS]",
    "    C1 --> C2[YOLOv3 2018<br/>Multi-scale<br/>mAP: 57.9%<br/>Speed: 30 FPS]",
    "    C2 --> C3[YOLOv5 2020<br/>PyTorch<br/>mAP: 66.3%<br/>Speed: 140 FPS]",
    "    C3 --> C4[YOLOv8 2023<br/>Anchor-free<br/>mAP: 71.8%<br/>Speed: 80 FPS]",
    "    ",
    "    B --> D{Trade-off}",
    "    C --> D",
    "    D --> E[Two-Stage: Higher mAP<br/>Slower inference<br/>Better small objects]",
    "    D --> F[Single-Stage: Real-time<br/>Lower mAP<br/>Better large objects]",
    "    ",
    "    style B1 fill:#ffe1e1",
    "    style B4 fill:#e1ffe1",
    "    style C1 fill:#ffe1e1",
    "    style C4 fill:#e1f5ff",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4605ddf",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udee0\ufe0f Notebook Structure\n",
    "\n",
    "1. **Mathematical Foundations** - Bounding boxes, IoU, anchor boxes, loss functions\n",
    "2. **Two-Stage Detection (Faster R-CNN)** - Region proposals, RoI pooling, implementation\n",
    "3. **Single-Stage Detection (YOLOv8)** - Architecture, anchor-free detection, training\n",
    "4. **Evaluation Metrics** - mAP calculation, COCO metrics, per-class analysis\n",
    "5. **Real-Time Optimization** - TensorRT, quantization, FPS benchmarking\n",
    "6. **Production Deployment** - 8 semiconductor + general AI/ML projects\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udce6 Prerequisites\n",
    "\n",
    "**Libraries:**\n",
    "```bash\n",
    "# YOLOv8 (Ultralytics - state-of-the-art object detection)\n",
    "pip install ultralytics\n",
    "\n",
    "# Detectron2 (Facebook AI - Faster R-CNN, Mask R-CNN)\n",
    "pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "\n",
    "# Core libraries\n",
    "pip install torch torchvision opencv-python albumentations\n",
    "pip install pycocotools  # COCO evaluation metrics\n",
    "pip install onnx onnxruntime tensorrt  # Deployment\n",
    "\n",
    "# Visualization\n",
    "pip install matplotlib seaborn pillow\n",
    "```\n",
    "\n",
    "**Prior Knowledge:**\n",
    "- Notebook 052: Deep Learning Frameworks (PyTorch basics)\n",
    "- Notebook 053: CNN Architectures (convolution, ResNet)\n",
    "- Notebook 054: Transfer Learning (fine-tuning pre-trained models)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcca Dataset Overview\n",
    "\n",
    "### Synthetic PCB Defect Dataset (6 Classes)\n",
    "\n",
    "We'll generate **2000 PCB images** (640\u00d7640) with labeled defects:\n",
    "\n",
    "| Class | Description | Avg. Instances/Image | Size Range (px) | Business Impact |\n",
    "|-------|-------------|----------------------|-----------------|-----------------|\n",
    "| 0 | Solder bridge | 0-3 | 10-30 | $50K-$200K/incident (short circuit) |\n",
    "| 1 | Missing component | 0-2 | 20-50 | $100K-$500K (functionality loss) |\n",
    "| 2 | Misaligned IC | 0-1 | 40-80 | $20K-$100K (assembly rework) |\n",
    "| 3 | Scratch on trace | 0-4 | 15-60 | $10K-$50K (intermittent failure) |\n",
    "| 4 | Contamination | 0-5 | 5-20 | $5K-$30K (yield loss) |\n",
    "| 5 | Cold solder joint | 0-3 | 8-25 | $30K-$150K (reliability issue) |\n",
    "\n",
    "**Annotation Format (YOLO):**\n",
    "```\n",
    "# Format: <class_id> <x_center> <y_center> <width> <height> (normalized 0-1)\n",
    "0 0.523 0.341 0.045 0.032  # Solder bridge\n",
    "1 0.720 0.615 0.038 0.051  # Missing component\n",
    "3 0.102 0.890 0.095 0.015  # Scratch\n",
    "```\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Multi-scale objects:** Small (contamination 5px) to large (IC 80px)\n",
    "- **Class imbalance:** Missing components rare (0-2/image), contamination common (0-5/image)\n",
    "- **Occlusion:** Components can overlap (e.g., scratch over IC)\n",
    "- **Real-time requirement:** 30 FPS for production inspection\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf93 Learning Strategy\n",
    "\n",
    "### Progressive Complexity\n",
    "1. **Understand fundamentals:** IoU, anchor boxes, multi-task loss\n",
    "2. **Implement two-stage:** Faster R-CNN (accuracy-focused)\n",
    "3. **Implement single-stage:** YOLOv8 (speed-focused)\n",
    "4. **Compare & optimize:** mAP vs FPS trade-off, deployment\n",
    "\n",
    "### Experimentation Framework\n",
    "For each detector, we'll measure:\n",
    "- **mAP@0.5** (primary metric, IoU threshold = 0.5)\n",
    "- **mAP@0.5:0.95** (COCO metric, average over IoU thresholds 0.5-0.95)\n",
    "- **Inference FPS** (frames per second on GPU)\n",
    "- **Model size** (parameters, disk size)\n",
    "- **Per-class AP** (identify weak classes for improvement)\n",
    "\n",
    "### Success Criteria\n",
    "- **mAP@0.5:** \u22650.85 (85% of defects detected with IoU \u2265 0.5)\n",
    "- **mAP@0.5:0.95:** \u22650.60 (robust across multiple IoU thresholds)\n",
    "- **Inference speed:** \u226530 FPS on NVIDIA RTX 3080\n",
    "- **Production deployment:** <50ms latency on Jetson Nano\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd17 How This Fits in the Learning Path\n",
    "\n",
    "**Previous Notebooks:**\n",
    "- 053: CNN Architectures \u2192 Image classification (what is in the image?)\n",
    "- 054: Transfer Learning \u2192 Efficient training with pre-trained models\n",
    "\n",
    "**This Notebook (055):**\n",
    "- **Object detection** \u2192 Classification + Localization (what + where?)\n",
    "- **Real-time inference** \u2192 Production-ready deployment\n",
    "- **Multiple objects** \u2192 Detect 10-100 objects per image\n",
    "\n",
    "**Next Notebooks:**\n",
    "- 056: RNN/LSTM \u2192 Sequential data (test patterns over time)\n",
    "- 057: Seq2Seq & Attention \u2192 Sequence-to-sequence learning\n",
    "- 058: Semantic Segmentation \u2192 Pixel-wise classification\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\ude80 Let's Begin!\n",
    "\n",
    "We'll start with mathematical foundations of object detection, then implement both two-stage (Faster R-CNN) and single-stage (YOLOv8) detectors.\n",
    "\n",
    "---\n",
    "\n",
    "# \ud83d\udcd0 Part 1: Mathematical Foundations of Object Detection\n",
    "\n",
    "## \ud83e\uddee Core Concepts\n",
    "\n",
    "### 1. Bounding Box Representation\n",
    "\n",
    "**Two formats:**\n",
    "\n",
    "**Format 1: (x, y, w, h)** - YOLO format\n",
    "- $(x, y)$: Center coordinates\n",
    "- $(w, h)$: Width and height\n",
    "- All values normalized to [0, 1]\n",
    "\n",
    "**Format 2: (x\u2081, y\u2081, x\u2082, y\u2082)** - Pascal VOC format\n",
    "- $(x_1, y_1)$: Top-left corner\n",
    "- $(x_2, y_2)$: Bottom-right corner\n",
    "- Absolute pixel coordinates\n",
    "\n",
    "**Conversion:**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1 &= x - \\frac{w}{2}, \\quad y_1 = y - \\frac{h}{2} \\\\\n",
    "x_2 &= x + \\frac{w}{2}, \\quad y_2 = y + \\frac{h}{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "YOLO format:  (0.5, 0.5, 0.2, 0.3) on 640\u00d7640 image\n",
    "\u2192 Center: (320, 320), Size: (128, 192)\n",
    "\u2192 Pascal VOC: (256, 224, 384, 416)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Intersection over Union (IoU)\n",
    "\n",
    "**Definition:** Measure of overlap between predicted box $B_p$ and ground truth box $B_{gt}$\n",
    "\n",
    "$$\n",
    "\\text{IoU}(B_p, B_{gt}) = \\frac{\\text{Area}(B_p \\cap B_{gt})}{\\text{Area}(B_p \\cup B_{gt})} = \\frac{\\text{Intersection}}{\\text{Union}}\n",
    "$$\n",
    "\n",
    "**Calculation Steps:**\n",
    "1. Find intersection rectangle: \n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   x_1^i &= \\max(x_1^p, x_1^{gt}), \\quad y_1^i = \\max(y_1^p, y_1^{gt}) \\\\\n",
    "   x_2^i &= \\min(x_2^p, x_2^{gt}), \\quad y_2^i = \\min(y_2^p, y_2^{gt})\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "\n",
    "2. Compute areas:\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   \\text{Area}_{\\text{intersection}} &= \\max(0, x_2^i - x_1^i) \\times \\max(0, y_2^i - y_1^i) \\\\\n",
    "   \\text{Area}_{\\text{union}} &= \\text{Area}_p + \\text{Area}_{gt} - \\text{Area}_{\\text{intersection}}\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "\n",
    "3. IoU:\n",
    "   $$\n",
    "   \\text{IoU} = \\frac{\\text{Area}_{\\text{intersection}}}{\\text{Area}_{\\text{union}}}\n",
    "   $$\n",
    "\n",
    "**IoU Interpretation:**\n",
    "- **IoU \u2265 0.5:** Good detection (commonly used threshold)\n",
    "- **IoU \u2265 0.7:** High-quality detection (strict evaluation)\n",
    "- **IoU < 0.5:** False positive (bounding box too inaccurate)\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Ground truth: (100, 100, 200, 200)  # 100\u00d7100 box\n",
    "Prediction:   (120, 120, 220, 220)  # 100\u00d7100 box, shifted\n",
    "\n",
    "Intersection: (120, 120, 200, 200) \u2192 80\u00d780 = 6400 px\u00b2\n",
    "Union: 10000 + 10000 - 6400 = 13600 px\u00b2\n",
    "IoU = 6400 / 13600 = 0.47 (FAILS at threshold 0.5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Generalized IoU (GIoU) - Better Loss Function\n",
    "\n",
    "**Problem with IoU loss:** When boxes don't overlap (IoU = 0), gradient is zero \u2192 No learning signal\n",
    "\n",
    "**Solution: GIoU** - Considers smallest enclosing box\n",
    "\n",
    "$$\n",
    "\\text{GIoU} = \\text{IoU} - \\frac{\\text{Area}_C - \\text{Area}_{\\text{union}}}{\\text{Area}_C}\n",
    "$$\n",
    "\n",
    "where $C$ is the smallest box enclosing both $B_p$ and $B_{gt}$.\n",
    "\n",
    "**Properties:**\n",
    "- **GIoU \u2208 [-1, 1]** (IoU only [0, 1])\n",
    "- **GIoU = IoU** when boxes overlap\n",
    "- **GIoU < 0** when boxes don't overlap (provides gradient!)\n",
    "\n",
    "**Loss function:**\n",
    "$$\n",
    "\\mathcal{L}_{\\text{GIoU}} = 1 - \\text{GIoU}\n",
    "$$\n",
    "\n",
    "**Further improvements:**\n",
    "- **DIoU (Distance IoU):** Penalizes distance between box centers\n",
    "- **CIoU (Complete IoU):** Adds aspect ratio consistency\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Anchor Boxes (YOLOv3-v5)\n",
    "\n",
    "**Problem:** How to predict multiple objects of different sizes?\n",
    "\n",
    "**Solution:** Pre-define anchor boxes at multiple scales\n",
    "\n",
    "**Anchor box:** Template bounding box with specific aspect ratio and scale\n",
    "- Small anchors: (10\u00d710, 15\u00d720, 20\u00d715) for contamination, small defects\n",
    "- Medium anchors: (30\u00d730, 40\u00d750, 50\u00d740) for solder joints, scratches\n",
    "- Large anchors: (60\u00d760, 80\u00d7100, 100\u00d780) for ICs, large components\n",
    "\n",
    "**YOLOv3 uses 9 anchors (3 scales \u00d7 3 aspect ratios):**\n",
    "```\n",
    "Small:  (10,13), (16,30), (33,23)    # For 8\u00d78 grid (stride 8)\n",
    "Medium: (30,61), (62,45), (59,119)   # For 16\u00d716 grid (stride 16)\n",
    "Large:  (116,90), (156,198), (373,326) # For 32\u00d732 grid (stride 32)\n",
    "```\n",
    "\n",
    "**Prediction:** Model predicts **offsets** from anchor box\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x &= \\sigma(t_x) + c_x  \\quad \\text{(c_x = grid cell x-coordinate)} \\\\\n",
    "y &= \\sigma(t_y) + c_y  \\quad \\text{(c_y = grid cell y-coordinate)} \\\\\n",
    "w &= p_w \\cdot e^{t_w}  \\quad \\text{(p_w = anchor width)} \\\\\n",
    "h &= p_h \\cdot e^{t_h}  \\quad \\text{(p_h = anchor height)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is sigmoid function, $t_x, t_y, t_w, t_h$ are model predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Anchor-Free Detection (YOLOv8)\n",
    "\n",
    "**Problem with anchors:** Requires careful tuning (k-means clustering on dataset), not universal\n",
    "\n",
    "**YOLOv8 Solution:** Directly predict box coordinates (no anchors!)\n",
    "\n",
    "**Architecture:**\n",
    "- **Feature maps:** 3 scales (P3/8, P4/16, P5/32)\n",
    "- **Output:** For each pixel $(i, j)$ on feature map:\n",
    "  - Classification: $C$ classes (softmax)\n",
    "  - Regression: $(x_1, y_1, x_2, y_2)$ bounding box (relative to pixel)\n",
    "  - Objectness: Confidence score $p \\in [0, 1]$\n",
    "\n",
    "**Key innovation: Task Aligned Assigner (TAA)**\n",
    "- Dynamically assigns ground truth boxes to feature map locations\n",
    "- No fixed anchors \u2192 More flexible, better for diverse object sizes\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Multi-Task Loss Function\n",
    "\n",
    "**Object detection requires 3 simultaneous tasks:**\n",
    "\n",
    "1. **Classification Loss** - What is the object?\n",
    "   $$\n",
    "   \\mathcal{L}_{\\text{cls}} = -\\sum_{i \\in \\text{positive}} \\log(\\hat{p}_i^{c_i})\n",
    "   $$\n",
    "   where $\\hat{p}_i^{c_i}$ is predicted probability for true class $c_i$.\n",
    "\n",
    "2. **Localization Loss** - Where is the object?\n",
    "   $$\n",
    "   \\mathcal{L}_{\\text{loc}} = \\sum_{i \\in \\text{positive}} \\text{GIoU}(\\hat{b}_i, b_i)\n",
    "   $$\n",
    "   where $\\hat{b}_i$ is predicted box, $b_i$ is ground truth box.\n",
    "\n",
    "3. **Objectness Loss** - Is there an object?\n",
    "   $$\n",
    "   \\mathcal{L}_{\\text{obj}} = -\\sum_{i} \\left[ o_i \\log(\\hat{o}_i) + (1 - o_i) \\log(1 - \\hat{o}_i) \\right]\n",
    "   $$\n",
    "   where $o_i = 1$ if cell contains object center, 0 otherwise.\n",
    "\n",
    "**Total Loss (YOLOv8):**\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\lambda_{\\text{cls}} \\mathcal{L}_{\\text{cls}} + \\lambda_{\\text{loc}} \\mathcal{L}_{\\text{loc}} + \\lambda_{\\text{obj}} \\mathcal{L}_{\\text{obj}}\n",
    "$$\n",
    "\n",
    "Typical weights: $\\lambda_{\\text{cls}} = 1.0$, $\\lambda_{\\text{loc}} = 5.0$, $\\lambda_{\\text{obj}} = 1.0$\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Non-Maximum Suppression (NMS)\n",
    "\n",
    "**Problem:** Model predicts many overlapping boxes for same object (e.g., 50 boxes around one IC)\n",
    "\n",
    "**Solution: NMS** - Keep only the best box per object\n",
    "\n",
    "**Algorithm:**\n",
    "```\n",
    "1. Sort all predictions by confidence score (descending)\n",
    "2. While predictions remain:\n",
    "   a. Take highest-confidence box B\n",
    "   b. Add B to final detections\n",
    "   c. Remove all boxes with IoU(box, B) > threshold (e.g., 0.45)\n",
    "3. Return final detections\n",
    "```\n",
    "\n",
    "**Pseudocode:**\n",
    "```python\n",
    "def nms(boxes, scores, iou_threshold=0.45):\n",
    "    # boxes: (N, 4), scores: (N,)\n",
    "    keep = []\n",
    "    sorted_indices = scores.argsort()[::-1]  # Descending order\n",
    "    \n",
    "    while len(sorted_indices) > 0:\n",
    "        i = sorted_indices[0]\n",
    "        keep.append(i)\n",
    "        \n",
    "        # Compute IoU with remaining boxes\n",
    "        ious = compute_iou(boxes[i], boxes[sorted_indices[1:]])\n",
    "        \n",
    "        # Keep only boxes with IoU < threshold\n",
    "        mask = ious < iou_threshold\n",
    "        sorted_indices = sorted_indices[1:][mask]\n",
    "    \n",
    "    return boxes[keep], scores[keep]\n",
    "```\n",
    "\n",
    "**Variants:**\n",
    "- **Soft-NMS:** Decay scores instead of removing (smooth suppression)\n",
    "- **DIoU-NMS:** Use DIoU instead of IoU (considers box center distance)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcca Evaluation Metrics: Mean Average Precision (mAP)\n",
    "\n",
    "### Precision & Recall\n",
    "\n",
    "**Precision:** Of all predicted boxes, how many are correct?\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{\\text{Correct detections}}{\\text{All detections}}\n",
    "$$\n",
    "\n",
    "**Recall:** Of all ground truth objects, how many did we find?\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN} = \\frac{\\text{Correct detections}}{\\text{All ground truth objects}}\n",
    "$$\n",
    "\n",
    "**True Positive (TP):** Predicted box with IoU \u2265 threshold (e.g., 0.5)  \n",
    "**False Positive (FP):** Predicted box with IoU < threshold OR duplicate  \n",
    "**False Negative (FN):** Ground truth object not detected\n",
    "\n",
    "---\n",
    "\n",
    "### Average Precision (AP) - Per Class\n",
    "\n",
    "**Steps:**\n",
    "1. Sort predictions by confidence (descending)\n",
    "2. For each prediction, compute precision and recall\n",
    "3. Plot Precision-Recall curve\n",
    "4. **AP = Area under P-R curve**\n",
    "\n",
    "**Example (Class: Solder bridge):**\n",
    "```\n",
    "Ground truth: 10 solder bridges in test set\n",
    "\n",
    "Predictions (sorted by confidence):\n",
    "1. Confidence=0.95, IoU=0.82 \u2192 TP \u2192 Precision=1/1=1.00, Recall=1/10=0.10\n",
    "2. Confidence=0.92, IoU=0.76 \u2192 TP \u2192 Precision=2/2=1.00, Recall=2/10=0.20\n",
    "3. Confidence=0.88, IoU=0.35 \u2192 FP \u2192 Precision=2/3=0.67, Recall=2/10=0.20\n",
    "4. Confidence=0.85, IoU=0.91 \u2192 TP \u2192 Precision=3/4=0.75, Recall=3/10=0.30\n",
    "...\n",
    "\n",
    "AP = Integral of P-R curve \u2248 0.87\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Mean Average Precision (mAP)\n",
    "\n",
    "**mAP@0.5:** Average AP across all classes at IoU threshold = 0.5\n",
    "$$\n",
    "\\text{mAP@0.5} = \\frac{1}{C} \\sum_{c=1}^{C} \\text{AP}_c^{0.5}\n",
    "$$\n",
    "\n",
    "**mAP@0.5:0.95 (COCO metric):** Average mAP over IoU thresholds [0.5, 0.55, 0.60, ..., 0.95]\n",
    "$$\n",
    "\\text{mAP@0.5:0.95} = \\frac{1}{10} \\sum_{t=0.5}^{0.95} \\text{mAP}^t\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- **mAP@0.5 = 0.85:** 85% of objects detected with \"good\" localization\n",
    "- **mAP@0.5:0.95 = 0.60:** Robust detection across multiple IoU thresholds (harder metric)\n",
    "\n",
    "**COCO Metrics (Standard Benchmark):**\n",
    "- **mAP:** mAP@0.5:0.95 (primary metric)\n",
    "- **mAP@0.5:** Lenient (accepts rough boxes)\n",
    "- **mAP@0.75:** Strict (requires tight boxes)\n",
    "- **mAP_small:** AP for small objects (area < 32\u00b2)\n",
    "- **mAP_medium:** AP for medium objects (32\u00b2 < area < 96\u00b2)\n",
    "- **mAP_large:** AP for large objects (area > 96\u00b2)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Two-Stage vs Single-Stage Detectors\n",
    "\n",
    "### Two-Stage Detectors (Faster R-CNN)\n",
    "\n",
    "**Stage 1: Region Proposal Network (RPN)**\n",
    "- Generate ~2000 candidate object locations (region proposals)\n",
    "- Use anchor boxes at multiple scales/aspect ratios\n",
    "- Binary classification: Object vs Background\n",
    "- Bounding box regression: Refine anchor positions\n",
    "\n",
    "**Stage 2: RoI Classifier**\n",
    "- Extract features from each region proposal (RoI Pooling)\n",
    "- Multi-class classification: Which object class?\n",
    "- Bounding box regression: Further refine box coordinates\n",
    "\n",
    "**Pros:**\n",
    "- \u2705 **Higher mAP** (more accurate, especially for small objects)\n",
    "- \u2705 **Better localization** (two refinement stages)\n",
    "- \u2705 **State-of-the-art on COCO** (Mask R-CNN: mAP 37-40%)\n",
    "\n",
    "**Cons:**\n",
    "- \u274c **Slower** (10-20 FPS on GPU, two forward passes)\n",
    "- \u274c **More complex** (two-stage training, harder to optimize)\n",
    "- \u274c **Not real-time** (unsuitable for video processing)\n",
    "\n",
    "---\n",
    "\n",
    "### Single-Stage Detectors (YOLO, SSD, RetinaNet)\n",
    "\n",
    "**Single forward pass:**\n",
    "- Divide image into grid (e.g., 20\u00d720 for YOLOv3)\n",
    "- Each grid cell predicts: Bounding boxes + Classes + Confidence\n",
    "- No region proposals (direct prediction)\n",
    "\n",
    "**Pros:**\n",
    "- \u2705 **Real-time** (30-140 FPS on GPU)\n",
    "- \u2705 **Simple architecture** (end-to-end training)\n",
    "- \u2705 **Edge deployment** (YOLO runs on Jetson Nano, mobile devices)\n",
    "\n",
    "**Cons:**\n",
    "- \u274c **Lower mAP** (5-10% behind two-stage on small objects)\n",
    "- \u274c **Struggles with small/dense objects** (grid-based prediction limits)\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Metric | Faster R-CNN | YOLOv8-Medium | YOLOv8-Nano |\n",
    "|--------|--------------|---------------|-------------|\n",
    "| **mAP@0.5 (COCO)** | 42.0% | 50.2% | 37.3% |\n",
    "| **mAP@0.5:0.95 (COCO)** | 21.9% | 37.4% | 25.0% |\n",
    "| **Inference (GPU)** | 15 FPS | 80 FPS | 180 FPS |\n",
    "| **Parameters** | 137M | 26M | 3M |\n",
    "| **Model Size** | 520 MB | 50 MB | 6 MB |\n",
    "| **Small objects** | \u2705 Excellent | \u2705 Good | \u26a0\ufe0f Fair |\n",
    "| **Real-time** | \u274c No | \u2705 Yes | \u2705 Yes |\n",
    "\n",
    "**Recommendation for Semiconductor:**\n",
    "- **PCB defect detection (real-time):** YOLOv8-Medium (best balance)\n",
    "- **Die-level inspection (accuracy):** Faster R-CNN (small components need precision)\n",
    "- **Edge deployment (Jetson Nano):** YOLOv8-Nano (3M params, 180 FPS)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd2c Next Steps\n",
    "\n",
    "Now that we understand the theory, let's implement:\n",
    "1. **YOLOv8** for real-time PCB defect detection\n",
    "2. **Faster R-CNN** for high-precision die component localization\n",
    "3. **Performance comparison** - mAP, FPS, deployment trade-offs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b199d6",
   "metadata": {},
   "source": [
    "# \ud83d\ude80 Part 2: YOLOv8 Implementation - PCB Defect Detection\n",
    "\n",
    "## \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Train YOLOv8-Medium to detect 6 defect types on PCB images in real-time (80 FPS).\n",
    "\n",
    "**Key Points:**\n",
    "- **Ultralytics YOLOv8:** Latest YOLO version (2023), anchor-free, state-of-the-art\n",
    "- **Dataset:** 2000 synthetic PCB images with 6 defect classes\n",
    "- **Training:** Transfer learning from COCO pre-trained weights\n",
    "- **Metrics:** mAP@0.5, mAP@0.5:0.95, per-class AP, confusion matrix\n",
    "\n",
    "**YOLOv8 Architecture:**\n",
    "```\n",
    "Input (640\u00d7640\u00d73)\n",
    "    \u2193\n",
    "Backbone: CSPDarknet (Modified from YOLOv5)\n",
    "    \u251c\u2500 Conv + C2f blocks (C2f = improved C3 with skip connections)\n",
    "    \u251c\u2500 SPPF (Spatial Pyramid Pooling - Fast)\n",
    "    \u2193\n",
    "Neck: PANet (Path Aggregation Network)\n",
    "    \u251c\u2500 Bottom-up + Top-down feature fusion\n",
    "    \u251c\u2500 3 detection heads (P3/8, P4/16, P5/32 scales)\n",
    "    \u2193\n",
    "Head: Anchor-free decoupled head\n",
    "    \u251c\u2500 Classification branch (6 defect classes)\n",
    "    \u251c\u2500 Regression branch (bounding box)\n",
    "    \u251c\u2500 Objectness branch (confidence)\n",
    "    \u2193\n",
    "Output: N detections \u00d7 (x, y, w, h, confidence, class_prob[6])\n",
    "    \u2193\n",
    "NMS (Non-Maximum Suppression)\n",
    "    \u2193\n",
    "Final Detections\n",
    "```\n",
    "\n",
    "**Why YOLOv8 for Semiconductor:**\n",
    "- **Real-time:** 80 FPS on RTX 3080 (production line speed)\n",
    "- **Accurate:** mAP@0.5 \u2248 85-90% (industry requirement: >80%)\n",
    "- **Deployable:** ONNX/TensorRT export for Jetson Nano\n",
    "- **Small model size:** 50 MB (YOLOv8-Medium) fits on edge devices\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd27 Implementation: YOLOv8 PCB Defect Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bd971d",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1226ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Part 2: YOLOv8 Implementation\n",
    "# ========================================\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageDraw\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "# ========================================\n",
    "# Generate Synthetic PCB Dataset\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING SYNTHETIC PCB DEFECT DATASET\")\n",
    "print(\"=\"*70)\n",
    "# Create dataset directory structure\n",
    "dataset_root = Path(\"pcb_defect_dataset\")\n",
    "(dataset_root / \"images\" / \"train\").mkdir(parents=True, exist_ok=True)\n",
    "(dataset_root / \"images\" / \"val\").mkdir(parents=True, exist_ok=True)\n",
    "(dataset_root / \"images\" / \"test\").mkdir(parents=True, exist_ok=True)\n",
    "(dataset_root / \"labels\" / \"train\").mkdir(parents=True, exist_ok=True)\n",
    "(dataset_root / \"labels\" / \"val\").mkdir(parents=True, exist_ok=True)\n",
    "(dataset_root / \"labels\" / \"test\").mkdir(parents=True, exist_ok=True)\n",
    "# Defect classes\n",
    "classes = [\n",
    "    \"solder_bridge\",      # 0\n",
    "    \"missing_component\",  # 1\n",
    "    \"misaligned_ic\",      # 2\n",
    "    \"scratch\",            # 3\n",
    "    \"contamination\",      # 4\n",
    "    \"cold_solder\"         # 5\n",
    "]\n",
    "def generate_pcb_image(img_size=640, num_defects_range=(1, 8)):\n",
    "    \"\"\"Generate synthetic PCB image with defects.\"\"\"\n",
    "    # Create base PCB (green background with copper traces)\n",
    "    img = np.ones((img_size, img_size, 3), dtype=np.uint8) * np.array([34, 139, 34])  # Green PCB\n",
    "    \n",
    "    # Add copper traces (brown lines)\n",
    "    for _ in range(20):\n",
    "        if np.random.rand() > 0.5:\n",
    "            # Horizontal trace\n",
    "            y = np.random.randint(0, img_size)\n",
    "            thickness = np.random.randint(3, 8)\n",
    "            cv2.line(img, (0, y), (img_size, y), (139, 69, 19), thickness)\n",
    "        else:\n",
    "            # Vertical trace\n",
    "            x = np.random.randint(0, img_size)\n",
    "            thickness = np.random.randint(3, 8)\n",
    "            cv2.line(img, (x, 0), (x, img_size), (139, 69, 19), thickness)\n",
    "    \n",
    "    # Add components (gray/black rectangles)\n",
    "    for _ in range(np.random.randint(30, 50)):\n",
    "        x = np.random.randint(20, img_size - 60)\n",
    "        y = np.random.randint(20, img_size - 60)\n",
    "        w = np.random.randint(15, 50)\n",
    "        h = np.random.randint(10, 40)\n",
    "        color = tuple(np.random.randint(40, 80, 3).tolist())\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), color, -1)\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, 5, img.shape).astype(np.uint8)\n",
    "    img = cv2.add(img, noise)\n",
    "    \n",
    "    # Generate defects\n",
    "    num_defects = np.random.randint(*num_defects_range)\n",
    "    annotations = []  # List of (class_id, x_center, y_center, width, height) normalized\n",
    "    \n",
    "    for _ in range(num_defects):\n",
    "        defect_class = np.random.choice(range(len(classes)), p=[0.15, 0.10, 0.08, 0.25, 0.30, 0.12])\n",
    "        \n",
    "        if defect_class == 0:  # Solder bridge\n",
    "            x = np.random.randint(50, img_size - 50)\n",
    "            y = np.random.randint(50, img_size - 50)\n",
    "            w, h = np.random.randint(10, 30), np.random.randint(5, 15)\n",
    "            cv2.ellipse(img, (x, y), (w//2, h//2), 0, 0, 360, (200, 200, 200), -1)\n",
    "        \n",
    "        elif defect_class == 1:  # Missing component\n",
    "            x = np.random.randint(50, img_size - 100)\n",
    "            y = np.random.randint(50, img_size - 80)\n",
    "            w, h = np.random.randint(25, 55), np.random.randint(20, 50)\n",
    "            # Draw empty space (show PCB background)\n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (34, 139, 34), -1)\n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 0), 2)  # Outline\n",
    "        \n",
    "        elif defect_class == 2:  # Misaligned IC\n",
    "            x = np.random.randint(50, img_size - 100)\n",
    "            y = np.random.randint(50, img_size - 100)\n",
    "            w, h = np.random.randint(40, 80), np.random.randint(40, 80)\n",
    "            # Draw rotated rectangle (misaligned)\n",
    "            angle = np.random.randint(5, 20)\n",
    "            rect = ((x+w//2, y+h//2), (w, h), angle)\n",
    "            box = cv2.boxPoints(rect).astype(int)\n",
    "            cv2.fillPoly(img, [box], (60, 60, 60))\n",
    "        \n",
    "        elif defect_class == 3:  # Scratch\n",
    "            x1 = np.random.randint(0, img_size - 100)\n",
    "            y1 = np.random.randint(0, img_size - 20)\n",
    "            length = np.random.randint(50, 150)\n",
    "            angle = np.random.uniform(-30, 30)\n",
    "            x2 = int(x1 + length * np.cos(np.radians(angle)))\n",
    "            y2 = int(y1 + length * np.sin(np.radians(angle)))\n",
    "            thickness = np.random.randint(2, 5)\n",
    "            cv2.line(img, (x1, y1), (x2, y2), (180, 180, 180), thickness)\n",
    "            # Bounding box for annotation\n",
    "            x = min(x1, x2)\n",
    "            y = min(y1, y2)\n",
    "            w = abs(x2 - x1) + 10\n",
    "            h = abs(y2 - y1) + 10\n",
    "        \n",
    "        elif defect_class == 4:  # Contamination\n",
    "            x = np.random.randint(10, img_size - 30)\n",
    "            y = np.random.randint(10, img_size - 30)\n",
    "            w, h = np.random.randint(5, 20), np.random.randint(5, 20)\n",
    "            color = tuple(np.random.randint(150, 255, 3).tolist())\n",
    "            cv2.circle(img, (x, y), w//2, color, -1)\n",
    "        \n",
    "        elif defect_class == 5:  # Cold solder joint\n",
    "            x = np.random.randint(30, img_size - 30)\n",
    "            y = np.random.randint(30, img_size - 30)\n",
    "            w, h = np.random.randint(8, 25), np.random.randint(8, 25)\n",
    "            # Draw irregular blob (cold solder)\n",
    "            points = []\n",
    "            for angle in range(0, 360, 30):\n",
    "                r = np.random.randint(w//2 - 3, w//2 + 3)\n",
    "                px = int(x + r * np.cos(np.radians(angle)))\n",
    "                py = int(y + r * np.sin(np.radians(angle)))\n",
    "                points.append([px, py])\n",
    "            cv2.fillPoly(img, [np.array(points)], (150, 150, 150))\n",
    "        \n",
    "        # Compute normalized YOLO annotation (x_center, y_center, width, height)\n",
    "        x_center = (x + w/2) / img_size\n",
    "        y_center = (y + h/2) / img_size\n",
    "        width_norm = w / img_size\n",
    "        height_norm = h / img_size\n",
    "        \n",
    "        # Clip to [0, 1] and ensure valid box\n",
    "        x_center = np.clip(x_center, 0, 1)\n",
    "        y_center = np.clip(y_center, 0, 1)\n",
    "        width_norm = np.clip(width_norm, 0.01, 1)\n",
    "        height_norm = np.clip(height_norm, 0.01, 1)\n",
    "        \n",
    "        annotations.append((defect_class, x_center, y_center, width_norm, height_norm))\n",
    "    \n",
    "    return img, annotations\n",
    "# Generate dataset\n",
    "num_train = 1400\n",
    "num_val = 400\n",
    "num_test = 200\n",
    "total = num_train + num_val + num_test\n",
    "print(f\"\\nGenerating {total} PCB images...\")\n",
    "print(f\"  Train: {num_train}\")\n",
    "print(f\"  Val:   {num_val}\")\n",
    "print(f\"  Test:  {num_test}\")\n",
    "start_time = time.time()\n",
    "for split, num_images in [(\"train\", num_train), (\"val\", num_val), (\"test\", num_test)]:\n",
    "    for i in range(num_images):\n",
    "        img, annotations = generate_pcb_image()\n",
    "        \n",
    "        # Save image\n",
    "        img_path = dataset_root / \"images\" / split / f\"pcb_{split}_{i:04d}.jpg\"\n",
    "        cv2.imwrite(str(img_path), img)\n",
    "        \n",
    "        # Save labels (YOLO format)\n",
    "        label_path = dataset_root / \"labels\" / split / f\"pcb_{split}_{i:04d}.txt\"\n",
    "        with open(label_path, 'w') as f:\n",
    "            for ann in annotations:\n",
    "                class_id, x_c, y_c, w, h = ann\n",
    "                f.write(f\"{class_id} {x_c:.6f} {y_c:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "    \n",
    "    if (split == \"train\" and i % 500 == 0) or split != \"train\":\n",
    "        print(f\"  Generated {split}: {i+1}/{num_images}\")\n",
    "generation_time = time.time() - start_time\n",
    "print(f\"\\n\u2713 Dataset generated in {generation_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ddef01",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Create YAML Configuration\n",
    "# ========================================\n",
    "yaml_config = {\n",
    "    'path': str(dataset_root.absolute()),\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'test': 'images/test',\n",
    "    'nc': len(classes),\n",
    "    'names': classes\n",
    "}\n",
    "yaml_path = dataset_root / \"data.yaml\"\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.dump(yaml_config, f, sort_keys=False)\n",
    "print(f\"\\n\u2713 Created dataset configuration: {yaml_path}\")\n",
    "# ========================================\n",
    "# Visualize Sample Images\n",
    "# ========================================\n",
    "print(\"\\nVisualizing sample PCB images with defects...\")\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "for idx in range(6):\n",
    "    img_path = dataset_root / \"images\" / \"train\" / f\"pcb_train_{idx:04d}.jpg\"\n",
    "    label_path = dataset_root / \"labels\" / \"train\" / f\"pcb_train_{idx:04d}.txt\"\n",
    "    \n",
    "    img = cv2.imread(str(img_path))\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img_rgb.shape[:2]\n",
    "    \n",
    "    # Read annotations\n",
    "    with open(label_path, 'r') as f:\n",
    "        annotations = [line.strip().split() for line in f.readlines()]\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    for ann in annotations:\n",
    "        class_id, x_c, y_c, width, height = map(float, ann)\n",
    "        class_id = int(class_id)\n",
    "        \n",
    "        # Convert to pixel coordinates\n",
    "        x_c_px, y_c_px = int(x_c * w), int(y_c * h)\n",
    "        w_px, h_px = int(width * w), int(height * h)\n",
    "        x1 = int(x_c_px - w_px/2)\n",
    "        y1 = int(y_c_px - h_px/2)\n",
    "        x2 = int(x_c_px + w_px/2)\n",
    "        y2 = int(y_c_px + h_px/2)\n",
    "        \n",
    "        # Draw box\n",
    "        color = plt.cm.tab10(class_id)[:3]\n",
    "        color_255 = tuple(int(c*255) for c in color)\n",
    "        cv2.rectangle(img_rgb, (x1, y1), (x2, y2), color_255, 2)\n",
    "        cv2.putText(img_rgb, classes[class_id], (x1, y1-5), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, color_255, 1)\n",
    "    \n",
    "    axes[idx].imshow(img_rgb)\n",
    "    axes[idx].set_title(f'PCB Sample {idx+1}\\n({len(annotations)} defects)', fontsize=10)\n",
    "    axes[idx].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('pcb_samples.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\u2713 Saved visualization to 'pcb_samples.png'\")\n",
    "plt.show()\n",
    "# ========================================\n",
    "# Train YOLOv8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f84209f",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f5d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING YOLOv8-MEDIUM ON PCB DEFECT DATASET\")\n",
    "print(\"=\"*70)\n",
    "# Load pre-trained YOLOv8-Medium\n",
    "model = YOLO('yolov8m.pt')  # Medium model (26M parameters)\n",
    "print(f\"\\nModel: YOLOv8-Medium\")\n",
    "print(f\"  Parameters: ~26M\")\n",
    "print(f\"  Pre-trained: COCO dataset\")\n",
    "# Training configuration\n",
    "train_config = {\n",
    "    'data': str(yaml_path),\n",
    "    'epochs': 50,\n",
    "    'imgsz': 640,\n",
    "    'batch': 16,\n",
    "    'device': device,\n",
    "    'workers': 4,\n",
    "    'patience': 10,  # Early stopping\n",
    "    'save': True,\n",
    "    'project': 'runs/detect',\n",
    "    'name': 'pcb_yolov8m',\n",
    "    'exist_ok': True,\n",
    "    'verbose': True\n",
    "}\n",
    "print(f\"\\nTraining configuration:\")\n",
    "for k, v in train_config.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "# Train model\n",
    "print(f\"\\nStarting training...\")\n",
    "start_time = time.time()\n",
    "results = model.train(**train_config)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\n\u2713 Training completed in {training_time/60:.2f} minutes\")\n",
    "# ========================================\n",
    "# Evaluate Model\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATING YOLOv8 ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "# Load best weights\n",
    "best_model = YOLO('runs/detect/pcb_yolov8m/weights/best.pt')\n",
    "# Evaluate on test set\n",
    "metrics = best_model.val(data=str(yaml_path), split='test')\n",
    "print(f\"\\nTest Set Metrics:\")\n",
    "print(f\"  mAP@0.5:       {metrics.box.map50:.4f}\")\n",
    "print(f\"  mAP@0.5:0.95:  {metrics.box.map:.4f}\")\n",
    "print(f\"  Precision:     {metrics.box.mp:.4f}\")\n",
    "print(f\"  Recall:        {metrics.box.mr:.4f}\")\n",
    "# Per-class AP\n",
    "print(f\"\\nPer-Class Average Precision (AP@0.5):\")\n",
    "for idx, class_name in enumerate(classes):\n",
    "    ap = metrics.box.ap50[idx]\n",
    "    print(f\"  {class_name:20s}: {ap:.4f}\")\n",
    "# ========================================\n",
    "# Inference Speed Benchmark\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INFERENCE SPEED BENCHMARK\")\n",
    "print(\"=\"*70)\n",
    "# Load test image\n",
    "test_img_path = dataset_root / \"images\" / \"test\" / \"pcb_test_0000.jpg\"\n",
    "test_img = cv2.imread(str(test_img_path))\n",
    "# Warm-up\n",
    "for _ in range(10):\n",
    "    _ = best_model(test_img, verbose=False)\n",
    "# Benchmark\n",
    "num_runs = 100\n",
    "start_time = time.time()\n",
    "for _ in range(num_runs):\n",
    "    results = best_model(test_img, verbose=False)\n",
    "inference_time = (time.time() - start_time) / num_runs\n",
    "fps = 1 / inference_time\n",
    "print(f\"\\nInference Benchmarks ({num_runs} runs):\")\n",
    "print(f\"  Average latency:  {inference_time*1000:.2f} ms\")\n",
    "print(f\"  Throughput (FPS): {fps:.2f}\")\n",
    "print(f\"  Device:           {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100cbab",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b4823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Visualize Predictions\n",
    "# ========================================\n",
    "print(\"\\nVisualizing predictions on test images...\")\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "for idx in range(6):\n",
    "    test_img_path = dataset_root / \"images\" / \"test\" / f\"pcb_test_{idx:04d}.jpg\"\n",
    "    img = cv2.imread(str(test_img_path))\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Run inference\n",
    "    results = best_model(img, verbose=False)[0]\n",
    "    \n",
    "    # Draw predictions\n",
    "    for box in results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "        conf = float(box.conf[0])\n",
    "        cls = int(box.cls[0])\n",
    "        \n",
    "        if conf > 0.5:  # Confidence threshold\n",
    "            color = plt.cm.tab10(cls)[:3]\n",
    "            color_255 = tuple(int(c*255) for c in color)\n",
    "            cv2.rectangle(img_rgb, (x1, y1), (x2, y2), color_255, 2)\n",
    "            label = f\"{classes[cls]} {conf:.2f}\"\n",
    "            cv2.putText(img_rgb, label, (x1, y1-5), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, color_255, 1)\n",
    "    \n",
    "    axes[idx].imshow(img_rgb)\n",
    "    axes[idx].set_title(f'Test Image {idx+1}\\n({len(results.boxes)} detections)', fontsize=10)\n",
    "    axes[idx].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('pcb_predictions.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\u2713 Saved predictions to 'pcb_predictions.png'\")\n",
    "plt.show()\n",
    "# ========================================\n",
    "# Export to ONNX (for deployment)\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPORTING MODEL TO ONNX\")\n",
    "print(\"=\"*70)\n",
    "onnx_path = best_model.export(format='onnx', simplify=True)\n",
    "print(f\"\\n\u2713 Exported to ONNX: {onnx_path}\")\n",
    "print(f\"  Compatible with: TensorRT, ONNX Runtime, OpenVINO\")\n",
    "print(f\"  Deployment targets: Jetson Nano, edge devices, production servers\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"YOLOV8 TRAINING & EVALUATION COMPLETE\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a8463",
   "metadata": {},
   "source": [
    "# \ud83c\udfaf Part 3: Faster R-CNN Comparison & Real-World Projects\n",
    "\n",
    "## \ud83d\udcdd Faster R-CNN Overview\n",
    "\n",
    "**Two-Stage Architecture:**\n",
    "\n",
    "```\n",
    "Stage 1: Region Proposal Network (RPN)\n",
    "    Input (640\u00d7640\u00d73)\n",
    "        \u2193\n",
    "    Backbone: ResNet-50\n",
    "        \u2193\n",
    "    Feature Map (20\u00d720\u00d72048)\n",
    "        \u2193\n",
    "    Anchor Generation (9 anchors/cell \u00d7 400 cells = 3600 proposals)\n",
    "        \u251c\u2500 3 scales: 128\u00b2, 256\u00b2, 512\u00b2\n",
    "        \u2514\u2500 3 aspect ratios: 1:1, 1:2, 2:1\n",
    "        \u2193\n",
    "    Binary Classification: Object vs Background\n",
    "    Bounding Box Regression: Refine anchor positions\n",
    "        \u2193\n",
    "    Top-N proposals (e.g., N=300) sent to Stage 2\n",
    "\n",
    "Stage 2: RoI Head\n",
    "    For each proposal:\n",
    "        \u2193\n",
    "    RoI Pooling (extract 7\u00d77 features)\n",
    "        \u2193\n",
    "    Fully Connected Layers\n",
    "        \u251c\u2500 Multi-class Classification (6 classes)\n",
    "        \u2514\u2500 Bounding Box Refinement\n",
    "        \u2193\n",
    "    NMS (remove duplicates)\n",
    "        \u2193\n",
    "    Final Detections\n",
    "```\n",
    "\n",
    "**Key Differences: Faster R-CNN vs YOLOv8:**\n",
    "\n",
    "| Feature | Faster R-CNN | YOLOv8 |\n",
    "|---------|--------------|--------|\n",
    "| **Stages** | Two-stage (RPN + RoI Head) | Single-stage (direct prediction) |\n",
    "| **Anchors** | Fixed anchors (9 per cell) | Anchor-free |\n",
    "| **Speed** | 10-20 FPS | 80-180 FPS |\n",
    "| **mAP** | Higher (+3-5% on small objects) | Good (but 3-5% lower) |\n",
    "| **Small objects** | \u2705 Excellent (RPN captures small regions) | \u26a0\ufe0f Fair (grid-based limits) |\n",
    "| **Complexity** | High (two-stage training) | Low (end-to-end) |\n",
    "| **Use case** | High-precision (die components, medical) | Real-time (PCB inspection, video) |\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\ude80 Real-World Projects\n",
    "\n",
    "### **\ud83d\udd2c Semiconductor Projects (Post-Silicon Validation)**\n",
    "\n",
    "#### **Project 1: Production PCB Defect Inspector with Real-Time Feedback**\n",
    "\n",
    "**Objective:** Deploy YOLOv8 on factory production line for 100% automated visual inspection\n",
    "\n",
    "**Business Value:** $10M-$40M/year from automated defect detection + $2M-$5M labor savings\n",
    "\n",
    "**System Architecture:**\n",
    "- **Input:** 4K camera (3840\u00d72160) mounted above conveyor belt\n",
    "- **Pre-processing:** Crop PCB region \u2192 Resize to 640\u00d7640 \u2192 YOLOv8 inference\n",
    "- **Output:** Pass/Fail decision + defect locations + confidence scores\n",
    "- **Throughput:** 30 PCBs/minute (2 sec/PCB including handling)\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "# Pseudocode\n",
    "class PCBInspector:\n",
    "    def __init__(self):\n",
    "        self.model = YOLO('yolov8m_pcb.pt')  # Fine-tuned on customer PCBs\n",
    "        self.camera = IndustrialCamera(resolution=(3840, 2160), fps=30)\n",
    "        self.pass_threshold = 0.85  # Confidence for defect detection\n",
    "    \n",
    "    def inspect_pcb(self, pcb_id):\n",
    "        # Capture image\n",
    "        img = self.camera.capture()\n",
    "        \n",
    "        # Detect PCB region (ROI extraction)\n",
    "        pcb_roi = self.detect_pcb_region(img)\n",
    "        pcb_resized = cv2.resize(pcb_roi, (640, 640))\n",
    "        \n",
    "        # Run YOLOv8\n",
    "        results = self.model(pcb_resized)[0]\n",
    "        \n",
    "        # Decision logic\n",
    "        defects = [box for box in results.boxes if box.conf > self.pass_threshold]\n",
    "        \n",
    "        if len(defects) == 0:\n",
    "            verdict = \"PASS\"\n",
    "            action = \"ship_to_next_stage\"\n",
    "        else:\n",
    "            verdict = \"FAIL\"\n",
    "            action = \"quarantine_for_rework\"\n",
    "            # Log defect details\n",
    "            for defect in defects:\n",
    "                cls = int(defect.cls)\n",
    "                conf = float(defect.conf)\n",
    "                x, y, w, h = defect.xywh[0].tolist()\n",
    "                self.log_defect(pcb_id, class_name=classes[cls], \n",
    "                               location=(x,y), confidence=conf)\n",
    "        \n",
    "        return verdict, action, defects\n",
    "    \n",
    "    def deploy_on_line(self):\n",
    "        while True:\n",
    "            pcb_id = self.conveyor.get_next_pcb()\n",
    "            verdict, action, defects = self.inspect_pcb(pcb_id)\n",
    "            self.conveyor.route_pcb(pcb_id, action)\n",
    "            self.dashboard.update(pcb_id, verdict, defects)\n",
    "```\n",
    "\n",
    "**Success Metrics:**\n",
    "- **Detection accuracy:** \u226599% (mAP@0.5 \u2265 0.95 on production data)\n",
    "- **False positive rate:** <1% (minimize good PCBs flagged as defective)\n",
    "- **False negative rate:** <0.5% (critical: don't miss defects)\n",
    "- **Throughput:** 30 PCBs/min (2 sec/PCB inspection time)\n",
    "- **ROI:** Payback period <6 months\n",
    "\n",
    "**Deployment Stack:**\n",
    "- NVIDIA Jetson AGX Xavier (32GB, 512 CUDA cores)\n",
    "- YOLOv8-Medium quantized to INT8 (50 MB \u2192 12 MB)\n",
    "- TensorRT engine for 80 FPS inference\n",
    "- PostgreSQL for defect logging + analytics\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 2: Die-Level Component Localization for Test Probe Placement**\n",
    "\n",
    "**Objective:** Detect 1000+ probe pads on 5mm \u00d7 5mm die to validate test program accuracy\n",
    "\n",
    "**Business Value:** $15M-$60M/year from reduced test escapes + faster test program debug\n",
    "\n",
    "**Architecture:**\n",
    "- **Model:** Faster R-CNN with ResNet-101 backbone (higher mAP for small pads)\n",
    "- **Input:** 4096\u00d74096 die image from microscope \u2192 Crop to 1024\u00d71024 tiles\n",
    "- **Output:** Bounding boxes around each pad (0.05mm \u00d7 0.05mm, ~5px on image)\n",
    "- **Post-processing:** Cluster detections by spatial proximity (pads in rows/columns)\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "# Detectron2 (Facebook AI) for Faster R-CNN\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = \"die_component_detector_rcnn.pth\"  # Fine-tuned weights\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # Analog pad, Digital pad, Power pad\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.90  # High confidence threshold\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Inference on die image\n",
    "die_image = cv2.imread(\"die_4096x4096.jpg\")\n",
    "outputs = predictor(die_image)\n",
    "\n",
    "# Extract pad locations\n",
    "pads = outputs[\"instances\"].to(\"cpu\")\n",
    "boxes = pads.pred_boxes.tensor.numpy()  # (N, 4)\n",
    "classes = pads.pred_classes.numpy()     # (N,)\n",
    "scores = pads.scores.numpy()            # (N,)\n",
    "\n",
    "print(f\"Detected {len(boxes)} probe pads\")\n",
    "print(f\"  Analog pads:  {(classes == 0).sum()}\")\n",
    "print(f\"  Digital pads: {(classes == 1).sum()}\")\n",
    "print(f\"  Power pads:   {(classes == 2).sum()}\")\n",
    "\n",
    "# Validate against test program expectations\n",
    "expected_pads = load_test_program_pad_list()  # From test program database\n",
    "detected_coords = [(box[0]+box[2])/2, (box[1]+box[3])/2] for box in boxes]\n",
    "\n",
    "# Match detected pads to expected pads (Hungarian algorithm)\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "cost_matrix = compute_distance_matrix(detected_coords, expected_pads)\n",
    "row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "# Compute alignment error\n",
    "alignment_errors = [cost_matrix[i, j] for i, j in zip(row_ind, col_ind)]\n",
    "mean_error = np.mean(alignment_errors)\n",
    "\n",
    "if mean_error < 2.0:  # < 2 pixels (~0.01mm)\n",
    "    print(\"\u2713 Test probe alignment PASSED\")\n",
    "else:\n",
    "    print(f\"\u2717 Test probe alignment FAILED (mean error: {mean_error:.2f} px)\")\n",
    "```\n",
    "\n",
    "**Success Metrics:**\n",
    "- **Detection recall:** \u226599.5% (miss <5 pads out of 1000)\n",
    "- **Localization accuracy:** RMSE < 2 pixels (~0.01mm)\n",
    "- **Inference time:** <5 sec per die (Faster R-CNN on Tesla V100)\n",
    "- **Cost savings:** $15M-$60M/year from eliminating test escapes\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 3: Wafer Map Spatial Defect Cluster Detection**\n",
    "\n",
    "**Objective:** Detect multiple simultaneous defect clusters on wafer map (vs whole-map classification)\n",
    "\n",
    "**Business Value:** $30M-$120M/year from precise root-cause identification\n",
    "\n",
    "**Architecture:**\n",
    "- **Model:** YOLOv8-Small (faster for 300\u00d7300 wafer maps)\n",
    "- **Input:** Grayscale wafer map (300\u00d7300) with die pass/fail data\n",
    "- **Output:** Bounding boxes around defect clusters (center, edge, ring, etc.)\n",
    "- **Analysis:** Correlate cluster locations with process steps (litho zones, etching)\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "# Wafer map object detection\n",
    "wafer_map = load_wafer_test_results(wafer_id)  # 300\u00d7300 binary map (0=pass, 1=fail)\n",
    "\n",
    "# Convert to RGB for YOLOv8\n",
    "wafer_map_rgb = cv2.cvtColor(wafer_map, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# Detect defect clusters\n",
    "cluster_detector = YOLO('yolov8s_wafer_clusters.pt')\n",
    "results = cluster_detector(wafer_map_rgb)[0]\n",
    "\n",
    "# Extract clusters\n",
    "clusters = []\n",
    "for box in results.boxes:\n",
    "    x, y, w, h = box.xywh[0].tolist()\n",
    "    cluster_type = classes[int(box.cls)]\n",
    "    num_dies = count_failing_dies_in_region(wafer_map, (x, y, w, h))\n",
    "    \n",
    "    clusters.append({\n",
    "        'type': cluster_type,  # e.g., 'center_cluster', 'edge_defect', 'ring'\n",
    "        'location': (x, y),\n",
    "        'size': (w, h),\n",
    "        'num_dies': num_dies,\n",
    "        'confidence': float(box.conf)\n",
    "    })\n",
    "\n",
    "# Root-cause analysis\n",
    "for cluster in clusters:\n",
    "    # Correlate with process steps\n",
    "    litho_zone = map_to_lithography_zone(cluster['location'])\n",
    "    etch_region = map_to_etching_region(cluster['location'])\n",
    "    \n",
    "    print(f\"Cluster: {cluster['type']}\")\n",
    "    print(f\"  Location: {cluster['location']}\")\n",
    "    print(f\"  Affected dies: {cluster['num_dies']}\")\n",
    "    print(f\"  Lithography zone: {litho_zone}\")\n",
    "    print(f\"  Etching region: {etch_region}\")\n",
    "    \n",
    "    # Suggest root cause\n",
    "    if cluster['type'] == 'center_cluster':\n",
    "        root_cause = \"Possible contamination during deposition\"\n",
    "    elif cluster['type'] == 'edge_defect':\n",
    "        root_cause = \"Chuck/vacuum edge effect\"\n",
    "    elif cluster['type'] == 'ring':\n",
    "        root_cause = \"Non-uniform plasma etching\"\n",
    "    else:\n",
    "        root_cause = \"Unknown - requires further investigation\"\n",
    "    \n",
    "    print(f\"  Suggested root cause: {root_cause}\\n\")\n",
    "```\n",
    "\n",
    "**Success Metrics:**\n",
    "- **Cluster detection mAP@0.5:** \u22650.90\n",
    "- **Spatial resolution:** 10mm \u00d7 10mm regions (30 pixels)\n",
    "- **Multi-cluster capability:** Detect up to 10 simultaneous clusters per wafer\n",
    "- **Root-cause accuracy:** \u226580% agreement with expert analysis\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 4: Real-Time SEM Image Defect Detection**\n",
    "\n",
    "**Objective:** Real-time defect detection during SEM imaging (replace post-scan manual review)\n",
    "\n",
    "**Business Value:** $5M-$20M/year from faster defect identification + reduced SEM time\n",
    "\n",
    "**Architecture:**\n",
    "- **Model:** YOLOv8-Nano (3M params, 250 FPS for real-time)\n",
    "- **Input:** 1024\u00d71024 SEM image stream (30 FPS from SEM)\n",
    "- **Output:** Real-time overlay of defect boxes on SEM display\n",
    "- **Integration:** Custom SEM control software (C++ API)\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "# Real-time SEM defect detector\n",
    "class SEMDefectDetector:\n",
    "    def __init__(self):\n",
    "        self.model = YOLO('yolov8n_sem_defects.pt')  # Nano model for speed\n",
    "        self.defect_classes = ['scratch', 'pit', 'void', 'contamination', 'crack']\n",
    "    \n",
    "    def process_sem_stream(self, sem_device):\n",
    "        while True:\n",
    "            # Capture frame from SEM\n",
    "            frame = sem_device.get_frame()  # 1024\u00d71024 grayscale\n",
    "            \n",
    "            # Replicate to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "            \n",
    "            # Run YOLOv8-Nano (< 4ms inference)\n",
    "            results = self.model(frame_rgb, verbose=False)[0]\n",
    "            \n",
    "            # Draw detections in real-time\n",
    "            for box in results.boxes:\n",
    "                if box.conf > 0.7:\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "                    cls = int(box.cls)\n",
    "                    conf = float(box.conf)\n",
    "                    \n",
    "                    # Overlay on SEM display\n",
    "                    cv2.rectangle(frame_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    label = f\"{self.defect_classes[cls]} {conf:.2f}\"\n",
    "                    cv2.putText(frame_rgb, label, (x1, y1-10),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                    \n",
    "                    # Alert operator if critical defect\n",
    "                    if cls in [2, 4]:  # Void or crack (critical)\n",
    "                        sem_device.trigger_alert(f\"Critical defect: {self.defect_classes[cls]}\")\n",
    "            \n",
    "            # Display annotated frame\n",
    "            sem_device.update_display(frame_rgb)\n",
    "            \n",
    "            # Check for stop signal\n",
    "            if sem_device.stop_requested():\n",
    "                break\n",
    "\n",
    "# Deploy\n",
    "detector = SEMDefectDetector()\n",
    "detector.process_sem_stream(SEM_Device_API())\n",
    "```\n",
    "\n",
    "**Success Metrics:**\n",
    "- **Inference speed:** <4ms per frame (250 FPS on RTX 3080)\n",
    "- **Detection accuracy:** mAP@0.5 \u2265 0.88\n",
    "- **False alarm rate:** <5% (critical for operator acceptance)\n",
    "- **SEM throughput:** +40% (operators focus on critical areas only)\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83c\udf10 General AI/ML Projects**\n",
    "\n",
    "#### **Project 5: Autonomous Vehicle Object Detection**\n",
    "\n",
    "**Objective:** Real-time detection of vehicles, pedestrians, cyclists, traffic signs\n",
    "\n",
    "**Architecture:** YOLOv8-Large on NVIDIA Drive platform (30 FPS, 4K resolution)\n",
    "\n",
    "**Success Metrics:** mAP@0.5 \u2265 0.70, <50ms latency, zero critical misses\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 6: Retail Inventory Management**\n",
    "\n",
    "**Objective:** Detect out-of-stock products on shelves, planogram compliance\n",
    "\n",
    "**Architecture:** Faster R-CNN for small product detection (100+ products per image)\n",
    "\n",
    "**Success Metrics:** mAP@0.5 \u2265 0.92, <2 sec per shelf image\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 7: Medical CT Scan Lesion Detection**\n",
    "\n",
    "**Objective:** Detect tumors, nodules, lesions in 3D CT scans\n",
    "\n",
    "**Architecture:** 3D Faster R-CNN (volumetric detection), ResNet3D-50 backbone\n",
    "\n",
    "**Success Metrics:** Sensitivity \u226595%, False positives <3 per scan\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 8: Wildlife Monitoring with Camera Traps**\n",
    "\n",
    "**Objective:** Detect and classify animals in remote camera trap images\n",
    "\n",
    "**Architecture:** YOLOv8-Medium fine-tuned on wildlife dataset (50 species)\n",
    "\n",
    "**Success Metrics:** mAP@0.5 \u2265 0.88, runs on solar-powered edge device\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf93 Key Takeaways & Best Practices\n",
    "\n",
    "### **Object Detection Strategy Selection**\n",
    "\n",
    "| Requirement | Recommended Detector | Rationale |\n",
    "|-------------|----------------------|-----------|\n",
    "| **Real-time (\u226530 FPS)** | YOLOv8-Medium/Small | Single-stage, optimized for speed |\n",
    "| **High mAP (small objects)** | Faster R-CNN | Two-stage, RPN captures small regions |\n",
    "| **Edge deployment** | YOLOv8-Nano | 3M params, 6MB model, 180 FPS |\n",
    "| **Multi-class (100+ classes)** | Faster R-CNN | Better classification head |\n",
    "| **Video processing** | YOLOv8 | Real-time, temporal consistency |\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Best Practices**\n",
    "\n",
    "1. **Data Augmentation for Object Detection:**\n",
    "   ```python\n",
    "   import albumentations as A\n",
    "   \n",
    "   transform = A.Compose([\n",
    "       A.HorizontalFlip(p=0.5),\n",
    "       A.RandomBrightnessContrast(p=0.3),\n",
    "       A.Rotate(limit=15, p=0.5),\n",
    "       A.GaussianBlur(p=0.2),\n",
    "       A.Cutout(num_holes=4, max_h_size=32, max_w_size=32, p=0.3)  # Occlusion\n",
    "   ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "   ```\n",
    "\n",
    "2. **Class Imbalance Handling:**\n",
    "   - Oversample rare classes (e.g., missing_component)\n",
    "   - Use weighted loss (penalize misclassifications of rare classes more)\n",
    "   - Focal loss: $FL(p_t) = -(1-p_t)^\\gamma \\log(p_t)$ (focus on hard examples)\n",
    "\n",
    "3. **Anchor Tuning (for anchor-based detectors):**\n",
    "   ```python\n",
    "   # K-means clustering on training boxes\n",
    "   from scipy.cluster.vq import kmeans\n",
    "   \n",
    "   all_boxes = []  # Collect (w, h) from training set\n",
    "   for annotation in dataset:\n",
    "       for box in annotation['boxes']:\n",
    "           all_boxes.append([box[2], box[3]])  # width, height\n",
    "   \n",
    "   # Cluster into 9 anchors\n",
    "   anchors, _ = kmeans(np.array(all_boxes), 9)\n",
    "   print(\"Optimized anchors:\", anchors)\n",
    "   ```\n",
    "\n",
    "4. **Hyperparameter Tuning:**\n",
    "   - **IoU threshold (NMS):** 0.4-0.5 (lower = fewer duplicates, higher = more detections)\n",
    "   - **Confidence threshold:** 0.25-0.5 (lower = higher recall, higher = higher precision)\n",
    "   - **Image size:** 640\u00d7640 (standard), 1280\u00d71280 (for small objects, 4\u00d7 slower)\n",
    "   - **Batch size:** 16-32 (GPU memory dependent)\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation & Debugging**\n",
    "\n",
    "**Tools:**\n",
    "1. **Confusion Matrix:** Identify class misclassifications\n",
    "2. **Per-Class AP:** Find weak classes (AP < 0.5)\n",
    "3. **IoU Distribution:** Check if boxes are well-aligned (most IoU > 0.7)\n",
    "4. **FP/FN Analysis:** Manually review false positives and false negatives\n",
    "\n",
    "**Common Issues & Fixes:**\n",
    "\n",
    "| Issue | Symptom | Fix |\n",
    "|-------|---------|-----|\n",
    "| **Low mAP** | AP < 0.5 for all classes | More training data, better augmentation |\n",
    "| **Low recall** | Many false negatives | Lower confidence threshold, more anchors |\n",
    "| **Low precision** | Many false positives | Higher confidence threshold, better NMS |\n",
    "| **Small objects missed** | mAP_small < 0.3 | Increase image size (640 \u2192 1280), FPN neck |\n",
    "| **Slow inference** | FPS < 30 | Smaller model (YOLOv8-Nano), quantization, TensorRT |\n",
    "\n",
    "---\n",
    "\n",
    "### **Production Deployment Checklist**\n",
    "\n",
    "\u2705 **Model Optimization:**\n",
    "- [ ] ONNX export for framework independence\n",
    "- [ ] INT8 quantization (4\u00d7 smaller, 2-3\u00d7 faster)\n",
    "- [ ] TensorRT compilation (NVIDIA GPUs, 2-5\u00d7 speedup)\n",
    "- [ ] Model pruning (remove 30-50% weights, minimal accuracy loss)\n",
    "\n",
    "\u2705 **Inference Pipeline:**\n",
    "- [ ] Batch processing (process 4-16 images at once)\n",
    "- [ ] GPU memory management (avoid OOM errors)\n",
    "- [ ] Pre-processing optimization (resize, normalize in parallel)\n",
    "- [ ] Post-processing (NMS, coordinate conversion in C++ for speed)\n",
    "\n",
    "\u2705 **Monitoring & Maintenance:**\n",
    "- [ ] Log prediction confidence distributions (detect distribution drift)\n",
    "- [ ] Track mAP on production data (monthly evaluation)\n",
    "- [ ] Active learning (flag low-confidence predictions for labeling)\n",
    "- [ ] A/B testing (compare new model versions before deployment)\n",
    "\n",
    "\u2705 **Edge Deployment (Jetson Nano):**\n",
    "- [ ] YOLOv8-Nano or YOLOv8-Small (\u226410M params)\n",
    "- [ ] TensorRT INT8 engine (6-12 MB model size)\n",
    "- [ ] 30 FPS target on Jetson Nano\n",
    "- [ ] Fallback to CPU if GPU unavailable\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcda What's Next?\n",
    "\n",
    "**Upcoming Notebooks:**\n",
    "- **056: RNN/LSTM/GRU** \u2192 Sequential test pattern analysis (time-series wafer data)\n",
    "- **057: Seq2Seq & Attention** \u2192 Test sequence optimization with encoder-decoder\n",
    "- **058: Semantic Segmentation** \u2192 Pixel-wise classification (wafer map heat maps)\n",
    "- **059: Instance Segmentation (Mask R-CNN)** \u2192 Object detection + precise masks\n",
    "\n",
    "---\n",
    "\n",
    "## \u2705 Learning Objectives Review\n",
    "\n",
    "1. \u2705 **Object Detection Fundamentals** - IoU, anchor boxes, multi-task loss\n",
    "2. \u2705 **Two-Stage Detectors** - Faster R-CNN architecture, RPN, RoI pooling\n",
    "3. \u2705 **Single-Stage Detectors** - YOLOv8 anchor-free detection, CSPDarknet backbone\n",
    "4. \u2705 **Loss Functions** - GIoU loss, focal loss for class imbalance\n",
    "5. \u2705 **Non-Maximum Suppression** - NMS algorithm, soft-NMS, DIoU-NMS\n",
    "6. \u2705 **Evaluation Metrics** - mAP@0.5, mAP@0.5:0.95, COCO metrics\n",
    "7. \u2705 **Real-Time Detection** - YOLOv8 achieves 80 FPS, TensorRT optimization\n",
    "8. \u2705 **Semiconductor Applications** - PCB defects, die components, wafer clusters\n",
    "\n",
    "**Key Skill Acquired:** Deploy production-grade object detection systems for real-time visual inspection!\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcd6 Additional Resources\n",
    "\n",
    "**Must-Read Papers:**\n",
    "- \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\" (Ren et al., 2015)\n",
    "- \"You Only Look Once: Unified, Real-Time Object Detection\" (Redmon et al., 2015)\n",
    "- \"Focal Loss for Dense Object Detection\" (Lin et al., 2017) - RetinaNet\n",
    "- \"YOLOv8: Ultralytics\" (2023) - Latest YOLO version\n",
    "\n",
    "**Courses & Tutorials:**\n",
    "- CS231n (Stanford) - Lecture 11: Object Detection\n",
    "- Ultralytics YOLOv8 Documentation - https://docs.ultralytics.com\n",
    "- Detectron2 (Facebook AI) - https://detectron2.readthedocs.io\n",
    "\n",
    "**Deployment Tools:**\n",
    "- **TensorRT** - https://developer.nvidia.com/tensorrt\n",
    "- **ONNX Runtime** - https://onnxruntime.ai\n",
    "- **OpenVINO** (Intel) - https://docs.openvino.ai\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Final Summary\n",
    "\n",
    "**Object Detection Mastery:**\n",
    "- **Two-stage (Faster R-CNN):** Best mAP, slower (15 FPS), use for high-precision tasks\n",
    "- **Single-stage (YOLOv8):** Real-time (80 FPS), good mAP, use for production\n",
    "- **Edge deployment:** YOLOv8-Nano (3M params, 180 FPS) on Jetson Nano\n",
    "\n",
    "**Semiconductor Impact:**\n",
    "- **PCB inspection:** $10M-$40M/year from automated defect detection\n",
    "- **Die localization:** $15M-$60M/year from reduced test escapes\n",
    "- **Wafer analysis:** $30M-$120M/year from precise root-cause identification\n",
    "\n",
    "**You're now ready to build real-time visual inspection systems!** \ud83d\ude80\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations on completing Notebook 055!** \ud83c\udf89\n",
    "\n",
    "Next notebook: **056_RNN_LSTM_GRU.ipynb** - Sequential data analysis for time-series test patterns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92371ce",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# FASTER R-CNN IMPLEMENTATION\n",
    "# Comparison with YOLOv8 on PCB Defect Detection\n",
    "# ========================================\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "# ========================================\n",
    "# DATASET PREPARATION\n",
    "# Reuse PCB dataset from YOLOv8\n",
    "# ========================================\n",
    "class PCBDefectDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for PCB defect detection\n",
    "    Converts YOLO format to PyTorch Faster R-CNN format\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.label_dir = Path(label_dir)\n",
    "        self.transform = transform\n",
    "        self.image_files = list(self.image_dir.glob(\"*.jpg\"))\n",
    "        \n",
    "        # Class mapping (YOLO class IDs to names)\n",
    "        self.classes = ['scratch', 'short', 'open', 'mouse_bite', 'spur']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Load corresponding label\n",
    "        label_path = self.label_dir / (img_path.stem + \".txt\")\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        if label_path.exists():\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    # YOLO format: class_id x_center y_center width height (normalized)\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) == 5:\n",
    "                        class_id = int(parts[0])\n",
    "                        x_center = float(parts[1]) * image.width\n",
    "                        y_center = float(parts[2]) * image.height\n",
    "                        width = float(parts[3]) * image.width\n",
    "                        height = float(parts[4]) * image.height\n",
    "                        \n",
    "                        # Convert to [x_min, y_min, x_max, y_max]\n",
    "                        x_min = x_center - width / 2\n",
    "                        y_min = y_center - height / 2\n",
    "                        x_max = x_center + width / 2\n",
    "                        y_max = y_center + height / 2\n",
    "                        \n",
    "                        boxes.append([x_min, y_min, x_max, y_max])\n",
    "                        labels.append(class_id + 1)  # Faster R-CNN uses 1-based indexing (0 = background)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        # Image ID\n",
    "        image_id = torch.tensor([idx])\n",
    "        \n",
    "        # Area (for COCO evaluation)\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        # No crowd instances\n",
    "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        \n",
    "        # Target dictionary\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": iscrowd\n",
    "        }\n",
    "        \n",
    "        # Convert image to tensor\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = torchvision.transforms.functional.to_tensor(image)\n",
    "        \n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b726aa33",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06709a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# DATA LOADING\n",
    "# ========================================\n",
    "# Create datasets\n",
    "train_dataset = PCBDefectDataset(\n",
    "    image_dir='pcb_dataset/images/train',\n",
    "    label_dir='pcb_dataset/labels/train'\n",
    ")\n",
    "val_dataset = PCBDefectDataset(\n",
    "    image_dir='pcb_dataset/images/val',\n",
    "    label_dir='pcb_dataset/labels/val'\n",
    ")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "# Collate function (handle variable number of boxes per image)\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "# Data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "# ========================================\n",
    "# MODEL INITIALIZATION\n",
    "# ========================================\n",
    "def create_fasterrcnn_model(num_classes):\n",
    "    \"\"\"\n",
    "    Create Faster R-CNN model with ResNet-50-FPN backbone\n",
    "    \n",
    "    Architecture:\n",
    "    - Backbone: ResNet-50 (pre-trained on ImageNet)\n",
    "    - Neck: Feature Pyramid Network (FPN) for multi-scale features\n",
    "    - RPN: Region Proposal Network (generates ~2000 proposals)\n",
    "    - RoI Head: Classification + Bounding Box Regression\n",
    "    \"\"\"\n",
    "    # Load pre-trained model\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the pre-trained head with a new one\n",
    "    # num_classes = 5 defect classes + 1 background class\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "# Initialize model\n",
    "num_classes = 6  # 5 defect classes + 1 background\n",
    "model = create_fasterrcnn_model(num_classes)\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"Model initialized on {device}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6d4def",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6df55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# TRAINING SETUP\n",
    "# ========================================\n",
    "# Optimizer (SGD with momentum)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "# Learning rate scheduler (reduce LR after 10 epochs)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "# ========================================\n",
    "# TRAINING LOOP\n",
    "# ========================================\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for images, targets in data_loader:\n",
    "        # Move to device\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        # Total loss (RPN loss + RoI loss)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Print progress every 50 batches\n",
    "        if num_batches % 50 == 0:\n",
    "            print(f\"  Batch {num_batches}/{len(data_loader)}, Loss: {losses.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss\n",
    "# Training loop\n",
    "num_epochs = 15  # Fewer epochs than YOLOv8 (Faster R-CNN converges faster with pre-trained weights)\n",
    "print(\"\\nStarting Faster R-CNN training...\")\n",
    "print(\"=\" * 60)\n",
    "training_start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    avg_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Time: {epoch_time:.2f} sec\")\n",
    "    print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(\"-\" * 60)\n",
    "training_time = time.time() - training_start_time\n",
    "print(f\"\\nTraining completed in {training_time:.2f} sec ({training_time/60:.2f} min)\")\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'faster_rcnn_pcb_defects.pth')\n",
    "print(\"Model saved to faster_rcnn_pcb_defects.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0159cf",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d939920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# EVALUATION\n",
    "# ========================================\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate Faster R-CNN model\n",
    "    Compute mAP@0.5 and per-class metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            \n",
    "            # Inference\n",
    "            predictions = model(images)\n",
    "            \n",
    "            # Collect predictions and targets\n",
    "            all_predictions.extend([{k: v.to('cpu') for k, v in pred.items()} for pred in predictions])\n",
    "            all_targets.extend([{k: v.to('cpu') for k, v in t.items()} for t in targets])\n",
    "    \n",
    "    # Compute mAP (simplified - full COCO evaluation requires pycocotools)\n",
    "    # Here we compute precision at IoU=0.5\n",
    "    \n",
    "    iou_threshold = 0.5\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "    \n",
    "    for pred, target in zip(all_predictions, all_targets):\n",
    "        pred_boxes = pred['boxes']\n",
    "        pred_labels = pred['labels']\n",
    "        pred_scores = pred['scores']\n",
    "        \n",
    "        target_boxes = target['boxes']\n",
    "        target_labels = target['labels']\n",
    "        \n",
    "        # Filter predictions by confidence threshold\n",
    "        keep = pred_scores > 0.5\n",
    "        pred_boxes = pred_boxes[keep]\n",
    "        pred_labels = pred_labels[keep]\n",
    "        \n",
    "        # Match predictions to targets\n",
    "        matched_targets = set()\n",
    "        \n",
    "        for i, pred_box in enumerate(pred_boxes):\n",
    "            max_iou = 0\n",
    "            max_iou_idx = -1\n",
    "            \n",
    "            for j, target_box in enumerate(target_boxes):\n",
    "                if j in matched_targets:\n",
    "                    continue\n",
    "                \n",
    "                # Compute IoU\n",
    "                iou = compute_iou(pred_box, target_box)\n",
    "                \n",
    "                if iou > max_iou:\n",
    "                    max_iou = iou\n",
    "                    max_iou_idx = j\n",
    "            \n",
    "            # Check if match\n",
    "            if max_iou >= iou_threshold and pred_labels[i] == target_labels[max_iou_idx]:\n",
    "                total_tp += 1\n",
    "                matched_targets.add(max_iou_idx)\n",
    "            else:\n",
    "                total_fp += 1\n",
    "        \n",
    "        # False negatives (unmatched targets)\n",
    "        total_fn += len(target_boxes) - len(matched_targets)\n",
    "    \n",
    "    # Compute metrics\n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Approximate mAP (simplified)\n",
    "    map_50 = (precision + recall) / 2  # Simplified approximation\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'map_50': map_50,\n",
    "        'tp': total_tp,\n",
    "        'fp': total_fp,\n",
    "        'fn': total_fn\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4314ef5",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Function: compute_iou\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    \"\"\"Compute IoU between two boxes [x1, y1, x2, y2]\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    \n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "print(\"\\nEvaluating Faster R-CNN on validation set...\")\n",
    "metrics = evaluate_model(model, val_loader, device)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FASTER R-CNN EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Precision:    {metrics['precision']:.4f}\")\n",
    "print(f\"Recall:       {metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score:     {metrics['f1_score']:.4f}\")\n",
    "print(f\"mAP@0.5:      {metrics['map_50']:.4f}\")\n",
    "print(f\"True Positives:  {metrics['tp']}\")\n",
    "print(f\"False Positives: {metrics['fp']}\")\n",
    "print(f\"False Negatives: {metrics['fn']}\")\n",
    "# ========================================\n",
    "# INFERENCE SPEED COMPARISON\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INFERENCE SPEED COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "# Prepare test images\n",
    "test_images = [val_dataset[i][0] for i in range(10)]\n",
    "test_images_batch = [img.to(device) for img in test_images]\n",
    "# YOLOv8 inference (from previous cell)\n",
    "yolo_model = YOLO('runs/detect/train/weights/best.pt')\n",
    "print(\"\\nYOLOv8 Inference:\")\n",
    "yolo_start = time.time()\n",
    "for img_tensor in test_images:\n",
    "    # Convert tensor to numpy for YOLOv8\n",
    "    img_np = (img_tensor.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "    _ = yolo_model(img_np, verbose=False)\n",
    "yolo_time = time.time() - yolo_start\n",
    "yolo_fps = len(test_images) / yolo_time\n",
    "print(f\"  Total time: {yolo_time:.4f} sec\")\n",
    "print(f\"  Per image:  {yolo_time/len(test_images)*1000:.2f} ms\")\n",
    "print(f\"  FPS:        {yolo_fps:.1f}\")\n",
    "# Faster R-CNN inference\n",
    "print(\"\\nFaster R-CNN Inference:\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    rcnn_start = time.time()\n",
    "    _ = model(test_images_batch)\n",
    "    rcnn_time = time.time() - rcnn_start\n",
    "rcnn_fps = len(test_images) / rcnn_time\n",
    "print(f\"  Total time: {rcnn_time:.4f} sec\")\n",
    "print(f\"  Per image:  {rcnn_time/len(test_images)*1000:.2f} ms\")\n",
    "print(f\"  FPS:        {rcnn_fps:.1f}\")\n",
    "# Comparison\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"SPEED COMPARISON:\")\n",
    "print(f\"  YOLOv8 is {rcnn_time/yolo_time:.1f}\u00d7 faster than Faster R-CNN\")\n",
    "print(f\"  YOLOv8:       {yolo_fps:.1f} FPS\")\n",
    "print(f\"  Faster R-CNN: {rcnn_fps:.1f} FPS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a2277d",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 6\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# VISUALIZATION\n",
    "# ========================================\n",
    "def visualize_predictions_rcnn(model, dataset, device, num_samples=5):\n",
    "    \"\"\"Visualize Faster R-CNN predictions\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(20, 4))\n",
    "    \n",
    "    classes = ['background', 'scratch', 'short', 'open', 'mouse_bite', 'spur']\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'cyan']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, ax in enumerate(axes):\n",
    "            # Get image and target\n",
    "            image, target = dataset[i]\n",
    "            \n",
    "            # Inference\n",
    "            image_tensor = image.to(device).unsqueeze(0)\n",
    "            prediction = model(image_tensor)[0]\n",
    "            \n",
    "            # Convert image to numpy for plotting\n",
    "            img_np = image.permute(1, 2, 0).numpy()\n",
    "            \n",
    "            ax.imshow(img_np)\n",
    "            \n",
    "            # Plot predictions\n",
    "            boxes = prediction['boxes'].cpu().numpy()\n",
    "            labels = prediction['labels'].cpu().numpy()\n",
    "            scores = prediction['scores'].cpu().numpy()\n",
    "            \n",
    "            for box, label, score in zip(boxes, labels, scores):\n",
    "                if score > 0.5:  # Confidence threshold\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    width = x2 - x1\n",
    "                    height = y2 - y1\n",
    "                    \n",
    "                    rect = plt.Rectangle((x1, y1), width, height,\n",
    "                                        fill=False, color=colors[label], linewidth=2)\n",
    "                    ax.add_patch(rect)\n",
    "                    \n",
    "                    # Label\n",
    "                    ax.text(x1, y1-5, f\"{classes[label]} {score:.2f}\",\n",
    "                           color=colors[label], fontsize=8,\n",
    "                           bbox=dict(facecolor='white', alpha=0.7))\n",
    "            \n",
    "            ax.axis('off')\n",
    "            ax.set_title(f\"Sample {i+1}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('faster_rcnn_predictions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPredictions saved to faster_rcnn_predictions.png\")\n",
    "print(\"\\nGenerating Faster R-CNN prediction visualizations...\")\n",
    "visualize_predictions_rcnn(model, val_dataset, device, num_samples=5)\n",
    "# ========================================\n",
    "# FINAL COMPARISON TABLE\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"YOLOV8 VS FASTER R-CNN: COMPREHENSIVE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "comparison_data = {\n",
    "    'Metric': ['mAP@0.5', 'Inference Speed (ms)', 'FPS', 'Model Size (MB)', \n",
    "               'Training Time (min)', 'Parameters (M)', 'GPU Memory (GB)', \n",
    "               'Small Object Detection', 'Real-time Capable', 'Edge Deployable'],\n",
    "    'YOLOv8-Medium': ['0.72', f'{yolo_time/len(test_images)*1000:.1f}', f'{yolo_fps:.1f}', \n",
    "                      '52', '25', '25.9', '2.5', 'Good', '\u2713', '\u2713'],\n",
    "    'Faster R-CNN': [f'{metrics[\"map_50\"]:.2f}', f'{rcnn_time/len(test_images)*1000:.1f}', \n",
    "                     f'{rcnn_fps:.1f}', '167', '20', '41.8', '4.2', 'Excellent', '\u2717', '\u2717']\n",
    "}\n",
    "import pandas as pd\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\", df_comparison.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\u2713 YOLOv8:       Best for real-time applications (production lines, video)\")\n",
    "print(\"\u2713 Faster R-CNN: Best for high-precision tasks (die inspection, medical)\")\n",
    "print(\"\u2713 YOLOv8:       3-5\u00d7 faster, smaller model, edge deployable\")\n",
    "print(\"\u2713 Faster R-CNN: Higher mAP (+0.03-0.05), better small object detection\")\n",
    "print(\"\\nRecommendation for PCB defects: YOLOv8 (real-time requirement > 30 FPS)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n\u2705 Notebook 055 complete!\")\n",
    "print(\"Next: 056_RNN_LSTM_GRU.ipynb - Sequential data analysis\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}