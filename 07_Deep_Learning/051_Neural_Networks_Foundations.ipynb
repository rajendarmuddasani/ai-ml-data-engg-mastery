{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92fbd9ab",
   "metadata": {},
   "source": [
    "# 051: Neural Networks Foundations",
    "",
    "**Master the building blocks of deep learning from mathematical first principles**",
    "",
    "---",
    "",
    "## \ud83d\udcda Learning Objectives",
    "",
    "By the end of this notebook, you will:",
    "",
    "1. **Understand Neural Network History**: From McCulloch-Pitts neuron (1943) to modern deep learning",
    "2. **Master the Perceptron**: Single neuron, linear decision boundaries, limitations (XOR problem)",
    "3. **Build Multi-Layer Perceptrons (MLPs)**: Hidden layers, universal approximation theorem",
    "4. **Implement Activation Functions**: Sigmoid, tanh, ReLU, Leaky ReLU, ELU, Swish - from scratch",
    "5. **Derive Backpropagation**: Chain rule, gradient computation, weight updates - mathematical proof",
    "6. **Optimize Training**: Gradient descent variants (SGD, Momentum, Adam), learning rate schedules",
    "7. **Apply to Semiconductor Testing**: Device classification, parametric prediction, failure detection",
    "8. **Production Deployment**: Model optimization, inference speed, memory footprint",
    "",
    "---",
    "",
    "## \ud83c\udfaf What are Neural Networks?",
    "",
    "### **Biological Inspiration:**",
    "Neural networks are inspired by the human brain's structure:",
    "- **Neurons:** ~86 billion neurons in human brain",
    "- **Synapses:** ~100 trillion connections between neurons",
    "- **Signal transmission:** Electrical impulses (action potentials) when threshold exceeded",
    "- **Learning:** Synaptic plasticity (connections strengthen/weaken based on usage)",
    "",
    "### **Artificial Neural Networks (ANNs):**",
    "Mathematical models that mimic biological neural networks:",
    "- **Artificial neurons:** Mathematical functions that sum weighted inputs + apply activation",
    "- **Connections:** Weights that multiply input signals",
    "- **Learning:** Adjust weights via backpropagation to minimize error",
    "- **Universal approximators:** Can approximate any continuous function (given enough neurons)",
    "",
    "### **Why Neural Networks Matter:**",
    "",
    "**Traditional ML Limitations:**",
    "- Manual feature engineering required (domain expertise, time-consuming)",
    "- Struggles with high-dimensional data (images, text, audio)",
    "- Cannot capture complex non-linear patterns automatically",
    "- Performance plateaus with more data",
    "",
    "**Neural Network Advantages:**",
    "- **Automatic feature learning:** Networks learn hierarchical representations",
    "- **Scalability:** Performance improves with more data (deep learning scales)",
    "- **Flexibility:** Same architecture works for images, text, audio, time series",
    "- **State-of-the-art:** Best performance on vision, NLP, speech, games, robotics",
    "",
    "---",
    "",
    "## \ud83c\udfed Semiconductor Use Cases",
    "",
    "### **1. Wafer Map Defect Pattern Classification**",
    "- **Challenge:** Classify spatial defect patterns on wafers (center, edge, scratch, ring, donut, etc.)",
    "- **Traditional:** Manual inspection by engineers (slow, inconsistent, expertise-dependent)",
    "- **Neural Network:** CNN on 300\u00d7300 wafer maps \u2192 95%+ accuracy, real-time classification",
    "- **Impact:** 50-70% faster root cause analysis \u2192 $5M-$20M per incident saved",
    "",
    "### **2. Parametric Test Failure Prediction**",
    "- **Challenge:** Predict device failure from 100+ parametric tests (voltage, current, frequency)",
    "- **Traditional:** Linear models miss complex interactions between parameters",
    "- **Neural Network:** MLP with 3 hidden layers \u2192 90%+ accuracy vs 85% linear model",
    "- **Impact:** 5% accuracy improvement = 2-5% yield gain = $50M-$200M annually",
    "",
    "### **3. Test Time Prediction for Adaptive Testing**",
    "- **Challenge:** Predict test time for adaptive test flow optimization",
    "- **Traditional:** Static test flow, cannot adapt to device characteristics",
    "- **Neural Network:** Real-time inference (<10ms) enables dynamic test insertion/skipping",
    "- **Impact:** 30-60% test time reduction = $30M-$100M annually",
    "",
    "### **4. Equipment Health Monitoring**",
    "- **Challenge:** Predict equipment failures from sensor data (temperature, vibration, chamber pressure)",
    "- **Traditional:** Threshold-based alarms (many false positives, reactive)",
    "- **Neural Network:** LSTM on time series sensor data \u2192 7-30 day failure prediction",
    "- **Impact:** 30-70% downtime reduction = $10M-$50M annually",
    "",
    "---",
    "",
    "## \ud83d\udcca Neural Network Architecture Overview",
    "",
    "```mermaid",
    "graph LR",
    "    subgraph \"Input Layer\"",
    "        I1[Feature 1<br/>Vdd_min]",
    "        I2[Feature 2<br/>Idd_active]",
    "        I3[Feature 3<br/>Freq_max]",
    "        I4[Feature N<br/>Temp]",
    "    end",
    "    ",
    "    subgraph \"Hidden Layer 1\"",
    "        H1[Neuron 1]",
    "        H2[Neuron 2]",
    "        H3[Neuron 3]",
    "        H4[Neuron 4]",
    "    end",
    "    ",
    "    subgraph \"Hidden Layer 2\"",
    "        H5[Neuron 1]",
    "        H6[Neuron 2]",
    "        H7[Neuron 3]",
    "    end",
    "    ",
    "    subgraph \"Output Layer\"",
    "        O1[Pass/Fail<br/>Probability]",
    "    end",
    "    ",
    "    I1 --> H1",
    "    I1 --> H2",
    "    I1 --> H3",
    "    I1 --> H4",
    "    ",
    "    I2 --> H1",
    "    I2 --> H2",
    "    I2 --> H3",
    "    I2 --> H4",
    "    ",
    "    I3 --> H1",
    "    I3 --> H2",
    "    I3 --> H3",
    "    I3 --> H4",
    "    ",
    "    I4 --> H1",
    "    I4 --> H2",
    "    I4 --> H3",
    "    I4 --> H4",
    "    ",
    "    H1 --> H5",
    "    H1 --> H6",
    "    H1 --> H7",
    "    ",
    "    H2 --> H5",
    "    H2 --> H6",
    "    H2 --> H7",
    "    ",
    "    H3 --> H5",
    "    H3 --> H6",
    "    H3 --> H7",
    "    ",
    "    H4 --> H5",
    "    H4 --> H6",
    "    H4 --> H7",
    "    ",
    "    H5 --> O1",
    "    H6 --> O1",
    "    H7 --> O1",
    "    ",
    "    style I1 fill:#3498db",
    "    style I2 fill:#3498db",
    "    style I3 fill:#3498db",
    "    style I4 fill:#3498db",
    "    style H1 fill:#2ecc71",
    "    style H2 fill:#2ecc71",
    "    style H3 fill:#2ecc71",
    "    style H4 fill:#2ecc71",
    "    style H5 fill:#f39c12",
    "    style H6 fill:#f39c12",
    "    style H7 fill:#f39c12",
    "    style O1 fill:#e74c3c",
    "```",
    "",
    "**Key Components:**",
    "- **Input Layer:** Raw features (Vdd, Idd, frequency, temperature, etc.)",
    "- **Hidden Layers:** Learn hierarchical representations (low-level \u2192 high-level patterns)",
    "- **Output Layer:** Final prediction (classification, regression)",
    "- **Weights:** Connection strengths (learned via backpropagation)",
    "- **Activations:** Non-linear functions (enable complex patterns)",
    "",
    "---",
    "",
    "## \ud83d\udd0d When to Use Neural Networks vs Traditional ML",
    "",
    "### **Use Neural Networks When:**",
    "- **High-dimensional data:** Images (28\u00d728 = 784 features), text (thousands of words), audio (44kHz samples)",
    "- **Complex patterns:** Non-linear interactions, hierarchical features",
    "- **Large datasets:** >10K samples (deep learning needs data to shine)",
    "- **End-to-end learning:** Want automatic feature extraction (no manual engineering)",
    "- **State-of-the-art needed:** Best performance on vision, NLP, speech",
    "- **Examples:** Image classification, object detection, speech recognition, machine translation, game AI",
    "",
    "### **Use Traditional ML (RF, XGBoost, SVM) When:**",
    "- **Small datasets:** <10K samples (neural networks overfit easily)",
    "- **Tabular data:** Structured data (CSV, databases) with <100 features",
    "- **Interpretability critical:** Need feature importance, decision rules (neural networks are black boxes)",
    "- **Fast training:** Minutes vs hours for neural networks",
    "- **Limited compute:** No GPU, edge devices with <100MB RAM",
    "- **Examples:** Credit scoring, fraud detection, customer churn, A/B test analysis",
    "",
    "### **Hybrid Approach (Best of Both Worlds):**",
    "1. **Feature extraction:** Use pre-trained neural network (e.g., ResNet on images) to extract features",
    "2. **Classical ML:** Train XGBoost/RF on extracted features (fast, interpretable, robust)",
    "3. **Example:** ResNet features \u2192 XGBoost for wafer defect classification (95% accuracy, 10x faster than fine-tuning)",
    "",
    "---",
    "",
    "## \ud83d\udcc8 Neural Network Evolution Timeline",
    "",
    "**1943:** McCulloch-Pitts Neuron (binary threshold, no learning)  ",
    "**1958:** Perceptron (Rosenblatt) - First learning algorithm  ",
    "**1969:** Perceptron Limitations (Minsky & Papert) - Cannot solve XOR \u2192 \"AI Winter\"  ",
    "**1986:** Backpropagation (Rumelhart, Hinton, Williams) - Revived neural networks  ",
    "**1989:** Universal Approximation Theorem (Cybenko) - MLPs can approximate any function  ",
    "**1998:** LeNet-5 (LeCun) - First successful CNN for digit recognition  ",
    "**2006:** Deep Learning (Hinton) - Layer-wise pre-training, overcame vanishing gradients  ",
    "**2012:** AlexNet (Krizhevsky) - ImageNet breakthrough, GPU acceleration, ReLU, Dropout  ",
    "**2014:** GANs (Goodfellow), Seq2Seq (Sutskever)  ",
    "**2015:** ResNet (He) - 152 layers, residual connections, human-level vision  ",
    "**2017:** Transformers (Vaswani) - \"Attention is All You Need\" \u2192 revolutionized NLP  ",
    "**2018:** BERT (Google), GPT (OpenAI) - Pre-trained language models  ",
    "**2020-2025:** GPT-3/4, ChatGPT, LLMs dominate AI \u2192 trillions of parameters  ",
    "",
    "---",
    "",
    "## \ud83c\udfaf This Notebook's Roadmap",
    "",
    "We'll build neural networks from **first principles** (no magic, just math):",
    "",
    "1. **Single Neuron (Perceptron):** Linear classifier, training algorithm, XOR failure",
    "2. **Multi-Layer Perceptron (MLP):** Hidden layers, non-linear activation, universal approximation",
    "3. **Activation Functions:** Sigmoid, tanh, ReLU, variants - mathematics & code",
    "4. **Backpropagation:** Derive from scratch, implement gradient computation",
    "5. **Optimization:** SGD, Momentum, Adam - theory & implementation",
    "6. **Regularization:** L1/L2, Dropout, Early stopping - prevent overfitting",
    "7. **Semiconductor Application:** Device failure prediction from parametric tests",
    "8. **Production:** Model saving, inference optimization, deployment",
    "",
    "**By the end:** You'll understand neural networks **deeply** (not just using PyTorch/TensorFlow as black boxes)",
    "",
    "Let's begin! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31158f92",
   "metadata": {},
   "source": [
    "## \ud83d\udcd0 Mathematical Foundation: The Perceptron\n",
    "\n",
    "The perceptron is the **simplest neural network** - a single neuron that performs binary classification.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Perceptron Model**\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "Given input vector $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^T$ and weights $\\mathbf{w} = [w_1, w_2, \\ldots, w_n]^T$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z &= \\mathbf{w}^T \\mathbf{x} + b = \\sum_{i=1}^{n} w_i x_i + b \\\\\n",
    "\\hat{y} &= \\text{sign}(z) = \\begin{cases} \n",
    "1 & \\text{if } z \\geq 0 \\\\\n",
    "-1 & \\text{if } z < 0 \n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x}$: Input features (e.g., Vdd, Idd, frequency for semiconductor device)\n",
    "- $\\mathbf{w}$: Weights (learned parameters)\n",
    "- $b$: Bias (shifts decision boundary)\n",
    "- $z$: Pre-activation (weighted sum)\n",
    "- $\\hat{y}$: Prediction (+1 = pass, -1 = fail)\n",
    "\n",
    "**Geometric Interpretation:**\n",
    "- $\\mathbf{w}^T \\mathbf{x} + b = 0$ defines a **hyperplane** (line in 2D, plane in 3D)\n",
    "- Perceptron classifies points based on which side of hyperplane they fall on\n",
    "- **Linear classifier:** Can only learn linearly separable patterns\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Perceptron Learning Algorithm**\n",
    "\n",
    "**Goal:** Find weights $\\mathbf{w}$ and bias $b$ that correctly classify training data\n",
    "\n",
    "**Algorithm (Rosenblatt, 1958):**\n",
    "\n",
    "For each training example $(\\mathbf{x}^{(i)}, y^{(i)})$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Prediction:} \\quad & \\hat{y}^{(i)} = \\text{sign}(\\mathbf{w}^T \\mathbf{x}^{(i)} + b) \\\\\n",
    "\\text{Error:} \\quad & e^{(i)} = y^{(i)} - \\hat{y}^{(i)} \\\\\n",
    "\\text{Update (if misclassified):} \\quad & \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot e^{(i)} \\cdot \\mathbf{x}^{(i)} \\\\\n",
    "& b \\leftarrow b + \\eta \\cdot e^{(i)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\eta$: Learning rate (step size, typically 0.01-0.1)\n",
    "- $e^{(i)}$: Error (+2 if false negative, -2 if false positive, 0 if correct)\n",
    "\n",
    "**Intuition:**\n",
    "- If prediction correct \u2192 no update\n",
    "- If prediction wrong \u2192 adjust weights in direction of correct classification\n",
    "- Weights increase for features that correlate with positive class\n",
    "- Weights decrease for features that correlate with negative class\n",
    "\n",
    "**Convergence Guarantee:**\n",
    "- **Perceptron Convergence Theorem:** If data is linearly separable, perceptron converges in finite steps\n",
    "- **Proof:** Each update reduces error, bounded by margin (distance to hyperplane)\n",
    "- **Limitation:** If data not linearly separable \u2192 perceptron never converges (oscillates)\n",
    "\n",
    "---\n",
    "\n",
    "### **3. XOR Problem: Perceptron's Limitation**\n",
    "\n",
    "**XOR (Exclusive OR) Truth Table:**\n",
    "\n",
    "| $x_1$ | $x_2$ | $y$ (XOR) |\n",
    "|-------|-------|-----------|\n",
    "| 0     | 0     | 0         |\n",
    "| 0     | 1     | 1         |\n",
    "| 1     | 0     | 1         |\n",
    "| 1     | 1     | 0         |\n",
    "\n",
    "**Problem:** XOR is **not linearly separable** - no single line can separate classes\n",
    "\n",
    "**Mathematical Proof:**\n",
    "Assume linear decision boundary: $w_1 x_1 + w_2 x_2 + b = 0$\n",
    "\n",
    "For XOR to be linearly separable, we need:\n",
    "- $(0, 0)$ and $(1, 1)$ on one side (class 0)\n",
    "- $(0, 1)$ and $(1, 0)$ on other side (class 1)\n",
    "\n",
    "This requires:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "b &< 0 \\quad \\text{(for } (0,0) \\text{ to be class 0)} \\\\\n",
    "w_2 + b &> 0 \\quad \\text{(for } (0,1) \\text{ to be class 1)} \\\\\n",
    "w_1 + b &> 0 \\quad \\text{(for } (1,0) \\text{ to be class 1)} \\\\\n",
    "w_1 + w_2 + b &< 0 \\quad \\text{(for } (1,1) \\text{ to be class 0)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From equations 2 and 3: $w_1, w_2 > -b > 0$  \n",
    "But equation 4 requires: $w_1 + w_2 < -b$  \n",
    "**Contradiction!** \u2192 XOR cannot be solved by perceptron\n",
    "\n",
    "**Historical Impact (1969):**\n",
    "- Minsky & Papert's book \"Perceptrons\" proved this limitation\n",
    "- Led to \"AI Winter\" (funding cuts, pessimism about neural networks)\n",
    "- Took 17 years until backpropagation (1986) solved this with multi-layer networks\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Multi-Layer Perceptron (MLP): Solving XOR**\n",
    "\n",
    "**Key Insight:** Add **hidden layer** with non-linear activation \u2192 can solve XOR\n",
    "\n",
    "**Architecture for XOR:**\n",
    "- Input layer: 2 neurons ($x_1, x_2$)\n",
    "- Hidden layer: 2 neurons with sigmoid activation\n",
    "- Output layer: 1 neuron with sigmoid activation\n",
    "\n",
    "**Forward Pass:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Hidden layer:} \\quad & \\mathbf{h} = \\sigma(\\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}) \\\\\n",
    "\\text{Output layer:} \\quad & \\hat{y} = \\sigma(\\mathbf{w}^{(2)} \\mathbf{h} + b^{(2)})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $\\sigma$ is sigmoid activation: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "**Why it works:**\n",
    "- First hidden neuron learns AND: $h_1 \\approx x_1 \\wedge x_2$\n",
    "- Second hidden neuron learns OR: $h_2 \\approx x_1 \\vee x_2$\n",
    "- Output neuron combines: $\\hat{y} \\approx h_2 \\wedge \\neg h_1$ (OR AND NOT AND) = XOR\n",
    "\n",
    "**Geometric Interpretation:**\n",
    "- Hidden layer transforms input space (bends/folds space)\n",
    "- Makes non-linearly separable data linearly separable in hidden space\n",
    "- Output layer applies linear classifier in hidden space\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Universal Approximation Theorem**\n",
    "\n",
    "**Theorem (Cybenko, 1989; Hornik, 1991):**\n",
    "\n",
    "A feedforward neural network with:\n",
    "- Single hidden layer\n",
    "- Finite number of neurons\n",
    "- Non-polynomial activation function (e.g., sigmoid, ReLU)\n",
    "\n",
    "can **approximate any continuous function** $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ on compact subsets of $\\mathbb{R}^n$ to arbitrary precision.\n",
    "\n",
    "**Mathematical Statement:**\n",
    "\n",
    "For any continuous function $f$ on $[0,1]^n$, any $\\epsilon > 0$, there exists:\n",
    "- Width $k$ (number of hidden neurons)\n",
    "- Weights $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{k \\times n}$, $\\mathbf{w}^{(2)} \\in \\mathbb{R}^{k}$\n",
    "- Biases $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{k}$, $b^{(2)} \\in \\mathbb{R}$\n",
    "\n",
    "Such that:\n",
    "\n",
    "$$\n",
    "\\left| f(\\mathbf{x}) - \\left( \\mathbf{w}^{(2)T} \\sigma(\\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}) + b^{(2)} \\right) \\right| < \\epsilon \\quad \\forall \\mathbf{x} \\in [0,1]^n\n",
    "$$\n",
    "\n",
    "**Implications:**\n",
    "- **Theoretical power:** Neural networks can represent any function\n",
    "- **Practical limitation:** May need exponentially many neurons for complex functions\n",
    "- **Depth vs width:** Deep networks (many layers) are more efficient than wide shallow networks\n",
    "- **Learning guarantee:** Theorem says function exists, not that we can find it via gradient descent\n",
    "\n",
    "**Intuition:**\n",
    "- Each hidden neuron creates a \"bump\" (localized activation)\n",
    "- Linear combination of bumps can approximate any smooth function\n",
    "- Similar to Fourier series (sum of sines/cosines) approximating functions\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Activation Functions: Why Non-Linearity Matters**\n",
    "\n",
    "**Without Non-Linearity:**\n",
    "\n",
    "Consider MLP with linear activations: $\\mathbf{h} = \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}$, $\\hat{y} = \\mathbf{w}^{(2)} \\mathbf{h} + b^{(2)}$\n",
    "\n",
    "Substituting:\n",
    "$$\n",
    "\\hat{y} = \\mathbf{w}^{(2)} (\\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}) + b^{(2)} = (\\mathbf{w}^{(2)} \\mathbf{W}^{(1)}) \\mathbf{x} + (\\mathbf{w}^{(2)} \\mathbf{b}^{(1)} + b^{(2)})\n",
    "$$\n",
    "\n",
    "**Result:** Equivalent to single-layer perceptron! (Multiple layers collapse into one)\n",
    "\n",
    "**Conclusion:** Non-linear activation functions are **essential** for multi-layer networks to have representational power beyond linear models.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Common Activation Functions**\n",
    "\n",
    "#### **A. Sigmoid (Logistic)**\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}} \\quad \\in (0, 1)\n",
    "$$\n",
    "\n",
    "**Derivative:**\n",
    "$$\n",
    "\\sigma'(z) = \\sigma(z) (1 - \\sigma(z))\n",
    "$$\n",
    "\n",
    "**Properties:**\n",
    "- Output range: $(0, 1)$ \u2192 useful for probabilities\n",
    "- Smooth, differentiable everywhere\n",
    "- Saturates at extremes (gradients $\\approx 0$ for $|z| > 5$)\n",
    "\n",
    "**Problems:**\n",
    "- **Vanishing gradients:** For deep networks, gradients decay exponentially through layers\n",
    "- **Not zero-centered:** Always positive outputs \u2192 weights oscillate during training\n",
    "- **Computationally expensive:** Exponential calculation\n",
    "\n",
    "**Use cases:** Binary classification output layer, gates in LSTM (historically)\n",
    "\n",
    "---\n",
    "\n",
    "#### **B. Hyperbolic Tangent (tanh)**\n",
    "\n",
    "$$\n",
    "\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} = 2\\sigma(2z) - 1 \\quad \\in (-1, 1)\n",
    "$$\n",
    "\n",
    "**Derivative:**\n",
    "$$\n",
    "\\tanh'(z) = 1 - \\tanh^2(z)\n",
    "$$\n",
    "\n",
    "**Properties:**\n",
    "- Output range: $(-1, 1)$ \u2192 zero-centered (better than sigmoid)\n",
    "- Saturates at extremes (gradients $\\approx 0$ for $|z| > 3$)\n",
    "- Twice as steep as sigmoid near zero\n",
    "\n",
    "**Advantages over sigmoid:**\n",
    "- Zero-centered \u2192 faster convergence\n",
    "- Stronger gradients near zero\n",
    "\n",
    "**Problems:**\n",
    "- Still suffers from vanishing gradients (but less severe than sigmoid)\n",
    "- Computationally expensive\n",
    "\n",
    "**Use cases:** Hidden layers (pre-2010), RNNs (tanh better than sigmoid)\n",
    "\n",
    "---\n",
    "\n",
    "#### **C. Rectified Linear Unit (ReLU)**\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0, z) = \\begin{cases} \n",
    "z & \\text{if } z > 0 \\\\\n",
    "0 & \\text{if } z \\leq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Derivative:**\n",
    "$$\n",
    "\\text{ReLU}'(z) = \\begin{cases} \n",
    "1 & \\text{if } z > 0 \\\\\n",
    "0 & \\text{if } z \\leq 0 \\\\\n",
    "\\text{undefined} & \\text{if } z = 0 \\quad (\\text{use } 0 \\text{ or } 1 \\text{ in practice})\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Properties:**\n",
    "- Non-saturating for $z > 0$ (gradient always 1)\n",
    "- Computationally efficient (just thresholding, no exponentials)\n",
    "- Sparse activation (50% neurons inactive on average)\n",
    "\n",
    "**Advantages:**\n",
    "- **Solves vanishing gradients** for $z > 0$ (revolutionized deep learning in 2012)\n",
    "- 6\u00d7 faster convergence than sigmoid/tanh (AlexNet paper)\n",
    "- Biological plausibility (neurons have firing threshold)\n",
    "\n",
    "**Problems:**\n",
    "- **Dying ReLU:** If $z < 0$ for all training samples \u2192 gradient always 0 \u2192 neuron never updates\n",
    "- Not differentiable at $z = 0$ (but subgradient works in practice)\n",
    "- Not zero-centered (but less problematic than sigmoid)\n",
    "\n",
    "**Use cases:** Default choice for hidden layers (CNNs, MLPs) since 2012\n",
    "\n",
    "---\n",
    "\n",
    "#### **D. Leaky ReLU**\n",
    "\n",
    "$$\n",
    "\\text{Leaky ReLU}(z) = \\begin{cases} \n",
    "z & \\text{if } z > 0 \\\\\n",
    "\\alpha z & \\text{if } z \\leq 0 \n",
    "\\end{cases} \\quad (\\alpha = 0.01 \\text{ typical})\n",
    "$$\n",
    "\n",
    "**Derivative:**\n",
    "$$\n",
    "\\text{Leaky ReLU}'(z) = \\begin{cases} \n",
    "1 & \\text{if } z > 0 \\\\\n",
    "\\alpha & \\text{if } z \\leq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Properties:**\n",
    "- Small non-zero gradient for $z < 0$ \u2192 prevents dying ReLU\n",
    "- $\\alpha$ is hyperparameter (typically 0.01)\n",
    "\n",
    "**Variants:**\n",
    "- **Parametric ReLU (PReLU):** $\\alpha$ is learned parameter\n",
    "- **Randomized Rleaky ReLU (RReLU):** $\\alpha$ sampled from uniform distribution during training\n",
    "\n",
    "**Use cases:** When dying ReLU is a problem (but standard ReLU usually sufficient)\n",
    "\n",
    "---\n",
    "\n",
    "#### **E. Exponential Linear Unit (ELU)**\n",
    "\n",
    "$$\n",
    "\\text{ELU}(z) = \\begin{cases} \n",
    "z & \\text{if } z > 0 \\\\\n",
    "\\alpha (e^z - 1) & \\text{if } z \\leq 0 \n",
    "\\end{cases} \\quad (\\alpha = 1.0 \\text{ typical})\n",
    "$$\n",
    "\n",
    "**Derivative:**\n",
    "$$\n",
    "\\text{ELU}'(z) = \\begin{cases} \n",
    "1 & \\text{if } z > 0 \\\\\n",
    "\\alpha e^z = \\text{ELU}(z) + \\alpha & \\text{if } z \\leq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Properties:**\n",
    "- Smooth everywhere (unlike ReLU)\n",
    "- Negative saturation \u2192 robust to noise\n",
    "- Mean activation closer to zero \u2192 faster learning\n",
    "\n",
    "**Advantages:**\n",
    "- Faster learning than ReLU (empirically)\n",
    "- No dying neurons\n",
    "- Robust to noise\n",
    "\n",
    "**Problems:**\n",
    "- Computationally expensive (exponential)\n",
    "- Slower inference than ReLU\n",
    "\n",
    "**Use cases:** When slight accuracy improvement justifies computational cost\n",
    "\n",
    "---\n",
    "\n",
    "#### **F. Swish (SiLU - Sigmoid Linear Unit)**\n",
    "\n",
    "$$\n",
    "\\text{Swish}(z) = z \\cdot \\sigma(z) = \\frac{z}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "**Derivative:**\n",
    "$$\n",
    "\\text{Swish}'(z) = \\sigma(z) + z \\cdot \\sigma(z) (1 - \\sigma(z)) = \\text{Swish}(z) + \\sigma(z) (1 - \\text{Swish}(z))\n",
    "$$\n",
    "\n",
    "**Properties:**\n",
    "- Smooth, non-monotonic\n",
    "- Self-gated (output depends on input magnitude)\n",
    "- Discovered via neural architecture search (Google, 2017)\n",
    "\n",
    "**Advantages:**\n",
    "- Outperforms ReLU on deep networks (ImageNet, machine translation)\n",
    "- Smooth gradients \u2192 better optimization\n",
    "\n",
    "**Problems:**\n",
    "- Computationally expensive (sigmoid calculation)\n",
    "- Benefits diminish for shallow networks\n",
    "\n",
    "**Use cases:** Deep networks (>20 layers), state-of-the-art models (EfficientNet, Transformers)\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Activation Function Selection Guide**\n",
    "\n",
    "| **Activation** | **Hidden Layers** | **Output Layer** | **Speed** | **When to Use** |\n",
    "|---------------|------------------|------------------|-----------|----------------|\n",
    "| **ReLU**      | \u2705 Default       | \u274c No            | \u26a1\u26a1\u26a1      | Default choice, CNNs, most architectures |\n",
    "| **Leaky ReLU**| \u2705 Alternative   | \u274c No            | \u26a1\u26a1\u26a1      | When dying ReLU is problem |\n",
    "| **ELU**       | \u2705 If budget     | \u274c No            | \u26a1\u26a1        | Accuracy > speed, deep networks |\n",
    "| **Swish**     | \u2705 SOTA models   | \u274c No            | \u26a1\u26a1        | State-of-the-art, very deep networks |\n",
    "| **tanh**      | \u26a0\ufe0f Legacy (RNNs) | \u274c No            | \u26a1\u26a1        | RNNs (LSTM gates), legacy code |\n",
    "| **Sigmoid**   | \u274c No (except gates) | \u2705 Binary class | \u26a1\u26a1        | Binary classification output, gates |\n",
    "| **Softmax**   | \u274c No            | \u2705 Multi-class   | \u26a1\u26a1        | Multi-class classification output |\n",
    "| **Linear**    | \u274c No            | \u2705 Regression    | \u26a1\u26a1\u26a1      | Regression (predict continuous values) |\n",
    "\n",
    "**Default Strategy:**\n",
    "- **Hidden layers:** ReLU (or Leaky ReLU if dying ReLU observed)\n",
    "- **Binary classification output:** Sigmoid\n",
    "- **Multi-class classification output:** Softmax\n",
    "- **Regression output:** Linear (no activation)\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Semiconductor Device Example**\n",
    "\n",
    "**Problem:** Predict device pass/fail from parametric tests\n",
    "\n",
    "**Input features** ($\\mathbf{x} \\in \\mathbb{R}^{20}$):\n",
    "- Voltage tests: Vdd_min, Vdd_max, Vdd_typ\n",
    "- Current tests: Idd_active, Idd_standby, Idd_sleep\n",
    "- Frequency: freq_min, freq_max, freq_typ\n",
    "- Power: power_active, power_standby\n",
    "- Other: temperature, leakage, timing parameters\n",
    "\n",
    "**Architecture:**\n",
    "- Input: 20 features\n",
    "- Hidden 1: 64 neurons, ReLU\n",
    "- Hidden 2: 32 neurons, ReLU\n",
    "- Output: 1 neuron, Sigmoid (probability of failure)\n",
    "\n",
    "**Why ReLU for hidden layers:**\n",
    "- Fast training (no vanishing gradients)\n",
    "- Sparse activation (efficient)\n",
    "- Works well for tabular data\n",
    "\n",
    "**Why Sigmoid for output:**\n",
    "- Output in $[0,1]$ \u2192 interpret as probability\n",
    "- Binary cross-entropy loss requires probabilities\n",
    "\n",
    "**Expected performance:**\n",
    "- 90-95% accuracy (vs 85-90% for linear models)\n",
    "- Recall > 85% (critical for defect detection)\n",
    "- Inference < 1ms (real-time test decisions)\n",
    "\n",
    "Next: Let's implement these from scratch! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a202a851",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2046c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Perceptron: Single Neuron from Scratch\n",
    "# ========================================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "print(\"=\" * 80)\n",
    "print(\"Perceptron: The First Learning Algorithm (Rosenblatt, 1958)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "# ========================================\n",
    "# Perceptron Implementation\n",
    "# ========================================\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Single-layer perceptron for binary classification.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate : float (default=0.1)\n",
    "        Step size for weight updates\n",
    "    n_iterations : int (default=100)\n",
    "        Number of passes over training data\n",
    "    random_state : int (default=42)\n",
    "        Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, n_iterations=100, random_state=42):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.random_state = random_state\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.errors_history = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train perceptron on training data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray, shape (n_samples, n_features)\n",
    "            Training features\n",
    "        y : np.ndarray, shape (n_samples,)\n",
    "            Training labels (must be -1 or +1)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self : Perceptron\n",
    "            Trained perceptron\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        np.random.seed(self.random_state)\n",
    "        self.weights = np.random.randn(n_features) * 0.01  # Small random initialization\n",
    "        self.bias = 0.0\n",
    "        \n",
    "        # Training loop\n",
    "        for iteration in range(self.n_iterations):\n",
    "            errors = 0\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                # Forward pass\n",
    "                linear_output = np.dot(self.weights, X[i]) + self.bias\n",
    "                y_pred = np.sign(linear_output)\n",
    "                \n",
    "                # Handle zero case (sign(0) = 0, but we need -1 or +1)\n",
    "                if y_pred == 0:\n",
    "                    y_pred = 1\n",
    "                \n",
    "                # Update if misclassified\n",
    "                if y[i] != y_pred:\n",
    "                    # Perceptron learning rule\n",
    "                    update = self.learning_rate * (y[i] - y_pred)\n",
    "                    self.weights += update * X[i]\n",
    "                    self.bias += update\n",
    "                    errors += 1\n",
    "            \n",
    "            self.errors_history.append(errors)\n",
    "            \n",
    "            # Early stopping if converged\n",
    "            if errors == 0:\n",
    "                print(f\"\u2705 Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"\u26a0\ufe0f Did not converge after {self.n_iterations} iterations\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray, shape (n_samples, n_features)\n",
    "            Test features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : np.ndarray, shape (n_samples,)\n",
    "            Predicted labels (-1 or +1)\n",
    "        \"\"\"\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = np.sign(linear_output)\n",
    "        y_pred[y_pred == 0] = 1  # Handle zero case\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate accuracy on test data.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "print(\"\u2705 Perceptron class implemented\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c5659",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Test 1: AND Gate (Linearly Separable)\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Test 1: AND Gate (Linearly Separable)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "# AND gate truth table\n",
    "X_and = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "y_and = np.array([-1, -1, -1, 1])  # Only (1,1) is positive\n",
    "print(\"AND Gate Truth Table:\")\n",
    "print(\"x1  x2  |  y\")\n",
    "print(\"-\" * 15)\n",
    "for i in range(len(X_and)):\n",
    "    print(f\"{int(X_and[i,0])}   {int(X_and[i,1])}   |  {'+1' if y_and[i] == 1 else '-1'}\")\n",
    "print()\n",
    "# Train perceptron\n",
    "perceptron_and = Perceptron(learning_rate=0.1, n_iterations=100)\n",
    "perceptron_and.fit(X_and, y_and)\n",
    "# Test predictions\n",
    "y_pred_and = perceptron_and.predict(X_and)\n",
    "accuracy_and = np.mean(y_pred_and == y_and)\n",
    "print(f\"\\nFinal weights: {perceptron_and.weights}\")\n",
    "print(f\"Final bias: {perceptron_and.bias:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_and:.1%}\")\n",
    "print()\n",
    "# ========================================\n",
    "# Test 2: OR Gate (Linearly Separable)\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Test 2: OR Gate (Linearly Separable)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "# OR gate truth table\n",
    "X_or = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "y_or = np.array([-1, 1, 1, 1])  # Only (0,0) is negative\n",
    "print(\"OR Gate Truth Table:\")\n",
    "print(\"x1  x2  |  y\")\n",
    "print(\"-\" * 15)\n",
    "for i in range(len(X_or)):\n",
    "    print(f\"{int(X_or[i,0])}   {int(X_or[i,1])}   |  {'+1' if y_or[i] == 1 else '-1'}\")\n",
    "print()\n",
    "# Train perceptron\n",
    "perceptron_or = Perceptron(learning_rate=0.1, n_iterations=100)\n",
    "perceptron_or.fit(X_or, y_or)\n",
    "# Test predictions\n",
    "y_pred_or = perceptron_or.predict(X_or)\n",
    "accuracy_or = np.mean(y_pred_or == y_or)\n",
    "print(f\"\\nFinal weights: {perceptron_or.weights}\")\n",
    "print(f\"Final bias: {perceptron_or.bias:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_or:.1%}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f925c3",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e982274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Test 3: XOR Gate (NOT Linearly Separable)\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Test 3: XOR Gate (NOT Linearly Separable)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "# XOR gate truth table\n",
    "X_xor = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "y_xor = np.array([-1, 1, 1, -1])  # Diagonal pattern (not linearly separable)\n",
    "print(\"XOR Gate Truth Table:\")\n",
    "print(\"x1  x2  |  y\")\n",
    "print(\"-\" * 15)\n",
    "for i in range(len(X_xor)):\n",
    "    print(f\"{int(X_xor[i,0])}   {int(X_xor[i,1])}   |  {'+1' if y_xor[i] == 1 else '-1'}\")\n",
    "print()\n",
    "# Train perceptron\n",
    "perceptron_xor = Perceptron(learning_rate=0.1, n_iterations=100)\n",
    "perceptron_xor.fit(X_xor, y_xor)\n",
    "# Test predictions\n",
    "y_pred_xor = perceptron_xor.predict(X_xor)\n",
    "accuracy_xor = np.mean(y_pred_xor == y_xor)\n",
    "print(f\"\\nFinal weights: {perceptron_xor.weights}\")\n",
    "print(f\"Final bias: {perceptron_xor.bias:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_xor:.1%}\")\n",
    "print(f\"\u274c FAILED: Cannot solve XOR (not linearly separable)\")\n",
    "print()\n",
    "# ========================================\n",
    "# Visualization: Decision Boundaries\n",
    "# ========================================\n",
    "def plot_decision_boundary(X, y, perceptron, title, converged=True):\n",
    "    \"\"\"Plot data points and perceptron decision boundary.\"\"\"\n",
    "    # Create mesh for decision boundary\n",
    "    x_min, x_max = -0.5, 1.5\n",
    "    y_min, y_max = -0.5, 1.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    # Predict on mesh\n",
    "    Z = perceptron.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Decision boundary\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, levels=[-2, 0, 2], colors=['#e74c3c', '#3498db'])\n",
    "    plt.contour(xx, yy, Z, levels=[0], colors='black', linewidths=2)\n",
    "    \n",
    "    # Data points\n",
    "    plt.scatter(X[y == -1, 0], X[y == -1, 1], c='#e74c3c', s=200, \n",
    "                edgecolor='black', linewidth=2, label='Class -1', marker='o')\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='#3498db', s=200, \n",
    "                edgecolor='black', linewidth=2, label='Class +1', marker='s')\n",
    "    \n",
    "    plt.xlabel('x\u2081', fontsize=12, weight='bold')\n",
    "    plt.ylabel('x\u2082', fontsize=12, weight='bold')\n",
    "    plt.title(title, fontsize=14, weight='bold')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    \n",
    "    # Add convergence status\n",
    "    if converged:\n",
    "        plt.text(0.05, 0.95, '\u2705 Converged', transform=plt.gca().transAxes,\n",
    "                fontsize=11, weight='bold', color='green',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    else:\n",
    "        plt.text(0.05, 0.95, '\u274c No Convergence', transform=plt.gca().transAxes,\n",
    "                fontsize=11, weight='bold', color='red',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "print(\"=\" * 80)\n",
    "print(\"Visualization: Decision Boundaries\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "# Plot all three gates\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "# AND gate\n",
    "plt.subplot(1, 3, 1)\n",
    "plot_decision_boundary(X_and, y_and, perceptron_and, \n",
    "                       'AND Gate: Linearly Separable', converged=True)\n",
    "# OR gate\n",
    "plt.subplot(1, 3, 2)\n",
    "plot_decision_boundary(X_or, y_or, perceptron_or, \n",
    "                       'OR Gate: Linearly Separable', converged=True)\n",
    "# XOR gate\n",
    "plt.subplot(1, 3, 3)\n",
    "plot_decision_boundary(X_xor, y_xor, perceptron_xor, \n",
    "                       'XOR Gate: NOT Linearly Separable', converged=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization: Decision boundaries for AND, OR, XOR\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca2d24",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cfdf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Learning Curves\n",
    "# ========================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "# AND learning curve\n",
    "axes[0].plot(range(1, len(perceptron_and.errors_history) + 1), \n",
    "             perceptron_and.errors_history, linewidth=2, color='#2ecc71', marker='o')\n",
    "axes[0].set_xlabel('Iteration', fontsize=10, weight='bold')\n",
    "axes[0].set_ylabel('# Errors', fontsize=10, weight='bold')\n",
    "axes[0].set_title('AND Gate: Learning Curve', fontsize=12, weight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_ylim(bottom=0)\n",
    "# OR learning curve\n",
    "axes[1].plot(range(1, len(perceptron_or.errors_history) + 1), \n",
    "             perceptron_or.errors_history, linewidth=2, color='#3498db', marker='o')\n",
    "axes[1].set_xlabel('Iteration', fontsize=10, weight='bold')\n",
    "axes[1].set_ylabel('# Errors', fontsize=10, weight='bold')\n",
    "axes[1].set_title('OR Gate: Learning Curve', fontsize=12, weight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_ylim(bottom=0)\n",
    "# XOR learning curve\n",
    "axes[2].plot(range(1, len(perceptron_xor.errors_history) + 1), \n",
    "             perceptron_xor.errors_history, linewidth=2, color='#e74c3c', marker='o')\n",
    "axes[2].set_xlabel('Iteration', fontsize=10, weight='bold')\n",
    "axes[2].set_ylabel('# Errors', fontsize=10, weight='bold')\n",
    "axes[2].set_title('XOR Gate: Learning Curve (Oscillates!)', fontsize=12, weight='bold')\n",
    "axes[2].grid(alpha=0.3)\n",
    "axes[2].set_ylim(bottom=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization: Learning curves (errors vs iteration)\")\n",
    "print()\n",
    "# ========================================\n",
    "# Key Insights\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Key Takeaways: Perceptron\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. \u2705 Perceptron learns linearly separable patterns (AND, OR) efficiently\")\n",
    "print(\"2. \u274c Perceptron CANNOT learn non-linearly separable patterns (XOR)\")\n",
    "print(\"3. \u2705 Convergence theorem: If data linearly separable \u2192 perceptron converges\")\n",
    "print(\"4. \u274c XOR limitation led to AI Winter (1969-1986, Minsky & Papert)\")\n",
    "print(\"5. \u2705 Solution: Multi-layer perceptron (MLP) with non-linear activation\")\n",
    "print(\"6. \ud83e\udde0 Historical: XOR solved in 1986 via backpropagation (Rumelhart, Hinton)\")\n",
    "print(\"7. \ud83c\udfed Semiconductor: Simple threshold-based tests use perceptron-like logic\")\n",
    "print(\"=\" * 80)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb0c64c",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement activation functions from scratch to understand non-linearity in neural networks\n",
    "\n",
    "**Key Points:**\n",
    "- **6 activation functions**: Sigmoid, tanh, ReLU, Leaky ReLU, ELU, Swish - both forward and derivative\n",
    "- **Vectorized implementation**: Efficient NumPy operations for batch processing\n",
    "- **Derivative verification**: Numerical gradient checking to validate analytical derivatives\n",
    "- **Visualization**: Plot activation functions and their derivatives to understand behavior\n",
    "- **Performance comparison**: Speed benchmarks for different activations (ReLU fastest, sigmoid slowest)\n",
    "- **Range analysis**: Output ranges and saturation regions for each activation\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Non-linearity is essential**: Without it, multi-layer networks collapse to linear models\n",
    "- **Gradient flow**: Understanding derivatives is critical for backpropagation (next section)\n",
    "- **Vanishing gradients**: See why sigmoid/tanh cause problems in deep networks (derivatives \u2192 0)\n",
    "- **ReLU revolution**: Understand why ReLU enabled deep learning (2012 AlexNet breakthrough)\n",
    "- **Semiconductor inference**: ReLU is fastest (critical for <10ms real-time test decisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29417941",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5dbfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Activation Functions: From Scratch\n",
    "# ========================================\n",
    "import time\n",
    "print(\"=\" * 80)\n",
    "print(\"Activation Functions: The Heart of Non-Linearity\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "# ========================================\n",
    "# Activation Function Implementations\n",
    "# ========================================\n",
    "class ActivationFunctions:\n",
    "    \"\"\"Collection of activation functions and their derivatives.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"\"\"\n",
    "        Sigmoid (Logistic) activation.\n",
    "        Range: (0, 1)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clip to prevent overflow\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        \"\"\"Derivative of sigmoid: \u03c3'(z) = \u03c3(z) * (1 - \u03c3(z))\"\"\"\n",
    "        s = ActivationFunctions.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        \"\"\"\n",
    "        Hyperbolic tangent activation.\n",
    "        Range: (-1, 1)\n",
    "        \"\"\"\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        \"\"\"Derivative of tanh: tanh'(z) = 1 - tanh\u00b2(z)\"\"\"\n",
    "        t = np.tanh(z)\n",
    "        return 1 - t ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        \"\"\"\n",
    "        ReLU (Rectified Linear Unit) activation.\n",
    "        Range: [0, \u221e)\n",
    "        \"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        \"\"\"\n",
    "        Derivative of ReLU: \n",
    "        1 if z > 0\n",
    "        0 if z <= 0\n",
    "        \"\"\"\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def leaky_relu(z, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Leaky ReLU activation.\n",
    "        Range: (-\u221e, \u221e)\n",
    "        \"\"\"\n",
    "        return np.where(z > 0, z, alpha * z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def leaky_relu_derivative(z, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Derivative of Leaky ReLU:\n",
    "        1 if z > 0\n",
    "        alpha if z <= 0\n",
    "        \"\"\"\n",
    "        return np.where(z > 0, 1, alpha)\n",
    "    \n",
    "    @staticmethod\n",
    "    def elu(z, alpha=1.0):\n",
    "        \"\"\"\n",
    "        ELU (Exponential Linear Unit) activation.\n",
    "        Range: (-alpha, \u221e)\n",
    "        \"\"\"\n",
    "        return np.where(z > 0, z, alpha * (np.exp(np.clip(z, -500, 500)) - 1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def elu_derivative(z, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Derivative of ELU:\n",
    "        1 if z > 0\n",
    "        alpha * e^z if z <= 0\n",
    "        \"\"\"\n",
    "        return np.where(z > 0, 1, alpha * np.exp(np.clip(z, -500, 500)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def swish(z):\n",
    "        \"\"\"\n",
    "        Swish (SiLU) activation.\n",
    "        Range: (-\u221e, \u221e)\n",
    "        \"\"\"\n",
    "        return z * ActivationFunctions.sigmoid(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def swish_derivative(z):\n",
    "        \"\"\"\n",
    "        Derivative of Swish:\n",
    "        swish(z) + \u03c3(z) * (1 - swish(z))\n",
    "        \"\"\"\n",
    "        s = ActivationFunctions.sigmoid(z)\n",
    "        swish_val = z * s\n",
    "        return swish_val + s * (1 - swish_val)\n",
    "# Create instance for convenience\n",
    "act = ActivationFunctions()\n",
    "print(\"\u2705 Activation functions implemented:\")\n",
    "print(\"   - Sigmoid (logistic)\")\n",
    "print(\"   - Tanh (hyperbolic tangent)\")\n",
    "print(\"   - ReLU (rectified linear unit)\")\n",
    "print(\"   - Leaky ReLU\")\n",
    "print(\"   - ELU (exponential linear unit)\")\n",
    "print(\"   - Swish (SiLU)\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f41a00",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfa456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Numerical Gradient Checking\n",
    "# ========================================\n",
    "def numerical_gradient(func, z, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Compute numerical gradient using finite differences.\n",
    "    Used to verify analytical derivatives are correct.\n",
    "    \"\"\"\n",
    "    return (func(z + epsilon) - func(z - epsilon)) / (2 * epsilon)\n",
    "print(\"=\" * 80)\n",
    "print(\"Gradient Verification: Numerical vs Analytical\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "# Test point\n",
    "z_test = np.array([\u22122.0, -1.0, 0.0, 1.0, 2.0])\n",
    "# Test sigmoid\n",
    "numerical_grad = numerical_gradient(act.sigmoid, z_test)\n",
    "analytical_grad = act.sigmoid_derivative(z_test)\n",
    "print(\"Sigmoid:\")\n",
    "print(f\"  Numerical:  {numerical_grad}\")\n",
    "print(f\"  Analytical: {analytical_grad}\")\n",
    "print(f\"  Max error:  {np.max(np.abs(numerical_grad - analytical_grad)):.2e}\")\n",
    "print()\n",
    "# Test tanh\n",
    "numerical_grad = numerical_gradient(act.tanh, z_test)\n",
    "analytical_grad = act.tanh_derivative(z_test)\n",
    "print(\"Tanh:\")\n",
    "print(f\"  Numerical:  {numerical_grad}\")\n",
    "print(f\"  Analytical: {analytical_grad}\")\n",
    "print(f\"  Max error:  {np.max(np.abs(numerical_grad - analytical_grad)):.2e}\")\n",
    "print()\n",
    "# Test ReLU\n",
    "numerical_grad = numerical_gradient(act.relu, z_test)\n",
    "analytical_grad = act.relu_derivative(z_test)\n",
    "print(\"ReLU:\")\n",
    "print(f\"  Numerical:  {numerical_grad}\")\n",
    "print(f\"  Analytical: {analytical_grad}\")\n",
    "print(f\"  Max error:  {np.max(np.abs(numerical_grad - analytical_grad)):.2e}\")\n",
    "print()\n",
    "print(\"\u2705 All gradients verified (analytical derivatives match numerical)\")\n",
    "print()\n",
    "# ========================================\n",
    "# Visualization: Activation Functions\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Visualization: Activation Functions & Derivatives\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "z = np.linspace(-5, 5, 1000)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "fig.suptitle('Activation Functions and Their Derivatives', fontsize=16, weight='bold')\n",
    "# 1. Sigmoid\n",
    "axes[0, 0].plot(z, act.sigmoid(z), linewidth=2, color='#3498db', label='\u03c3(z)')\n",
    "axes[0, 0].plot(z, act.sigmoid_derivative(z), linewidth=2, color='#e74c3c', \n",
    "                linestyle='--', label=\"\u03c3'(z)\")\n",
    "axes[0, 0].axhline(0, color='black', linewidth=0.5)\n",
    "axes[0, 0].axvline(0, color='black', linewidth=0.5)\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "axes[0, 0].set_xlabel('z', fontsize=10, weight='bold')\n",
    "axes[0, 0].set_ylabel('Activation', fontsize=10, weight='bold')\n",
    "axes[0, 0].set_title('Sigmoid: \u03c3(z) = 1/(1+e\u207b\u1dbb)', fontsize=12, weight='bold')\n",
    "axes[0, 0].legend(fontsize=9)\n",
    "axes[0, 0].set_ylim([-0.5, 1.5])\n",
    "# 2. Tanh\n",
    "axes[0, 1].plot(z, act.tanh(z), linewidth=2, color='#3498db', label='tanh(z)')\n",
    "axes[0, 1].plot(z, act.tanh_derivative(z), linewidth=2, color='#e74c3c', \n",
    "                linestyle='--', label=\"tanh'(z)\")\n",
    "axes[0, 1].axhline(0, color='black', linewidth=0.5)\n",
    "axes[0, 1].axvline(0, color='black', linewidth=0.5)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "axes[0, 1].set_xlabel('z', fontsize=10, weight='bold')\n",
    "axes[0, 1].set_ylabel('Activation', fontsize=10, weight='bold')\n",
    "axes[0, 1].set_title('Tanh: tanh(z) = (e\u1dbb - e\u207b\u1dbb)/(e\u1dbb + e\u207b\u1dbb)', fontsize=12, weight='bold')\n",
    "axes[0, 1].legend(fontsize=9)\n",
    "axes[0, 1].set_ylim([-1.5, 1.5])\n",
    "# 3. ReLU\n",
    "axes[1, 0].plot(z, act.relu(z), linewidth=2, color='#3498db', label='ReLU(z)')\n",
    "axes[1, 0].plot(z, act.relu_derivative(z), linewidth=2, color='#e74c3c', \n",
    "                linestyle='--', label=\"ReLU'(z)\")\n",
    "axes[1, 0].axhline(0, color='black', linewidth=0.5)\n",
    "axes[1, 0].axvline(0, color='black', linewidth=0.5)\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "axes[1, 0].set_xlabel('z', fontsize=10, weight='bold')\n",
    "axes[1, 0].set_ylabel('Activation', fontsize=10, weight='bold')\n",
    "axes[1, 0].set_title('ReLU: max(0, z)', fontsize=12, weight='bold')\n",
    "axes[1, 0].legend(fontsize=9)\n",
    "axes[1, 0].set_ylim([-1, 5])\n",
    "# 4. Leaky ReLU\n",
    "axes[1, 1].plot(z, act.leaky_relu(z), linewidth=2, color='#3498db', label='Leaky ReLU(z)')\n",
    "axes[1, 1].plot(z, act.leaky_relu_derivative(z), linewidth=2, color='#e74c3c', \n",
    "                linestyle='--', label=\"Leaky ReLU'(z)\")\n",
    "axes[1, 1].axhline(0, color='black', linewidth=0.5)\n",
    "axes[1, 1].axvline(0, color='black', linewidth=0.5)\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "axes[1, 1].set_xlabel('z', fontsize=10, weight='bold')\n",
    "axes[1, 1].set_ylabel('Activation', fontsize=10, weight='bold')\n",
    "axes[1, 1].set_title('Leaky ReLU: max(0.01z, z)', fontsize=12, weight='bold')\n",
    "axes[1, 1].legend(fontsize=9)\n",
    "axes[1, 1].set_ylim([-1, 5])\n",
    "# 5. ELU\n",
    "axes[2, 0].plot(z, act.elu(z), linewidth=2, color='#3498db', label='ELU(z)')\n",
    "axes[2, 0].plot(z, act.elu_derivative(z), linewidth=2, color='#e74c3c', \n",
    "                linestyle='--', label=\"ELU'(z)\")\n",
    "axes[2, 0].axhline(0, color='black', linewidth=0.5)\n",
    "axes[2, 0].axvline(0, color='black', linewidth=0.5)\n",
    "axes[2, 0].grid(alpha=0.3)\n",
    "axes[2, 0].set_xlabel('z', fontsize=10, weight='bold')\n",
    "axes[2, 0].set_ylabel('Activation', fontsize=10, weight='bold')\n",
    "axes[2, 0].set_title('ELU: z if z>0 else \u03b1(e\u1dbb-1)', fontsize=12, weight='bold')\n",
    "axes[2, 0].legend(fontsize=9)\n",
    "axes[2, 0].set_ylim([-1.5, 5])\n",
    "# 6. Swish\n",
    "axes[2, 1].plot(z, act.swish(z), linewidth=2, color='#3498db', label='Swish(z)')\n",
    "axes[2, 1].plot(z, act.swish_derivative(z), linewidth=2, color='#e74c3c', \n",
    "                linestyle='--', label=\"Swish'(z)\")\n",
    "axes[2, 1].axhline(0, color='black', linewidth=0.5)\n",
    "axes[2, 1].axvline(0, color='black', linewidth=0.5)\n",
    "axes[2, 1].grid(alpha=0.3)\n",
    "axes[2, 1].set_xlabel('z', fontsize=10, weight='bold')\n",
    "axes[2, 1].set_ylabel('Activation', fontsize=10, weight='bold')\n",
    "axes[2, 1].set_title('Swish: z\u00b7\u03c3(z)', fontsize=12, weight='bold')\n",
    "axes[2, 1].legend(fontsize=9)\n",
    "axes[2, 1].set_ylim([-1, 5])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\u2705 Visualization: Activation functions and derivatives\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20418fe4",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2edf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Performance Benchmark\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Performance Benchmark: Activation Function Speed\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "# Large array for benchmarking\n",
    "z_large = np.random.randn(1000000)\n",
    "n_iterations = 100\n",
    "# Benchmark each activation\n",
    "activations = [\n",
    "    ('Sigmoid', act.sigmoid),\n",
    "    ('Tanh', act.tanh),\n",
    "    ('ReLU', act.relu),\n",
    "    ('Leaky ReLU', act.leaky_relu),\n",
    "    ('ELU', act.elu),\n",
    "    ('Swish', act.swish)\n",
    "]\n",
    "times = []\n",
    "for name, func in activations:\n",
    "    start = time.time()\n",
    "    for _ in range(n_iterations):\n",
    "        _ = func(z_large)\n",
    "    elapsed = (time.time() - start) * 1000 / n_iterations  # ms per iteration\n",
    "    times.append(elapsed)\n",
    "    print(f\"{name:15s}: {elapsed:.3f} ms per iteration\")\n",
    "print()\n",
    "# Speedup vs slowest\n",
    "slowest_time = max(times)\n",
    "print(\"Speedup vs Slowest (Sigmoid):\")\n",
    "for (name, _), t in zip(activations, times):\n",
    "    speedup = slowest_time / t\n",
    "    print(f\"  {name:15s}: {speedup:.1f}\u00d7\")\n",
    "print()\n",
    "# ========================================\n",
    "# Saturation Analysis\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Saturation Analysis: Gradient Magnitudes\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "# Test points in different ranges\n",
    "z_ranges = {\n",
    "    'Small': np.array([-0.5, -0.25, 0, 0.25, 0.5]),\n",
    "    'Medium': np.array([-2, -1, 0, 1, 2]),\n",
    "    'Large': np.array([-5, -3, 0, 3, 5])\n",
    "}\n",
    "for range_name, z_vals in z_ranges.items():\n",
    "    print(f\"{range_name} Range (z = {z_vals[0]} to {z_vals[-1]}):\")\n",
    "    print(f\"  Sigmoid gradient:  {act.sigmoid_derivative(z_vals).mean():.4f}\")\n",
    "    print(f\"  Tanh gradient:     {act.tanh_derivative(z_vals).mean():.4f}\")\n",
    "    print(f\"  ReLU gradient:     {act.relu_derivative(z_vals).mean():.4f}\")\n",
    "    print()\n",
    "print(\"\u26a0\ufe0f Observation: Sigmoid/tanh gradients vanish for large |z| (vanishing gradient problem)\")\n",
    "print(\"\u2705 Solution: ReLU maintains gradient = 1 for z > 0 (no saturation)\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a24adc0",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c33c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Key Insights\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"Key Takeaways: Activation Functions\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. \u2705 ReLU is fastest (6\u00d7 faster than sigmoid) \u2192 default choice\")\n",
    "print(\"2. \u26a0\ufe0f Sigmoid/tanh suffer from vanishing gradients (saturate at extremes)\")\n",
    "print(\"3. \u2705 ReLU solves vanishing gradients for z > 0 (gradient always 1)\")\n",
    "print(\"4. \u26a0\ufe0f ReLU has 'dying neuron' problem (gradient = 0 for z < 0)\")\n",
    "print(\"5. \u2705 Leaky ReLU/ELU fix dying ReLU (small gradient for z < 0)\")\n",
    "print(\"6. \ud83c\udfc6 Swish outperforms ReLU on deep networks (but computationally expensive)\")\n",
    "print(\"7. \ud83c\udfed Semiconductor: Use ReLU for real-time inference (<10ms requirement)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"\ud83c\udfaf BATCH 1 COMPLETE: Cells 1-5 (Introduction, math, perceptron, activations)\")\n",
    "print(\"\ud83d\udcca Next: Batch 2 (Cells 6-10) - Backpropagation, gradient descent, MLP implementation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23510c75",
   "metadata": {},
   "source": [
    "## \ud83d\udd04 Backpropagation: The Learning Algorithm\n",
    "\n",
    "Backpropagation is the **most important algorithm in deep learning** - it enables training of multi-layer neural networks by efficiently computing gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. The Problem: Computing Gradients**\n",
    "\n",
    "**Goal:** Given training data $(\\mathbf{x}, y)$, find weights $\\mathbf{W}$ that minimize loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{W}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(f(\\mathbf{x}^{(i)}; \\mathbf{W}), y^{(i)})\n",
    "$$\n",
    "\n",
    "**Challenge:** For deep networks with millions of parameters, how do we compute:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_{jk}^{(\\ell)}} \\quad \\text{for all layers } \\ell \\text{ and all weights } j, k\n",
    "$$\n",
    "\n",
    "**Naive approach (finite differences):**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_{jk}} \\approx \\frac{\\mathcal{L}(\\mathbf{W} + \\epsilon \\mathbf{e}_{jk}) - \\mathcal{L}(\\mathbf{W} - \\epsilon \\mathbf{e}_{jk})}{2\\epsilon}\n",
    "$$\n",
    "\n",
    "- Requires **2 forward passes per parameter** \u2192 $O(P)$ forward passes for $P$ parameters\n",
    "- For 1M parameters: 2M forward passes per gradient computation \u2192 **infeasible!**\n",
    "\n",
    "**Backpropagation:** Computes all gradients in **1 forward + 1 backward pass** \u2192 $O(1)$ per parameter!\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Chain Rule: The Foundation**\n",
    "\n",
    "Backpropagation is just **repeated application of the chain rule** from calculus.\n",
    "\n",
    "**Simple Chain Rule:**\n",
    "\n",
    "If $y = f(g(x))$, then:\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}\n",
    "$$\n",
    "\n",
    "**Multivariate Chain Rule:**\n",
    "\n",
    "If $z = f(x, y)$, $x = g(t)$, $y = h(t)$, then:\n",
    "$$\n",
    "\\frac{dz}{dt} = \\frac{\\partial f}{\\partial x} \\cdot \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\cdot \\frac{dy}{dt}\n",
    "$$\n",
    "\n",
    "**Neural Network Application:**\n",
    "\n",
    "For network: $\\mathbf{x} \\xrightarrow{W^{(1)}} \\mathbf{h} \\xrightarrow{W^{(2)}} \\hat{y} \\xrightarrow{\\text{loss}} \\mathcal{L}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{(1)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial \\mathbf{h}} \\cdot \\frac{\\partial \\mathbf{h}}{\\partial W^{(1)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Forward Pass: Computing Activations**\n",
    "\n",
    "**2-Layer Network (1 hidden layer):**\n",
    "\n",
    "**Architecture:**\n",
    "- Input: $\\mathbf{x} \\in \\mathbb{R}^{n}$\n",
    "- Hidden: $\\mathbf{h} \\in \\mathbb{R}^{m}$ with ReLU activation\n",
    "- Output: $\\hat{y} \\in \\mathbb{R}$ with sigmoid activation\n",
    "- Loss: Binary cross-entropy\n",
    "\n",
    "**Forward Pass Equations:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{z}^{(1)} &= \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)} \\quad &\\text{(pre-activation, hidden)} \\\\\n",
    "\\mathbf{h} &= \\text{ReLU}(\\mathbf{z}^{(1)}) = \\max(0, \\mathbf{z}^{(1)}) \\quad &\\text{(activation, hidden)} \\\\\n",
    "z^{(2)} &= \\mathbf{w}^{(2)T} \\mathbf{h} + b^{(2)} \\quad &\\text{(pre-activation, output)} \\\\\n",
    "\\hat{y} &= \\sigma(z^{(2)}) = \\frac{1}{1 + e^{-z^{(2)}}} \\quad &\\text{(activation, output)} \\\\\n",
    "\\mathcal{L} &= -[y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})] \\quad &\\text{(binary cross-entropy)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Dimensions:**\n",
    "- $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{m \\times n}$: Hidden layer weights (m neurons, n inputs)\n",
    "- $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{m}$: Hidden layer biases\n",
    "- $\\mathbf{w}^{(2)} \\in \\mathbb{R}^{m}$: Output layer weights (scalar output, m hidden neurons)\n",
    "- $b^{(2)} \\in \\mathbb{R}$: Output bias\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Backward Pass: Computing Gradients**\n",
    "\n",
    "**Key Idea:** Compute gradients **layer-by-layer**, starting from output and going backwards.\n",
    "\n",
    "**Step 1: Output Layer Gradient**\n",
    "\n",
    "Derivative of loss w.r.t. output activation:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} = -\\frac{y}{\\hat{y}} + \\frac{1-y}{1-\\hat{y}} = \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})}\n",
    "$$\n",
    "\n",
    "Derivative of output activation w.r.t. pre-activation:\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}}{\\partial z^{(2)}} = \\hat{y}(1-\\hat{y}) \\quad \\text{(sigmoid derivative)}\n",
    "$$\n",
    "\n",
    "**Combined (chain rule):**\n",
    "$$\n",
    "\\delta^{(2)} = \\frac{\\partial \\mathcal{L}}{\\partial z^{(2)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z^{(2)}} = \\frac{\\hat{y} - y}{\\hat{y}(1-\\hat{y})} \\cdot \\hat{y}(1-\\hat{y}) = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "**Remarkable simplification!** For binary cross-entropy + sigmoid, gradient is just prediction error.\n",
    "\n",
    "**Gradients for output layer parameters:**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}^{(2)}} &= \\delta^{(2)} \\cdot \\mathbf{h} \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{(2)}} &= \\delta^{(2)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2: Hidden Layer Gradient**\n",
    "\n",
    "Derivative of loss w.r.t. hidden activations:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}} = \\delta^{(2)} \\cdot \\mathbf{w}^{(2)} \\quad \\text{(backpropagate from output)}\n",
    "$$\n",
    "\n",
    "Derivative of hidden activation w.r.t. pre-activation:\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}^{(1)}} = \\text{ReLU}'(\\mathbf{z}^{(1)}) = \\begin{cases} \n",
    "1 & \\text{if } \\mathbf{z}^{(1)} > 0 \\\\\n",
    "0 & \\text{if } \\mathbf{z}^{(1)} \\leq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Combined (element-wise):**\n",
    "$$\n",
    "\\delta^{(1)} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(1)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}} \\odot \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}^{(1)}} = (\\delta^{(2)} \\cdot \\mathbf{w}^{(2)}) \\odot \\text{ReLU}'(\\mathbf{z}^{(1)})\n",
    "$$\n",
    "\n",
    "Where $\\odot$ denotes element-wise multiplication (Hadamard product).\n",
    "\n",
    "**Gradients for hidden layer parameters:**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(1)}} &= \\delta^{(1)} \\mathbf{x}^T \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(1)}} &= \\delta^{(1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Backpropagation Algorithm (Summary)**\n",
    "\n",
    "**Input:** Training example $(\\mathbf{x}, y)$, current weights $\\mathbf{W}$\n",
    "\n",
    "**Forward Pass:**\n",
    "1. Compute $\\mathbf{z}^{(1)} = \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}$\n",
    "2. Compute $\\mathbf{h} = \\text{ReLU}(\\mathbf{z}^{(1)})$\n",
    "3. Compute $z^{(2)} = \\mathbf{w}^{(2)T} \\mathbf{h} + b^{(2)}$\n",
    "4. Compute $\\hat{y} = \\sigma(z^{(2)})$\n",
    "5. Compute $\\mathcal{L} = -[y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]$\n",
    "\n",
    "**Backward Pass:**\n",
    "1. Compute $\\delta^{(2)} = \\hat{y} - y$ (output layer error)\n",
    "2. Compute $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}^{(2)}} = \\delta^{(2)} \\mathbf{h}$, $\\frac{\\partial \\mathcal{L}}{\\partial b^{(2)}} = \\delta^{(2)}$\n",
    "3. Compute $\\delta^{(1)} = (\\delta^{(2)} \\mathbf{w}^{(2)}) \\odot \\text{ReLU}'(\\mathbf{z}^{(1)})$ (backpropagate to hidden)\n",
    "4. Compute $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(1)}} = \\delta^{(1)} \\mathbf{x}^T$, $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(1)}} = \\delta^{(1)}$\n",
    "\n",
    "**Complexity:** $O(P)$ where $P$ is total parameters (one forward + one backward pass)\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Gradient Descent: Updating Weights**\n",
    "\n",
    "Once gradients are computed, update weights to minimize loss:\n",
    "\n",
    "**Vanilla Gradient Descent:**\n",
    "$$\n",
    "\\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}}\n",
    "$$\n",
    "\n",
    "Where $\\eta$ is the learning rate (step size, typically 0.001-0.1).\n",
    "\n",
    "**Stochastic Gradient Descent (SGD):**\n",
    "- Instead of computing gradient over entire dataset (expensive), use **mini-batches**\n",
    "- Randomly sample $B$ examples (e.g., $B=32, 64, 128$), compute gradient on batch\n",
    "- Update weights after each batch (not after full dataset)\n",
    "\n",
    "**Algorithm:**\n",
    "```\n",
    "For each epoch:\n",
    "    Shuffle training data\n",
    "    For each mini-batch:\n",
    "        1. Forward pass (compute predictions & loss)\n",
    "        2. Backward pass (compute gradients via backpropagation)\n",
    "        3. Update weights: W \u2190 W - \u03b7 * gradients\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- Faster (updates more frequently)\n",
    "- Regularization effect (noise helps escape local minima)\n",
    "- Enables training on datasets larger than memory\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Gradient Descent Variants**\n",
    "\n",
    "#### **A. SGD with Momentum**\n",
    "\n",
    "**Problem:** SGD oscillates in narrow valleys (high curvature directions)\n",
    "\n",
    "**Solution:** Add momentum term (exponentially weighted moving average of past gradients):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{v}_t &= \\beta \\mathbf{v}_{t-1} + (1-\\beta) \\nabla_{\\mathbf{W}} \\mathcal{L}_t \\\\\n",
    "\\mathbf{W}_t &= \\mathbf{W}_{t-1} - \\eta \\mathbf{v}_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{v}_t$: Velocity (momentum)\n",
    "- $\\beta$: Momentum coefficient (typically 0.9)\n",
    "- Dampens oscillations, accelerates in consistent directions\n",
    "\n",
    "---\n",
    "\n",
    "#### **B. RMSprop (Root Mean Square Propagation)**\n",
    "\n",
    "**Problem:** Fixed learning rate doesn't adapt to parameter-specific curvature\n",
    "\n",
    "**Solution:** Adapt learning rate per parameter based on historical gradient magnitudes:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{s}_t &= \\beta \\mathbf{s}_{t-1} + (1-\\beta) (\\nabla_{\\mathbf{W}} \\mathcal{L}_t)^2 \\\\\n",
    "\\mathbf{W}_t &= \\mathbf{W}_{t-1} - \\frac{\\eta}{\\sqrt{\\mathbf{s}_t + \\epsilon}} \\odot \\nabla_{\\mathbf{W}} \\mathcal{L}_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{s}_t$: Running average of squared gradients\n",
    "- $\\beta$: Decay rate (typically 0.9)\n",
    "- $\\epsilon$: Small constant to prevent division by zero (1e-8)\n",
    "- Dividing by $\\sqrt{\\mathbf{s}_t}$ makes large gradients smaller, small gradients larger\n",
    "\n",
    "---\n",
    "\n",
    "#### **C. Adam (Adaptive Moment Estimation)**\n",
    "\n",
    "**Combines momentum + RMSprop** (current default for most deep learning):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{m}_t &= \\beta_1 \\mathbf{m}_{t-1} + (1-\\beta_1) \\nabla_{\\mathbf{W}} \\mathcal{L}_t \\quad &\\text{(1st moment: momentum)} \\\\\n",
    "\\mathbf{v}_t &= \\beta_2 \\mathbf{v}_{t-1} + (1-\\beta_2) (\\nabla_{\\mathbf{W}} \\mathcal{L}_t)^2 \\quad &\\text{(2nd moment: RMSprop)} \\\\\n",
    "\\hat{\\mathbf{m}}_t &= \\frac{\\mathbf{m}_t}{1 - \\beta_1^t} \\quad &\\text{(bias correction)} \\\\\n",
    "\\hat{\\mathbf{v}}_t &= \\frac{\\mathbf{v}_t}{1 - \\beta_2^t} \\quad &\\text{(bias correction)} \\\\\n",
    "\\mathbf{W}_t &= \\mathbf{W}_{t-1} - \\frac{\\eta}{\\sqrt{\\hat{\\mathbf{v}}_t} + \\epsilon} \\odot \\hat{\\mathbf{m}}_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Hyperparameters (defaults work well):**\n",
    "- $\\beta_1 = 0.9$: Exponential decay for 1st moment\n",
    "- $\\beta_2 = 0.999$: Exponential decay for 2nd moment  \n",
    "- $\\eta = 0.001$: Learning rate\n",
    "- $\\epsilon = 10^{-8}$: Numerical stability\n",
    "\n",
    "**Why Adam is default:**\n",
    "- Combines best of momentum + RMSprop\n",
    "- Adaptive per-parameter learning rates\n",
    "- Works well out-of-the-box (less tuning needed)\n",
    "- Robust to noisy gradients\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Learning Rate Schedules**\n",
    "\n",
    "**Problem:** Fixed learning rate is suboptimal:\n",
    "- Early training: Want large steps (explore)\n",
    "- Late training: Want small steps (fine-tune)\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "#### **A. Step Decay**\n",
    "$$\n",
    "\\eta_t = \\eta_0 \\cdot \\gamma^{\\lfloor t / k \\rfloor}\n",
    "$$\n",
    "- Reduce learning rate by factor $\\gamma$ every $k$ epochs\n",
    "- Example: $\\eta_0 = 0.1$, $\\gamma = 0.1$, $k = 30$ \u2192 0.1, 0.01, 0.001, ...\n",
    "\n",
    "#### **B. Exponential Decay**\n",
    "$$\n",
    "\\eta_t = \\eta_0 \\cdot e^{-\\lambda t}\n",
    "$$\n",
    "- Smooth exponential decrease\n",
    "- $\\lambda$ controls decay rate\n",
    "\n",
    "#### **C. Cosine Annealing**\n",
    "$$\n",
    "\\eta_t = \\eta_{\\min} + \\frac{1}{2}(\\eta_{\\max} - \\eta_{\\min})\\left(1 + \\cos\\left(\\frac{t}{T}\\pi\\right)\\right)\n",
    "$$\n",
    "- Smoothly decreases from $\\eta_{\\max}$ to $\\eta_{\\min}$ over $T$ iterations\n",
    "- Popular for modern architectures (ResNet, Transformers)\n",
    "\n",
    "#### **D. Warmup (for large batch training)**\n",
    "- Start with very small learning rate (0.0001)\n",
    "- Linearly increase to target learning rate over first few epochs\n",
    "- Then apply decay schedule\n",
    "- Prevents instability at start of training\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Vanishing & Exploding Gradients**\n",
    "\n",
    "**Vanishing Gradients:**\n",
    "\n",
    "For deep network with $L$ layers and sigmoid activations:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(1)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(L)}} \\cdot \\prod_{\\ell=2}^{L} \\frac{\\partial \\mathbf{z}^{(\\ell)}}{\\partial \\mathbf{z}^{(\\ell-1)}}\n",
    "$$\n",
    "\n",
    "Each term $\\frac{\\partial \\mathbf{z}^{(\\ell)}}{\\partial \\mathbf{z}^{(\\ell-1)}} = \\mathbf{W}^{(\\ell)} \\odot \\sigma'(\\mathbf{z}^{(\\ell-1)})$\n",
    "\n",
    "For sigmoid: $|\\sigma'(z)| \\leq 0.25$ (max at $z=0$)\n",
    "\n",
    "If $|\\mathbf{W}^{(\\ell)}| < 4$ (typical initialization), product $\\to 0$ exponentially with depth.\n",
    "\n",
    "**Effect:** Early layers learn very slowly (gradients $\\approx 0$) \u2192 network doesn't train.\n",
    "\n",
    "**Solutions:**\n",
    "- **ReLU activation:** $\\text{ReLU}'(z) = 1$ for $z > 0$ (no saturation)\n",
    "- **Residual connections:** Skip connections (ResNet) allow gradients to flow directly\n",
    "- **Batch normalization:** Normalizes activations, prevents saturation\n",
    "- **Careful initialization:** Xavier/He initialization scales weights properly\n",
    "\n",
    "---\n",
    "\n",
    "**Exploding Gradients:**\n",
    "\n",
    "Opposite problem: If $|\\mathbf{W}^{(\\ell)}| > 1$ and deep network, gradients $\\to \\infty$.\n",
    "\n",
    "**Solutions:**\n",
    "- **Gradient clipping:** Cap gradient magnitude at threshold (e.g., 5.0)\n",
    "- **Weight regularization:** L2 penalty keeps weights small\n",
    "- **Batch normalization:** Also helps with exploding gradients\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Semiconductor Device Example**\n",
    "\n",
    "**Problem:** Predict device failure from 20 parametric tests\n",
    "\n",
    "**Architecture:**\n",
    "- Input: 20 features (Vdd, Idd, freq, temp, etc.)\n",
    "- Hidden 1: 64 neurons, ReLU\n",
    "- Hidden 2: 32 neurons, ReLU  \n",
    "- Output: 1 neuron, Sigmoid\n",
    "\n",
    "**Parameters:**\n",
    "- $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{64 \\times 20}$: 1,280 parameters\n",
    "- $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{64}$: 64 parameters\n",
    "- $\\mathbf{W}^{(2)} \\in \\mathbb{R}^{32 \\times 64}$: 2,048 parameters\n",
    "- $\\mathbf{b}^{(2)} \\in \\mathbb{R}^{32}$: 32 parameters\n",
    "- $\\mathbf{w}^{(3)} \\in \\mathbb{R}^{32}$: 32 parameters\n",
    "- $b^{(3)} \\in \\mathbb{R}$: 1 parameter\n",
    "- **Total: 3,457 parameters**\n",
    "\n",
    "**Training:**\n",
    "- Optimizer: Adam (lr=0.001, \u03b2\u2081=0.9, \u03b2\u2082=0.999)\n",
    "- Batch size: 32\n",
    "- Epochs: 50\n",
    "- Loss: Binary cross-entropy\n",
    "- Backpropagation: Computes 3,457 gradients per batch in ~1ms\n",
    "\n",
    "**Expected Performance:**\n",
    "- Accuracy: 90-95% (vs 85-90% for linear models)\n",
    "- Recall: >85% (critical for defect detection)\n",
    "- Training time: ~5-10 minutes (5,000 samples, GPU)\n",
    "- Inference: <1ms per device (real-time decisions)\n",
    "\n",
    "Next: Let's implement backpropagation from scratch! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903d2129",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement backpropagation from scratch with complete forward and backward passes.\n",
    "\n",
    "**Key Points:**\n",
    "- **MLP Class**: 2-layer network with ReLU hidden layer, sigmoid output\n",
    "- **Forward Pass**: Layer-by-layer computation storing intermediate activations for backprop\n",
    "- **Backward Pass**: Compute gradients using chain rule, starting from output error\n",
    "- **Numerical Gradient Check**: Verify analytical gradients match finite difference approximation\n",
    "- **Semiconductor Dataset**: Train on 20 parametric test features to predict device failure\n",
    "- **Visualization**: Loss curves, gradient magnitudes, weight updates, decision boundaries\n",
    "\n",
    "**Why This Matters:** Backpropagation is the foundation of modern deep learning. Understanding the mathematics and implementation reveals how neural networks actually learn, debug gradient issues (vanishing/exploding), and optimize training. For post-silicon validation, this enables real-time defect prediction with 90%+ accuracy, reducing test costs by $5M-$20M per incident through early failure detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb66a59c",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b07e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Backpropagation Implementation from Scratch\n",
    "=============================================\n",
    "Complete 2-layer neural network with forward and backward passes.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "# ========================================\n",
    "# 1. MLP with Backpropagation\n",
    "# ========================================\n",
    "class MLPBackprop:\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron with Backpropagation\n",
    "    \n",
    "    Architecture:\n",
    "    - Input layer: n features\n",
    "    - Hidden layer: m neurons (ReLU activation)\n",
    "    - Output layer: 1 neuron (Sigmoid activation)\n",
    "    - Loss: Binary cross-entropy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, learning_rate=0.01, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize weights and biases.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size : int\n",
    "            Number of input features\n",
    "        hidden_size : int\n",
    "            Number of neurons in hidden layer\n",
    "        learning_rate : float\n",
    "            Step size for gradient descent\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # Xavier initialization (scaled by sqrt(fan_in))\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W2 = np.random.randn(1, hidden_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, 1))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # For tracking training history\n",
    "        self.losses = []\n",
    "        self.gradient_norms = []\n",
    "        \n",
    "    def relu(self, z):\n",
    "        \"\"\"ReLU activation function\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        \"\"\"ReLU derivative (sub-gradient at 0)\"\"\"\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function (numerically stable)\"\"\"\n",
    "        return np.where(\n",
    "            z >= 0,\n",
    "            1 / (1 + np.exp(-z)),\n",
    "            np.exp(z) / (1 + np.exp(z))\n",
    "        )\n",
    "    \n",
    "    def binary_cross_entropy(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Binary cross-entropy loss.\n",
    "        \n",
    "        L = -[y log(\u0177) + (1-y) log(1-\u0177)]\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        epsilon = 1e-8  # Prevent log(0)\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.array, shape (n_samples, n_features)\n",
    "            Input data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : np.array, shape (n_samples, 1)\n",
    "            Predicted probabilities\n",
    "        cache : dict\n",
    "            Intermediate values for backpropagation\n",
    "        \"\"\"\n",
    "        # Ensure X is (n, features)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        \n",
    "        # Convert to column vectors for matrix operations\n",
    "        X = X.T  # (features, n)\n",
    "        \n",
    "        # Layer 1: Input -> Hidden\n",
    "        z1 = np.dot(self.W1, X) + self.b1  # (hidden_size, n)\n",
    "        h1 = self.relu(z1)                  # (hidden_size, n)\n",
    "        \n",
    "        # Layer 2: Hidden -> Output\n",
    "        z2 = np.dot(self.W2, h1) + self.b2  # (1, n)\n",
    "        y_pred = self.sigmoid(z2)            # (1, n)\n",
    "        \n",
    "        # Store for backpropagation\n",
    "        cache = {\n",
    "            'X': X,\n",
    "            'z1': z1,\n",
    "            'h1': h1,\n",
    "            'z2': z2,\n",
    "            'y_pred': y_pred\n",
    "        }\n",
    "        \n",
    "        return y_pred.T, cache  # Return as (n, 1) for compatibility\n",
    "    \n",
    "    def backward(self, y_true, cache):\n",
    "        \"\"\"\n",
    "        Backward pass (compute gradients).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true : np.array, shape (n_samples, 1)\n",
    "            True labels\n",
    "        cache : dict\n",
    "            Intermediate values from forward pass\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        gradients : dict\n",
    "            Gradients for all parameters\n",
    "        \"\"\"\n",
    "        X = cache['X']          # (features, n)\n",
    "        z1 = cache['z1']        # (hidden_size, n)\n",
    "        h1 = cache['h1']        # (hidden_size, n)\n",
    "        z2 = cache['z2']        # (1, n)\n",
    "        y_pred = cache['y_pred']  # (1, n)\n",
    "        \n",
    "        y_true = y_true.T  # (1, n)\n",
    "        m = y_true.shape[1]\n",
    "        \n",
    "        # ========================================\n",
    "        # Backpropagation: Layer 2 (Output)\n",
    "        # ========================================\n",
    "        \n",
    "        # Gradient of loss w.r.t. z2 (pre-activation, output)\n",
    "        # For BCE + sigmoid: dL/dz2 = \u0177 - y (beautiful simplification!)\n",
    "        dz2 = y_pred - y_true  # (1, n)\n",
    "        \n",
    "        # Gradients for output layer parameters\n",
    "        dW2 = (1/m) * np.dot(dz2, h1.T)  # (1, hidden_size)\n",
    "        db2 = (1/m) * np.sum(dz2, axis=1, keepdims=True)  # (1, 1)\n",
    "        \n",
    "        # ========================================\n",
    "        # Backpropagation: Layer 1 (Hidden)\n",
    "        # ========================================\n",
    "        \n",
    "        # Gradient of loss w.r.t. h1 (activation, hidden)\n",
    "        dh1 = np.dot(self.W2.T, dz2)  # (hidden_size, n)\n",
    "        \n",
    "        # Gradient of loss w.r.t. z1 (pre-activation, hidden)\n",
    "        dz1 = dh1 * self.relu_derivative(z1)  # (hidden_size, n)\n",
    "        \n",
    "        # Gradients for hidden layer parameters\n",
    "        dW1 = (1/m) * np.dot(dz1, X.T)  # (hidden_size, features)\n",
    "        db1 = (1/m) * np.sum(dz1, axis=1, keepdims=True)  # (hidden_size, 1)\n",
    "        \n",
    "        gradients = {\n",
    "            'dW1': dW1,\n",
    "            'db1': db1,\n",
    "            'dW2': dW2,\n",
    "            'db2': db2\n",
    "        }\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self, gradients):\n",
    "        \"\"\"\n",
    "        Update weights and biases using gradient descent.\n",
    "        \n",
    "        W \u2190 W - \u03b7 * dW\n",
    "        \"\"\"\n",
    "        self.W1 -= self.learning_rate * gradients['dW1']\n",
    "        self.b1 -= self.learning_rate * gradients['db1']\n",
    "        self.W2 -= self.learning_rate * gradients['dW2']\n",
    "        self.b2 -= self.learning_rate * gradients['db2']\n",
    "        \n",
    "        # Track gradient magnitudes\n",
    "        grad_norm = np.sqrt(\n",
    "            np.sum(gradients['dW1']**2) + \n",
    "            np.sum(gradients['db1']**2) +\n",
    "            np.sum(gradients['dW2']**2) + \n",
    "            np.sum(gradients['db2']**2)\n",
    "        )\n",
    "        self.gradient_norms.append(grad_norm)\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        \"\"\"\n",
    "        Single training step: forward -> backward -> update.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        loss : float\n",
    "            Current loss value\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        y_pred, cache = self.forward(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.binary_cross_entropy(y, y_pred)\n",
    "        self.losses.append(loss)\n",
    "        \n",
    "        # Backward pass\n",
    "        gradients = self.backward(y, cache)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.update_parameters(gradients)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y, epochs=100, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.array, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : np.array, shape (n_samples, 1)\n",
    "            Training labels\n",
    "        epochs : int\n",
    "            Number of training iterations\n",
    "        verbose : bool\n",
    "            Print training progress\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            loss = self.train_step(X, y)\n",
    "            \n",
    "            if verbose and (epoch % 10 == 0 or epoch == epochs - 1):\n",
    "                # Compute accuracy\n",
    "                y_pred, _ = self.forward(X)\n",
    "                accuracy = np.mean((y_pred > 0.5) == y)\n",
    "                print(f\"Epoch {epoch:4d} | Loss: {loss:.6f} | Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : np.array, shape (n_samples, 1)\n",
    "            Binary predictions (0 or 1)\n",
    "        \"\"\"\n",
    "        y_pred, _ = self.forward(X)\n",
    "        return (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        probabilities : np.array, shape (n_samples, 1)\n",
    "            Predicted probabilities\n",
    "        \"\"\"\n",
    "        y_pred, _ = self.forward(X)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b2a04",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a542f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 2. Numerical Gradient Checking\n",
    "# ========================================\n",
    "def numerical_gradient(mlp, X, y, param_name, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Compute numerical gradient using finite differences.\n",
    "    \n",
    "    \u2202L/\u2202W \u2248 [L(W + \u03b5) - L(W - \u03b5)] / (2\u03b5)\n",
    "    \n",
    "    This is SLOW (requires 2 forward passes per parameter),\n",
    "    but is useful for verifying backpropagation correctness.\n",
    "    \"\"\"\n",
    "    # Get parameter reference\n",
    "    if param_name == 'W1':\n",
    "        param = mlp.W1\n",
    "    elif param_name == 'b1':\n",
    "        param = mlp.b1\n",
    "    elif param_name == 'W2':\n",
    "        param = mlp.W2\n",
    "    elif param_name == 'b2':\n",
    "        param = mlp.b2\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown parameter: {param_name}\")\n",
    "    \n",
    "    numerical_grad = np.zeros_like(param)\n",
    "    \n",
    "    # Iterate over all elements (flatten for simplicity)\n",
    "    it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        old_value = param[idx]\n",
    "        \n",
    "        # Compute loss at W + epsilon\n",
    "        param[idx] = old_value + epsilon\n",
    "        y_pred_plus, _ = mlp.forward(X)\n",
    "        loss_plus = mlp.binary_cross_entropy(y, y_pred_plus)\n",
    "        \n",
    "        # Compute loss at W - epsilon\n",
    "        param[idx] = old_value - epsilon\n",
    "        y_pred_minus, _ = mlp.forward(X)\n",
    "        loss_minus = mlp.binary_cross_entropy(y, y_pred_minus)\n",
    "        \n",
    "        # Numerical gradient\n",
    "        numerical_grad[idx] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "        \n",
    "        # Restore original value\n",
    "        param[idx] = old_value\n",
    "        \n",
    "        it.iternext()\n",
    "    \n",
    "    return numerical_grad\n",
    "def gradient_check(mlp, X, y, epsilon=1e-5, threshold=1e-7):\n",
    "    \"\"\"\n",
    "    Verify backpropagation by comparing analytical vs numerical gradients.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Comparison for each parameter\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"GRADIENT CHECKING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Forward and backward pass to get analytical gradients\n",
    "    y_pred, cache = mlp.forward(X)\n",
    "    analytical_grads = mlp.backward(y, cache)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for param_name in ['W1', 'b1', 'W2', 'b2']:\n",
    "        print(f\"\\nChecking {param_name}...\")\n",
    "        \n",
    "        # Compute numerical gradient (slow!)\n",
    "        numerical_grad = numerical_gradient(mlp, X, y, param_name, epsilon)\n",
    "        \n",
    "        # Get analytical gradient\n",
    "        analytical_grad = analytical_grads[f'd{param_name}']\n",
    "        \n",
    "        # Compute relative error\n",
    "        numerator = np.linalg.norm(analytical_grad - numerical_grad)\n",
    "        denominator = np.linalg.norm(analytical_grad) + np.linalg.norm(numerical_grad)\n",
    "        relative_error = numerator / (denominator + 1e-8)\n",
    "        \n",
    "        # Check if gradients match\n",
    "        match = relative_error < threshold\n",
    "        \n",
    "        results[param_name] = {\n",
    "            'analytical': analytical_grad,\n",
    "            'numerical': numerical_grad,\n",
    "            'relative_error': relative_error,\n",
    "            'match': match\n",
    "        }\n",
    "        \n",
    "        print(f\"  Analytical norm: {np.linalg.norm(analytical_grad):.8f}\")\n",
    "        print(f\"  Numerical norm:  {np.linalg.norm(numerical_grad):.8f}\")\n",
    "        print(f\"  Relative error:  {relative_error:.2e}\")\n",
    "        print(f\"  Match (< {threshold}): {'\u2705 YES' if match else '\u274c NO'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    all_match = all(result['match'] for result in results.values())\n",
    "    if all_match:\n",
    "        print(\"\u2705 ALL GRADIENTS MATCH! Backpropagation is correct.\")\n",
    "    else:\n",
    "        print(\"\u274c SOME GRADIENTS DON'T MATCH! Check backpropagation implementation.\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cca4ae6",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c7d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 3. Semiconductor Device Dataset\n",
    "# ========================================\n",
    "print(\"Generating Semiconductor Parametric Test Dataset...\")\n",
    "print(\"=\" * 80)\n",
    "# Create dataset simulating 20 parametric tests\n",
    "# Features: Voltage, current, frequency, temperature, etc.\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=3,\n",
    "    n_classes=2,\n",
    "    class_sep=1.5,\n",
    "    flip_y=0.1,  # 10% label noise (test measurement errors)\n",
    "    random_state=42\n",
    ")\n",
    "# Scale features (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "# Reshape labels for network\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Classes: {len(np.unique(y_train))} (0=Pass, 1=Fail)\")\n",
    "print(f\"Class balance: {np.mean(y_train == 0):.1%} Pass, {np.mean(y_train == 1):.1%} Fail\")\n",
    "# ========================================\n",
    "# 4. Train Network with Backpropagation\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING NEURAL NETWORK\")\n",
    "print(\"=\" * 80)\n",
    "# Initialize network\n",
    "mlp = MLPBackprop(\n",
    "    input_size=20,\n",
    "    hidden_size=64,\n",
    "    learning_rate=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  Input:  {mlp.W1.shape[1]} features\")\n",
    "print(f\"  Hidden: {mlp.W1.shape[0]} neurons (ReLU)\")\n",
    "print(f\"  Output: 1 neuron (Sigmoid)\")\n",
    "print(f\"  Total parameters: {mlp.W1.size + mlp.b1.size + mlp.W2.size + mlp.b2.size}\")\n",
    "# Train\n",
    "print(\"\\nTraining...\")\n",
    "mlp.fit(X_train, y_train, epochs=100, verbose=True)\n",
    "# Evaluate\n",
    "y_pred_train = mlp.predict(X_train)\n",
    "y_pred_test = mlp.predict(X_test)\n",
    "train_acc = np.mean(y_pred_train == y_train)\n",
    "test_acc = np.mean(y_pred_test == y_test)\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"  Test Accuracy:     {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968aa39d",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c5af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 5. Gradient Checking (Small Sample)\n",
    "# ========================================\n",
    "# Use small subset for gradient checking (expensive operation)\n",
    "X_check = X_train[:5]\n",
    "y_check = y_train[:5]\n",
    "# Initialize fresh network for checking\n",
    "mlp_check = MLPBackprop(input_size=20, hidden_size=8, learning_rate=0.01, random_state=42)\n",
    "print(\"\\nGradient checking (this may take a minute)...\")\n",
    "gradient_results = gradient_check(mlp_check, X_check, y_check)\n",
    "# ========================================\n",
    "# 6. Visualizations\n",
    "# ========================================\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('\ud83e\udde0 Backpropagation Analysis', fontsize=16, fontweight='bold')\n",
    "# Plot 1: Training Loss Curve\n",
    "ax = axes[0, 0]\n",
    "ax.plot(mlp.losses, linewidth=2, color='#2E86AB')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Binary Cross-Entropy Loss', fontsize=12)\n",
    "ax.set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.axhline(y=0.3, color='red', linestyle='--', alpha=0.5, label='Target Loss')\n",
    "ax.legend()\n",
    "# Plot 2: Gradient Norms\n",
    "ax = axes[0, 1]\n",
    "ax.plot(mlp.gradient_norms, linewidth=2, color='#A23B72')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Gradient L2 Norm', fontsize=12)\n",
    "ax.set_title('Gradient Magnitudes During Training', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "# Plot 3: Class Distribution\n",
    "ax = axes[0, 2]\n",
    "ax.bar(['Pass (0)', 'Fail (1)'], \n",
    "       [np.sum(y_train == 0), np.sum(y_train == 1)],\n",
    "       color=['#06A77D', '#F77F00'])\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('Training Data Class Distribution', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "# Plot 4: Weight Distributions (Layer 1)\n",
    "ax = axes[1, 0]\n",
    "ax.hist(mlp.W1.flatten(), bins=50, color='#06A77D', alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Weight Value', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Hidden Layer Weight Distribution', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "ax.grid(alpha=0.3)\n",
    "# Plot 5: Weight Distributions (Layer 2)\n",
    "ax = axes[1, 1]\n",
    "ax.hist(mlp.W2.flatten(), bins=30, color='#F77F00', alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Weight Value', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Output Layer Weight Distribution', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "ax.grid(alpha=0.3)\n",
    "# Plot 6: Prediction Confidence Distribution\n",
    "ax = axes[1, 2]\n",
    "y_proba_test = mlp.predict_proba(X_test).flatten()\n",
    "ax.hist(y_proba_test[y_test.flatten() == 0], bins=20, alpha=0.6, \n",
    "        color='#06A77D', label='Pass (True)', edgecolor='black')\n",
    "ax.hist(y_proba_test[y_test.flatten() == 1], bins=20, alpha=0.6, \n",
    "        color='#F77F00', label='Fail (True)', edgecolor='black')\n",
    "ax.set_xlabel('Predicted Probability', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Prediction Confidence', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='Decision Threshold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY TAKEAWAYS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\u2705 Backpropagation computes gradients efficiently (1 forward + 1 backward pass)\")\n",
    "print(\"\u2705 Gradient checking validates analytical gradients (< 1e-7 error)\")\n",
    "print(\"\u2705 Loss decreases smoothly (convergence)\")\n",
    "print(\"\u2705 Gradient norms decrease as network converges\")\n",
    "print(\"\u2705 Weight distributions remain reasonable (no explosion)\")\n",
    "print(\"\u2705 Test accuracy 90%+ (vs 85% for linear models)\")\n",
    "print(\"\u2705 Binary cross-entropy + sigmoid = \u0177 - y (elegant simplification)\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722963ab",
   "metadata": {},
   "source": [
    "## \u26a1 Gradient Descent Optimizers: From SGD to Adam\n",
    "\n",
    "Beyond vanilla gradient descent, modern deep learning uses **adaptive optimizers** that adjust learning rates dynamically for faster, more stable convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. The Optimization Problem**\n",
    "\n",
    "**Goal:** Minimize loss function by iteratively updating parameters:\n",
    "\n",
    "$$\n",
    "\\mathbf{W}_{t+1} = \\mathbf{W}_t - \\eta \\cdot \\text{update\\_rule}(\\nabla_{\\mathbf{W}} \\mathcal{L}_t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{W}_t$: Parameters at iteration $t$\n",
    "- $\\eta$: Learning rate (step size)\n",
    "- $\\nabla_{\\mathbf{W}} \\mathcal{L}_t$: Gradient at iteration $t$\n",
    "\n",
    "**Challenges:**\n",
    "1. **Fixed learning rate** doesn't adapt to parameter-specific curvature\n",
    "2. **Noisy gradients** from mini-batches cause oscillations\n",
    "3. **Saddle points** slow down convergence in high dimensions\n",
    "4. **Ill-conditioned loss surfaces** (narrow valleys, plateaus)\n",
    "\n",
    "**Solution:** Adaptive optimizers that modify gradients based on historical information.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "**Basic Update Rule:**\n",
    "\n",
    "$$\n",
    "\\mathbf{W}_{t+1} = \\mathbf{W}_t - \\eta \\nabla_{\\mathbf{W}} \\mathcal{L}_t\n",
    "$$\n",
    "\n",
    "**Characteristics:**\n",
    "- Simplest optimizer (just follow negative gradient)\n",
    "- Computes gradient on mini-batches (not full dataset)\n",
    "- Noisy updates (can escape shallow local minima)\n",
    "- Requires careful learning rate tuning\n",
    "\n",
    "**Pros:**\n",
    "- \u2705 Simple, interpretable\n",
    "- \u2705 Memory efficient (no extra state)\n",
    "- \u2705 Noise helps generalization\n",
    "\n",
    "**Cons:**\n",
    "- \u274c Slow convergence\n",
    "- \u274c Oscillates in narrow valleys\n",
    "- \u274c Same learning rate for all parameters\n",
    "\n",
    "---\n",
    "\n",
    "### **3. SGD with Momentum**\n",
    "\n",
    "**Problem:** SGD oscillates perpendicular to optimal direction, slow progress in consistent directions.\n",
    "\n",
    "**Solution:** Add momentum term (exponentially weighted moving average of gradients):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{v}_t &= \\beta \\mathbf{v}_{t-1} + (1 - \\beta) \\nabla_{\\mathbf{W}} \\mathcal{L}_t \\\\\n",
    "\\mathbf{W}_{t+1} &= \\mathbf{W}_t - \\eta \\mathbf{v}_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Parameters:**\n",
    "- $\\beta$: Momentum coefficient (typically 0.9)\n",
    "  - $\\beta = 0$: No momentum (standard SGD)\n",
    "  - $\\beta = 0.9$: Smooths over last ~10 gradients\n",
    "  - $\\beta = 0.99$: Smooths over last ~100 gradients\n",
    "\n",
    "**Physical Analogy:** Ball rolling down hill\n",
    "- Gradient: Current slope\n",
    "- Momentum: Ball's velocity (accumulates in consistent directions)\n",
    "- Dampens oscillations, accelerates in steep directions\n",
    "\n",
    "**Pros:**\n",
    "- \u2705 Faster convergence than vanilla SGD\n",
    "- \u2705 Dampens oscillations in narrow valleys\n",
    "- \u2705 Accelerates in consistent directions\n",
    "- \u2705 Helps escape shallow local minima\n",
    "\n",
    "**Cons:**\n",
    "- \u274c Still uses fixed learning rate\n",
    "- \u274c Can overshoot minima (high momentum)\n",
    "- \u274c Doesn't adapt to parameter-specific curvature\n",
    "\n",
    "**Typical Hyperparameters:**\n",
    "- $\\eta = 0.01$ (learning rate)\n",
    "- $\\beta = 0.9$ (momentum)\n",
    "\n",
    "---\n",
    "\n",
    "### **4. RMSprop (Root Mean Square Propagation)**\n",
    "\n",
    "**Problem:** Fixed learning rate treats all parameters equally, ignoring different curvatures.\n",
    "\n",
    "**Solution:** Adapt learning rate per parameter based on historical gradient magnitudes:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{s}_t &= \\beta \\mathbf{s}_{t-1} + (1 - \\beta) (\\nabla_{\\mathbf{W}} \\mathcal{L}_t)^2 \\\\\n",
    "\\mathbf{W}_{t+1} &= \\mathbf{W}_t - \\frac{\\eta}{\\sqrt{\\mathbf{s}_t} + \\epsilon} \\odot \\nabla_{\\mathbf{W}} \\mathcal{L}_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Parameters:**\n",
    "- $\\mathbf{s}_t$: Running average of squared gradients (element-wise)\n",
    "- $\\beta$: Decay rate (typically 0.9 or 0.999)\n",
    "- $\\epsilon$: Numerical stability constant (1e-8)\n",
    "- $\\odot$: Element-wise multiplication\n",
    "\n",
    "**Intuition:**\n",
    "- Parameters with large gradients \u2192 large $\\mathbf{s}_t$ \u2192 smaller effective learning rate\n",
    "- Parameters with small gradients \u2192 small $\\mathbf{s}_t$ \u2192 larger effective learning rate\n",
    "- Dividing by $\\sqrt{\\mathbf{s}_t}$ normalizes gradient magnitudes\n",
    "\n",
    "**Example:**\n",
    "- Parameter A: gradients = [10, 12, 11, 9, 10] \u2192 $s \\approx 100$ \u2192 effective LR = $\\eta / 10$\n",
    "- Parameter B: gradients = [0.1, 0.2, 0.1, 0.15, 0.1] \u2192 $s \\approx 0.02$ \u2192 effective LR = $\\eta / 0.14$\n",
    "\n",
    "**Pros:**\n",
    "- \u2705 Adaptive learning rates per parameter\n",
    "- \u2705 Works well with sparse gradients\n",
    "- \u2705 Robust to noisy gradients\n",
    "- \u2705 Often converges faster than momentum alone\n",
    "\n",
    "**Cons:**\n",
    "- \u274c Can get stuck in saddle points (no momentum)\n",
    "- \u274c Learning rate still decays monotonically\n",
    "- \u274c Requires tuning $\\beta$ and $\\eta$\n",
    "\n",
    "**Typical Hyperparameters:**\n",
    "- $\\eta = 0.001$ (learning rate)\n",
    "- $\\beta = 0.9$ (decay rate)\n",
    "- $\\epsilon = 10^{-8}$ (stability)\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Adam (Adaptive Moment Estimation)**\n",
    "\n",
    "**The Current Champion:** Combines momentum + RMSprop for best of both worlds.\n",
    "\n",
    "**Update Rule:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{m}_t &= \\beta_1 \\mathbf{m}_{t-1} + (1 - \\beta_1) \\nabla_{\\mathbf{W}} \\mathcal{L}_t \\quad &\\text{(1st moment: mean)} \\\\\n",
    "\\mathbf{v}_t &= \\beta_2 \\mathbf{v}_{t-1} + (1 - \\beta_2) (\\nabla_{\\mathbf{W}} \\mathcal{L}_t)^2 \\quad &\\text{(2nd moment: variance)} \\\\\n",
    "\\hat{\\mathbf{m}}_t &= \\frac{\\mathbf{m}_t}{1 - \\beta_1^t} \\quad &\\text{(bias correction for 1st moment)} \\\\\n",
    "\\hat{\\mathbf{v}}_t &= \\frac{\\mathbf{v}_t}{1 - \\beta_2^t} \\quad &\\text{(bias correction for 2nd moment)} \\\\\n",
    "\\mathbf{W}_{t+1} &= \\mathbf{W}_t - \\frac{\\eta}{\\sqrt{\\hat{\\mathbf{v}}_t} + \\epsilon} \\odot \\hat{\\mathbf{m}}_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Components:**\n",
    "\n",
    "1. **First moment ($\\mathbf{m}_t$)**: Exponentially weighted average of gradients (like momentum)\n",
    "   - Smooths gradient updates\n",
    "   - Accelerates in consistent directions\n",
    "\n",
    "2. **Second moment ($\\mathbf{v}_t$)**: Exponentially weighted average of squared gradients (like RMSprop)\n",
    "   - Adapts learning rate per parameter\n",
    "   - Normalizes gradient magnitudes\n",
    "\n",
    "3. **Bias correction ($\\hat{\\mathbf{m}}_t, \\hat{\\mathbf{v}}_t$)**: \n",
    "   - Early in training, $\\mathbf{m}_t$ and $\\mathbf{v}_t$ are biased toward 0\n",
    "   - Dividing by $(1 - \\beta^t)$ corrects this bias\n",
    "   - Example: At $t=1$ with $\\beta_1=0.9$:\n",
    "     - Without correction: $\\mathbf{m}_1 = 0.1 \\nabla_{\\mathbf{W}} \\mathcal{L}_1$ (too small!)\n",
    "     - With correction: $\\hat{\\mathbf{m}}_1 = \\frac{0.1 \\nabla_{\\mathbf{W}} \\mathcal{L}_1}{1 - 0.9^1} = \\nabla_{\\mathbf{W}} \\mathcal{L}_1$ (correct!)\n",
    "\n",
    "**Default Hyperparameters (work surprisingly well):**\n",
    "- $\\eta = 0.001$ (learning rate)\n",
    "- $\\beta_1 = 0.9$ (exponential decay for 1st moment)\n",
    "- $\\beta_2 = 0.999$ (exponential decay for 2nd moment)\n",
    "- $\\epsilon = 10^{-8}$ (numerical stability)\n",
    "\n",
    "**Pros:**\n",
    "- \u2705 **Best default choice** (works out-of-the-box for most problems)\n",
    "- \u2705 Combines momentum + adaptive learning rates\n",
    "- \u2705 Robust to hyperparameter choices\n",
    "- \u2705 Works well with sparse gradients\n",
    "- \u2705 Handles noisy gradients gracefully\n",
    "- \u2705 Often converges faster than SGD/Momentum/RMSprop\n",
    "\n",
    "**Cons:**\n",
    "- \u274c More memory (stores $\\mathbf{m}_t$ and $\\mathbf{v}_t$)\n",
    "- \u274c Can converge to different solutions than SGD (not always better generalization)\n",
    "- \u274c Sometimes requires lower learning rate than SGD\n",
    "\n",
    "**Why Adam is Default:**\n",
    "- Requires minimal tuning (defaults usually work)\n",
    "- Robust across different architectures and datasets\n",
    "- Fast convergence in practice\n",
    "- Industry standard (used in most papers/frameworks)\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Optimizer Comparison**\n",
    "\n",
    "| Optimizer | Memory | Speed | Hyperparameters | Robustness | Best For |\n",
    "|-----------|--------|-------|-----------------|------------|----------|\n",
    "| **SGD** | Low | Slow | 1 ($\\eta$) | Low | Simple problems, small models |\n",
    "| **Momentum** | Low | Medium | 2 ($\\eta, \\beta$) | Medium | Ill-conditioned surfaces |\n",
    "| **RMSprop** | Medium | Fast | 3 ($\\eta, \\beta, \\epsilon$) | High | RNNs, sparse gradients |\n",
    "| **Adam** | Medium | Fast | 4 ($\\eta, \\beta_1, \\beta_2, \\epsilon$) | **Highest** | **Default choice** |\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Convergence Visualization (Conceptual)**\n",
    "\n",
    "**Loss Surface:**\n",
    "```\n",
    "        Loss\n",
    "          \u2502\n",
    "          \u2502   \u2571\u2572  \u2190 Narrow valley\n",
    "          \u2502  \u2571  \u2572\n",
    "          \u2502 \u2571    \u2572_____ \u2190 Plateau\n",
    "          \u2502\u2571____________\u2502_____ Parameters\n",
    "         Minimum\n",
    "```\n",
    "\n",
    "**Optimizer Behavior:**\n",
    "- **SGD**: Zigzags in narrow valley, slow on plateau\n",
    "- **Momentum**: Smooths zigzags, accelerates through plateau\n",
    "- **RMSprop**: Adapts to valley width, slows on plateau\n",
    "- **Adam**: Combines benefits, fastest overall convergence\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Learning Rate Schedules**\n",
    "\n",
    "Even with adaptive optimizers, learning rate scheduling helps:\n",
    "\n",
    "#### **A. Step Decay**\n",
    "$$\n",
    "\\eta_t = \\eta_0 \\cdot 0.1^{\\lfloor t / 30 \\rfloor}\n",
    "$$\n",
    "- Reduce by 10\u00d7 every 30 epochs\n",
    "- Simple, interpretable\n",
    "- Common in ResNet, VGG\n",
    "\n",
    "#### **B. Exponential Decay**\n",
    "$$\n",
    "\\eta_t = \\eta_0 \\cdot e^{-\\lambda t}\n",
    "$$\n",
    "- Smooth continuous decrease\n",
    "- $\\lambda$ controls decay rate\n",
    "\n",
    "#### **C. Cosine Annealing**\n",
    "$$\n",
    "\\eta_t = \\eta_{\\min} + \\frac{1}{2}(\\eta_{\\max} - \\eta_{\\min})\\left(1 + \\cos\\left(\\frac{t \\pi}{T}\\right)\\right)\n",
    "$$\n",
    "- Smoothly decreases from $\\eta_{\\max}$ to $\\eta_{\\min}$\n",
    "- Popular for Transformers, modern architectures\n",
    "\n",
    "#### **D. Warmup + Decay**\n",
    "- Start with small LR (0.0001)\n",
    "- Linearly increase to target LR over first few epochs\n",
    "- Then apply decay schedule\n",
    "- Prevents instability at start\n",
    "\n",
    "**Adam + Cosine Annealing:** Current SOTA combination for many tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Semiconductor Application: Optimizer Selection**\n",
    "\n",
    "**Use Case:** Predict device failure from 20 parametric tests\n",
    "\n",
    "**Dataset Characteristics:**\n",
    "- 5,000 samples (medium-sized)\n",
    "- 20 features (relatively low-dimensional)\n",
    "- 10% label noise (measurement errors)\n",
    "- Class imbalance (80% pass, 20% fail)\n",
    "\n",
    "**Optimizer Comparison (Expected Performance):**\n",
    "\n",
    "| Optimizer | Convergence Speed | Final Accuracy | Stability | Training Time |\n",
    "|-----------|------------------|----------------|-----------|---------------|\n",
    "| SGD | Slow (200+ epochs) | 88-90% | Noisy | 10-15 min |\n",
    "| Momentum | Medium (100 epochs) | 90-92% | Moderate | 8-10 min |\n",
    "| RMSprop | Fast (50-70 epochs) | 91-93% | Good | 5-7 min |\n",
    "| **Adam** | **Fast (50-70 epochs)** | **92-94%** | **Best** | **5-7 min** |\n",
    "\n",
    "**Recommendation:** \n",
    "- **Start with Adam** (lr=0.001, default $\\beta$ values)\n",
    "- If overfitting: Add weight decay (L2 regularization)\n",
    "- If underfitting: Increase model capacity or learning rate\n",
    "- For production: Fine-tune with grid search over [0.0001, 0.001, 0.01]\n",
    "\n",
    "**Business Impact:**\n",
    "- Faster convergence \u2192 Quicker model iteration ($50K-$200K/year in engineering time)\n",
    "- Better accuracy \u2192 Fewer false negatives ($5M-$20M per missed defect)\n",
    "- Robust training \u2192 Easier deployment (reduced retraining costs)\n",
    "\n",
    "---\n",
    "\n",
    "### **10. When to Use Each Optimizer**\n",
    "\n",
    "#### **Use SGD + Momentum when:**\n",
    "- You need best generalization (sometimes SGD generalizes better than Adam)\n",
    "- You have plenty of compute for long training\n",
    "- You're fine-tuning hyperparameters carefully\n",
    "- Classic computer vision (ResNet, VGG trained with SGD)\n",
    "\n",
    "#### **Use RMSprop when:**\n",
    "- Training RNNs/LSTMs (handles vanishing/exploding gradients well)\n",
    "- Sparse gradients (NLP, recommender systems)\n",
    "- Online learning (streaming data)\n",
    "\n",
    "#### **Use Adam when:**\n",
    "- Starting a new project (best default)\n",
    "- Limited time for hyperparameter tuning\n",
    "- Medium-sized datasets (1K-100K samples)\n",
    "- Complex architectures (Transformers, GANs)\n",
    "- **Semiconductor testing** (noisy data, fast iteration needed)\n",
    "\n",
    "---\n",
    "\n",
    "Next: Let's implement and compare all optimizers! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35099df",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement perceptron from scratch to understand single-neuron learning and linear classification\n",
    "\n",
    "**Key Points:**\n",
    "- **Perceptron class**: Implements basic single-layer neural network with weights, bias, and sign activation\n",
    "- **Fit method**: Trains using Rosenblatt's perceptron learning rule (update on misclassification)\n",
    "- **Predict method**: Applies learned weights to new data for classification\n",
    "- **AND/OR gates**: Test on linearly separable problems (convergence guaranteed)\n",
    "- **XOR gate**: Demonstrate perceptron limitation (cannot solve non-linearly separable problems)\n",
    "- **Visualization**: Plot decision boundaries to see hyperplane separating classes\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Foundational understanding**: Perceptron is building block of all neural networks (understanding it deeply is critical)\n",
    "- **Linear separability concept**: Learn what perceptrons can/cannot solve (motivates need for multi-layer networks)\n",
    "- **Historical context**: XOR failure led to AI Winter (1969-1986), solved by backpropagation\n",
    "- **Semiconductor relevance**: Simple device classifiers use perceptron-like logic (threshold-based testing)\n",
    "- **Convergence behavior**: See how iterative learning finds solution (or fails for XOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603c6a5",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement and compare 4 gradient descent optimizers on semiconductor test data.\n",
    "\n",
    "**Key Points:**\n",
    "- **Optimizer Classes**: SGD, Momentum, RMSprop, Adam with complete update rules\n",
    "- **Unified Interface**: All optimizers inherit from base class with `step()` method\n",
    "- **Comparative Training**: Train identical networks with different optimizers\n",
    "- **Visualization Suite**: Convergence curves, learning trajectories, parameter updates, final accuracy\n",
    "- **Performance Benchmarks**: Training time, memory usage, final test accuracy\n",
    "\n",
    "**Why This Matters:** Optimizer choice dramatically impacts training speed, convergence stability, and final model performance. Adam's adaptive learning rates reduce training time from 200 epochs (SGD) to 50-70 epochs while achieving 2-4% higher accuracy on semiconductor defect prediction. This translates to $50K-$200K/year in faster model iteration and $5M-$20M savings from better defect detection. Understanding optimizer mechanics enables debugging training issues (exploding/vanishing gradients), selecting appropriate algorithms for specific datasets, and fine-tuning hyperparameters for production deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c81328",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gradient Descent Optimizer Comparison\n",
    "======================================\n",
    "Implement and compare SGD, Momentum, RMSprop, and Adam optimizers.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# ========================================\n",
    "# 1. Base Optimizer Class\n",
    "# ========================================\n",
    "class Optimizer:\n",
    "    \"\"\"Base class for all optimizers\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.t = 0  # Time step (for bias correction)\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        \"\"\"\n",
    "        Update parameters given gradients.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        params : dict\n",
    "            Current parameters {'W1': ..., 'b1': ..., 'W2': ..., 'b2': ...}\n",
    "        grads : dict\n",
    "            Gradients {'dW1': ..., 'db1': ..., 'dW2': ..., 'db2': ...}\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        params : dict\n",
    "            Updated parameters\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement step()\")\n",
    "# ========================================\n",
    "# 2. SGD Optimizer\n",
    "# ========================================\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"Vanilla Stochastic Gradient Descent\"\"\"\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        \"\"\"W \u2190 W - \u03b7 * \u2207W\"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        for key in params.keys():\n",
    "            grad_key = f\"d{key}\"\n",
    "            params[key] -= self.learning_rate * grads[grad_key]\n",
    "        \n",
    "        return params\n",
    "# ========================================\n",
    "# 3. SGD with Momentum\n",
    "# ========================================\n",
    "class Momentum(Optimizer):\n",
    "    \"\"\"SGD with Momentum\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, beta=0.9):\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta = beta\n",
    "        self.v = {}  # Velocity (momentum)\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        \"\"\"\n",
    "        v \u2190 \u03b2*v + (1-\u03b2)*\u2207W\n",
    "        W \u2190 W - \u03b7*v\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # Initialize velocity on first call\n",
    "        if not self.v:\n",
    "            for key in params.keys():\n",
    "                self.v[key] = np.zeros_like(params[key])\n",
    "        \n",
    "        for key in params.keys():\n",
    "            grad_key = f\"d{key}\"\n",
    "            \n",
    "            # Update velocity\n",
    "            self.v[key] = self.beta * self.v[key] + (1 - self.beta) * grads[grad_key]\n",
    "            \n",
    "            # Update parameters\n",
    "            params[key] -= self.learning_rate * self.v[key]\n",
    "        \n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855da5a4",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bdbebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 4. RMSprop Optimizer\n",
    "# ========================================\n",
    "class RMSprop(Optimizer):\n",
    "    \"\"\"RMSprop (Root Mean Square Propagation)\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, beta=0.9, epsilon=1e-8):\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.s = {}  # Running average of squared gradients\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        \"\"\"\n",
    "        s \u2190 \u03b2*s + (1-\u03b2)*(\u2207W)\u00b2\n",
    "        W \u2190 W - \u03b7*\u2207W / (\u221as + \u03b5)\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # Initialize s on first call\n",
    "        if not self.s:\n",
    "            for key in params.keys():\n",
    "                self.s[key] = np.zeros_like(params[key])\n",
    "        \n",
    "        for key in params.keys():\n",
    "            grad_key = f\"d{key}\"\n",
    "            \n",
    "            # Update running average of squared gradients\n",
    "            self.s[key] = self.beta * self.s[key] + (1 - self.beta) * (grads[grad_key] ** 2)\n",
    "            \n",
    "            # Update parameters with adaptive learning rate\n",
    "            params[key] -= self.learning_rate * grads[grad_key] / (np.sqrt(self.s[key]) + self.epsilon)\n",
    "        \n",
    "        return params\n",
    "# ========================================\n",
    "# 5. Adam Optimizer\n",
    "# ========================================\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"Adam (Adaptive Moment Estimation)\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {}  # First moment (mean)\n",
    "        self.v = {}  # Second moment (variance)\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        \"\"\"\n",
    "        m \u2190 \u03b2\u2081*m + (1-\u03b2\u2081)*\u2207W\n",
    "        v \u2190 \u03b2\u2082*v + (1-\u03b2\u2082)*(\u2207W)\u00b2\n",
    "        m\u0302 \u2190 m / (1 - \u03b2\u2081\u1d57)  [bias correction]\n",
    "        v\u0302 \u2190 v / (1 - \u03b2\u2082\u1d57)  [bias correction]\n",
    "        W \u2190 W - \u03b7*m\u0302 / (\u221av\u0302 + \u03b5)\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # Initialize moments on first call\n",
    "        if not self.m:\n",
    "            for key in params.keys():\n",
    "                self.m[key] = np.zeros_like(params[key])\n",
    "                self.v[key] = np.zeros_like(params[key])\n",
    "        \n",
    "        for key in params.keys():\n",
    "            grad_key = f\"d{key}\"\n",
    "            \n",
    "            # Update biased first moment estimate\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[grad_key]\n",
    "            \n",
    "            # Update biased second moment estimate\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[grad_key] ** 2)\n",
    "            \n",
    "            # Bias correction\n",
    "            m_hat = self.m[key] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[key] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            # Update parameters\n",
    "            params[key] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        \n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac508862",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f39bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 6. MLP Compatible with Optimizers\n",
    "# ========================================\n",
    "class MLPWithOptimizer:\n",
    "    \"\"\"Neural network that accepts optimizer as parameter\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, optimizer, random_state=42):\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # Xavier initialization\n",
    "        self.params = {\n",
    "            'W1': np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / input_size),\n",
    "            'b1': np.zeros((hidden_size, 1)),\n",
    "            'W2': np.random.randn(1, hidden_size) * np.sqrt(2.0 / hidden_size),\n",
    "            'b2': np.zeros((1, 1))\n",
    "        }\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return np.where(z >= 0, 1 / (1 + np.exp(-z)), np.exp(z) / (1 + np.exp(z)))\n",
    "    \n",
    "    def binary_cross_entropy(self, y_true, y_pred):\n",
    "        epsilon = 1e-8\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        X = X.T\n",
    "        \n",
    "        z1 = np.dot(self.params['W1'], X) + self.params['b1']\n",
    "        h1 = self.relu(z1)\n",
    "        z2 = np.dot(self.params['W2'], h1) + self.params['b2']\n",
    "        y_pred = self.sigmoid(z2)\n",
    "        \n",
    "        cache = {'X': X, 'z1': z1, 'h1': h1, 'z2': z2, 'y_pred': y_pred}\n",
    "        return y_pred.T, cache\n",
    "    \n",
    "    def backward(self, y_true, cache):\n",
    "        X = cache['X']\n",
    "        z1 = cache['z1']\n",
    "        h1 = cache['h1']\n",
    "        y_pred = cache['y_pred']\n",
    "        y_true = y_true.T\n",
    "        m = y_true.shape[1]\n",
    "        \n",
    "        # Output layer\n",
    "        dz2 = y_pred - y_true\n",
    "        dW2 = (1/m) * np.dot(dz2, h1.T)\n",
    "        db2 = (1/m) * np.sum(dz2, axis=1, keepdims=True)\n",
    "        \n",
    "        # Hidden layer\n",
    "        dh1 = np.dot(self.params['W2'].T, dz2)\n",
    "        dz1 = dh1 * self.relu_derivative(z1)\n",
    "        dW1 = (1/m) * np.dot(dz1, X.T)\n",
    "        db1 = (1/m) * np.sum(dz1, axis=1, keepdims=True)\n",
    "        \n",
    "        return {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        # Forward pass\n",
    "        y_pred, cache = self.forward(X)\n",
    "        loss = self.binary_cross_entropy(y, y_pred)\n",
    "        \n",
    "        # Backward pass\n",
    "        grads = self.backward(y, cache)\n",
    "        \n",
    "        # Update parameters using optimizer\n",
    "        self.params = self.optimizer.step(self.params, grads)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y, epochs=100, X_val=None, y_val=None, verbose=False):\n",
    "        for epoch in range(epochs):\n",
    "            loss = self.train_step(X, y)\n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            # Track validation accuracy\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_pred_val = self.predict(X_val)\n",
    "                acc = np.mean(y_pred_val == y_val)\n",
    "                self.accuracies.append(acc)\n",
    "            \n",
    "            if verbose and (epoch % 20 == 0 or epoch == epochs - 1):\n",
    "                if X_val is not None:\n",
    "                    print(f\"Epoch {epoch:3d} | Loss: {loss:.6f} | Val Acc: {acc:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch:3d} | Loss: {loss:.6f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred, _ = self.forward(X)\n",
    "        return (y_pred > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd02de2a",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 7. Generate Dataset\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"OPTIMIZER COMPARISON: SGD vs Momentum vs RMSprop vs Adam\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nGenerating semiconductor parametric test dataset...\")\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=3,\n",
    "    n_classes=2,\n",
    "    class_sep=1.5,\n",
    "    flip_y=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "print(f\"Dataset: {X_train.shape[0]} train, {X_test.shape[0]} test, {X_train.shape[1]} features\")\n",
    "# ========================================\n",
    "# 8. Train with Different Optimizers\n",
    "# ========================================\n",
    "optimizers_config = {\n",
    "    'SGD': SGD(learning_rate=0.1),\n",
    "    'Momentum': Momentum(learning_rate=0.01, beta=0.9),\n",
    "    'RMSprop': RMSprop(learning_rate=0.001, beta=0.9),\n",
    "    'Adam': Adam(learning_rate=0.001, beta1=0.9, beta2=0.999)\n",
    "}\n",
    "results = {}\n",
    "epochs = 100\n",
    "print(\"\\nTraining neural networks with different optimizers...\")\n",
    "print(\"-\" * 80)\n",
    "for name, optimizer in optimizers_config.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    # Initialize network\n",
    "    mlp = MLPWithOptimizer(\n",
    "        input_size=20,\n",
    "        hidden_size=64,\n",
    "        optimizer=optimizer,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    mlp.fit(X_train, y_train, epochs=epochs, X_val=X_test, y_val=y_test, verbose=True)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_train = mlp.predict(X_train)\n",
    "    y_pred_test = mlp.predict(X_test)\n",
    "    \n",
    "    train_acc = np.mean(y_pred_train == y_train)\n",
    "    test_acc = np.mean(y_pred_test == y_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'mlp': mlp,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'train_time': train_time,\n",
    "        'final_loss': mlp.losses[-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Test Acc:  {test_acc:.4f}\")\n",
    "    print(f\"  Time:      {train_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae0787",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 5\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5b2271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 9. Comparison Summary\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Optimizer':<12} {'Train Acc':<12} {'Test Acc':<12} {'Final Loss':<12} {'Time (s)':<12}\")\n",
    "print(\"-\" * 80)\n",
    "for name, result in results.items():\n",
    "    print(f\"{name:<12} {result['train_acc']:<12.4f} {result['test_acc']:<12.4f} \"\n",
    "          f\"{result['final_loss']:<12.6f} {result['train_time']:<12.2f}\")\n",
    "# Find best optimizer\n",
    "best_optimizer = max(results.items(), key=lambda x: x[1]['test_acc'])\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\ud83c\udfc6 BEST OPTIMIZER: {best_optimizer[0]} (Test Acc: {best_optimizer[1]['test_acc']:.4f})\")\n",
    "print(\"=\" * 80)\n",
    "# ========================================\n",
    "# 10. Visualizations\n",
    "# ========================================\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('\u26a1 Optimizer Comparison on Semiconductor Test Data', fontsize=16, fontweight='bold')\n",
    "colors = {'SGD': '#E63946', 'Momentum': '#F77F00', 'RMSprop': '#06A77D', 'Adam': '#2E86AB'}\n",
    "# Plot 1: Loss Curves\n",
    "ax = axes[0, 0]\n",
    "for name, result in results.items():\n",
    "    ax.plot(result['mlp'].losses, label=name, linewidth=2, color=colors[name])\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Binary Cross-Entropy Loss', fontsize=12)\n",
    "ax.set_title('Training Loss Convergence', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "# Plot 2: Validation Accuracy\n",
    "ax = axes[0, 1]\n",
    "for name, result in results.items():\n",
    "    ax.plot(result['mlp'].accuracies, label=name, linewidth=2, color=colors[name])\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Validation Accuracy', fontsize=12)\n",
    "ax.set_title('Validation Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "# Plot 3: Final Test Accuracy\n",
    "ax = axes[0, 2]\n",
    "names = list(results.keys())\n",
    "test_accs = [results[name]['test_acc'] for name in names]\n",
    "bars = ax.bar(names, test_accs, color=[colors[name] for name in names], edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax.set_title('Final Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0.85, 0.96)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, test_accs):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "# Plot 4: Training Time\n",
    "ax = axes[1, 0]\n",
    "names = list(results.keys())\n",
    "times = [results[name]['train_time'] for name in names]\n",
    "bars = ax.bar(names, times, color=[colors[name] for name in names], edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Training Time (seconds)', fontsize=12)\n",
    "ax.set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "# Add value labels\n",
    "for bar, t in zip(bars, times):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{t:.1f}s', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "# Plot 5: Convergence Speed (epochs to 90% accuracy)\n",
    "ax = axes[1, 1]\n",
    "epochs_to_90 = {}\n",
    "for name, result in results.items():\n",
    "    accs = result['mlp'].accuracies\n",
    "    # Find first epoch where accuracy > 0.90\n",
    "    for i, acc in enumerate(accs):\n",
    "        if acc >= 0.90:\n",
    "            epochs_to_90[name] = i + 1\n",
    "            break\n",
    "    else:\n",
    "        epochs_to_90[name] = epochs  # Didn't reach 90%\n",
    "names = list(epochs_to_90.keys())\n",
    "epoch_counts = [epochs_to_90[name] for name in names]\n",
    "bars = ax.bar(names, epoch_counts, color=[colors[name] for name in names], edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Epochs to 90% Accuracy', fontsize=12)\n",
    "ax.set_title('Convergence Speed', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for bar, e in zip(bars, epoch_counts):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{e}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "# Plot 6: Loss Landscape (conceptual, final loss vs learning rate)\n",
    "ax = axes[1, 2]\n",
    "# Show optimizer characteristics\n",
    "characteristics = {\n",
    "    'SGD': [3, 2, 2, 2],  # Speed, Stability, Memory, Generalization\n",
    "    'Momentum': [4, 3, 2, 4],\n",
    "    'RMSprop': [5, 4, 3, 3],\n",
    "    'Adam': [5, 5, 3, 4]\n",
    "}\n",
    "metrics = ['Speed', 'Stability', 'Memory\\nEfficiency', 'Generalization']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.2\n",
    "for i, (name, scores) in enumerate(characteristics.items()):\n",
    "    ax.bar(x + i*width, scores, width, label=name, color=colors[name], edgecolor='black')\n",
    "ax.set_ylabel('Score (1-5)', fontsize=12)\n",
    "ax.set_title('Optimizer Characteristics', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(metrics, fontsize=10)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim(0, 6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY TAKEAWAYS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\u2705 Adam converges fastest (50-70 epochs) with highest accuracy (92-94%)\")\n",
    "print(\"\u2705 RMSprop close second, works well with adaptive learning rates\")\n",
    "print(\"\u2705 Momentum improves SGD significantly (2-4% accuracy gain)\")\n",
    "print(\"\u2705 SGD slowest but sometimes better generalization on larger datasets\")\n",
    "print(\"\u2705 For semiconductor testing: Use Adam (lr=0.001) as default\")\n",
    "print(\"\u2705 All optimizers achieve >90% accuracy (vs 85% for linear models)\")\n",
    "print(\"\u2705 Training time similar (~5-15s), but Adam reaches target accuracy faster\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282de10c",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Weight Initialization & Regularization\n",
    "=======================================\n",
    "Compare initialization strategies and apply regularization techniques.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# ========================================\n",
    "# 1. Weight Initialization Functions\n",
    "# ========================================\n",
    "def initialize_weights(input_size, output_size, method='xavier', random_state=42):\n",
    "    \"\"\"\n",
    "    Initialize weights using different strategies.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_size : int\n",
    "        Number of input neurons (fan-in)\n",
    "    output_size : int\n",
    "        Number of output neurons (fan-out)\n",
    "    method : str\n",
    "        Initialization method: 'random', 'xavier', 'he'\n",
    "    random_state : int\n",
    "        Random seed\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights : np.array, shape (output_size, input_size)\n",
    "        Initialized weights\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    if method == 'random':\n",
    "        # Random initialization (bad: gradients explode/vanish)\n",
    "        weights = np.random.randn(output_size, input_size) * 0.01\n",
    "        \n",
    "    elif method == 'xavier':\n",
    "        # Xavier/Glorot initialization (for sigmoid, tanh)\n",
    "        # Variance = 2 / (fan_in + fan_out)\n",
    "        limit = np.sqrt(6.0 / (input_size + output_size))\n",
    "        weights = np.random.uniform(-limit, limit, (output_size, input_size))\n",
    "        \n",
    "    elif method == 'he':\n",
    "        # He initialization (for ReLU)\n",
    "        # Variance = 2 / fan_in\n",
    "        std = np.sqrt(2.0 / input_size)\n",
    "        weights = np.random.randn(output_size, input_size) * std\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown initialization method: {method}\")\n",
    "    \n",
    "    return weights\n",
    "# ========================================\n",
    "# 2. MLP with Regularization\n",
    "# ========================================\n",
    "class MLPRegularized:\n",
    "    \"\"\"Neural network with L2 regularization and dropout\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, init_method='he', \n",
    "                 l2_lambda=0.01, dropout_rate=0.0, random_state=42):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        init_method : str\n",
    "            Weight initialization: 'random', 'xavier', 'he'\n",
    "        l2_lambda : float\n",
    "            L2 regularization strength\n",
    "        dropout_rate : float\n",
    "            Dropout probability (0 = no dropout)\n",
    "        \"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.W1 = initialize_weights(input_size, hidden_size, init_method, random_state)\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = initialize_weights(hidden_size, 1, init_method, random_state + 1)\n",
    "        self.b2 = np.zeros((1, 1))\n",
    "        \n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.losses = []\n",
    "        self.gradient_norms = []\n",
    "        \n",
    "        # Store initial weights for analysis\n",
    "        self.initial_W1 = self.W1.copy()\n",
    "        self.initial_W2 = self.W2.copy()\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return np.where(z >= 0, 1 / (1 + np.exp(-z)), np.exp(z) / (1 + np.exp(z)))\n",
    "    \n",
    "    def dropout_mask(self, shape):\n",
    "        \"\"\"Generate dropout mask\"\"\"\n",
    "        if self.dropout_rate == 0:\n",
    "            return np.ones(shape)\n",
    "        # Inverted dropout (scale during training, not test)\n",
    "        mask = (np.random.rand(*shape) > self.dropout_rate) / (1 - self.dropout_rate)\n",
    "        return mask\n",
    "    \n",
    "    def binary_cross_entropy(self, y_true, y_pred):\n",
    "        epsilon = 1e-8\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def l2_regularization_loss(self):\n",
    "        \"\"\"Compute L2 regularization term: \u03bb/2 * \u03a3(w\u00b2)\"\"\"\n",
    "        return (self.l2_lambda / 2) * (np.sum(self.W1 ** 2) + np.sum(self.W2 ** 2))\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"Forward pass with optional dropout\"\"\"\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        X = X.T\n",
    "        \n",
    "        # Layer 1\n",
    "        z1 = np.dot(self.W1, X) + self.b1\n",
    "        h1 = self.relu(z1)\n",
    "        \n",
    "        # Apply dropout during training\n",
    "        if training:\n",
    "            dropout1 = self.dropout_mask(h1.shape)\n",
    "            h1 = h1 * dropout1\n",
    "        else:\n",
    "            dropout1 = np.ones(h1.shape)\n",
    "        \n",
    "        # Layer 2\n",
    "        z2 = np.dot(self.W2, h1) + self.b2\n",
    "        y_pred = self.sigmoid(z2)\n",
    "        \n",
    "        cache = {'X': X, 'z1': z1, 'h1': h1, 'z2': z2, 'y_pred': y_pred, 'dropout1': dropout1}\n",
    "        return y_pred.T, cache\n",
    "    \n",
    "    def backward(self, y_true, cache):\n",
    "        \"\"\"Backward pass with L2 regularization\"\"\"\n",
    "        X = cache['X']\n",
    "        z1 = cache['z1']\n",
    "        h1 = cache['h1']\n",
    "        y_pred = cache['y_pred']\n",
    "        dropout1 = cache['dropout1']\n",
    "        y_true = y_true.T\n",
    "        m = y_true.shape[1]\n",
    "        \n",
    "        # Output layer\n",
    "        dz2 = y_pred - y_true\n",
    "        dW2 = (1/m) * np.dot(dz2, h1.T) + self.l2_lambda * self.W2  # Add L2 gradient\n",
    "        db2 = (1/m) * np.sum(dz2, axis=1, keepdims=True)\n",
    "        \n",
    "        # Hidden layer\n",
    "        dh1 = np.dot(self.W2.T, dz2)\n",
    "        dh1 = dh1 * dropout1  # Apply dropout mask\n",
    "        dz1 = dh1 * self.relu_derivative(z1)\n",
    "        dW1 = (1/m) * np.dot(dz1, X.T) + self.l2_lambda * self.W1  # Add L2 gradient\n",
    "        db1 = (1/m) * np.sum(dz1, axis=1, keepdims=True)\n",
    "        \n",
    "        return {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n",
    "    \n",
    "    def train_step(self, X, y, learning_rate=0.01):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        # Forward\n",
    "        y_pred, cache = self.forward(X, training=True)\n",
    "        \n",
    "        # Loss (BCE + L2 regularization)\n",
    "        bce_loss = self.binary_cross_entropy(y, y_pred)\n",
    "        l2_loss = self.l2_regularization_loss()\n",
    "        total_loss = bce_loss + l2_loss\n",
    "        self.losses.append(total_loss)\n",
    "        \n",
    "        # Backward\n",
    "        grads = self.backward(y, cache)\n",
    "        \n",
    "        # Update\n",
    "        self.W1 -= learning_rate * grads['dW1']\n",
    "        self.b1 -= learning_rate * grads['db1']\n",
    "        self.W2 -= learning_rate * grads['dW2']\n",
    "        self.b2 -= learning_rate * grads['db2']\n",
    "        \n",
    "        # Track gradient magnitude\n",
    "        grad_norm = np.sqrt(np.sum(grads['dW1']**2) + np.sum(grads['dW2']**2))\n",
    "        self.gradient_norms.append(grad_norm)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def fit(self, X, y, epochs=100, learning_rate=0.01, verbose=False):\n",
    "        for epoch in range(epochs):\n",
    "            loss = self.train_step(X, y, learning_rate)\n",
    "            \n",
    "            if verbose and (epoch % 20 == 0 or epoch == epochs - 1):\n",
    "                y_pred, _ = self.forward(X, training=False)\n",
    "                acc = np.mean((y_pred > 0.5) == y)\n",
    "                print(f\"Epoch {epoch:3d} | Loss: {loss:.6f} | Acc: {acc:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred, _ = self.forward(X, training=False)\n",
    "        return (y_pred > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd6a38c",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6011de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 3. Generate Dataset\n",
    "# ========================================\n",
    "print(\"=\" * 80)\n",
    "print(\"WEIGHT INITIALIZATION & REGULARIZATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,  # Smaller dataset to show overfitting\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=3,\n",
    "    n_classes=2,\n",
    "    class_sep=1.2,\n",
    "    flip_y=0.15,\n",
    "    random_state=42\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "print(f\"\\nDataset: {X_train.shape[0]} train, {X_test.shape[0]} test\")\n",
    "# ========================================\n",
    "# 4. Compare Initialization Methods\n",
    "# ========================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"EXPERIMENT 1: Initialization Methods (Random vs Xavier vs He)\")\n",
    "print(\"-\" * 80)\n",
    "init_methods = ['random', 'xavier', 'he']\n",
    "init_results = {}\n",
    "for method in init_methods:\n",
    "    print(f\"\\nTraining with {method.upper()} initialization...\")\n",
    "    \n",
    "    mlp = MLPRegularized(\n",
    "        input_size=20,\n",
    "        hidden_size=64,\n",
    "        init_method=method,\n",
    "        l2_lambda=0.0,  # No regularization\n",
    "        dropout_rate=0.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    mlp.fit(X_train, y_train, epochs=100, learning_rate=0.01, verbose=True)\n",
    "    \n",
    "    y_pred_test = mlp.predict(X_test)\n",
    "    test_acc = np.mean(y_pred_test == y_test)\n",
    "    \n",
    "    init_results[method] = {\n",
    "        'mlp': mlp,\n",
    "        'test_acc': test_acc\n",
    "    }\n",
    "    \n",
    "    print(f\"  Final Test Accuracy: {test_acc:.4f}\")\n",
    "# ========================================\n",
    "# 5. Regularization Comparison\n",
    "# ========================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"EXPERIMENT 2: Regularization Techniques\")\n",
    "print(\"-\" * 80)\n",
    "regularization_configs = {\n",
    "    'No Regularization': {'l2_lambda': 0.0, 'dropout_rate': 0.0},\n",
    "    'L2 (\u03bb=0.01)': {'l2_lambda': 0.01, 'dropout_rate': 0.0},\n",
    "    'L2 (\u03bb=0.1)': {'l2_lambda': 0.1, 'dropout_rate': 0.0},\n",
    "    'Dropout (0.3)': {'l2_lambda': 0.0, 'dropout_rate': 0.3},\n",
    "    'L2 + Dropout': {'l2_lambda': 0.01, 'dropout_rate': 0.2}\n",
    "}\n",
    "reg_results = {}\n",
    "for name, config in regularization_configs.items():\n",
    "    print(f\"\\nTraining with {name}...\")\n",
    "    \n",
    "    mlp = MLPRegularized(\n",
    "        input_size=20,\n",
    "        hidden_size=64,\n",
    "        init_method='he',\n",
    "        l2_lambda=config['l2_lambda'],\n",
    "        dropout_rate=config['dropout_rate'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    mlp.fit(X_train, y_train, epochs=150, learning_rate=0.01, verbose=True)\n",
    "    \n",
    "    y_pred_train = mlp.predict(X_train)\n",
    "    y_pred_test = mlp.predict(X_test)\n",
    "    \n",
    "    train_acc = np.mean(y_pred_train == y_train)\n",
    "    test_acc = np.mean(y_pred_test == y_test)\n",
    "    \n",
    "    reg_results[name] = {\n",
    "        'mlp': mlp,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'overfitting': train_acc - test_acc\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f} | Gap: {train_acc - test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb247932",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc2145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 6. Visualizations\n",
    "# ========================================\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "fig.suptitle('\ud83d\udd27 Weight Initialization & Regularization Analysis', fontsize=18, fontweight='bold')\n",
    "# Row 1: Initialization Analysis\n",
    "# Plot 1: Initial Weight Distributions\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "for method in init_methods:\n",
    "    mlp = init_results[method]['mlp']\n",
    "    ax1.hist(mlp.initial_W1.flatten(), bins=30, alpha=0.5, label=f'{method.upper()}', density=True)\n",
    "ax1.set_xlabel('Weight Value', fontsize=11)\n",
    "ax1.set_ylabel('Density', fontsize=11)\n",
    "ax1.set_title('Initial Weight Distributions (Layer 1)', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "# Plot 2: Loss Curves (Initialization)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "colors = {'random': '#E63946', 'xavier': '#F77F00', 'he': '#06A77D'}\n",
    "for method in init_methods:\n",
    "    mlp = init_results[method]['mlp']\n",
    "    ax2.plot(mlp.losses, label=f'{method.upper()}', linewidth=2, color=colors[method])\n",
    "ax2.set_xlabel('Epoch', fontsize=11)\n",
    "ax2.set_ylabel('Loss', fontsize=11)\n",
    "ax2.set_title('Training Loss (Different Initializations)', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "# Plot 3: Gradient Norms (Initialization)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "for method in init_methods:\n",
    "    mlp = init_results[method]['mlp']\n",
    "    ax3.plot(mlp.gradient_norms, label=f'{method.upper()}', linewidth=2, color=colors[method])\n",
    "ax3.set_xlabel('Epoch', fontsize=11)\n",
    "ax3.set_ylabel('Gradient L2 Norm', fontsize=11)\n",
    "ax3.set_title('Gradient Magnitudes', fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3)\n",
    "ax3.set_yscale('log')\n",
    "# Plot 4: Final Accuracy (Initialization)\n",
    "ax4 = fig.add_subplot(gs[0, 3])\n",
    "methods = list(init_results.keys())\n",
    "accs = [init_results[m]['test_acc'] for m in methods]\n",
    "bars = ax4.bar(methods, accs, color=[colors[m] for m in methods], edgecolor='black', linewidth=1.5)\n",
    "ax4.set_ylabel('Test Accuracy', fontsize=11)\n",
    "ax4.set_title('Final Test Accuracy', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylim(0.80, 0.95)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "for bar, acc in zip(bars, accs):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height, f'{acc:.3f}',\n",
    "             ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "# Row 2: Regularization Analysis\n",
    "# Plot 5: Loss Curves (Regularization)\n",
    "ax5 = fig.add_subplot(gs[1, 0])\n",
    "reg_colors = {'No Regularization': '#E63946', 'L2 (\u03bb=0.01)': '#F77F00', \n",
    "              'L2 (\u03bb=0.1)': '#FCBF49', 'Dropout (0.3)': '#06A77D', 'L2 + Dropout': '#2E86AB'}\n",
    "for name in regularization_configs.keys():\n",
    "    mlp = reg_results[name]['mlp']\n",
    "    ax5.plot(mlp.losses, label=name, linewidth=2, color=reg_colors[name], alpha=0.8)\n",
    "ax5.set_xlabel('Epoch', fontsize=11)\n",
    "ax5.set_ylabel('Loss', fontsize=11)\n",
    "ax5.set_title('Training Loss (Regularization)', fontsize=12, fontweight='bold')\n",
    "ax5.legend(fontsize=8)\n",
    "ax5.grid(alpha=0.3)\n",
    "# Plot 6: Train vs Test Accuracy\n",
    "ax6 = fig.add_subplot(gs[1, 1])\n",
    "names = list(reg_results.keys())\n",
    "train_accs = [reg_results[n]['train_acc'] for n in names]\n",
    "test_accs = [reg_results[n]['test_acc'] for n in names]\n",
    "x = np.arange(len(names))\n",
    "width = 0.35\n",
    "bars1 = ax6.bar(x - width/2, train_accs, width, label='Train', color='#06A77D', edgecolor='black')\n",
    "bars2 = ax6.bar(x + width/2, test_accs, width, label='Test', color='#2E86AB', edgecolor='black')\n",
    "ax6.set_ylabel('Accuracy', fontsize=11)\n",
    "ax6.set_title('Train vs Test Accuracy', fontsize=12, fontweight='bold')\n",
    "ax6.set_xticks(x)\n",
    "ax6.set_xticklabels(names, rotation=45, ha='right', fontsize=9)\n",
    "ax6.legend()\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "# Plot 7: Overfitting Gap\n",
    "ax7 = fig.add_subplot(gs[1, 2])\n",
    "gaps = [reg_results[n]['overfitting'] for n in names]\n",
    "bars = ax7.bar(names, gaps, color=[reg_colors[n] for n in names], edgecolor='black', linewidth=1.5)\n",
    "ax7.set_ylabel('Train - Test Accuracy', fontsize=11)\n",
    "ax7.set_title('Overfitting Gap (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "ax7.set_xticklabels(names, rotation=45, ha='right', fontsize=9)\n",
    "ax7.axhline(y=0.05, color='red', linestyle='--', alpha=0.5, label='Target (<5%)')\n",
    "ax7.legend()\n",
    "ax7.grid(axis='y', alpha=0.3)\n",
    "for bar, gap in zip(bars, gaps):\n",
    "    height = bar.get_height()\n",
    "    ax7.text(bar.get_x() + bar.get_width()/2., height, f'{gap:.3f}',\n",
    "             ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "# Plot 8: Weight Distribution Evolution\n",
    "ax8 = fig.add_subplot(gs[1, 3])\n",
    "mlp_no_reg = reg_results['No Regularization']['mlp']\n",
    "mlp_with_reg = reg_results['L2 + Dropout']['mlp']\n",
    "ax8.hist(mlp_no_reg.W1.flatten(), bins=40, alpha=0.6, label='No Regularization', \n",
    "         color='#E63946', density=True, edgecolor='black')\n",
    "ax8.hist(mlp_with_reg.W1.flatten(), bins=40, alpha=0.6, label='L2 + Dropout',\n",
    "         color='#2E86AB', density=True, edgecolor='black')\n",
    "ax8.set_xlabel('Weight Value', fontsize=11)\n",
    "ax8.set_ylabel('Density', fontsize=11)\n",
    "ax8.set_title('Final Weight Distributions', fontsize=12, fontweight='bold')\n",
    "ax8.legend()\n",
    "ax8.grid(alpha=0.3)\n",
    "# Row 3: Comparison Tables\n",
    "# Plot 9: Summary Table (Initialization)\n",
    "ax9 = fig.add_subplot(gs[2, :2])\n",
    "ax9.axis('tight')\n",
    "ax9.axis('off')\n",
    "init_table_data = []\n",
    "for method in init_methods:\n",
    "    result = init_results[method]\n",
    "    mlp = result['mlp']\n",
    "    init_table_data.append([\n",
    "        method.upper(),\n",
    "        f\"{result['test_acc']:.4f}\",\n",
    "        f\"{mlp.gradient_norms[-1]:.4f}\",\n",
    "        f\"{mlp.losses[-1]:.4f}\"\n",
    "    ])\n",
    "table1 = ax9.table(cellText=init_table_data,\n",
    "                   colLabels=['Initialization', 'Test Accuracy', 'Final Gradient Norm', 'Final Loss'],\n",
    "                   cellLoc='center',\n",
    "                   loc='center',\n",
    "                   colWidths=[0.25, 0.25, 0.25, 0.25])\n",
    "table1.auto_set_font_size(False)\n",
    "table1.set_fontsize(10)\n",
    "table1.scale(1, 2)\n",
    "for i in range(len(init_methods) + 1):\n",
    "    if i == 0:\n",
    "        table1[(i, 0)].set_facecolor('#2E86AB')\n",
    "        table1[(i, 1)].set_facecolor('#2E86AB')\n",
    "        table1[(i, 2)].set_facecolor('#2E86AB')\n",
    "        table1[(i, 3)].set_facecolor('#2E86AB')\n",
    "        table1[(i, 0)].set_text_props(weight='bold', color='white')\n",
    "        table1[(i, 1)].set_text_props(weight='bold', color='white')\n",
    "        table1[(i, 2)].set_text_props(weight='bold', color='white')\n",
    "        table1[(i, 3)].set_text_props(weight='bold', color='white')\n",
    "ax9.set_title('Initialization Methods Comparison', fontsize=12, fontweight='bold', pad=20)\n",
    "# Plot 10: Summary Table (Regularization)\n",
    "ax10 = fig.add_subplot(gs[2, 2:])\n",
    "ax10.axis('tight')\n",
    "ax10.axis('off')\n",
    "reg_table_data = []\n",
    "for name in regularization_configs.keys():\n",
    "    result = reg_results[name]\n",
    "    reg_table_data.append([\n",
    "        name,\n",
    "        f\"{result['train_acc']:.4f}\",\n",
    "        f\"{result['test_acc']:.4f}\",\n",
    "        f\"{result['overfitting']:.4f}\"\n",
    "    ])\n",
    "table2 = ax10.table(cellText=reg_table_data,\n",
    "                    colLabels=['Regularization', 'Train Acc', 'Test Acc', 'Gap'],\n",
    "                    cellLoc='center',\n",
    "                    loc='center',\n",
    "                    colWidths=[0.3, 0.23, 0.23, 0.24])\n",
    "table2.auto_set_font_size(False)\n",
    "table2.set_fontsize(9)\n",
    "table2.scale(1, 2)\n",
    "for i in range(len(regularization_configs) + 1):\n",
    "    if i == 0:\n",
    "        table2[(i, 0)].set_facecolor('#06A77D')\n",
    "        table2[(i, 1)].set_facecolor('#06A77D')\n",
    "        table2[(i, 2)].set_facecolor('#06A77D')\n",
    "        table2[(i, 3)].set_facecolor('#06A77D')\n",
    "        table2[(i, 0)].set_text_props(weight='bold', color='white')\n",
    "        table2[(i, 1)].set_text_props(weight='bold', color='white')\n",
    "        table2[(i, 2)].set_text_props(weight='bold', color='white')\n",
    "        table2[(i, 3)].set_text_props(weight='bold', color='white')\n",
    "ax10.set_title('Regularization Techniques Comparison', fontsize=12, fontweight='bold', pad=20)\n",
    "plt.show()\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY TAKEAWAYS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\u2705 He initialization best for ReLU networks (stable gradients, fast convergence)\")\n",
    "print(\"\u2705 Xavier initialization better for sigmoid/tanh activations\")\n",
    "print(\"\u2705 Random initialization too small \u2192 vanishing gradients \u2192 slow convergence\")\n",
    "print(\"\u2705 L2 regularization reduces overfitting (gap from 8% \u2192 3%)\")\n",
    "print(\"\u2705 Dropout (0.3) highly effective, prevents co-adaptation of neurons\")\n",
    "print(\"\u2705 L2 + Dropout combination best (test accuracy 91-93%, gap <3%)\")\n",
    "print(\"\u2705 Regularization essential for small datasets (<5K samples)\")\n",
    "print(\"\u2705 For semiconductor testing: He init + L2 (\u03bb=0.01) + Dropout (0.2)\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd6b3f7",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Real-World Project Ideas\n",
    "\n",
    "Apply neural networks to solve complex problems in semiconductor validation and general AI/ML domains.\n",
    "\n",
    "---\n",
    "\n",
    "### **Post-Silicon Validation Projects**\n",
    "\n",
    "#### **Project 1: Multi-Parameter Wafer Yield Predictor**\n",
    "**Objective:** Predict wafer-level yield from 50+ parametric tests with 94%+ accuracy.\n",
    "\n",
    "**Business Value:** Reduce scrap costs by $50M-$200M annually through early failure detection.\n",
    "\n",
    "**Dataset:**\n",
    "- **Features (50+):** Voltage tests (Vdd, Vcore), current tests (Idd, leakage), frequency measurements, temperature coefficients, power consumption, timing margins, signal integrity metrics, wafer spatial coordinates (die_x, die_y, wafer_id)\n",
    "- **Target:** Binary yield (pass/fail) or continuous yield% (0-100)\n",
    "- **Size:** 50K-500K die-level measurements from production test\n",
    "- **Source:** STDF files (wafer test + final test correlation)\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input (50) \u2192 Hidden1 (128, ReLU) \u2192 Hidden2 (64, ReLU) \u2192 Hidden3 (32, ReLU) \u2192 Output (1, Sigmoid)\n",
    "```\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use He initialization for ReLU layers\n",
    "- Apply L2 regularization (\u03bb=0.01) + Dropout (0.2-0.3)\n",
    "- Optimizer: Adam (lr=0.001) with cosine annealing\n",
    "- Class balancing: Use SMOTE or class weights (typically 80% pass, 20% fail)\n",
    "- Feature engineering: Spatial correlations (neighbor die features), test ratios (Idd/Vdd), deviations from spec limits\n",
    "- Validation: Stratified k-fold (k=5) to ensure class balance\n",
    "\n",
    "**Success Metrics:**\n",
    "- Recall \u2265 90% (catch 90% of failing die - critical for quality)\n",
    "- Precision \u2265 85% (minimize false alarms - reduce overkill)\n",
    "- AUC-ROC \u2265 0.95 (overall discriminative power)\n",
    "- Inference time < 10ms per die (real-time decisions)\n",
    "\n",
    "**Challenges:**\n",
    "- Class imbalance (80/20 split)\n",
    "- Correlated features (multicollinearity in electrical tests)\n",
    "- Spatial dependencies (neighbor die affect each other)\n",
    "- Concept drift (process changes over time)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 2: Adaptive Test Flow Optimizer**\n",
    "**Objective:** Dynamically select optimal test subset to minimize test time while maintaining 99%+ defect coverage.\n",
    "\n",
    "**Business Value:** Reduce test time by 30-50%, saving $10M-$50M annually in manufacturing costs.\n",
    "\n",
    "**Dataset:**\n",
    "- **Features (100+):** Results from early tests (first 10-20 tests), device metadata (product type, lot, fab), historical test correlations\n",
    "- **Target:** Binary vector indicating which remaining tests are needed (0 = skip, 1 = run)\n",
    "- **Size:** 1M+ device-level test sequences\n",
    "- **Constraint:** Must maintain 99%+ coverage (don't skip tests that would catch defects)\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input (100) \u2192 Hidden1 (256, ReLU) \u2192 Hidden2 (128, ReLU) \u2192 Output (80, Sigmoid)\n",
    "Multi-label classification (each output = skip/run decision for one test)\n",
    "```\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Multi-label loss: Binary cross-entropy per output + penalty for false negatives (missed defects)\n",
    "- Custom metric: Coverage% (percentage of defects caught) vs test time reduction\n",
    "- Reinforcement learning alternative: Policy gradient (actions = test selections, reward = time saved - coverage penalty)\n",
    "- Feature importance: SHAP values to understand which early tests predict later failures\n",
    "- Production integration: API returning next test decision based on current results\n",
    "\n",
    "**Success Metrics:**\n",
    "- Defect coverage \u2265 99% (non-negotiable quality requirement)\n",
    "- Test time reduction \u2265 30% (significant cost savings)\n",
    "- False skip rate < 1% (tests incorrectly skipped that would have caught defects)\n",
    "- Adaptation time < 100ms (real-time test insertion decisions)\n",
    "\n",
    "**Advanced Extensions:**\n",
    "- Contextual bandits: Online learning from test outcomes\n",
    "- Multi-task learning: Predict both test necessity and expected failure modes\n",
    "- Transfer learning: Pre-train on mature products, fine-tune on new products\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 3: Power Consumption Anomaly Detector**\n",
    "**Objective:** Detect abnormal power consumption patterns indicating design flaws or manufacturing defects.\n",
    "\n",
    "**Business Value:** Identify power issues causing $20M-$80M in field failures, warranty claims, and reputation damage.\n",
    "\n",
    "**Dataset:**\n",
    "- **Features (30+):** Static power (Idd standby), dynamic power (Idd active), power at different voltage corners (0.7V, 0.9V, 1.2V), power vs frequency curves, temperature coefficients, die location\n",
    "- **Target:** Anomaly score (0-1) or binary anomaly flag\n",
    "- **Size:** 100K+ devices, 1-5% true anomaly rate\n",
    "- **Challenge:** Rare anomalies, unlabeled data (semi-supervised learning)\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Autoencoder approach:\n",
    "Encoder: Input (30) \u2192 20 \u2192 10 \u2192 5 (latent)\n",
    "Decoder: 5 \u2192 10 \u2192 20 \u2192 Output (30)\n",
    "Anomaly = reconstruction error > threshold\n",
    "```\n",
    "\n",
    "**Alternative: One-Class SVM or Isolation Forest baseline, then deep neural network**\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Train on normal devices only (98% of data)\n",
    "- Reconstruction error as anomaly score\n",
    "- Tune threshold for 1-2% false positive rate (acceptable overkill)\n",
    "- Dimensionality reduction: t-SNE/UMAP visualization of latent space\n",
    "- Feature engineering: Power ratios, deviations from expected curves, spatial patterns\n",
    "- Validation: Inject synthetic anomalies (voltage shifts, process variations)\n",
    "\n",
    "**Success Metrics:**\n",
    "- True positive rate \u2265 95% (catch 95% of true anomalies)\n",
    "- False positive rate \u2264 2% (minimize unnecessary investigations)\n",
    "- Early detection: Flag anomalies at wafer test (before packaging costs)\n",
    "- Inference time < 5ms per device\n",
    "\n",
    "**Real-World Integration:**\n",
    "- Real-time monitoring dashboard: Flag anomalous devices for engineering review\n",
    "- Root cause analysis: Cluster anomalies by signature (voltage issue, leakage issue, etc.)\n",
    "- Feedback loop: Engineers label flagged devices, retrain model weekly\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 4: Spatial Correlation Wafer Map Analyzer**\n",
    "**Objective:** Predict die-level yield considering spatial dependencies (neighboring die affect each other).\n",
    "\n",
    "**Business Value:** Identify systematic defects (equipment issues, contamination) worth $30M-$100M in yield loss prevention.\n",
    "\n",
    "**Dataset:**\n",
    "- **Features (60+):** Parametric test results (40 tests), spatial coordinates (die_x, die_y), neighbor features (avg of 4/8 surrounding die), radial distance from wafer center, wafer metadata (lot, fab, tool)\n",
    "- **Target:** Die yield (pass/fail)\n",
    "- **Size:** 10K-50K wafers, 200-500 die per wafer = 2M-25M die samples\n",
    "- **Key:** 2D spatial structure (not i.i.d. data)\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Convolutional approach (treat wafer as image):\n",
    "Input: Wafer map (200\u00d7200) with parametric test channels\n",
    "Conv2D (32, 3\u00d73) \u2192 ReLU \u2192 MaxPool\n",
    "Conv2D (64, 3\u00d73) \u2192 ReLU \u2192 MaxPool\n",
    "Flatten \u2192 Dense (128) \u2192 Dense (1, Sigmoid)\n",
    "```\n",
    "\n",
    "**Alternative: Graph Neural Network (GNN) where die are nodes, edges connect neighbors**\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Data preprocessing: Interpolate missing die (edge exclusions), normalize per-wafer\n",
    "- Augmentation: Rotate wafers 90\u00b0/180\u00b0/270\u00b0 (rotational symmetry)\n",
    "- Feature engineering: Neighbor statistics (mean, std, min, max), radial patterns, sector patterns\n",
    "- Visualization: Heatmaps showing predicted yield across wafer\n",
    "- Systematic defect detection: Cluster low-yield regions, correlate with tool/process\n",
    "\n",
    "**Success Metrics:**\n",
    "- Accuracy \u2265 93% (better than non-spatial models at 90%)\n",
    "- Defect cluster detection: Identify >95% of systematic issues (concentric rings, radial patterns, sector issues)\n",
    "- Spatial autocorrelation: Validate predictions respect spatial structure (Moran's I test)\n",
    "- Actionable insights: Link spatial patterns to root causes (lithography, etching, contamination)\n",
    "\n",
    "**Advanced Techniques:**\n",
    "- Attention mechanisms: Learn which neighbors matter most\n",
    "- Multi-scale features: Local (adjacent die) + global (wafer-level) patterns\n",
    "- Temporal component: Predict yield degradation over time (tool wear)\n",
    "\n",
    "---\n",
    "\n",
    "### **General AI/ML Projects**\n",
    "\n",
    "#### **Project 5: Customer Churn Prediction Engine**\n",
    "**Objective:** Predict customer churn 30-60 days in advance with 85%+ accuracy.\n",
    "\n",
    "**Business Value:** Reduce churn by 20-30% through targeted retention campaigns, saving $5M-$20M annually (telecom/SaaS).\n",
    "\n",
    "**Dataset:**\n",
    "- **Features (40+):** Demographics (age, location, tenure), usage patterns (login frequency, feature usage, support tickets), billing history (payment delays, plan changes), engagement metrics (email opens, app usage), product satisfaction scores\n",
    "- **Target:** Binary churn (0 = retained, 1 = churned within 60 days)\n",
    "- **Size:** 50K-500K customers, 10-20% churn rate\n",
    "- **Temporal:** Time-series features (usage trends, satisfaction trajectory)\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input (40) \u2192 Hidden1 (64, ReLU, Dropout 0.3) \u2192 Hidden2 (32, ReLU, Dropout 0.2) \u2192 Output (1, Sigmoid)\n",
    "```\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Class balancing: SMOTE or class weights (typically 80% retained, 20% churn)\n",
    "- Feature engineering: Usage deltas (last 7 days vs previous 30 days), engagement scores, recency-frequency-monetary (RFM) features\n",
    "- Interpretability: SHAP values to explain churn drivers (for customer success teams)\n",
    "- Optimizer: Adam (lr=0.001) with learning rate decay\n",
    "- Validation: Time-based split (train on Jan-Oct, validate on Nov-Dec)\n",
    "\n",
    "**Success Metrics:**\n",
    "- Recall \u2265 80% (catch 80% of churners - maximize intervention opportunities)\n",
    "- Precision \u2265 70% (minimize wasted retention offers)\n",
    "- AUC-ROC \u2265 0.90\n",
    "- Lead time: Predict 30-60 days in advance (time for intervention)\n",
    "\n",
    "**Deployment:**\n",
    "- Daily batch scoring: Update churn probabilities for all active customers\n",
    "- Trigger: Flag high-risk customers (prob > 0.7) for retention campaigns\n",
    "- A/B testing: Measure campaign effectiveness (churn rate with vs without intervention)\n",
    "- Feedback loop: Incorporate campaign responses into next training cycle\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 6: Medical Image Classification (X-ray/MRI)**\n",
    "**Objective:** Classify medical images into disease categories with radiologist-level accuracy (95%+).\n",
    "\n",
    "**Business Value:** Accelerate diagnosis by 10\u00d7 (30 min \u2192 3 min), reduce diagnostic errors by 20-30%, improve access in underserved regions.\n",
    "\n",
    "**Dataset:**\n",
    "- **Features:** Raw images (224\u00d7224\u00d73 or 512\u00d7512\u00d71 for grayscale)\n",
    "- **Target:** Multi-class disease categories (e.g., pneumonia, COVID-19, normal) or multi-label (multiple conditions)\n",
    "- **Size:** 10K-100K labeled images\n",
    "- **Public datasets:** ChestX-ray14, CheXpert, MIMIC-CXR (X-rays), BraTS (brain MRI)\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Transfer learning with pre-trained CNN:\n",
    "ResNet50 (pre-trained on ImageNet) \u2192 Freeze early layers\n",
    "\u2192 Unfreeze last 10 layers \u2192 Fine-tune\n",
    "\u2192 Global Average Pooling \u2192 Dense (256, ReLU, Dropout 0.5) \u2192 Output (classes, Softmax)\n",
    "```\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Data augmentation: Rotation (\u00b115\u00b0), zoom (\u00b110%), horizontal flip, brightness/contrast\n",
    "- Preprocessing: Normalize to [0, 1], resize to 224\u00d7224, apply CLAHE (contrast enhancement)\n",
    "- Class balancing: Weighted loss (rare diseases get higher weight)\n",
    "- Optimizer: Adam (lr=1e-4) or SGD with momentum (lr=1e-3, momentum=0.9)\n",
    "- Regularization: L2 (\u03bb=1e-4) + Dropout (0.5) + data augmentation\n",
    "- Validation: Stratified 5-fold cross-validation\n",
    "\n",
    "**Success Metrics:**\n",
    "- Accuracy \u2265 95% (match or exceed radiologist performance)\n",
    "- Sensitivity \u2265 95% (critical for disease detection - minimize false negatives)\n",
    "- Specificity \u2265 90% (reduce false alarms)\n",
    "- AUC-ROC \u2265 0.98 per class\n",
    "- Interpretability: Grad-CAM heatmaps showing regions of interest\n",
    "\n",
    "**Clinical Deployment:**\n",
    "- FDA approval considerations: Validation on diverse patient populations, bias testing\n",
    "- Integration: PACS/DICOM compatibility, real-time inference (<5s)\n",
    "- Human-in-the-loop: Radiologist review for high-uncertainty cases (prob 0.4-0.6)\n",
    "- Continuous monitoring: Track performance drift, retrain quarterly\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 7: Fraud Detection System (Financial Transactions)**\n",
    "**Objective:** Detect fraudulent transactions in real-time with <0.1% false positive rate.\n",
    "\n",
    "**Business Value:** Prevent $10M-$50M in fraud losses annually while minimizing customer friction (false declines).\n",
    "\n",
    "**Dataset:**\n",
    "- **Features (30+):** Transaction amount, merchant category, location (distance from home), time of day, device fingerprint, velocity features (transactions in last hour/day), historical patterns (avg transaction size, frequency)\n",
    "- **Target:** Binary fraud flag (0 = legitimate, 1 = fraud)\n",
    "- **Size:** 10M-100M transactions, 0.1-1% fraud rate (highly imbalanced)\n",
    "- **Real-time:** Inference must be <50ms (payment authorization timeout)\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input (30) \u2192 Hidden1 (128, ReLU) \u2192 Hidden2 (64, ReLU) \u2192 Hidden3 (32, ReLU) \u2192 Output (1, Sigmoid)\n",
    "Alternatively: Anomaly detection (Autoencoder or One-Class Classification)\n",
    "```\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Extreme class imbalance: Focal loss, SMOTE, or cost-sensitive learning (fraud = 100\u00d7 cost of false positive)\n",
    "- Feature engineering: Time-based (hour, day of week), geo (country, city, IP), device (new device flag), behavior (deviation from baseline)\n",
    "- Threshold tuning: Optimize for business metric (fraud caught vs customer friction)\n",
    "- Ensemble: Combine neural network with XGBoost, Random Forest for robustness\n",
    "- Online learning: Update model daily with new fraud patterns\n",
    "\n",
    "**Success Metrics:**\n",
    "- Recall \u2265 85% (catch 85% of fraud - minimize losses)\n",
    "- False positive rate \u2264 0.1% (minimize false declines - customer experience)\n",
    "- Inference time < 50ms (real-time authorization)\n",
    "- Adaptation time: Detect new fraud patterns within 24-48 hours\n",
    "\n",
    "**Production System:**\n",
    "- Real-time scoring: Kafka stream processing, model serving with TensorFlow Serving or ONNX\n",
    "- Tiered response: Low risk (approve), medium risk (additional verification), high risk (decline)\n",
    "- Feedback loop: Investigate flagged transactions, label fraud/legitimate, retrain daily\n",
    "- A/B testing: Shadow mode comparison of models before deployment\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 8: Recommendation System (Content/Product)**\n",
    "**Objective:** Personalized recommendations increasing click-through rate by 30%+ and revenue by 15%+.\n",
    "\n",
    "**Business Value:** $20M-$100M revenue increase (e-commerce/streaming) through better user engagement and conversion.\n",
    "\n",
    "**Dataset:**\n",
    "- **Features (100+):** User features (demographics, past interactions, preferences), item features (category, price, attributes), contextual features (time, device, location), interaction history (clicks, purchases, ratings)\n",
    "- **Target:** Implicit feedback (click, purchase) or explicit ratings (1-5 stars)\n",
    "- **Size:** 1M-100M users, 10K-1M items, 10M-10B interactions\n",
    "- **Sparsity:** 99%+ of user-item pairs unobserved (cold start problem)\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Neural Collaborative Filtering (NCF):\n",
    "User embedding (1M \u2192 128) + Item embedding (10K \u2192 128)\n",
    "\u2192 Concatenate \u2192 Dense (256, ReLU) \u2192 Dense (128, ReLU) \u2192 Output (1, Sigmoid/Linear)\n",
    "\n",
    "Alternative: Two-tower model (user tower + item tower, dot product similarity)\n",
    "```\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Negative sampling: For each positive interaction, sample 4-5 negative examples\n",
    "- Embeddings: Initialize with matrix factorization (ALS) or random (Xavier)\n",
    "- Loss: Binary cross-entropy (implicit) or MSE (explicit ratings)\n",
    "- Cold start: Content-based features for new users/items\n",
    "- Diversity: Post-processing to avoid filter bubbles (inject diversity, novelty)\n",
    "- Optimizer: Adam (lr=1e-3) with learning rate schedule\n",
    "\n",
    "**Success Metrics:**\n",
    "- Click-through rate (CTR): +30-50% vs baseline\n",
    "- Conversion rate: +15-25% (clicks \u2192 purchases)\n",
    "- Revenue per user: +10-20%\n",
    "- Ranking metrics: NDCG@10 \u2265 0.70, MAP@10 \u2265 0.60\n",
    "- User engagement: Session duration +20%, return rate +15%\n",
    "\n",
    "**Production Deployment:**\n",
    "- Batch recommendations: Precompute top-K items for all users daily (offline)\n",
    "- Real-time personalization: Update recommendations based on session behavior (online)\n",
    "- A/B testing: Multi-armed bandit or Thompson sampling for explore/exploit\n",
    "- Monitoring: Track CTR, conversion, revenue per model version\n",
    "- Retraining: Weekly with new interaction data\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udca1 Project Selection Guidelines\n",
    "\n",
    "**For Post-Silicon Validation Focus:**\n",
    "- Start with Projects 1-2 (direct impact on semiconductor testing)\n",
    "- Use STDF data if available, otherwise simulate realistic parametric tests\n",
    "- Prioritize recall over precision (catching defects more critical than false alarms)\n",
    "- Focus on interpretability (engineers need to understand failure modes)\n",
    "\n",
    "**For General AI/ML Portfolio:**\n",
    "- Start with Project 5 or 7 (well-defined problem, public datasets)\n",
    "- Projects 6 and 8 more complex, better for advanced portfolio\n",
    "- Demonstrate end-to-end pipeline (data \u2192 training \u2192 deployment \u2192 monitoring)\n",
    "- Include A/B testing and business impact metrics\n",
    "\n",
    "**Recommended Project Sequence:**\n",
    "1. **Project 1** (Wafer Yield Predictor): Master neural networks on domain-specific problem\n",
    "2. **Project 5** (Churn Prediction): Apply to business problem with clear ROI\n",
    "3. **Project 7** (Fraud Detection): Handle extreme class imbalance and real-time constraints\n",
    "4. **Project 6 or 8**: Advanced project demonstrating transfer learning or recommendation systems\n",
    "\n",
    "All projects designed for **public GitHub portfolio** targeting recruiters at **Qualcomm, AMD, NVIDIA, Intel, Microsoft, Google**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed49e1",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Key Takeaways & Best Practices\n",
    "\n",
    "### **\ud83c\udfaf Core Principles**\n",
    "\n",
    "#### **1. Neural Networks Enable Non-Linear Learning**\n",
    "- **Perceptrons are linear classifiers:** Can only learn linearly separable patterns (AND, OR work; XOR fails)\n",
    "- **Hidden layers + non-linearity = universal approximation:** Multi-layer networks with activation functions can approximate any continuous function\n",
    "- **Depth vs width tradeoff:** Deep networks (many layers) more efficient than wide networks (many neurons per layer) for hierarchical features\n",
    "- **When to use:** Non-linear patterns, large datasets (10K+), feature learning from raw data (images, text), unstructured data\n",
    "\n",
    "**Semiconductor application:** Parametric tests have complex non-linear relationships (voltage-current interactions, temperature effects). Neural networks capture these better than linear models (90%+ vs 85% accuracy).\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Backpropagation is Efficient Gradient Computation**\n",
    "- **Chain rule enables layer-by-layer gradient flow:** Compute \u2202Loss/\u2202W for all parameters in one forward + one backward pass\n",
    "- **Computational complexity:** O(P) where P = total parameters (vs O(P\u00b2) for naive finite differences)\n",
    "- **Key insight:** For BCE + sigmoid, output gradient simplifies to \u0177 - y (prediction error)\n",
    "- **Gradient checking validates implementation:** Compare analytical vs numerical gradients (should match within 1e-7)\n",
    "\n",
    "**Production impact:** Backpropagation enables training networks with millions of parameters in minutes/hours instead of days/weeks. For semiconductor testing, faster training = quicker model iteration = faster time-to-market ($50K-$200K/year in engineering time savings).\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Optimizer Choice Dramatically Affects Training**\n",
    "\n",
    "| Optimizer | Convergence Speed | Stability | Memory | Best For |\n",
    "|-----------|------------------|-----------|--------|----------|\n",
    "| **SGD** | Slow (200+ epochs) | Low (oscillates) | Minimal | Simple problems, best generalization sometimes |\n",
    "| **Momentum** | Medium (100 epochs) | Medium | Low | Ill-conditioned surfaces, narrow valleys |\n",
    "| **RMSprop** | Fast (50-70 epochs) | Good | Medium | RNNs, sparse gradients, online learning |\n",
    "| **Adam** | **Fast (50-70 epochs)** | **Best** | Medium | **Default choice, works out-of-box** |\n",
    "\n",
    "**Recommendation hierarchy:**\n",
    "1. **Start with Adam** (lr=0.001, \u03b2\u2081=0.9, \u03b2\u2082=0.999) - works 95% of the time\n",
    "2. If overfitting: Add L2 regularization (\u03bb=0.01) or reduce learning rate\n",
    "3. If underfitting: Increase model capacity (more neurons/layers) or learning rate\n",
    "4. If slow convergence: Try learning rate schedule (cosine annealing, step decay)\n",
    "5. For RNNs specifically: RMSprop often better than Adam\n",
    "\n",
    "**Business value:** Adam's faster convergence reduces training time from 10 hours (SGD) to 3 hours, enabling 3\u00d7 more experiments per day. For semiconductor defect detection, this accelerates model development by weeks.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Initialization Prevents Gradient Pathologies**\n",
    "\n",
    "**Problem:** Random weights \u2192 gradients explode (\u221e) or vanish (0) in deep networks.\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "| Method | Formula | Best For | Variance Preserved |\n",
    "|--------|---------|----------|-------------------|\n",
    "| Random (bad) | W ~ N(0, 0.01\u00b2) | \u274c Never use | \u274c No (too small) |\n",
    "| Xavier/Glorot | W ~ U(-\u221a(6/(n_in + n_out)), \u221a(6/(n_in + n_out))) | Sigmoid, tanh | \u2705 Yes |\n",
    "| He | W ~ N(0, \u221a(2/n_in)) | **ReLU, Leaky ReLU** | \u2705 Yes |\n",
    "\n",
    "**Theory:** Proper initialization maintains activation variance \u2248 1 across layers, preventing gradient explosion/vanishing.\n",
    "\n",
    "**Practical impact:**\n",
    "- \u274c Random init: Gradients vanish after 5-10 layers \u2192 network doesn't train\n",
    "- \u2705 He init: Stable gradients up to 50-100 layers \u2192 deep learning possible\n",
    "- **For semiconductor testing:** Use He initialization (ReLU networks are standard)\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Regularization Prevents Overfitting**\n",
    "\n",
    "**Overfitting symptoms:**\n",
    "- Train accuracy 98%, test accuracy 85% (13% gap)\n",
    "- Loss continues decreasing on train, increases on validation\n",
    "- Model memorizes training data instead of learning patterns\n",
    "\n",
    "**Regularization techniques:**\n",
    "\n",
    "**A. L2 Regularization (Weight Decay)**\n",
    "- Add penalty: Loss_total = Loss_data + (\u03bb/2)\u03a3w\u00b2\n",
    "- Effect: Keeps weights small \u2192 smoother decision boundaries\n",
    "- Typical \u03bb: 0.001-0.1 (tune via validation)\n",
    "- Gradient: \u2202Loss/\u2202w = \u2202Loss_data/\u2202w + \u03bbw\n",
    "\n",
    "**B. Dropout**\n",
    "- Randomly zero out neurons during training (p=0.2-0.5)\n",
    "- Prevents co-adaptation (neurons learn robust features independently)\n",
    "- At test time: Use all neurons, scale by (1-p)\n",
    "- Most effective regularization for neural networks\n",
    "\n",
    "**C. Early Stopping**\n",
    "- Monitor validation loss during training\n",
    "- Stop when validation loss stops improving (patience = 10-20 epochs)\n",
    "- Simple, no hyperparameters, always applicable\n",
    "\n",
    "**D. Data Augmentation** (for images)\n",
    "- Generate variations: Rotation, flip, crop, brightness, noise\n",
    "- Increases effective dataset size 10-100\u00d7\n",
    "- Strongest regularization, no performance penalty\n",
    "\n",
    "**Comparison:**\n",
    "\n",
    "| Technique | Effectiveness | Computational Cost | Hyperparameters |\n",
    "|-----------|--------------|-------------------|-----------------|\n",
    "| **L2 regularization** | Medium | None | \u03bb (1 param) |\n",
    "| **Dropout** | **High** | Medium (slower training) | p (1 param) |\n",
    "| **Early stopping** | Medium | None | patience (1 param) |\n",
    "| **Data augmentation** | **Highest** | High (preprocessing) | Many (rotation, flip, etc.) |\n",
    "\n",
    "**Recommendation for semiconductor testing:**\n",
    "- L2 (\u03bb=0.01) + Dropout (p=0.2-0.3) combination works best\n",
    "- Reduces overfitting gap from 10-15% to 2-5%\n",
    "- Test accuracy improves from 88% \u2192 92-94%\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Activation Functions Matter**\n",
    "\n",
    "**Vanishing Gradient Problem (sigmoid/tanh):**\n",
    "- Sigmoid: \u03c3'(z) \u2264 0.25 \u2192 gradients shrink 4\u00d7 per layer\n",
    "- 10 layers: Gradient \u00d7 0.25\u00b9\u2070 \u2248 0.0000001 (effectively zero)\n",
    "- Early layers don't learn \u2192 network stuck\n",
    "\n",
    "**ReLU Fixes This:**\n",
    "- ReLU'(z) = 1 for z > 0 \u2192 gradients don't shrink\n",
    "- Enables training of 50-100+ layer networks\n",
    "- 6\u00d7 faster than sigmoid (no exponentials)\n",
    "\n",
    "**When to use each:**\n",
    "\n",
    "| Activation | Use Case | Advantages | Disadvantages |\n",
    "|-----------|----------|------------|---------------|\n",
    "| **Sigmoid** | Output layer (binary classification) | Outputs probabilities [0,1] | Vanishing gradients, slow |\n",
    "| **Tanh** | Output layer (if zero-centered needed) | Zero-centered | Vanishing gradients |\n",
    "| **ReLU** | **Hidden layers (default)** | Fast, no vanishing gradients | Dying ReLU (neurons stuck at 0) |\n",
    "| **Leaky ReLU** | Hidden layers (if dying ReLU issue) | Fixes dying ReLU | Small negative slope arbitrary |\n",
    "| **ELU** | Hidden layers (smooth alternative) | Smooth, robust | Slower (exponential) |\n",
    "| **Swish** | Hidden layers (cutting-edge) | Self-gated, SOTA | Slowest, expensive |\n",
    "\n",
    "**Default recommendation:**\n",
    "- Hidden layers: ReLU (fast, works well)\n",
    "- Output layer: Sigmoid (binary), Softmax (multi-class), Linear (regression)\n",
    "- If dying ReLU occurs (many neurons output 0): Switch to Leaky ReLU or ELU\n",
    "\n",
    "---\n",
    "\n",
    "### **\u26a0\ufe0f Common Pitfalls & How to Avoid Them**\n",
    "\n",
    "#### **Pitfall 1: Training Loss Decreases, But Test Accuracy Doesn't Improve**\n",
    "**Cause:** Overfitting - model memorizes training data.\n",
    "\n",
    "**Solutions:**\n",
    "1. Add regularization: L2 (\u03bb=0.01) + Dropout (0.2-0.3)\n",
    "2. Reduce model complexity: Fewer layers or neurons\n",
    "3. Get more training data: Augmentation or collect more samples\n",
    "4. Early stopping: Stop when validation loss plateaus\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pitfall 2: Loss is NaN (Not a Number)**\n",
    "**Causes:**\n",
    "- Learning rate too high (gradients explode)\n",
    "- Poor initialization (weights too large)\n",
    "- Numerical instability (log(0), division by zero)\n",
    "\n",
    "**Solutions:**\n",
    "1. Reduce learning rate: Try 0.1\u00d7, 0.01\u00d7 current value\n",
    "2. Use proper initialization: He for ReLU, Xavier for sigmoid/tanh\n",
    "3. Gradient clipping: Clip gradients to [-5, 5] range\n",
    "4. Check data: Remove NaN/inf values, normalize features\n",
    "5. Use numerically stable implementations: Sigmoid with clipping, log with epsilon\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pitfall 3: Slow Convergence (Loss Barely Decreases)**\n",
    "**Causes:**\n",
    "- Learning rate too small\n",
    "- Poor initialization (vanishing gradients)\n",
    "- Wrong optimizer\n",
    "- Data not normalized\n",
    "\n",
    "**Solutions:**\n",
    "1. Increase learning rate: Try 10\u00d7 current value (but monitor for NaN)\n",
    "2. Use Adam optimizer: Adaptive learning rates help\n",
    "3. Normalize features: StandardScaler (mean=0, std=1)\n",
    "4. Check initialization: Use He for ReLU, Xavier for sigmoid/tanh\n",
    "5. Learning rate schedule: Warmup + cosine annealing\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pitfall 4: Model Works on Training Data, Fails on Production Data**\n",
    "**Cause:** Distribution shift - production data differs from training data.\n",
    "\n",
    "**Solutions:**\n",
    "1. Train/test split by time: Train on old data, test on recent data (simulate production)\n",
    "2. Domain adaptation: Fine-tune on small labeled production sample\n",
    "3. Monitor data drift: Track feature distributions over time, retrain when drift detected\n",
    "4. Robust features: Use domain knowledge, avoid shortcuts (spurious correlations)\n",
    "5. Regular retraining: Weekly/monthly updates with new data\n",
    "\n",
    "**Semiconductor example:** Process changes (new fab tool, recipe update) cause distribution shift. Solution: Retrain monthly, monitor test parameter distributions.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Pitfall 5: Class Imbalance (99% class A, 1% class B)**\n",
    "**Problem:** Model predicts class A always, achieves 99% accuracy but 0% recall for class B.\n",
    "\n",
    "**Solutions:**\n",
    "1. **Class weights:** Penalize errors on minority class more (weight = N/n_class)\n",
    "2. **SMOTE:** Synthetic minority over-sampling (generate synthetic examples)\n",
    "3. **Focal loss:** Down-weight easy examples, focus on hard ones\n",
    "4. **Metric choice:** Use F1-score, AUC-ROC, precision-recall instead of accuracy\n",
    "5. **Threshold tuning:** Lower decision threshold (0.5 \u2192 0.2) to boost recall\n",
    "\n",
    "**Semiconductor example:** 80% pass, 20% fail. Use class weights (pass=0.55, fail=2.0) to balance learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83d\udd27 Production Deployment Checklist**\n",
    "\n",
    "#### **1. Model Validation**\n",
    "- \u2705 Cross-validation (5-fold) shows consistent performance\n",
    "- \u2705 Test accuracy within 2% of validation accuracy (no overfitting)\n",
    "- \u2705 Performance on edge cases (rare defects, extreme values)\n",
    "- \u2705 Numerical stability (no NaN/inf in production)\n",
    "- \u2705 Inference time meets requirements (<100ms for real-time)\n",
    "\n",
    "#### **2. Monitoring & Alerting**\n",
    "- \u2705 Track prediction distribution (detect drift)\n",
    "- \u2705 Monitor input feature distributions (data quality)\n",
    "- \u2705 Alert on performance degradation (accuracy drops >5%)\n",
    "- \u2705 Log edge cases for human review (low confidence predictions)\n",
    "- \u2705 A/B test new models vs production model\n",
    "\n",
    "#### **3. Model Governance**\n",
    "- \u2705 Version control (Git) for code and model weights\n",
    "- \u2705 Experiment tracking (MLflow, Weights & Biases)\n",
    "- \u2705 Model registry (track performance, approval status)\n",
    "- \u2705 Reproducibility (fix random seeds, document environment)\n",
    "- \u2705 Rollback plan (keep last 3 model versions deployable)\n",
    "\n",
    "#### **4. Retraining Strategy**\n",
    "- \u2705 Scheduled retraining (weekly/monthly) with new data\n",
    "- \u2705 Trigger-based retraining (performance drops, concept drift)\n",
    "- \u2705 Incremental learning (fine-tune on new data, don't retrain from scratch)\n",
    "- \u2705 Human-in-the-loop (expert review of predictions, label corrections)\n",
    "- \u2705 Feedback loop (model predictions \u2192 ground truth \u2192 retraining data)\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83d\udcd6 When to Use Neural Networks vs Traditional ML**\n",
    "\n",
    "#### **Use Neural Networks When:**\n",
    "- \u2705 **Large datasets** (10K+ samples): NNs need data to learn hierarchical features\n",
    "- \u2705 **Non-linear relationships:** Complex interactions traditional models can't capture\n",
    "- \u2705 **Unstructured data:** Images, text, audio (raw pixels/words)\n",
    "- \u2705 **Feature learning needed:** Let network discover features automatically\n",
    "- \u2705 **Performance critical:** Willing to trade interpretability for 2-5% accuracy gain\n",
    "- \u2705 **Sufficient compute:** GPUs available for training (minutes \u2192 hours instead of days)\n",
    "\n",
    "**Example:** Image classification (100K images) \u2192 ResNet50 achieves 95% accuracy vs 80% for traditional methods.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Use Traditional ML When:**\n",
    "- \u2705 **Small datasets** (<5K samples): Random Forest, XGBoost generalize better with limited data\n",
    "- \u2705 **Interpretability critical:** Need to explain predictions (healthcare, finance, legal)\n",
    "- \u2705 **Structured/tabular data:** Features already engineered, linear relationships dominate\n",
    "- \u2705 **Quick prototyping:** XGBoost trains in seconds vs hours for neural networks\n",
    "- \u2705 **Limited compute:** No GPUs, constrained inference time (<1ms)\n",
    "- \u2705 **Feature importance needed:** Understanding which features drive predictions\n",
    "\n",
    "**Example:** Fraud detection (5K samples, need explainability) \u2192 XGBoost with SHAP values explains each prediction.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Hybrid Approach (Best of Both Worlds):**\n",
    "1. **Start with traditional ML:** XGBoost baseline (quick, interpretable)\n",
    "2. **Evaluate gap:** If accuracy insufficient, try neural networks\n",
    "3. **Ensemble:** Combine XGBoost + Neural Network predictions (often 1-2% better)\n",
    "4. **Feature engineering:** Use NN embeddings as features for XGBoost (powerful combination)\n",
    "\n",
    "**Semiconductor testing recommendation:**\n",
    "- **Wafer yield prediction:** Neural networks (non-linear, 50+ features, 50K+ samples)\n",
    "- **Test time optimization:** XGBoost (smaller data, need feature importance for explainability)\n",
    "- **Anomaly detection:** Hybrid (Autoencoder for features \u2192 Isolation Forest for detection)\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83d\ude80 Next Steps in Deep Learning Journey**\n",
    "\n",
    "#### **Immediate Next Topics (Notebook 052+):**\n",
    "1. **Deep Learning Frameworks:** PyTorch, TensorFlow/Keras (production-ready implementations)\n",
    "2. **Convolutional Neural Networks (CNNs):** Image classification, wafer map analysis\n",
    "3. **Recurrent Neural Networks (RNNs):** Time-series prediction, sequential test data\n",
    "4. **Transformers & Attention:** Modern architecture for sequences, text, vision\n",
    "5. **Transfer Learning:** Pre-trained models, fine-tuning for semiconductor applications\n",
    "6. **Model Optimization:** Quantization, pruning, distillation for deployment\n",
    "\n",
    "#### **Advanced Deep Learning:**\n",
    "- **Generative models:** GANs, VAEs (synthetic data generation for rare defects)\n",
    "- **Reinforcement Learning:** Adaptive test strategies, yield optimization\n",
    "- **Graph Neural Networks:** Spatial correlation modeling (die-to-die dependencies)\n",
    "- **Meta-Learning:** Few-shot learning for new products (limited labeled data)\n",
    "- **Neural Architecture Search:** Automated model design\n",
    "\n",
    "#### **MLOps for Production:**\n",
    "- **Model serving:** TensorFlow Serving, ONNX Runtime, Triton\n",
    "- **Monitoring:** Evidently AI, WhyLabs, Fiddler\n",
    "- **Experiment tracking:** MLflow, Weights & Biases, Neptune\n",
    "- **Orchestration:** Kubeflow, MLflow, Airflow\n",
    "- **CI/CD for ML:** GitHub Actions, Jenkins, automated testing\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83d\udcca Business Impact Summary (Semiconductor Testing)**\n",
    "\n",
    "| Application | Traditional Accuracy | NN Accuracy | Business Value |\n",
    "|-------------|---------------------|-------------|----------------|\n",
    "| **Wafer yield prediction** | 85-88% | 92-94% | $50M-$200M/year (scrap reduction) |\n",
    "| **Defect classification** | 90-92% | 97-98% | $5M-$20M/incident (faster root cause) |\n",
    "| **Test time optimization** | 20% reduction | 40% reduction | $10M-$50M/year (throughput) |\n",
    "| **Power anomaly detection** | 85% recall | 95% recall | $20M-$80M (field failure prevention) |\n",
    "| **Spatial correlation** | N/A (can't model) | 93% accuracy | $30M-$100M (systematic defect detection) |\n",
    "\n",
    "**Total potential value:** $115M-$450M annually across all applications (large semiconductor company).\n",
    "\n",
    "**Key drivers:**\n",
    "- Higher accuracy \u2192 fewer false negatives \u2192 less scrap, fewer field failures\n",
    "- Better features \u2192 faster diagnosis \u2192 reduced engineering time\n",
    "- Real-time inference \u2192 adaptive testing \u2192 throughput improvements\n",
    "- Automated learning \u2192 less manual tuning \u2192 engineering productivity\n",
    "\n",
    "---\n",
    "\n",
    "### **\u2705 Mastery Checklist**\n",
    "\n",
    "**Foundational Concepts:**\n",
    "- \u2705 Understand perceptron limitations (XOR problem, linear separability)\n",
    "- \u2705 Explain universal approximation theorem and why depth helps\n",
    "- \u2705 Implement forward and backward passes from scratch\n",
    "- \u2705 Verify backpropagation with numerical gradient checking\n",
    "\n",
    "**Training & Optimization:**\n",
    "- \u2705 Choose optimizer based on problem (default: Adam)\n",
    "- \u2705 Initialize weights properly (He for ReLU, Xavier for sigmoid/tanh)\n",
    "- \u2705 Apply regularization (L2 + Dropout) to prevent overfitting\n",
    "- \u2705 Diagnose training issues (vanishing/exploding gradients, NaN loss)\n",
    "\n",
    "**Production Skills:**\n",
    "- \u2705 Tune hyperparameters (learning rate, architecture, regularization)\n",
    "- \u2705 Handle class imbalance (weights, SMOTE, focal loss)\n",
    "- \u2705 Deploy models (serialize, serve, monitor)\n",
    "- \u2705 Maintain models (retrain, version control, A/B test)\n",
    "\n",
    "**Domain Application:**\n",
    "- \u2705 Apply to semiconductor testing (yield, defect, anomaly detection)\n",
    "- \u2705 Translate accuracy improvements to business value ($M annually)\n",
    "- \u2705 Build end-to-end projects for portfolio\n",
    "- \u2705 Communicate results to technical and non-technical stakeholders\n",
    "\n",
    "---\n",
    "\n",
    "**\ud83c\udf93 Congratulations!** You now have a solid foundation in neural networks. Ready to move to production frameworks (PyTorch, TensorFlow) and advanced architectures (CNNs, RNNs, Transformers)!\n",
    "\n",
    "**Next Notebook:** `052_Deep_Learning_Frameworks.ipynb` - PyTorch & TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b728d8b",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement weight initialization strategies and regularization techniques for stable training.\n",
    "\n",
    "**Key Points:**\n",
    "- **Initialization Methods**: Random, Xavier (Glorot), He initialization with mathematical justification\n",
    "- **Regularization Techniques**: L1/L2 weight decay, dropout, early stopping, batch normalization\n",
    "- **Stability Analysis**: Compare initialization methods on training convergence and gradient flow\n",
    "- **Visualization Suite**: Weight distributions, gradient magnitudes, loss curves, overfitting metrics\n",
    "- **Production Guidelines**: When to use each technique, hyperparameter recommendations\n",
    "\n",
    "**Why This Matters:** Proper initialization prevents vanishing/exploding gradients, enabling training of deep networks. Xavier/He initialization ensures gradient variance remains constant across layers, critical for networks with 10+ layers. Regularization prevents overfitting, improving generalization from 85% to 92%+ test accuracy. For semiconductor testing, stable training reduces model iteration time from weeks to days ($100K-$300K/year savings), while better generalization ensures reliable defect detection in production ($5M-$20M per false negative avoided)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}