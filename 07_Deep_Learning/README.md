# 07 - Deep Learning

**Purpose:** Master neural networks, CNNs, RNNs, transformers, and modern deep learning architectures

Deep learning has revolutionized AI, powering breakthroughs in computer vision, natural language processing, and reinforcement learning. This comprehensive section covers everything from neural network fundamentals to state-of-the-art architectures like Transformers, GANs, and multi-agent systems. Essential for anyone building modern AI applications.

## ðŸ“Š Learning Path Statistics

- **Total Notebooks:** 27
- **Completion Status:** âœ… All complete
- **Topics Covered:** Neural networks, CNNs, RNNs, Transformers, BERT, GPT, GANs, RL, vision models, NLP, model optimization
- **Applications:** Image classification, object detection, NLP, sequence modeling, generative AI, reinforcement learning

---

## ðŸ“š Notebooks

### **Neural Network Foundations (051-052)**

#### [051_Neural_Networks_Foundations.ipynb](051_Neural_Networks_Foundations.ipynb)
**Fundamentals of Neural Networks and Backpropagation**

Build neural networks from scratch to understand the mathematical foundations: forward propagation, backpropagation, gradient descent, and training dynamics.

**Topics:** Perceptrons, multi-layer networks, activation functions (ReLU, sigmoid, tanh), loss functions, backpropagation algorithm, gradient descent variants (SGD, momentum, Adam), weight initialization (Xavier, He), vanishing/exploding gradients

**Applications:** Parametric test classification, yield prediction with non-linear patterns, device binning, test time regression

**Key Techniques:** Implement backprop from scratch using NumPy, understand computational graphs, diagnose training issues (learning curves, gradient norms)

---

#### [052_Deep_Learning_Frameworks.ipynb](052_Deep_Learning_Frameworks.ipynb)
**PyTorch and TensorFlow for Production DL**

Master industry-standard deep learning frameworks: PyTorch for research flexibility and TensorFlow for production deployment.

**Topics:** PyTorch tensors and autograd, TensorFlow/Keras APIs, model definition patterns, training loops, GPU acceleration, mixed precision training, distributed training, model serialization

**Applications:** Build production-ready neural networks for wafer defect detection, test time forecasting, parametric outlier detection

**Key Techniques:** PyTorch Lightning for clean code, TensorBoard for experiment tracking, ONNX for framework interoperability

---

### **Computer Vision (053-055)**

#### [053_CNN_Architectures.ipynb](053_CNN_Architectures.ipynb)
**Convolutional Neural Networks and Classic Architectures**

Master CNNs for image analysis: from basic convolutions to ResNet, Inception, and EfficientNet architectures.

**Topics:** Convolution operations, pooling layers, CNN architectures (LeNet, AlexNet, VGG, ResNet, Inception, EfficientNet), skip connections, depthwise separable convolutions, architecture design principles

**Applications:** Wafer map defect classification, die-level image analysis, equipment camera inspection, PCB defect detection

**Key Techniques:** Implement convolutions from scratch, understand receptive fields, visualize learned filters, apply data augmentation

---

#### [054_Transfer_Learning_Fine_Tuning.ipynb](054_Transfer_Learning_Fine_Tuning.ipynb)
**Transfer Learning for Computer Vision**

Leverage pre-trained models (ImageNet) for custom tasks: feature extraction vs fine-tuning, domain adaptation strategies.

**Topics:** Transfer learning concepts, feature extraction, fine-tuning strategies, layer freezing, learning rate scheduling, domain shift handling, few-shot learning

**Applications:** Wafer defect classification with limited labeled data (transfer from ImageNet), custom device inspection, PCB analysis

**Key Techniques:** Fine-tune ResNet50/EfficientNet, progressive unfreezing, discriminative learning rates, domain adaptation

---

#### [055_Object_Detection_YOLO_RCNN.ipynb](055_Object_Detection_YOLO_RCNN.ipynb)
**Object Detection: YOLO, R-CNN, and Modern Detectors**

Master object detection for locating and classifying multiple objects in images: from two-stage (R-CNN) to one-stage (YOLO) detectors.

**Topics:** Object detection fundamentals, R-CNN family (Fast R-CNN, Faster R-CNN), YOLO (v3-v8), SSD, RetinaNet, anchor boxes, non-max suppression, evaluation metrics (mAP, IoU)

**Applications:** Wafer map defect localization, die-level anomaly detection, equipment component inspection, PCB component detection

**Key Techniques:** Train YOLOv8 on custom datasets, optimize for real-time inference, handle class imbalance in detection

---

### **Sequence Models (056-062)**

#### [056_RNN_LSTM_GRU.ipynb](056_RNN_LSTM_GRU.ipynb)
**Recurrent Neural Networks for Sequential Data**

Master RNNs for time series and sequences: vanilla RNNs, LSTM (Long Short-Term Memory), GRU (Gated Recurrent Units).

**Topics:** RNN fundamentals, backpropagation through time (BPTT), LSTM architecture (gates: forget, input, output), GRU architecture, vanishing gradient solutions, bidirectional RNNs

**Applications:** Test time prediction sequences, parametric trend forecasting, equipment sensor data modeling, multi-step yield prediction

**Key Techniques:** Implement LSTM from scratch, compare LSTM vs GRU vs vanilla RNN, handle variable-length sequences

---

#### [057_Seq2Seq_Attention.ipynb](057_Seq2Seq_Attention.ipynb)
**Sequence-to-Sequence Models with Attention**

Learn encoder-decoder architectures for sequence transformation tasks: translation, summarization, and time series forecasting.

**Topics:** Seq2Seq architecture, encoder-decoder pattern, attention mechanisms (Bahdanau, Luong), beam search decoding, teacher forcing, attention visualization

**Applications:** Test program instruction translation, log summarization, multi-variate time series forecasting, alarm message processing

**Key Techniques:** Build Seq2Seq models in PyTorch, implement attention from scratch, visualize attention weights

---

#### [058_Transformers_Self_Attention.ipynb](058_Transformers_Self_Attention.ipynb)
**Transformer Architecture and Self-Attention**

Understand the revolutionary Transformer architecture that powers modern NLP: self-attention, positional encoding, multi-head attention.

**Topics:** Self-attention mechanism, scaled dot-product attention, multi-head attention, positional encoding, encoder-decoder transformers, layer normalization, residual connections

**Applications:** Parametric test sequence modeling, multi-variate time series forecasting, log analysis, test program optimization

**Key Techniques:** Implement self-attention from scratch, understand attention patterns, apply transformers to time series (not just NLP)

---

#### [059_BERT_Transfer_Learning_NLP.ipynb](059_BERT_Transfer_Learning_NLP.ipynb)
**BERT for NLP Transfer Learning**

Master BERT (Bidirectional Encoder Representations from Transformers) for NLP tasks: pre-training and fine-tuning strategies.

**Topics:** BERT architecture, masked language modeling (MLM), next sentence prediction (NSP), fine-tuning for classification/NER/QA, tokenization (WordPiece), domain adaptation

**Applications:** Test failure log classification, defect description analysis, equipment alarm categorization, documentation search

**Key Techniques:** Fine-tune BERT for text classification, extract embeddings, apply to domain-specific text (semiconductor jargon)

---

#### [060_GPT_Autoregressive_Language_Models.ipynb](060_GPT_Autoregressive_Language_Models.ipynb)
**GPT Architecture for Text Generation**

Learn autoregressive language models (GPT family) for text generation and completion tasks.

**Topics:** GPT architecture, causal self-attention, autoregressive generation, sampling strategies (temperature, top-k, nucleus), perplexity, pre-training vs fine-tuning

**Applications:** Test program code generation, failure report summarization, documentation generation, automated defect descriptions

**Key Techniques:** Fine-tune GPT-2 on custom text, implement text generation with sampling strategies, evaluate perplexity

---

#### [061_RLHF_Instruction_Following.ipynb](061_RLHF_Instruction_Following.ipynb)
**Reinforcement Learning from Human Feedback (RLHF)**

Master RLHF for aligning language models with human preferences: the technique behind ChatGPT's instruction-following capabilities.

**Topics:** RLHF pipeline, reward modeling, PPO (Proximal Policy Optimization), human preference collection, InstructGPT/ChatGPT approach, safety considerations

**Applications:** Fine-tune models for domain-specific instruction following (test engineering tasks), preference learning for equipment control

**Key Techniques:** Implement RLHF pipeline, train reward models, apply PPO for policy optimization

---

#### [062_Seq2Seq_Neural_Machine_Translation.ipynb](062_Seq2Seq_Neural_Machine_Translation.ipynb)
**Neural Machine Translation with Seq2Seq and Transformers**

Build translation systems using Seq2Seq models and Transformers: encoder-decoder architecture at scale.

**Topics:** NMT fundamentals, Seq2Seq with attention, Transformer for translation, beam search, BLEU score, subword tokenization (BPE, SentencePiece), multilingual models

**Applications:** Translate test specifications across languages, convert legacy test code formats, cross-site communication (multinational fabs)

**Key Techniques:** Train Transformer translation models, optimize beam search, evaluate with BLEU/METEOR, handle rare words

---

### **Generative AI & RL (063-077)**

#### [063_Generative_Adversarial_Networks.ipynb](063_Generative_Adversarial_Networks.ipynb)
**GANs for Image Generation**

Master Generative Adversarial Networks for creating synthetic data: vanilla GAN, DCGAN, StyleGAN, conditional GANs.

**Topics:** GAN architecture (generator vs discriminator), training dynamics, mode collapse, loss functions (Wasserstein, least squares), conditional GANs, StyleGAN, evaluation metrics (FID, IS)

**Applications:** Generate synthetic wafer maps for data augmentation, create rare defect patterns, synthetic parametric test data for privacy

**Key Techniques:** Train stable GANs, diagnose mode collapse, generate conditional images, evaluate quality with FID score

---

#### [064_Reinforcement_Learning_Basics.ipynb](064_Reinforcement_Learning_Basics.ipynb)
**Foundations of Reinforcement Learning**

Learn RL fundamentals: Markov Decision Processes, Q-learning, policy gradients, value functions.

**Topics:** MDP formulation, Bellman equations, Q-learning, SARSA, policy iteration, value iteration, exploration vs exploitation (Îµ-greedy, UCB), temporal difference learning

**Applications:** Optimize test program sequences, adaptive test flow control, equipment scheduling, fab resource allocation

**Key Techniques:** Implement Q-learning from scratch, solve grid world problems, apply to test optimization

---

#### [065_Deep_Reinforcement_Learning.ipynb](065_Deep_Reinforcement_Learning.ipynb)
**Deep Q-Networks and Policy Gradients**

Master deep RL algorithms: DQN, DDQN, policy gradients, Actor-Critic methods.

**Topics:** DQN (Deep Q-Network), experience replay, target networks, Double DQN, policy gradient theorem, REINFORCE, Actor-Critic, A3C, PPO

**Applications:** Adaptive test flow optimization, real-time equipment control, multi-stage process optimization, robotic wafer handling

**Key Techniques:** Implement DQN with experience replay, train policy gradient methods, stabilize training with target networks

---

#### [066_Attention_Mechanisms.ipynb](066_Attention_Mechanisms.ipynb)
**Advanced Attention Mechanisms**

Deep dive into attention variants: self-attention, cross-attention, sparse attention, efficient transformers.

**Topics:** Attention fundamentals, self-attention vs cross-attention, multi-head attention, sparse attention (Longformer, BigBird), efficient transformers (Linformer, Performer), attention visualization

**Applications:** Long sequence modeling (parametric tests with 1000+ parameters), multi-modal fusion (image + text), time series with long dependencies

**Key Techniques:** Implement custom attention mechanisms, visualize attention patterns, apply sparse attention for long sequences

---

#### [067_Neural_Architecture_Search.ipynb](067_Neural_Architecture_Search.ipynb)
**Automated Neural Network Design (NAS)**

Master NAS for discovering optimal architectures: reinforcement learning, evolutionary, and gradient-based NAS.

**Topics:** NAS algorithms (RL-based, evolutionary, DARTS), search spaces (cell-based, macro), weight sharing (ENAS), hardware-aware NAS, efficiency metrics

**Applications:** Design custom architectures for wafer map analysis, optimize for edge deployment (tester hardware), multi-task architectures

**Key Techniques:** Implement simple NAS with random search, apply DARTS for differentiable search, evaluate architectures efficiently

---

#### [068_Model_Compression_Quantization.ipynb](068_Model_Compression_Quantization.ipynb)
**Model Optimization for Deployment**

Master techniques for compressing models: pruning, quantization, knowledge distillation, for edge/mobile deployment.

**Topics:** Model pruning (structured, unstructured), quantization (INT8, INT4), quantization-aware training, knowledge distillation, neural architecture distillation, TensorRT optimization

**Applications:** Deploy models on tester hardware (limited compute), real-time inference (<10ms), reduce model size 4-10x

**Key Techniques:** Apply INT8 quantization, prune networks 50-90%, distill large models to small students, optimize with TensorRT/ONNX

---

#### [069_Federated_Learning.ipynb](069_Federated_Learning.ipynb)
**Federated Learning for Distributed Training**

Learn privacy-preserving ML: train models across multiple sites without centralizing data.

**Topics:** Federated learning algorithms (FedAvg, FedProx), differential privacy, secure aggregation, communication efficiency, non-IID data challenges, personalization

**Applications:** Multi-site yield prediction (data stays at each fab), cross-company model training (IP protection), privacy-preserving analytics

**Key Techniques:** Implement FedAvg, apply differential privacy, handle non-IID data distributions, optimize communication

---

#### [070_Edge_AI_TinyML.ipynb](070_Edge_AI_TinyML.ipynb)
**TinyML for Resource-Constrained Devices**

Master deployment on microcontrollers and edge devices: model optimization for <1MB memory, <10mW power.

**Topics:** TinyML constraints, model architecture design for edge, quantization for MCUs, TensorFlow Lite Micro, model conversion, latency optimization, power consumption

**Applications:** On-tester real-time inference, edge parametric screening, portable test equipment, battery-powered sensors

**Key Techniques:** Convert models to TFLite, deploy on ARM Cortex-M, optimize for <100KB flash, profile latency/power

---

#### [071_Transformers_BERT.ipynb](071_Transformers_BERT.ipynb)
**Advanced Transformer and BERT Techniques**

Deep dive into BERT variants and Transformer optimizations: RoBERTa, ALBERT, DeBERTa, domain adaptation.

**Topics:** BERT variants (RoBERTa, ALBERT, DeBERTa, DistilBERT), domain-specific pre-training, continued pre-training, adapter layers, prompt engineering for BERT

**Applications:** Semiconductor-specific NLP (test logs, defect reports), technical documentation search, failure root cause extraction

**Key Techniques:** Domain-adaptive pre-training on semiconductor text, fine-tune with limited labels, apply adapter layers

---

#### [072_GPT_Large_Language_Models.ipynb](072_GPT_Large_Language_Models.ipynb)
**Large Language Models and GPT Architectures**

Master large-scale autoregressive models: GPT-2, GPT-3, scaling laws, emergent capabilities.

**Topics:** GPT architecture at scale, scaling laws, few-shot learning, in-context learning, emergent abilities, prompt engineering, API usage (OpenAI), fine-tuning large models

**Applications:** Test program code generation, documentation generation, automated defect report summarization, conversational test engineering assistant

**Key Techniques:** Prompt engineering for few-shot learning, fine-tune GPT models, use OpenAI API effectively, evaluate generation quality

---

#### [073_Vision_Transformers.ipynb](073_Vision_Transformers.ipynb)
**Vision Transformers (ViT) for Image Analysis**

Apply Transformers to computer vision: ViT, Swin Transformer, replacing CNNs with pure attention.

**Topics:** Vision Transformer (ViT) architecture, patch embeddings, Swin Transformer (hierarchical), DeiT (data-efficient), hybrid CNN-Transformer models, self-supervised pre-training (MAE)

**Applications:** Wafer map classification without CNNs, die-level defect detection, high-resolution image analysis, hierarchical image features

**Key Techniques:** Train ViT from scratch, fine-tune pre-trained ViT models, compare to ResNet, apply masked autoencoding

---

#### [074_Multimodal_Models.ipynb](074_Multimodal_Models.ipynb)
**Multimodal Learning: Vision + Language**

Master models that combine multiple modalities: CLIP, DALL-E, Flamingo, vision-language pre-training.

**Topics:** Multimodal architectures, CLIP (contrastive vision-language learning), image captioning, visual question answering (VQA), DALL-E, Flamingo, multimodal fusion strategies

**Applications:** Combine wafer maps + test logs for root cause analysis, image + parametric data fusion, automated defect report generation with images

**Key Techniques:** Fine-tune CLIP for custom vision-language tasks, implement multimodal fusion, generate image captions

---

#### [075_Reinforcement_Learning.ipynb](075_Reinforcement_Learning.ipynb)
**Advanced Reinforcement Learning Algorithms**

Deep dive into modern RL: DQN variants, policy gradient methods, Actor-Critic, off-policy learning.

**Topics:** Rainbow DQN (combining improvements), Dueling DQN, prioritized experience replay, A2C/A3C, PPO (Proximal Policy Optimization), TRPO, SAC (Soft Actor-Critic)

**Applications:** Advanced test flow optimization, multi-objective equipment control, continuous action spaces (process parameters), long-horizon planning

**Key Techniques:** Implement Rainbow DQN, train PPO for continuous control, apply SAC to real-world tasks, benchmark algorithms

---

#### [076_Deep_Reinforcement_Learning.ipynb](076_Deep_Reinforcement_Learning.ipynb)
**Model-Based RL and Advanced Topics**

Master model-based RL, world models, planning, and hybrid approaches.

**Topics:** Model-based RL, world models (PlaNet, Dreamer), Dyna architecture, MCTS (Monte Carlo Tree Search), AlphaGo/AlphaZero, imagination-based planning, model ensembles

**Applications:** Equipment behavior modeling (digital twins), predictive process control, planning test sequences, sample-efficient learning

**Key Techniques:** Train world models for prediction, combine model-based + model-free RL, apply MCTS for planning

---

#### [077_Multi_Agent_RL.ipynb](077_Multi_Agent_RL.ipynb)
**Multi-Agent Reinforcement Learning**

Learn MARL for coordinating multiple agents: cooperative, competitive, and mixed scenarios.

**Topics:** MARL fundamentals, independent Q-learning (IQL), centralized training decentralized execution (CTDE), QMIX, MADDPG, emergent cooperation, game theory (Nash equilibrium)

**Applications:** Multi-equipment coordination in fab, distributed test systems, multi-robot wafer handling, collaborative process optimization

**Key Techniques:** Implement IQL and QMIX, train cooperative agents, handle partial observability, apply to multi-robot scenarios

---

## ðŸ”— Prerequisites

**Required Knowledge:**
- **02-05:** Classical ML (regression, trees, clustering)
- **06_Time_Series:** Sequence modeling fundamentals
- **Calculus:** Derivatives, chain rule, partial derivatives (for backpropagation)
- **Linear Algebra:** Matrix operations, dot products, eigenvalues
- **Python Libraries:** NumPy, PyTorch or TensorFlow

**Recommended Background:**
- **GPU Computing:** CUDA basics, memory management
- **Optimization:** Gradient descent, Adam optimizer, learning rate scheduling
- **Probability:** Distributions, expectation, Bayes' theorem (for RL)

---

## ðŸŽ¯ Key Learning Outcomes

By completing this section, you will:

âœ… **Build Neural Networks from Scratch:** Implement backpropagation, understand training dynamics  
âœ… **Master PyTorch/TensorFlow:** Production-ready deep learning code  
âœ… **Apply CNNs to Images:** Wafer maps, defect detection, equipment inspection  
âœ… **Model Sequences with RNNs/LSTMs:** Time series, parametric trends, test sequences  
âœ… **Understand Transformers:** Self-attention, BERT, GPT architectures  
âœ… **Generate Synthetic Data:** GANs for data augmentation, privacy-preserving AI  
âœ… **Apply Reinforcement Learning:** Optimize test flows, equipment control, resource allocation  
âœ… **Deploy Optimized Models:** Quantization, pruning, edge deployment  
âœ… **Combine Modalities:** Vision + language for root cause analysis  
âœ… **Train Multi-Agent Systems:** Coordinate multiple equipment/robots  

---

## ðŸ“ˆ Architecture Comparison Table

| Architecture | Best For | Training | Inference | Interpretability |
|--------------|----------|----------|-----------|------------------|
| **CNN** | Images (spatial) | Medium | Fast | Low |
| **RNN/LSTM** | Short sequences | Slow | Medium | Low |
| **Transformer** | Long sequences | Fast (parallel) | Medium | Medium (attention) |
| **GAN** | Generation | Hard (unstable) | Fast | Very Low |
| **RL** | Control, optimization | Very Slow | Fast | Low |
| **ViT** | Images (transformers) | Slow (data-hungry) | Medium | Medium |

---

## ðŸ­ Post-Silicon Validation Applications

### 1. **Wafer Map Defect Classification (CNN/ViT)**
- **Input:** 2D wafer maps (die-level pass/fail patterns)
- **Model:** ResNet50 or ViT for pattern recognition
- **Output:** Defect category (scratch, cluster, edge, random)
- **Value:** Reduce manual inspection time 70-90%

### 2. **Test Time Prediction (LSTM/Transformer)**
- **Input:** Historical parametric test times (100+ tests per device)
- **Model:** LSTM or Transformer for sequence modeling
- **Output:** Predicted test time for capacity planning
- **Value:** Optimize parallel test execution, reduce costs 10-15%

### 3. **Synthetic Defect Generation (GAN)**
- **Input:** Real wafer maps (limited rare defect examples)
- **Model:** Conditional GAN for targeted defect synthesis
- **Output:** Synthetic defect patterns for training data augmentation
- **Value:** Improve classifier accuracy 5-10% on rare defects

### 4. **Adaptive Test Flow Optimization (Deep RL)**
- **Input:** Real-time test results, equipment state
- **Model:** DQN or PPO for sequential decision making
- **Output:** Optimal next test or early termination decision
- **Value:** Reduce test time 15-25% while maintaining quality

### 5. **Multimodal Root Cause Analysis (CLIP/Multimodal)**
- **Input:** Wafer maps + test logs + equipment sensor data
- **Model:** Multimodal Transformer combining vision + text
- **Output:** Likely root cause with confidence scores
- **Value:** Accelerate failure analysis 50-70%

---

## ðŸ”„ Next Steps

After mastering Deep Learning:

1. **08_Modern_AI:** Apply Transformers to LLMs, RAG systems, AI agents
2. **10_MLOps:** Deploy deep learning models with CI/CD, monitoring, A/B testing
3. **09_Data_Engineering:** Build data pipelines for training large models (petabyte-scale)
4. **13_MLOps_Production_ML:** Advanced production patterns (model serving, feature stores)

**Advanced Topics:**
- **Diffusion Models:** Stable Diffusion, DDPM for image generation
- **Multimodal LLMs:** GPT-4V, Gemini, LLaVA combining vision + language at scale
- **Neural Rendering:** NeRF, 3D reconstruction for equipment inspection

---

## ðŸ“ Project Ideas

### Post-Silicon Validation Projects

1. **Wafer Defect Detector (CNN + Transfer Learning)**
   - Fine-tune EfficientNet-B0 on wafer map images (10K samples)
   - Apply data augmentation (rotation, flip, noise injection)
   - Deploy as REST API (<50ms inference)
   - Target: 95% accuracy on test set

2. **LSTM Test Time Forecaster**
   - Model parametric test time sequences (100+ tests, 52 weeks history)
   - Compare LSTM vs Transformer performance
   - Implement attention visualization for interpretability
   - Target: MAPE <5% for 4-week ahead forecasts

3. **GAN-Based Defect Augmentation**
   - Train StyleGAN on wafer maps (generate rare defects)
   - Implement conditional GAN (control defect type)
   - Evaluate quality with FID score
   - Use synthetic data to improve classifier +5-10%

4. **Deep RL Test Optimization**
   - Formulate test flow as MDP (states: test results, actions: next test)
   - Train DQN or PPO to minimize test time while maintaining coverage
   - Simulate with historical test data (1M devices)
   - Target: 20% test time reduction

### General AI Projects

5. **Image Classification with ViT**
   - Train Vision Transformer on CIFAR-100 or ImageNet subset
   - Compare to ResNet50 (accuracy, training time, parameters)
   - Visualize attention maps for interpretability
   - Apply MAE pre-training for data efficiency

6. **Seq2Seq Chatbot with Transformers**
   - Build encoder-decoder Transformer for conversational AI
   - Train on Cornell Movie Dialogs or custom dataset
   - Implement beam search and sampling strategies
   - Deploy with FastAPI backend

7. **Stock Trading RL Agent**
   - Model stock trading as MDP (states: prices, actions: buy/sell/hold)
   - Train PPO or SAC for continuous action spaces
   - Backtest on historical data (S&P 500, 10 years)
   - Compare to buy-and-hold strategy

8. **Multimodal Image Captioning**
   - Fine-tune CLIP or BLIP for image captioning
   - Train on COCO dataset (100K images with captions)
   - Evaluate with BLEU, METEOR, CIDEr metrics
   - Build web demo with Gradio

---

**Total Notebooks in Section:** 27  
**Estimated Completion Time:** 54-80 hours  
**Difficulty Level:** Intermediate to Advanced  
**Prerequisites:** ML fundamentals, calculus, linear algebra, Python proficiency

*Last Updated: December 2025*
