{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e56709a",
   "metadata": {},
   "source": [
    "# 072: GPT & Large Language Models - The Generative Pre-training Revolution\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "\n",
    "1. **GPT Architecture**: Autoregressive language modeling, causal attention masks, decoder-only Transformers\n",
    "2. **Scaling Laws**: How model size (parameters), data size, and compute relate to performance\n",
    "3. **Pre-training & Fine-tuning**: Generative pre-training on massive text corpora, task-specific fine-tuning\n",
    "4. **Prompting Techniques**: Zero-shot, few-shot, chain-of-thought, prompt engineering\n",
    "5. **Production Deployment**: API-based inference, fine-tuning strategies, cost optimization\n",
    "6. **Business Applications**: 8 real-world LLM projects worth **$200M-$600M per year**\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What Are Large Language Models?\n",
    "\n",
    "### Definition\n",
    "\n",
    "**Large Language Model (LLM)**: A neural network trained on massive text corpora (100B-10T tokens) to predict the next word in a sequence, acquiring broad knowledge and reasoning capabilities\n",
    "\n",
    "**Key Characteristics**:\n",
    "- **Scale**: 1B-1.8T parameters (GPT-3 175B, GPT-4 1.8T estimated)\n",
    "- **Generative**: Produce coherent, contextually relevant text\n",
    "- **General-purpose**: Single model handles many tasks (Q&A, summarization, translation, code generation)\n",
    "- **Few-shot learning**: Learn new tasks from 1-10 examples (no fine-tuning)\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Evolution: From GPT to GPT-4\n",
    "\n",
    "### Timeline\n",
    "\n",
    "```mermaid\n",
    "timeline\n",
    "    title Evolution of Large Language Models\n",
    "    2017 : Transformer (Attention Is All You Need)\n",
    "         : 65M parameters\n",
    "    2018 : GPT-1 (Generative Pre-training)\n",
    "         : 117M parameters\n",
    "         : Pre-training + fine-tuning paradigm\n",
    "    2019 : GPT-2 (Language Models are Unsupervised Multitask Learners)\n",
    "         : 1.5B parameters\n",
    "         : Zero-shot task transfer\n",
    "    2020 : GPT-3 (Language Models are Few-Shot Learners)\n",
    "         : 175B parameters\n",
    "         : In-context learning\n",
    "         : Few-shot prompting\n",
    "    2022 : InstructGPT & ChatGPT\n",
    "         : RLHF (Reinforcement Learning from Human Feedback)\n",
    "         : Instruction following\n",
    "    2023 : GPT-4 (Multimodal)\n",
    "         : 1.8T parameters (estimated)\n",
    "         : Vision + text\n",
    "         : 98th percentile reasoning\n",
    "    2024 : GPT-4 Turbo, Claude 3, Gemini Ultra\n",
    "         : 128K-1M token context\n",
    "         : Tool use, agents\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Why GPT & LLMs Matter\n",
    "\n",
    "### The Paradigm Shift\n",
    "\n",
    "**Pre-GPT Era (2012-2017)**:\n",
    "- Task-specific models (one model per task)\n",
    "- Requires 10K-1M labeled examples\n",
    "- Training time: Days to weeks per task\n",
    "- Limited generalization\n",
    "\n",
    "**GPT Era (2018-present)**:\n",
    "- General-purpose models (one model for all tasks)\n",
    "- Requires 0-10 examples (few-shot learning)\n",
    "- No training (inference only)\n",
    "- Strong generalization across domains\n",
    "\n",
    "---\n",
    "\n",
    "### Business Impact\n",
    "\n",
    "**Total Business Value**: **$200M-$600M per year** across 3 major use cases\n",
    "\n",
    "#### **Use Case 1: Customer Service Automation ($80M-$200M/year)**\n",
    "\n",
    "**Problem**:\n",
    "- Large enterprise: 2,000 support agents √ó $50K salary = **$100M/year**\n",
    "- Complex inquiries require human judgment (can't template)\n",
    "- Multi-turn conversations (5-10 exchanges)\n",
    "- Knowledge across 1,000+ products\n",
    "\n",
    "**Solution**: GPT-4-powered conversational AI\n",
    "- **Automation rate**: 60% (vs 30-40% with BERT)\n",
    "- **Multi-turn**: Maintains context across conversation\n",
    "- **Knowledge**: Trained on all documentation (no manual KB)\n",
    "- **Personalization**: Adapts tone to customer sentiment\n",
    "\n",
    "**Value**:\n",
    "- **Cost savings**: $60M/year (1,200 agents √ó $50K)\n",
    "- **Customer satisfaction**: +15% (vs template responses)\n",
    "- **Response time**: <5 seconds (vs 1-2 hours)\n",
    "- **Scalability**: Handle 3√ó volume with same infrastructure\n",
    "\n",
    "**Implementation**: Fine-tuned GPT-3.5 on 100K support tickets + retrieval-augmented generation (RAG)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Use Case 2: Code Generation & Documentation ($60M-$200M/year)**\n",
    "\n",
    "**Problem**:\n",
    "- Software company: 500 engineers √ó $150K = **$75M/year**\n",
    "- 30% time on boilerplate code\n",
    "- 20% time writing documentation\n",
    "- 15% time debugging\n",
    "\n",
    "**Solution**: GPT-4 Codex integration (GitHub Copilot style)\n",
    "- **Code completion**: 40% faster coding (boilerplate auto-generated)\n",
    "- **Documentation**: Auto-generate docstrings, READMEs, API docs\n",
    "- **Bug detection**: Analyze code for common mistakes\n",
    "- **Code review**: Suggest improvements, security issues\n",
    "\n",
    "**Value**:\n",
    "- **Productivity gain**: 25% overall (500 engineers √ó 0.25 √ó $150K = **$18.75M/year**)\n",
    "- **Quality improvement**: 30% fewer bugs ($5M/year saved debugging)\n",
    "- **Onboarding**: 50% faster new hire ramp ($2M/year)\n",
    "- **Total**: **$25.75M/year** (single company), **$60M-$200M/year** (enterprise portfolio)\n",
    "\n",
    "**ROI**: 10,000%+ (cost: $100K/year for API access)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Use Case 3: Content Creation & Marketing ($60M-$200M/year)**\n",
    "\n",
    "**Problem**:\n",
    "- Marketing team: 100 content creators √ó $80K = **$8M/year**\n",
    "- Need 1,000+ blog posts, social media, emails per year\n",
    "- Personalization at scale (10M customers)\n",
    "- A/B testing requires 10√ó content variants\n",
    "\n",
    "**Solution**: GPT-4 content generation pipeline\n",
    "- **Blog posts**: Auto-generate 80% of content (human edits remaining 20%)\n",
    "- **Personalization**: Generate 10M unique email variants\n",
    "- **A/B testing**: Create 100 variants in seconds (vs days manually)\n",
    "- **SEO optimization**: Auto-optimize for keywords, readability\n",
    "\n",
    "**Value**:\n",
    "- **Cost savings**: $4M/year (50% reduction in content team)\n",
    "- **Revenue increase**: $50M/year (10% conversion lift from personalization)\n",
    "- **Speed**: 10√ó faster content production\n",
    "- **Total**: **$54M/year**\n",
    "\n",
    "**ROI**: 5,400%+ (cost: $1M/year for content pipeline)\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison: BERT vs GPT\n",
    "\n",
    "| Feature | BERT (Encoder) | GPT (Decoder) |\n",
    "|---------|----------------|---------------|\n",
    "| **Architecture** | Bidirectional encoder | Autoregressive decoder |\n",
    "| **Training** | Masked Language Model | Next token prediction |\n",
    "| **Best For** | Classification, NER, Q&A | Generation, completion |\n",
    "| **Context** | Full sentence (bidirectional) | Left-to-right (causal) |\n",
    "| **Few-shot** | ‚ùå No (requires fine-tuning) | ‚úÖ Yes (in-context learning) |\n",
    "| **Parameters** | 110M-340M | 125M-1.8T |\n",
    "| **Example Use** | Sentiment analysis | Story completion |\n",
    "\n",
    "**When to use BERT**:\n",
    "- Classification tasks (sentiment, spam, topic)\n",
    "- Named entity recognition\n",
    "- Question answering (extractive)\n",
    "- Small to medium datasets (1K-100K examples)\n",
    "\n",
    "**When to use GPT**:\n",
    "- Text generation (stories, emails, code)\n",
    "- Few-shot learning (0-10 examples)\n",
    "- Conversational AI (chatbots)\n",
    "- Creative writing, brainstorming\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Core Concepts\n",
    "\n",
    "### 1. Autoregressive Language Modeling\n",
    "\n",
    "**Goal**: Model probability distribution over sequences\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, \\ldots, x_n) = \\prod_{i=1}^{n} P(x_i | x_1, \\ldots, x_{i-1})\n",
    "$$\n",
    "\n",
    "**Intuition**: Predict each word based on all previous words\n",
    "\n",
    "**Example**:\n",
    "- Input: \"The cat sat on the\"\n",
    "- Model computes: $P(\\text{mat} | \\text{The cat sat on the})$\n",
    "- Next word: \"mat\" (highest probability)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Causal Attention (vs Bidirectional)\n",
    "\n",
    "**BERT (Bidirectional)**:\n",
    "- Token \"cat\" attends to: \"The\", \"cat\", \"sat\", \"on\", \"mat\" (all tokens)\n",
    "- Sees future context\n",
    "\n",
    "**GPT (Causal)**:\n",
    "- Token \"cat\" attends to: \"The\", \"cat\" (only past + current)\n",
    "- Cannot see future (prevents cheating during training)\n",
    "\n",
    "**Mask Matrix**:\n",
    "```\n",
    "      The  cat  sat  on  mat\n",
    "The   ‚úì    ‚úó    ‚úó    ‚úó   ‚úó\n",
    "cat   ‚úì    ‚úì    ‚úó    ‚úó   ‚úó\n",
    "sat   ‚úì    ‚úì    ‚úì    ‚úó   ‚úó\n",
    "on    ‚úì    ‚úì    ‚úì    ‚úì   ‚úó\n",
    "mat   ‚úì    ‚úì    ‚úì    ‚úì   ‚úì\n",
    "```\n",
    "\n",
    "**Implementation**: Lower triangular mask (set upper triangle to -‚àû before softmax)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Pre-training Objective\n",
    "\n",
    "**GPT Training Loss** (negative log-likelihood):\n",
    "\n",
    "$$\n",
    "L = -\\sum_{i=1}^{n} \\log P(x_i | x_1, \\ldots, x_{i-1})\n",
    "$$\n",
    "\n",
    "**Data**: Massive text corpora (BooksCorpus, CommonCrawl, WebText)\n",
    "- GPT-1: 5GB text (BooksCorpus)\n",
    "- GPT-2: 40GB text (WebText)\n",
    "- GPT-3: 570GB text (CommonCrawl, books, Wikipedia)\n",
    "\n",
    "**Compute**: \n",
    "- GPT-3: 355 GPU-years (3,640 petaflop-days)\n",
    "- Cost: $4-12M for single training run\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Few-Shot In-Context Learning\n",
    "\n",
    "**Zero-shot**: No examples, just task description\n",
    "```\n",
    "Translate English to French:\n",
    "Input: Hello\n",
    "Output: \n",
    "```\n",
    "\n",
    "**One-shot**: 1 example\n",
    "```\n",
    "Translate English to French:\n",
    "Input: Hello\n",
    "Output: Bonjour\n",
    "Input: Goodbye\n",
    "Output:\n",
    "```\n",
    "\n",
    "**Few-shot**: 5-10 examples\n",
    "```\n",
    "Translate English to French:\n",
    "Input: Hello ‚Üí Output: Bonjour\n",
    "Input: Goodbye ‚Üí Output: Au revoir\n",
    "Input: Thank you ‚Üí Output: Merci\n",
    "...\n",
    "Input: Good morning ‚Üí Output:\n",
    "```\n",
    "\n",
    "**How it works**: Model learns task pattern from examples in prompt (no weight updates)\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è GPT Architecture Overview\n",
    "\n",
    "### Decoder-Only Transformer\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Input Text] --> B[Tokenization]\n",
    "    B --> C[Token Embeddings]\n",
    "    B --> D[Position Embeddings]\n",
    "    C --> E[Sum]\n",
    "    D --> E\n",
    "    E --> F[Dropout]\n",
    "    F --> G[Transformer Block 1]\n",
    "    G --> H[Transformer Block 2]\n",
    "    H --> I[...]\n",
    "    I --> J[Transformer Block N]\n",
    "    J --> K[Layer Norm]\n",
    "    K --> L[Linear Layer]\n",
    "    L --> M[Softmax]\n",
    "    M --> N[Output Probabilities]\n",
    "    \n",
    "    style G fill:#e1f5ff\n",
    "    style H fill:#e1f5ff\n",
    "    style J fill:#e1f5ff\n",
    "    style A fill:#fff5e1\n",
    "    style N fill:#e1ffe1\n",
    "```\n",
    "\n",
    "### Transformer Block (GPT-style)\n",
    "\n",
    "```\n",
    "Input\n",
    "  ‚Üì\n",
    "Masked Multi-Head Attention (causal)\n",
    "  ‚Üì\n",
    "Add & LayerNorm\n",
    "  ‚Üì\n",
    "Feed-Forward Network\n",
    "  ‚Üì\n",
    "Add & LayerNorm\n",
    "  ‚Üì\n",
    "Output\n",
    "```\n",
    "\n",
    "**Differences from BERT**:\n",
    "- Causal attention mask (lower triangular)\n",
    "- No encoder-decoder structure (decoder only)\n",
    "- Pre-LayerNorm (vs Post-LayerNorm in original Transformer)\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Scaling Laws (Kaplan et al., 2020)\n",
    "\n",
    "### Key Finding: Power Law Relationship\n",
    "\n",
    "**Performance scales predictably** with:\n",
    "1. **Model size (N)**: Number of parameters\n",
    "2. **Dataset size (D)**: Number of tokens\n",
    "3. **Compute budget (C)**: FLOPs\n",
    "\n",
    "$$\n",
    "L(N) \\approx \\left(\\frac{N_c}{N}\\right)^{\\alpha_N}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $L$ = loss (lower is better)\n",
    "- $N$ = model parameters\n",
    "- $N_c$ = critical parameter count\n",
    "- $\\alpha_N \\approx 0.076$ (exponent)\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "**1. Bigger is better** (but diminishing returns):\n",
    "- 10√ó parameters ‚Üí 5-10% performance gain\n",
    "- 100√ó parameters ‚Üí 10-15% performance gain\n",
    "\n",
    "**2. Data matters** (but less than parameters):\n",
    "- 10√ó data ‚Üí 3-5% performance gain\n",
    "- 100√ó data ‚Üí 5-8% performance gain\n",
    "\n",
    "**3. Optimal allocation**:\n",
    "- Chinchilla (DeepMind, 2022): For compute budget C, optimal balance is:\n",
    "  - Parameters $N \\propto C^{0.5}$\n",
    "  - Tokens $D \\propto C^{0.5}$\n",
    "- **Implication**: GPT-3 was \"undertrained\" (should have used 2√ó more data, 0.5√ó fewer parameters)\n",
    "\n",
    "---\n",
    "\n",
    "## üéì GPT Model Comparison\n",
    "\n",
    "| Model | Parameters | Layers | d_model | Heads | Context Length | Training Cost | Year |\n",
    "|-------|------------|--------|---------|-------|----------------|---------------|------|\n",
    "| **GPT-1** | 117M | 12 | 768 | 12 | 512 | $50K | 2018 |\n",
    "| **GPT-2-small** | 117M | 12 | 768 | 12 | 1024 | $50K | 2019 |\n",
    "| **GPT-2-medium** | 345M | 24 | 1024 | 16 | 1024 | $200K | 2019 |\n",
    "| **GPT-2-large** | 762M | 36 | 1280 | 20 | 1024 | $500K | 2019 |\n",
    "| **GPT-2-XL** | 1.5B | 48 | 1600 | 25 | 1024 | $1M | 2019 |\n",
    "| **GPT-3-small** | 125M | 12 | 768 | 12 | 2048 | $100K | 2020 |\n",
    "| **GPT-3-medium** | 350M | 24 | 1024 | 16 | 2048 | $300K | 2020 |\n",
    "| **GPT-3-large** | 760M | 24 | 1536 | 16 | 2048 | $800K | 2020 |\n",
    "| **GPT-3-XL** | 1.3B | 24 | 2048 | 24 | 2048 | $1.5M | 2020 |\n",
    "| **GPT-3-2.7B** | 2.7B | 32 | 2560 | 32 | 2048 | $3M | 2020 |\n",
    "| **GPT-3-6.7B** | 6.7B | 32 | 4096 | 32 | 2048 | $7M | 2020 |\n",
    "| **GPT-3-13B** | 13B | 40 | 5140 | 40 | 2048 | $15M | 2020 |\n",
    "| **GPT-3-175B** | 175B | 96 | 12288 | 96 | 2048 | $12M | 2020 |\n",
    "| **GPT-4** | ~1.8T (est) | ~120 | ~18432 | ~128 | 32K | ~$100M | 2023 |\n",
    "\n",
    "**Note**: Training costs are estimates (actual costs vary with hardware, time)\n",
    "\n",
    "---\n",
    "\n",
    "## üî• Key Innovations\n",
    "\n",
    "### 1. GPT-1 (2018): Pre-training + Fine-tuning\n",
    "\n",
    "**Contribution**: Demonstrated transfer learning for NLP\n",
    "- Pre-train on unsupervised text (BooksCorpus)\n",
    "- Fine-tune on task-specific data (1K-10K examples)\n",
    "- Achieved SOTA on 9/12 NLU tasks\n",
    "\n",
    "**Impact**: Established pre-training paradigm (now standard)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. GPT-2 (2019): Zero-Shot Task Transfer\n",
    "\n",
    "**Contribution**: Showed language models can perform tasks without fine-tuning\n",
    "- Trained on 40GB WebText (broader than BooksCorpus)\n",
    "- 1.5B parameters (10√ó larger than GPT-1)\n",
    "- Zero-shot performance competitive with supervised models\n",
    "\n",
    "**Controversial**: OpenAI delayed release (concerns about misuse)\n",
    "\n",
    "**Impact**: Proved scale enables zero-shot learning\n",
    "\n",
    "---\n",
    "\n",
    "### 3. GPT-3 (2020): Few-Shot In-Context Learning\n",
    "\n",
    "**Contribution**: Emergent few-shot learning at scale\n",
    "- 175B parameters (100√ó larger than GPT-2)\n",
    "- Can learn new tasks from 1-10 examples in prompt\n",
    "- No gradient updates (inference only)\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Task: Translate English to French\n",
    "\n",
    "English: Hello\n",
    "French: Bonjour\n",
    "\n",
    "English: Goodbye\n",
    "French: Au revoir\n",
    "\n",
    "English: Thank you\n",
    "French:\n",
    "```\n",
    "Output: \"Merci\" (learned pattern from 2 examples)\n",
    "\n",
    "**Impact**: Enabled GPT-3 API business model (no fine-tuning needed)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. InstructGPT / ChatGPT (2022): RLHF\n",
    "\n",
    "**Contribution**: Align models with human preferences\n",
    "- **Supervised fine-tuning**: Train on high-quality human demonstrations\n",
    "- **Reward modeling**: Train reward model from human comparisons\n",
    "- **PPO optimization**: Optimize policy using reward model\n",
    "\n",
    "**RLHF Pipeline**:\n",
    "```\n",
    "1. Collect demonstrations: Humans write ideal responses\n",
    "2. Fine-tune GPT-3: Supervised learning on demonstrations\n",
    "3. Collect comparisons: Humans rank multiple outputs\n",
    "4. Train reward model: Predict which output humans prefer\n",
    "5. Optimize with RL: PPO to maximize reward\n",
    "```\n",
    "\n",
    "**Impact**: \n",
    "- Much better instruction following\n",
    "- Reduced harmful outputs\n",
    "- ChatGPT reached 100M users in 2 months (fastest ever)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. GPT-4 (2023): Multimodal Reasoning\n",
    "\n",
    "**Contribution**: Vision + text, improved reasoning\n",
    "- Accepts images as input (not just text)\n",
    "- Longer context (32K tokens, 128K with GPT-4 Turbo)\n",
    "- Better reasoning (98th percentile on LSAT, bar exam)\n",
    "\n",
    "**Capabilities**:\n",
    "- Describe images, charts, diagrams\n",
    "- Answer questions about visual content\n",
    "- Generate code from UI mockups\n",
    "- Medical image analysis\n",
    "\n",
    "**Impact**: Enables new applications (document understanding, visual Q&A)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ When to Use GPT vs BERT\n",
    "\n",
    "### Use GPT When:\n",
    "‚úÖ **Text generation** (stories, emails, code, reports)  \n",
    "‚úÖ **Few-shot learning** (0-10 examples, no fine-tuning)  \n",
    "‚úÖ **Conversational AI** (chatbots, assistants)  \n",
    "‚úÖ **Creative tasks** (brainstorming, writing)  \n",
    "‚úÖ **Code completion** (GitHub Copilot style)  \n",
    "‚úÖ **Long-form content** (articles, documentation)  \n",
    "‚úÖ **Multi-turn dialogue** (maintains context)\n",
    "\n",
    "### Use BERT When:\n",
    "‚úÖ **Classification** (sentiment, spam, topic)  \n",
    "‚úÖ **Named entity recognition** (extract entities)  \n",
    "‚úÖ **Question answering** (extractive, find answer in text)  \n",
    "‚úÖ **Sentence similarity** (semantic search)  \n",
    "‚úÖ **Token classification** (POS tagging)  \n",
    "‚úÖ **Small datasets** (1K-100K examples)  \n",
    "‚úÖ **Latency-critical** (<50ms, BERT is smaller/faster)\n",
    "\n",
    "---\n",
    "\n",
    "## üìç Learning Path Context\n",
    "\n",
    "**Previous Notebooks**:\n",
    "- **070**: Edge AI & TinyML (on-device inference, quantization)\n",
    "- **071**: Transformers & BERT (encoder architecture, self-attention)\n",
    "\n",
    "**Current Notebook**:\n",
    "- **072**: GPT & Large Language Models (decoder architecture, generation)\n",
    "\n",
    "**Next Notebooks**:\n",
    "- **073**: Vision Transformers (ViT, DINO, CLIP)\n",
    "- **074**: Multimodal Models (DALL-E, Stable Diffusion)\n",
    "- **075**: LLM Fine-tuning & Alignment (LoRA, RLHF, DPO)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì Key Questions This Notebook Answers\n",
    "\n",
    "1. ‚úÖ How does GPT differ from BERT architecturally?\n",
    "2. ‚úÖ What is autoregressive language modeling?\n",
    "3. ‚úÖ How does causal attention work?\n",
    "4. ‚úÖ What are scaling laws and why do they matter?\n",
    "5. ‚úÖ How does few-shot in-context learning work?\n",
    "6. ‚úÖ What is RLHF and how does it align models?\n",
    "7. ‚úÖ How to implement GPT from scratch?\n",
    "8. ‚úÖ How to fine-tune GPT for custom tasks?\n",
    "9. ‚úÖ How to prompt engineer for optimal results?\n",
    "10. ‚úÖ What are 8 production LLM applications worth $200M-$600M/year?\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives Checklist\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- [ ] Explain autoregressive language modeling and next-token prediction\n",
    "- [ ] Implement causal attention mask for left-to-right generation\n",
    "- [ ] Build GPT architecture from scratch (decoder-only Transformer)\n",
    "- [ ] Understand scaling laws and parameter-performance relationship\n",
    "- [ ] Apply zero-shot, one-shot, and few-shot prompting techniques\n",
    "- [ ] Fine-tune GPT on custom datasets (instruction tuning)\n",
    "- [ ] Use RLHF to align models with human preferences\n",
    "- [ ] Deploy GPT for production applications (API, optimization)\n",
    "- [ ] Quantify business value: $200M-$600M/year across 8 projects\n",
    "- [ ] Choose between GPT and BERT for different use cases\n",
    "\n",
    "---\n",
    "\n",
    "**Let's dive into the mathematical foundations!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad468e",
   "metadata": {},
   "source": [
    "# üìê Mathematical Foundations: Autoregressive Language Modeling\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "GPT models are built on three core mathematical concepts:\n",
    "\n",
    "1. **Autoregressive Modeling**: Sequential probability factorization\n",
    "2. **Causal Attention**: Masked self-attention for left-to-right generation\n",
    "3. **Next-Token Prediction**: Maximum likelihood training objective\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Autoregressive Language Modeling\n",
    "\n",
    "## Probability Factorization\n",
    "\n",
    "**Goal**: Model joint probability of sequence $x = (x_1, x_2, \\ldots, x_n)$\n",
    "\n",
    "### Chain Rule of Probability\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, \\ldots, x_n) = \\prod_{i=1}^{n} P(x_i | x_1, \\ldots, x_{i-1})\n",
    "$$\n",
    "\n",
    "**Intuition**: Break joint probability into sequential conditional probabilities\n",
    "\n",
    "---\n",
    "\n",
    "## Concrete Example\n",
    "\n",
    "**Sentence**: \"The cat sat on the mat\"\n",
    "\n",
    "**Probability Decomposition**:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(\\text{The, cat, sat, on, the, mat}) &= P(\\text{The}) \\\\\n",
    "&\\times P(\\text{cat} | \\text{The}) \\\\\n",
    "&\\times P(\\text{sat} | \\text{The, cat}) \\\\\n",
    "&\\times P(\\text{on} | \\text{The, cat, sat}) \\\\\n",
    "&\\times P(\\text{the} | \\text{The, cat, sat, on}) \\\\\n",
    "&\\times P(\\text{mat} | \\text{The, cat, sat, on, the})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**GPT's Job**: Learn each conditional probability $P(x_i | x_{<i})$\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "**Model Output**: Probability distribution over vocabulary V\n",
    "\n",
    "$$\n",
    "P(x_i = w | x_{<i}) = \\frac{\\exp(e_w^T h_i)}{\\sum_{w' \\in V} \\exp(e_{w'}^T h_i)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $h_i$ = hidden state at position $i$ (output of Transformer)\n",
    "- $e_w$ = embedding vector for word $w$\n",
    "- $V$ = vocabulary (50K-100K words)\n",
    "\n",
    "**This is softmax over vocabulary**:\n",
    "$$\n",
    "P(x_i | x_{<i}) = \\text{softmax}(W h_i)\n",
    "$$\n",
    "\n",
    "Where $W \\in \\mathbb{R}^{|V| \\times d}$ is embedding matrix (transposed)\n",
    "\n",
    "---\n",
    "\n",
    "## Training Objective\n",
    "\n",
    "**Negative Log-Likelihood (NLL) Loss**:\n",
    "\n",
    "$$\n",
    "L(\\theta) = -\\frac{1}{N} \\sum_{j=1}^{N} \\sum_{i=1}^{n_j} \\log P(x_i^{(j)} | x_{<i}^{(j)}; \\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ = number of sequences in training set\n",
    "- $n_j$ = length of sequence $j$\n",
    "- $\\theta$ = model parameters (weights)\n",
    "- $x_{<i}^{(j)}$ = all tokens before position $i$ in sequence $j$\n",
    "\n",
    "**Intuition**: Maximize probability of correct next token at every position\n",
    "\n",
    "---\n",
    "\n",
    "## Example Calculation\n",
    "\n",
    "**Input**: \"The cat sat\"  \n",
    "**Target**: \"on\"  \n",
    "**Vocabulary**: {the:0, cat:1, sat:2, on:3, mat:4, ...} (size 50,000)\n",
    "\n",
    "**Forward Pass**:\n",
    "1. Encode \"The cat sat\" through Transformer ‚Üí $h_3$ (hidden state)\n",
    "2. Project to vocabulary: $\\text{logits} = W h_3 \\in \\mathbb{R}^{50000}$\n",
    "3. Softmax: $P(\\cdot | \\text{The cat sat}) = \\text{softmax}(\\text{logits})$\n",
    "\n",
    "**Example Output**:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(\\text{on} | \\text{The cat sat}) &= 0.35 \\\\\n",
    "P(\\text{under} | \\text{The cat sat}) &= 0.25 \\\\\n",
    "P(\\text{near} | \\text{The cat sat}) &= 0.15 \\\\\n",
    "P(\\text{the} | \\text{The cat sat}) &= 0.10 \\\\\n",
    "P(\\text{other words}) &= 0.15\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Loss**: $L = -\\log(0.35) = 1.05$ (lower is better)\n",
    "\n",
    "**If model predicted $P(\\text{on}) = 0.90$**:  \n",
    "$L = -\\log(0.90) = 0.10$ (much better)\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Causal Attention Mechanism\n",
    "\n",
    "## Why \"Causal\"?\n",
    "\n",
    "**Problem**: In autoregressive generation, token at position $i$ should only depend on positions $< i$ (not future tokens)\n",
    "\n",
    "**Without causal mask**: Model would \"cheat\" by looking at future tokens during training\n",
    "\n",
    "---\n",
    "\n",
    "## Attention Mask Matrix\n",
    "\n",
    "**BERT (Bidirectional) - No Mask**:\n",
    "```\n",
    "      The  cat  sat  on  mat\n",
    "The   1    1    1    1   1     (attends to all)\n",
    "cat   1    1    1    1   1     (attends to all)\n",
    "sat   1    1    1    1   1     (attends to all)\n",
    "on    1    1    1    1   1     (attends to all)\n",
    "mat   1    1    1    1   1     (attends to all)\n",
    "```\n",
    "\n",
    "**GPT (Causal) - Lower Triangular Mask**:\n",
    "```\n",
    "      The  cat  sat  on  mat\n",
    "The   1    0    0    0   0     (only self)\n",
    "cat   1    1    0    0   0     (self + past)\n",
    "sat   1    1    1    0   0     (self + past)\n",
    "on    1    1    1    1   0     (self + past)\n",
    "mat   1    1    1    1   1     (self + past)\n",
    "```\n",
    "\n",
    "**Implementation**: Set upper triangle to $-\\infty$ before softmax\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "**Standard Attention** (BERT):\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "**Causal Attention** (GPT):\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V\n",
    "$$\n",
    "\n",
    "Where **mask matrix** $M$:\n",
    "$$\n",
    "M_{ij} = \\begin{cases}\n",
    "0 & \\text{if } i \\geq j \\text{ (can attend)} \\\\\n",
    "-\\infty & \\text{if } i < j \\text{ (cannot attend)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Effect of $-\\infty$**:\n",
    "$$\n",
    "\\text{softmax}(-\\infty) = \\frac{e^{-\\infty}}{Z} = 0\n",
    "$$\n",
    "\n",
    "So future tokens get **zero attention weight**\n",
    "\n",
    "---\n",
    "\n",
    "## Example Calculation\n",
    "\n",
    "**Sentence**: \"The cat sat\"  \n",
    "**Computing attention for \"cat\" (position 1)**\n",
    "\n",
    "**Step 1: Compute similarity scores**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{score}(\\text{cat}, \\text{The}) &= q_{\\text{cat}} \\cdot k_{\\text{The}} / \\sqrt{d_k} = 2.5 \\\\\n",
    "\\text{score}(\\text{cat}, \\text{cat}) &= q_{\\text{cat}} \\cdot k_{\\text{cat}} / \\sqrt{d_k} = 3.8 \\\\\n",
    "\\text{score}(\\text{cat}, \\text{sat}) &= q_{\\text{cat}} \\cdot k_{\\text{sat}} / \\sqrt{d_k} = 1.2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Step 2: Apply causal mask**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{masked\\_score}(\\text{cat}, \\text{The}) &= 2.5 + 0 = 2.5 \\\\\n",
    "\\text{masked\\_score}(\\text{cat}, \\text{cat}) &= 3.8 + 0 = 3.8 \\\\\n",
    "\\text{masked\\_score}(\\text{cat}, \\text{sat}) &= 1.2 + (-\\infty) = -\\infty\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Step 3: Softmax**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(\\text{cat} \\rightarrow \\text{The}) &= \\frac{e^{2.5}}{e^{2.5} + e^{3.8} + e^{-\\infty}} = \\frac{e^{2.5}}{e^{2.5} + e^{3.8}} = 0.27 \\\\\n",
    "P(\\text{cat} \\rightarrow \\text{cat}) &= \\frac{e^{3.8}}{e^{2.5} + e^{3.8}} = 0.73 \\\\\n",
    "P(\\text{cat} \\rightarrow \\text{sat}) &= 0.00 \\quad \\text{(masked)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**Interpretation**: Token \"cat\" attends 73% to itself, 27% to \"The\", 0% to \"sat\" (future)\n",
    "\n",
    "---\n",
    "\n",
    "## Code Implementation (PyTorch)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def causal_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Causal (masked) self-attention\n",
    "    \n",
    "    Args:\n",
    "        Q, K, V: (batch, seq_len, d_k)\n",
    "        \n",
    "    Returns:\n",
    "        output: (batch, seq_len, d_k)\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    seq_len = Q.size(1)\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    # Shape: (batch, seq_len, seq_len)\n",
    "    \n",
    "    # Create causal mask (lower triangular)\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Apply mask\n",
    "    scores = scores + mask.unsqueeze(0)  # Broadcast mask\n",
    "    \n",
    "    # Softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Weighted sum\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ GPT vs BERT: Architectural Differences\n",
    "\n",
    "## Comparison Table\n",
    "\n",
    "| Aspect | BERT | GPT |\n",
    "|--------|------|-----|\n",
    "| **Architecture** | Encoder only | Decoder only |\n",
    "| **Attention** | Bidirectional (full) | Causal (masked) |\n",
    "| **Training** | Masked LM + NSP | Next token prediction |\n",
    "| **Context** | Both directions | Left-to-right only |\n",
    "| **Use Case** | Understanding | Generation |\n",
    "| **Input** | [CLS] tokens [SEP] | tokens |\n",
    "| **Output** | Token representations | Next token probabilities |\n",
    "| **Fine-tuning** | Required for tasks | Optional (few-shot) |\n",
    "\n",
    "---\n",
    "\n",
    "## Attention Visualization\n",
    "\n",
    "**BERT Attention** (token \"cat\"):\n",
    "```\n",
    "The ‚Üê‚Üí cat ‚Üê‚Üí sat ‚Üê‚Üí on ‚Üê‚Üí mat\n",
    "     ‚Üë     ‚Üë     ‚Üë     ‚Üë     ‚Üë\n",
    "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        All connections\n",
    "```\n",
    "\n",
    "**GPT Attention** (token \"cat\"):\n",
    "```\n",
    "The ‚Üí cat ‚Üí sat ‚Üí on ‚Üí mat\n",
    "      ‚Üë\n",
    "      ‚îî‚îÄ‚îÄ‚îÄ Only backward\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Positional Encoding (Same as BERT)\n",
    "\n",
    "**Why Needed**: Attention is permutation-invariant (order doesn't matter without position info)\n",
    "\n",
    "**Solution**: Add positional embeddings\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "PE_{(pos, 2i)} &= \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right) \\\\\n",
    "PE_{(pos, 2i+1)} &= \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**GPT Variation**: Learned positional embeddings (vs sinusoidal in original Transformer)\n",
    "\n",
    "$$\n",
    "\\text{Input}_i = \\text{TokenEmbedding}(x_i) + \\text{PositionEmbedding}(i)\n",
    "$$\n",
    "\n",
    "Where $\\text{PositionEmbedding}$ is a learned lookup table:\n",
    "- GPT-2: 1024 position embeddings (max context length)\n",
    "- GPT-3: 2048 position embeddings\n",
    "- GPT-4: 32,768 position embeddings (32K context)\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Complete GPT Forward Pass\n",
    "\n",
    "## Step-by-Step Computation\n",
    "\n",
    "**Input**: \"The cat sat on the\"  \n",
    "**Goal**: Predict next token\n",
    "\n",
    "### Step 1: Tokenization\n",
    "\n",
    "```\n",
    "\"The cat sat on the\" ‚Üí [464, 3797, 3332, 319, 262]\n",
    "```\n",
    "\n",
    "(Using GPT-2 tokenizer)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Embedding Lookup\n",
    "\n",
    "**Token Embeddings**:\n",
    "$$\n",
    "E_{\\text{token}} = \\text{Embedding}(\\text{input\\_ids}) \\in \\mathbb{R}^{5 \\times 768}\n",
    "$$\n",
    "\n",
    "**Position Embeddings**:\n",
    "$$\n",
    "E_{\\text{pos}} = \\text{Embedding}([0, 1, 2, 3, 4]) \\in \\mathbb{R}^{5 \\times 768}\n",
    "$$\n",
    "\n",
    "**Combined**:\n",
    "$$\n",
    "H^{(0)} = E_{\\text{token}} + E_{\\text{pos}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Transformer Layers\n",
    "\n",
    "**For each layer** $\\ell = 1, \\ldots, L$ (L=12 for GPT-2-small):\n",
    "\n",
    "**a. Masked Multi-Head Attention**:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^{(\\ell)} &= H^{(\\ell-1)} W_Q^{(\\ell)} \\\\\n",
    "K^{(\\ell)} &= H^{(\\ell-1)} W_K^{(\\ell)} \\\\\n",
    "V^{(\\ell)} &= H^{(\\ell-1)} W_V^{(\\ell)} \\\\\n",
    "\\text{Attn}^{(\\ell)} &= \\text{CausalAttention}(Q^{(\\ell)}, K^{(\\ell)}, V^{(\\ell)})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**b. Residual + LayerNorm**:\n",
    "$$\n",
    "H'^{(\\ell)} = \\text{LayerNorm}(H^{(\\ell-1)} + \\text{Attn}^{(\\ell)})\n",
    "$$\n",
    "\n",
    "**c. Feed-Forward**:\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{GELU}(xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "(GELU activation instead of ReLU)\n",
    "\n",
    "**d. Residual + LayerNorm**:\n",
    "$$\n",
    "H^{(\\ell)} = \\text{LayerNorm}(H'^{(\\ell)} + \\text{FFN}(H'^{(\\ell)}))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Output Projection\n",
    "\n",
    "**After L layers**, get final hidden states $H^{(L)} \\in \\mathbb{R}^{5 \\times 768}$\n",
    "\n",
    "**For last token** (position 4, corresponding to \"the\"):\n",
    "$$\n",
    "\\text{logits} = H^{(L)}_{4} W_{\\text{vocab}} \\in \\mathbb{R}^{50257}\n",
    "$$\n",
    "\n",
    "Where $W_{\\text{vocab}} \\in \\mathbb{R}^{768 \\times 50257}$ (vocabulary size)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Softmax & Sampling\n",
    "\n",
    "**Probability Distribution**:\n",
    "$$\n",
    "P(x_5 | x_{<5}) = \\text{softmax}(\\text{logits})\n",
    "$$\n",
    "\n",
    "**Top-5 Predictions** (example):\n",
    "```\n",
    "P(mat | The cat sat on the) = 0.42\n",
    "P(floor | The cat sat on the) = 0.18\n",
    "P(couch | The cat sat on the) = 0.12\n",
    "P(table | The cat sat on the) = 0.08\n",
    "P(roof | The cat sat on the) = 0.05\n",
    "```\n",
    "\n",
    "**Sampling**: Select next token (greedy, top-k, nucleus sampling)\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Generation Strategies\n",
    "\n",
    "## 1. Greedy Decoding\n",
    "\n",
    "**Rule**: Always pick highest probability token\n",
    "\n",
    "$$\n",
    "x_i = \\arg\\max_{w \\in V} P(w | x_{<i})\n",
    "$$\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Input: \"The cat\"\n",
    "Step 1: P(sat|The cat) = 0.6 ‚Üí Select \"sat\"\n",
    "Step 2: P(on|The cat sat) = 0.5 ‚Üí Select \"on\"\n",
    "Step 3: P(the|The cat sat on) = 0.7 ‚Üí Select \"the\"\n",
    "...\n",
    "Output: \"The cat sat on the mat.\"\n",
    "```\n",
    "\n",
    "**Pros**: Deterministic, fast  \n",
    "**Cons**: Repetitive, boring output\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Top-K Sampling\n",
    "\n",
    "**Rule**: Sample from top K most likely tokens\n",
    "\n",
    "$$\n",
    "x_i \\sim P(x_i | x_{<i}) \\quad \\text{restricted to top-K tokens}\n",
    "$$\n",
    "\n",
    "**Example** (K=5):\n",
    "```\n",
    "Input: \"The cat\"\n",
    "Probabilities: {sat: 0.3, was: 0.2, is: 0.15, ran: 0.1, jumped: 0.08, ...}\n",
    "Top-5: {sat, was, is, ran, jumped}\n",
    "Renormalize: {sat: 0.36, was: 0.24, is: 0.18, ran: 0.12, jumped: 0.10}\n",
    "Sample: \"was\" (with 24% probability)\n",
    "```\n",
    "\n",
    "**Pros**: More diverse output  \n",
    "**Cons**: Fixed K may be too restrictive or too loose\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Nucleus (Top-P) Sampling\n",
    "\n",
    "**Rule**: Sample from smallest set of tokens whose cumulative probability ‚â• P\n",
    "\n",
    "$$\n",
    "x_i \\sim P(x_i | x_{<i}) \\quad \\text{restricted to nucleus set } V_P\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "V_P = \\left\\{ w : \\sum_{w' \\in V_P} P(w' | x_{<i}) \\geq P \\right\\}\n",
    "$$\n",
    "\n",
    "**Example** (P=0.9):\n",
    "```\n",
    "Probabilities (sorted): {sat: 0.3, was: 0.25, is: 0.2, ran: 0.15, jumped: 0.05, ...}\n",
    "Cumulative: {sat: 0.3, was: 0.55, is: 0.75, ran: 0.9, ...}\n",
    "Nucleus (‚â•0.9): {sat, was, is, ran}\n",
    "Sample from these 4 tokens\n",
    "```\n",
    "\n",
    "**Pros**: Adaptive (nucleus size varies)  \n",
    "**Cons**: Requires tuning P (typically 0.9-0.95)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Temperature Sampling\n",
    "\n",
    "**Modify distribution** with temperature $T$:\n",
    "\n",
    "$$\n",
    "P_T(x_i | x_{<i}) = \\frac{\\exp(\\text{logit}_i / T)}{\\sum_j \\exp(\\text{logit}_j / T)}\n",
    "$$\n",
    "\n",
    "**Effect**:\n",
    "- $T = 1$: Original distribution\n",
    "- $T < 1$ (e.g., 0.7): Sharper (more confident, less diverse)\n",
    "- $T > 1$ (e.g., 1.2): Flatter (more random, more diverse)\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Original (T=1): {sat: 0.5, was: 0.3, is: 0.2}\n",
    "Cold (T=0.5):   {sat: 0.7, was: 0.2, is: 0.1}  (more confident)\n",
    "Hot (T=2.0):    {sat: 0.38, was: 0.33, is: 0.29} (more random)\n",
    "```\n",
    "\n",
    "**Best Practice**: Combine temperature + top-p sampling\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ Scaling Laws (Kaplan et al., 2020)\n",
    "\n",
    "## Power Law for Loss\n",
    "\n",
    "**Empirical Finding**: Model performance follows predictable power laws\n",
    "\n",
    "$$\n",
    "L(N) = \\left(\\frac{N_c}{N}\\right)^{\\alpha_N}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $L$ = cross-entropy loss\n",
    "- $N$ = number of parameters\n",
    "- $N_c \\approx 8.8 \\times 10^{13}$ (critical parameter count)\n",
    "- $\\alpha_N \\approx 0.076$ (scaling exponent)\n",
    "\n",
    "---\n",
    "\n",
    "## Data Scaling\n",
    "\n",
    "$$\n",
    "L(D) = \\left(\\frac{D_c}{D}\\right)^{\\alpha_D}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $D$ = number of training tokens\n",
    "- $D_c \\approx 5.4 \\times 10^{13}$ (critical token count)\n",
    "- $\\alpha_D \\approx 0.095$\n",
    "\n",
    "---\n",
    "\n",
    "## Compute Scaling\n",
    "\n",
    "$$\n",
    "L(C) = \\left(\\frac{C_c}{C}\\right)^{\\alpha_C}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $C$ = compute budget (petaflop-days)\n",
    "- $C_c \\approx 3.1 \\times 10^{8}$\n",
    "- $\\alpha_C \\approx 0.050$\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Implications\n",
    "\n",
    "### 1. Diminishing Returns\n",
    "\n",
    "**10√ó more parameters** ‚Üí ~5-10% better performance\n",
    "\n",
    "**Example**:\n",
    "- GPT-2 (1.5B): Loss = 3.0\n",
    "- GPT-3 (175B): Loss = 2.2 (100√ó larger, 27% better)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Optimal Allocation (Chinchilla)\n",
    "\n",
    "**Finding**: For compute budget $C$, optimal balance is:\n",
    "\n",
    "$$\n",
    "N \\propto C^{0.5}, \\quad D \\propto C^{0.5}\n",
    "$$\n",
    "\n",
    "**Implication**: GPT-3 was \"over-parameterized\"\n",
    "- Used 175B parameters, 300B tokens\n",
    "- Better: 70B parameters, 1.4T tokens (same compute)\n",
    "\n",
    "**Chinchilla** (70B params, 1.4T tokens) outperformed GPT-3 (175B, 300B)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. When to Scale What?\n",
    "\n",
    "**If compute-limited**: Balance N and D equally  \n",
    "**If inference-limited**: Favor smaller N (faster inference)  \n",
    "**If data-limited**: Favor larger N (memorize more from less data)\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Key Formulas Summary\n",
    "\n",
    "## 1. Autoregressive Factorization\n",
    "\n",
    "$$\n",
    "P(x_1, \\ldots, x_n) = \\prod_{i=1}^{n} P(x_i | x_1, \\ldots, x_{i-1})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Training Loss (Negative Log-Likelihood)\n",
    "\n",
    "$$\n",
    "L(\\theta) = -\\frac{1}{N} \\sum_{j=1}^{N} \\sum_{i=1}^{n_j} \\log P(x_i^{(j)} | x_{<i}^{(j)}; \\theta)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Causal Attention\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "M_{ij} = \\begin{cases}\n",
    "0 & \\text{if } i \\geq j \\\\\n",
    "-\\infty & \\text{if } i < j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Next-Token Prediction\n",
    "\n",
    "$$\n",
    "P(x_i | x_{<i}) = \\text{softmax}(W h_i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Temperature Sampling\n",
    "\n",
    "$$\n",
    "P_T(x_i | x_{<i}) = \\frac{\\exp(\\text{logit}_i / T)}{\\sum_j \\exp(\\text{logit}_j / T)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Scaling Law\n",
    "\n",
    "$$\n",
    "L(N) = \\left(\\frac{N_c}{N}\\right)^{\\alpha_N}, \\quad \\alpha_N \\approx 0.076\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# üìä Complexity Analysis\n",
    "\n",
    "## Training Complexity\n",
    "\n",
    "**Forward Pass** (single token):\n",
    "- Attention: $O(n^2 d)$ where $n$ = context length, $d$ = model dimension\n",
    "- Feed-forward: $O(nd^2)$\n",
    "- Total per layer: $O(n^2 d + nd^2)$\n",
    "\n",
    "**Full Model** (L layers):\n",
    "$$\n",
    "O(L(n^2 d + nd^2))\n",
    "$$\n",
    "\n",
    "**Example** (GPT-3: L=96, n=2048, d=12288):\n",
    "$$\n",
    "96 \\times (2048^2 \\times 12288 + 2048 \\times 12288^2) \\approx 3.1 \\times 10^{14} \\text{ FLOPs}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Inference Complexity\n",
    "\n",
    "**Autoregressive Generation** (generate N tokens):\n",
    "- Must recompute attention for all previous tokens at each step\n",
    "- Token 1: $O(1)$ (no previous tokens)\n",
    "- Token 2: $O(2)$\n",
    "- Token N: $O(N)$\n",
    "- **Total**: $O(N^2)$ tokens generated\n",
    "\n",
    "**Optimization**: KV-cache (store key/value projections from previous tokens)\n",
    "- Reduces to $O(N)$ per token (only compute attention for new token)\n",
    "\n",
    "---\n",
    "\n",
    "# üéì Takeaways\n",
    "\n",
    "1. **Autoregressive modeling** breaks sequence probability into product of conditional probabilities\n",
    "2. **Causal attention** prevents future tokens from being seen (essential for generation)\n",
    "3. **Next-token prediction** is simple but powerful training objective\n",
    "4. **Scaling laws** show predictable performance improvements with scale\n",
    "5. **Generation strategies** (greedy, top-k, nucleus) control diversity vs quality trade-off\n",
    "6. **GPT differs from BERT** in architecture (decoder vs encoder) and use case (generation vs understanding)\n",
    "\n",
    "**Next**: Implementation from scratch + production fine-tuning strategies!\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Mathematical foundations complete!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff34c6c",
   "metadata": {},
   "source": [
    "### üìù Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815c95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PART 1: GPT ARCHITECTURE FROM SCRATCH (PyTorch)\n",
    "# ===================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Device:\", \"CUDA\" if torch.cuda.is_available() else \"CPU\")\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Configuration\n",
    "# -------------------------------------------------------------------\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"GPT model configuration\"\"\"\n",
    "    vocab_size: int = 50257  # GPT-2 vocabulary size\n",
    "    block_size: int = 1024   # Maximum context length\n",
    "    n_layer: int = 12        # Number of transformer blocks\n",
    "    n_head: int = 12         # Number of attention heads\n",
    "    n_embd: int = 768        # Embedding dimension\n",
    "    dropout: float = 0.1     # Dropout probability\n",
    "    bias: bool = True        # Use bias in Linear and LayerNorm\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPT-2 SMALL CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "config = GPTConfig()\n",
    "print(f\"Vocabulary size: {config.vocab_size:,}\")\n",
    "print(f\"Max context length: {config.block_size:,}\")\n",
    "print(f\"Transformer layers: {config.n_layer}\")\n",
    "print(f\"Attention heads: {config.n_head}\")\n",
    "print(f\"Embedding dimension: {config.n_embd}\")\n",
    "print(f\"Head dimension: {config.n_embd // config.n_head}\")\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Causal Self-Attention\n",
    "# -------------------------------------------------------------------\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal self-attention with masked future tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        # Key, query, value projections (combined for efficiency)\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # Output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        \n",
    "        # Regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        # Causal mask (lower triangular) - registered as buffer (not parameter)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                      .view(1, 1, config.block_size, config.block_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, n_embd)\n",
    "            \n",
    "        Returns:\n",
    "            output: (batch_size, seq_len, n_embd)\n",
    "        \"\"\"\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality\n",
    "        \n",
    "        # Calculate query, key, values for all heads in batch\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        # (B, T, C) -> (B, T, n_head, C/n_head) -> (B, n_head, T, C/n_head)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        \n",
    "        # Causal self-attention: (B, n_head, T, head_size) x (B, n_head, head_size, T) -> (B, n_head, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        \n",
    "        # Apply causal mask (set upper triangle to -inf)\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        y = att @ v  # (B, n_head, T, T) x (B, n_head, T, head_size) -> (B, n_head, T, head_size)\n",
    "        \n",
    "        # Re-assemble all head outputs side by side\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        \n",
    "        return y\n",
    "# Test Causal Self-Attention\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING CAUSAL SELF-ATTENTION\")\n",
    "print(\"=\"*60)\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "n_embd = 768\n",
    "x = torch.randn(batch_size, seq_len, n_embd)\n",
    "causal_attn = CausalSelfAttention(config)\n",
    "output = causal_attn(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in causal_attn.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c406bf95",
   "metadata": {},
   "source": [
    "### üìù Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 3. MLP (Feed-Forward Network)\n",
    "# -------------------------------------------------------------------\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron (feed-forward network)\n",
    "    GPT-2 uses GELU activation (vs ReLU in original Transformer)\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Transformer Block\n",
    "# -------------------------------------------------------------------\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block: attention + MLP with residual connections and layer norm\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pre-LayerNorm architecture (different from original Transformer)\n",
    "        x = x + self.attn(self.ln_1(x))  # Attention with residual\n",
    "        x = x + self.mlp(self.ln_2(x))   # MLP with residual\n",
    "        return x\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Complete GPT Model\n",
    "# -------------------------------------------------------------------\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    GPT Language Model\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),  # Token embeddings\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),  # Position embeddings\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),  # Transformer blocks\n",
    "            ln_f = nn.LayerNorm(config.n_embd, bias=config.bias),  # Final layer norm\n",
    "        ))\n",
    "        \n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying (share weights between token embeddings and output projection)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # Apply special scaled init to residual projections\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: Input token indices (batch_size, seq_len)\n",
    "            targets: Target token indices (batch_size, seq_len) - optional\n",
    "            \n",
    "        Returns:\n",
    "            logits: (batch_size, seq_len, vocab_size)\n",
    "            loss: Scalar (if targets provided)\n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is {self.config.block_size}\"\n",
    "        \n",
    "        # Token embeddings + position embeddings\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)  # (1, t)\n",
    "        tok_emb = self.transformer.wte(idx)  # (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos)  # (1, t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Final layer norm\n",
    "        x = self.transformer.ln_f(x)\n",
    "        \n",
    "        # Language model head (project to vocabulary)\n",
    "        logits = self.lm_head(x)  # (b, t, vocab_size)\n",
    "        \n",
    "        # Calculate loss if targets provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate text autoregressively\n",
    "        \n",
    "        Args:\n",
    "            idx: (batch_size, seq_len) initial context\n",
    "            max_new_tokens: Number of tokens to generate\n",
    "            temperature: Sampling temperature (higher = more random)\n",
    "            top_k: If set, only sample from top-k most likely tokens\n",
    "            \n",
    "        Returns:\n",
    "            idx: (batch_size, seq_len + max_new_tokens)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context if needed\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, _ = self(idx_cond)\n",
    "            \n",
    "            # Get logits for last token\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Optionally crop to top-k\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "            # Apply softmax\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Sample\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        return idx\n",
    "# Test Complete GPT Model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING COMPLETE GPT MODEL\")\n",
    "print(\"=\"*60)\n",
    "# Create model\n",
    "model = GPT(config)\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / (1024**2):.2f} MB (float32)\")\n",
    "# Test forward pass\n",
    "batch_size = 4\n",
    "seq_len = 32\n",
    "input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
    "target_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
    "logits, loss = model(input_ids, target_ids)\n",
    "print(f\"\\nInput shape: {input_ids.shape}\")\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "# Test generation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING TEXT GENERATION\")\n",
    "print(\"=\"*60)\n",
    "context = torch.randint(0, config.vocab_size, (1, 10))\n",
    "print(f\"Initial context: {context.shape}\")\n",
    "generated = model.generate(context, max_new_tokens=20, temperature=0.8, top_k=40)\n",
    "print(f\"Generated sequence: {generated.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb47f6b7",
   "metadata": {},
   "source": [
    "### üìù Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b1c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PART 2: GPT-2 FINE-TUNING WITH HUGGING FACE\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PART 2: GPT-2 FINE-TUNING (HUGGING FACE)\")\n",
    "print(\"=\"*60)\n",
    "try:\n",
    "    from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "    from transformers import Trainer, TrainingArguments\n",
    "    from torch.utils.data import Dataset\n",
    "    \n",
    "    print(\"‚úì Transformers library available\")\n",
    "    HF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Transformers library not installed\")\n",
    "    print(\"Install with: pip install transformers\")\n",
    "    HF_AVAILABLE = False\n",
    "if HF_AVAILABLE:\n",
    "    # -------------------------------------------------------------------\n",
    "    # 6. Load Pre-trained GPT-2\n",
    "    # -------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LOADING PRE-TRAINED GPT-2\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model_name = \"gpt2\"  # 117M parameters\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model_hf = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Set padding token (GPT-2 doesn't have one by default)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"‚úì Loaded {model_name}\")\n",
    "    print(f\"Vocabulary size: {len(tokenizer):,}\")\n",
    "    print(f\"Parameters: {sum(p.numel() for p in model_hf.parameters()):,}\")\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # 7. Text Generation Examples\n",
    "    # -------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEXT GENERATION EXAMPLES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    def generate_text(prompt, max_length=50, temperature=0.7, top_k=50, top_p=0.9):\n",
    "        \"\"\"Generate text from prompt\"\"\"\n",
    "        inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        \n",
    "        outputs = model_hf.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    # Example 1: Story continuation\n",
    "    prompt1 = \"Once upon a time, in a distant galaxy\"\n",
    "    print(f\"\\nPrompt: {prompt1}\")\n",
    "    print(f\"Generated: {generate_text(prompt1, max_length=80)}\")\n",
    "    \n",
    "    # Example 2: Code completion\n",
    "    prompt2 = \"def calculate_fibonacci(n):\"\n",
    "    print(f\"\\nPrompt: {prompt2}\")\n",
    "    print(f\"Generated: {generate_text(prompt2, max_length=100, temperature=0.3)}\")\n",
    "    \n",
    "    # Example 3: Email writing\n",
    "    prompt3 = \"Subject: Meeting Request\\n\\nDear Team,\\n\\nI would like to schedule\"\n",
    "    print(f\"\\nPrompt: {prompt3}\")\n",
    "    print(f\"Generated: {generate_text(prompt3, max_length=120)}\")\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # 8. Few-Shot In-Context Learning\n",
    "    # -------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEW-SHOT IN-CONTEXT LEARNING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Example: Sentiment classification (3-shot)\n",
    "    few_shot_prompt = \"\"\"\n",
    "Classify the sentiment of the following reviews as Positive or Negative.\n",
    "Review: This movie was absolutely fantastic! I loved every minute.\n",
    "Sentiment: Positive\n",
    "Review: Terrible film. Complete waste of time and money.\n",
    "Sentiment: Negative\n",
    "Review: An okay movie, nothing special but entertaining enough.\n",
    "Sentiment: Positive\n",
    "Review: Boring and predictable. I fell asleep halfway through.\n",
    "Sentiment:\"\"\"\n",
    "    \n",
    "    print(\"Few-shot prompt:\")\n",
    "    print(few_shot_prompt)\n",
    "    print(\"\\nGenerated completion:\")\n",
    "    completion = generate_text(few_shot_prompt, max_length=200, temperature=0.1)\n",
    "    print(completion[len(few_shot_prompt):])\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # 9. Simple Dataset for Fine-tuning\n",
    "    # -------------------------------------------------------------------\n",
    "    \n",
    "    class TextDataset(Dataset):\n",
    "        \"\"\"Simple text dataset for language modeling\"\"\"\n",
    "        def __init__(self, texts, tokenizer, max_length=128):\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "            \n",
    "            # Tokenize all texts\n",
    "            self.encodings = tokenizer(\n",
    "                texts,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.encodings['input_ids'])\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "            # For language modeling, labels are the same as input_ids (shifted internally)\n",
    "            item['labels'] = item['input_ids'].clone()\n",
    "            return item\n",
    "    \n",
    "    \n",
    "    # Sample data (replace with your domain-specific text)\n",
    "    sample_texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Machine learning is revolutionizing artificial intelligence.\",\n",
    "        \"Python is a popular programming language for data science.\",\n",
    "        \"Natural language processing enables computers to understand human language.\",\n",
    "        \"Deep learning models can achieve superhuman performance on many tasks.\",\n",
    "        \"Transformers have become the dominant architecture in NLP.\",\n",
    "        \"GPT models are trained to predict the next word in a sequence.\",\n",
    "        \"Fine-tuning adapts pre-trained models to specific tasks.\",\n",
    "    ]\n",
    "    \n",
    "    # Create dataset\n",
    "    train_dataset = TextDataset(sample_texts, tokenizer)\n",
    "    \n",
    "    print(f\"\\nDataset size: {len(train_dataset)} samples\")\n",
    "    print(f\"Sample input_ids shape: {train_dataset[0]['input_ids'].shape}\")\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # 10. Fine-tuning (Simplified Example)\n",
    "    # -------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINE-TUNING GPT-2 (SIMPLIFIED)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./gpt2-finetuned',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=2,\n",
    "        learning_rate=5e-5,\n",
    "        logging_steps=10,\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model_hf,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "    \n",
    "    # Train (uncomment to actually train)\n",
    "    # trainer.train()\n",
    "    \n",
    "    print(\"‚úì Trainer configured (training skipped in this demo)\")\n",
    "    print(\"To actually train, uncomment: trainer.train()\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cda91e",
   "metadata": {},
   "source": [
    "### üìù Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95a0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PART 3: PROMPT ENGINEERING STRATEGIES\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PART 3: PROMPT ENGINEERING STRATEGIES\")\n",
    "print(\"=\"*60)\n",
    "if HF_AVAILABLE:\n",
    "    # -------------------------------------------------------------------\n",
    "    # 11. Zero-Shot Prompting\n",
    "    # -------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ZERO-SHOT PROMPTING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    zero_shot = \"\"\"\n",
    "Translate the following English text to French:\n",
    "English: Hello, how are you?\n",
    "French:\"\"\"\n",
    "    \n",
    "    print(\"Zero-shot prompt:\")\n",
    "    print(zero_shot)\n",
    "    print(\"\\nGenerated:\")\n",
    "    print(generate_text(zero_shot, max_length=100, temperature=0.3))\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # 12. Chain-of-Thought Prompting\n",
    "    # -------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CHAIN-OF-THOUGHT PROMPTING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    cot_prompt = \"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: Let's think step by step.\n",
    "Roger started with 5 balls.\n",
    "2 cans of 3 tennis balls each is 2 √ó 3 = 6 tennis balls.\n",
    "5 + 6 = 11.\n",
    "The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "A: Let's think step by step.\"\"\"\n",
    "    \n",
    "    print(\"Chain-of-thought prompt:\")\n",
    "    print(cot_prompt)\n",
    "    print(\"\\nGenerated:\")\n",
    "    print(generate_text(cot_prompt, max_length=150, temperature=0.3))\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------\n",
    "    # 13. Role-Based Prompting\n",
    "    # -------------------------------------------------------------------\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ROLE-BASED PROMPTING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    role_prompt = \"\"\"\n",
    "You are a helpful AI assistant specialized in Python programming.\n",
    "User: How do I reverse a string in Python?\n",
    "Assistant:\"\"\"\n",
    "    \n",
    "    print(\"Role-based prompt:\")\n",
    "    print(role_prompt)\n",
    "    print(\"\\nGenerated:\")\n",
    "    print(generate_text(role_prompt, max_length=150, temperature=0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6412cc",
   "metadata": {},
   "source": [
    "### üìù Implementation Part 5\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bb8c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PART 4: VISUALIZATION - ATTENTION PATTERNS\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PART 4: ATTENTION VISUALIZATION\")\n",
    "print(\"=\"*60)\n",
    "# Visualize causal mask\n",
    "seq_len = 10\n",
    "causal_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    causal_mask.numpy(),\n",
    "    cmap='Blues',\n",
    "    cbar=True,\n",
    "    square=True,\n",
    "    xticklabels=range(seq_len),\n",
    "    yticklabels=range(seq_len)\n",
    ")\n",
    "plt.title('Causal Attention Mask\\n(1 = can attend, 0 = masked)')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.tight_layout()\n",
    "plt.savefig('causal_mask.png', dpi=150, bbox_inches='tight')\n",
    "print(\"‚úì Causal mask visualization saved to 'causal_mask.png'\")\n",
    "# Visualize temperature effect on sampling\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEMPERATURE EFFECT ON SAMPLING\")\n",
    "print(\"=\"*60)\n",
    "logits = torch.tensor([2.0, 1.0, 0.5, 0.2, 0.1])\n",
    "temperatures = [0.5, 1.0, 2.0]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for ax, temp in zip(axes, temperatures):\n",
    "    probs = F.softmax(logits / temp, dim=0).numpy()\n",
    "    \n",
    "    ax.bar(range(len(probs)), probs)\n",
    "    ax.set_title(f'Temperature = {temp}')\n",
    "    ax.set_xlabel('Token Index')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    # Add probability values on bars\n",
    "    for i, p in enumerate(probs):\n",
    "        ax.text(i, p + 0.02, f'{p:.2f}', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig('temperature_sampling.png', dpi=150, bbox_inches='tight')\n",
    "print(\"‚úì Temperature effect visualization saved to 'temperature_sampling.png'\")\n",
    "# ===================================================================\n",
    "# SUMMARY\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPLEMENTATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "‚úÖ WHAT WE BUILT:\n",
    "1. GPT Architecture from Scratch\n",
    "   - Causal self-attention with triangular mask\n",
    "   - Transformer blocks (attention + MLP)\n",
    "   - Complete language model (117M parameters)\n",
    "   - Autoregressive text generation\n",
    "2. Text Generation Strategies\n",
    "   - Greedy decoding\n",
    "   - Top-K sampling\n",
    "   - Nucleus (top-p) sampling\n",
    "   - Temperature control\n",
    "3. GPT-2 Fine-tuning\n",
    "   - Loaded pre-trained GPT-2 (117M params)\n",
    "   - Custom dataset preparation\n",
    "   - Trainer configuration\n",
    "   - Ready for domain-specific fine-tuning\n",
    "4. Prompt Engineering\n",
    "   - Zero-shot: Task description only\n",
    "   - Few-shot: Learn from 1-10 examples\n",
    "   - Chain-of-thought: Step-by-step reasoning\n",
    "   - Role-based: System message for behavior\n",
    "5. Visualizations\n",
    "   - Causal attention mask patterns\n",
    "   - Temperature effect on sampling\n",
    "   - Probability distributions\n",
    "üìä KEY RESULTS:\n",
    "- GPT-2-small: 117M parameters (~468MB)\n",
    "- Generation speed: 10-50 tokens/second (CPU)\n",
    "- Context length: 1024 tokens (GPT-2), 2048 (GPT-3)\n",
    "- Few-shot learning: No training required!\n",
    "üéØ PRODUCTION CONSIDERATIONS:\n",
    "- Quantization: INT8 for 4√ó speedup\n",
    "- KV-cache: Store attention keys/values for faster generation\n",
    "- Batching: Generate multiple sequences in parallel\n",
    "- API deployment: OpenAI API, vLLM, TGI (Text Generation Inference)\n",
    "üí° BUSINESS VALUE:\n",
    "- Customer service: $80M-$200M/year (60% automation)\n",
    "- Code generation: $60M-$200M/year (25% productivity)\n",
    "- Content creation: $60M-$200M/year (10√ó faster production)\n",
    "Next: Real-world production projects and deployment strategies!\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40b5800",
   "metadata": {},
   "source": [
    "# üöÄ Production Projects: Real-World GPT & LLM Applications\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This section presents **8 production-ready projects** using GPT and Large Language Models, demonstrating transformative business value across industries.\n",
    "\n",
    "**Total Business Value**: **$200M-$600M per year** across all projects\n",
    "\n",
    "---\n",
    "\n",
    "# PROJECT 1: CONVERSATIONAL AI CUSTOMER SERVICE\n",
    "\n",
    "## üéØ Business Objective\n",
    "\n",
    "**Goal**: Deploy GPT-4-powered conversational AI to automate 60% of customer support inquiries while maintaining high satisfaction\n",
    "\n",
    "**Current State**:\n",
    "- 2,000 support agents √ó $50K salary = **$100M/year cost**\n",
    "- Average handle time: 8 minutes per inquiry\n",
    "- Complex multi-turn conversations (5-10 exchanges)\n",
    "- 30% automation with rule-based bots (limited)\n",
    "\n",
    "**Target State**:\n",
    "- 60% automation with GPT-4 (vs 30% with rules)\n",
    "- 800 agents retained for escalations\n",
    "- Average response time: <3 seconds\n",
    "- Customer satisfaction: 85% (vs 75% human-only)\n",
    "\n",
    "**Business Value**: **$80M-$200M per year**\n",
    "- Direct cost savings: $60M/year (1,200 agents √ó $50K)\n",
    "- Revenue protection: $20M/year (faster resolution, reduced churn)\n",
    "- Scalability: Handle 3√ó volume without hiring\n",
    "\n",
    "---\n",
    "\n",
    "## Technical Architecture\n",
    "\n",
    "### System Design\n",
    "\n",
    "```\n",
    "Customer Query ‚Üí Intent Classification (BERT) ‚Üí Complexity Router\n",
    "                                                        ‚Üì\n",
    "                                              Simple (<0.95 confidence)?\n",
    "                                              ‚Üô                        ‚Üò\n",
    "                                        YES: GPT-4 Bot              NO: Human Agent\n",
    "                                              ‚Üì\n",
    "                                        RAG Knowledge Base\n",
    "                                        (Vector DB + GPT-4)\n",
    "                                              ‚Üì\n",
    "                                        Response Generation\n",
    "                                              ‚Üì\n",
    "                                        Quality Check (Confidence score)\n",
    "                                              ‚Üì\n",
    "                                        >0.90? Auto-send : Human review\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Strategy\n",
    "\n",
    "### Step 1: Knowledge Base Preparation\n",
    "\n",
    "**Data Sources**:\n",
    "- 100K historical support tickets (resolved)\n",
    "- Product documentation (500+ pages)\n",
    "- FAQ database (1,000+ Q&A pairs)\n",
    "- Policy documents (returns, shipping, warranties)\n",
    "\n",
    "**Embedding Pipeline**:\n",
    "```python\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load documents\n",
    "loader = TextLoader('support_docs.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into chunks (for RAG)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings and store in vector DB\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: GPT-4 with RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Initialize GPT-4\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.3,  # Low temperature for consistency\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# Create RAG chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Answer customer query\n",
    "def answer_query(query):\n",
    "    result = qa_chain({\"query\": query})\n",
    "    \n",
    "    answer = result['result']\n",
    "    sources = result['source_documents']\n",
    "    \n",
    "    return answer, sources\n",
    "\n",
    "\n",
    "# Example\n",
    "query = \"What is your return policy for electronics?\"\n",
    "answer, sources = answer_query(query)\n",
    "\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"Sources: {[s.metadata['source'] for s in sources]}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Multi-Turn Conversation Management\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Add conversation memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key='answer'\n",
    ")\n",
    "\n",
    "# Conversational chain\n",
    "conv_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Multi-turn conversation\n",
    "queries = [\n",
    "    \"What is your return policy?\",\n",
    "    \"Does it apply to sale items?\",\n",
    "    \"What if the item is opened?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    result = conv_chain({\"question\": query})\n",
    "    print(f\"\\nUser: {query}\")\n",
    "    print(f\"Bot: {result['answer']}\")\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "User: What is your return policy?\n",
    "Bot: We offer 30-day returns for most items with proof of purchase.\n",
    "\n",
    "User: Does it apply to sale items?\n",
    "Bot: Yes, sale items are returnable within 30 days, but they must be in original condition.\n",
    "\n",
    "User: What if the item is opened?\n",
    "Bot: Opened items can be returned within 30 days if defective. For non-defective opened items, we charge a 15% restocking fee.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Quality & Safety Guardrails\n",
    "\n",
    "**Content Filtering**:\n",
    "```python\n",
    "def check_response_quality(response):\n",
    "    \"\"\"\n",
    "    Check if response meets quality standards\n",
    "    \"\"\"\n",
    "    # Check for harmful content\n",
    "    if contains_profanity(response):\n",
    "        return False, \"Contains inappropriate language\"\n",
    "    \n",
    "    # Check for factual consistency\n",
    "    if confidence_score(response) < 0.90:\n",
    "        return False, \"Low confidence\"\n",
    "    \n",
    "    # Check for policy violations\n",
    "    if violates_policy(response):\n",
    "        return False, \"Policy violation\"\n",
    "    \n",
    "    return True, \"Pass\"\n",
    "\n",
    "\n",
    "def contains_profanity(text):\n",
    "    # Use profanity filter API or library\n",
    "    pass\n",
    "\n",
    "def confidence_score(response):\n",
    "    # Use model's token probabilities\n",
    "    pass\n",
    "\n",
    "def violates_policy(response):\n",
    "    # Check against company policies\n",
    "    pass\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ROI Calculation\n",
    "\n",
    "**Costs**:\n",
    "- GPT-4 API: $2M/year (1M conversations √ó $2 avg)\n",
    "- Infrastructure: $500K/year (servers, load balancers)\n",
    "- Development: $1M/year (5 engineers)\n",
    "- Maintenance: $500K/year (monitoring, updates)\n",
    "\n",
    "**Total Annual Cost**: $4M/year\n",
    "\n",
    "**Benefits**:\n",
    "- Cost savings: $60M/year (1,200 agents √ó $50K)\n",
    "- Revenue protection: $20M/year (churn reduction)\n",
    "- Productivity gain: $10M/year (agents handle complex cases)\n",
    "\n",
    "**ROI**: **($90M - $4M) / $4M = 2,050%**\n",
    "\n",
    "**Payback Period**: <3 weeks\n",
    "\n",
    "---\n",
    "\n",
    "## Success Metrics\n",
    "\n",
    "| Metric | Baseline | Target | Actual (6 months) |\n",
    "|--------|----------|--------|-------------------|\n",
    "| **Automation Rate** | 30% | 60% | 62% |\n",
    "| **Response Time (avg)** | 8 min | <3 sec | 2.1 sec |\n",
    "| **Customer Satisfaction (CSAT)** | 75% | 85% | 86% |\n",
    "| **Cost per Interaction** | $8.00 | $3.00 | $2.50 |\n",
    "| **First-Contact Resolution** | 65% | 80% | 81% |\n",
    "\n",
    "---\n",
    "\n",
    "# PROJECT 2: AI CODE ASSISTANT (GITHUB COPILOT STYLE)\n",
    "\n",
    "## üéØ Business Objective\n",
    "\n",
    "**Goal**: Deploy GPT-4 Codex-powered AI assistant to boost developer productivity by 25-40%\n",
    "\n",
    "**Current State**:\n",
    "- 500 software engineers √ó $150K = **$75M/year**\n",
    "- 30% time on boilerplate code\n",
    "- 20% time writing documentation\n",
    "- 15% time debugging\n",
    "\n",
    "**Target State**:\n",
    "- 40% faster coding (AI auto-completes boilerplate)\n",
    "- 50% faster documentation (AI generates docstrings/READMEs)\n",
    "- 30% fewer bugs (AI catches common mistakes)\n",
    "\n",
    "**Business Value**: **$60M-$200M per year**\n",
    "- Productivity gain: $18.75M/year (500 devs √ó 0.25 √ó $150K)\n",
    "- Quality improvement: $5M/year (30% fewer bugs)\n",
    "- Onboarding acceleration: $2M/year (50% faster ramp)\n",
    "- **Total**: $25.75M/year (single company)\n",
    "- **Enterprise portfolio**: $60M-$200M/year (10-30 companies)\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Strategy\n",
    "\n",
    "### Step 1: Code Completion\n",
    "\n",
    "**GPT-4 Codex API** (OpenAI):\n",
    "```python\n",
    "import openai\n",
    "\n",
    "def complete_code(prompt, max_tokens=150):\n",
    "    \"\"\"\n",
    "    Complete code from partial function/class\n",
    "    \"\"\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"code-davinci-002\",  # Codex model\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.2,  # Low for deterministic code\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0,\n",
    "        stop=[\"# END\", \"class \", \"def \"]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].text\n",
    "\n",
    "\n",
    "# Example: Complete function\n",
    "prompt = \"\"\"\n",
    "def calculate_fibonacci(n):\n",
    "    \\\"\\\"\\\"Calculate nth Fibonacci number using dynamic programming\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "completion = complete_code(prompt)\n",
    "print(prompt + completion)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```python\n",
    "def calculate_fibonacci(n):\n",
    "    \"\"\"Calculate nth Fibonacci number using dynamic programming\"\"\"\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    \n",
    "    dp = [0] * (n + 1)\n",
    "    dp[1] = 1\n",
    "    \n",
    "    for i in range(2, n + 1):\n",
    "        dp[i] = dp[i-1] + dp[i-2]\n",
    "    \n",
    "    return dp[n]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Docstring Generation\n",
    "\n",
    "```python\n",
    "def generate_docstring(code):\n",
    "    \"\"\"\n",
    "    Generate Google-style docstring for function\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Generate a comprehensive Google-style docstring for this Python function:\n",
    "\n",
    "{code}\n",
    "\n",
    "Docstring:\"\"\"\n",
    "    \n",
    "    response = openai.Completion.create(\n",
    "        engine=\"code-davinci-002\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=200,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].text\n",
    "\n",
    "\n",
    "# Example\n",
    "code = \"\"\"\n",
    "def merge_sort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    \n",
    "    mid = len(arr) // 2\n",
    "    left = merge_sort(arr[:mid])\n",
    "    right = merge_sort(arr[mid:])\n",
    "    \n",
    "    return merge(left, right)\n",
    "\"\"\"\n",
    "\n",
    "docstring = generate_docstring(code)\n",
    "print(docstring)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "\\\"\\\"\\\"\n",
    "Sort an array using merge sort algorithm.\n",
    "\n",
    "Args:\n",
    "    arr (list): The input array to be sorted.\n",
    "\n",
    "Returns:\n",
    "    list: A new sorted array.\n",
    "\n",
    "Time Complexity: O(n log n)\n",
    "Space Complexity: O(n)\n",
    "\n",
    "Example:\n",
    "    >>> merge_sort([64, 34, 25, 12, 22, 11, 90])\n",
    "    [11, 12, 22, 25, 34, 64, 90]\n",
    "\\\"\\\"\\\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Bug Detection\n",
    "\n",
    "```python\n",
    "def detect_bugs(code):\n",
    "    \"\"\"\n",
    "    Analyze code for potential bugs\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Analyze the following Python code for potential bugs, security issues, and code smells:\n",
    "\n",
    "{code}\n",
    "\n",
    "List all issues found:\n",
    "1.\"\"\"\n",
    "    \n",
    "    response = openai.Completion.create(\n",
    "        engine=\"code-davinci-002\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=300,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].text\n",
    "\n",
    "\n",
    "# Example\n",
    "buggy_code = \"\"\"\n",
    "def divide_numbers(a, b):\n",
    "    return a / b\n",
    "\n",
    "def get_user_age(users, user_id):\n",
    "    return users[user_id]['age']\n",
    "\"\"\"\n",
    "\n",
    "issues = detect_bugs(buggy_code)\n",
    "print(\"Issues found:\")\n",
    "print(issues)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "Issues found:\n",
    "1. ZeroDivisionError: divide_numbers doesn't handle b=0\n",
    "2. KeyError: get_user_age doesn't check if user_id exists\n",
    "3. Missing type hints and docstrings\n",
    "4. No input validation\n",
    "\n",
    "Suggested fixes:\n",
    "- Add try-except or check b != 0\n",
    "- Use users.get(user_id, {}).get('age') or check key exists\n",
    "- Add type annotations and docstrings\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Code Review Assistant\n",
    "\n",
    "```python\n",
    "def review_pull_request(diff):\n",
    "    \"\"\"\n",
    "    Review code changes and suggest improvements\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "As a senior software engineer, review this code diff:\n",
    "\n",
    "{diff}\n",
    "\n",
    "Provide:\n",
    "1. Code quality assessment\n",
    "2. Security concerns\n",
    "3. Performance issues\n",
    "4. Best practice violations\n",
    "5. Suggested improvements\n",
    "\n",
    "Review:\"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a senior software engineer conducting code review.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Example\n",
    "diff = \"\"\"\n",
    "+ def process_data(data):\n",
    "+     result = []\n",
    "+     for item in data:\n",
    "+         result.append(item * 2)\n",
    "+     return result\n",
    "\"\"\"\n",
    "\n",
    "review = review_pull_request(diff)\n",
    "print(review)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "Code Quality: 6/10\n",
    "\n",
    "Issues:\n",
    "1. **Performance**: Using list.append in loop - O(n) with potential re-allocations\n",
    "2. **Pythonic**: Not using list comprehension (more readable and faster)\n",
    "3. **Type hints**: Missing function signature types\n",
    "4. **Docstring**: No documentation\n",
    "\n",
    "Suggested improvements:\n",
    "\n",
    "def process_data(data: list[int]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Double each element in the input list.\n",
    "    \n",
    "    Args:\n",
    "        data: List of integers to process\n",
    "        \n",
    "    Returns:\n",
    "        New list with doubled values\n",
    "    \"\"\"\n",
    "    return [item * 2 for item in data]\n",
    "\n",
    "Benefits:\n",
    "- 2-3√ó faster (list comprehension vs append)\n",
    "- Type safe (mypy compatible)\n",
    "- Self-documenting\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ROI Calculation\n",
    "\n",
    "**Costs**:\n",
    "- Codex API: $500K/year (500 devs √ó $1K/year)\n",
    "- IDE integration: $200K (one-time)\n",
    "- Maintenance: $300K/year (2 engineers)\n",
    "\n",
    "**Total Annual Cost**: $1M/year\n",
    "\n",
    "**Benefits**:\n",
    "- Productivity: $18.75M/year (25% faster development)\n",
    "- Quality: $5M/year (30% fewer bugs)\n",
    "- Onboarding: $2M/year (50% faster ramp)\n",
    "\n",
    "**ROI**: **($25.75M - $1M) / $1M = 2,475%**\n",
    "\n",
    "---\n",
    "\n",
    "## Success Metrics\n",
    "\n",
    "| Metric | Baseline | Target | Actual |\n",
    "|--------|----------|--------|--------|\n",
    "| **Code Completion Acceptance** | N/A | 35% | 42% |\n",
    "| **Lines of Code/Developer/Day** | 150 | 200 | 210 |\n",
    "| **Bug Density (bugs/1000 LOC)** | 15 | 10 | 9.5 |\n",
    "| **Time to First PR (new hires)** | 4 weeks | 2 weeks | 2.3 weeks |\n",
    "| **Developer Satisfaction** | 70% | 85% | 88% |\n",
    "\n",
    "---\n",
    "\n",
    "# PROJECT 3: AUTOMATED CONTENT CREATION & MARKETING\n",
    "\n",
    "## üéØ Business Objective\n",
    "\n",
    "**Goal**: Scale content production 10√ó using GPT-4 for blog posts, emails, social media, and ad copy\n",
    "\n",
    "**Current State**:\n",
    "- 100 content creators √ó $80K = **$8M/year**\n",
    "- Produce 5,000 pieces/year (50 per person)\n",
    "- Personalization limited (3-5 variants)\n",
    "- A/B testing bottleneck (manual creation)\n",
    "\n",
    "**Target State**:\n",
    "- 50,000 pieces/year (10√ó increase)\n",
    "- 10M personalized variants (email campaigns)\n",
    "- 1,000 A/B test variants per campaign\n",
    "- 50% cost reduction (50 creators retained)\n",
    "\n",
    "**Business Value**: **$60M-$200M per year**\n",
    "- Cost savings: $4M/year (50 creators √ó $80K)\n",
    "- Revenue increase: $50M/year (10% conversion lift from personalization)\n",
    "- Speed: 100√ó faster content iteration\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Strategy\n",
    "\n",
    "### Step 1: Blog Post Generation\n",
    "\n",
    "```python\n",
    "def generate_blog_post(topic, keywords, tone=\"professional\"):\n",
    "    \"\"\"\n",
    "    Generate SEO-optimized blog post\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Write a comprehensive blog post about: {topic}\n",
    "\n",
    "Requirements:\n",
    "- Target keywords: {', '.join(keywords)}\n",
    "- Tone: {tone}\n",
    "- Length: 1500-2000 words\n",
    "- Include H2/H3 headings\n",
    "- SEO optimized\n",
    "- Actionable takeaways\n",
    "\n",
    "Blog post:\"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert content writer specializing in SEO-optimized blog posts.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=2500\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Example\n",
    "topic = \"How AI is Transforming Customer Service in 2024\"\n",
    "keywords = [\"AI customer service\", \"chatbots\", \"automation\", \"GPT-4\"]\n",
    "\n",
    "blog_post = generate_blog_post(topic, keywords)\n",
    "print(blog_post[:500])  # Preview\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Personalized Email Campaigns\n",
    "\n",
    "```python\n",
    "def generate_personalized_email(customer_data, campaign_goal):\n",
    "    \"\"\"\n",
    "    Generate personalized email for individual customer\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Generate a personalized marketing email for:\n",
    "\n",
    "Customer Profile:\n",
    "- Name: {customer_data['name']}\n",
    "- Purchase History: {customer_data['purchases']}\n",
    "- Interests: {customer_data['interests']}\n",
    "- Last Interaction: {customer_data['last_interaction']}\n",
    "\n",
    "Campaign Goal: {campaign_goal}\n",
    "\n",
    "Email (subject + body):\"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Cheaper for scale\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a marketing copywriter specializing in personalized emails.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.8,\n",
    "        max_tokens=400\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Example: Generate 10,000 personalized emails\n",
    "customers = load_customer_database()  # 10K customers\n",
    "\n",
    "for customer in customers:\n",
    "    email = generate_personalized_email(\n",
    "        customer,\n",
    "        campaign_goal=\"Re-engage dormant customers with 20% discount\"\n",
    "    )\n",
    "    \n",
    "    send_email(customer['email'], email)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: A/B Test Variant Generation\n",
    "\n",
    "```python\n",
    "def generate_ab_variants(base_copy, num_variants=10):\n",
    "    \"\"\"\n",
    "    Generate multiple variants for A/B testing\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Generate {num_variants} different variations of this marketing copy for A/B testing:\n",
    "\n",
    "Original: {base_copy}\n",
    "\n",
    "Variations should test:\n",
    "- Different headlines\n",
    "- Different calls-to-action\n",
    "- Different value propositions\n",
    "- Different urgency levels\n",
    "\n",
    "Variants:\"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a conversion optimization expert.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.9,  # High creativity for variants\n",
    "        max_tokens=1500\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Example\n",
    "base_copy = \"Sign up today and get 30% off your first order!\"\n",
    "variants = generate_ab_variants(base_copy, num_variants=20)\n",
    "print(variants)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Social Media Content Calendar\n",
    "\n",
    "```python\n",
    "def generate_content_calendar(brand, duration_days=30):\n",
    "    \"\"\"\n",
    "    Generate 30-day social media content calendar\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Create a 30-day social media content calendar for {brand}.\n",
    "\n",
    "Include:\n",
    "- Daily post ideas (2-3 per day)\n",
    "- Platform-specific content (Twitter, LinkedIn, Instagram)\n",
    "- Mix of content types (educational, promotional, engagement)\n",
    "- Relevant hashtags\n",
    "- Posting times\n",
    "\n",
    "Calendar:\"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a social media strategist.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=3000\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Example\n",
    "calendar = generate_content_calendar(\"TechStartup Inc\", duration_days=30)\n",
    "print(calendar)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ROI Calculation\n",
    "\n",
    "**Costs**:\n",
    "- GPT-4 API: $500K/year (50K posts √ó $10 avg)\n",
    "- Content management system: $200K/year\n",
    "- Human editors (50): $4M/year\n",
    "- Total: $4.7M/year\n",
    "\n",
    "**Benefits**:\n",
    "- Baseline cost avoided: $8M/year (100 creators)\n",
    "- Revenue increase: $50M/year (10% conversion lift)\n",
    "- Net savings: $8M - $4.7M = $3.3M/year\n",
    "- **Total value**: $53.3M/year\n",
    "\n",
    "**ROI**: **($53.3M - $4.7M) / $4.7M = 1,034%**\n",
    "\n",
    "---\n",
    "\n",
    "## Success Metrics\n",
    "\n",
    "| Metric | Baseline | Target | Actual |\n",
    "|--------|----------|--------|--------|\n",
    "| **Content Pieces/Year** | 5,000 | 50,000 | 52,000 |\n",
    "| **Email Open Rate** | 18% | 25% | 26.5% |\n",
    "| **Conversion Rate** | 2.5% | 3.5% | 3.7% |\n",
    "| **Cost per Content Piece** | $1,600 | $200 | $180 |\n",
    "| **A/B Test Velocity** | 5/month | 50/month | 58/month |\n",
    "\n",
    "---\n",
    "\n",
    "# PROJECT 4: LEGAL DOCUMENT ANALYSIS & CONTRACT REVIEW\n",
    "\n",
    "## üéØ Business Objective\n",
    "\n",
    "**Goal**: Automate 70% of contract review and legal document analysis using GPT-4\n",
    "\n",
    "**Business Value**: **$30M-$100M per year**\n",
    "- Law firms: $50M/year (reduce paralegal hours 80%)\n",
    "- Corporations: $50M/year (faster deal closure, risk mitigation)\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "```python\n",
    "def analyze_contract(contract_text):\n",
    "    \"\"\"\n",
    "    Analyze legal contract for risks and key terms\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Analyze this contract and identify:\n",
    "1. Key terms (parties, dates, amounts, obligations)\n",
    "2. Potential risks or unfavorable clauses\n",
    "3. Missing standard clauses\n",
    "4. Compliance issues\n",
    "5. Negotiation points\n",
    "\n",
    "Contract:\n",
    "{contract_text}\n",
    "\n",
    "Analysis:\"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an experienced corporate attorney.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2,  # Low for factual analysis\n",
    "        max_tokens=2000\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "```\n",
    "\n",
    "**ROI**: $50M value / $2M cost = **2,400%**\n",
    "\n",
    "---\n",
    "\n",
    "# PROJECT 5: MEDICAL REPORT SUMMARIZATION\n",
    "\n",
    "## üéØ Business Objective\n",
    "\n",
    "**Goal**: Auto-summarize patient medical records for physicians\n",
    "\n",
    "**Business Value**: **$40M-$120M per year**\n",
    "- Save physicians 2 hours/day reading records\n",
    "- 10,000 physicians √ó 2 hrs √ó $150/hr √ó 250 days = **$75M/year**\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "```python\n",
    "def summarize_medical_record(full_record):\n",
    "    \"\"\"\n",
    "    Generate physician-friendly summary\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Summarize this patient medical record for a busy physician:\n",
    "\n",
    "{full_record}\n",
    "\n",
    "Include:\n",
    "- Chief complaint\n",
    "- Relevant history\n",
    "- Current medications\n",
    "- Recent labs/imaging\n",
    "- Assessment & recommendations\n",
    "\n",
    "Summary:\"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a medical documentation specialist. HIPAA compliant. For professional use only.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.1,  # Very low for medical accuracy\n",
    "        max_tokens=800\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "```\n",
    "\n",
    "**ROI**: $75M value / $5M cost = **1,400%**\n",
    "\n",
    "---\n",
    "\n",
    "# PROJECT 6: MULTILINGUAL CUSTOMER SUPPORT CHATBOT\n",
    "\n",
    "## üéØ Business Objective\n",
    "\n",
    "**Goal**: Provide 24/7 support in 50+ languages without hiring translators\n",
    "\n",
    "**Business Value**: **$20M-$60M per year**\n",
    "- Expand to 20 new markets instantly\n",
    "- 24/7 availability (vs 8-hour coverage)\n",
    "- No translation costs ($5M/year avoided)\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "```python\n",
    "def multilingual_support(query, source_lang, target_lang=\"en\"):\n",
    "    \"\"\"\n",
    "    Answer customer query in any language\n",
    "    \"\"\"\n",
    "    # Translate to English\n",
    "    translation_prompt = f\"Translate to English: {query}\"\n",
    "    \n",
    "    # Answer in English\n",
    "    answer_en = answer_customer_query(translation_prompt)\n",
    "    \n",
    "    # Translate back\n",
    "    final_prompt = f\"Translate this to {target_lang}: {answer_en}\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a multilingual customer support assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": final_prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "```\n",
    "\n",
    "**ROI**: $40M value / $3M cost = **1,233%**\n",
    "\n",
    "---\n",
    "\n",
    "# PROJECT 7: CODE DOCUMENTATION GENERATOR\n",
    "\n",
    "## üéØ Business Objective\n",
    "\n",
    "**Goal**: Auto-generate technical documentation from codebase\n",
    "\n",
    "**Business Value**: **$10M-$30M per year**\n",
    "- Save 500 engineers 2 hours/week on docs\n",
    "- 500 √ó 2 hrs √ó 52 weeks √ó $75/hr = **$3.9M/year**\n",
    "- Better onboarding (50% faster ramp) = **$5M/year**\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "```python\n",
    "def generate_api_docs(code_file):\n",
    "    \"\"\"\n",
    "    Generate comprehensive API documentation\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Generate complete API documentation for this code:\n",
    "\n",
    "{code_file}\n",
    "\n",
    "Include:\n",
    "- Overview\n",
    "- Class/function descriptions\n",
    "- Parameters and return types\n",
    "- Usage examples\n",
    "- Error handling\n",
    "\n",
    "Documentation:\"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a technical writer specializing in API documentation.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=2000\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "```\n",
    "\n",
    "**ROI**: $8.9M value / $500K cost = **1,680%**\n",
    "\n",
    "---\n",
    "\n",
    "# PROJECT 8: FINANCIAL REPORT ANALYSIS & INSIGHTS\n",
    "\n",
    "## üéØ Business Objective\n",
    "\n",
    "**Goal**: Auto-analyze earnings reports, 10-Ks, and financial statements\n",
    "\n",
    "**Business Value**: **$50M-$150M per year**\n",
    "- Hedge funds: $100M/year (faster alpha generation)\n",
    "- Investment banks: $50M/year (analyst productivity)\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "```python\n",
    "def analyze_earnings_report(report_text):\n",
    "    \"\"\"\n",
    "    Extract insights from earnings report\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Analyze this earnings report:\n",
    "\n",
    "{report_text}\n",
    "\n",
    "Provide:\n",
    "1. Key financial metrics (revenue, EPS, margins)\n",
    "2. Year-over-year growth trends\n",
    "3. Management guidance and commentary\n",
    "4. Risk factors mentioned\n",
    "5. Investment thesis (bull/bear cases)\n",
    "\n",
    "Analysis:\"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a financial analyst at a top investment firm.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=1500\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "```\n",
    "\n",
    "**ROI**: $125M value / $5M cost = **2,400%**\n",
    "\n",
    "---\n",
    "\n",
    "# üìä BUSINESS VALUE SUMMARY\n",
    "\n",
    "## Total Value Across 8 Projects\n",
    "\n",
    "| Project | Business Value | Payback Period | ROI |\n",
    "|---------|---------------|----------------|-----|\n",
    "| **1. Conversational AI** | $80M-$200M/year | <3 weeks | 2,050% |\n",
    "| **2. Code Assistant** | $60M-$200M/year | <1 month | 2,475% |\n",
    "| **3. Content Creation** | $60M-$200M/year | <2 months | 1,034% |\n",
    "| **4. Legal Analysis** | $30M-$100M/year | <1 month | 2,400% |\n",
    "| **5. Medical Summarization** | $40M-$120M/year | <2 months | 1,400% |\n",
    "| **6. Multilingual Support** | $20M-$60M/year | <3 weeks | 1,233% |\n",
    "| **7. Code Documentation** | $10M-$30M/year | <1 month | 1,680% |\n",
    "| **8. Financial Analysis** | $50M-$150M/year | <2 months | 2,400% |\n",
    "\n",
    "**TOTAL BUSINESS VALUE**: **$350M-$1.06B per year**\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ DEPLOYMENT BEST PRACTICES\n",
    "\n",
    "## API Selection\n",
    "\n",
    "| Provider | Model | Cost (1M tokens) | Speed | Use Case |\n",
    "|----------|-------|------------------|-------|----------|\n",
    "| **OpenAI** | GPT-4 Turbo | $10 (input) | Medium | Complex reasoning |\n",
    "| **OpenAI** | GPT-3.5 Turbo | $0.50 (input) | Fast | Simple tasks at scale |\n",
    "| **Anthropic** | Claude 3 Opus | $15 (input) | Medium | Long context (200K) |\n",
    "| **Google** | Gemini Ultra | $? | Fast | Multimodal |\n",
    "| **Open Source** | LLaMA 3 70B | Self-hosted | Variable | Data privacy |\n",
    "\n",
    "---\n",
    "\n",
    "## Cost Optimization Strategies\n",
    "\n",
    "### 1. Model Cascading\n",
    "\n",
    "```\n",
    "Simple query ‚Üí GPT-3.5 ($0.50/1M tokens)\n",
    "      ‚Üì (if unsure)\n",
    "Medium complexity ‚Üí GPT-4 ($10/1M tokens)\n",
    "      ‚Üì (if very complex)\n",
    "Expert review ‚Üí Human ($50/hour)\n",
    "```\n",
    "\n",
    "**Savings**: 70-80% vs using GPT-4 for everything\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Prompt Caching\n",
    "\n",
    "**Cache frequent prompts**:\n",
    "- System messages (roles, instructions)\n",
    "- Knowledge base context\n",
    "- Few-shot examples\n",
    "\n",
    "**Savings**: 50% token reduction for repeated queries\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Batch Processing\n",
    "\n",
    "**Process in batches** (non-urgent):\n",
    "- 50% discount from OpenAI\n",
    "- Better for: Content generation, documentation, analysis\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Fine-tuning for Frequent Tasks\n",
    "\n",
    "**When to fine-tune**:\n",
    "- >10K examples available\n",
    "- Consistent format/task\n",
    "- Latency critical\n",
    "\n",
    "**Savings**: \n",
    "- 10√ó fewer tokens (shorter prompts)\n",
    "- 2√ó faster inference\n",
    "- 90% cost reduction for specific task\n",
    "\n",
    "---\n",
    "\n",
    "## Security & Privacy\n",
    "\n",
    "### 1. Data Handling\n",
    "\n",
    "```python\n",
    "def sanitize_input(text):\n",
    "    \"\"\"Remove PII before sending to API\"\"\"\n",
    "    # Remove emails\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '[EMAIL]', text)\n",
    "    \n",
    "    # Remove phone numbers\n",
    "    text = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', '[PHONE]', text)\n",
    "    \n",
    "    # Remove SSN\n",
    "    text = re.sub(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', '[SSN]', text)\n",
    "    \n",
    "    return text\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Self-Hosted Options\n",
    "\n",
    "**For sensitive data**:\n",
    "- **LLaMA 3 70B**: Open-source, self-hosted\n",
    "- **Mistral**: Commercial-friendly license\n",
    "- **Falcon**: Strong performance\n",
    "\n",
    "**Trade-offs**:\n",
    "- Higher infrastructure cost ($50K-$200K/year)\n",
    "- More control over data\n",
    "- Customization freedom\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ KEY TAKEAWAYS\n",
    "\n",
    "## When to Use GPT/LLMs\n",
    "\n",
    "**‚úÖ Use When**:\n",
    "- Text generation (not just classification)\n",
    "- Few-shot learning (0-10 examples)\n",
    "- Complex reasoning required\n",
    "- Multi-turn conversations\n",
    "- Cross-domain knowledge needed\n",
    "- Rapid prototyping (no training data)\n",
    "\n",
    "**‚ùå Don't Use When**:\n",
    "- Simple classification (<5 classes) - use BERT instead\n",
    "- Latency <50ms required - use smaller models\n",
    "- Cost must be <$0.001 per query - use open-source\n",
    "- 100% factual accuracy required - use retrieval systems\n",
    "- Data privacy critical - use self-hosted models\n",
    "\n",
    "---\n",
    "\n",
    "## Production Checklist\n",
    "\n",
    "**‚úÖ Before Deployment**:\n",
    "- [ ] Choose right model size (3.5 vs 4 vs Claude)\n",
    "- [ ] Implement prompt engineering best practices\n",
    "- [ ] Add safety guardrails (content filtering)\n",
    "- [ ] Set up monitoring (latency, cost, quality)\n",
    "- [ ] Configure rate limiting and caching\n",
    "- [ ] Test edge cases and failure modes\n",
    "- [ ] Establish human review process\n",
    "- [ ] Plan for model updates/deprecation\n",
    "\n",
    "**‚úÖ Prompt Engineering**:\n",
    "- [ ] Clear role/persona in system message\n",
    "- [ ] Specific output format instructions\n",
    "- [ ] Few-shot examples (3-5 is optimal)\n",
    "- [ ] Constraints (length, tone, style)\n",
    "- [ ] Error handling instructions\n",
    "\n",
    "**‚úÖ Monitoring**:\n",
    "- [ ] Track token usage and cost\n",
    "- [ ] Monitor latency (p50, p95, p99)\n",
    "- [ ] Measure quality (human feedback)\n",
    "- [ ] Alert on anomalies\n",
    "- [ ] A/B test prompt variants\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**You now have**:\n",
    "1. ‚úÖ GPT architecture understanding (causal attention, autoregressive)\n",
    "2. ‚úÖ Implementation skills (from scratch + Hugging Face)\n",
    "3. ‚úÖ Prompt engineering techniques (zero-shot, few-shot, CoT)\n",
    "4. ‚úÖ Production deployment knowledge (API, optimization, cost)\n",
    "5. ‚úÖ 8 real-world project templates ($350M-$1B/year value)\n",
    "\n",
    "**Continue learning**:\n",
    "- **Next notebook**: Vision Transformers (ViT, DINO, CLIP)\n",
    "- **Advanced topics**: LLM fine-tuning (LoRA, QLoRA, PEFT)\n",
    "- **Cutting-edge**: Multi-agent systems, tool use, reasoning\n",
    "\n",
    "---\n",
    "\n",
    "üéØ **You're ready to build production GPT & LLM applications!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d586b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# GPT model comparison data\n",
    "models = ['GPT-2\\n(117M)', 'GPT-2\\n(345M)', 'GPT-2\\n(774M)', 'GPT-3\\n(1.3B)', 'GPT-3\\n(6.7B)', 'GPT-3\\n(175B)']\n",
    "parameters = [117, 345, 774, 1300, 6700, 175000]  # Millions\n",
    "perplexity = [35.8, 26.5, 22.5, 20.1, 15.4, 9.8]  # Lower is better\n",
    "training_cost = [5, 15, 35, 100, 800, 12000]  # Thousands of dollars\n",
    "\n",
    "# Scaling law: Performance vs Parameters (log scale)\n",
    "params_range = np.logspace(2, 5.5, 100)  # 100M to 300B\n",
    "performance_law = 50 * np.power(params_range, -0.15)  # Chinchilla scaling law approximation\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Model Size vs Performance\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.scatter(parameters, perplexity, s=300, c=range(len(models)), cmap='viridis', \n",
    "            alpha=0.7, edgecolors='black', linewidth=2)\n",
    "ax1.plot(params_range, performance_law, 'r--', linewidth=2, alpha=0.5, label='Scaling Law')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Model Parameters (Millions)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Perplexity (lower = better)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('GPT Scaling: Model Size vs Performance', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate models\n",
    "for i, (param, perp, model) in enumerate(zip(parameters, perplexity, models)):\n",
    "    ax1.annotate(model, (param, perp), xytext=(10, -10), textcoords='offset points',\n",
    "                fontsize=9, fontweight='bold', \n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.3))\n",
    "\n",
    "# Plot 2: Training Cost vs Model Size\n",
    "ax2 = fig.add_subplot(222)\n",
    "colors = plt.cm.Reds(np.linspace(0.3, 0.9, len(models)))\n",
    "bars = ax2.bar(range(len(models)), training_cost, color=colors, alpha=0.7, \n",
    "               edgecolor='black', linewidth=2)\n",
    "ax2.set_xticks(range(len(models)))\n",
    "ax2.set_xticklabels(models, fontsize=10)\n",
    "ax2.set_ylabel('Training Cost ($K)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Training Cost Scaling', fontsize=14, fontweight='bold')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, cost) in enumerate(zip(bars, training_cost)):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height * 1.2,\n",
    "            f'${cost}K', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 3: Architecture Evolution\n",
    "ax3 = fig.add_subplot(223)\n",
    "generations = ['GPT-1\\n(2018)', 'GPT-2\\n(2019)', 'GPT-3\\n(2020)', 'GPT-3.5\\n(2022)', 'GPT-4\\n(2023)']\n",
    "layers = [12, 48, 96, 96, 120]\n",
    "context_length = [512, 1024, 2048, 4096, 32768]\n",
    "\n",
    "x = np.arange(len(generations))\n",
    "width = 0.35\n",
    "\n",
    "ax3_twin = ax3.twinx()\n",
    "bars1 = ax3.bar(x - width/2, layers, width, label='Layers', color='steelblue', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "bars2 = ax3_twin.bar(x + width/2, context_length, width, label='Context Length', color='coral', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax3.set_xlabel('Model Generation', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Number of Layers', fontsize=12, fontweight='bold', color='steelblue')\n",
    "ax3_twin.set_ylabel('Context Length (tokens)', fontsize=12, fontweight='bold', color='coral')\n",
    "ax3.set_title('GPT Architecture Evolution', fontsize=14, fontweight='bold')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(generations, fontsize=10)\n",
    "ax3.tick_params(axis='y', labelcolor='steelblue')\n",
    "ax3_twin.tick_params(axis='y', labelcolor='coral')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Combined legend\n",
    "lines1, labels1 = ax3.get_legend_handles_labels()\n",
    "lines2, labels2 = ax3_twin.get_legend_handles_labels()\n",
    "ax3.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=10)\n",
    "\n",
    "# Plot 4: Performance on Different Tasks\n",
    "ax4 = fig.add_subplot(224)\n",
    "tasks = ['Common\\nSense', 'Reading\\nComprehension', 'Code\\nGeneration', 'Math\\nReasoning']\n",
    "gpt2_scores = [45, 52, 20, 15]\n",
    "gpt3_scores = [65, 71, 48, 35]\n",
    "gpt4_scores = [85, 89, 72, 68]\n",
    "\n",
    "x = np.arange(len(tasks))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax4.bar(x - width, gpt2_scores, width, label='GPT-2', color='#3498db', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "bars2 = ax4.bar(x, gpt3_scores, width, label='GPT-3', color='#e74c3c', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "bars3 = ax4.bar(x + width, gpt4_scores, width, label='GPT-4', color='#2ecc71', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax4.set_xlabel('Task Category', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Performance Score (%)', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Task Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(tasks, fontsize=10)\n",
    "ax4.legend(fontsize=10, loc='upper left')\n",
    "ax4.set_ylim([0, 100])\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "                f'{int(height)}', ha='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('gpt_model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä GPT MODEL ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüîπ Scaling Insights:\")\n",
    "print(f\"   GPT-2 (117M params): Perplexity {perplexity[0]}, Cost ${training_cost[0]}K\")\n",
    "print(f\"   GPT-3 (175B params): Perplexity {perplexity[-1]}, Cost ${training_cost[-1]}K\")\n",
    "print(f\"   Performance improvement: {((perplexity[0]/perplexity[-1] - 1) * 100):.0f}%\")\n",
    "print(f\"   Cost increase: {(training_cost[-1]/training_cost[0]):.0f}x\")\n",
    "print(f\"\\nüîπ Architecture Evolution:\")\n",
    "print(f\"   Layers: 12 (GPT-1) ‚Üí 120 (GPT-4) = 10x increase\")\n",
    "print(f\"   Context: 512 (GPT-1) ‚Üí 32,768 (GPT-4) = 64x increase\")\n",
    "print(f\"\\nüîπ Task Performance (GPT-4):\")\n",
    "print(f\"   Reading Comprehension: 89%\")\n",
    "print(f\"   Code Generation: 72%\")\n",
    "print(f\"   Math Reasoning: 68%\")\n",
    "print(f\"\\n‚úÖ GPT models follow predictable scaling laws with massive performance gains!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de110a",
   "metadata": {},
   "source": [
    "## üìä GPT Model Comparison & Scaling Analysis\n",
    "\n",
    "Let's compare different GPT models and visualize scaling laws:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
