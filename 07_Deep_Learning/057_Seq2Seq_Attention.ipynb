{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3632eda4",
   "metadata": {},
   "source": [
    "# 057: Seq2Seq & Attention Mechanisms",
    "",
    "## \ud83d\udcda Learning Objectives",
    "",
    "By the end of this notebook, you will master:",
    "",
    "1. **Encoder-Decoder Architecture** - Transform input sequences to output sequences of different lengths",
    "2. **Seq2Seq Fundamentals** - Context vector, teacher forcing, inference strategies",
    "3. **Attention Mechanism** - Overcome fixed-length bottleneck, dynamic context weighting",
    "4. **Attention Variants** - Bahdanau (additive), Luong (multiplicative), self-attention",
    "5. **Beam Search** - Generate multiple candidate sequences, select best output",
    "6. **Semiconductor Applications** - Test sequence optimization, failure report generation",
    "7. **Production Deployment** - ONNX export, inference optimization, real-time translation",
    "8. **Modern Extensions** - Transformer foundations, multi-head attention preview",
    "",
    "---",
    "",
    "## \ud83c\udfaf Why Seq2Seq Matters",
    "",
    "### **The Variable-Length Problem**",
    "",
    "**Traditional RNNs (Notebook 056):**",
    "- Fixed-length input \u2192 Fixed-length output",
    "- Example: 20 test cycles \u2192 Binary classification (pass/fail)",
    "",
    "**Seq2Seq enables:**",
    "- Variable-length input \u2192 Variable-length output",
    "- Example: Test failure pattern (20 steps) \u2192 Diagnostic report (50 words)",
    "",
    "**Real-World Applications:**",
    "1. **Machine Translation:** English (5 words) \u2192 French (7 words)",
    "2. **Text Summarization:** Article (1000 words) \u2192 Summary (100 words)",
    "3. **Speech Recognition:** Audio (3 seconds) \u2192 Text (10 words)",
    "4. **Test Report Generation:** Parametric data (20 cycles) \u2192 Failure analysis (50 tokens)",
    "",
    "---",
    "",
    "## \ud83c\udfed Semiconductor Use Case: Automated Failure Diagnosis",
    "",
    "### **Problem Statement**",
    "",
    "**Objective:** Generate natural language failure reports from sequential parametric test data",
    "",
    "**Current Manual Process:**",
    "1. Test engineer reviews 20-cycle parametric data",
    "2. Identifies degradation patterns manually (takes 10-15 minutes)",
    "3. Writes failure report: \"Device shows gradual Vdd voltage drop from 1.05V to 0.98V over cycles 10-20, accompanied by 15% leakage current increase. Root cause: likely gate oxide degradation. Recommendation: Bin as reliability fail.\"",
    "",
    "**Automated Seq2Seq Process:**",
    "1. Input: Sequential test data (20 cycles \u00d7 15 parameters)",
    "2. Encoder: LSTM encodes parametric patterns",
    "3. Decoder: LSTM generates failure report token-by-token",
    "4. Attention: Focus on specific cycles during generation",
    "5. Output: Automated diagnostic report (50 tokens, <1 second)",
    "",
    "**Business Value:**",
    "- **Time savings:** $5M-$20M/year from 95% faster failure analysis (15 min \u2192 1 sec)",
    "- **Consistency:** Eliminate human reporting variability",
    "- **Scalability:** Analyze 100K failures/day (vs 50 manually)",
    "- **Knowledge preservation:** Encode expert knowledge in model",
    "",
    "---",
    "",
    "## \ud83d\udcca What We'll Build",
    "",
    "```mermaid",
    "graph TB",
    "    A[Input Sequence<br/>Test Data: 20 cycles] --> B[ENCODER<br/>LSTM]",
    "    B --> C[Context Vector<br/>Fixed-size representation]",
    "    C --> D[DECODER<br/>LSTM with Attention]",
    "    D --> E[Output Sequence<br/>Failure Report: 50 tokens]",
    "    ",
    "    B -.->|All hidden states| F[Attention Mechanism]",
    "    F -.->|Weighted context| D",
    "    ",
    "    style A fill:#e1f5ff",
    "    style C fill:#fff3cd",
    "    style E fill:#d4edda",
    "    style F fill:#f8d7da",
    "```",
    "",
    "**Architecture Comparison:**",
    "",
    "| Model | Context | Bottleneck | Long Sequences | Use Case |",
    "|-------|---------|------------|----------------|----------|",
    "| **Vanilla Seq2Seq** | Single vector | \u2717 Yes | Poor | Short sequences (<10) |",
    "| **Seq2Seq + Attention** | Weighted sum | \u2713 No | Excellent | Long sequences (50+) |",
    "| **Transformer** | Self-attention | \u2713 No | Excellent | Very long (1000+), parallelizable |",
    "",
    "---",
    "",
    "## \ud83d\udd27 Prerequisites",
    "",
    "```python",
    "# Core libraries",
    "import numpy as np",
    "import pandas as pd",
    "import matplotlib.pyplot as plt",
    "import seaborn as sns",
    "",
    "# PyTorch for neural networks",
    "import torch",
    "import torch.nn as nn",
    "import torch.optim as optim",
    "from torch.utils.data import Dataset, DataLoader",
    "import torch.nn.functional as F",
    "",
    "# NLP utilities",
    "from collections import Counter",
    "import string",
    "import random",
    "",
    "# Visualization",
    "import warnings",
    "warnings.filterwarnings('ignore')",
    "",
    "# Set random seeds",
    "np.random.seed(42)",
    "torch.manual_seed(42)",
    "random.seed(42)",
    "```",
    "",
    "**Installation:**",
    "```bash",
    "pip install torch numpy pandas matplotlib seaborn",
    "```",
    "",
    "---",
    "",
    "## \ud83d\udcc8 Success Metrics",
    "",
    "**Model Performance:**",
    "- **BLEU Score:** \u22650.40 (measure translation quality)",
    "- **Perplexity:** <20 (measure generation confidence)",
    "- **Accuracy:** \u226585% token-level accuracy",
    "- **Semantic similarity:** \u22650.80 (cosine similarity with reference reports)",
    "",
    "**Computational Efficiency:**",
    "- **Training time:** <30 min on CPU for 10K sequence pairs",
    "- **Inference time:** <100ms per report generation",
    "- **Model size:** <10MB (edge deployable)",
    "",
    "**Business Impact:**",
    "- **Time savings:** 95% reduction (15 min \u2192 1 sec per failure analysis)",
    "- **Throughput:** 100K reports/day (vs 50 manually)",
    "- **Cost savings:** $5M-$20M/year from automated diagnostics",
    "",
    "---",
    "",
    "## \ud83d\uddc2\ufe0f Notebook Structure",
    "",
    "1. **Mathematical Foundations** - Encoder-decoder equations, attention scores, alignment",
    "2. **Data Generation** - Synthetic test sequences paired with diagnostic reports",
    "3. **Vanilla Seq2Seq** - Baseline encoder-decoder without attention",
    "4. **Seq2Seq with Attention** - Bahdanau attention mechanism",
    "5. **Beam Search Decoding** - Generate multiple candidates, select best",
    "6. **Attention Visualization** - Heatmaps showing which input timesteps matter",
    "7. **Real-World Projects** - 8 production applications",
    "8. **Key Takeaways** - When to use seq2seq, optimization strategies",
    "",
    "Let's start! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827e56a5",
   "metadata": {},
   "source": [
    "# \ud83d\udcd0 Part 1: Mathematical Foundations\n",
    "\n",
    "## \ud83d\udd00 Vanilla Seq2Seq Architecture\n",
    "\n",
    "### **Two-Stage Pipeline: Encoder \u2192 Decoder**\n",
    "\n",
    "**Stage 1: Encoder** (compress input sequence into fixed-size context vector)\n",
    "\n",
    "Given input sequence $X = (x_1, x_2, ..., x_T)$ where $T$ = input length:\n",
    "\n",
    "$$\n",
    "h_t^{enc} = \\text{LSTM}_{enc}(x_t, h_{t-1}^{enc})\n",
    "$$\n",
    "\n",
    "Context vector (final hidden state):\n",
    "\n",
    "$$\n",
    "c = h_T^{enc}\n",
    "$$\n",
    "\n",
    "**Stage 2: Decoder** (generate output sequence from context vector)\n",
    "\n",
    "Given target sequence $Y = (y_1, y_2, ..., y_{T'})$ where $T'$ = output length:\n",
    "\n",
    "$$\n",
    "h_t^{dec} = \\text{LSTM}_{dec}(y_{t-1}, h_{t-1}^{dec})\n",
    "$$\n",
    "\n",
    "Initial hidden state:\n",
    "\n",
    "$$\n",
    "h_0^{dec} = c = h_T^{enc}\n",
    "$$\n",
    "\n",
    "Output distribution at each timestep:\n",
    "\n",
    "$$\n",
    "P(y_t | y_1, ..., y_{t-1}, X) = \\text{softmax}(W_{out} h_t^{dec} + b_{out})\n",
    "$$\n",
    "\n",
    "### **Example: Test Data \u2192 Failure Report**\n",
    "\n",
    "**Input sequence (20 cycles):**\n",
    "```\n",
    "x\u2081 = [Vdd=1.05, Idd=250, Temp=75, ...]  (cycle 1)\n",
    "x\u2082 = [Vdd=1.04, Idd=248, Temp=76, ...]  (cycle 2)\n",
    "...\n",
    "x\u2082\u2080 = [Vdd=0.98, Idd=210, Temp=82, ...] (cycle 20)\n",
    "```\n",
    "\n",
    "**Encoder processing:**\n",
    "```\n",
    "h\u2081\u1d49\u207f\u1d9c = LSTM([1.05, 250, 75, ...], h\u2080)\n",
    "h\u2082\u1d49\u207f\u1d9c = LSTM([1.04, 248, 76, ...], h\u2081\u1d49\u207f\u1d9c)\n",
    "...\n",
    "h\u2082\u2080\u1d49\u207f\u1d9c = LSTM([0.98, 210, 82, ...], h\u2081\u2089\u1d49\u207f\u1d9c)\n",
    "\n",
    "Context c = h\u2082\u2080\u1d49\u207f\u1d9c  \u2190 Single vector represents entire sequence!\n",
    "```\n",
    "\n",
    "**Decoder generation:**\n",
    "```\n",
    "Start with <SOS> (start-of-sequence token)\n",
    "\n",
    "h\u2081\u1d48\u1d49\u1d9c = LSTM(<SOS>, h\u2080\u1d48\u1d49\u1d9c = c)\n",
    "\u2192 P(y\u2081) = softmax(W\u00b7h\u2081\u1d48\u1d49\u1d9c)\n",
    "\u2192 y\u2081 = \"Device\"\n",
    "\n",
    "h\u2082\u1d48\u1d49\u1d9c = LSTM(\"Device\", h\u2081\u1d48\u1d49\u1d9c)\n",
    "\u2192 P(y\u2082) = softmax(W\u00b7h\u2082\u1d48\u1d49\u1d9c)\n",
    "\u2192 y\u2082 = \"shows\"\n",
    "\n",
    "h\u2083\u1d48\u1d49\u1d9c = LSTM(\"shows\", h\u2082\u1d48\u1d49\u1d9c)\n",
    "\u2192 P(y\u2083) = softmax(W\u00b7h\u2083\u1d48\u1d49\u1d9c)\n",
    "\u2192 y\u2083 = \"voltage\"\n",
    "\n",
    "... (continue until <EOS> token generated)\n",
    "\n",
    "Final output: \"Device shows voltage drop from 1.05V to 0.98V\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## \u26a0\ufe0f The Bottleneck Problem\n",
    "\n",
    "### **Fixed-Length Context Vector Limitation**\n",
    "\n",
    "**Problem:** Entire input sequence compressed into single vector $c \\in \\mathbb{R}^h$ (e.g., 512 dimensions)\n",
    "\n",
    "**For long sequences (T=100):**\n",
    "- Early information (cycles 1-20) gets overwritten by later cycles (80-100)\n",
    "- Decoder has no direct access to encoder hidden states $h_1^{enc}, h_2^{enc}, ..., h_T^{enc}$\n",
    "- All information flows through bottleneck $c$\n",
    "\n",
    "**Analogy:**\n",
    "- Reading 100-page book\n",
    "- Summarizing entire book in one sentence\n",
    "- Trying to answer detailed questions from that one sentence \u2190 Information loss!\n",
    "\n",
    "### **Empirical Evidence**\n",
    "\n",
    "Performance of vanilla seq2seq by input length:\n",
    "\n",
    "| Input Length | BLEU Score | Comment |\n",
    "|--------------|------------|---------|\n",
    "| 10 | 0.45 | Good |\n",
    "| 20 | 0.38 | Acceptable |\n",
    "| 50 | 0.25 | Poor (information loss) |\n",
    "| 100 | 0.12 | Terrible (severe bottleneck) |\n",
    "\n",
    "**Solution:** Attention mechanism! \u2728\n",
    "\n",
    "---\n",
    "\n",
    "## \u2728 Attention Mechanism\n",
    "\n",
    "### **Key Idea: Dynamic Context**\n",
    "\n",
    "Instead of single context vector $c$, compute **different context for each decoder timestep**:\n",
    "\n",
    "$$\n",
    "c_t = \\sum_{i=1}^{T} \\alpha_{t,i} h_i^{enc}\n",
    "$$\n",
    "\n",
    "where $\\alpha_{t,i}$ = attention weight (how much to focus on encoder timestep $i$ when generating decoder output $t$)\n",
    "\n",
    "### **Attention Weight Computation (Bahdanau Attention)**\n",
    "\n",
    "**Step 1: Compute alignment scores** (how well encoder state $h_i^{enc}$ matches decoder state $h_{t-1}^{dec}$)\n",
    "\n",
    "$$\n",
    "e_{t,i} = v_a^T \\tanh(W_a h_{t-1}^{dec} + U_a h_i^{enc})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $W_a \\in \\mathbb{R}^{d_a \\times d_h}$ : Decoder state projection\n",
    "- $U_a \\in \\mathbb{R}^{d_a \\times d_h}$ : Encoder state projection\n",
    "- $v_a \\in \\mathbb{R}^{d_a}$ : Attention vector\n",
    "- $d_a$ : Attention dimension (e.g., 128)\n",
    "- $d_h$ : Hidden dimension (e.g., 512)\n",
    "\n",
    "**Step 2: Normalize to attention weights** (via softmax)\n",
    "\n",
    "$$\n",
    "\\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_{j=1}^{T} \\exp(e_{t,j})}\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "- $\\alpha_{t,i} \\in [0, 1]$ : Probability of attending to timestep $i$\n",
    "- $\\sum_{i=1}^{T} \\alpha_{t,i} = 1$ : Weights sum to 1\n",
    "\n",
    "**Step 3: Compute context vector** (weighted sum of encoder states)\n",
    "\n",
    "$$\n",
    "c_t = \\sum_{i=1}^{T} \\alpha_{t,i} h_i^{enc}\n",
    "$$\n",
    "\n",
    "**Step 4: Decoder with context**\n",
    "\n",
    "$$\n",
    "h_t^{dec} = \\text{LSTM}_{dec}([y_{t-1}; c_t], h_{t-1}^{dec})\n",
    "$$\n",
    "\n",
    "where $[y_{t-1}; c_t]$ = concatenation of previous output and context\n",
    "\n",
    "**Step 5: Output distribution**\n",
    "\n",
    "$$\n",
    "P(y_t | y_1, ..., y_{t-1}, X) = \\text{softmax}(W_{out} h_t^{dec} + b_{out})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcca Attention Example: Test Data \u2192 Report\n",
    "\n",
    "**Encoder hidden states (20 cycles):**\n",
    "```\n",
    "h\u2081\u1d49\u207f\u1d9c = [0.12, -0.34, 0.56, ...] (512-dim) \u2190 Cycle 1 info\n",
    "h\u2082\u1d49\u207f\u1d9c = [0.15, -0.31, 0.58, ...] (512-dim) \u2190 Cycle 2 info\n",
    "...\n",
    "h\u2082\u2080\u1d49\u207f\u1d9c = [-0.22, 0.45, -0.67, ...] (512-dim) \u2190 Cycle 20 info\n",
    "```\n",
    "\n",
    "**Decoder timestep t=3 (generating word \"voltage\"):**\n",
    "\n",
    "**Step 1: Compute alignment scores with all encoder timesteps**\n",
    "\n",
    "```python\n",
    "# Current decoder state\n",
    "h\u2082\u1d48\u1d49\u1d9c = [...] (512-dim, after generating \"Device shows\")\n",
    "\n",
    "# Alignment scores\n",
    "e\u2083,\u2081 = score(h\u2082\u1d48\u1d49\u1d9c, h\u2081\u1d49\u207f\u1d9c) = 0.5   \u2190 Low (cycle 1 not relevant for \"voltage\")\n",
    "e\u2083,\u2082 = score(h\u2082\u1d48\u1d49\u1d9c, h\u2082\u1d49\u207f\u1d9c) = 0.6\n",
    "...\n",
    "e\u2083,\u2081\u2080 = score(h\u2082\u1d48\u1d49\u1d9c, h\u2081\u2080\u1d49\u207f\u1d9c) = 2.8  \u2190 High! (Vdd starts dropping at cycle 10)\n",
    "e\u2083,\u2081\u2081 = score(h\u2082\u1d48\u1d49\u1d9c, h\u2081\u2081\u1d49\u207f\u1d9c) = 2.5\n",
    "...\n",
    "e\u2083,\u2082\u2080 = score(h\u2082\u1d48\u1d49\u1d9c, h\u2082\u2080\u1d49\u207f\u1d9c) = 1.2\n",
    "```\n",
    "\n",
    "**Step 2: Softmax to get attention weights**\n",
    "\n",
    "```python\n",
    "\u03b1\u2083,\u2081 = exp(0.5) / Z = 0.02   \u2190 2% attention to cycle 1\n",
    "\u03b1\u2083,\u2082 = exp(0.6) / Z = 0.03\n",
    "...\n",
    "\u03b1\u2083,\u2081\u2080 = exp(2.8) / Z = 0.35  \u2190 35% attention to cycle 10 (voltage drop starts!)\n",
    "\u03b1\u2083,\u2081\u2081 = exp(2.5) / Z = 0.28\n",
    "...\n",
    "\u03b1\u2083,\u2082\u2080 = exp(1.2) / Z = 0.08\n",
    "```\n",
    "\n",
    "**Step 3: Compute context (weighted sum)**\n",
    "\n",
    "```python\n",
    "c\u2083 = 0.02\u00b7h\u2081\u1d49\u207f\u1d9c + 0.03\u00b7h\u2082\u1d49\u207f\u1d9c + ... + 0.35\u00b7h\u2081\u2080\u1d49\u207f\u1d9c + 0.28\u00b7h\u2081\u2081\u1d49\u207f\u1d9c + ... + 0.08\u00b7h\u2082\u2080\u1d49\u207f\u1d9c\n",
    "```\n",
    "\n",
    "Result: $c_3$ heavily weighted toward cycles 10-15 (where voltage degradation occurs)!\n",
    "\n",
    "**Step 4: Generate next word**\n",
    "\n",
    "```python\n",
    "h\u2083\u1d48\u1d49\u1d9c = LSTM([embedding(\"shows\"); c\u2083], h\u2082\u1d48\u1d49\u1d9c)\n",
    "P(y\u2083) = softmax(W\u00b7h\u2083\u1d48\u1d49\u1d9c)\n",
    "y\u2083 = \"voltage\"  \u2190 Informed by cycles 10-15 specifically!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Attention Variants\n",
    "\n",
    "### **1. Bahdanau (Additive) Attention** (described above)\n",
    "\n",
    "$$\n",
    "e_{t,i} = v_a^T \\tanh(W_a h_{t-1}^{dec} + U_a h_i^{enc})\n",
    "$$\n",
    "\n",
    "**Pros:** Learns complex alignment, works well\n",
    "**Cons:** Computationally expensive (matrix multiplications + tanh)\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Luong (Multiplicative) Attention**\n",
    "\n",
    "**Dot product attention:**\n",
    "\n",
    "$$\n",
    "e_{t,i} = h_{t-1}^{dec} \\cdot h_i^{enc}\n",
    "$$\n",
    "\n",
    "**Scaled dot product** (prevent large values):\n",
    "\n",
    "$$\n",
    "e_{t,i} = \\frac{h_{t-1}^{dec} \\cdot h_i^{enc}}{\\sqrt{d_h}}\n",
    "$$\n",
    "\n",
    "**General attention** (with learned weight matrix):\n",
    "\n",
    "$$\n",
    "e_{t,i} = h_{t-1}^{dec}^T W_a h_i^{enc}\n",
    "$$\n",
    "\n",
    "**Pros:** Faster (dot product is efficient)\n",
    "**Cons:** Less expressive than additive\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Self-Attention** (Foundation for Transformers)\n",
    "\n",
    "Attend to other positions in the **same sequence**:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $Q$ (Query) = $h_i^{enc} W_Q$ : \"What am I looking for?\"\n",
    "- $K$ (Key) = $h_j^{enc} W_K$ : \"What do I contain?\"\n",
    "- $V$ (Value) = $h_j^{enc} W_V$ : \"What information do I provide?\"\n",
    "\n",
    "**Example:** Sentence \"Device shows voltage drop\"\n",
    "- \"voltage\" attends to \"drop\" (semantic relationship)\n",
    "- \"Device\" attends to \"voltage\" and \"drop\" (subject-object relationship)\n",
    "\n",
    "**Used in:** Transformers (BERT, GPT), next notebook!\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd0d Teacher Forcing\n",
    "\n",
    "### **Training Strategy**\n",
    "\n",
    "**Problem during training:** If model generates wrong word at step $t$, error propagates to step $t+1, t+2, ...$\n",
    "\n",
    "**Solution:** Use **ground truth** previous token during training:\n",
    "\n",
    "$$\n",
    "h_t^{dec} = \\text{LSTM}(y_{t-1}^{\\text{true}}, h_{t-1}^{dec}) \\quad \\text{(not } y_{t-1}^{\\text{predicted}}\\text{)}\n",
    "$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "Target: \"Device shows voltage drop\"\n",
    "\n",
    "Without teacher forcing (wrong path):\n",
    "  Step 1: Predict \"Device\" \u2713\n",
    "  Step 2: Predict \"has\" \u2717 (wrong!)\n",
    "  Step 3: Input \"has\" \u2192 Predict \"been\" \u2717 (compounding error)\n",
    "  Step 4: Input \"been\" \u2192 Predict \"tested\" \u2717\n",
    "\n",
    "With teacher forcing (corrects at each step):\n",
    "  Step 1: Predict \"Device\" \u2713\n",
    "  Step 2: Input \"Device\" (truth) \u2192 Predict \"shows\" \u2713\n",
    "  Step 3: Input \"shows\" (truth) \u2192 Predict \"voltage\" \u2713\n",
    "  Step 4: Input \"voltage\" (truth) \u2192 Predict \"drop\" \u2713\n",
    "```\n",
    "\n",
    "**Trade-off:**\n",
    "- **Training:** Fast convergence (use teacher forcing 100%)\n",
    "- **Inference:** No ground truth available (use model's own predictions)\n",
    "- **Solution:** Scheduled sampling (gradually reduce teacher forcing ratio from 100% \u2192 0%)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd0e Beam Search Decoding\n",
    "\n",
    "### **Greedy Decoding Problem**\n",
    "\n",
    "**Greedy:** Select most probable word at each step\n",
    "\n",
    "$$\n",
    "y_t = \\arg\\max_{y} P(y | y_1, ..., y_{t-1}, X)\n",
    "$$\n",
    "\n",
    "**Problem:** Locally optimal but globally suboptimal\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Step 1: \"Device\" (P=0.7) vs \"The\" (P=0.6)\n",
    "  \u2192 Greedy picks \"Device\"\n",
    "\n",
    "Step 2 (given \"Device\"):\n",
    "  \"shows\" (P=0.3) \u2192 Total: 0.7 \u00d7 0.3 = 0.21\n",
    "\n",
    "Step 2 (given \"The\"):\n",
    "  \"device\" (P=0.8) \u2192 Total: 0.6 \u00d7 0.8 = 0.48 (BETTER!)\n",
    "\n",
    "But greedy already committed to \"Device\" at step 1!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Beam Search Solution**\n",
    "\n",
    "Keep top-$k$ most probable **sequences** (not just next words):\n",
    "\n",
    "**Algorithm (beam size $k=3$):**\n",
    "\n",
    "```\n",
    "Step 1: Generate top-3 first words\n",
    "  Beam: [\"Device\" (0.7), \"The\" (0.6), \"Test\" (0.5)]\n",
    "\n",
    "Step 2: Expand each beam candidate (3 \u00d7 vocab_size options)\n",
    "  From \"Device\": [\"Device shows\" (0.21), \"Device has\" (0.14), \"Device exhibits\" (0.10)]\n",
    "  From \"The\": [\"The device\" (0.48), \"The test\" (0.15), \"The failure\" (0.09)]\n",
    "  From \"Test\": [\"Test results\" (0.25), \"Test shows\" (0.12), \"Test indicates\" (0.08)]\n",
    "  \n",
    "  Keep top-3: [\"The device\" (0.48), \"Test results\" (0.25), \"Device shows\" (0.21)]\n",
    "\n",
    "Step 3: Expand again...\n",
    "  \u2192 Continue until all beams generate <EOS> or max_length reached\n",
    "\n",
    "Final: Return highest-scoring complete sequence\n",
    "```\n",
    "\n",
    "**Beam size trade-off:**\n",
    "- $k=1$ : Greedy (fast, suboptimal)\n",
    "- $k=5$ : Good balance (2-3 BLEU points better than greedy)\n",
    "- $k=20$ : Minimal gains (diminishing returns, 20\u00d7 slower)\n",
    "\n",
    "**Typical values:** $k=3$ to $k=10$\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udccf Evaluation Metrics\n",
    "\n",
    "### **1. BLEU Score** (Bilingual Evaluation Understudy)\n",
    "\n",
    "Measures n-gram overlap between generated and reference text:\n",
    "\n",
    "$$\n",
    "\\text{BLEU} = \\text{BP} \\times \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $p_n$ = Precision of n-grams (unigrams, bigrams, trigrams, 4-grams)\n",
    "- $w_n$ = Weight (typically uniform: $w_n = 1/N$)\n",
    "- BP = Brevity penalty (penalize short outputs)\n",
    "\n",
    "**Interpretation:**\n",
    "- BLEU = 1.0 : Perfect match\n",
    "- BLEU = 0.5 : Moderate quality\n",
    "- BLEU = 0.3 : Poor quality\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Perplexity**\n",
    "\n",
    "Measure of model confidence (lower is better):\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = \\exp\\left(-\\frac{1}{T} \\sum_{t=1}^{T} \\log P(y_t | y_1, ..., y_{t-1}, X)\\right)\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- Perplexity = 10 : Model confident (on average, 10 choices per word)\n",
    "- Perplexity = 100 : Model uncertain (100 plausible choices)\n",
    "- Perplexity = 1 : Perfect (always predicts correct word)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Summary: Vanilla vs Attention\n",
    "\n",
    "| Aspect | Vanilla Seq2Seq | Seq2Seq + Attention |\n",
    "|--------|-----------------|---------------------|\n",
    "| **Context** | Single vector $c$ | Dynamic $c_t$ per timestep |\n",
    "| **Bottleneck** | \u2717 Yes (fixed-size) | \u2713 No (access all encoder states) |\n",
    "| **Long sequences** | Poor (info loss) | Excellent (direct access) |\n",
    "| **Interpretability** | \u2717 Black box | \u2713 Attention weights show focus |\n",
    "| **Parameters** | Fewer | More (attention params) |\n",
    "| **Speed** | Faster | Slower (compute attention) |\n",
    "| **BLEU (T=50)** | 0.25 | 0.42 (+68%!) |\n",
    "\n",
    "**Recommendation:** Always use attention for production seq2seq models!\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\ude80 Next Steps\n",
    "\n",
    "Now let's implement:\n",
    "1. Synthetic test data \u2192 failure report dataset\n",
    "2. Vanilla seq2seq (baseline)\n",
    "3. Seq2seq with Bahdanau attention\n",
    "4. Beam search decoding\n",
    "5. Attention visualization (heatmaps)\n",
    "6. Real-world projects\n",
    "\n",
    "Let's code! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c2f07",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca210f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# PART 2: DATA GENERATION\n",
    "# Test Sequences \u2192 Failure Report Pairs\n",
    "# ========================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from collections import Counter\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATING SEQ2SEQ DATASET\")\n",
    "print(\"=\" * 60)\n",
    "# ========================================\n",
    "# VOCABULARY SETUP\n",
    "# ========================================\n",
    "# Template failure report patterns\n",
    "REPORT_TEMPLATES = [\n",
    "    \"device shows {param} {direction} from {start}to {end} over cycles {start_cycle} to {end_cycle} with {additional} root cause {cause} recommendation {action}\",\n",
    "    \"parametric test reveals {param} degradation {start}to {end} {additional} failure mode {cause} suggest {action}\",\n",
    "    \"observed {param} drift {direction} starting cycle {start_cycle} magnitude {delta} indicates {cause} action {action}\",\n",
    "    \"test sequence shows {param} anomaly from {start}to {end} cycles {start_cycle} dash {end_cycle} likely {cause} bin as {action}\",\n",
    "    \"device exhibits {param} variation {direction} amplitude {delta} timeframe cycles {start_cycle} to {end_cycle} diagnosis {cause} {action}\"\n",
    "]\n",
    "# Parameter names\n",
    "PARAM_NAMES = ['voltage', 'current', 'frequency', 'power', 'temperature', \n",
    "               'leakage', 'timing', 'jitter', 'noise']\n",
    "# Directions\n",
    "DIRECTIONS = ['increase', 'decrease', 'drop', 'rise', 'drift']\n",
    "# Root causes\n",
    "CAUSES = ['oxide_degradation', 'metal_migration', 'hot_carrier_injection', \n",
    "          'time_dependent_dielectric_breakdown', 'thermal_stress',\n",
    "          'electromigration', 'process_variation', 'defect_induced']\n",
    "# Actions\n",
    "ACTIONS = ['reliability_fail', 'performance_fail', 'quarantine', \n",
    "           'extended_test', 'engineering_analysis', 'rework']\n",
    "# Additional descriptors\n",
    "ADDITIONALS = ['accompanied_by', 'correlated_with', 'combined_with', 'along_with']\n",
    "# Build vocabulary\n",
    "vocab_words = (PARAM_NAMES + DIRECTIONS + CAUSES + ACTIONS + ADDITIONALS +\n",
    "               ['device', 'shows', 'from', 'to', 'over', 'cycles', 'with', \n",
    "                'root', 'cause', 'recommendation', 'parametric', 'test', \n",
    "                'reveals', 'degradation', 'failure', 'mode', 'suggest',\n",
    "                'observed', 'drift', 'starting', 'cycle', 'magnitude',\n",
    "                'indicates', 'action', 'sequence', 'anomaly', 'dash',\n",
    "                'likely', 'bin', 'as', 'exhibits', 'variation', 'amplitude',\n",
    "                'timeframe', 'diagnosis'] +\n",
    "               [str(i) for i in range(0, 21)] +  # cycle numbers\n",
    "               [f'{i//10}.{i%10}' for i in range(0, 30)] +  # voltage values\n",
    "               ['<PAD>', '<SOS>', '<EOS>', '<UNK>'])  # special tokens\n",
    "vocab = {word: idx for idx, word in enumerate(set(vocab_words))}\n",
    "idx2word = {idx: word for word, idx in vocab.items()}\n",
    "VOCAB_SIZE = len(vocab)\n",
    "PAD_IDX = vocab['<PAD>']\n",
    "SOS_IDX = vocab['<SOS>']\n",
    "EOS_IDX = vocab['<EOS>']\n",
    "UNK_IDX = vocab['<UNK>']\n",
    "print(f\"\\nVocabulary size: {VOCAB_SIZE}\")\n",
    "print(f\"Special tokens:\")\n",
    "print(f\"  <PAD>: {PAD_IDX}\")\n",
    "print(f\"  <SOS>: {SOS_IDX}\")\n",
    "print(f\"  <EOS>: {EOS_IDX}\")\n",
    "print(f\"  <UNK>: {UNK_IDX}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344a2ce0",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421e99c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# GENERATE DATASET\n",
    "# ========================================\n",
    "def generate_test_sequence(seq_length=20, num_features=15):\n",
    "    \"\"\"Generate synthetic parametric test sequence with degradation pattern\"\"\"\n",
    "    \n",
    "    base_values = np.array([\n",
    "        1.05, 250, 2.4, 0.6, 75, 10, 50, 50, 100, 100, 20, -80, 1.0, 40, -9\n",
    "    ])\n",
    "    \n",
    "    sequence = np.zeros((seq_length, num_features))\n",
    "    \n",
    "    # Determine failure type and parameters\n",
    "    fail_param_idx = random.randint(0, 8)  # Which parameter fails\n",
    "    fail_start_cycle = random.randint(8, 15)  # When degradation starts\n",
    "    fail_direction = random.choice([-1, 1])  # Increase or decrease\n",
    "    \n",
    "    for t in range(seq_length):\n",
    "        if t < fail_start_cycle:\n",
    "            # Normal operation\n",
    "            noise = np.random.normal(0, 0.01, num_features)\n",
    "            sequence[t] = base_values + noise\n",
    "        else:\n",
    "            # Degradation after fail_start_cycle\n",
    "            drift_factor = ((t - fail_start_cycle) / (seq_length - fail_start_cycle)) ** 1.5\n",
    "            \n",
    "            drift = np.zeros(num_features)\n",
    "            drift[fail_param_idx] = fail_direction * 0.1 * drift_factor\n",
    "            \n",
    "            noise = np.random.normal(0, 0.02, num_features)\n",
    "            sequence[t] = base_values + drift + noise\n",
    "    \n",
    "    metadata = {\n",
    "        'param_idx': fail_param_idx,\n",
    "        'param_name': PARAM_NAMES[min(fail_param_idx, len(PARAM_NAMES)-1)],\n",
    "        'start_cycle': fail_start_cycle,\n",
    "        'end_cycle': seq_length - 1,\n",
    "        'direction': 'increase' if fail_direction > 0 else 'decrease',\n",
    "        'start_value': sequence[fail_start_cycle, fail_param_idx],\n",
    "        'end_value': sequence[-1, fail_param_idx],\n",
    "        'delta': abs(sequence[-1, fail_param_idx] - sequence[fail_start_cycle, fail_param_idx])\n",
    "    }\n",
    "    \n",
    "    return sequence, metadata\n",
    "def generate_failure_report(metadata):\n",
    "    \"\"\"Generate natural language failure report from metadata\"\"\"\n",
    "    \n",
    "    template = random.choice(REPORT_TEMPLATES)\n",
    "    \n",
    "    # Format values\n",
    "    start_val = f\"{metadata['start_value']:.2f}\"\n",
    "    end_val = f\"{metadata['end_value']:.2f}\"\n",
    "    delta_val = f\"{metadata['delta']:.2f}\"\n",
    "    \n",
    "    # Fill template\n",
    "    report = template.format(\n",
    "        param=metadata['param_name'],\n",
    "        direction=metadata['direction'],\n",
    "        start=start_val,\n",
    "        end=end_val,\n",
    "        start_cycle=str(metadata['start_cycle']),\n",
    "        end_cycle=str(metadata['end_cycle']),\n",
    "        delta=delta_val,\n",
    "        additional=random.choice(ADDITIONALS),\n",
    "        cause=random.choice(CAUSES),\n",
    "        action=random.choice(ACTIONS)\n",
    "    )\n",
    "    \n",
    "    return report\n",
    "# Generate dataset\n",
    "NUM_SAMPLES = 10000\n",
    "SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 15\n",
    "print(f\"\\nGenerating {NUM_SAMPLES} sequence-to-sequence pairs...\")\n",
    "sequences = []\n",
    "reports = []\n",
    "for i in range(NUM_SAMPLES):\n",
    "    seq, metadata = generate_test_sequence(SEQ_LENGTH, NUM_FEATURES)\n",
    "    report = generate_failure_report(metadata)\n",
    "    \n",
    "    sequences.append(seq)\n",
    "    reports.append(report)\n",
    "    \n",
    "    if (i + 1) % 2000 == 0:\n",
    "        print(f\"  Generated {i+1}/{NUM_SAMPLES} samples\")\n",
    "print(f\"\u2713 Generated {len(sequences)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc4fed8",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31997088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# TOKENIZATION\n",
    "# ========================================\n",
    "def tokenize_report(report):\n",
    "    \"\"\"Convert report string to token indices\"\"\"\n",
    "    tokens = report.lower().replace('_', ' ').split()\n",
    "    indices = [SOS_IDX] + [vocab.get(token, UNK_IDX) for token in tokens] + [EOS_IDX]\n",
    "    return indices\n",
    "# Tokenize all reports\n",
    "tokenized_reports = [tokenize_report(report) for report in reports]\n",
    "# Statistics\n",
    "report_lengths = [len(report) for report in tokenized_reports]\n",
    "print(f\"\\nReport statistics:\")\n",
    "print(f\"  Min length:  {min(report_lengths)}\")\n",
    "print(f\"  Max length:  {max(report_lengths)}\")\n",
    "print(f\"  Mean length: {np.mean(report_lengths):.1f}\")\n",
    "print(f\"  Median:      {np.median(report_lengths):.1f}\")\n",
    "# Set max length (cover 95% of reports)\n",
    "MAX_REPORT_LENGTH = int(np.percentile(report_lengths, 95))\n",
    "print(f\"  Max length (95th percentile): {MAX_REPORT_LENGTH}\")\n",
    "# ========================================\n",
    "# EXAMPLES\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXAMPLE SEQUENCE-TO-REPORT PAIRS\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Input sequence shape: {sequences[i].shape}\")\n",
    "    print(f\"  First 3 cycles:\")\n",
    "    for t in range(3):\n",
    "        print(f\"    Cycle {t+1}: Vdd={sequences[i][t,0]:.3f}, Idd={sequences[i][t,1]:.1f}, Temp={sequences[i][t,4]:.1f}\")\n",
    "    print(f\"  Last 3 cycles:\")\n",
    "    for t in range(17, 20):\n",
    "        print(f\"    Cycle {t+1}: Vdd={sequences[i][t,0]:.3f}, Idd={sequences[i][t,1]:.1f}, Temp={sequences[i][t,4]:.1f}\")\n",
    "    print(f\"\\n  Output report:\")\n",
    "    print(f\"    \\\"{reports[i]}\\\"\")\n",
    "    print(f\"  Tokenized ({len(tokenized_reports[i])} tokens):\")\n",
    "    print(f\"    {tokenized_reports[i][:15]}...\")\n",
    "# ========================================\n",
    "# PYTORCH DATASET\n",
    "# ========================================\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, sequences, tokenized_reports, max_report_length):\n",
    "        self.sequences = torch.FloatTensor(np.array(sequences))\n",
    "        self.reports = tokenized_reports\n",
    "        self.max_length = max_report_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        report = self.reports[idx]\n",
    "        \n",
    "        # Pad report to max_length\n",
    "        if len(report) < self.max_length:\n",
    "            report = report + [PAD_IDX] * (self.max_length - len(report))\n",
    "        else:\n",
    "            report = report[:self.max_length]\n",
    "        \n",
    "        return seq, torch.LongTensor(report)\n",
    "# Train/val/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_seqs, temp_seqs, train_reports, temp_reports = train_test_split(\n",
    "    sequences, tokenized_reports, test_size=0.3, random_state=42\n",
    ")\n",
    "val_seqs, test_seqs, val_reports, test_reports = train_test_split(\n",
    "    temp_seqs, temp_reports, test_size=0.5, random_state=42\n",
    ")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET SPLIT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train: {len(train_seqs):,} samples\")\n",
    "print(f\"Val:   {len(val_seqs):,} samples\")\n",
    "print(f\"Test:  {len(test_seqs):,} samples\")\n",
    "# Create datasets\n",
    "train_dataset = Seq2SeqDataset(train_seqs, train_reports, MAX_REPORT_LENGTH)\n",
    "val_dataset = Seq2SeqDataset(val_seqs, val_reports, MAX_REPORT_LENGTH)\n",
    "test_dataset = Seq2SeqDataset(test_seqs, test_reports, MAX_REPORT_LENGTH)\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(f\"\\nDataLoaders created:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches:   {len(val_loader)}\")\n",
    "print(f\"  Test batches:  {len(test_loader)}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2713 Data preparation complete!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd618a2",
   "metadata": {},
   "source": [
    "# \ud83e\udde0 Part 3: Model Implementations\n",
    "\n",
    "## \ud83d\udcdd Architecture Overview\n",
    "\n",
    "We'll build and compare:\n",
    "\n",
    "1. **Vanilla Seq2Seq** (baseline with fixed context bottleneck)\n",
    "2. **Seq2Seq + Bahdanau Attention** (dynamic context, best performance)\n",
    "3. **Beam Search Decoder** (improve generation quality)\n",
    "\n",
    "**Model Configuration:**\n",
    "- Encoder: LSTM (input_size=15, hidden_size=256, num_layers=2)\n",
    "- Decoder: LSTM (input_size=embedding_dim, hidden_size=256, num_layers=2)\n",
    "- Embedding: 128-dimensional word embeddings\n",
    "- Attention: Bahdanau (additive) with attention_dim=128\n",
    "- Optimizer: Adam (lr=0.001)\n",
    "- Loss: CrossEntropyLoss (ignore PAD tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b02fa8f",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ffdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# PART 3: SEQ2SEQ WITH ATTENTION\n",
    "# Complete Implementation\n",
    "# ========================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "# ========================================\n",
    "# ENCODER\n",
    "# ========================================\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Encoder: Parametric test sequence \u2192 Hidden states\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            outputs: (batch, seq_len, hidden_size) - all hidden states\n",
    "            hidden: tuple of (h_n, c_n) where each is (num_layers, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        outputs, hidden = self.lstm(x)\n",
    "        return outputs, hidden\n",
    "# ========================================\n",
    "# BAHDANAU ATTENTION\n",
    "# ========================================\n",
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Additive attention mechanism\n",
    "    Score: v_a^T tanh(W_a h_dec + U_a h_enc)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, attention_dim):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        \n",
    "        self.W_a = nn.Linear(hidden_size, attention_dim)  # Decoder projection\n",
    "        self.U_a = nn.Linear(hidden_size, attention_dim)  # Encoder projection\n",
    "        self.v_a = nn.Linear(attention_dim, 1)            # Attention vector\n",
    "    \n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            decoder_hidden: (batch, hidden_size) - current decoder state\n",
    "            encoder_outputs: (batch, seq_len, hidden_size) - all encoder states\n",
    "        \n",
    "        Returns:\n",
    "            context: (batch, hidden_size) - weighted sum of encoder outputs\n",
    "            attention_weights: (batch, seq_len) - attention distribution\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        \n",
    "        # Expand decoder hidden to match seq_len\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1)  # (batch, 1, hidden)\n",
    "        decoder_hidden = decoder_hidden.repeat(1, seq_len, 1)  # (batch, seq_len, hidden)\n",
    "        \n",
    "        # Compute alignment scores\n",
    "        # energy: (batch, seq_len, attention_dim)\n",
    "        energy = torch.tanh(self.W_a(decoder_hidden) + self.U_a(encoder_outputs))\n",
    "        \n",
    "        # scores: (batch, seq_len)\n",
    "        scores = self.v_a(energy).squeeze(2)\n",
    "        \n",
    "        # Attention weights (softmax over seq_len)\n",
    "        attention_weights = F.softmax(scores, dim=1)\n",
    "        \n",
    "        # Context vector (weighted sum)\n",
    "        # context: (batch, hidden_size)\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        \n",
    "        return context, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c58fce",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef2dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# DECODER WITH ATTENTION\n",
    "# ========================================\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Decoder with Bahdanau Attention\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=2, \n",
    "                 attention_dim=128, dropout=0.2):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Word embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_IDX)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = BahdanauAttention(hidden_size, attention_dim)\n",
    "        \n",
    "        # LSTM (input = embedding + context)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim + hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input_token, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Single-step decoding\n",
    "        \n",
    "        Args:\n",
    "            input_token: (batch,) - current token\n",
    "            hidden: tuple of (h, c) from previous step\n",
    "            encoder_outputs: (batch, seq_len, hidden_size)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, vocab_size) - logits for next token\n",
    "            hidden: tuple of (h, c) for next step\n",
    "            attention_weights: (batch, seq_len)\n",
    "        \"\"\"\n",
    "        # Embedding\n",
    "        embedded = self.embedding(input_token)  # (batch, embedding_dim)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Get last layer hidden state for attention\n",
    "        # hidden[0] shape: (num_layers, batch, hidden_size)\n",
    "        decoder_hidden = hidden[0][-1]  # (batch, hidden_size)\n",
    "        \n",
    "        # Compute attention context\n",
    "        context, attention_weights = self.attention(decoder_hidden, encoder_outputs)\n",
    "        \n",
    "        # Concatenate embedding and context\n",
    "        lstm_input = torch.cat([embedded, context], dim=1)  # (batch, embedding_dim + hidden_size)\n",
    "        lstm_input = lstm_input.unsqueeze(1)  # (batch, 1, embedding_dim + hidden_size)\n",
    "        \n",
    "        # LSTM step\n",
    "        output, hidden = self.lstm(lstm_input, hidden)\n",
    "        \n",
    "        # Remove time dimension\n",
    "        output = output.squeeze(1)  # (batch, hidden_size)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        output = self.fc_out(output)  # (batch, vocab_size)\n",
    "        \n",
    "        return output, hidden, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acb02ca",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519ddf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# SEQ2SEQ MODEL\n",
    "# ========================================\n",
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Seq2Seq model with attention\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2SeqWithAttention, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: (batch, src_seq_len, input_size) - encoder input\n",
    "            trg: (batch, trg_seq_len) - decoder target tokens\n",
    "            teacher_forcing_ratio: probability of using ground truth\n",
    "        \n",
    "        Returns:\n",
    "            outputs: (batch, trg_seq_len, vocab_size)\n",
    "            attention_weights_all: list of (batch, src_seq_len) per timestep\n",
    "        \"\"\"\n",
    "        batch_size = src.size(0)\n",
    "        trg_seq_len = trg.size(1)\n",
    "        vocab_size = self.decoder.vocab_size\n",
    "        \n",
    "        # Encode source sequence\n",
    "        encoder_outputs, encoder_hidden = self.encoder(src)\n",
    "        \n",
    "        # Initialize decoder hidden state with encoder's final hidden state\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        # Start with SOS token\n",
    "        decoder_input = trg[:, 0]  # (batch,) - should be SOS_IDX\n",
    "        \n",
    "        # Store outputs and attention weights\n",
    "        outputs = torch.zeros(batch_size, trg_seq_len, vocab_size).to(device)\n",
    "        attention_weights_all = []\n",
    "        \n",
    "        # Decode step by step\n",
    "        for t in range(1, trg_seq_len):\n",
    "            # Decoder step\n",
    "            output, decoder_hidden, attention_weights = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            \n",
    "            # Store output and attention\n",
    "            outputs[:, t, :] = output\n",
    "            attention_weights_all.append(attention_weights)\n",
    "            \n",
    "            # Teacher forcing\n",
    "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "            if use_teacher_forcing:\n",
    "                decoder_input = trg[:, t]  # Ground truth\n",
    "            else:\n",
    "                decoder_input = output.argmax(dim=1)  # Model's prediction\n",
    "        \n",
    "        return outputs, attention_weights_all\n",
    "# ========================================\n",
    "# INITIALIZE MODELS\n",
    "# ========================================\n",
    "INPUT_SIZE = 15          # 15 parametric features\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "EMBEDDING_DIM = 128\n",
    "ATTENTION_DIM = 128\n",
    "DROPOUT = 0.2\n",
    "encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)\n",
    "decoder = DecoderWithAttention(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_SIZE, \n",
    "                                NUM_LAYERS, ATTENTION_DIM, DROPOUT)\n",
    "model = Seq2SeqWithAttention(encoder, decoder).to(device)\n",
    "# Count parameters\n",
    "encoder_params = sum(p.numel() for p in encoder.parameters())\n",
    "decoder_params = sum(p.numel() for p in decoder.parameters())\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Encoder parameters:  {encoder_params:,}\")\n",
    "print(f\"Decoder parameters:  {decoder_params:,}\")\n",
    "print(f\"Total parameters:    {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdef7205",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff91bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# TRAINING FUNCTION\n",
    "# ========================================\n",
    "def train_epoch(model, dataloader, optimizer, criterion, teacher_forcing_ratio=0.5):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_idx, (src, trg) in enumerate(dataloader):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, _ = model(src, trg, teacher_forcing_ratio)\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        # outputs: (batch, trg_len, vocab_size)\n",
    "        # trg: (batch, trg_len)\n",
    "        output_dim = outputs.shape[-1]\n",
    "        outputs = outputs[:, 1:, :].reshape(-1, output_dim)  # Ignore first (SOS)\n",
    "        trg = trg[:, 1:].reshape(-1)  # Ignore first (SOS)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, trg)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(dataloader)\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, trg in dataloader:\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            \n",
    "            # Forward pass (no teacher forcing during eval)\n",
    "            outputs, _ = model(src, trg, teacher_forcing_ratio=0)\n",
    "            \n",
    "            # Reshape for loss\n",
    "            output_dim = outputs.shape[-1]\n",
    "            outputs = outputs[:, 1:, :].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(outputs, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(dataloader)\n",
    "# ========================================\n",
    "# TRAINING LOOP\n",
    "# ========================================\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "NUM_EPOCHS = 15\n",
    "TEACHER_FORCING_RATIO = 0.5\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING SEQ2SEQ WITH ATTENTION\")\n",
    "print(\"=\" * 60)\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "training_start = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, TEACHER_FORCING_RATIO)\n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_seq2seq_attention.pth')\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} ({epoch_time:.1f}s)\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "training_time = time.time() - training_start\n",
    "print(f\"\\n\u2713 Training completed in {training_time:.1f} sec ({training_time/60:.1f} min)\")\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_seq2seq_attention.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a6c3e5",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 5\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78141402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# INFERENCE FUNCTION\n",
    "# ========================================\n",
    "def generate_report(model, src_sequence, max_length=50):\n",
    "    \"\"\"\n",
    "    Generate failure report from test sequence\n",
    "    \n",
    "    Args:\n",
    "        src_sequence: (1, seq_len, input_size) - single test sequence\n",
    "        max_length: maximum report length\n",
    "    \n",
    "    Returns:\n",
    "        generated_tokens: list of token indices\n",
    "        attention_weights: list of (1, src_seq_len)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode\n",
    "        encoder_outputs, encoder_hidden = model.encoder(src_sequence)\n",
    "        \n",
    "        # Initialize decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = torch.LongTensor([SOS_IDX]).to(device)\n",
    "        \n",
    "        generated_tokens = [SOS_IDX]\n",
    "        attention_weights = []\n",
    "        \n",
    "        for t in range(max_length):\n",
    "            # Decoder step\n",
    "            output, decoder_hidden, attn_weights = model.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            \n",
    "            # Get next token\n",
    "            next_token = output.argmax(dim=1).item()\n",
    "            generated_tokens.append(next_token)\n",
    "            attention_weights.append(attn_weights.cpu().numpy())\n",
    "            \n",
    "            # Stop if EOS\n",
    "            if next_token == EOS_IDX:\n",
    "                break\n",
    "            \n",
    "            # Next input\n",
    "            decoder_input = torch.LongTensor([next_token]).to(device)\n",
    "    \n",
    "    return generated_tokens, attention_weights\n",
    "# ========================================\n",
    "# TEST EXAMPLES\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATING FAILURE REPORTS\")\n",
    "print(\"=\" * 60)\n",
    "# Get a few test examples\n",
    "test_examples = [(test_dataset[i][0], test_dataset[i][1]) for i in range(3)]\n",
    "for idx, (src, trg) in enumerate(test_examples):\n",
    "    print(f\"\\nExample {idx+1}:\")\n",
    "    \n",
    "    # Generate report\n",
    "    src_input = src.unsqueeze(0).to(device)  # (1, seq_len, input_size)\n",
    "    generated_tokens, attn_weights = generate_report(model, src_input)\n",
    "    \n",
    "    # Convert tokens to words\n",
    "    generated_words = [idx2word[token] for token in generated_tokens if token not in [PAD_IDX, SOS_IDX, EOS_IDX]]\n",
    "    target_words = [idx2word[token.item()] for token in trg if token.item() not in [PAD_IDX, SOS_IDX, EOS_IDX]]\n",
    "    \n",
    "    print(f\"  Generated: {' '.join(generated_words)}\")\n",
    "    print(f\"  Target:    {' '.join(target_words)}\")\n",
    "    print(f\"  Length:    Generated={len(generated_words)}, Target={len(target_words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8528172",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 6\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# VISUALIZATION: TRAINING CURVES\n",
    "# ========================================\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.plot(train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "ax.plot(val_losses, label='Val Loss', color='red', linewidth=2)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Seq2Seq with Attention: Training Progress\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('seq2seq_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n\u2713 Training curves saved to seq2seq_training_curves.png\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2713 Model training and inference complete!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940c8c0b",
   "metadata": {},
   "source": [
    "# \ud83d\ude80 Part 4: Real-World Projects & Advanced Topics\n",
    "\n",
    "## \ud83d\udd2c Semiconductor Projects (Post-Silicon Validation)\n",
    "\n",
    "### **Project 1: Automated Test Failure Report Generation at Scale**\n",
    "\n",
    "**Objective:** Generate natural language failure reports for 100K daily failures across 5 fabs\n",
    "\n",
    "**Business Value:** $15M-$60M/year from 95% faster failure analysis + knowledge preservation\n",
    "\n",
    "**Production Architecture:**\n",
    "```\n",
    "High-Throughput Pipeline:\n",
    "    Test Data (20 cycles \u00d7 15 params)\n",
    "        \u2193\n",
    "    Encoder: Bidirectional LSTM (512 hidden, 2 layers)\n",
    "        \u2193\n",
    "    Multi-Head Attention (8 heads, 64-dim each)\n",
    "        \u2193\n",
    "    Decoder: LSTM with copy mechanism\n",
    "        \u2193\n",
    "    Generated Report (30-50 tokens)\n",
    "        \u2193\n",
    "    Post-processing: Grammar correction, formatting\n",
    "        \u2193\n",
    "    Human Review: 5% sample audit (confidence < 0.8)\n",
    "```\n",
    "\n",
    "**Advanced Features:**\n",
    "\n",
    "1. **Copy Mechanism** (handle out-of-vocabulary numbers):\n",
    "```python\n",
    "class DecoderWithCopy(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, attention_dim):\n",
    "        super().__init__()\n",
    "        # Standard decoder components\n",
    "        self.attention = BahdanauAttention(hidden_size, attention_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim + hidden_size, hidden_size)\n",
    "        \n",
    "        # Copy mechanism\n",
    "        self.p_gen_linear = nn.Linear(hidden_size * 2 + embedding_dim, 1)\n",
    "    \n",
    "    def forward(self, input_token, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_token)\n",
    "        context, attn_weights = self.attention(hidden[0][-1], encoder_outputs)\n",
    "        \n",
    "        # Standard generation probability\n",
    "        lstm_input = torch.cat([embedded, context], dim=1)\n",
    "        output, hidden = self.lstm(lstm_input.unsqueeze(1), hidden)\n",
    "        \n",
    "        vocab_dist = F.softmax(self.fc_out(output.squeeze(1)), dim=1)\n",
    "        \n",
    "        # Copy probability\n",
    "        p_gen_input = torch.cat([output.squeeze(1), context, embedded], dim=1)\n",
    "        p_gen = torch.sigmoid(self.p_gen_linear(p_gen_input))\n",
    "        \n",
    "        # Final distribution: p_gen * vocab_dist + (1 - p_gen) * attn_weights\n",
    "        # This allows copying numerical values directly from input (e.g., \"1.05V\")\n",
    "        final_dist = p_gen * vocab_dist + (1 - p_gen) * attn_weights\n",
    "        \n",
    "        return final_dist, hidden, attn_weights\n",
    "```\n",
    "\n",
    "2. **Multi-Fab Adaptation** (domain adaptation across fabs):\n",
    "```python\n",
    "# Train base model on Fab A data\n",
    "base_model = Seq2SeqWithAttention(encoder, decoder)\n",
    "train(base_model, fab_a_data)\n",
    "\n",
    "# Fine-tune for Fab B with small dataset (1K samples vs 100K for base)\n",
    "fab_b_model = copy.deepcopy(base_model)\n",
    "\n",
    "# Freeze encoder, only train decoder\n",
    "for param in fab_b_model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Fine-tune on Fab B data\n",
    "train(fab_b_model, fab_b_data, epochs=5, lr=0.0001)\n",
    "\n",
    "# Result: 85% \u2192 92% accuracy with just 1K Fab B samples\n",
    "```\n",
    "\n",
    "3. **Confidence-Based Routing:**\n",
    "```python\n",
    "def generate_with_confidence(model, test_sequence):\n",
    "    generated_tokens, attn_weights = generate_report(model, test_sequence)\n",
    "    \n",
    "    # Compute confidence (average probability of selected tokens)\n",
    "    confidence = compute_generation_confidence(generated_tokens, attn_weights)\n",
    "    \n",
    "    if confidence > 0.8:\n",
    "        return \"AUTO_APPROVED\", generated_tokens\n",
    "    elif confidence > 0.5:\n",
    "        return \"REVIEW_QUEUE\", generated_tokens  # Human audits 20%\n",
    "    else:\n",
    "        return \"MANUAL_REQUIRED\", None  # Fall back to human (5%)\n",
    "```\n",
    "\n",
    "**Deployment Stats:**\n",
    "- **Throughput:** 100K reports/day (vs 50 manually)\n",
    "- **Latency:** <100ms per report (real-time)\n",
    "- **Accuracy:** BLEU=0.42, 88% human-rated quality\n",
    "- **Cost savings:** $15M-$60M/year\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 2: Root Cause Analysis with Attention Visualization**\n",
    "\n",
    "**Objective:** Generate root cause hypotheses with visual explanations (which cycles matter)\n",
    "\n",
    "**Architecture:** Seq2Seq + Attention Heatmaps for interpretability\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "def explain_failure(model, test_sequence):\n",
    "    \"\"\"\n",
    "    Generate report + attention heatmap showing critical cycles\n",
    "    \"\"\"\n",
    "    generated_tokens, attn_weights = generate_report(model, test_sequence)\n",
    "    \n",
    "    # Aggregate attention across all decoder timesteps\n",
    "    avg_attention = np.mean(attn_weights, axis=0)  # (seq_len,)\n",
    "    \n",
    "    # Identify critical cycles (attention > threshold)\n",
    "    threshold = avg_attention.mean() + avg_attention.std()\n",
    "    critical_cycles = np.where(avg_attention > threshold)[0]\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.bar(range(len(avg_attention)), avg_attention, color='skyblue')\n",
    "    plt.axhline(y=threshold, color='red', linestyle='--', label='Critical Threshold')\n",
    "    plt.xlabel(\"Test Cycle\")\n",
    "    plt.ylabel(\"Attention Weight\")\n",
    "    plt.title(\"Root Cause: Model focused on cycles \" + str(critical_cycles.tolist()))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return generated_tokens, critical_cycles\n",
    "\n",
    "# Example output:\n",
    "# Generated Report: \"Device shows voltage drop from 1.05 to 0.98 over cycles 10 to 20\"\n",
    "# Critical Cycles: [10, 11, 12, 13, 14, 19, 20]\n",
    "# \u2192 Attention heatmap confirms model focused on degradation period!\n",
    "```\n",
    "\n",
    "**Business Impact:**\n",
    "- **Faster RCA:** 15 min \u2192 1 sec per failure\n",
    "- **Consistency:** 100% reproducible analysis (vs human variability)\n",
    "- **Explainability:** Visual proof for engineering teams\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 3: Multi-Language Reporting (English + Chinese)**\n",
    "\n",
    "**Objective:** Generate failure reports in both English and Chinese for global fabs\n",
    "\n",
    "**Architecture:** Shared encoder + Language-specific decoders\n",
    "\n",
    "```python\n",
    "class MultilingualSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, english_decoder, chinese_decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoders = {\n",
    "            'en': english_decoder,\n",
    "            'zh': chinese_decoder\n",
    "        }\n",
    "    \n",
    "    def forward(self, src, trg, language='en'):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(src)\n",
    "        decoder = self.decoders[language]\n",
    "        outputs, attn = decoder(trg, encoder_hidden, encoder_outputs)\n",
    "        return outputs, attn\n",
    "\n",
    "# Training: Alternate between English and Chinese batches\n",
    "for epoch in range(num_epochs):\n",
    "    for en_batch in english_loader:\n",
    "        loss_en = train_step(model, en_batch, language='en')\n",
    "    \n",
    "    for zh_batch in chinese_loader:\n",
    "        loss_zh = train_step(model, zh_batch, language='zh')\n",
    "```\n",
    "\n",
    "**Success Metrics:**\n",
    "- English BLEU: 0.42\n",
    "- Chinese BLEU: 0.38\n",
    "- Shared encoder reduces parameters by 40%\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 4: Hierarchical Seq2Seq for Long Test Sequences**\n",
    "\n",
    "**Objective:** Handle 100-cycle test sequences (vs 20 in base model)\n",
    "\n",
    "**Architecture:** Two-level hierarchy\n",
    "- **Level 1:** Compress 100 cycles \u2192 10 segment embeddings (10 cycles each)\n",
    "- **Level 2:** Seq2seq on 10 segments \u2192 Report\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "class HierarchicalEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Low-level encoder (processes 10-cycle segments)\n",
    "        self.segment_encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # High-level encoder (processes segment embeddings)\n",
    "        self.sequence_encoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 100 cycles, input_size)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Split into 10 segments of 10 cycles each\n",
    "        segments = x.view(batch_size, 10, 10, -1)  # (batch, 10 segments, 10 cycles, features)\n",
    "        \n",
    "        # Encode each segment\n",
    "        segment_embeddings = []\n",
    "        for i in range(10):\n",
    "            segment = segments[:, i, :, :]  # (batch, 10 cycles, features)\n",
    "            _, (h_n, _) = self.segment_encoder(segment)\n",
    "            segment_embeddings.append(h_n[-1])  # (batch, hidden)\n",
    "        \n",
    "        # Stack segments\n",
    "        segment_sequence = torch.stack(segment_embeddings, dim=1)  # (batch, 10, hidden)\n",
    "        \n",
    "        # Encode segment sequence\n",
    "        outputs, hidden = self.sequence_encoder(segment_sequence)\n",
    "        \n",
    "        return outputs, hidden\n",
    "```\n",
    "\n",
    "**Performance:**\n",
    "- Handles 100-cycle sequences (5\u00d7 longer)\n",
    "- BLEU: 0.39 (vs 0.42 for 20-cycle base model)\n",
    "- Inference time: 150ms (vs 100ms for base)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf10 General AI/ML Projects\n",
    "\n",
    "### **Project 5: Neural Machine Translation (English \u2194 French)**\n",
    "\n",
    "**Objective:** Translate between languages (e.g., \"Hello world\" \u2192 \"Bonjour le monde\")\n",
    "\n",
    "**Architecture:** Transformer-based Seq2Seq (next notebook!)\n",
    "\n",
    "**Dataset:** WMT14 English-French (40M sentence pairs)\n",
    "\n",
    "**Success Metrics:** BLEU > 30 (state-of-the-art: 42)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 6: Abstractive Text Summarization**\n",
    "\n",
    "**Objective:** Summarize news articles (1000 words \u2192 100-word summary)\n",
    "\n",
    "**Architecture:** Seq2Seq + Pointer-Generator (copy important phrases)\n",
    "\n",
    "**Dataset:** CNN/Daily Mail (300K article-summary pairs)\n",
    "\n",
    "**Challenge:** Factual accuracy (avoid hallucination)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 7: Code Comment Generation**\n",
    "\n",
    "**Objective:** Auto-generate docstrings from code (Python function \u2192 Description)\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "Input (Code):\n",
    "  def calculate_yield(pass_count, total_count):\n",
    "      return pass_count / total_count * 100\n",
    "\n",
    "Output (Comment):\n",
    "  \"Calculate percentage yield by dividing passed devices by total tested devices and multiplying by 100\"\n",
    "```\n",
    "\n",
    "**Dataset:** GitHub CodeSearchNet (2M code-comment pairs)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 8: Dialogue Response Generation (Chatbots)**\n",
    "\n",
    "**Objective:** Generate contextual responses in conversations\n",
    "\n",
    "**Architecture:** Seq2Seq with context encoding (last 3 turns)\n",
    "\n",
    "**Dataset:** Ubuntu Dialogue Corpus (1M conversations)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf93 Key Takeaways & Best Practices\n",
    "\n",
    "### **When to Use Seq2Seq vs Other Architectures**\n",
    "\n",
    "| Task Type | Input | Output | Recommended Architecture |\n",
    "|-----------|-------|--------|--------------------------|\n",
    "| **Classification** | Sequence | Label | RNN/LSTM (Notebook 056) |\n",
    "| **Sequence Tagging** | Sequence | Sequence (same length) | BiLSTM-CRF |\n",
    "| **Sequence Translation** | Sequence | Sequence (different length) | **Seq2Seq + Attention** |\n",
    "| **Long Sequences (>100)** | Long sequence | Sequence | **Transformer** (Notebook 058) |\n",
    "| **Real-time (latency < 10ms)** | Sequence | Sequence | Lightweight Seq2Seq (GRU, small hidden) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Best Practices**\n",
    "\n",
    "**1. Teacher Forcing Schedule:**\n",
    "```python\n",
    "def get_teacher_forcing_ratio(epoch, total_epochs):\n",
    "    \"\"\"Linearly decay from 1.0 to 0.0\"\"\"\n",
    "    return max(0.0, 1.0 - epoch / total_epochs)\n",
    "\n",
    "# Gradually reduce reliance on ground truth\n",
    "for epoch in range(num_epochs):\n",
    "    tf_ratio = get_teacher_forcing_ratio(epoch, num_epochs)\n",
    "    train_epoch(model, train_loader, optimizer, criterion, tf_ratio)\n",
    "```\n",
    "\n",
    "**2. Gradient Clipping (prevent exploding gradients):**\n",
    "```python\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "```\n",
    "\n",
    "**3. Learning Rate Warm-up:**\n",
    "```python\n",
    "def get_lr(step, d_model=512, warmup_steps=4000):\n",
    "    \"\"\"Transformer learning rate schedule\"\"\"\n",
    "    return d_model**(-0.5) * min(step**(-0.5), step * warmup_steps**(-1.5))\n",
    "```\n",
    "\n",
    "**4. Dropout for Regularization:**\n",
    "```python\n",
    "self.dropout = nn.Dropout(0.2)  # 20% dropout\n",
    "embedded = self.dropout(self.embedding(input_token))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Inference Optimization**\n",
    "\n",
    "**1. Beam Search Tuning:**\n",
    "```python\n",
    "# Trade-off: Beam size vs Speed\n",
    "beam_size = 5  # Good balance (vs 1=greedy, 10=slow)\n",
    "\n",
    "# Length penalty (prefer longer outputs)\n",
    "length_penalty = 0.6  # Typical range: 0.5-1.0\n",
    "\n",
    "# Prevent repetition\n",
    "no_repeat_ngram_size = 3  # Block trigram repetition\n",
    "```\n",
    "\n",
    "**2. Caching for Inference:**\n",
    "```python\n",
    "# Cache encoder outputs (avoid re-encoding)\n",
    "encoder_outputs = model.encoder(src)  # Compute once\n",
    "for beam in beams:\n",
    "    # Reuse encoder_outputs for all beam candidates\n",
    "    decoder_output = model.decoder(beam, encoder_outputs)\n",
    "```\n",
    "\n",
    "**3. Quantization for Edge Deployment:**\n",
    "```python\n",
    "# Quantize to INT8 (4\u00d7 smaller, 2-3\u00d7 faster)\n",
    "import torch.quantization as quantization\n",
    "\n",
    "model_fp32 = model.cpu()\n",
    "model_int8 = quantization.quantize_dynamic(\n",
    "    model_fp32, {nn.LSTM, nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Result: 50 MB \u2192 12 MB, 100ms \u2192 40ms\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Best Practices**\n",
    "\n",
    "**1. Multiple Reference Translations:**\n",
    "```python\n",
    "# BLEU with 4 references (more robust)\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "references = [\n",
    "    ['device', 'shows', 'voltage', 'drop'],  # Reference 1\n",
    "    ['device', 'exhibits', 'voltage', 'degradation'],  # Reference 2\n",
    "    ['voltage', 'drops', 'in', 'device'],  # Reference 3\n",
    "    ['observed', 'voltage', 'reduction']  # Reference 4\n",
    "]\n",
    "\n",
    "hypothesis = ['device', 'shows', 'voltage', 'drop']\n",
    "\n",
    "bleu = corpus_bleu([[ref] for ref in references], [hypothesis])\n",
    "```\n",
    "\n",
    "**2. Human Evaluation (gold standard):**\n",
    "- Fluency (1-5): Grammatical correctness\n",
    "- Adequacy (1-5): Semantic correctness\n",
    "- Relevance (1-5): Matches context\n",
    "\n",
    "**3. Attention Visualization for Debugging:**\n",
    "```python\n",
    "def visualize_attention(src, trg, attn_weights):\n",
    "    \"\"\"\n",
    "    Heatmap showing which input positions influenced each output word\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(attn_weights, xticklabels=src, yticklabels=trg, cmap='YlGnBu')\n",
    "    plt.xlabel(\"Input (Test Cycles)\")\n",
    "    plt.ylabel(\"Output (Report Words)\")\n",
    "    plt.title(\"Attention Weights\")\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Pitfalls & Solutions**\n",
    "\n",
    "| Problem | Symptom | Solution |\n",
    "|---------|---------|----------|\n",
    "| **Exposure Bias** | Good training loss, poor inference | Scheduled sampling, reduce teacher forcing |\n",
    "| **Repetition** | \"voltage voltage voltage drop drop\" | Beam search with no_repeat_ngram_size=3 |\n",
    "| **Short Outputs** | Generates only 5 words, stops early | Length penalty in beam search |\n",
    "| **Out-of-Vocabulary** | Can't generate numbers (e.g., \"1.05\") | Copy mechanism, BPE tokenization |\n",
    "| **Slow Inference** | >1 sec per sentence | Beam size=3, quantization, caching |\n",
    "| **Poor Long Sequences** | BLEU drops for input >50 | Use Transformer (next notebook) |\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcda What's Next?\n",
    "\n",
    "**Upcoming Notebooks:**\n",
    "- **058: Transformers & Self-Attention** \u2192 No RNN, full parallelization, BERT/GPT foundations\n",
    "- **059: BERT & Transfer Learning for NLP** \u2192 Pre-trained models, fine-tuning\n",
    "- **060: GPT & Generative Models** \u2192 Autoregressive generation, language models\n",
    "\n",
    "---\n",
    "\n",
    "## \u2705 Learning Objectives Review\n",
    "\n",
    "1. \u2705 **Encoder-Decoder Architecture** - Two-stage pipeline, context vector\n",
    "2. \u2705 **Seq2Seq Fundamentals** - Teacher forcing, inference, bottleneck problem\n",
    "3. \u2705 **Attention Mechanism** - Dynamic context, Bahdanau vs Luong, self-attention preview\n",
    "4. \u2705 **Attention Variants** - Additive, multiplicative, scaled dot-product\n",
    "5. \u2705 **Beam Search** - Top-k decoding, length penalty, no-repeat\n",
    "6. \u2705 **Semiconductor Applications** - Automated failure reports, root cause analysis\n",
    "7. \u2705 **Production Deployment** - Copy mechanism, multi-lingual, quantization\n",
    "8. \u2705 **Modern Extensions** - Transformer preview, multi-head attention\n",
    "\n",
    "**Key Skill Acquired:** Build production-grade seq2seq models for sequence transformation tasks!\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcd6 Additional Resources\n",
    "\n",
    "**Must-Read Papers:**\n",
    "- \"Sequence to Sequence Learning with Neural Networks\" (Sutskever et al., 2014) - Original seq2seq\n",
    "- \"Neural Machine Translation by Jointly Learning to Align and Translate\" (Bahdanau et al., 2015) - Attention mechanism\n",
    "- \"Effective Approaches to Attention-based Neural Machine Translation\" (Luong et al., 2015) - Multiplicative attention\n",
    "- \"Attention Is All You Need\" (Vaswani et al., 2017) - Transformer architecture (next notebook!)\n",
    "\n",
    "**Courses & Tutorials:**\n",
    "- CS224n (Stanford) - Lecture 8: Machine Translation, Seq2Seq, Attention\n",
    "- Fast.ai NLP - Seq2Seq models\n",
    "- PyTorch Seq2Seq Tutorial - https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "**Libraries & Tools:**\n",
    "- **Fairseq** (Facebook) - https://github.com/facebookresearch/fairseq\n",
    "- **OpenNMT** - https://opennmt.net\n",
    "- **Hugging Face Transformers** - https://huggingface.co/transformers\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Final Summary\n",
    "\n",
    "**Seq2Seq Mastery:**\n",
    "- **Vanilla Seq2Seq:** Fixed context bottleneck (BLEU ~0.25 for long sequences)\n",
    "- **Seq2Seq + Attention:** Dynamic context, no bottleneck (BLEU ~0.42, +68%!)\n",
    "- **Beam Search:** Better generation quality (+2-5 BLEU points vs greedy)\n",
    "- **Production Tips:** Copy mechanism, quantization, attention visualization\n",
    "\n",
    "**Semiconductor Impact:**\n",
    "- **Automated reporting:** $15M-$60M/year from 95% faster failure analysis\n",
    "- **Root cause analysis:** Visual attention heatmaps show critical cycles\n",
    "- **Multi-fab deployment:** Domain adaptation with 1K samples\n",
    "\n",
    "**You're now ready to build sequence transformation systems!** \ud83d\ude80\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations on completing Notebook 057!** \ud83c\udf89\n",
    "\n",
    "Next notebook: **058_Transformers_Self_Attention.ipynb** - Revolutionize NLP with attention-only architecture!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}