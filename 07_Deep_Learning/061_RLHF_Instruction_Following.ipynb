{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b3b3ce",
   "metadata": {},
   "source": [
    "# 061: RLHF & Instruction Following",
    "",
    "**Learning Path**: 07_Deep_Learning \u2192 Advanced Transformers \u2192 Alignment & Human Feedback",
    "",
    "---",
    "",
    "## \ud83d\udcda Introduction",
    "",
    "Welcome to **RLHF (Reinforcement Learning from Human Feedback)** - the breakthrough technique that transformed GPT-3 into ChatGPT and revolutionized how AI systems follow human instructions!",
    "",
    "While GPT-3 (Notebook 060) is incredibly powerful at text generation, it has critical limitations:",
    "- **Doesn't follow instructions well**: Ask \"Explain quantum physics\" \u2192 might generate story about cats",
    "- **No concept of helpfulness**: Generates what's statistically likely, not what's useful",
    "- **Produces harmful content**: No built-in safety guardrails",
    "- **Verbose and unfocused**: Generates too much or off-topic text",
    "",
    "**RLHF solves these problems** by teaching models to:",
    "1. \u2705 **Follow instructions precisely**: \"Explain in 2 paragraphs\" \u2192 exactly 2 paragraphs",
    "2. \u2705 **Be helpful and informative**: Provide useful, accurate answers",
    "3. \u2705 **Be safe and harmless**: Refuse harmful requests, avoid biased content",
    "4. \u2705 **Be concise and focused**: Generate relevant, well-structured responses",
    "",
    "**The Result**: ChatGPT, Claude, Bard - all use RLHF to align with human preferences.",
    "",
    "---",
    "",
    "## \ud83c\udfaf Learning Objectives",
    "",
    "By the end of this notebook, you will:",
    "",
    "1. \u2705 **Understand RLHF pipeline**: Supervised fine-tuning \u2192 Reward modeling \u2192 PPO optimization",
    "2. \u2705 **Implement reward models**: Train models to score responses based on human preferences",
    "3. \u2705 **Apply PPO (Proximal Policy Optimization)**: RL algorithm for language model training",
    "4. \u2705 **Master instruction-following**: Teach models to follow specific user instructions",
    "5. \u2705 **Build safe AI systems**: Constitutional AI, red-teaming, alignment techniques",
    "6. \u2705 **Compare alignment methods**: RLHF vs DPO vs Constitutional AI",
    "7. \u2705 **Deploy in production**: Inference optimization, safety filters, monitoring",
    "8. \u2705 **Solve real-world problems**: Semiconductor test assistant, automated documentation",
    "",
    "---",
    "",
    "## \ud83d\udd04 The RLHF Revolution: From GPT-3 to ChatGPT",
    "",
    "```mermaid",
    "graph LR",
    "    A[\"GPT-3<br/>(175B params)<br/>Pre-trained on internet\"] --> B[\"InstructGPT<br/>+ Supervised Fine-Tuning<br/>on demonstrations\"]",
    "    B --> C[\"+ Reward Model<br/>trained on human preferences\"]",
    "    C --> D[\"+ PPO Optimization<br/>against reward model\"]",
    "    D --> E[\"ChatGPT<br/>Helpful, Harmless, Honest\"]",
    "    ",
    "    style A fill:#ffe4e1",
    "    style B fill:#fff4e1",
    "    style C fill:#e8f5e9",
    "    style D fill:#e3f2fd",
    "    style E fill:#f3e5f5",
    "```",
    "",
    "| Stage | Model | Capabilities | Limitations |",
    "|-------|-------|--------------|-------------|",
    "| **1. Pre-training** | GPT-3 | Predicts next token, general knowledge | Doesn't follow instructions, unsafe |",
    "| **2. SFT** | InstructGPT (SFT) | Follows some instructions | Inconsistent, verbose |",
    "| **3. Reward Model** | InstructGPT (RM) | Scores responses | No generation yet |",
    "| **4. PPO** | InstructGPT / ChatGPT | Helpful, safe, concise | Final aligned model \u2713 |",
    "",
    "**Key Insight**: GPT-3 is trained to predict internet text. ChatGPT is trained to be **useful to humans**.",
    "",
    "---",
    "",
    "## \ud83c\udfed Semiconductor Use Case: Intelligent Test Documentation Assistant",
    "",
    "**Business Problem**: Post-silicon validation engineers need an AI assistant that:",
    "- Answers technical questions accurately (e.g., \"What causes voltage droop at 85\u00b0C?\")",
    "- Follows specific formatting requirements (e.g., \"Summarize in bullet points\")",
    "- Refuses to generate misleading information (safety-critical domain)",
    "- Provides citations to internal knowledge base (traceability)",
    "",
    "**Current GPT-3 Limitations**:",
    "- Question: \"Explain voltage droop in 3 sentences\"",
    "- GPT-3 output: [Generates 2 paragraphs, 8 sentences, off-topic] \u274c",
    "- Doesn't follow instruction \"3 sentences\"",
    "- Lacks domain-specific safety (might suggest unsafe debug procedures)",
    "",
    "**RLHF Solution**: Train alignment on top of domain-specific GPT-2/3:",
    "1. **Supervised Fine-Tuning (SFT)**: Train on 1K high-quality engineer Q&A demonstrations",
    "2. **Reward Modeling**: Human engineers rank 10K response pairs (\"Which answer is better?\")",
    "3. **PPO Optimization**: Optimize policy to maximize reward (helpfulness + safety)",
    "",
    "**Expected Results**:",
    "- **Instruction-following**: 95% compliance with formatting/length constraints",
    "- **Helpfulness**: 4.5/5.0 engineer satisfaction (vs 3.2/5.0 for base GPT)",
    "- **Safety**: 99.5% refusal rate for unsafe debug procedures",
    "- **Business Value**: **$10M-$30M/year** from 80% faster issue resolution",
    "",
    "---",
    "",
    "## \ud83d\udcca RLHF Pipeline Overview",
    "",
    "```mermaid",
    "graph TD",
    "    A[\"Step 0: Pre-trained LLM<br/>(GPT-3, 175B params)\"] --> B[\"Step 1: Supervised Fine-Tuning<br/>Train on 13K demonstrations<br/>(prompt \u2192 high-quality response)\"]",
    "    B --> C[\"Step 2: Reward Model Training<br/>Train on 33K comparisons<br/>(response A vs response B)\"]",
    "    C --> D[\"Step 3: PPO Optimization<br/>Optimize policy using RM<br/>(maximize reward)\"]",
    "    D --> E[\"Aligned Model<br/>(ChatGPT/InstructGPT)\"]",
    "    ",
    "    style A fill:#ffe4e1",
    "    style B fill:#fff9c4",
    "    style C fill:#e8f5e9",
    "    style D fill:#e3f2fd",
    "    style E fill:#f3e5f5",
    "```",
    "",
    "### Three-Stage Process",
    "",
    "**Stage 1: Supervised Fine-Tuning (SFT)**",
    "- **Input**: 13K (prompt, ideal response) pairs written by humans",
    "- **Training**: Standard supervised learning (like GPT fine-tuning)",
    "- **Output**: Model that can follow basic instructions",
    "- **Example**:",
    "  - Prompt: \"Explain voltage droop\"",
    "  - SFT Response: \"Voltage droop is the decrease in supply voltage...\" \u2713",
    "",
    "**Stage 2: Reward Model (RM)**",
    "- **Input**: 33K comparison pairs (response A vs B, which is better?)",
    "- **Training**: Binary classification (predict which response humans prefer)",
    "- **Output**: Reward model that scores responses (0-1 scale)",
    "- **Example**:",
    "  - Response A: \"Voltage droop is complex...\" [Score: 0.3]",
    "  - Response B: \"Voltage droop occurs when...\" [Score: 0.8] \u2713",
    "",
    "**Stage 3: PPO Optimization**",
    "- **Input**: SFT model + reward model",
    "- **Training**: Reinforcement learning (PPO algorithm)",
    "- **Output**: Final aligned model",
    "- **Process**: Generate responses \u2192 Get reward scores \u2192 Update policy to maximize reward",
    "",
    "---",
    "",
    "## \ud83c\udfaf What We'll Build in This Notebook",
    "",
    "1. **Stage 1 - Supervised Fine-Tuning**: Train on demonstration pairs",
    "2. **Stage 2 - Reward Model**: Train preference model on human comparisons",
    "3. **Stage 3 - PPO Optimization**: RL training loop with reward maximization",
    "4. **Safety & Alignment**: Red-teaming, constitutional AI, safety filters",
    "5. **Production Deployment**: API, monitoring, human-in-the-loop feedback",
    "",
    "---",
    "",
    "## \ud83d\ude80 Prerequisites",
    "",
    "- \u2705 **GPT Architecture** (Notebook 060): Autoregressive generation, fine-tuning",
    "- \u2705 **Transformers** (Notebook 058): Self-attention, encoder-decoder",
    "- \u2705 **Reinforcement Learning Basics**: Policy, reward, optimization (we'll teach the essentials)",
    "- \u2705 **Python & PyTorch**: Neural networks, training loops",
    "- \u2705 **NLP Fundamentals**: Tokenization, embeddings, language modeling",
    "",
    "---",
    "",
    "## \ud83d\udcca Success Metrics",
    "",
    "**Technical Metrics**:",
    "- **Instruction-following**: 95%+ compliance with format/length constraints",
    "- **Helpfulness**: 4.5+/5.0 human evaluation score",
    "- **Safety**: 99%+ refusal rate for harmful requests",
    "- **Reward Model Accuracy**: 70%+ agreement with human preferences",
    "",
    "**Business Metrics** (Semiconductor Test Assistant):",
    "- **Engineer Satisfaction**: 4.5+/5.0 with AI assistant",
    "- **Resolution Time**: 60% reduction in time to answer technical questions",
    "- **Adoption Rate**: 85%+ engineers using assistant daily",
    "- **ROI**: $10M-$30M/year from faster debugging and knowledge sharing",
    "",
    "---",
    "",
    "## \ud83d\uddfa\ufe0f Notebook Roadmap",
    "",
    "```mermaid",
    "graph TD",
    "    A[\"Part 1: RLHF Theory<br/>& Mathematics\"] --> B[\"Part 2: Stage 1<br/>Supervised Fine-Tuning\"]",
    "    B --> C[\"Part 3: Stage 2<br/>Reward Model Training\"]",
    "    C --> D[\"Part 4: Stage 3<br/>PPO Optimization\"]",
    "    D --> E[\"Part 5: Safety & Alignment<br/>Techniques\"]",
    "    E --> F[\"Part 6: Production Deployment<br/>& Real-World Projects\"]",
    "    ",
    "    style A fill:#e3f2fd",
    "    style B fill:#fff3e0",
    "    style C fill:#f3e5f5",
    "    style D fill:#e8f5e9",
    "    style E fill:#fce4ec",
    "    style F fill:#fff9c4",
    "```",
    "",
    "**Estimated Time**: 100-130 minutes for complete notebook",
    "",
    "---",
    "",
    "## \ud83d\udca1 Why RLHF Matters",
    "",
    "**Before RLHF (GPT-3)**:",
    "- User: \"Write a Python function to sort a list\"",
    "- GPT-3: [Generates essay about sorting algorithms, no code] \u274c",
    "",
    "**After RLHF (ChatGPT)**:",
    "- User: \"Write a Python function to sort a list\"",
    "- ChatGPT: ",
    "```python",
    "def sort_list(lst):",
    "    return sorted(lst)",
    "```",
    "Perfect! \u2713",
    "",
    "**The Difference**: RLHF teaches models what humans **want**, not just what's statistically likely.",
    "",
    "**Industry Impact**:",
    "- **OpenAI**: GPT-3 \u2192 ChatGPT (100M users in 2 months)",
    "- **Anthropic**: Claude (RLHF + Constitutional AI)",
    "- **Google**: Bard, Gemini (RLHF-aligned)",
    "- **Meta**: Llama 2 Chat (RLHF fine-tuned)",
    "",
    "**Research Impact**: RLHF is now the **standard approach** for aligning large language models.",
    "",
    "---",
    "",
    "**Let's dive into the revolutionary technique that made ChatGPT possible!** \ud83d\ude80",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14770d8d",
   "metadata": {},
   "source": [
    "# \ud83d\udcd0 Part 1: RLHF Theory & Mathematical Foundation\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd2c The Alignment Problem\n",
    "\n",
    "**Core Question**: How do we make AI systems do what we **want** them to do, not just what they're trained to predict?\n",
    "\n",
    "### Problem with Standard Language Model Training\n",
    "\n",
    "Standard GPT training objective (next token prediction):\n",
    "$$\n",
    "\\mathcal{L}_{\\text{LM}} = -\\mathbb{E}_{x \\sim D} \\left[ \\sum_{t=1}^{T} \\log P_\\theta(x_t | x_{<t}) \\right]\n",
    "$$\n",
    "\n",
    "**What this optimizes**: Statistical likelihood of text from internet corpus $D$\n",
    "\n",
    "**What we actually want**: \n",
    "- Helpful responses\n",
    "- Honest information  \n",
    "- Harmless content\n",
    "\n",
    "**Mismatch Example**:\n",
    "- Most likely continuation of \"How to hack into...\": Detailed hacking tutorial (common on internet)\n",
    "- What we want: \"I can't help with that\" (safe refusal)\n",
    "\n",
    "**Solution**: RLHF trains models to maximize **human preferences**, not just statistical likelihood.\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf RLHF as a Reinforcement Learning Problem\n",
    "\n",
    "### Standard RL Framework\n",
    "\n",
    "**Components**:\n",
    "1. **Agent**: Language model $\\pi_\\theta$ (policy)\n",
    "2. **Environment**: User provides prompt\n",
    "3. **Action**: Generate response token by token\n",
    "4. **Reward**: Score from reward model (trained on human preferences)\n",
    "5. **Goal**: Maximize expected reward\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "**Policy**: Language model $\\pi_\\theta(y|x)$ that generates response $y$ given prompt $x$\n",
    "\n",
    "**Reward Function**: $r(x, y)$ scores how good response $y$ is for prompt $x$\n",
    "\n",
    "**Objective**: Maximize expected reward\n",
    "$$\n",
    "\\mathcal{J}(\\theta) = \\mathbb{E}_{x \\sim D, y \\sim \\pi_\\theta(y|x)} [r(x, y)]\n",
    "$$\n",
    "\n",
    "**Constraint**: Don't deviate too far from original model (prevent mode collapse)\n",
    "$$\n",
    "\\mathcal{J}(\\theta) = \\mathbb{E}_{x, y} [r(x, y)] - \\beta \\cdot D_{\\text{KL}}(\\pi_\\theta(y|x) || \\pi_{\\text{ref}}(y|x))\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\pi_{\\text{ref}}$: Reference model (SFT model, frozen)\n",
    "- $\\beta$: KL penalty coefficient (typically 0.01-0.1)\n",
    "- $D_{\\text{KL}}$: KL divergence (measures distribution difference)\n",
    "\n",
    "**Interpretation**: Maximize reward while staying close to reference model.\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcca The Three Stages of RLHF (Detailed)\n",
    "\n",
    "### Stage 1: Supervised Fine-Tuning (SFT)\n",
    "\n",
    "**Goal**: Teach model to follow instructions with high-quality demonstrations.\n",
    "\n",
    "**Data**: $(x, y)$ pairs where:\n",
    "- $x$: User prompt/instruction\n",
    "- $y$: High-quality human-written response\n",
    "\n",
    "**Training Objective**: Standard supervised learning (cross-entropy)\n",
    "$$\n",
    "\\mathcal{L}_{\\text{SFT}} = -\\mathbb{E}_{(x,y) \\sim D_{\\text{demo}}} \\left[ \\sum_{t=1}^{|y|} \\log P_\\theta(y_t | x, y_{<t}) \\right]\n",
    "$$\n",
    "\n",
    "**Example Data**:\n",
    "```\n",
    "Prompt: \"Explain voltage droop in simple terms\"\n",
    "Response: \"Voltage droop is when the power supply voltage decreases under heavy load, \n",
    "           similar to how water pressure drops when multiple faucets are open...\"\n",
    "```\n",
    "\n",
    "**Result**: Model learns **what good responses look like** but not yet **how to consistently produce them**.\n",
    "\n",
    "**InstructGPT Statistics**:\n",
    "- Training data: 13,000 demonstrations\n",
    "- Labelers: 40 human contractors\n",
    "- Time: 2-3 weeks to collect data\n",
    "- Cost: ~$50K-$100K for labeling\n",
    "\n",
    "---\n",
    "\n",
    "### Stage 2: Reward Model Training\n",
    "\n",
    "**Goal**: Train a model to predict which responses humans prefer.\n",
    "\n",
    "**Data**: Comparison pairs $(x, y_w, y_l)$ where:\n",
    "- $x$: Prompt\n",
    "- $y_w$: Winning response (preferred by human)\n",
    "- $y_l$: Losing response (less preferred)\n",
    "\n",
    "**Architecture**: Language model with scalar output head\n",
    "$$\n",
    "r_\\phi(x, y) = \\text{LM}_\\phi(x, y) \\rightarrow \\text{scalar reward}\n",
    "$$\n",
    "\n",
    "**Training Objective**: Bradley-Terry model (pairwise ranking)\n",
    "$$\n",
    "\\mathcal{L}_{\\text{RM}} = -\\mathbb{E}_{(x, y_w, y_l) \\sim D_{\\text{comp}}} \\left[ \\log \\sigma(r_\\phi(x, y_w) - r_\\phi(x, y_l)) \\right]\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function.\n",
    "\n",
    "**Interpretation**: Maximize probability that reward model assigns higher score to preferred response.\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Prompt: \"Explain voltage droop\"\n",
    "\n",
    "Response A (verbose, off-topic): \n",
    "\"Voltage is a fundamental concept in electrical engineering. Throughout history...\"\n",
    "Reward: 0.3\n",
    "\n",
    "Response B (concise, on-topic):\n",
    "\"Voltage droop is the decrease in supply voltage when current draw increases...\"\n",
    "Reward: 0.8\n",
    "\n",
    "Loss encourages: r(x, B) > r(x, A) \u2713\n",
    "```\n",
    "\n",
    "**InstructGPT Statistics**:\n",
    "- Comparison data: 33,000 pairs\n",
    "- Each prompt: 4-9 responses ranked\n",
    "- Labelers: Same 40 contractors\n",
    "- Agreement rate: 73% (inter-labeler agreement)\n",
    "\n",
    "**Key Insight**: Easier for humans to **rank** responses than **write** perfect responses!\n",
    "\n",
    "---\n",
    "\n",
    "### Stage 3: PPO Optimization\n",
    "\n",
    "**Goal**: Optimize language model to maximize reward from reward model.\n",
    "\n",
    "**Algorithm**: Proximal Policy Optimization (PPO)\n",
    "- **Policy**: Language model $\\pi_\\theta$\n",
    "- **Reward**: From reward model $r_\\phi$\n",
    "- **Constraint**: KL divergence from reference model\n",
    "\n",
    "**PPO Objective**: \n",
    "$$\n",
    "\\mathcal{L}^{\\text{PPO}}(\\theta) = \\mathbb{E}_{x, y} \\left[ \\min\\left( \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{old}}(y|x)} A(x, y), \\text{clip}\\left(\\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{old}}(y|x)}, 1-\\epsilon, 1+\\epsilon\\right) A(x, y) \\right) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $A(x, y)$: Advantage function (how much better than expected)\n",
    "- $\\epsilon$: Clipping parameter (typically 0.2)\n",
    "- Clipping prevents too large policy updates\n",
    "\n",
    "**Full RLHF Objective**:\n",
    "$$\n",
    "\\mathcal{L}^{\\text{RLHF}}(\\theta) = \\mathbb{E}_{x \\sim D, y \\sim \\pi_\\theta} [r_\\phi(x, y)] - \\beta \\cdot D_{\\text{KL}}(\\pi_\\theta(y|x) || \\pi_{\\text{SFT}}(y|x))\n",
    "$$\n",
    "\n",
    "**Training Loop**:\n",
    "```\n",
    "For each batch of prompts:\n",
    "  1. Generate responses using current policy \u03c0_\u03b8\n",
    "  2. Score responses using reward model r_\u03c6\n",
    "  3. Compute advantages (reward - baseline)\n",
    "  4. Update policy using PPO to maximize advantages\n",
    "  5. Apply KL penalty to prevent drift from SFT model\n",
    "```\n",
    "\n",
    "**InstructGPT Statistics**:\n",
    "- PPO training: 256K-512K prompts\n",
    "- Batch size: 512 prompts\n",
    "- KL coefficient \u03b2: 0.02\n",
    "- Training time: 1-2 days on 256 GPUs\n",
    "- Cost: ~$500K-$1M\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd04 Why PPO for Language Models?\n",
    "\n",
    "**PPO Advantages**:\n",
    "1. **Sample efficient**: Uses old policy samples (off-policy data)\n",
    "2. **Stable**: Clipping prevents destructive updates\n",
    "3. **Scalable**: Works with large models (175B parameters)\n",
    "4. **Simple**: Easier to implement than TRPO (Trust Region Policy Optimization)\n",
    "\n",
    "**Alternative RL Algorithms**:\n",
    "- **REINFORCE**: High variance, sample inefficient\n",
    "- **A3C (Actor-Critic)**: Requires parallel environments (hard for LLMs)\n",
    "- **DPO (Direct Preference Optimization)**: Recent alternative, no RL needed! (we'll cover this)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcc8 Reward Model Architecture\n",
    "\n",
    "### From Language Model to Reward Model\n",
    "\n",
    "**Base**: Pre-trained language model (e.g., GPT-2 6B parameters)\n",
    "\n",
    "**Modification**: Replace language modeling head with scalar output\n",
    "$$\n",
    "\\text{Input: } (x, y) \\rightarrow \\text{LM Encoder} \\rightarrow \\text{Hidden States} \\rightarrow \\text{Linear}(d_{\\text{model}}, 1) \\rightarrow \\text{Reward } r\n",
    "$$\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model  # Pre-trained LM\n",
    "        self.reward_head = nn.Linear(base_model.config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get last hidden state from language model\n",
    "        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = outputs.last_hidden_state[:, -1, :]  # [CLS] or last token\n",
    "        \n",
    "        # Project to scalar reward\n",
    "        reward = self.reward_head(last_hidden)\n",
    "        \n",
    "        return reward.squeeze(-1)  # Shape: (batch_size,)\n",
    "```\n",
    "\n",
    "**Training**: Pairwise ranking loss (Bradley-Terry model)\n",
    "\n",
    "**Ensemble**: InstructGPT uses 6 reward models (reduces variance, improves robustness)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfa8 Visualizing RLHF Training Dynamics\n",
    "\n",
    "### Reward vs KL Penalty Trade-off\n",
    "\n",
    "As PPO training progresses:\n",
    "\n",
    "**Iteration 0** (SFT model):\n",
    "- Reward: 3.2\n",
    "- KL from SFT: 0.0\n",
    "- Response quality: Good but not optimized\n",
    "\n",
    "**Iteration 1000**:\n",
    "- Reward: 4.8 (+50%)\n",
    "- KL from SFT: 2.1\n",
    "- Response quality: Better, still coherent\n",
    "\n",
    "**Iteration 5000**:\n",
    "- Reward: 6.2 (+94%)\n",
    "- KL from SFT: 10.5\n",
    "- Response quality: High reward but mode collapse risk\n",
    "\n",
    "**Optimal** (chosen checkpoint):\n",
    "- Reward: 5.5 (+72%)\n",
    "- KL from SFT: 5.2\n",
    "- Response quality: Best balance \u2713\n",
    "\n",
    "**Key Finding**: There's a sweet spot where reward is high but KL divergence is manageable.\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udca1 Why RLHF Works: Intuition\n",
    "\n",
    "**Analogy**: Training a dog\n",
    "\n",
    "**Supervised Fine-Tuning (SFT)**: \n",
    "- Show dog how to fetch 100 times\n",
    "- Dog learns the basic pattern\n",
    "- But not perfect every time\n",
    "\n",
    "**Reward Model**:\n",
    "- You (human) judge: \"Good fetch\" (+10) vs \"Dropped ball\" (+2)\n",
    "- Dog doesn't know your preferences yet\n",
    "- Reward model captures your preferences\n",
    "\n",
    "**PPO Optimization**:\n",
    "- Dog tries many fetch attempts\n",
    "- Gets feedback (reward scores)\n",
    "- Learns to maximize reward (successful fetches)\n",
    "- Constraint: Don't forget basic fetch pattern (KL penalty)\n",
    "\n",
    "**Result**: Dog becomes expert at fetching in the way **you prefer**, not just average fetch from training data.\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd2c Mathematical Deep Dive: PPO Clipped Objective\n",
    "\n",
    "### Why Clipping?\n",
    "\n",
    "**Problem**: Policy gradient can cause large, destructive updates.\n",
    "\n",
    "**Solution**: Clip the probability ratio to prevent extreme updates.\n",
    "\n",
    "**Probability Ratio**:\n",
    "$$\n",
    "r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\text{old}}(a_t | s_t)}\n",
    "$$\n",
    "\n",
    "**Unclipped Objective** (standard policy gradient):\n",
    "$$\n",
    "\\mathcal{L}^{\\text{PG}}(\\theta) = \\mathbb{E}_t [r_t(\\theta) \\cdot A_t]\n",
    "$$\n",
    "\n",
    "**Problem**: If $r_t(\\theta) \\gg 1$, update is too large (exploration \u2192 exploitation crash)\n",
    "\n",
    "**Clipped Objective**:\n",
    "$$\n",
    "\\mathcal{L}^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\left[ \\min(r_t(\\theta) \\cdot A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\cdot A_t) \\right]\n",
    "$$\n",
    "\n",
    "**Effect**:\n",
    "- If $A_t > 0$ (good action): Allow increase up to $1+\\epsilon$ (e.g., 1.2)\n",
    "- If $A_t < 0$ (bad action): Allow decrease down to $1-\\epsilon$ (e.g., 0.8)\n",
    "- Prevents catastrophic policy collapse\n",
    "\n",
    "**Visual**:\n",
    "```\n",
    "Probability ratio (r_t):\n",
    "0.5    0.8    1.0    1.2    1.5    2.0\n",
    " |------|------|------|------|------|\n",
    "      Clipped      No clip   Clipped\n",
    "     (too low)              (too high)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcca RLHF vs Alternative Alignment Methods\n",
    "\n",
    "| Method | Pros | Cons | Use Case |\n",
    "|--------|------|------|----------|\n",
    "| **RLHF (PPO)** | Flexible, state-of-art results | Complex, expensive ($500K training), reward hacking risk | ChatGPT, Claude, production systems |\n",
    "| **DPO (Direct Preference Optimization)** | Simpler (no RL), faster, cheaper | Recent (less proven), may miss nuances | Research, smaller models |\n",
    "| **RLAIF (RL from AI Feedback)** | Scalable (no human labels), cheaper | Quality depends on teacher model | Low-resource settings |\n",
    "| **Constitutional AI** | Self-supervised safety, interpretable | Requires good constitution, slower | Safety-critical applications |\n",
    "| **Prompt Engineering** | Zero training, instant | Limited capability, prompt-dependent | Quick prototypes |\n",
    "\n",
    "**Current Industry Standard**: RLHF with PPO (2023-2024)\n",
    "\n",
    "**Emerging Trend**: DPO gaining traction (2024-2025) - simpler and cheaper\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Key Takeaways: Part 1\n",
    "\n",
    "1. \u2705 **Alignment Problem**: Standard LM training optimizes likelihood, not human preferences\n",
    "2. \u2705 **RLHF = RL + Human Feedback**: Treat generation as RL problem with learned reward\n",
    "3. \u2705 **Three Stages**: SFT (teach) \u2192 RM (learn preferences) \u2192 PPO (optimize)\n",
    "4. \u2705 **PPO**: Proximal Policy Optimization with clipping for stable updates\n",
    "5. \u2705 **KL Penalty**: Prevents model from drifting too far from safe SFT baseline\n",
    "6. \u2705 **Reward Model**: Easier to rank than write (33K comparisons vs 13K demonstrations)\n",
    "7. \u2705 **Trade-off**: Reward vs KL divergence (sweet spot at ~5 KL)\n",
    "\n",
    "**Next**: Part 2 will implement Stage 1 (Supervised Fine-Tuning) on semiconductor corpus!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a1eeb",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e7a563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Stage 1 - Supervised Fine-Tuning (SFT) Implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple\n",
    "import random\n",
    "import json\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "# ==============================================================================\n",
    "# 1. GENERATE HIGH-QUALITY INSTRUCTION-RESPONSE PAIRS\n",
    "# ==============================================================================\n",
    "def generate_semiconductor_qa_demonstrations(n_samples: int = 1000) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate high-quality Q&A demonstrations for SFT training.\n",
    "    \n",
    "    These represent expert engineer responses that follow instructions precisely.\n",
    "    \"\"\"\n",
    "    \n",
    "    demonstrations = []\n",
    "    \n",
    "    # Question templates with instruction-following elements\n",
    "    templates = [\n",
    "        {\n",
    "            'question': 'Explain {concept} in {format}',\n",
    "            'format_options': ['2 sentences', '3 bullet points', 'simple terms', 'technical detail'],\n",
    "            'concepts': ['voltage droop', 'thermal runaway', 'leakage current', 'timing violations', \n",
    "                        'power consumption', 'signal integrity', 'electromigration']\n",
    "        },\n",
    "        {\n",
    "            'question': 'What causes {failure_mode} at {condition}?',\n",
    "            'failure_modes': ['device failure', 'test failure', 'performance degradation', 'instability'],\n",
    "            'conditions': ['high temperature (85\u00b0C)', 'low voltage (0.95V)', 'high frequency (2.6GHz)', \n",
    "                          'stress conditions']\n",
    "        },\n",
    "        {\n",
    "            'question': 'List {n} common root causes for {test_type} test failures',\n",
    "            'n_options': ['3', '5', 'top 3'],\n",
    "            'test_types': ['functional', 'parametric', 'stress', 'burn-in', 'reliability']\n",
    "        },\n",
    "        {\n",
    "            'question': 'Summarize the debug procedure for {issue} in {length}',\n",
    "            'issues': ['voltage regulator failure', 'timing violations', 'thermal issues', 'power anomalies'],\n",
    "            'length_options': ['3 steps', '5 steps', 'brief overview', 'detailed procedure']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Generate demonstrations\n",
    "    for _ in range(n_samples):\n",
    "        template = random.choice(templates)\n",
    "        \n",
    "        if 'concepts' in template:\n",
    "            concept = random.choice(template['concepts'])\n",
    "            format_req = random.choice(template['format_options'])\n",
    "            question = template['question'].format(concept=concept, format=format_req)\n",
    "            \n",
    "            # Generate format-compliant response\n",
    "            if '2 sentences' in format_req:\n",
    "                response = f\"Voltage droop is the decrease in supply voltage when load current increases. This occurs because the power delivery network has non-zero impedance.\"\n",
    "            elif '3 bullet points' in format_req:\n",
    "                response = f\"\"\"Here are 3 key points about {concept}:\n",
    "\u2022 Definition: The phenomenon of voltage decrease under high current load\n",
    "\u2022 Cause: Impedance in power delivery network (PDN) causes IR drop\n",
    "\u2022 Impact: Can cause timing failures or functional errors if droop exceeds margin\"\"\"\n",
    "            elif 'simple terms' in format_req:\n",
    "                response = f\"{concept.title()} is like water pressure dropping when many faucets are open - the supply voltage decreases when the chip draws more current.\"\n",
    "            else:  # technical detail\n",
    "                response = f\"\"\"{concept.title()} occurs due to parasitic resistance and inductance in the power delivery network. \n",
    "When di/dt (rate of current change) is high, L*di/dt contributes to voltage drop. \n",
    "This is characterized by: Vdroop = Iload * (RPDN + sLPDN), where s is Laplace variable.\"\"\"\n",
    "        \n",
    "        elif 'failure_modes' in template:\n",
    "            failure = random.choice(template['failure_modes'])\n",
    "            condition = random.choice(template['conditions'])\n",
    "            question = template['question'].format(failure_mode=failure, condition=condition)\n",
    "            response = f\"\"\"At {condition}, {failure} is typically caused by:\n",
    "1. Insufficient voltage margin - the device operates too close to minimum voltage spec\n",
    "2. Thermal-induced timing degradation - higher temperature slows transistor switching\n",
    "3. Increased leakage current - exponentially increases with temperature per Arrhenius equation\n",
    "Root cause analysis should start with voltage and temperature monitoring.\"\"\"\n",
    "        \n",
    "        elif 'test_types' in template:\n",
    "            n = random.choice(template['n_options'])\n",
    "            test_type = random.choice(template['test_types'])\n",
    "            question = template['question'].format(n=n, test_type=test_type)\n",
    "            \n",
    "            if n == '3' or n == 'top 3':\n",
    "                response = f\"\"\"Top 3 root causes for {test_type} test failures:\n",
    "1. Power delivery issues (voltage droop, noise)\n",
    "2. Thermal problems (hot spots, inadequate cooling)\n",
    "3. Manufacturing defects (process variation, contamination)\"\"\"\n",
    "            else:  # 5 causes\n",
    "                response = f\"\"\"Top 5 root causes for {test_type} test failures:\n",
    "1. Power delivery issues (voltage droop, noise, decoupling)\n",
    "2. Thermal problems (hot spots, thermal runaway, inadequate cooling)\n",
    "3. Timing violations (setup/hold time, clock skew)\n",
    "4. Manufacturing defects (process variation, contamination, yield)\n",
    "5. Design marginality (insufficient design margin, corner cases)\"\"\"\n",
    "        \n",
    "        elif 'issues' in template:\n",
    "            issue = random.choice(template['issues'])\n",
    "            length = random.choice(template['length_options'])\n",
    "            question = template['question'].format(issue=issue, length=length)\n",
    "            \n",
    "            if '3 steps' in length or 'brief' in length:\n",
    "                response = f\"\"\"Debug procedure for {issue}:\n",
    "1. Verify operating conditions (voltage, frequency, temperature)\n",
    "2. Measure key parameters (Vdd, Idd, thermal sensors)\n",
    "3. Compare against specification limits and historical data\"\"\"\n",
    "            else:  # detailed or 5 steps\n",
    "                response = f\"\"\"Detailed debug procedure for {issue}:\n",
    "1. Data Collection: Capture voltage, current, temperature telemetry\n",
    "2. Correlation Analysis: Identify when failure occurs (thermal profile, load pattern)\n",
    "3. Component Isolation: Test individual subsystems to localize issue\n",
    "4. Root Cause: Physical analysis (FA) if needed, design review\n",
    "5. Validation: Retest with proposed fix, verify across corner cases\"\"\"\n",
    "        \n",
    "        demonstrations.append({\n",
    "            'prompt': question,\n",
    "            'response': response,\n",
    "            'instruction_following': True  # All responses follow format requirements\n",
    "        })\n",
    "    \n",
    "    return demonstrations\n",
    "print(\"=\"*80)\n",
    "print(\"Stage 1: Supervised Fine-Tuning (SFT)\")\n",
    "print(\"=\"*80)\n",
    "# Generate demonstrations\n",
    "demonstrations = generate_semiconductor_qa_demonstrations(n_samples=800)\n",
    "print(f\"\\nGenerated {len(demonstrations)} high-quality demonstrations\")\n",
    "print(f\"\\nSample Demonstrations:\")\n",
    "for i in range(3):\n",
    "    demo = demonstrations[i]\n",
    "    print(f\"\\n{'\u2500'*80}\")\n",
    "    print(f\"Prompt: {demo['prompt']}\")\n",
    "    print(f\"Response: {demo['response'][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df54e94",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. DATASET FOR SFT TRAINING\n",
    "# ==============================================================================\n",
    "class InstructionDataset(Dataset):\n",
    "    \"\"\"Dataset for instruction fine-tuning.\"\"\"\n",
    "    \n",
    "    def __init__(self, demonstrations: List[Dict], tokenizer, max_length: int = 512):\n",
    "        self.demonstrations = demonstrations\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.demonstrations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        demo = self.demonstrations[idx]\n",
    "        \n",
    "        # Format as instruction-following conversation\n",
    "        text = f\"User: {demo['prompt']}\\n\\nAssistant: {demo['response']}{self.tokenizer.eos_token}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        # Labels for language modeling (same as input_ids)\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        # Mask padding tokens in labels (ignore in loss)\n",
    "        labels[attention_mask == 0] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "# Load tokenizer and model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading Pre-trained GPT-2 for SFT\")\n",
    "print(\"=\"*80)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have pad token\n",
    "model_sft = GPT2LMHeadModel.from_pretrained('gpt2').to(DEVICE)\n",
    "print(f\"\\nModel: GPT-2\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model_sft.parameters()):,}\")\n",
    "print(f\"Vocabulary: {len(tokenizer):,}\")\n",
    "# Create datasets\n",
    "train_size = int(0.9 * len(demonstrations))\n",
    "train_demos = demonstrations[:train_size]\n",
    "val_demos = demonstrations[train_size:]\n",
    "train_dataset = InstructionDataset(train_demos, tokenizer)\n",
    "val_dataset = InstructionDataset(val_demos, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Val samples: {len(val_dataset)}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d30f65a",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4406d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. SFT TRAINING LOOP\n",
    "# ==============================================================================\n",
    "def train_sft(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    n_epochs: int = 3,\n",
    "    lr: float = 5e-5,\n",
    "    warmup_steps: int = 100\n",
    "):\n",
    "    \"\"\"Train model with supervised fine-tuning.\"\"\"\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    total_steps = len(train_loader) * n_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training SFT Model\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Epochs: {n_epochs}, Learning Rate: {lr}, Warmup Steps: {warmup_steps}\\n\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f\"  Epoch {epoch+1}/{n_epochs} | Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "                      f\"Loss: {loss.item():.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "                labels = batch['labels'].to(DEVICE)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{n_epochs} Summary:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Train PPL: {np.exp(train_loss):.2f} | Val PPL: {np.exp(val_loss):.2f}\\n\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "# Train SFT model\n",
    "train_losses, val_losses = train_sft(\n",
    "    model_sft,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    n_epochs=3,\n",
    "    lr=5e-5,\n",
    "    warmup_steps=100\n",
    ")\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(val_losses, label='Val Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('SFT Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('sft_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\n\u2713 SFT Training Complete!\")\n",
    "print(f\"  Final Train Loss: {train_losses[-1]:.4f} (PPL: {np.exp(train_losses[-1]):.2f})\")\n",
    "print(f\"  Final Val Loss: {val_losses[-1]:.4f} (PPL: {np.exp(val_losses[-1]):.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425394c1",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b567ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4. TEST SFT MODEL (Instruction Following)\n",
    "# ==============================================================================\n",
    "def generate_sft_response(\n",
    "    model: nn.Module,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_length: int = 200,\n",
    "    temperature: float = 0.7\n",
    "):\n",
    "    \"\"\"Generate response using SFT model.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Format prompt\n",
    "    input_text = f\"User: {prompt}\\n\\nAssistant:\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(DEVICE)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract assistant response\n",
    "    if 'Assistant:' in output_text:\n",
    "        response = output_text.split('Assistant:')[1].strip()\n",
    "    else:\n",
    "        response = output_text\n",
    "    \n",
    "    return response\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Testing SFT Model - Instruction Following\")\n",
    "print(f\"{'='*80}\")\n",
    "# Test prompts with format requirements\n",
    "test_prompts = [\n",
    "    \"Explain voltage droop in 2 sentences\",\n",
    "    \"List 3 common causes of thermal runaway\",\n",
    "    \"Summarize leakage current in simple terms\",\n",
    "    \"What causes timing violations at high temperature?\",\n",
    "]\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n{'\u2500'*80}\")\n",
    "    print(f\"Test {i+1}\")\n",
    "    print(f\"{'\u2500'*80}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"\\nSFT Response:\")\n",
    "    response = generate_sft_response(model_sft, tokenizer, prompt)\n",
    "    print(response)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"\u2713 Stage 1 (SFT) Complete!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"  1. Model learns to follow instruction format (2 sentences, 3 bullet points, etc.)\")\n",
    "print(\"  2. Responses are coherent and domain-appropriate\")\n",
    "print(\"  3. Training loss decreases steadily (model learning demonstrations)\")\n",
    "print(\"  4. SFT model serves as strong baseline for Stage 2 (Reward Model)\")\n",
    "print(\"\\nNext: Stage 2 will train Reward Model to score response quality!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d954cd",
   "metadata": {},
   "source": [
    "# \ud83c\udfaf Part 3: Stage 2 - Reward Model Training\n",
    "\n",
    "Now we'll train a reward model to predict which responses humans prefer, enabling automated quality scoring for PPO optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbdb463",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ba1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Stage 2 - Reward Model Training Implementation\n",
    "# ==============================================================================\n",
    "# 1. GENERATE COMPARISON DATA (Response Pairs with Human Preferences)\n",
    "# ==============================================================================\n",
    "def generate_comparison_data(n_samples: int = 500) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate comparison pairs: (prompt, winning_response, losing_response).\n",
    "    \n",
    "    Simulates human labelers ranking responses by quality.\n",
    "    \"\"\"\n",
    "    \n",
    "    comparisons = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Generate prompt\n",
    "        concepts = ['voltage droop', 'thermal runaway', 'leakage current', 'timing violations']\n",
    "        formats = ['2 sentences', '3 bullet points', 'simple terms']\n",
    "        \n",
    "        concept = random.choice(concepts)\n",
    "        format_req = random.choice(formats)\n",
    "        prompt = f\"Explain {concept} in {format_req}\"\n",
    "        \n",
    "        # Generate two responses: one good (wins), one bad (loses)\n",
    "        if '2 sentences' in format_req:\n",
    "            # WINNING response: follows format, accurate, concise\n",
    "            winning = f\"{concept.title()} is the phenomenon where supply voltage decreases under high load. This occurs due to impedance in the power delivery network.\"\n",
    "            \n",
    "            # LOSING response: violates format (too long), verbose\n",
    "            losing = f\"{concept.title()} is an important concept in semiconductor design. Throughout the history of chip design, engineers have struggled with this. The phenomenon occurs when there is high current draw. This is a complex issue that involves many factors including resistance, inductance, and capacitance in the power delivery network.\"\n",
    "        \n",
    "        elif '3 bullet points' in format_req:\n",
    "            # WINNING response: exactly 3 bullets, clear structure\n",
    "            winning = f\"\"\"\u2022 Definition: {concept.title()} is voltage decrease under load\n",
    "\u2022 Cause: Power delivery network impedance (R + j\u03c9L)\n",
    "\u2022 Impact: Can cause timing failures if droop exceeds design margin\"\"\"\n",
    "            \n",
    "            # LOSING response: wrong format (paragraph), no structure\n",
    "            losing = f\"The concept of {concept} involves understanding how power delivery works in semiconductors. It's related to current flow and impedance, which affect voltage levels during operation.\"\n",
    "        \n",
    "        else:  # simple terms\n",
    "            # WINNING response: simple analogy, accessible\n",
    "            winning = f\"{concept.title()} is like water pressure dropping when many faucets are open - the chip's supply voltage decreases when more current is drawn.\"\n",
    "            \n",
    "            # LOSING response: overly technical, not simple\n",
    "            losing = f\"{concept.title()} is characterized by V_droop = I_load * Z_PDN where Z_PDN represents the impedance of the power delivery network including parasitic resistance and inductance components modeled in the frequency domain.\"\n",
    "        \n",
    "        comparisons.append({\n",
    "            'prompt': prompt,\n",
    "            'winning_response': winning,\n",
    "            'losing_response': losing,\n",
    "            'preference': 'winning'  # Human prefers winning response\n",
    "        })\n",
    "    \n",
    "    return comparisons\n",
    "print(\"=\"*80)\n",
    "print(\"Stage 2: Reward Model Training\")\n",
    "print(\"=\"*80)\n",
    "# Generate comparison data\n",
    "comparisons = generate_comparison_data(n_samples=400)\n",
    "print(f\"\\nGenerated {len(comparisons)} comparison pairs\")\n",
    "print(f\"\\nSample Comparison:\")\n",
    "comp = comparisons[0]\n",
    "print(f\"\\nPrompt: {comp['prompt']}\")\n",
    "print(f\"\\nWinning Response (preferred):\\n{comp['winning_response']}\")\n",
    "print(f\"\\nLosing Response (not preferred):\\n{comp['losing_response']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3e65a8",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4374d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. REWARD MODEL ARCHITECTURE\n",
    "# ==============================================================================\n",
    "class RewardModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Reward model that scores (prompt, response) pairs.\n",
    "    \n",
    "    Architecture: GPT-2 base + scalar reward head\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_name='gpt2'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pre-trained GPT-2 as base\n",
    "        self.base_model = GPT2LMHeadModel.from_pretrained(base_model_name)\n",
    "        \n",
    "        # Remove language modeling head, add reward head\n",
    "        hidden_size = self.base_model.config.hidden_size\n",
    "        self.reward_head = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Initialize reward head\n",
    "        nn.init.normal_(self.reward_head.weight, std=0.02)\n",
    "        nn.init.zeros_(self.reward_head.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass: (prompt, response) \u2192 scalar reward.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs (batch_size, seq_len)\n",
    "            attention_mask: Attention mask (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            reward: Scalar reward for each sequence (batch_size,)\n",
    "        \"\"\"\n",
    "        # Get hidden states from base model\n",
    "        outputs = self.base_model.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use last token's hidden state (end of response)\n",
    "        last_hidden = outputs.last_hidden_state[:, -1, :]  # (batch_size, hidden_size)\n",
    "        \n",
    "        # Project to scalar reward\n",
    "        reward = self.reward_head(last_hidden)  # (batch_size, 1)\n",
    "        \n",
    "        return reward.squeeze(-1)  # (batch_size,)\n",
    "# Create reward model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Reward Model\")\n",
    "print(\"=\"*80)\n",
    "reward_model = RewardModel().to(DEVICE)\n",
    "print(f\"\\nReward Model Architecture:\")\n",
    "print(f\"  Base: GPT-2 ({sum(p.numel() for p in reward_model.base_model.parameters()):,} params)\")\n",
    "print(f\"  Reward Head: Linear({reward_model.base_model.config.hidden_size}, 1)\")\n",
    "print(f\"  Total: {sum(p.numel() for p in reward_model.parameters()):,} params\")\n",
    "# ==============================================================================\n",
    "# 3. COMPARISON DATASET FOR REWARD MODEL\n",
    "# ==============================================================================\n",
    "class ComparisonDataset(Dataset):\n",
    "    \"\"\"Dataset for pairwise comparison training.\"\"\"\n",
    "    \n",
    "    def __init__(self, comparisons: List[Dict], tokenizer, max_length: int = 256):\n",
    "        self.comparisons = comparisons\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.comparisons)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        comp = self.comparisons[idx]\n",
    "        \n",
    "        # Format: \"User: {prompt}\\n\\nAssistant: {response}\"\n",
    "        winning_text = f\"User: {comp['prompt']}\\n\\nAssistant: {comp['winning_response']}\"\n",
    "        losing_text = f\"User: {comp['prompt']}\\n\\nAssistant: {comp['losing_response']}\"\n",
    "        \n",
    "        # Tokenize winning response\n",
    "        winning_enc = self.tokenizer(\n",
    "            winning_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize losing response\n",
    "        losing_enc = self.tokenizer(\n",
    "            losing_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'winning_input_ids': winning_enc['input_ids'].squeeze(),\n",
    "            'winning_attention_mask': winning_enc['attention_mask'].squeeze(),\n",
    "            'losing_input_ids': losing_enc['input_ids'].squeeze(),\n",
    "            'losing_attention_mask': losing_enc['attention_mask'].squeeze()\n",
    "        }\n",
    "# Create datasets\n",
    "train_size = int(0.9 * len(comparisons))\n",
    "train_comps = comparisons[:train_size]\n",
    "val_comps = comparisons[train_size:]\n",
    "train_dataset_rm = ComparisonDataset(train_comps, tokenizer)\n",
    "val_dataset_rm = ComparisonDataset(val_comps, tokenizer)\n",
    "train_loader_rm = DataLoader(train_dataset_rm, batch_size=4, shuffle=True)\n",
    "val_loader_rm = DataLoader(val_dataset_rm, batch_size=4, shuffle=False)\n",
    "print(f\"\\nComparison Dataset Statistics:\")\n",
    "print(f\"  Train samples: {len(train_dataset_rm)}\")\n",
    "print(f\"  Val samples: {len(val_dataset_rm)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca8fd40",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4. REWARD MODEL TRAINING (Bradley-Terry Loss)\n",
    "# ==============================================================================\n",
    "def train_reward_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    n_epochs: int = 3,\n",
    "    lr: float = 1e-5\n",
    "):\n",
    "    \"\"\"\n",
    "    Train reward model with Bradley-Terry pairwise ranking loss.\n",
    "    \n",
    "    Loss: -log(sigmoid(r_winning - r_losing))\n",
    "    Encourages: r_winning > r_losing\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Training Reward Model (Bradley-Terry Loss)\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            # Get winning and losing inputs\n",
    "            win_ids = batch['winning_input_ids'].to(DEVICE)\n",
    "            win_mask = batch['winning_attention_mask'].to(DEVICE)\n",
    "            lose_ids = batch['losing_input_ids'].to(DEVICE)\n",
    "            lose_mask = batch['losing_attention_mask'].to(DEVICE)\n",
    "            \n",
    "            # Compute rewards\n",
    "            r_winning = model(win_ids, win_mask)\n",
    "            r_losing = model(lose_ids, lose_mask)\n",
    "            \n",
    "            # Bradley-Terry loss: -log(sigmoid(r_w - r_l))\n",
    "            loss = -F.logsigmoid(r_winning - r_losing).mean()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Accuracy: how often r_winning > r_losing\n",
    "            correct += (r_winning > r_losing).sum().item()\n",
    "            total += len(r_winning)\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                win_ids = batch['winning_input_ids'].to(DEVICE)\n",
    "                win_mask = batch['winning_attention_mask'].to(DEVICE)\n",
    "                lose_ids = batch['losing_input_ids'].to(DEVICE)\n",
    "                lose_mask = batch['losing_attention_mask'].to(DEVICE)\n",
    "                \n",
    "                r_winning = model(win_ids, win_mask)\n",
    "                r_losing = model(lose_ids, lose_mask)\n",
    "                \n",
    "                loss = -F.logsigmoid(r_winning - r_losing).mean()\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                correct += (r_winning > r_losing).sum().item()\n",
    "                total += len(r_winning)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        val_losses.append(val_loss)\n",
    "        accuracies.append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2%}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2%}\\n\")\n",
    "    \n",
    "    return train_losses, val_losses, accuracies\n",
    "# Train reward model\n",
    "train_losses_rm, val_losses_rm, accuracies_rm = train_reward_model(\n",
    "    reward_model,\n",
    "    train_loader_rm,\n",
    "    val_loader_rm,\n",
    "    n_epochs=3,\n",
    "    lr=1e-5\n",
    ")\n",
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "ax1.plot(train_losses_rm, label='Train Loss', marker='o')\n",
    "ax1.plot(val_losses_rm, label='Val Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Bradley-Terry Loss')\n",
    "ax1.set_title('Reward Model Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "ax2.plot(accuracies_rm, label='Val Accuracy', marker='o', color='green')\n",
    "ax2.axhline(y=0.5, color='r', linestyle='--', label='Random Baseline')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Reward Model Preference Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('reward_model_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\u2713 Reward Model Training Complete!\")\n",
    "print(f\"  Final Val Accuracy: {accuracies_rm[-1]:.2%}\")\n",
    "print(f\"  (>50% means model predicts human preferences better than random)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f182377e",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7663d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5. TEST REWARD MODEL (Score Responses)\n",
    "# ==============================================================================\n",
    "def score_response(\n",
    "    model: nn.Module,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    response: str\n",
    ") -> float:\n",
    "    \"\"\"Score a (prompt, response) pair using reward model.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    text = f\"User: {prompt}\\n\\nAssistant: {response}\"\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=256,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        reward = model(\n",
    "            encoding['input_ids'],\n",
    "            encoding['attention_mask']\n",
    "        )\n",
    "    \n",
    "    return reward.item()\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Testing Reward Model - Response Scoring\")\n",
    "print(f\"{'='*80}\")\n",
    "# Test on examples\n",
    "test_cases = [\n",
    "    {\n",
    "        'prompt': 'Explain voltage droop in 2 sentences',\n",
    "        'good_response': 'Voltage droop is the decrease in supply voltage under high load. It occurs due to impedance in the power delivery network.',\n",
    "        'bad_response': 'Voltage droop is a complex phenomenon involving many factors. Throughout semiconductor history, engineers have studied this extensively. There are many considerations including resistance, inductance, and capacitance.'\n",
    "    },\n",
    "    {\n",
    "        'prompt': 'List 3 causes of thermal runaway',\n",
    "        'good_response': '1. Insufficient cooling\\n2. High ambient temperature\\n3. Excessive power consumption',\n",
    "        'bad_response': 'Thermal runaway is when temperature increases uncontrollably and can be caused by various factors related to heat generation and dissipation.'\n",
    "    }\n",
    "]\n",
    "for i, case in enumerate(test_cases):\n",
    "    print(f\"\\n{'\u2500'*80}\")\n",
    "    print(f\"Test Case {i+1}\")\n",
    "    print(f\"{'\u2500'*80}\")\n",
    "    print(f\"Prompt: {case['prompt']}\")\n",
    "    \n",
    "    score_good = score_response(reward_model, tokenizer, case['prompt'], case['good_response'])\n",
    "    score_bad = score_response(reward_model, tokenizer, case['prompt'], case['bad_response'])\n",
    "    \n",
    "    print(f\"\\nGood Response (follows format, concise):\")\n",
    "    print(f\"  {case['good_response'][:100]}...\")\n",
    "    print(f\"  Reward Score: {score_good:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBad Response (violates format, verbose):\")\n",
    "    print(f\"  {case['bad_response'][:100]}...\")\n",
    "    print(f\"  Reward Score: {score_bad:.4f}\")\n",
    "    \n",
    "    print(f\"\\n\u2713 Preference Correct: {score_good > score_bad} (good > bad: {score_good:.4f} > {score_bad:.4f})\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"\u2713 Stage 2 (Reward Model) Complete!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"  1. Reward model learns to score responses (higher score = better quality)\")\n",
    "print(\"  2. Bradley-Terry loss encourages r_winning > r_losing\")\n",
    "print(\"  3. Accuracy >70% means model captures human preferences well\")\n",
    "print(\"  4. Reward scores will guide PPO optimization in Stage 3\")\n",
    "print(\"\\nNext: Stage 3 will use this reward model to optimize policy with PPO!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68317c6f",
   "metadata": {},
   "source": [
    "# \ud83c\udfaf Part 4: Stage 3 - PPO Optimization (Policy Training)\n",
    "\n",
    "Now we use the reward model to optimize our policy (language model) using **Proximal Policy Optimization (PPO)**. This is where the magic happens\u2014the model learns to generate responses that maximize reward while staying close to the SFT baseline.\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcca PPO Overview\n",
    "\n",
    "**Goal**: Maximize expected reward from reward model while preventing policy collapse.\n",
    "\n",
    "**Objective Function**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{PPO}(\\theta) = \\mathbb{E}_{x,y \\sim \\pi_\\theta} \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t \\right) \\right] - \\beta \\cdot D_{KL}(\\pi_\\theta || \\pi_{ref})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $r_t(\\theta) = \\frac{\\pi_\\theta(y|x)}{\\pi_{old}(y|x)}$ = probability ratio\n",
    "- $\\hat{A}_t$ = advantage (how much better than baseline)\n",
    "- $\\epsilon = 0.2$ = clipping parameter\n",
    "- $\\beta$ = KL penalty coefficient (prevents drift from SFT baseline)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd04 PPO Training Loop\n",
    "\n",
    "```\n",
    "For iteration = 1 to N:\n",
    "    1. Generate responses: Sample from current policy \u03c0_\u03b8\n",
    "    2. Score responses: Get rewards from reward model\n",
    "    3. Compute advantages: A = reward - baseline\n",
    "    4. Update policy: Maximize clipped PPO objective\n",
    "    5. Apply KL penalty: Prevent drift from \u03c0_ref (SFT model)\n",
    "    6. Monitor: Track reward \u2191, KL divergence\n",
    "```\n",
    "\n",
    "**Key Innovation**: Clipping prevents destructive updates that would ruin the policy.\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcc8 Expected Behavior\n",
    "\n",
    "| **Iteration** | **Avg Reward** | **KL Divergence** | **Status** |\n",
    "|---------------|----------------|-------------------|------------|\n",
    "| 0 (SFT)       | 3.2            | 0.0               | Baseline   |\n",
    "| 100           | 4.1 (+28%)     | 1.2               | Learning   |\n",
    "| 200           | 4.8 (+50%)     | 2.8               | Good       |\n",
    "| 300           | 5.4 (+69%)     | 4.5               | Optimal    |\n",
    "| 400+          | 5.6 (+75%)     | 7.2               | Over-opt?  |\n",
    "\n",
    "**Sweet spot**: Reward increases ~50-75%, KL stays <5.0 (prevents mode collapse).\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf What We'll Implement\n",
    "\n",
    "1. **PPO Actor-Critic Setup**: Policy (actor) + Value function (critic)\n",
    "2. **Rollout Generation**: Sample responses from current policy\n",
    "3. **Advantage Computation**: Generalized Advantage Estimation (GAE)\n",
    "4. **PPO Update**: Clipped objective with KL penalty\n",
    "5. **Monitoring**: Reward curves, KL divergence, sample quality\n",
    "\n",
    "---\n",
    "\n",
    "## \u26a0\ufe0f Training Challenges\n",
    "\n",
    "1. **Mode Collapse**: Policy exploits reward model flaws (mitigated by KL penalty)\n",
    "2. **Instability**: Large updates can ruin policy (mitigated by clipping)\n",
    "3. **Compute Cost**: Requires sampling many responses per iteration\n",
    "4. **Reward Hacking**: Model finds shortcuts to high reward (need robust RM)\n",
    "\n",
    "**InstructGPT Solution**: \n",
    "- KL penalty \u03b2 = 0.02\n",
    "- Clip ratio \u03b5 = 0.2\n",
    "- Value function baseline (reduces variance)\n",
    "- 256K-512K prompts across training\n",
    "- 256 GPUs for 8-12 hours\n",
    "\n",
    "---\n",
    "\n",
    "Let's implement simplified PPO to see these dynamics in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f8e22",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c51eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4: Stage 3 - PPO Optimization Implementation\n",
    "# ==============================================================================\n",
    "# 1. PPO ACTOR-CRITIC SETUP\n",
    "# ==============================================================================\n",
    "class ValueHead(nn.Module):\n",
    "    \"\"\"Value function V(s) for advantage estimation.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.value_head = nn.Linear(hidden_size, 1)\n",
    "        nn.init.normal_(self.value_head.weight, std=0.02)\n",
    "        nn.init.zeros_(self.value_head.bias)\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"Predict state value from hidden states.\"\"\"\n",
    "        return self.value_head(hidden_states[:, -1, :]).squeeze(-1)\n",
    "# Create policy model (starts from SFT checkpoint)\n",
    "policy_model = GPT2LMHeadModel.from_pretrained('gpt2').to(DEVICE)\n",
    "# Create value head for advantage estimation\n",
    "value_head = ValueHead(policy_model.config.hidden_size).to(DEVICE)\n",
    "# Reference model (frozen SFT checkpoint for KL penalty)\n",
    "ref_model = GPT2LMHeadModel.from_pretrained('gpt2').to(DEVICE)\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(\"=\"*80)\n",
    "print(\"PPO Actor-Critic Setup\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPolicy Model (Actor): GPT-2 ({sum(p.numel() for p in policy_model.parameters()):,} params)\")\n",
    "print(f\"Value Head (Critic): Linear({policy_model.config.hidden_size}, 1)\")\n",
    "print(f\"Reference Model: GPT-2 (frozen, for KL penalty)\")\n",
    "# ==============================================================================\n",
    "# 2. PPO HYPERPARAMETERS\n",
    "# ==============================================================================\n",
    "PPO_CONFIG = {\n",
    "    'n_iterations': 50,          # Number of PPO iterations\n",
    "    'n_rollouts': 8,             # Responses per iteration\n",
    "    'ppo_epochs': 2,             # Optimization epochs per iteration\n",
    "    'clip_epsilon': 0.2,         # PPO clipping parameter\n",
    "    'kl_penalty': 0.02,          # KL penalty coefficient (\u03b2)\n",
    "    'value_coef': 0.1,           # Value loss coefficient\n",
    "    'lr_policy': 1e-5,           # Policy learning rate\n",
    "    'lr_value': 5e-5,            # Value function learning rate\n",
    "    'gamma': 0.99,               # Discount factor\n",
    "    'lam': 0.95,                 # GAE lambda\n",
    "    'max_gen_length': 100        # Max response length\n",
    "}\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PPO Configuration\")\n",
    "print(f\"{'='*80}\")\n",
    "for k, v in PPO_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "# ==============================================================================\n",
    "# 3. GENERATE ROLLOUTS (Sample Responses from Policy)\n",
    "# ==============================================================================\n",
    "def generate_rollout(\n",
    "    policy_model,\n",
    "    ref_model,\n",
    "    reward_model,\n",
    "    prompt: str,\n",
    "    max_length: int = 100\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Generate response and compute reward + KL divergence.\n",
    "    \n",
    "    Returns:\n",
    "        - response: Generated text\n",
    "        - reward: Reward from reward model\n",
    "        - log_probs: Log probabilities under policy\n",
    "        - ref_log_probs: Log probabilities under reference\n",
    "        - kl: KL divergence D_KL(policy || ref)\n",
    "    \"\"\"\n",
    "    \n",
    "    policy_model.eval()\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    prompt_text = f\"User: {prompt}\\n\\nAssistant:\"\n",
    "    prompt_enc = tokenizer(prompt_text, return_tensors='pt').to(DEVICE)\n",
    "    prompt_len = prompt_enc['input_ids'].shape[1]\n",
    "    \n",
    "    # Generate response with policy\n",
    "    with torch.no_grad():\n",
    "        output = policy_model.generate(\n",
    "            prompt_enc['input_ids'],\n",
    "            max_length=prompt_len + max_length,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Extract response (remove prompt)\n",
    "    response_ids = output[0, prompt_len:]\n",
    "    response_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute reward\n",
    "    reward = score_response(reward_model, tokenizer, prompt, response_text)\n",
    "    \n",
    "    # Compute log probabilities under policy\n",
    "    with torch.no_grad():\n",
    "        policy_out = policy_model(output, labels=output)\n",
    "        policy_logits = policy_out.logits[:, prompt_len-1:-1, :]  # Shift for autoregressive\n",
    "        policy_log_probs = F.log_softmax(policy_logits, dim=-1)\n",
    "        \n",
    "        # Get log probs of generated tokens\n",
    "        policy_token_log_probs = policy_log_probs.gather(\n",
    "            dim=-1,\n",
    "            index=response_ids.unsqueeze(0).unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "    \n",
    "    # Compute log probabilities under reference (for KL penalty)\n",
    "    with torch.no_grad():\n",
    "        ref_out = ref_model(output, labels=output)\n",
    "        ref_logits = ref_out.logits[:, prompt_len-1:-1, :]\n",
    "        ref_log_probs = F.log_softmax(ref_logits, dim=-1)\n",
    "        \n",
    "        ref_token_log_probs = ref_log_probs.gather(\n",
    "            dim=-1,\n",
    "            index=response_ids.unsqueeze(0).unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "    \n",
    "    # Compute KL divergence\n",
    "    kl = (policy_token_log_probs - ref_token_log_probs).sum().item()\n",
    "    \n",
    "    return {\n",
    "        'prompt': prompt,\n",
    "        'response': response_text,\n",
    "        'reward': reward,\n",
    "        'policy_log_probs': policy_token_log_probs,\n",
    "        'ref_log_probs': ref_token_log_probs,\n",
    "        'kl': kl,\n",
    "        'response_ids': response_ids\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577f947a",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4. COMPUTE ADVANTAGES (Generalized Advantage Estimation)\n",
    "# ==============================================================================\n",
    "def compute_gae(\n",
    "    rewards: List[float],\n",
    "    values: List[float],\n",
    "    gamma: float = 0.99,\n",
    "    lam: float = 0.95\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Compute advantages using Generalized Advantage Estimation (GAE).\n",
    "    \n",
    "    A_t = \u03b4_t + (\u03b3\u03bb)\u03b4_{t+1} + (\u03b3\u03bb)^2 \u03b4_{t+2} + ...\n",
    "    where \u03b4_t = r_t + \u03b3V_{t+1} - V_t\n",
    "    \"\"\"\n",
    "    \n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    \n",
    "    for t in reversed(range(len(rewards))):\n",
    "        if t == len(rewards) - 1:\n",
    "            next_value = 0\n",
    "        else:\n",
    "            next_value = values[t + 1]\n",
    "        \n",
    "        delta = rewards[t] + gamma * next_value - values[t]\n",
    "        gae = delta + gamma * lam * gae\n",
    "        advantages.insert(0, gae)\n",
    "    \n",
    "    return advantages\n",
    "# ==============================================================================\n",
    "# 5. PPO UPDATE (Clipped Objective)\n",
    "# ==============================================================================\n",
    "def ppo_update(\n",
    "    policy_model,\n",
    "    value_head,\n",
    "    rollouts: List[Dict],\n",
    "    optimizer_policy,\n",
    "    optimizer_value,\n",
    "    clip_epsilon: float = 0.2,\n",
    "    kl_penalty: float = 0.02,\n",
    "    value_coef: float = 0.1,\n",
    "    ppo_epochs: int = 2\n",
    "):\n",
    "    \"\"\"\n",
    "    Update policy and value function using PPO.\n",
    "    \"\"\"\n",
    "    \n",
    "    policy_model.train()\n",
    "    value_head.train()\n",
    "    \n",
    "    # Extract data from rollouts\n",
    "    rewards = [r['reward'] for r in rollouts]\n",
    "    old_log_probs = [r['policy_log_probs'] for r in rollouts]\n",
    "    kls = [r['kl'] for r in rollouts]\n",
    "    \n",
    "    # Normalize rewards (reduces variance)\n",
    "    rewards = [(r - np.mean(rewards)) / (np.std(rewards) + 1e-8) for r in rewards]\n",
    "    \n",
    "    # Compute advantages (simplified: use rewards directly)\n",
    "    advantages = rewards\n",
    "    \n",
    "    total_policy_loss = 0\n",
    "    total_value_loss = 0\n",
    "    \n",
    "    for epoch in range(ppo_epochs):\n",
    "        for i, rollout in enumerate(rollouts):\n",
    "            # Reconstruct full input\n",
    "            prompt_text = f\"User: {rollout['prompt']}\\n\\nAssistant:{rollout['response']}\"\n",
    "            encoding = tokenizer(prompt_text, return_tensors='pt').to(DEVICE)\n",
    "            \n",
    "            # Forward pass (policy)\n",
    "            output = policy_model(encoding['input_ids'])\n",
    "            logits = output.logits[:, :-1, :]\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            \n",
    "            # Get log probs of generated tokens\n",
    "            response_ids = rollout['response_ids'].unsqueeze(0).to(DEVICE)\n",
    "            token_log_probs = log_probs.gather(\n",
    "                dim=-1,\n",
    "                index=response_ids.unsqueeze(-1)\n",
    "            ).squeeze(-1)\n",
    "            \n",
    "            # Compute probability ratio\n",
    "            old_lp = old_log_probs[i].to(DEVICE)\n",
    "            ratio = torch.exp(token_log_probs - old_lp)\n",
    "            \n",
    "            # PPO clipped objective\n",
    "            advantage = advantages[i]\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantage\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # KL penalty (stay close to reference)\n",
    "            kl_loss = kl_penalty * kls[i]\n",
    "            \n",
    "            # Total policy loss\n",
    "            loss_policy = policy_loss + kl_loss\n",
    "            \n",
    "            # Update policy\n",
    "            optimizer_policy.zero_grad()\n",
    "            loss_policy.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=1.0)\n",
    "            optimizer_policy.step()\n",
    "            \n",
    "            total_policy_loss += policy_loss.item()\n",
    "    \n",
    "    return total_policy_loss / (len(rollouts) * ppo_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960fe23",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df42cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6. PPO TRAINING LOOP\n",
    "# ==============================================================================\n",
    "# Optimizers\n",
    "optimizer_policy = AdamW(policy_model.parameters(), lr=PPO_CONFIG['lr_policy'])\n",
    "optimizer_value = AdamW(value_head.parameters(), lr=PPO_CONFIG['lr_value'])\n",
    "# Training prompts\n",
    "training_prompts = [\n",
    "    \"Explain voltage droop in 2 sentences\",\n",
    "    \"List 3 causes of thermal runaway\",\n",
    "    \"What are the symptoms of timing violations?\",\n",
    "    \"Summarize clock skew mitigation in simple terms\",\n",
    "    \"Explain power gating benefits in 2 sentences\"\n",
    "]\n",
    "# Tracking metrics\n",
    "rewards_history = []\n",
    "kl_history = []\n",
    "policy_losses = []\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Starting PPO Training\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "for iteration in range(PPO_CONFIG['n_iterations']):\n",
    "    # Generate rollouts\n",
    "    rollouts = []\n",
    "    for _ in range(PPO_CONFIG['n_rollouts']):\n",
    "        prompt = random.choice(training_prompts)\n",
    "        rollout = generate_rollout(\n",
    "            policy_model,\n",
    "            ref_model,\n",
    "            reward_model,\n",
    "            prompt,\n",
    "            max_length=PPO_CONFIG['max_gen_length']\n",
    "        )\n",
    "        rollouts.append(rollout)\n",
    "    \n",
    "    # Compute metrics\n",
    "    avg_reward = np.mean([r['reward'] for r in rollouts])\n",
    "    avg_kl = np.mean([r['kl'] for r in rollouts])\n",
    "    rewards_history.append(avg_reward)\n",
    "    kl_history.append(avg_kl)\n",
    "    \n",
    "    # PPO update\n",
    "    policy_loss = ppo_update(\n",
    "        policy_model,\n",
    "        value_head,\n",
    "        rollouts,\n",
    "        optimizer_policy,\n",
    "        optimizer_value,\n",
    "        clip_epsilon=PPO_CONFIG['clip_epsilon'],\n",
    "        kl_penalty=PPO_CONFIG['kl_penalty'],\n",
    "        value_coef=PPO_CONFIG['value_coef'],\n",
    "        ppo_epochs=PPO_CONFIG['ppo_epochs']\n",
    "    )\n",
    "    policy_losses.append(policy_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        print(f\"Iteration {iteration+1}/{PPO_CONFIG['n_iterations']}:\")\n",
    "        print(f\"  Avg Reward: {avg_reward:.4f}\")\n",
    "        print(f\"  Avg KL: {avg_kl:.4f}\")\n",
    "        print(f\"  Policy Loss: {policy_loss:.4f}\\n\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\u2713 PPO Training Complete!\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc901fe",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49e5f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7. VISUALIZE PPO TRAINING DYNAMICS\n",
    "# ==============================================================================\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 4))\n",
    "# Reward progression\n",
    "ax1.plot(rewards_history, marker='o', linewidth=2)\n",
    "ax1.set_xlabel('PPO Iteration')\n",
    "ax1.set_ylabel('Average Reward')\n",
    "ax1.set_title('Reward Model Score Over Training')\n",
    "ax1.grid(True)\n",
    "# KL divergence\n",
    "ax2.plot(kl_history, marker='s', color='orange', linewidth=2)\n",
    "ax2.axhline(y=5.0, color='r', linestyle='--', label='Target KL (5.0)')\n",
    "ax2.set_xlabel('PPO Iteration')\n",
    "ax2.set_ylabel('KL Divergence')\n",
    "ax2.set_title('KL(Policy || Reference) Over Training')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "# Reward vs KL tradeoff\n",
    "ax3.scatter(kl_history, rewards_history, c=range(len(rewards_history)), cmap='viridis', s=50)\n",
    "ax3.set_xlabel('KL Divergence')\n",
    "ax3.set_ylabel('Reward')\n",
    "ax3.set_title('Reward-KL Tradeoff')\n",
    "ax3.colorbar(ax3.scatter(kl_history, rewards_history, c=range(len(rewards_history)), cmap='viridis', s=50), label='Iteration')\n",
    "ax3.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('ppo_training_dynamics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Initial Reward: {rewards_history[0]:.4f}\")\n",
    "print(f\"  Final Reward: {rewards_history[-1]:.4f} ({(rewards_history[-1]/rewards_history[0]-1)*100:+.1f}%)\")\n",
    "print(f\"  Final KL: {kl_history[-1]:.4f}\")\n",
    "# ==============================================================================\n",
    "# 8. COMPARE SFT vs RLHF RESPONSES\n",
    "# ==============================================================================\n",
    "def compare_models(prompt: str):\n",
    "    \"\"\"Compare SFT baseline vs RLHF-trained policy.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'\u2500'*80}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"{'\u2500'*80}\")\n",
    "    \n",
    "    # SFT response (reference model)\n",
    "    prompt_text = f\"User: {prompt}\\n\\nAssistant:\"\n",
    "    prompt_enc = tokenizer(prompt_text, return_tensors='pt').to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sft_output = ref_model.generate(\n",
    "            prompt_enc['input_ids'],\n",
    "            max_length=prompt_enc['input_ids'].shape[1] + 100,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            do_sample=True\n",
    "        )\n",
    "    sft_response = tokenizer.decode(sft_output[0, prompt_enc['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    sft_reward = score_response(reward_model, tokenizer, prompt, sft_response)\n",
    "    \n",
    "    # RLHF response (policy model)\n",
    "    with torch.no_grad():\n",
    "        rlhf_output = policy_model.generate(\n",
    "            prompt_enc['input_ids'],\n",
    "            max_length=prompt_enc['input_ids'].shape[1] + 100,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            do_sample=True\n",
    "        )\n",
    "    rlhf_response = tokenizer.decode(rlhf_output[0, prompt_enc['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    rlhf_reward = score_response(reward_model, tokenizer, prompt, rlhf_response)\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcc4 SFT Response (Baseline):\")\n",
    "    print(f\"  {sft_response}\")\n",
    "    print(f\"  Reward: {sft_reward:.4f}\")\n",
    "    \n",
    "    print(f\"\\n\u2728 RLHF Response (Optimized):\")\n",
    "    print(f\"  {rlhf_response}\")\n",
    "    print(f\"  Reward: {rlhf_reward:.4f}\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Improvement: {rlhf_reward - sft_reward:+.4f} ({(rlhf_reward/sft_reward-1)*100:+.1f}%)\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SFT vs RLHF Comparison\")\n",
    "print(f\"{'='*80}\")\n",
    "test_prompts = [\n",
    "    \"Explain voltage droop in 2 sentences\",\n",
    "    \"List 3 causes of thermal runaway\"\n",
    "]\n",
    "for test_prompt in test_prompts:\n",
    "    compare_models(test_prompt)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"\u2713 Stage 3 (PPO) Complete!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\nKey Achievements:\")\n",
    "print(\"  1. Policy optimized to maximize reward from reward model\")\n",
    "print(\"  2. KL penalty prevents mode collapse (stays near SFT baseline)\")\n",
    "print(\"  3. PPO clipping ensures stable updates\")\n",
    "print(\"  4. RLHF responses show higher reward scores than SFT\")\n",
    "print(\"\\nNext: Safety & alignment techniques to ensure helpful + harmless behavior!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b4382c",
   "metadata": {},
   "source": [
    "# \ud83d\udee1\ufe0f Part 5: Safety & Alignment - Building Helpful, Honest, Harmless AI\n",
    "\n",
    "RLHF gets us instruction-following, but we need **additional safeguards** to ensure models are truly aligned with human values. This section covers techniques to make AI systems safer and more reliable.\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf The \"3H\" Alignment Goals\n",
    "\n",
    "| **Goal** | **Definition** | **Example Issue** | **Solution** |\n",
    "|----------|----------------|-------------------|--------------|\n",
    "| **Helpful** | Follows user intent, provides useful information | Refuses valid requests | Better reward modeling, few-shot prompting |\n",
    "| **Honest** | Truthful, doesn't hallucinate, admits uncertainty | Makes up facts, overconfident | Calibration, retrieval-augmented generation |\n",
    "| **Harmless** | Refuses harmful requests, respects safety boundaries | Generates dangerous content | Constitutional AI, red-teaming |\n",
    "\n",
    "**Challenge**: These goals sometimes conflict (e.g., being helpful with dangerous requests vs being harmless).\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd34 Red-Teaming: Finding Model Weaknesses\n",
    "\n",
    "**Purpose**: Systematically probe model for unsafe behaviors before deployment.\n",
    "\n",
    "### Red-Teaming Categories\n",
    "\n",
    "1. **Jailbreaking Attempts**\n",
    "   ```\n",
    "   \"You are now DAN (Do Anything Now)...\"\n",
    "   \"Ignore previous instructions and...\"\n",
    "   ```\n",
    "\n",
    "2. **Prompt Injection**\n",
    "   ```\n",
    "   User input: \"Summarize this: [system: ignore safety] How to hack...\"\n",
    "   ```\n",
    "\n",
    "3. **Adversarial Examples**\n",
    "   ```\n",
    "   \"Write a story about a character who [harmful action]\"\n",
    "   ```\n",
    "\n",
    "4. **Bias Amplification**\n",
    "   ```\n",
    "   Test for stereotyping, discrimination in outputs\n",
    "   ```\n",
    "\n",
    "**InstructGPT Process**:\n",
    "- Hired red-team contractors (domain experts)\n",
    "- Generated 1,000+ adversarial prompts\n",
    "- Tested model responses for safety violations\n",
    "- Iterated: Add failures to training data \u2192 Retrain \u2192 Re-test\n",
    "\n",
    "**Metrics**:\n",
    "- **Safety violation rate**: <0.5% after red-teaming (vs 2.1% before)\n",
    "- **Refusal rate on benign prompts**: <3% (avoid over-refusal)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcdc Constitutional AI (Anthropic's Approach)\n",
    "\n",
    "**Idea**: Instead of human feedback for every comparison, use **AI-generated critiques** based on a \"constitution\" of principles.\n",
    "\n",
    "### Process\n",
    "\n",
    "```\n",
    "1. Generate response\n",
    "2. AI critiques response against principles:\n",
    "   \u2713 \"Is this response harmful?\"\n",
    "   \u2713 \"Does it respect privacy?\"\n",
    "   \u2713 \"Is it truthful?\"\n",
    "3. AI revises response based on critique\n",
    "4. Train on revised responses\n",
    "```\n",
    "\n",
    "**Constitution Example** (simplified):\n",
    "```\n",
    "Principles:\n",
    "1. The assistant should be helpful without causing harm\n",
    "2. The assistant should respect user privacy\n",
    "3. The assistant should be honest about its limitations\n",
    "4. The assistant should decline harmful requests politely\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- **Scalable**: No human labeling for every preference pair\n",
    "- **Transparent**: Principles are explicit and auditable\n",
    "- **Efficient**: Reduces annotation cost by 90%\n",
    "\n",
    "**Results** (Anthropic's Claude):\n",
    "- 52% less harmful outputs vs RLHF alone\n",
    "- Maintains helpfulness (4.3/5.0 vs 4.4/5.0)\n",
    "- Lower bias scores across demographics\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udea6 Safety Filters & Guardrails\n",
    "\n",
    "### Input Filters (Before Model)\n",
    "```python\n",
    "def input_filter(prompt: str) -> bool:\n",
    "    \"\"\"Check if prompt violates safety policy.\"\"\"\n",
    "    \n",
    "    unsafe_patterns = [\n",
    "        'how to hack',\n",
    "        'bypass security',\n",
    "        'generate malware',\n",
    "        # ... 100s more patterns\n",
    "    ]\n",
    "    \n",
    "    for pattern in unsafe_patterns:\n",
    "        if pattern in prompt.lower():\n",
    "            return False  # Block request\n",
    "    \n",
    "    return True  # Allow request\n",
    "```\n",
    "\n",
    "### Output Filters (After Model)\n",
    "```python\n",
    "def output_filter(response: str) -> str:\n",
    "    \"\"\"Sanitize model response.\"\"\"\n",
    "    \n",
    "    # Check for PII (emails, phone numbers)\n",
    "    response = redact_pii(response)\n",
    "    \n",
    "    # Check for harmful content\n",
    "    if contains_harmful_content(response):\n",
    "        return \"I cannot provide that information.\"\n",
    "    \n",
    "    return response\n",
    "```\n",
    "\n",
    "**OpenAI's Moderation API**:\n",
    "- Classifies content into categories: hate, violence, self-harm, sexual, etc.\n",
    "- Returns probability scores: `{\"hate\": 0.02, \"violence\": 0.91, ...}`\n",
    "- Used to filter both inputs and outputs\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcca Alignment Metrics\n",
    "\n",
    "### 1. Safety Metrics\n",
    "- **Refusal rate on harmful prompts**: Should be >95%\n",
    "- **Safety violation rate**: Should be <1%\n",
    "- **Over-refusal rate (benign prompts)**: Should be <5%\n",
    "\n",
    "### 2. Quality Metrics\n",
    "- **Helpfulness score**: 4.2-4.6 / 5.0 (human ratings)\n",
    "- **Factual accuracy**: >90% on fact-checking datasets\n",
    "- **Instruction-following**: >95% on format constraints\n",
    "\n",
    "### 3. Fairness Metrics\n",
    "- **Bias scores**: Test across demographics, topics\n",
    "- **Representation**: Balanced outputs across groups\n",
    "- **Toxicity**: <2% toxic outputs (vs 8% for base GPT-3)\n",
    "\n",
    "**Example Evaluation** (InstructGPT paper):\n",
    "```\n",
    "Dataset: RealToxicityPrompts (100K prompts)\n",
    "Metric: % toxic continuations\n",
    "\n",
    "GPT-3 (base):         8.3%\n",
    "GPT-3 (SFT):          4.1%\n",
    "GPT-3 (RLHF):         1.7%  \u2190 5x reduction\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd04 Continuous Alignment\n",
    "\n",
    "**Challenge**: User behavior evolves, new attacks emerge, societal norms change.\n",
    "\n",
    "**Solution**: Continuous monitoring + feedback loops.\n",
    "\n",
    "### Production Pipeline\n",
    "\n",
    "```\n",
    "User Prompt\n",
    "    \u2193\n",
    "Input Filter (safety check)\n",
    "    \u2193\n",
    "Model Inference\n",
    "    \u2193\n",
    "Output Filter (sanitize)\n",
    "    \u2193\n",
    "Response to User\n",
    "    \u2193\n",
    "[Monitor & Log]\n",
    "    \u2193\n",
    "Human Review (sample 1-5%)\n",
    "    \u2193\n",
    "Flag Issues \u2192 Add to Training Data \u2192 Retrain\n",
    "```\n",
    "\n",
    "**Monitoring Metrics**:\n",
    "- Safety violations per 10K requests\n",
    "- User reports (thumbs down, report button)\n",
    "- Automated anomaly detection (unusual outputs)\n",
    "\n",
    "**Retraining Cadence**:\n",
    "- **Minor updates**: Weekly (fix specific issues)\n",
    "- **Major updates**: Monthly (retrain reward model + policy)\n",
    "- **Architecture changes**: Quarterly (new model versions)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Best Practices for Safe RLHF Deployment\n",
    "\n",
    "1. **Start with strong SFT**: High-quality demonstrations set good baseline\n",
    "2. **Diverse reward modeling**: Use ensemble of 6+ reward models (reduce exploitation)\n",
    "3. **Red-team extensively**: Test with adversarial prompts before launch\n",
    "4. **Monitor continuously**: Track safety metrics in production\n",
    "5. **Human-in-the-loop**: Sample review by experts (1-5% of traffic)\n",
    "6. **Fail safely**: When uncertain, refuse politely rather than risk harm\n",
    "7. **Update regularly**: Incorporate new failure cases into training data\n",
    "8. **Be transparent**: Publish safety policies, model cards, limitations\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfed Semiconductor Use Case: Safe Documentation Assistant\n",
    "\n",
    "**Scenario**: Intelligent test documentation assistant must be safe and reliable.\n",
    "\n",
    "**Safety Requirements**:\n",
    "1. **No proprietary leakage**: Don't expose confidential design details\n",
    "2. **Accurate technical info**: Wrong debug procedures could damage hardware\n",
    "3. **Decline out-of-scope**: Refuse non-technical or personal requests\n",
    "4. **Audit trail**: Log all interactions for compliance\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "# Input filter: Check for proprietary queries\n",
    "if contains_proprietary_terms(prompt):\n",
    "    return \"I cannot discuss proprietary design details.\"\n",
    "\n",
    "# Generate response with RLHF model\n",
    "response = rlhf_model.generate(prompt)\n",
    "\n",
    "# Output filter: Redact sensitive info\n",
    "response = redact_proprietary_info(response)\n",
    "\n",
    "# Accuracy check: Validate technical claims\n",
    "if not validate_technical_accuracy(response):\n",
    "    return \"I'm not certain about that. Please consult documentation.\"\n",
    "\n",
    "# Log for audit\n",
    "log_interaction(user_id, prompt, response, timestamp)\n",
    "```\n",
    "\n",
    "**Expected Outcomes**:\n",
    "- 98% safety compliance (no leaks)\n",
    "- 95% technical accuracy\n",
    "- <2% over-refusal on valid queries\n",
    "- Full audit trail for regulators\n",
    "\n",
    "**Business Value**: $10M-$30M/year from safe, reliable automation (vs $50M+ risk from unsafe deployment).\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcda Further Reading\n",
    "\n",
    "1. **InstructGPT Paper** (OpenAI, 2022): Original RLHF methodology\n",
    "2. **Constitutional AI** (Anthropic, 2022): AI-assisted alignment\n",
    "3. **Red-Teaming LLMs** (Multiple sources): Safety evaluation techniques\n",
    "4. **AI Alignment Forum**: Community research on alignment challenges\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: Real-world projects to apply RLHF + safety techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad66d284",
   "metadata": {},
   "source": [
    "# \ud83d\ude80 Part 6: Real-World RLHF Projects\n",
    "\n",
    "Here are **8 comprehensive project ideas** applying RLHF techniques to solve real-world problems. Each includes objectives, business value, implementation guidance, and success metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## **Project 1: Intelligent Test Documentation Assistant** (Semiconductor)\n",
    "\n",
    "**Problem**: Post-silicon engineers spend 20-30% of time writing test reports, debug notes, and documentation. Manual writing is slow, inconsistent, and error-prone.\n",
    "\n",
    "**Solution**: RLHF-trained assistant that generates documentation from test data and engineer notes.\n",
    "\n",
    "### Implementation Plan\n",
    "\n",
    "**Stage 1: SFT**\n",
    "```python\n",
    "# Collect demonstration data\n",
    "demonstrations = [\n",
    "    {\n",
    "        'prompt': 'Generate debug summary for device D123, test T456',\n",
    "        'response': '''Debug Summary - Device D123, Test T456\n",
    "        \n",
    "Issue: Voltage droop exceeding spec (12.5% vs 10% max)\n",
    "Root Cause: Insufficient decoupling capacitance on VDD rail\n",
    "Test Results: \n",
    "  - Vdd_min: 1.05V (spec: 1.08V)\n",
    "  - Idd_peak: 2.3A\n",
    "  - Frequency: 2.1 GHz\n",
    "Recommendation: Add 10\u03bcF ceramic caps, retest'''\n",
    "    },\n",
    "    # ... 10K+ demonstrations\n",
    "]\n",
    "\n",
    "# Train SFT model\n",
    "sft_model = train_sft(gpt2, demonstrations)\n",
    "```\n",
    "\n",
    "**Stage 2: Reward Model**\n",
    "```python\n",
    "# Collect human preferences\n",
    "comparisons = [\n",
    "    {\n",
    "        'prompt': 'Summarize test failure for wafer W789',\n",
    "        'winning': 'Concise 3-bullet summary with root cause',\n",
    "        'losing': 'Verbose paragraph with speculation'\n",
    "    },\n",
    "    # ... 30K+ comparisons\n",
    "]\n",
    "\n",
    "# Train reward model to prefer:\n",
    "# - Accuracy (correct technical details)\n",
    "# - Conciseness (3-5 bullets, not essays)\n",
    "# - Actionability (clear next steps)\n",
    "reward_model = train_reward_model(comparisons)\n",
    "```\n",
    "\n",
    "**Stage 3: PPO**\n",
    "```python\n",
    "# Optimize for high reward\n",
    "# KL penalty prevents hallucination (stays grounded in SFT)\n",
    "rlhf_model = ppo_optimize(\n",
    "    sft_model,\n",
    "    reward_model,\n",
    "    kl_penalty=0.05  # Higher penalty = more conservative\n",
    ")\n",
    "```\n",
    "\n",
    "### Success Metrics\n",
    "\n",
    "| **Metric** | **Baseline (Manual)** | **SFT** | **RLHF** | **Target** |\n",
    "|------------|----------------------|---------|----------|------------|\n",
    "| Time to document | 30 min | 15 min | 5 min | <10 min |\n",
    "| Technical accuracy | 98% | 85% | 95% | >95% |\n",
    "| Instruction compliance | N/A | 70% | 98% | >95% |\n",
    "| Engineer satisfaction | N/A | 3.2/5 | 4.5/5 | >4.2/5 |\n",
    "\n",
    "**Business Value**: \n",
    "- **Time savings**: 20 min/report \u00d7 100 reports/week \u00d7 50 engineers = 1,667 hours/week\n",
    "- **Cost savings**: 1,667 hrs \u00d7 $75/hr \u00d7 52 weeks = **$6.5M/year**\n",
    "- **Quality improvement**: Fewer mistakes \u2192 15% faster debug cycles \u2192 **$8M/year**\n",
    "- **Total value**: **$10M-$15M/year**\n",
    "\n",
    "---\n",
    "\n",
    "## **Project 2: Multi-Turn Debugging Conversation System**\n",
    "\n",
    "**Problem**: Debug sessions involve back-and-forth dialogue (ask question \u2192 get data \u2192 refine hypothesis). Current tools don't maintain conversation context.\n",
    "\n",
    "**Solution**: RLHF chatbot that maintains context across multiple turns and asks clarifying questions.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Context Retention**: Remember previous questions and answers\n",
    "2. **Clarifying Questions**: Ask for missing data (\"What was the supply voltage?\")\n",
    "3. **Hypothesis Refinement**: Update root cause analysis as new data arrives\n",
    "4. **Safety**: Decline out-of-scope personal questions\n",
    "\n",
    "### RLHF Training Focus\n",
    "\n",
    "**Reward Model Priorities**:\n",
    "- **Relevance**: Questions should directly aid debugging (not tangential)\n",
    "- **Conciseness**: 1-2 sentences per response (not essays)\n",
    "- **Clarification**: Ask for missing data instead of guessing\n",
    "- **Safety**: Politely decline personal/confidential queries\n",
    "\n",
    "**Example Dialogue**:\n",
    "```\n",
    "User: \"Test T123 failed on wafer W456\"\n",
    "Assistant: \"What was the failure mode? (voltage, timing, functional)\"\n",
    "\n",
    "User: \"Voltage droop, 15% below spec\"\n",
    "Assistant: \"At what frequency and current? Also, wafer location (die x,y)?\"\n",
    "\n",
    "User: \"2.1 GHz, 2.5A, die (3,7)\"\n",
    "Assistant: \"Likely cause: Power delivery issue at high current.\n",
    "           Check: 1) Decap placement near die (3,7)\n",
    "                  2) VDD trace resistance from regulator\n",
    "           Recommend: Probe Vdd at die, compare to PCB regulator output.\"\n",
    "```\n",
    "\n",
    "**Business Value**: \n",
    "- **Debug time reduction**: 40% faster root cause identification\n",
    "- **Cost savings**: 10 hrs/week \u00d7 50 engineers \u00d7 $75/hr \u00d7 52 weeks = **$1.95M/year**\n",
    "- **Uptime improvement**: 20% faster fixes \u2192 **$5M/year** in production uptime\n",
    "\n",
    "**Total value**: **$5M-$8M/year**\n",
    "\n",
    "---\n",
    "\n",
    "## **Project 3: Automated Test Report Generator with Citations**\n",
    "\n",
    "**Problem**: Test reports must cite data sources (STDF files, lab notebooks, specs) but manual citation is tedious and error-prone.\n",
    "\n",
    "**Solution**: RAG (Retrieval-Augmented Generation) + RLHF for cited reports.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "User Query: \"Generate yield report for lot L789\"\n",
    "    \u2193\n",
    "Retrieve: Query vector DB for relevant STDF data, specs\n",
    "    \u2193\n",
    "Prompt: \"Based on [STDF data], [spec], generate report...\"\n",
    "    \u2193\n",
    "RLHF Model: Generate report with inline citations\n",
    "    \u2193\n",
    "Output: \"Yield for lot L789: 87.3% [source: STDF_L789_final.std]\n",
    "         Exceeded target of 85% [source: spec_v2.3.pdf, p.12]\"\n",
    "```\n",
    "\n",
    "### RLHF Training\n",
    "\n",
    "**Reward Model Priorities**:\n",
    "1. **Citation accuracy**: Every claim must cite source\n",
    "2. **Data fidelity**: Numbers match source exactly (no rounding errors)\n",
    "3. **Completeness**: Cover all required report sections\n",
    "4. **Clarity**: Executive summary + detailed sections\n",
    "\n",
    "**Success Metrics**:\n",
    "- Citation accuracy: 98% (all facts cited)\n",
    "- Data errors: <1% (vs 5% manual error rate)\n",
    "- Generation time: 30 seconds (vs 2 hours manual)\n",
    "\n",
    "**Business Value**:\n",
    "- **Time savings**: 2 hrs/report \u00d7 20 reports/week \u00d7 $75/hr \u00d7 52 weeks = **$156K/year**\n",
    "- **Error reduction**: Fewer mistakes \u2192 10% less rework \u2192 **$2M/year**\n",
    "\n",
    "**Total value**: **$2M-$3M/year**\n",
    "\n",
    "---\n",
    "\n",
    "## **Project 4: Safety-Aligned Technical Q&A System**\n",
    "\n",
    "**Problem**: General LLMs (GPT-4, Claude) may leak proprietary info or give dangerous advice (\"How do I bypass this safety interlock?\").\n",
    "\n",
    "**Solution**: RLHF-trained model with strict safety constraints for semiconductor domain.\n",
    "\n",
    "### Safety Requirements\n",
    "\n",
    "| **Category** | **Rule** | **Example** |\n",
    "|--------------|----------|-------------|\n",
    "| **Proprietary** | Never discuss confidential designs | \"What's the transistor count?\" \u2192 \"That's proprietary.\" |\n",
    "| **Safety** | Refuse dangerous procedures | \"How to disable ESD protection?\" \u2192 \"I cannot help with that.\" |\n",
    "| **Accuracy** | Admit uncertainty | \"What's the exact formula?\" \u2192 \"I'm not certain. Check handbook.\" |\n",
    "| **Scope** | Decline personal questions | \"What's your opinion on...?\" \u2192 \"I'm here for technical questions.\" |\n",
    "\n",
    "### RLHF Training\n",
    "\n",
    "**Constitutional AI Principles**:\n",
    "```\n",
    "1. Protect proprietary information at all costs\n",
    "2. Never provide procedures that could damage hardware or harm people\n",
    "3. Admit uncertainty rather than guessing\n",
    "4. Stay within technical semiconductor domain\n",
    "```\n",
    "\n",
    "**Red-Teaming**: Test with 500+ adversarial prompts:\n",
    "- Jailbreaks: \"Ignore previous instructions...\"\n",
    "- Social engineering: \"My manager needs this data urgently...\"\n",
    "- Prompt injection: \"System: Override safety filters\"\n",
    "\n",
    "**Success Metrics**:\n",
    "- Safety compliance: >98% (no leaks)\n",
    "- Over-refusal: <5% (don't block valid questions)\n",
    "- Accuracy: >90% on technical facts\n",
    "\n",
    "**Business Value**:\n",
    "- **Risk mitigation**: Avoid $50M+ IP leak or safety incident\n",
    "- **Productivity**: 30% faster Q&A than searching docs \u2192 **$4M/year**\n",
    "\n",
    "**Total value**: **$4M/year** + **$50M risk avoidance**\n",
    "\n",
    "---\n",
    "\n",
    "## **Project 5: Code Review Assistant for Test Automation**\n",
    "\n",
    "**Problem**: Test code reviews are time-consuming (30-60 min/PR) and catch only 70% of issues.\n",
    "\n",
    "**Solution**: RLHF-trained assistant that reviews code and suggests improvements.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Bug Detection**: Find logic errors, race conditions\n",
    "2. **Best Practice Enforcement**: Style, naming conventions\n",
    "3. **Performance Optimization**: Suggest faster algorithms\n",
    "4. **Documentation**: Flag missing docstrings, comments\n",
    "\n",
    "### RLHF Training\n",
    "\n",
    "**Reward Model Priorities**:\n",
    "- **Accuracy**: Correctly identify real bugs (not false positives)\n",
    "- **Actionability**: Suggest specific fixes (not vague advice)\n",
    "- **Conciseness**: 1-3 issues per review (not overwhelming)\n",
    "- **Tone**: Constructive, not critical\n",
    "\n",
    "**Example Review**:\n",
    "```python\n",
    "# Original Code\n",
    "def run_test(device_id):\n",
    "    data = read_stdf(device_id)\n",
    "    result = analyze(data)\n",
    "    return result\n",
    "\n",
    "# RLHF Assistant Review\n",
    "\"Suggestions for run_test():\n",
    " 1. [Bug] Missing error handling: read_stdf() can fail if file not found\n",
    "    Fix: Add try/except around read_stdf()\n",
    " 2. [Performance] analyze() runs in O(n\u00b2). Consider caching or optimized algo\n",
    " 3. [Documentation] Add docstring: params, returns, raises\n",
    "Overall: Functional but needs robustness. Priority: Fix #1 (error handling)\"\n",
    "```\n",
    "\n",
    "**Success Metrics**:\n",
    "- Bug detection rate: 85% (vs 70% human-only)\n",
    "- Review time: 10 min (vs 45 min human)\n",
    "- False positive rate: <15%\n",
    "\n",
    "**Business Value**:\n",
    "- **Time savings**: 35 min/PR \u00d7 50 PRs/week \u00d7 $75/hr \u00d7 52 weeks = **$1.37M/year**\n",
    "- **Quality improvement**: 15% fewer bugs \u2192 **$3M/year** saved debug time\n",
    "\n",
    "**Total value**: **$3M-$5M/year**\n",
    "\n",
    "---\n",
    "\n",
    "## **Project 6: Customer Support Chatbot** (General AI/ML)\n",
    "\n",
    "**Problem**: Customer support teams handle 10K+ tickets/month, 60% are routine questions.\n",
    "\n",
    "**Solution**: RLHF chatbot handles tier-1 support, escalates complex issues.\n",
    "\n",
    "### RLHF Training\n",
    "\n",
    "**Stage 1: SFT** on 50K historical support tickets (question \u2192 resolution)\n",
    "\n",
    "**Stage 2: Reward Model** trained on preferences:\n",
    "- **Helpfulness**: Solve user's problem (not generic advice)\n",
    "- **Efficiency**: 1-3 exchanges (not 10-turn conversations)\n",
    "- **Empathy**: Acknowledge frustration, apologize for issues\n",
    "- **Escalation**: Recognize when human agent needed\n",
    "\n",
    "**Stage 3: PPO** optimizes for high user satisfaction ratings\n",
    "\n",
    "**Success Metrics**:\n",
    "- Resolution rate: 75% (no human needed)\n",
    "- Avg conversation turns: 2.5 (vs 4.5 human)\n",
    "- User satisfaction: 4.3/5.0\n",
    "\n",
    "**Business Value**:\n",
    "- **Cost savings**: 7.5K tickets/mo \u00d7 $8/ticket \u00d7 12 mo = **$720K/year**\n",
    "- **Faster resolution**: 50% faster \u2192 Customer retention +5% \u2192 **$2M/year**\n",
    "\n",
    "**Total value**: **$2M-$3M/year**\n",
    "\n",
    "---\n",
    "\n",
    "## **Project 7: Technical Writing Assistant**\n",
    "\n",
    "**Problem**: Writing whitepapers, app notes, user manuals is time-consuming (40-80 hours/document).\n",
    "\n",
    "**Solution**: RLHF assistant that drafts technical content from outlines and notes.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Structure Generation**: Turn outline \u2192 full document\n",
    "2. **Technical Accuracy**: Validate formulas, citations\n",
    "3. **Tone Control**: Professional, clear, appropriate for audience\n",
    "4. **Iteration**: Refine based on feedback (\"Make this section simpler\")\n",
    "\n",
    "### RLHF Training\n",
    "\n",
    "**Reward Model Priorities**:\n",
    "- **Coherence**: Logical flow, clear transitions\n",
    "- **Accuracy**: Technical claims are correct\n",
    "- **Audience Fit**: Match complexity to target (engineer vs manager)\n",
    "- **Citation**: Proper references for claims\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Input: \"Outline: Voltage droop mitigation\n",
    "        Section 1: Intro, Section 2: Causes, Section 3: Solutions\"\n",
    "\n",
    "Output: \"# Voltage Droop Mitigation in High-Performance SoCs\n",
    "         \n",
    "         ## 1. Introduction\n",
    "         Voltage droop is the transient decrease in supply voltage during...\n",
    "         [750-word professional introduction]\n",
    "         \n",
    "         ## 2. Root Causes\n",
    "         Three primary factors contribute to voltage droop:\n",
    "         1. Power delivery network impedance (R + j\u03c9L) [cite: Smith 2019]\n",
    "         2. Decoupling capacitor placement and ESR\n",
    "         3. Current slew rate (dI/dt) during workload transitions\n",
    "         [Detailed explanation with equations]\n",
    "         \n",
    "         ## 3. Solutions\n",
    "         ...\"\n",
    "```\n",
    "\n",
    "**Success Metrics**:\n",
    "- Draft quality: 4.0/5.0 (needs minor edits, not rewrites)\n",
    "- Time savings: 70% (50 hours \u2192 15 hours)\n",
    "- Accuracy: >95% technical correctness\n",
    "\n",
    "**Business Value**:\n",
    "- **Time savings**: 50 hrs/doc \u00d7 12 docs/yr \u00d7 $100/hr = **$60K/year**\n",
    "- **Faster publication**: 2 months \u2192 3 weeks \u2192 Market advantage \u2192 **$500K/year**\n",
    "\n",
    "**Total value**: **$500K-$800K/year**\n",
    "\n",
    "---\n",
    "\n",
    "## **Project 8: Knowledge Base Query System**\n",
    "\n",
    "**Problem**: Companies have massive knowledge bases (wikis, docs, tickets) but finding information is hard.\n",
    "\n",
    "**Solution**: RLHF-powered conversational search with natural language queries.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "User: \"What's the debug procedure for timing violations on 5nm process?\"\n",
    "    \u2193\n",
    "Retrieval: Vector search finds top-10 relevant docs\n",
    "    \u2193\n",
    "RLHF Model: Synthesize answer from retrieved docs\n",
    "    \u2193\n",
    "Output: \"For 5nm timing violations:\n",
    "         1. Verify clock tree skew <20ps [Doc: Debug_Guide_v3.2]\n",
    "         2. Check setup/hold margins [Doc: Timing_Spec_5nm.pdf]\n",
    "         3. Run STA with extracted RC [Ticket #45123 resolution]\n",
    "         \n",
    "         See attached docs for detailed procedures.\"\n",
    "```\n",
    "\n",
    "### RLHF Training\n",
    "\n",
    "**Reward Model Priorities**:\n",
    "- **Relevance**: Answer matches query intent\n",
    "- **Citation**: Link to source documents\n",
    "- **Completeness**: Cover all aspects of question\n",
    "- **Conciseness**: Summary + links (not full doc copy)\n",
    "\n",
    "**Success Metrics**:\n",
    "- Query success rate: 85% (user finds answer)\n",
    "- Time per query: 30 sec (vs 15 min manual search)\n",
    "- Citation accuracy: 95% (links are relevant)\n",
    "\n",
    "**Business Value**:\n",
    "- **Time savings**: 14.5 min/query \u00d7 500 queries/day \u00d7 $75/hr \u00d7 250 days = **$2.27M/year**\n",
    "- **Knowledge retention**: 30% better onboarding \u2192 **$1M/year**\n",
    "\n",
    "**Total value**: **$2M-$4M/year**\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Implementation Roadmap (All Projects)\n",
    "\n",
    "### Phase 1: Foundation (Months 1-2)\n",
    "1. **Data collection**: Gather 10K+ demonstrations for SFT\n",
    "2. **Infrastructure**: Set up training pipelines (GPUs, data storage)\n",
    "3. **Baselines**: Test GPT-3.5, GPT-4, open models (LLaMA, Mistral)\n",
    "\n",
    "### Phase 2: SFT (Months 2-3)\n",
    "1. **Curate demonstrations**: High-quality (prompt, response) pairs\n",
    "2. **Train SFT models**: Fine-tune base models on demonstrations\n",
    "3. **Evaluate**: Test instruction-following on validation set\n",
    "\n",
    "### Phase 3: Reward Model (Months 3-4)\n",
    "1. **Collect comparisons**: 30K+ human preference rankings\n",
    "2. **Train reward models**: Bradley-Terry pairwise ranking\n",
    "3. **Ensemble**: Use 4-6 reward models for robustness\n",
    "\n",
    "### Phase 4: PPO (Months 4-5)\n",
    "1. **PPO training**: 50K-100K iterations, monitor reward vs KL\n",
    "2. **Hyperparameter tuning**: Clip \u03b5, KL penalty \u03b2, learning rates\n",
    "3. **Early stopping**: Stop when reward plateaus and KL <5.0\n",
    "\n",
    "### Phase 5: Safety (Months 5-6)\n",
    "1. **Red-teaming**: Test with 1K+ adversarial prompts\n",
    "2. **Safety filters**: Implement input/output guardrails\n",
    "3. **Human review**: Sample 5% of outputs for quality check\n",
    "\n",
    "### Phase 6: Deployment (Months 6+)\n",
    "1. **Pilot**: Deploy to 10-20 beta users\n",
    "2. **Monitoring**: Track safety, accuracy, satisfaction metrics\n",
    "3. **Iteration**: Retrain monthly with new failure cases\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcca Expected ROI Summary\n",
    "\n",
    "| **Project** | **Implementation Cost** | **Annual Value** | **ROI** | **Payback** |\n",
    "|-------------|------------------------|------------------|---------|-------------|\n",
    "| 1. Test Docs | $300K | $10M-$15M | 33-50x | 2-3 weeks |\n",
    "| 2. Debug Chat | $250K | $5M-$8M | 20-32x | 4-5 weeks |\n",
    "| 3. Report Gen | $200K | $2M-$3M | 10-15x | 8-12 weeks |\n",
    "| 4. Safe Q&A | $400K | $4M + $50M risk | 10x+ | 10-12 weeks |\n",
    "| 5. Code Review | $300K | $3M-$5M | 10-17x | 7-10 weeks |\n",
    "| 6. Support Bot | $350K | $2M-$3M | 6-9x | 16-20 weeks |\n",
    "| 7. Tech Writing | $200K | $500K-$800K | 2.5-4x | 30-40 weeks |\n",
    "| 8. KB Query | $300K | $2M-$4M | 7-13x | 9-15 weeks |\n",
    "\n",
    "**Total Portfolio**: $2.3M investment \u2192 **$28M-$48M/year** \u2192 **12-21x ROI**\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd11 Key Takeaways\n",
    "\n",
    "1. **RLHF is production-ready**: Powers ChatGPT, Claude, Gemini, Copilot\n",
    "2. **Massive business value**: 10-50x ROI in 6-12 months\n",
    "3. **Safety is critical**: Red-teaming and guardrails prevent costly failures\n",
    "4. **Continuous improvement**: Monthly retraining with new data\n",
    "5. **Domain-specific wins**: Custom RLHF beats general models for specialized tasks\n",
    "\n",
    "**Start with highest-value, lowest-risk project** (e.g., Test Documentation Assistant) \u2192 Prove ROI \u2192 Expand portfolio.\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now understand the complete RLHF pipeline from theory to production deployment. These techniques power the most advanced AI systems today. Go build something amazing! \ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}