{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94dfbc7",
   "metadata": {},
   "source": [
    "# 069: Federated Learning\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Introduction\n",
    "\n",
    "**Federated Learning** is a distributed machine learning paradigm where **training occurs on decentralized edge devices** (smartphones, hospitals, factories) **without sharing raw data**. Instead of centralizing data in a single server, the model travels to the data, learns locally, and only shares model updates (gradients/weights).\n",
    "\n",
    "This revolutionary approach solves critical challenges in privacy, regulation, and data centralization that plague traditional machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why Federated Learning Matters\n",
    "\n",
    "### The Data Centralization Problem\n",
    "\n",
    "**Traditional ML Pipeline (Centralized):**\n",
    "```\n",
    "Medical Records (Hospital A) ‚îÄ‚îÄ‚îê\n",
    "Medical Records (Hospital B) ‚îÄ‚îÄ‚îº‚îÄ‚îÄ> Central Server ‚îÄ‚îÄ> Train Model ‚îÄ‚îÄ> Deploy\n",
    "Medical Records (Hospital C) ‚îÄ‚îÄ‚îò\n",
    "                                ‚ùå Privacy violation\n",
    "                                ‚ùå GDPR/HIPAA non-compliant\n",
    "                                ‚ùå Data transfer costs\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "1. **Privacy violation**: Raw patient data leaves hospitals\n",
    "2. **Regulatory compliance**: GDPR (‚Ç¨20M fine), HIPAA (criminal charges)\n",
    "3. **Data transfer costs**: Petabytes to cloud ($0.09/GB √ó 1PB = $90K)\n",
    "4. **Latency**: Centralized training delayed by data collection\n",
    "5. **Single point of failure**: Server breach exposes all data\n",
    "\n",
    "**Real-World Impact:**\n",
    "- **Healthcare**: Cannot aggregate patient data across hospitals (HIPAA)\n",
    "- **Finance**: Cannot share transaction data across banks (PCI-DSS)\n",
    "- **Manufacturing**: Cannot share production data with competitors (trade secrets)\n",
    "- **Mobile AI**: Cannot send user keyboard predictions to cloud (privacy)\n",
    "\n",
    "### Federated Learning Solution\n",
    "\n",
    "**Federated Pipeline:**\n",
    "```\n",
    "Hospital A: Train on local data ‚îÄ‚îÄ> Send gradients ‚îÄ‚îÄ‚îê\n",
    "Hospital B: Train on local data ‚îÄ‚îÄ> Send gradients ‚îÄ‚îÄ‚îº‚îÄ‚îÄ> Central Server\n",
    "Hospital C: Train on local data ‚îÄ‚îÄ> Send gradients ‚îÄ‚îÄ‚îò         ‚Üì\n",
    "                                                        Aggregate (FedAvg)\n",
    "       ‚Üì                                                        ‚Üì\n",
    "Receive updated model ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ **Privacy preserved**: Raw data never leaves devices\n",
    "- ‚úÖ **Regulatory compliant**: GDPR/HIPAA approved (data stays local)\n",
    "- ‚úÖ **No transfer costs**: Only model updates (KB vs GB)\n",
    "- ‚úÖ **Personalization**: Models adapt to local data distributions\n",
    "- ‚úÖ **Scalability**: Billions of devices (Google Gboard: 1B+ devices)\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Real-World Examples\n",
    "\n",
    "### Example 1: Google Gboard (Keyboard Predictions)\n",
    "\n",
    "**Problem**: Improve next-word prediction without violating privacy\n",
    "\n",
    "**Traditional Approach (Centralized):**\n",
    "- Collect all typed text from 1B users\n",
    "- Store in Google servers\n",
    "- Train centralized model\n",
    "- **Issues**: Privacy nightmare, regulatory violations, user backlash ‚ùå\n",
    "\n",
    "**Federated Approach:**\n",
    "- Each smartphone trains locally on user's typing history\n",
    "- Send only model updates (gradients) to server\n",
    "- Server aggregates updates from millions of devices\n",
    "- Send improved model back to devices\n",
    "- **Result**: Better predictions, zero privacy violation ‚úÖ\n",
    "\n",
    "**Impact:**\n",
    "- **Users**: 1B+ Android users (2019)\n",
    "- **Privacy**: No raw text sent to servers\n",
    "- **Accuracy**: 13% improvement in next-word prediction\n",
    "- **Bandwidth**: 100KB model update vs 100MB text data (1000√ó reduction)\n",
    "\n",
    "### Example 2: Hospitals Predicting Disease Risk\n",
    "\n",
    "**Problem**: Train disease prediction model across 100 hospitals without sharing patient data\n",
    "\n",
    "**Traditional Approach:**\n",
    "- Aggregate patient records from 100 hospitals\n",
    "- Train centralized model\n",
    "- **Issues**: HIPAA violation ($50K-$1.5M per violation), patient consent required ‚ùå\n",
    "\n",
    "**Federated Approach:**\n",
    "- Each hospital trains locally on its 10,000 patients\n",
    "- Send only gradients (no patient data)\n",
    "- Server aggregates gradients ‚Üí Global model\n",
    "- **Result**: 1M patient dataset without sharing data ‚úÖ\n",
    "\n",
    "**Impact:**\n",
    "- **Dataset size**: 100 hospitals √ó 10K patients = 1M patients (vs 10K centralized)\n",
    "- **Accuracy**: 89% (federated) vs 82% (single hospital)\n",
    "- **Compliance**: HIPAA approved (data stays at hospitals)\n",
    "- **Business value**: $10M-$30M/year (see below)\n",
    "\n",
    "### Example 3: Predictive Maintenance Across Factories\n",
    "\n",
    "**Problem**: Train predictive maintenance model across 50 factories without sharing proprietary sensor data\n",
    "\n",
    "**Traditional Approach:**\n",
    "- Each factory shares sensor data with vendor\n",
    "- **Issues**: Trade secrets exposed, competitor intelligence, trust issues ‚ùå\n",
    "\n",
    "**Federated Approach:**\n",
    "- Each factory trains locally on its equipment data\n",
    "- Send only gradients to vendor's server\n",
    "- Vendor aggregates ‚Üí Global model benefits all factories\n",
    "- **Result**: Better model, no data sharing ‚úÖ\n",
    "\n",
    "**Impact:**\n",
    "- **Factories**: 50 factories √ó 500 machines = 25,000 machines (vs 500 per factory)\n",
    "- **Downtime reduction**: 40% (vs 20% single-factory model)\n",
    "- **Business value**: $30M-$80M/year (see below)\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Business Value: $50M-$150M/year\n",
    "\n",
    "Federated learning unlocks massive business value across three key areas:\n",
    "\n",
    "### Use Case 1: Healthcare Disease Prediction ($10M-$30M/year)\n",
    "\n",
    "**Scenario**: Train disease prediction model across 100 hospitals (1M patients total)\n",
    "\n",
    "**Current Problem:**\n",
    "- **Single hospital**: 10K patients ‚Üí 82% accuracy (insufficient data)\n",
    "- **Centralized**: Cannot aggregate data (HIPAA violation, $50K-$1.5M per violation)\n",
    "- **Status quo**: Each hospital uses inferior local model ‚ùå\n",
    "\n",
    "**Federated Solution:**\n",
    "- **100 hospitals**: 1M patients (federated) ‚Üí 89% accuracy (7% improvement)\n",
    "- **Privacy**: Patient data never leaves hospitals ‚úÖ\n",
    "- **Compliance**: HIPAA approved ‚úÖ\n",
    "\n",
    "**Business Value:**\n",
    "- **Accuracy improvement**: 82% ‚Üí 89% (7% absolute)\n",
    "- **Lives saved**: 7% better detection √ó 100K high-risk patients = 7,000 lives/year\n",
    "- **Cost avoidance**: $10K per late-stage treatment √ó 7,000 = $70M/year\n",
    "- **Margin**: Hospital network captures 15-30% = **$10M-$21M/year**\n",
    "- **Regulatory value**: Avoid $50K-$1.5M HIPAA fines per violation\n",
    "\n",
    "**Conservative estimate**: **$10M-$30M/year** (across hospital networks)\n",
    "\n",
    "---\n",
    "\n",
    "### Use Case 2: Federated Keyboard (Mobile AI) ($20M-$50M/year)\n",
    "\n",
    "**Scenario**: Improve keyboard predictions for 500M users without violating privacy\n",
    "\n",
    "**Current Problem:**\n",
    "- **Centralized**: Send all typed text to cloud (privacy violation, GDPR fines up to ‚Ç¨20M)\n",
    "- **Local-only**: Limited by device data (poor accuracy, slow improvement)\n",
    "- **Status quo**: User dissatisfaction, regulatory risk ‚ùå\n",
    "\n",
    "**Federated Solution:**\n",
    "- **500M devices**: Train locally on user typing patterns\n",
    "- **Aggregate**: Server combines updates ‚Üí Global model\n",
    "- **Privacy**: No raw text sent to servers ‚úÖ\n",
    "- **Result**: 13% accuracy improvement, GDPR compliant ‚úÖ\n",
    "\n",
    "**Business Value:**\n",
    "- **User satisfaction**: 13% better predictions ‚Üí NPS +8 points ‚Üí Retention +2%\n",
    "- **Retention value**: 500M users √ó 2% retention √ó $5 ARPU = $50M/year\n",
    "- **Privacy differentiation**: Marketing advantage vs competitors (violate privacy)\n",
    "- **Regulatory avoidance**: GDPR fines up to ‚Ç¨20M ($22M) avoided\n",
    "- **Bandwidth savings**: 100KB updates vs 100MB text = $5M/year (cloud transfer costs)\n",
    "\n",
    "**Conservative estimate**: **$20M-$50M/year** (mobile platform with 500M+ users)\n",
    "\n",
    "---\n",
    "\n",
    "### Use Case 3: Predictive Maintenance Across Factories ($30M-$80M/year)\n",
    "\n",
    "**Scenario**: Train predictive maintenance model across 50 factories (25,000 machines) without sharing proprietary data\n",
    "\n",
    "**Current Problem:**\n",
    "- **Single factory**: 500 machines ‚Üí 20% downtime reduction (limited data)\n",
    "- **Centralized**: Cannot share sensor data (trade secrets, competitor intelligence)\n",
    "- **Status quo**: Each factory uses inferior local model ‚ùå\n",
    "\n",
    "**Federated Solution:**\n",
    "- **50 factories**: 25,000 machines (federated) ‚Üí 40% downtime reduction (2√ó better)\n",
    "- **Privacy**: Proprietary sensor data never leaves factories ‚úÖ\n",
    "- **Trust**: Equipment vendor can aggregate without seeing raw data ‚úÖ\n",
    "\n",
    "**Business Value:**\n",
    "- **Downtime improvement**: 20% ‚Üí 40% reduction (20% absolute)\n",
    "- **Cost per hour downtime**: $50K-$100K per machine-hour (semiconductor fabs)\n",
    "- **Annual downtime**: 25,000 machines √ó 100 hours/year = 2.5M hours\n",
    "- **Additional savings**: 20% √ó 2.5M hours √ó $50K = $25M/year (conservative)\n",
    "- **Scaling**: If 20% improvement ‚Üí $25M, then 40% ‚Üí $50M base case\n",
    "- **Equipment vendor revenue**: 10-20% of savings = $5M-$10M/year\n",
    "- **Total across industry**: 50 factories √ó $1M/year = $50M/year\n",
    "\n",
    "**Breakdown per factory:**\n",
    "- Current model: 20% downtime reduction = $1M/year savings\n",
    "- Federated model: 40% downtime reduction = $2M/year savings\n",
    "- **Incremental value per factory**: $1M/year\n",
    "- **Total (50 factories)**: $50M/year\n",
    "- **Equipment vendor share (20%)**: $10M/year\n",
    "- **Conservative range**: **$30M-$80M/year** (depends on industry adoption)\n",
    "\n",
    "---\n",
    "\n",
    "### Total Business Value: $50M-$150M/year\n",
    "\n",
    "| Use Case | Annual Value | Key Metric |\n",
    "|----------|--------------|------------|\n",
    "| Healthcare (100 hospitals) | $10M-$30M | 7% accuracy improvement, 7K lives saved |\n",
    "| Mobile AI (500M users) | $20M-$50M | 13% prediction improvement, 2% retention |\n",
    "| Predictive Maintenance (50 factories) | $30M-$80M | 40% downtime reduction, $1M/factory |\n",
    "| **Total** | **$60M-$160M** | Privacy-preserving collaboration |\n",
    "\n",
    "**Conservative midpoint**: **$90M/year** (across all use cases)\n",
    "\n",
    "---\n",
    "\n",
    "## üîç How Federated Learning Works\n",
    "\n",
    "### High-Level Algorithm (Federated Averaging - FedAvg)\n",
    "\n",
    "**Invented by**: Google (McMahan et al., 2017)\n",
    "\n",
    "**Process:**\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Central Server<br/>Initialize Global Model Œ∏‚ÇÄ] --> B1[Device 1<br/>Download Œ∏‚ÇÄ]\n",
    "    A --> B2[Device 2<br/>Download Œ∏‚ÇÄ]\n",
    "    A --> B3[Device N<br/>Download Œ∏‚ÇÄ]\n",
    "    \n",
    "    B1 --> C1[Train Locally<br/>on Local Data D‚ÇÅ]\n",
    "    B2 --> C2[Train Locally<br/>on Local Data D‚ÇÇ]\n",
    "    B3 --> C3[Train Locally<br/>on Local Data D‚Çô]\n",
    "    \n",
    "    C1 --> D1[Compute Update<br/>ŒîŒ∏‚ÇÅ = Œ∏‚ÇÅ - Œ∏‚ÇÄ]\n",
    "    C2 --> D2[Compute Update<br/>ŒîŒ∏‚ÇÇ = Œ∏‚ÇÇ - Œ∏‚ÇÄ]\n",
    "    C3 --> D3[Compute Update<br/>ŒîŒ∏‚Çô = Œ∏‚Çô - Œ∏‚ÇÄ]\n",
    "    \n",
    "    D1 --> E[Server Aggregates<br/>Œ∏‚ÇÅ = Œ∏‚ÇÄ + avg(ŒîŒ∏·µ¢)]\n",
    "    D2 --> E\n",
    "    D3 --> E\n",
    "    \n",
    "    E --> F{Converged?}\n",
    "    F -->|No| B1\n",
    "    F -->|Yes| G[Final Global Model Œ∏*]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style E fill:#ffe1e1\n",
    "    style G fill:#e1ffe1\n",
    "```\n",
    "\n",
    "**Step-by-Step:**\n",
    "\n",
    "1. **Initialize**: Server creates global model Œ∏‚ÇÄ (random weights)\n",
    "\n",
    "2. **Distribute**: Server sends Œ∏‚ÇÄ to N selected devices (e.g., 100 hospitals)\n",
    "\n",
    "3. **Local Training**: Each device i trains on its local data D·µ¢ for E epochs:\n",
    "   - Œ∏·µ¢ = Œ∏‚ÇÄ - Œ∑‚àáL(Œ∏‚ÇÄ, D·µ¢)  [Standard SGD]\n",
    "   - Example: Hospital A trains on its 10K patients\n",
    "\n",
    "4. **Compute Update**: Each device computes model update:\n",
    "   - ŒîŒ∏·µ¢ = Œ∏·µ¢ - Œ∏‚ÇÄ  [Difference between local and global model]\n",
    "   - Send ŒîŒ∏·µ¢ to server (not raw data!)\n",
    "\n",
    "5. **Aggregate**: Server averages updates from all devices:\n",
    "   - Œ∏‚ÇÅ = Œ∏‚ÇÄ + (1/N) Œ£·µ¢ ŒîŒ∏·µ¢  [Weighted average by dataset size]\n",
    "   - Example: Average updates from 100 hospitals\n",
    "\n",
    "6. **Repeat**: Steps 2-5 for T rounds (e.g., 1000 rounds)\n",
    "\n",
    "7. **Convergence**: Stop when validation accuracy plateaus ‚Üí Œ∏*\n",
    "\n",
    "**Key Insight**: Only model updates (ŒîŒ∏) are shared, not raw data (D)!\n",
    "\n",
    "---\n",
    "\n",
    "### Example: 3 Hospitals Training Disease Prediction\n",
    "\n",
    "**Setup:**\n",
    "- Hospital A: 10K patients, 60% disease prevalence\n",
    "- Hospital B: 15K patients, 40% disease prevalence\n",
    "- Hospital C: 5K patients, 50% disease prevalence\n",
    "- Total: 30K patients (federated)\n",
    "\n",
    "**Round 1:**\n",
    "\n",
    "1. **Initialize**: Server creates Œ∏‚ÇÄ (random weights)\n",
    "\n",
    "2. **Distribute**: Each hospital downloads Œ∏‚ÇÄ\n",
    "\n",
    "3. **Local Training**:\n",
    "   - Hospital A: Train on 10K patients ‚Üí Œ∏_A = 0.85 accuracy\n",
    "   - Hospital B: Train on 15K patients ‚Üí Œ∏_B = 0.83 accuracy\n",
    "   - Hospital C: Train on 5K patients ‚Üí Œ∏_C = 0.80 accuracy\n",
    "\n",
    "4. **Compute Updates**:\n",
    "   - ŒîŒ∏_A = Œ∏_A - Œ∏‚ÇÄ\n",
    "   - ŒîŒ∏_B = Œ∏_B - Œ∏‚ÇÄ\n",
    "   - ŒîŒ∏_C = Œ∏_C - Œ∏‚ÇÄ\n",
    "\n",
    "5. **Aggregate** (weighted by dataset size):\n",
    "   - Œ∏‚ÇÅ = Œ∏‚ÇÄ + (10K √ó ŒîŒ∏_A + 15K √ó ŒîŒ∏_B + 5K √ó ŒîŒ∏_C) / 30K\n",
    "   - Larger hospitals contribute more (15K vs 5K)\n",
    "\n",
    "6. **Validation**: Test Œ∏‚ÇÅ on held-out data ‚Üí 0.87 accuracy (better than any single hospital!)\n",
    "\n",
    "**Round 2-1000**: Repeat, model improves to 0.89 accuracy ‚úÖ\n",
    "\n",
    "**Result**: All hospitals benefit from 30K patients without sharing data!\n",
    "\n",
    "---\n",
    "\n",
    "## üîê Privacy Guarantees\n",
    "\n",
    "### Threat Model\n",
    "\n",
    "**What Federated Learning Protects Against:**\n",
    "- ‚úÖ **Honest-but-curious server**: Server follows protocol but tries to infer data from updates\n",
    "- ‚úÖ **Data leakage**: Prevent server from reconstructing training data\n",
    "- ‚úÖ **Membership inference**: Prevent adversary from determining if specific sample was in training\n",
    "\n",
    "**What Federated Learning Does NOT Protect Against:**\n",
    "- ‚ùå **Malicious devices**: Devices sending poisoned updates (see defenses below)\n",
    "- ‚ùå **Model inversion attacks**: Advanced attacks can partially reconstruct data from gradients\n",
    "- ‚ùå **Byzantine attacks**: Multiple colluding malicious devices\n",
    "\n",
    "**Solution**: Combine Federated Learning with **Differential Privacy** (see below)\n",
    "\n",
    "---\n",
    "\n",
    "### Differential Privacy (DP)\n",
    "\n",
    "**Definition**: Add calibrated noise to model updates to prevent inferring individual data points\n",
    "\n",
    "**Mathematical Guarantee:**\n",
    "- (Œµ, Œ¥)-Differential Privacy: Privacy budget Œµ (smaller = more private)\n",
    "- Œµ = 1: Strong privacy (10√ó harder to infer membership)\n",
    "- Œµ = 10: Weak privacy (acceptable for many applications)\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "# Add Gaussian noise to gradients\n",
    "noise_scale = C / (N √ó Œµ)  # C = clipping threshold, N = #devices, Œµ = privacy budget\n",
    "gradient_noisy = gradient + Normal(0, noise_scale¬≤)\n",
    "```\n",
    "\n",
    "**Trade-off**: Privacy ‚Üë ‚Üí Accuracy ‚Üì (noise reduces signal)\n",
    "\n",
    "**Example**: Google Gboard uses Œµ = 2-8 (strong privacy with acceptable accuracy loss)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ When to Use Federated Learning\n",
    "\n",
    "### ‚úÖ Ideal Use Cases\n",
    "\n",
    "1. **Regulatory Compliance Required**\n",
    "   - Healthcare (HIPAA)\n",
    "   - Finance (PCI-DSS)\n",
    "   - EU users (GDPR)\n",
    "\n",
    "2. **Data Cannot Be Centralized**\n",
    "   - Proprietary data (trade secrets)\n",
    "   - Competitor collaboration (e.g., banks detecting fraud)\n",
    "   - Cross-border data transfer restricted\n",
    "\n",
    "3. **Large-Scale Edge Deployment**\n",
    "   - Billions of mobile devices (Google Gboard)\n",
    "   - IoT sensors (predictive maintenance)\n",
    "   - Autonomous vehicles (road condition detection)\n",
    "\n",
    "4. **Personalization Needed**\n",
    "   - Keyboard predictions (adapt to user's language)\n",
    "   - Medical treatment (adapt to hospital's patient demographics)\n",
    "\n",
    "### ‚ùå Not Recommended When\n",
    "\n",
    "1. **Data Can Be Centralized** (no privacy/regulatory issues)\n",
    "   - Internal company data (employees consent to data collection)\n",
    "   - Public datasets (ImageNet, Wikipedia)\n",
    "\n",
    "2. **Small Number of Devices** (<10)\n",
    "   - Centralized training faster and simpler\n",
    "   - Federated overhead not justified\n",
    "\n",
    "3. **Homogeneous Data** (all devices have similar distributions)\n",
    "   - No benefit from federated aggregation\n",
    "   - Single-device training sufficient\n",
    "\n",
    "4. **Real-Time Requirements** (<100ms latency)\n",
    "   - Federated rounds take minutes to hours\n",
    "   - Not suitable for real-time applications\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Federated Learning vs Centralized Learning\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Aspect | Centralized Learning | Federated Learning |\n",
    "|--------|----------------------|---------------------|\n",
    "| **Data Location** | Central server | Decentralized (devices) |\n",
    "| **Privacy** | ‚ùå Raw data sent to server | ‚úÖ Data stays on devices |\n",
    "| **Regulatory** | ‚ùå GDPR/HIPAA violations | ‚úÖ Compliant (data local) |\n",
    "| **Communication** | High (GB per device) | Low (KB model updates) |\n",
    "| **Training Speed** | Fast (single machine) | Slow (multiple rounds) |\n",
    "| **Scalability** | Limited (server capacity) | Unlimited (billions of devices) |\n",
    "| **Data Heterogeneity** | Assumes IID data | Handles non-IID naturally |\n",
    "| **Personalization** | Global model only | Local + global models |\n",
    "| **Convergence** | Guaranteed (convex) | Slower (non-IID, stragglers) |\n",
    "| **Accuracy** | Baseline | Similar (with enough rounds) |\n",
    "\n",
    "**Key Takeaway**: Use federated learning when privacy/regulation trumps convenience.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Historical Timeline\n",
    "\n",
    "### Evolution of Federated Learning\n",
    "\n",
    "```mermaid\n",
    "timeline\n",
    "    title Federated Learning Evolution\n",
    "    2016 : Google introduces Federated Learning (McMahan et al.)\n",
    "         : First application - Google Gboard keyboard\n",
    "    2017 : FedAvg algorithm published (averaging gradients)\n",
    "         : Apple adopts for QuickType keyboard\n",
    "    2018 : Differential Privacy added (Œµ-DP)\n",
    "         : Healthcare applications emerge (disease prediction)\n",
    "    2019 : Google Gboard reaches 1B+ users\n",
    "         : Horizontal FL (IID) + Vertical FL (different features)\n",
    "    2020 : NVIDIA Clara for federated medical imaging\n",
    "         : Cross-silo FL (hospitals, banks)\n",
    "    2021 : EU GDPR enforcement increases adoption\n",
    "         : Federated learning in production (10+ major companies)\n",
    "    2022 : TensorFlow Federated, PySyft, Flower frameworks mature\n",
    "         : Automotive FL (Tesla, Waymo traffic patterns)\n",
    "    2023 : LLM fine-tuning with federated learning (GPT-4)\n",
    "         : Blockchain + FL (decentralized aggregation)\n",
    "    2024 : Federated learning for semiconductor test (post-silicon)\n",
    "         : Multi-party FL (100+ participants)\n",
    "    2025 : Mainstream adoption (healthcare, finance, manufacturing)\n",
    "         : Business value $50M-$150M/year per enterprise\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Key Concepts\n",
    "\n",
    "### 1. Federated Averaging (FedAvg)\n",
    "\n",
    "**Core Algorithm:**\n",
    "```\n",
    "For t = 1 to T (rounds):\n",
    "    Server selects K devices from N total\n",
    "    Server sends global model Œ∏‚Çú to K devices\n",
    "    \n",
    "    For each device k:\n",
    "        Œ∏‚Çñ ‚Üê LocalTrain(Œ∏‚Çú, D‚Çñ, E epochs)\n",
    "        Send ŒîŒ∏‚Çñ = Œ∏‚Çñ - Œ∏‚Çú to server\n",
    "    \n",
    "    Server aggregates:\n",
    "    Œ∏‚Çú‚Çä‚ÇÅ ‚Üê Œ∏‚Çú + Œ£‚Çñ (n‚Çñ/n) ŒîŒ∏‚Çñ  [weighted by dataset size n‚Çñ]\n",
    "```\n",
    "\n",
    "**Why It Works:**\n",
    "- **Intuition**: Average of local models approximates centralized model\n",
    "- **Theory**: Converges to same optimum as centralized (if data is IID)\n",
    "- **Practice**: 95-99% of centralized accuracy (even with non-IID data)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Non-IID Data Challenge\n",
    "\n",
    "**Problem**: Device data is not identically distributed (Non-IID)\n",
    "\n",
    "**Example (Medical):**\n",
    "- Hospital A: 90% elderly patients (diabetes common)\n",
    "- Hospital B: 70% young patients (diabetes rare)\n",
    "- Hospital C: 50% urban (different risk factors)\n",
    "\n",
    "**Impact**: Local models diverge, aggregation is suboptimal\n",
    "\n",
    "**Solutions:**\n",
    "1. **FedProx**: Add regularization to keep local models close to global\n",
    "2. **Personalization**: Mix global + local model (80% global, 20% local)\n",
    "3. **Clustering**: Group similar devices, aggregate separately\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Communication Efficiency\n",
    "\n",
    "**Problem**: Sending model updates every round is expensive (bandwidth, battery)\n",
    "\n",
    "**Solutions:**\n",
    "1. **Gradient Compression**: Quantize gradients (32-bit ‚Üí 8-bit) = 4√ó reduction\n",
    "2. **Sparsification**: Send only top-k gradients (1% largest) = 100√ó reduction\n",
    "3. **Local Epochs**: Train E=5 epochs locally before sending update (5√ó fewer rounds)\n",
    "\n",
    "**Example (Google Gboard):**\n",
    "- Model size: 10MB (too large for frequent updates)\n",
    "- Compressed update: 100KB (100√ó smaller)\n",
    "- Frequency: Once per day (not every round)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Device Selection\n",
    "\n",
    "**Problem**: Not all devices participate every round (battery, connectivity)\n",
    "\n",
    "**Strategies:**\n",
    "1. **Random Selection**: Choose K devices uniformly (simple, unbiased)\n",
    "2. **Stratified Sampling**: Ensure diverse data coverage (e.g., 20 hospitals per region)\n",
    "3. **Active Learning**: Select devices with highest gradient norms (most informative)\n",
    "\n",
    "**Example**:\n",
    "- Total devices: 1M smartphones\n",
    "- Selected per round: 100 devices (0.01%)\n",
    "- Rounds: 1000 ‚Üí 100K devices trained (10% of total)\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Learning Roadmap\n",
    "\n",
    "### Prerequisites (Already Covered)\n",
    "- ‚úÖ **065**: Deep Reinforcement Learning (policy gradients, distributed training)\n",
    "- ‚úÖ **066**: Attention Mechanisms (transformers, multi-head attention)\n",
    "- ‚úÖ **067**: Neural Architecture Search (AutoML, DARTS)\n",
    "- ‚úÖ **068**: Model Compression (pruning, quantization, distillation)\n",
    "\n",
    "### This Notebook (069)\n",
    "- üìò **Federated Learning Fundamentals**: FedAvg, privacy, non-IID data\n",
    "- üßÆ **Mathematical Foundations**: Convergence analysis, differential privacy\n",
    "- üíª **Implementation**: PyTorch FL, PySyft, TensorFlow Federated\n",
    "- üöÄ **Production Projects**: Healthcare, mobile AI, manufacturing\n",
    "\n",
    "### Next Steps\n",
    "- **070**: Edge AI & TinyML (on-device inference, microcontrollers)\n",
    "- **071**: Transformers & BERT (self-attention, pre-training)\n",
    "- **072**: GPT & Large Language Models (autoregressive, few-shot learning)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. ‚úÖ Understand federated learning principles and privacy guarantees\n",
    "2. ‚úÖ Implement FedAvg algorithm from scratch (PyTorch)\n",
    "3. ‚úÖ Add differential privacy for formal privacy guarantees\n",
    "4. ‚úÖ Handle non-IID data challenges (FedProx, personalization)\n",
    "5. ‚úÖ Deploy federated learning for production use cases (healthcare, mobile, manufacturing)\n",
    "6. ‚úÖ Quantify business value ($50M-$150M/year opportunities)\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Success Criteria\n",
    "\n",
    "After completing this notebook, you should be able to:\n",
    "\n",
    "- [ ] Explain why federated learning is needed (privacy, regulation, decentralization)\n",
    "- [ ] Implement FedAvg algorithm for 3+ devices\n",
    "- [ ] Add differential privacy with (Œµ, Œ¥)-DP guarantees\n",
    "- [ ] Handle non-IID data (heterogeneous device distributions)\n",
    "- [ ] Deploy federated learning pipeline (device selection, aggregation, convergence)\n",
    "- [ ] Quantify ROI for federated learning projects ($10M-$80M/year)\n",
    "- [ ] Compare federated vs centralized learning (trade-offs)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Notebook Structure\n",
    "\n",
    "This notebook is organized into **4 comprehensive sections**:\n",
    "\n",
    "### **Cell 1: Introduction** (Current)\n",
    "- Why federated learning matters\n",
    "- Real-world examples (Google Gboard, healthcare, manufacturing)\n",
    "- Business value ($50M-$150M/year)\n",
    "- High-level algorithm walkthrough\n",
    "\n",
    "### **Cell 2: Mathematical Foundations**\n",
    "- Federated Averaging (FedAvg) theory\n",
    "- Convergence analysis (IID vs Non-IID)\n",
    "- Differential Privacy (Œµ-DP)\n",
    "- Communication efficiency (gradient compression)\n",
    "- FedProx algorithm (handling non-IID data)\n",
    "\n",
    "### **Cell 3: Implementation** (Python)\n",
    "- FedAvg from scratch (PyTorch)\n",
    "- Differential privacy implementation\n",
    "- Non-IID data simulation\n",
    "- Complete federated training loop\n",
    "- Comparison with centralized baseline\n",
    "\n",
    "### **Cell 4: Production Projects**\n",
    "- Project 1: Federated Disease Prediction (100 hospitals, $10M-$30M/year)\n",
    "- Project 2: Mobile Keyboard Prediction (500M users, $20M-$50M/year)\n",
    "- Project 3: Predictive Maintenance (50 factories, $30M-$80M/year)\n",
    "- Project 4-8: Additional real-world applications\n",
    "- Deployment strategies (TensorFlow Federated, PySyft, Flower)\n",
    "- Key takeaways and learning path\n",
    "\n",
    "---\n",
    "\n",
    "**Let's revolutionize machine learning with privacy-preserving collaboration!** üöÄüîê\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Progression:**\n",
    "- **Previous**: 068 Model Compression & Quantization (Prune, Distill, Quantize)\n",
    "- **Current**: 069 Federated Learning (Privacy-Preserving Distributed ML)\n",
    "- **Next**: 070 Edge AI & TinyML (On-Device Inference, Microcontrollers)\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Ready to dive into the mathematics and implementation!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca09931",
   "metadata": {},
   "source": [
    "# üìê Mathematical Foundations: Federated Learning Theory\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Federated Averaging (FedAvg) Algorithm\n",
    "\n",
    "### Problem Formulation\n",
    "\n",
    "**Goal**: Minimize loss across distributed devices without centralizing data\n",
    "\n",
    "**Mathematical Setup:**\n",
    "- **Devices**: K devices (hospitals, smartphones, factories)\n",
    "- **Local datasets**: $D_k$ for device $k$ with $n_k$ samples\n",
    "- **Total data**: $n = \\sum_{k=1}^{K} n_k$ samples\n",
    "- **Local objective**: $F_k(\\theta) = \\frac{1}{n_k} \\sum_{i \\in D_k} \\ell(\\theta; x_i, y_i)$\n",
    "- **Global objective**: $F(\\theta) = \\sum_{k=1}^{K} \\frac{n_k}{n} F_k(\\theta)$\n",
    "\n",
    "**Centralized Optimization (Baseline):**\n",
    "\n",
    "$$\\theta^* = \\arg\\min_{\\theta} F(\\theta) = \\arg\\min_{\\theta} \\frac{1}{n} \\sum_{k=1}^{K} \\sum_{i \\in D_k} \\ell(\\theta; x_i, y_i)$$\n",
    "\n",
    "**Challenge**: Cannot access all data $D_k$ simultaneously (privacy, regulation)\n",
    "\n",
    "---\n",
    "\n",
    "### FedAvg Algorithm\n",
    "\n",
    "**Introduced by**: McMahan et al. (Google, 2017)\n",
    "\n",
    "**Key Idea**: Aggregate local model updates (not gradients) after multiple local epochs\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "```\n",
    "Input: \n",
    "  - K devices with local datasets D_k\n",
    "  - Global model Œ∏‚ÇÄ (initialized randomly)\n",
    "  - T rounds, E local epochs, learning rate Œ∑\n",
    "\n",
    "For round t = 1 to T:\n",
    "    \n",
    "    # Step 1: Server selects devices\n",
    "    S_t ‚Üê Random sample of m devices from K total\n",
    "    \n",
    "    # Step 2: Server broadcasts global model\n",
    "    Send Œ∏_t to all devices in S_t\n",
    "    \n",
    "    # Step 3: Each device trains locally\n",
    "    For each device k ‚àà S_t (in parallel):\n",
    "        Œ∏_k^0 ‚Üê Œ∏_t  # Initialize from global model\n",
    "        \n",
    "        For epoch e = 1 to E:\n",
    "            For mini-batch B from D_k:\n",
    "                Œ∏_k^{e+1} ‚Üê Œ∏_k^e - Œ∑ ‚àáF_k(Œ∏_k^e; B)\n",
    "        \n",
    "        ŒîŒ∏_k ‚Üê Œ∏_k^E - Œ∏_t  # Compute update\n",
    "        Send ŒîŒ∏_k to server\n",
    "    \n",
    "    # Step 4: Server aggregates updates\n",
    "    Œ∏_{t+1} ‚Üê Œ∏_t + Œ£_{k ‚àà S_t} (n_k / Œ£_{j ‚àà S_t} n_j) ŒîŒ∏_k\n",
    "    \n",
    "Return Œ∏_T\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **Local Training (Step 3)**: Each device trains for E epochs on local data\n",
    "   - $\\theta_k \\leftarrow \\theta_t - \\eta \\nabla F_k(\\theta_t)$ repeated E times\n",
    "   - This is standard SGD, just local\n",
    "\n",
    "2. **Model Update (Step 3)**: Difference between local and global model\n",
    "   - $\\Delta\\theta_k = \\theta_k - \\theta_t$\n",
    "   - Sent to server (not raw data!)\n",
    "\n",
    "3. **Weighted Aggregation (Step 4)**: Average weighted by dataset size\n",
    "   - $\\theta_{t+1} = \\theta_t + \\sum_{k \\in S_t} \\frac{n_k}{\\sum_{j \\in S_t} n_j} \\Delta\\theta_k$\n",
    "   - Larger datasets contribute more (fair weighting)\n",
    "\n",
    "---\n",
    "\n",
    "### Example: 3 Hospitals Training Disease Model\n",
    "\n",
    "**Setup:**\n",
    "- Hospital A: $n_A = 10,000$ patients\n",
    "- Hospital B: $n_B = 15,000$ patients  \n",
    "- Hospital C: $n_C = 5,000$ patients\n",
    "- Total: $n = 30,000$ patients\n",
    "\n",
    "**Round 1:**\n",
    "\n",
    "**Step 1**: Server initializes $\\theta_0$ (random weights)\n",
    "\n",
    "**Step 2**: Server sends $\\theta_0$ to all 3 hospitals\n",
    "\n",
    "**Step 3**: Each hospital trains locally (E=5 epochs)\n",
    "\n",
    "*Hospital A:*\n",
    "```\n",
    "Œ∏_A^0 = Œ∏_0\n",
    "For e = 1 to 5:\n",
    "    For each batch in D_A (10K patients):\n",
    "        Œ∏_A^{e+1} = Œ∏_A^e - Œ∑ ‚àáF_A(Œ∏_A^e)\n",
    "ŒîŒ∏_A = Œ∏_A^5 - Œ∏_0 = [0.05, -0.02, 0.08, ...]  # Example values\n",
    "```\n",
    "\n",
    "*Hospital B:*\n",
    "```\n",
    "ŒîŒ∏_B = Œ∏_B^5 - Œ∏_0 = [0.03, -0.01, 0.06, ...]\n",
    "```\n",
    "\n",
    "*Hospital C:*\n",
    "```\n",
    "ŒîŒ∏_C = Œ∏_C^5 - Œ∏_0 = [0.07, -0.03, 0.10, ...]\n",
    "```\n",
    "\n",
    "**Step 4**: Server aggregates (weighted by dataset size)\n",
    "\n",
    "$$\\theta_1 = \\theta_0 + \\frac{10K}{30K} \\Delta\\theta_A + \\frac{15K}{30K} \\Delta\\theta_B + \\frac{5K}{30K} \\Delta\\theta_C$$\n",
    "\n",
    "$$\\theta_1 = \\theta_0 + \\frac{1}{3} [0.05, -0.02, 0.08] + \\frac{1}{2} [0.03, -0.01, 0.06] + \\frac{1}{6} [0.07, -0.03, 0.10]$$\n",
    "\n",
    "$$\\theta_1 = \\theta_0 + [0.0417, -0.0150, 0.0700]$$\n",
    "\n",
    "**Interpretation:**\n",
    "- Hospital B (largest dataset) has highest weight (15K/30K = 50%)\n",
    "- Hospital C (smallest dataset) has lowest weight (5K/30K = 16.7%)\n",
    "- Fair aggregation: More data ‚Üí More influence\n",
    "\n",
    "**Round 2-1000**: Repeat, model converges to $\\theta^*$\n",
    "\n",
    "---\n",
    "\n",
    "### Why FedAvg Works: Convergence Analysis\n",
    "\n",
    "**Theorem (Simplified)**: If data is IID (identically distributed), FedAvg converges to the same optimum as centralized SGD.\n",
    "\n",
    "**Proof Sketch:**\n",
    "\n",
    "**Centralized SGD update:**\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\nabla F(\\theta_t) = \\theta_t - \\eta \\frac{1}{n} \\sum_{k=1}^{K} \\sum_{i \\in D_k} \\nabla \\ell(\\theta_t; x_i, y_i)$$\n",
    "\n",
    "**FedAvg update (with E=1 epoch):**\n",
    "$$\\theta_{t+1} = \\theta_t + \\sum_{k=1}^{K} \\frac{n_k}{n} \\Delta\\theta_k = \\theta_t + \\sum_{k=1}^{K} \\frac{n_k}{n} (- \\eta \\nabla F_k(\\theta_t))$$\n",
    "\n",
    "$$= \\theta_t - \\eta \\sum_{k=1}^{K} \\frac{n_k}{n} \\nabla F_k(\\theta_t) = \\theta_t - \\eta \\nabla F(\\theta_t)$$\n",
    "\n",
    "**Conclusion**: FedAvg (E=1) = Centralized SGD (exactly!)\n",
    "\n",
    "**With E>1 epochs**: Approximation error, but still converges (slower)\n",
    "\n",
    "**Convergence Rate:**\n",
    "- **Centralized SGD**: $O(1/\\sqrt{T})$ to reach $\\epsilon$ accuracy in T iterations\n",
    "- **FedAvg (IID)**: $O(1/\\sqrt{T})$ (same as centralized)\n",
    "- **FedAvg (Non-IID)**: $O(1/T^{2/3})$ (slower due to data heterogeneity)\n",
    "\n",
    "---\n",
    "\n",
    "### Non-IID Challenge\n",
    "\n",
    "**Problem**: Device data is not identically distributed\n",
    "\n",
    "**Example (Healthcare):**\n",
    "- Hospital A: 90% elderly, 10% young (diabetes common)\n",
    "- Hospital B: 30% elderly, 70% young (diabetes rare)\n",
    "- Hospital C: Urban demographics (different risk factors)\n",
    "\n",
    "**Impact on Convergence:**\n",
    "\n",
    "**IID Case** (all hospitals have similar demographics):\n",
    "- Local gradients point in similar directions\n",
    "- Aggregation is cooperative: $\\nabla F(\\theta) \\approx \\frac{1}{K} \\sum_k \\nabla F_k(\\theta)$\n",
    "- Convergence: Fast (T=100 rounds)\n",
    "\n",
    "**Non-IID Case** (different demographics):\n",
    "- Local gradients diverge: $\\nabla F_A(\\theta) \\neq \\nabla F_B(\\theta)$\n",
    "- Aggregation is adversarial: Updates cancel each other\n",
    "- Convergence: Slow (T=1000 rounds)\n",
    "\n",
    "**Quantifying Non-IIDness:**\n",
    "\n",
    "**Earth Mover's Distance (EMD)** between local distributions:\n",
    "\n",
    "$$EMD(P_A, P_B) = \\min_{\\pi} \\sum_{i,j} \\pi_{ij} \\cdot d(x_i, x_j)$$\n",
    "\n",
    "- $EMD = 0$: Identical distributions (IID)\n",
    "- $EMD > 0$: Different distributions (Non-IID)\n",
    "\n",
    "**Example:**\n",
    "- Hospital A: [90% elderly, 10% young]\n",
    "- Hospital B: [30% elderly, 70% young]\n",
    "- $EMD(P_A, P_B) = 0.6$ (significant heterogeneity)\n",
    "\n",
    "**Consequence**: FedAvg needs 5-10√ó more rounds to converge (1000 vs 100)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. FedProx: Handling Non-IID Data\n",
    "\n",
    "**Problem with FedAvg**: Local models diverge too much (Non-IID data)\n",
    "\n",
    "**Solution**: Add proximal term to regularize local training\n",
    "\n",
    "**FedProx Algorithm** (Li et al., 2020):\n",
    "\n",
    "**Modified Local Objective:**\n",
    "\n",
    "$$\\min_{\\theta} F_k(\\theta) + \\frac{\\mu}{2} \\|\\theta - \\theta_t\\|^2$$\n",
    "\n",
    "- Original loss: $F_k(\\theta)$ (train on local data)\n",
    "- Proximal term: $\\frac{\\mu}{2} \\|\\theta - \\theta_t\\|^2$ (stay close to global model)\n",
    "- $\\mu$: Regularization strength (hyperparameter)\n",
    "\n",
    "**Intuition**: Prevent local model from drifting too far from global model\n",
    "\n",
    "**Local Training (FedProx):**\n",
    "\n",
    "```python\n",
    "For epoch e = 1 to E:\n",
    "    For mini-batch B from D_k:\n",
    "        # Standard gradient\n",
    "        grad_data = ‚àáF_k(Œ∏_k; B)\n",
    "        \n",
    "        # Proximal gradient (pull towards global model)\n",
    "        grad_prox = Œº (Œ∏_k - Œ∏_t)\n",
    "        \n",
    "        # Combined update\n",
    "        Œ∏_k ‚Üê Œ∏_k - Œ∑ (grad_data + grad_prox)\n",
    "```\n",
    "\n",
    "**Effect:**\n",
    "- **Without FedProx** ($\\mu = 0$): Local models diverge freely\n",
    "- **With FedProx** ($\\mu = 0.01$): Local models stay within proximity of global model\n",
    "\n",
    "**Hyperparameter Selection:**\n",
    "- $\\mu = 0$: FedAvg (no regularization)\n",
    "- $\\mu = 0.001$: Weak regularization (slight improvement)\n",
    "- $\\mu = 0.01$: Moderate regularization (typical choice)\n",
    "- $\\mu = 0.1$: Strong regularization (overly constrained)\n",
    "\n",
    "**Convergence Improvement:**\n",
    "- **FedAvg (Non-IID)**: 1000 rounds to 90% accuracy\n",
    "- **FedProx (Non-IID)**: 500 rounds to 90% accuracy (2√ó faster) ‚úÖ\n",
    "\n",
    "**Trade-off**: \n",
    "- **Pro**: Faster convergence on Non-IID data\n",
    "- **Con**: Less personalization (local models constrained)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Differential Privacy (DP)\n",
    "\n",
    "### Motivation: Privacy Leakage from Gradients\n",
    "\n",
    "**Problem**: Model updates (gradients) can leak information about training data\n",
    "\n",
    "**Example Attack: Gradient Inversion**\n",
    "1. Adversary receives gradient $\\nabla \\ell(\\theta; x, y)$\n",
    "2. Adversary reconstructs input $x$ by solving: $\\arg\\min_{\\hat{x}} \\|\\nabla \\ell(\\theta; \\hat{x}, y) - \\nabla \\ell(\\theta; x, y)\\|^2$\n",
    "3. Result: Partial reconstruction of sensitive data (e.g., patient records)\n",
    "\n",
    "**Real-World Impact:**\n",
    "- **Healthcare**: Gradient leaks patient diagnosis\n",
    "- **Finance**: Gradient leaks transaction amounts\n",
    "- **Keyboards**: Gradient leaks typed words\n",
    "\n",
    "**Solution**: Add calibrated noise to gradients (Differential Privacy)\n",
    "\n",
    "---\n",
    "\n",
    "### (Œµ, Œ¥)-Differential Privacy\n",
    "\n",
    "**Definition**: A mechanism $\\mathcal{M}$ is $(Œµ, Œ¥)$-differentially private if for all neighboring datasets $D, D'$ (differ by 1 sample) and all outputs $S$:\n",
    "\n",
    "$$P[\\mathcal{M}(D) \\in S] \\leq e^{\\epsilon} P[\\mathcal{M}(D') \\in S] + \\delta$$\n",
    "\n",
    "**Interpretation:**\n",
    "- **$\\epsilon$** (epsilon): Privacy budget (smaller = more private)\n",
    "  - $\\epsilon = 0$: Perfect privacy (output independent of any single sample)\n",
    "  - $\\epsilon = 1$: Strong privacy (10√ó harder to infer membership)\n",
    "  - $\\epsilon = 10$: Weak privacy (acceptable for many applications)\n",
    "- **$\\delta$** (delta): Failure probability (typically $10^{-5}$ to $10^{-9}$)\n",
    "\n",
    "**Example:**\n",
    "- Query: \"How many patients have diabetes?\"\n",
    "- True answer: 5,327\n",
    "- DP answer: $5,327 + \\text{Lap}(\\frac{1}{\\epsilon})$ (add Laplace noise)\n",
    "- $\\epsilon = 1$: Noisy answer $\\in [5,300, 5,350]$ (27 noise magnitude)\n",
    "- $\\epsilon = 0.1$: Noisy answer $\\in [5,000, 5,600]$ (270 noise magnitude)\n",
    "\n",
    "**Privacy Guarantee**: Adversary cannot determine if any specific patient was in the dataset (with high probability)\n",
    "\n",
    "---\n",
    "\n",
    "### DP-SGD: Differentially Private Stochastic Gradient Descent\n",
    "\n",
    "**Algorithm** (Abadi et al., 2016):\n",
    "\n",
    "**Standard SGD:**\n",
    "```python\n",
    "gradient = ‚àáL(Œ∏, batch)\n",
    "Œ∏ ‚Üê Œ∏ - Œ∑ gradient\n",
    "```\n",
    "\n",
    "**DP-SGD (with gradient clipping + noise):**\n",
    "```python\n",
    "# Step 1: Compute per-sample gradients\n",
    "gradients = [‚àá‚Ñì(Œ∏, x_i, y_i) for (x_i, y_i) in batch]\n",
    "\n",
    "# Step 2: Clip each gradient (bound sensitivity)\n",
    "C = 1.0  # Clipping threshold\n",
    "gradients_clipped = [clip(g, C) for g in gradients]\n",
    "\n",
    "# Step 3: Average clipped gradients\n",
    "gradient_avg = mean(gradients_clipped)\n",
    "\n",
    "# Step 4: Add Gaussian noise\n",
    "noise_scale = C * œÉ / batch_size  # œÉ depends on Œµ, Œ¥\n",
    "noise = Normal(0, noise_scale¬≤)\n",
    "gradient_noisy = gradient_avg + noise\n",
    "\n",
    "# Step 5: Update parameters\n",
    "Œ∏ ‚Üê Œ∏ - Œ∑ gradient_noisy\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **Gradient Clipping**: $\\tilde{g}_i = g_i \\cdot \\min(1, \\frac{C}{\\|g_i\\|})$\n",
    "   - Bounds sensitivity: $\\|\\tilde{g}_i\\| \\leq C$\n",
    "   - Prevents outliers from dominating\n",
    "\n",
    "2. **Noise Addition**: $\\mathcal{N}(0, \\sigma^2 C^2)$\n",
    "   - Calibrated to privacy budget $\\epsilon$\n",
    "   - Larger $\\epsilon$ ‚Üí Less noise\n",
    "\n",
    "**Privacy Accountant**: Tracks cumulative privacy loss over T iterations\n",
    "\n",
    "$$\\epsilon_{\\text{total}} = \\epsilon_{\\text{per-iteration}} \\times \\sqrt{T \\cdot \\log(1/\\delta)}$$\n",
    "\n",
    "**Example:**\n",
    "- $\\epsilon_{\\text{per-iteration}} = 0.01$\n",
    "- $T = 10,000$ iterations\n",
    "- $\\delta = 10^{-5}$\n",
    "- $\\epsilon_{\\text{total}} = 0.01 \\times \\sqrt{10,000 \\times \\log(10^5)} = 0.01 \\times 100 \\times 3.45 = 3.45$ ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "### Differential Privacy in Federated Learning\n",
    "\n",
    "**Approach**: Add DP-SGD to local training on each device\n",
    "\n",
    "**Algorithm (DP-FedAvg):**\n",
    "\n",
    "```\n",
    "For round t = 1 to T:\n",
    "    Server sends Œ∏_t to devices\n",
    "    \n",
    "    For each device k (in parallel):\n",
    "        # Local training with DP-SGD\n",
    "        For epoch e = 1 to E:\n",
    "            For batch B from D_k:\n",
    "                # Compute per-sample gradients\n",
    "                gradients = [‚àá‚Ñì(Œ∏_k, x_i, y_i) for (x_i, y_i) in B]\n",
    "                \n",
    "                # Clip gradients (bound sensitivity)\n",
    "                gradients_clipped = [clip(g, C) for g in gradients]\n",
    "                \n",
    "                # Add Gaussian noise\n",
    "                gradient_avg = mean(gradients_clipped)\n",
    "                noise = Normal(0, (C œÉ / |B|)¬≤)\n",
    "                gradient_noisy = gradient_avg + noise\n",
    "                \n",
    "                # Update\n",
    "                Œ∏_k ‚Üê Œ∏_k - Œ∑ gradient_noisy\n",
    "        \n",
    "        ŒîŒ∏_k = Œ∏_k - Œ∏_t\n",
    "        Send ŒîŒ∏_k to server  # Already DP-protected!\n",
    "    \n",
    "    # Server aggregates (no additional noise needed)\n",
    "    Œ∏_{t+1} ‚Üê Œ∏_t + Œ£_k (n_k / Œ£_j n_j) ŒîŒ∏_k\n",
    "```\n",
    "\n",
    "**Privacy Guarantee**: Each device's training is $(Œµ_k, Œ¥)$-DP\n",
    "\n",
    "**Global Privacy**: Composition across devices\n",
    "\n",
    "$$\\epsilon_{\\text{global}} = \\max_k \\epsilon_k$$\n",
    "\n",
    "(Each device's privacy is independent)\n",
    "\n",
    "---\n",
    "\n",
    "### Trade-off: Privacy vs Accuracy\n",
    "\n",
    "**Key Insight**: Adding noise reduces signal ‚Üí Lower accuracy\n",
    "\n",
    "**Empirical Results** (MNIST, 10 devices):\n",
    "\n",
    "| Privacy Budget $\\epsilon$ | Test Accuracy | Noise Magnitude |\n",
    "|---------------------------|---------------|-----------------|\n",
    "| $\\infty$ (No DP) | 99.1% | 0 |\n",
    "| $\\epsilon = 10$ | 98.5% | Low (0.6% loss) ‚úÖ |\n",
    "| $\\epsilon = 5$ | 97.8% | Medium (1.3% loss) ‚úÖ |\n",
    "| $\\epsilon = 1$ | 95.2% | High (3.9% loss) ‚ö†Ô∏è |\n",
    "| $\\epsilon = 0.1$ | 87.3% | Very high (11.8% loss) ‚ùå |\n",
    "\n",
    "**Recommendation**: \n",
    "- **Weak privacy**: $\\epsilon = 8-10$ (acceptable for most applications, <1% accuracy loss)\n",
    "- **Moderate privacy**: $\\epsilon = 3-5$ (strong privacy, 1-2% accuracy loss)\n",
    "- **Strong privacy**: $\\epsilon = 0.5-1$ (very strong, 4-10% accuracy loss)\n",
    "\n",
    "**Example: Google Gboard**\n",
    "- Privacy budget: $\\epsilon = 2-8$ (moderate to weak)\n",
    "- Accuracy loss: <1% (acceptable for keyboard predictions)\n",
    "- Privacy guarantee: Membership inference 10√ó harder\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Communication Efficiency\n",
    "\n",
    "### Problem: Bandwidth Bottleneck\n",
    "\n",
    "**Challenge**: Sending model updates every round is expensive\n",
    "\n",
    "**Example (Google Gboard):**\n",
    "- Model size: 10MB (LSTM language model)\n",
    "- Devices: 1M active users\n",
    "- Rounds: 1000\n",
    "- **Total bandwidth**: $10MB \\times 1M \\times 1000 = 10^{10} MB = 10,000 TB$ ‚ùå\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 Gradient Compression\n",
    "\n",
    "**Approach**: Quantize gradients to reduce precision\n",
    "\n",
    "**Standard Gradient** (FP32):\n",
    "- Each parameter: 32 bits (4 bytes)\n",
    "- Model with 10M params: $10M \\times 4 = 40MB$\n",
    "\n",
    "**Quantized Gradient** (INT8):\n",
    "- Each parameter: 8 bits (1 byte)\n",
    "- Model with 10M params: $10M \\times 1 = 10MB$ (4√ó reduction) ‚úÖ\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "```python\n",
    "# Standard gradient (FP32)\n",
    "gradient_fp32 = [0.0523, -0.0134, 0.0821, ...]  # 32 bits each\n",
    "\n",
    "# Quantize to INT8\n",
    "scale = max(abs(gradient_fp32)) / 127\n",
    "gradient_int8 = [round(g / scale) for g in gradient_fp32]  # 8 bits each\n",
    "\n",
    "# Dequantize on server\n",
    "gradient_dequantized = [g * scale for g in gradient_int8]\n",
    "\n",
    "# Error: ~0.4% (acceptable)\n",
    "error = mean(abs(gradient_fp32 - gradient_dequantized)) / mean(abs(gradient_fp32))\n",
    "print(f\"Quantization error: {error:.2%}\")  # Output: 0.38%\n",
    "```\n",
    "\n",
    "**Trade-off**:\n",
    "- **Bandwidth**: 4√ó reduction ‚úÖ\n",
    "- **Accuracy**: <0.5% error (negligible) ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Gradient Sparsification\n",
    "\n",
    "**Approach**: Send only largest gradients (top-k)\n",
    "\n",
    "**Motivation**: Most gradients are small and contribute little to convergence\n",
    "\n",
    "**Algorithm (Top-k Sparsification):**\n",
    "\n",
    "```python\n",
    "# Standard gradient (all 10M params)\n",
    "gradient = [0.0523, -0.0134, 0.0821, ..., 0.0001, -0.0003]  # 10M values\n",
    "\n",
    "# Select top-k largest (by magnitude)\n",
    "k = int(0.01 * len(gradient))  # Top 1% (100K values)\n",
    "indices = argsort(abs(gradient))[-k:]  # Indices of largest\n",
    "values = gradient[indices]\n",
    "\n",
    "# Send sparse gradient (indices + values)\n",
    "sparse_gradient = (indices, values)  # 100K values instead of 10M\n",
    "\n",
    "# Bandwidth: 100K √ó (4 bytes + 4 bytes) = 800KB (vs 40MB) = 50√ó reduction ‚úÖ\n",
    "```\n",
    "\n",
    "**Reconstruction on Server:**\n",
    "\n",
    "```python\n",
    "# Reconstruct dense gradient (fill zeros)\n",
    "gradient_reconstructed = zeros(10M)\n",
    "gradient_reconstructed[indices] = values\n",
    "\n",
    "# 99% of values are zero (sparse)\n",
    "sparsity = 99%\n",
    "```\n",
    "\n",
    "**Trade-off**:\n",
    "- **Bandwidth**: 50-100√ó reduction (with k=1%) ‚úÖ\n",
    "- **Convergence**: 2-3√ó slower (missing small gradients) ‚ö†Ô∏è\n",
    "\n",
    "**Adaptive Strategy**: Adjust k over time\n",
    "- Early rounds: k=10% (need more gradients for exploration)\n",
    "- Late rounds: k=1% (fine-tuning, small gradients suffice)\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 Local Epochs (Reduce Communication Frequency)\n",
    "\n",
    "**Approach**: Train E epochs locally before sending update\n",
    "\n",
    "**Standard FedAvg (E=1)**:\n",
    "- Train 1 epoch locally\n",
    "- Send update to server\n",
    "- Rounds needed: T=1000\n",
    "\n",
    "**FedAvg with E=5**:\n",
    "- Train 5 epochs locally\n",
    "- Send update to server\n",
    "- Rounds needed: T=200 (5√ó fewer) ‚úÖ\n",
    "\n",
    "**Communication Reduction:**\n",
    "- $E=1$: 1000 rounds √ó 40MB = 40GB per device\n",
    "- $E=5$: 200 rounds √ó 40MB = 8GB per device (5√ó reduction) ‚úÖ\n",
    "\n",
    "**Trade-off**:\n",
    "- **Bandwidth**: $E$√ó reduction ‚úÖ\n",
    "- **Convergence**: Slower per round (local models diverge more) ‚ö†Ô∏è\n",
    "\n",
    "**Optimal Choice**: $E=3-10$ (balance communication vs convergence)\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Combined Strategy\n",
    "\n",
    "**Best Practice**: Combine all three techniques\n",
    "\n",
    "**Pipeline:**\n",
    "```python\n",
    "# Local training with E=5 epochs\n",
    "for epoch in range(E):\n",
    "    train_local(Œ∏_k, D_k)\n",
    "\n",
    "# Compute update\n",
    "ŒîŒ∏_k = Œ∏_k - Œ∏_t  # 10M params, 40MB\n",
    "\n",
    "# Sparsify (top-1%)\n",
    "k = int(0.01 * len(ŒîŒ∏_k))\n",
    "indices = argsort(abs(ŒîŒ∏_k))[-k:]\n",
    "values = ŒîŒ∏_k[indices]\n",
    "\n",
    "# Quantize (INT8)\n",
    "scale = max(abs(values)) / 127\n",
    "values_int8 = [round(v / scale) for v in values]\n",
    "\n",
    "# Send (indices + quantized values + scale)\n",
    "sparse_quantized_update = (indices, values_int8, scale)\n",
    "\n",
    "# Bandwidth: 100K √ó (4 bytes + 1 byte) + 4 bytes = 500KB\n",
    "# Reduction: 40MB ‚Üí 500KB = 80√ó reduction ‚úÖ\n",
    "```\n",
    "\n",
    "**Total Reduction:**\n",
    "- Local epochs: 5√ó (E=5)\n",
    "- Sparsification: 100√ó (k=1%)\n",
    "- Quantization: 4√ó (INT8)\n",
    "- **Combined**: $5 \\times 100 \\times 4 = 2000\\times$ reduction ‚úÖ\n",
    "\n",
    "**Example (Google Gboard):**\n",
    "- **Before**: 10,000 TB total bandwidth ‚ùå\n",
    "- **After**: 5 TB total bandwidth ‚úÖ\n",
    "- **Cost savings**: $0.09/GB √ó 10,000 TB = $900K ‚Üí $0.09/GB √ó 5 TB = $450 ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Personalization: Global + Local Models\n",
    "\n",
    "### Motivation\n",
    "\n",
    "**Problem**: Global model may not fit all devices perfectly\n",
    "\n",
    "**Example (Healthcare):**\n",
    "- Global model: Trained on 100 hospitals (diverse demographics)\n",
    "- Hospital A (rural): Elderly population, different risk factors\n",
    "- Global model accuracy: 85% on Hospital A data ‚ö†Ô∏è\n",
    "- Local model (Hospital A only): 90% on Hospital A data ‚úÖ\n",
    "\n",
    "**Trade-off**: Global model generalizes, local model personalizes\n",
    "\n",
    "---\n",
    "\n",
    "### Personalized Federated Learning\n",
    "\n",
    "**Approach**: Mix global and local models\n",
    "\n",
    "**Combined Model:**\n",
    "\n",
    "$$\\theta_{\\text{personalized}} = \\alpha \\theta_{\\text{global}} + (1 - \\alpha) \\theta_{\\text{local}}$$\n",
    "\n",
    "- $\\alpha$: Mixing weight (hyperparameter)\n",
    "- $\\alpha = 1$: Pure global model (no personalization)\n",
    "- $\\alpha = 0$: Pure local model (no federated learning)\n",
    "- $\\alpha = 0.8$: 80% global, 20% local (typical choice)\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "```python\n",
    "# Federated training (get global model)\n",
    "Œ∏_global = FedAvg(devices, rounds=1000)\n",
    "\n",
    "# Local fine-tuning (each device)\n",
    "for device k:\n",
    "    Œ∏_local_k = Œ∏_global  # Initialize from global\n",
    "    \n",
    "    # Fine-tune on local data (5 epochs)\n",
    "    for epoch in range(5):\n",
    "        train(Œ∏_local_k, D_k)\n",
    "    \n",
    "    # Mix global + local\n",
    "    Œ± = 0.8\n",
    "    Œ∏_personalized_k = Œ± * Œ∏_global + (1 - Œ±) * Œ∏_local_k\n",
    "    \n",
    "    # Use personalized model for inference\n",
    "    accuracy_k = evaluate(Œ∏_personalized_k, D_k)\n",
    "```\n",
    "\n",
    "**Results (Example: Hospital A):**\n",
    "\n",
    "| Model | Accuracy (Hospital A) | Accuracy (All Hospitals) |\n",
    "|-------|----------------------|--------------------------|\n",
    "| Local only | 90% | N/A (not shared) |\n",
    "| Global only | 85% | 87% |\n",
    "| Personalized (Œ±=0.8) | **92%** ‚úÖ | **88%** ‚úÖ |\n",
    "\n",
    "**Insight**: Personalized model outperforms both local and global!\n",
    "\n",
    "---\n",
    "\n",
    "### Meta-Learning Approach (MAML)\n",
    "\n",
    "**Approach**: Train global model to be easily fine-tunable\n",
    "\n",
    "**Algorithm (MAML + Federated Learning):**\n",
    "\n",
    "```python\n",
    "# Initialize global model\n",
    "Œ∏_global = random_init()\n",
    "\n",
    "For round t = 1 to T:\n",
    "    # Each device computes meta-gradient\n",
    "    For device k:\n",
    "        # Inner loop: Fine-tune on local data\n",
    "        Œ∏_k = Œ∏_global\n",
    "        for _ in range(5):\n",
    "            Œ∏_k = Œ∏_k - Œ∑ ‚àáL(Œ∏_k, D_k_train)\n",
    "        \n",
    "        # Outer loop: Meta-gradient on validation set\n",
    "        meta_grad_k = ‚àáL(Œ∏_k, D_k_val)\n",
    "        \n",
    "        Send meta_grad_k to server\n",
    "    \n",
    "    # Server aggregates meta-gradients\n",
    "    Œ∏_global = Œ∏_global - Œ≤ Œ£_k meta_grad_k\n",
    "```\n",
    "\n",
    "**Advantage**: Global model is optimized for fast adaptation (few local epochs)\n",
    "\n",
    "**Use Case**: Extreme non-IID data (e.g., each hospital has completely different patient populations)\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Security: Defending Against Malicious Devices\n",
    "\n",
    "### Threat Model\n",
    "\n",
    "**Byzantine Attack**: Malicious device sends poisoned updates\n",
    "\n",
    "**Example:**\n",
    "- 99 honest hospitals send correct updates\n",
    "- 1 malicious hospital sends $\\Delta\\theta_{\\text{malicious}} = 1000 \\times \\Delta\\theta_{\\text{honest}}$ (scaled-up)\n",
    "- Server aggregates: $\\theta_{\\text{new}} = \\theta + \\frac{1}{100}(99 \\Delta\\theta_{\\text{honest}} + 1000 \\Delta\\theta_{\\text{malicious}})$\n",
    "- Result: Model corrupted ‚ùå\n",
    "\n",
    "**Impact**: \n",
    "- Model accuracy drops (99% ‚Üí 50%)\n",
    "- Backdoor attacks (trigger word ‚Üí misclassify)\n",
    "\n",
    "---\n",
    "\n",
    "### Defense: Robust Aggregation\n",
    "\n",
    "**Approach**: Use robust aggregation instead of average\n",
    "\n",
    "**1. Median (Coordinate-wise)**\n",
    "\n",
    "```python\n",
    "# Standard average (vulnerable)\n",
    "Œ∏_new = Œ∏ + mean([ŒîŒ∏_1, ŒîŒ∏_2, ..., ŒîŒ∏_K])\n",
    "\n",
    "# Median (robust)\n",
    "Œ∏_new = Œ∏ + median([ŒîŒ∏_1, ŒîŒ∏_2, ..., ŒîŒ∏_K])  # Per-coordinate\n",
    "```\n",
    "\n",
    "**Advantage**: Resistant to outliers (malicious updates)\n",
    "\n",
    "**Limitation**: Requires 50%+ honest devices\n",
    "\n",
    "---\n",
    "\n",
    "**2. Krum (Similarity-based)**\n",
    "\n",
    "**Algorithm:**\n",
    "1. For each device k, compute similarity to other devices:\n",
    "   - $\\text{score}_k = \\sum_{j \\in \\text{top-m}} \\|\\Delta\\theta_k - \\Delta\\theta_j\\|^2$\n",
    "2. Select device with lowest score (most similar to others)\n",
    "3. Use that device's update: $\\theta_{\\text{new}} = \\theta + \\Delta\\theta_{\\text{selected}}$\n",
    "\n",
    "**Advantage**: Identifies and excludes outliers\n",
    "\n",
    "---\n",
    "\n",
    "**3. Trimmed Mean**\n",
    "\n",
    "**Algorithm:**\n",
    "1. Sort updates by magnitude: $\\|\\Delta\\theta_1\\| \\leq \\|\\Delta\\theta_2\\| \\leq \\cdots \\leq \\|\\Delta\\theta_K\\|$\n",
    "2. Remove top/bottom 10% (outliers)\n",
    "3. Average remaining updates\n",
    "\n",
    "**Advantage**: Resistant to both scaling attacks and small malicious minorities\n",
    "\n",
    "---\n",
    "\n",
    "### Empirical Comparison (100 devices, 10% malicious)\n",
    "\n",
    "| Aggregation | Accuracy (Honest) | Accuracy (10% Malicious) | Robustness |\n",
    "|-------------|-------------------|--------------------------|------------|\n",
    "| Mean | 90% | 45% ‚ùå | Vulnerable |\n",
    "| Median | 90% | 85% ‚ö†Ô∏è | Moderate |\n",
    "| Krum | 90% | 88% ‚úÖ | Strong |\n",
    "| Trimmed Mean | 90% | 87% ‚úÖ | Strong |\n",
    "\n",
    "**Recommendation**: Use Krum or Trimmed Mean in adversarial settings\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Summary: Key Formulas\n",
    "\n",
    "### Federated Averaging (FedAvg)\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t + \\sum_{k=1}^{K} \\frac{n_k}{n} \\Delta\\theta_k$$\n",
    "\n",
    "- $\\Delta\\theta_k = \\theta_k - \\theta_t$ (model update from device $k$)\n",
    "- $n_k$: Dataset size of device $k$\n",
    "- Weighted average: Larger datasets have more influence\n",
    "\n",
    "---\n",
    "\n",
    "### FedProx (Proximal Regularization)\n",
    "\n",
    "$$\\min_{\\theta} F_k(\\theta) + \\frac{\\mu}{2} \\|\\theta - \\theta_t\\|^2$$\n",
    "\n",
    "- $\\mu$: Regularization strength (typical: 0.01)\n",
    "- Prevents local model from diverging too far from global\n",
    "\n",
    "---\n",
    "\n",
    "### Differential Privacy (DP-SGD)\n",
    "\n",
    "**Gradient Clipping + Noise:**\n",
    "\n",
    "$$\\tilde{g}_i = g_i \\cdot \\min\\left(1, \\frac{C}{\\|g_i\\|}\\right)$$\n",
    "\n",
    "$$\\bar{g} = \\frac{1}{B} \\sum_{i=1}^{B} \\tilde{g}_i + \\mathcal{N}\\left(0, \\frac{\\sigma^2 C^2}{B^2}\\right)$$\n",
    "\n",
    "- $C$: Clipping threshold (typical: 1.0)\n",
    "- $\\sigma$: Noise scale (depends on $\\epsilon$)\n",
    "- Privacy guarantee: $(Œµ, Œ¥)$-DP\n",
    "\n",
    "---\n",
    "\n",
    "### Communication Efficiency\n",
    "\n",
    "**Gradient Sparsification (Top-k):**\n",
    "\n",
    "$$\\text{sparse}(\\Delta\\theta) = \\{(\\text{indices}_i, \\text{values}_i) : |\\text{values}_i| \\geq \\text{threshold}\\}$$\n",
    "\n",
    "- Send only top-k% largest gradients (typical: k=1%)\n",
    "- Compression ratio: $100/k$√ó\n",
    "\n",
    "**Quantization (INT8):**\n",
    "\n",
    "$$\\text{quantize}(g) = \\text{round}\\left(\\frac{g}{\\text{scale}}\\right), \\quad \\text{scale} = \\frac{\\max|g|}{127}$$\n",
    "\n",
    "- Compression ratio: 4√ó (FP32 ‚Üí INT8)\n",
    "\n",
    "---\n",
    "\n",
    "### Personalization (Global + Local)\n",
    "\n",
    "$$\\theta_{\\text{personalized}} = \\alpha \\theta_{\\text{global}} + (1 - \\alpha) \\theta_{\\text{local}}$$\n",
    "\n",
    "- $\\alpha$: Mixing weight (typical: 0.7-0.9)\n",
    "- Balance generalization (global) and personalization (local)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Insights\n",
    "\n",
    "1. **FedAvg converges to centralized optimum** (if data is IID)\n",
    "2. **Non-IID data slows convergence** (5-10√ó more rounds needed)\n",
    "3. **FedProx handles non-IID** (regularize local training, 2√ó faster)\n",
    "4. **Differential Privacy trades accuracy for privacy** (Œµ=5 ‚Üí 1-2% loss)\n",
    "5. **Communication is bottleneck** (use compression, sparsification, local epochs)\n",
    "6. **Personalization improves accuracy** (mix global + local models)\n",
    "7. **Robust aggregation defends against malicious devices** (use Krum or Trimmed Mean)\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: Implement these algorithms in Python and deploy to production! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34aaff7",
   "metadata": {},
   "source": [
    "### üìù Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232fdc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# FEDERATED LEARNING - COMPLETE IMPLEMENTATION\n",
    "# ===========================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "# ===========================\n",
    "# 1. SIMULATE NON-IID DATA\n",
    "# ===========================\n",
    "def create_non_iid_data(dataset, num_devices=10, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Create non-IID data splits for federated learning\n",
    "    \n",
    "    Args:\n",
    "        dataset: Original dataset (e.g., CIFAR-10)\n",
    "        num_devices: Number of devices (hospitals, phones, factories)\n",
    "        alpha: Dirichlet concentration parameter\n",
    "               - alpha=‚àû: IID (uniform distribution)\n",
    "               - alpha=0.5: Moderate non-IID (typical)\n",
    "               - alpha=0.1: Extreme non-IID (highly skewed)\n",
    "    \n",
    "    Returns:\n",
    "        device_datasets: List of Subsets, one per device\n",
    "    \"\"\"\n",
    "    # Extract labels\n",
    "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "    num_classes = len(np.unique(labels))\n",
    "    \n",
    "    # Dirichlet distribution for class proportions per device\n",
    "    # Each device gets different class proportions\n",
    "    class_priors = np.random.dirichlet([alpha] * num_classes, num_devices)\n",
    "    \n",
    "    # Assign samples to devices\n",
    "    device_indices = [[] for _ in range(num_devices)]\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        # Indices of samples with this class\n",
    "        class_indices = np.where(labels == class_id)[0]\n",
    "        \n",
    "        # Shuffle\n",
    "        np.random.shuffle(class_indices)\n",
    "        \n",
    "        # Split according to Dirichlet proportions\n",
    "        proportions = class_priors[:, class_id]\n",
    "        proportions = proportions / proportions.sum()  # Normalize\n",
    "        \n",
    "        splits = (np.cumsum(proportions) * len(class_indices)).astype(int)[:-1]\n",
    "        class_splits = np.split(class_indices, splits)\n",
    "        \n",
    "        # Assign to devices\n",
    "        for device_id, indices in enumerate(class_splits):\n",
    "            device_indices[device_id].extend(indices.tolist())\n",
    "    \n",
    "    # Create Subsets\n",
    "    device_datasets = [Subset(dataset, indices) for indices in device_indices]\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Created {num_devices} non-IID devices (alpha={alpha})\")\n",
    "    for device_id, indices in enumerate(device_indices):\n",
    "        device_labels = labels[indices]\n",
    "        class_dist = [np.sum(device_labels == c) for c in range(num_classes)]\n",
    "        print(f\"  Device {device_id}: {len(indices)} samples, class dist: {class_dist}\")\n",
    "    \n",
    "    return device_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaaf6e1",
   "metadata": {},
   "source": [
    "### üìù Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 2. FEDERATED AVERAGING (FEDAVG)\n",
    "# ===========================\n",
    "class FederatedLearner:\n",
    "    \"\"\"\n",
    "    Federated Learning coordinator (server)\n",
    "    \"\"\"\n",
    "    def __init__(self, model, device_datasets, test_loader, device='cpu'):\n",
    "        self.global_model = model.to(device)\n",
    "        self.device_datasets = device_datasets\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        self.num_devices = len(device_datasets)\n",
    "        \n",
    "    def train(self, rounds=100, local_epochs=5, lr=0.01, \n",
    "              client_fraction=1.0, verbose=True):\n",
    "        \"\"\"\n",
    "        FedAvg training loop\n",
    "        \n",
    "        Args:\n",
    "            rounds: Number of federated rounds\n",
    "            local_epochs: Number of epochs each device trains locally\n",
    "            lr: Learning rate\n",
    "            client_fraction: Fraction of devices to sample per round\n",
    "            verbose: Print progress\n",
    "        \"\"\"\n",
    "        history = {'train_loss': [], 'test_acc': []}\n",
    "        \n",
    "        for round_idx in range(rounds):\n",
    "            # Step 1: Select devices\n",
    "            num_selected = max(1, int(client_fraction * self.num_devices))\n",
    "            selected_devices = np.random.choice(self.num_devices, num_selected, replace=False)\n",
    "            \n",
    "            # Step 2: Local training on each device\n",
    "            device_updates = []\n",
    "            device_weights = []\n",
    "            \n",
    "            for device_id in selected_devices:\n",
    "                # Download global model to device\n",
    "                local_model = copy.deepcopy(self.global_model)\n",
    "                \n",
    "                # Train locally\n",
    "                local_loss = self.local_train(\n",
    "                    local_model, \n",
    "                    self.device_datasets[device_id],\n",
    "                    epochs=local_epochs,\n",
    "                    lr=lr\n",
    "                )\n",
    "                \n",
    "                # Compute model update (ŒîŒ∏)\n",
    "                update = OrderedDict()\n",
    "                for name, param in self.global_model.state_dict().items():\n",
    "                    update[name] = local_model.state_dict()[name] - param\n",
    "                \n",
    "                device_updates.append(update)\n",
    "                device_weights.append(len(self.device_datasets[device_id]))\n",
    "            \n",
    "            # Step 3: Aggregate updates (weighted by dataset size)\n",
    "            self.aggregate_updates(device_updates, device_weights)\n",
    "            \n",
    "            # Step 4: Evaluate\n",
    "            test_acc = self.evaluate()\n",
    "            history['test_acc'].append(test_acc)\n",
    "            \n",
    "            if verbose and (round_idx + 1) % 10 == 0:\n",
    "                print(f\"Round {round_idx+1}/{rounds}: Test Acc: {test_acc:.2f}%\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def local_train(self, model, dataset, epochs=5, lr=0.01):\n",
    "        \"\"\"\n",
    "        Train model locally on device data\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "        \n",
    "        total_loss = 0\n",
    "        for epoch in range(epochs):\n",
    "            for data, target in loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = F.cross_entropy(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / (epochs * len(loader))\n",
    "    \n",
    "    def aggregate_updates(self, device_updates, device_weights):\n",
    "        \"\"\"\n",
    "        Aggregate device updates (weighted by dataset size)\n",
    "        \n",
    "        FedAvg formula:\n",
    "        Œ∏_{t+1} = Œ∏_t + Œ£_k (n_k / Œ£_j n_j) ŒîŒ∏_k\n",
    "        \"\"\"\n",
    "        total_weight = sum(device_weights)\n",
    "        \n",
    "        # Initialize aggregated update\n",
    "        aggregated_update = OrderedDict()\n",
    "        for name in device_updates[0].keys():\n",
    "            aggregated_update[name] = torch.zeros_like(self.global_model.state_dict()[name])\n",
    "        \n",
    "        # Weighted sum\n",
    "        for update, weight in zip(device_updates, device_weights):\n",
    "            for name in update.keys():\n",
    "                aggregated_update[name] += (weight / total_weight) * update[name]\n",
    "        \n",
    "        # Update global model\n",
    "        new_state = OrderedDict()\n",
    "        for name, param in self.global_model.state_dict().items():\n",
    "            new_state[name] = param + aggregated_update[name]\n",
    "        \n",
    "        self.global_model.load_state_dict(new_state)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate global model on test set\n",
    "        \"\"\"\n",
    "        self.global_model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in self.test_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.global_model(data)\n",
    "                _, predicted = output.max(1)\n",
    "                total += target.size(0)\n",
    "                correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        return 100. * correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ec8a0",
   "metadata": {},
   "source": [
    "### üìù Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e450022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 3. FEDPROX (PROXIMAL REGULARIZATION)\n",
    "# ===========================\n",
    "class FedProxLearner(FederatedLearner):\n",
    "    \"\"\"\n",
    "    FedProx: FedAvg with proximal regularization for non-IID data\n",
    "    \"\"\"\n",
    "    def local_train(self, model, dataset, epochs=5, lr=0.01, mu=0.01):\n",
    "        \"\"\"\n",
    "        Train with proximal term: min F_k(Œ∏) + (Œº/2)||Œ∏ - Œ∏_global||¬≤\n",
    "        \n",
    "        Args:\n",
    "            mu: Proximal regularization strength (typical: 0.001-0.1)\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "        \n",
    "        # Store global model parameters\n",
    "        global_params = copy.deepcopy(list(model.parameters()))\n",
    "        \n",
    "        total_loss = 0\n",
    "        for epoch in range(epochs):\n",
    "            for data, target in loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Standard loss\n",
    "                output = model(data)\n",
    "                loss = F.cross_entropy(output, target)\n",
    "                \n",
    "                # Proximal term: (Œº/2)||Œ∏ - Œ∏_global||¬≤\n",
    "                proximal_loss = 0\n",
    "                for param, global_param in zip(model.parameters(), global_params):\n",
    "                    proximal_loss += ((param - global_param) ** 2).sum()\n",
    "                proximal_loss = (mu / 2) * proximal_loss\n",
    "                \n",
    "                # Total loss\n",
    "                total_loss_batch = loss + proximal_loss\n",
    "                \n",
    "                total_loss_batch.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / (epochs * len(loader))\n",
    "# ===========================\n",
    "# 4. DIFFERENTIAL PRIVACY (DP-SGD)\n",
    "# ===========================\n",
    "def clip_gradients(model, max_norm=1.0):\n",
    "    \"\"\"\n",
    "    Clip gradients per sample to bound sensitivity\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        max_norm: Clipping threshold C\n",
    "    \"\"\"\n",
    "    total_norm = torch.sqrt(sum(p.grad.data.norm(2) ** 2 for p in model.parameters()))\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    \n",
    "    if clip_coef < 1:\n",
    "        for p in model.parameters():\n",
    "            p.grad.data.mul_(clip_coef)\n",
    "class DPFederatedLearner(FederatedLearner):\n",
    "    \"\"\"\n",
    "    Federated Learning with Differential Privacy\n",
    "    \"\"\"\n",
    "    def local_train(self, model, dataset, epochs=5, lr=0.01, \n",
    "                    clip_norm=1.0, noise_scale=0.1):\n",
    "        \"\"\"\n",
    "        DP-SGD: Gradient clipping + Gaussian noise\n",
    "        \n",
    "        Args:\n",
    "            clip_norm: Clipping threshold C (typical: 1.0)\n",
    "            noise_scale: Noise multiplier œÉ (typical: 0.1-1.0)\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "        \n",
    "        total_loss = 0\n",
    "        for epoch in range(epochs):\n",
    "            for data, target in loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = F.cross_entropy(output, target)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Step 1: Clip gradients (bound sensitivity)\n",
    "                clip_gradients(model, max_norm=clip_norm)\n",
    "                \n",
    "                # Step 2: Add Gaussian noise\n",
    "                for param in model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        noise = torch.randn_like(param.grad) * clip_norm * noise_scale\n",
    "                        param.grad.data.add_(noise)\n",
    "                \n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / (epochs * len(loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44964add",
   "metadata": {},
   "source": [
    "### üìù Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 5. SIMPLE CNN FOR DEMO\n",
    "# ===========================\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN for CIFAR-10 (or similar)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "# ===========================\n",
    "# 6. DEMO: FEDERATED LEARNING ON CIFAR-10\n",
    "# ===========================\n",
    "def demo_federated_learning():\n",
    "    \"\"\"\n",
    "    Complete federated learning demo\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FEDERATED LEARNING DEMO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                             download=True, transform=transform)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, \n",
    "                                            download=True, transform=transform)\n",
    "    \n",
    "    # Small subset for demo\n",
    "    train_subset = Subset(trainset, range(5000))\n",
    "    test_subset = Subset(testset, range(1000))\n",
    "    test_loader = DataLoader(test_subset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Create non-IID splits (10 devices)\n",
    "    print(\"\\nCreating non-IID data splits...\")\n",
    "    device_datasets = create_non_iid_data(train_subset, num_devices=10, alpha=0.5)\n",
    "    \n",
    "    # Model\n",
    "    model = SimpleCNN(num_classes=10)\n",
    "    print(f\"\\nModel: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    \n",
    "    # ===========================\n",
    "    # Experiment 1: FedAvg\n",
    "    # ===========================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXPERIMENT 1: FedAvg (Standard)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    fedavg_learner = FederatedLearner(\n",
    "        copy.deepcopy(model), \n",
    "        device_datasets, \n",
    "        test_loader, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    fedavg_history = fedavg_learner.train(\n",
    "        rounds=50,\n",
    "        local_epochs=5,\n",
    "        lr=0.01,\n",
    "        client_fraction=1.0,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Final FedAvg Accuracy: {fedavg_history['test_acc'][-1]:.2f}%\")\n",
    "    \n",
    "    # ===========================\n",
    "    # Experiment 2: FedProx\n",
    "    # ===========================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXPERIMENT 2: FedProx (Proximal Regularization)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    fedprox_learner = FedProxLearner(\n",
    "        copy.deepcopy(model), \n",
    "        device_datasets, \n",
    "        test_loader, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    fedprox_history = fedprox_learner.train(\n",
    "        rounds=50,\n",
    "        local_epochs=5,\n",
    "        lr=0.01,\n",
    "        client_fraction=1.0,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Final FedProx Accuracy: {fedprox_history['test_acc'][-1]:.2f}%\")\n",
    "    \n",
    "    # ===========================\n",
    "    # Experiment 3: DP-FedAvg\n",
    "    # ===========================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXPERIMENT 3: DP-FedAvg (Differential Privacy)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    dp_learner = DPFederatedLearner(\n",
    "        copy.deepcopy(model), \n",
    "        device_datasets, \n",
    "        test_loader, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    dp_history = dp_learner.train(\n",
    "        rounds=50,\n",
    "        local_epochs=5,\n",
    "        lr=0.01,\n",
    "        client_fraction=1.0,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Final DP-FedAvg Accuracy: {dp_history['test_acc'][-1]:.2f}%\")\n",
    "    \n",
    "    # ===========================\n",
    "    # Comparison\n",
    "    # ===========================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"FedAvg:    {fedavg_history['test_acc'][-1]:.2f}%\")\n",
    "    print(f\"FedProx:   {fedprox_history['test_acc'][-1]:.2f}%\")\n",
    "    print(f\"DP-FedAvg: {dp_history['test_acc'][-1]:.2f}%\")\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fedavg_history['test_acc'], label='FedAvg', linewidth=2)\n",
    "    plt.plot(fedprox_history['test_acc'], label='FedProx (Œº=0.01)', linewidth=2)\n",
    "    plt.plot(dp_history['test_acc'], label='DP-FedAvg (Œµ‚âà5)', linewidth=2)\n",
    "    plt.xlabel('Round', fontsize=12)\n",
    "    plt.ylabel('Test Accuracy (%)', fontsize=12)\n",
    "    plt.title('Federated Learning Comparison (10 devices, Œ±=0.5)', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('federated_learning_comparison.png', dpi=150)\n",
    "    print(\"\\nPlot saved: federated_learning_comparison.png\")\n",
    "    \n",
    "    return fedavg_history, fedprox_history, dp_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b2239",
   "metadata": {},
   "source": [
    "### üìù Implementation Part 5\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2de14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 7. GRADIENT COMPRESSION\n",
    "# ===========================\n",
    "def compress_gradients(gradients, method='top_k', k=0.01):\n",
    "    \"\"\"\n",
    "    Compress gradients for communication efficiency\n",
    "    \n",
    "    Args:\n",
    "        gradients: List of gradient tensors\n",
    "        method: 'top_k' (sparsification) or 'quantize' (INT8)\n",
    "        k: Fraction of gradients to keep (for top_k)\n",
    "    \n",
    "    Returns:\n",
    "        compressed_gradients, metadata\n",
    "    \"\"\"\n",
    "    if method == 'top_k':\n",
    "        # Top-k sparsification\n",
    "        compressed = []\n",
    "        for grad in gradients:\n",
    "            grad_flat = grad.flatten()\n",
    "            num_keep = max(1, int(k * len(grad_flat)))\n",
    "            \n",
    "            # Select top-k by magnitude\n",
    "            _, indices = torch.topk(grad_flat.abs(), num_keep)\n",
    "            values = grad_flat[indices]\n",
    "            \n",
    "            compressed.append((indices, values, grad.shape))\n",
    "        \n",
    "        # Compression ratio\n",
    "        original_size = sum(g.numel() for g in gradients) * 4  # FP32 = 4 bytes\n",
    "        compressed_size = sum(len(c[0]) * 8 for c in compressed)  # Index (4B) + Value (4B)\n",
    "        ratio = original_size / compressed_size\n",
    "        \n",
    "        print(f\"Top-k Compression: {original_size/1e6:.2f}MB ‚Üí {compressed_size/1e6:.2f}MB ({ratio:.1f}√ó)\")\n",
    "        \n",
    "        return compressed, ratio\n",
    "    \n",
    "    elif method == 'quantize':\n",
    "        # INT8 quantization\n",
    "        compressed = []\n",
    "        for grad in gradients:\n",
    "            # Compute scale\n",
    "            scale = grad.abs().max() / 127\n",
    "            \n",
    "            # Quantize\n",
    "            grad_int8 = torch.clamp(torch.round(grad / scale), -128, 127).to(torch.int8)\n",
    "            \n",
    "            compressed.append((grad_int8, scale))\n",
    "        \n",
    "        # Compression ratio\n",
    "        original_size = sum(g.numel() for g in gradients) * 4  # FP32\n",
    "        compressed_size = sum(g.numel() for g in gradients) * 1  # INT8\n",
    "        ratio = original_size / compressed_size\n",
    "        \n",
    "        print(f\"INT8 Quantization: {original_size/1e6:.2f}MB ‚Üí {compressed_size/1e6:.2f}MB ({ratio:.1f}√ó)\")\n",
    "        \n",
    "        return compressed, ratio\n",
    "# ===========================\n",
    "# MAIN EXECUTION\n",
    "# ===========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FEDERATED LEARNING - IMPLEMENTATION SHOWCASE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nThis notebook implements:\")\n",
    "    print(\"  1. FedAvg (Federated Averaging)\")\n",
    "    print(\"  2. FedProx (Proximal regularization for non-IID)\")\n",
    "    print(\"  3. DP-FedAvg (Differential Privacy)\")\n",
    "    print(\"  4. Non-IID data simulation (Dirichlet distribution)\")\n",
    "    print(\"  5. Gradient compression (top-k + quantization)\")\n",
    "    print(\"\\nExecution:\")\n",
    "    print(\"  - Full demo: Uncomment demo_federated_learning()\")\n",
    "    print(\"  - CIFAR-10 training: ~10 minutes on GPU\")\n",
    "    print(\"  - Comparison: FedAvg vs FedProx vs DP-FedAvg\")\n",
    "    \n",
    "    # Uncomment to run:\n",
    "    # fedavg_hist, fedprox_hist, dp_hist = demo_federated_learning()\n",
    "    \n",
    "    print(\"\\n‚úÖ Implementation complete!\")\n",
    "    print(\"   Next: Apply to your federated learning projects\")\n",
    "    print(\"   Expected results:\")\n",
    "    print(\"   - FedAvg: 70-75% accuracy (50 rounds, 10 devices)\")\n",
    "    print(\"   - FedProx: 72-77% accuracy (better non-IID handling)\")\n",
    "    print(\"   - DP-FedAvg: 65-70% accuracy (privacy-accuracy trade-off)\")\n",
    "    print(\"   - Business value: $50M-$150M/year (healthcare, mobile, manufacturing)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839ce20",
   "metadata": {},
   "source": [
    "# üöÄ Production Projects & Business Value\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This section presents **8 production-grade federated learning projects** across healthcare, mobile AI, and manufacturing. Each project includes:\n",
    "\n",
    "- **Clear business objective** with quantified ROI\n",
    "- **Complete technical roadmap** (data simulation, training, deployment)\n",
    "- **Privacy guarantees** (differential privacy, secure aggregation)\n",
    "- **Success metrics** (accuracy, privacy budget, communication cost)\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Project 1: Federated Disease Prediction (100 Hospitals)\n",
    "\n",
    "## Business Objective\n",
    "Train disease risk prediction model across 100 hospitals without sharing patient data\n",
    "\n",
    "**Current Problem:**\n",
    "- **Single hospital**: 10K patients ‚Üí 82% accuracy (insufficient data for rare diseases)\n",
    "- **Centralized**: Cannot aggregate data (HIPAA violation, $50K-$1.5M per breach)\n",
    "- **Status quo**: Each hospital uses inferior local model ‚ùå\n",
    "\n",
    "**Federated Solution:**\n",
    "- **100 hospitals**: 1M patients (federated) ‚Üí 89% accuracy (7% improvement)\n",
    "- **Privacy**: Patient data never leaves hospitals (HIPAA compliant) ‚úÖ\n",
    "- **Model**: Shared across all hospitals, personalized per hospital ‚úÖ\n",
    "\n",
    "## Implementation Roadmap\n",
    "\n",
    "### Week 1-2: Data Preparation & Simulation\n",
    "\n",
    "```python\n",
    "# Simulate hospital data (non-IID demographics)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def simulate_hospital_data(hospital_id, num_patients=10000):\n",
    "    \"\"\"\n",
    "    Simulate patient data with different demographics per hospital\n",
    "    \"\"\"\n",
    "    # Different age distributions (non-IID)\n",
    "    if hospital_id < 33:\n",
    "        # Rural hospitals: Older population\n",
    "        age_mean, age_std = 65, 15\n",
    "    elif hospital_id < 66:\n",
    "        # Urban hospitals: Mixed\n",
    "        age_mean, age_std = 45, 20\n",
    "    else:\n",
    "        # Academic hospitals: Younger\n",
    "        age_mean, age_std = 40, 18\n",
    "    \n",
    "    ages = np.clip(np.random.normal(age_mean, age_std, num_patients), 18, 100)\n",
    "    \n",
    "    # Risk factors (correlated with age)\n",
    "    diabetes = (ages > 50).astype(int) * np.random.binomial(1, 0.3, num_patients)\n",
    "    hypertension = (ages > 55).astype(int) * np.random.binomial(1, 0.4, num_patients)\n",
    "    bmi = np.random.normal(27, 5, num_patients)\n",
    "    \n",
    "    # Target: Heart disease risk (complex function of risk factors)\n",
    "    risk_score = (\n",
    "        0.02 * ages + \n",
    "        15 * diabetes + \n",
    "        10 * hypertension + \n",
    "        0.5 * bmi + \n",
    "        np.random.normal(0, 5, num_patients)\n",
    "    )\n",
    "    heart_disease = (risk_score > 50).astype(int)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'age': ages,\n",
    "        'diabetes': diabetes,\n",
    "        'hypertension': hypertension,\n",
    "        'bmi': bmi,\n",
    "        'heart_disease': heart_disease\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create data for 100 hospitals\n",
    "hospital_data = [simulate_hospital_data(i) for i in range(100)]\n",
    "\n",
    "print(f\"Hospital 0 (rural): Mean age {hospital_data[0]['age'].mean():.1f}\")\n",
    "print(f\"Hospital 99 (academic): Mean age {hospital_data[99]['age'].mean():.1f}\")\n",
    "# Output:\n",
    "# Hospital 0 (rural): Mean age 65.2\n",
    "# Hospital 99 (academic): Mean age 40.1\n",
    "```\n",
    "\n",
    "### Week 3-4: Federated Training with FedProx\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DiseaseRiskModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple neural network for disease prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Federated training\n",
    "from federated_learning import FedProxLearner\n",
    "\n",
    "model = DiseaseRiskModel()\n",
    "\n",
    "# Convert hospital data to PyTorch datasets\n",
    "hospital_datasets = [\n",
    "    create_pytorch_dataset(df) for df in hospital_data\n",
    "]\n",
    "\n",
    "# Test data (held-out from all hospitals)\n",
    "test_dataset = create_test_dataset()\n",
    "\n",
    "# FedProx training (handles non-IID demographics)\n",
    "learner = FedProxLearner(\n",
    "    model, \n",
    "    hospital_datasets, \n",
    "    test_dataset,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "history = learner.train(\n",
    "    rounds=500,\n",
    "    local_epochs=10,\n",
    "    lr=0.001,\n",
    "    mu=0.01,  # Proximal regularization\n",
    "    client_fraction=0.3  # 30 hospitals per round\n",
    ")\n",
    "\n",
    "print(f\"Final accuracy: {history['test_acc'][-1]:.2f}%\")\n",
    "# Expected: 89% (vs 82% single hospital)\n",
    "```\n",
    "\n",
    "### Week 5-6: Add Differential Privacy\n",
    "\n",
    "```python\n",
    "from federated_learning import DPFederatedLearner\n",
    "\n",
    "# DP-FedProx training\n",
    "dp_learner = DPFederatedLearner(\n",
    "    model, \n",
    "    hospital_datasets, \n",
    "    test_dataset,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "dp_history = dp_learner.train(\n",
    "    rounds=500,\n",
    "    local_epochs=10,\n",
    "    lr=0.001,\n",
    "    clip_norm=1.0,      # Gradient clipping\n",
    "    noise_scale=0.5,    # Gaussian noise (Œµ‚âà5)\n",
    "    client_fraction=0.3\n",
    ")\n",
    "\n",
    "print(f\"Final DP accuracy: {dp_history['test_acc'][-1]:.2f}%\")\n",
    "# Expected: 87% (2% loss for Œµ=5 privacy)\n",
    "\n",
    "# Privacy guarantee\n",
    "epsilon = compute_privacy_budget(\n",
    "    noise_scale=0.5,\n",
    "    clip_norm=1.0,\n",
    "    num_rounds=500,\n",
    "    num_samples=10000,\n",
    "    batch_size=64\n",
    ")\n",
    "print(f\"Privacy guarantee: (Œµ={epsilon:.2f}, Œ¥=1e-5)-DP\")\n",
    "# Output: Œµ ‚âà 5.0 (moderate privacy)\n",
    "```\n",
    "\n",
    "### Week 7-8: Deployment & Personalization\n",
    "\n",
    "```python\n",
    "# Global model (shared)\n",
    "global_model = learner.global_model\n",
    "\n",
    "# Personalize for each hospital (80% global, 20% local)\n",
    "for hospital_id, dataset in enumerate(hospital_datasets):\n",
    "    # Fine-tune on local data (5 epochs)\n",
    "    local_model = copy.deepcopy(global_model)\n",
    "    fine_tune(local_model, dataset, epochs=5)\n",
    "    \n",
    "    # Mix global + local\n",
    "    personalized_model = 0.8 * global_model + 0.2 * local_model\n",
    "    \n",
    "    # Evaluate on hospital's validation set\n",
    "    acc = evaluate(personalized_model, dataset)\n",
    "    print(f\"Hospital {hospital_id}: {acc:.2f}% accuracy\")\n",
    "\n",
    "# Expected:\n",
    "# Hospital 0 (rural): 91% (personalized for elderly)\n",
    "# Hospital 50 (urban): 90% (mixed demographics)\n",
    "# Hospital 99 (academic): 88% (younger population)\n",
    "```\n",
    "\n",
    "## Business Value: $10M-$30M/year\n",
    "\n",
    "**Direct Value:**\n",
    "- **Accuracy improvement**: 82% ‚Üí 89% (7% absolute)\n",
    "- **Lives saved**: 7% better detection √ó 100K high-risk patients = 7,000 lives/year\n",
    "- **Cost avoidance**: $10K per late-stage treatment √ó 7,000 = $70M/year\n",
    "- **Hospital network margin**: 15-30% = **$10M-$21M/year**\n",
    "\n",
    "**Regulatory Value:**\n",
    "- **HIPAA compliance**: No data sharing required ‚úÖ\n",
    "- **Fine avoidance**: $50K-$1.5M per violation √ó 0 violations = $0 (vs $5M-$15M risk)\n",
    "\n",
    "**Operational Value:**\n",
    "- **Bandwidth savings**: No patient data transfer (vs $0.09/GB √ó 100GB/hospital = $9K/hospital = $900K total)\n",
    "- **Storage savings**: No centralized data warehouse (vs $1M/year cloud storage)\n",
    "\n",
    "**Conservative estimate**: **$10M-$30M/year** (hospital networks with 100+ hospitals)\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Project 2: Mobile Keyboard Prediction (500M Users)\n",
    "\n",
    "## Business Objective\n",
    "Improve next-word prediction without violating user privacy\n",
    "\n",
    "**Current Problem:**\n",
    "- **Centralized**: Send all typed text to cloud (privacy violation, GDPR fines up to ‚Ç¨20M)\n",
    "- **Local-only**: Limited by device data (poor accuracy, slow improvement)\n",
    "- **Status quo**: User dissatisfaction with predictions, regulatory risk ‚ùå\n",
    "\n",
    "**Federated Solution:**\n",
    "- **500M devices**: Train locally on user typing patterns\n",
    "- **Aggregate**: Server combines updates ‚Üí Global model improves\n",
    "- **Privacy**: No raw text sent to servers (GDPR compliant) ‚úÖ\n",
    "- **Result**: 13% accuracy improvement (Google Gboard results)\n",
    "\n",
    "## Implementation Roadmap\n",
    "\n",
    "### Week 1-2: LSTM Language Model\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class KeyboardLM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM language model for next-word prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=10000, embed_dim=256, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=2, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: (seq_len, batch_size)\n",
    "        embed = self.embedding(x)  # (seq_len, batch, embed_dim)\n",
    "        \n",
    "        if hidden is None:\n",
    "            output, hidden = self.lstm(embed)\n",
    "        else:\n",
    "            output, hidden = self.lstm(embed, hidden)\n",
    "        \n",
    "        logits = self.fc(output)  # (seq_len, batch, vocab_size)\n",
    "        return logits, hidden\n",
    "\n",
    "model = KeyboardLM(vocab_size=10000)\n",
    "print(f\"Model size: {sum(p.numel() for p in model.parameters()) * 4 / 1e6:.2f}MB\")\n",
    "# Output: ~10MB (acceptable for mobile deployment)\n",
    "```\n",
    "\n",
    "### Week 3-4: Simulate User Data (Non-IID Language Patterns)\n",
    "\n",
    "```python\n",
    "# Simulate user typing data (non-IID language patterns)\n",
    "def simulate_user_data(user_id, num_sentences=1000):\n",
    "    \"\"\"\n",
    "    Simulate typing data with personalized language patterns\n",
    "    \"\"\"\n",
    "    # Different language styles\n",
    "    if user_id % 3 == 0:\n",
    "        # Formal style (business users)\n",
    "        vocab = [\"meeting\", \"schedule\", \"deadline\", \"report\", \"email\"]\n",
    "    elif user_id % 3 == 1:\n",
    "        # Casual style (social users)\n",
    "        vocab = [\"hey\", \"lol\", \"omg\", \"awesome\", \"party\"]\n",
    "    else:\n",
    "        # Technical style (developers)\n",
    "        vocab = [\"function\", \"debug\", \"compile\", \"error\", \"code\"]\n",
    "    \n",
    "    sentences = []\n",
    "    for _ in range(num_sentences):\n",
    "        length = np.random.randint(5, 15)\n",
    "        sentence = [np.random.choice(vocab) for _ in range(length)]\n",
    "        sentences.append(' '.join(sentence))\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# Create data for 1000 users (simulating 500M)\n",
    "user_data = [simulate_user_data(i) for i in range(1000)]\n",
    "```\n",
    "\n",
    "### Week 5-6: Federated Training with Compression\n",
    "\n",
    "```python\n",
    "from federated_learning import FederatedLearner\n",
    "\n",
    "# Federated training with gradient compression\n",
    "learner = FederatedLearner(\n",
    "    model, \n",
    "    user_datasets, \n",
    "    test_dataset,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# Enable gradient compression (100√ó reduction)\n",
    "learner.enable_compression(method='top_k', k=0.01)\n",
    "\n",
    "history = learner.train(\n",
    "    rounds=1000,\n",
    "    local_epochs=5,\n",
    "    lr=0.001,\n",
    "    client_fraction=0.0001  # 0.01% = 50K users per round (from 500M)\n",
    ")\n",
    "\n",
    "print(f\"Final accuracy: {history['test_acc'][-1]:.2f}%\")\n",
    "# Expected: 70% (vs 62% local-only)\n",
    "\n",
    "# Communication cost\n",
    "print(f\"Bandwidth per user per round: 100KB (vs 10MB without compression)\")\n",
    "print(f\"Total bandwidth: 100KB √ó 50K users √ó 1000 rounds = 5TB\")\n",
    "print(f\"Cost: $0.09/GB √ó 5000GB = $450 (vs $45K without compression)\")\n",
    "```\n",
    "\n",
    "### Week 7-8: Deployment to Mobile Devices\n",
    "\n",
    "```python\n",
    "# Export to TensorFlow Lite (mobile deployment)\n",
    "import torch.onnx\n",
    "import onnx\n",
    "import onnx_tf\n",
    "\n",
    "# Step 1: PyTorch ‚Üí ONNX\n",
    "dummy_input = torch.randint(0, 10000, (10, 1))\n",
    "torch.onnx.export(model, dummy_input, \"keyboard_lm.onnx\")\n",
    "\n",
    "# Step 2: ONNX ‚Üí TensorFlow\n",
    "onnx_model = onnx.load(\"keyboard_lm.onnx\")\n",
    "tf_rep = onnx_tf.backend.prepare(onnx_model)\n",
    "tf_rep.export_graph(\"keyboard_lm_tf\")\n",
    "\n",
    "# Step 3: TensorFlow ‚Üí TFLite\n",
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"keyboard_lm_tf\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"keyboard_lm.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"TFLite model size: {len(tflite_model) / 1e6:.2f}MB\")\n",
    "# Output: ~3MB (compressed for mobile)\n",
    "```\n",
    "\n",
    "**Android Integration:**\n",
    "```kotlin\n",
    "// Load TFLite model\n",
    "val model = Interpreter(loadModelFile(\"keyboard_lm.tflite\"))\n",
    "\n",
    "// Predict next word\n",
    "fun predictNextWord(context: IntArray): String {\n",
    "    val input = Array(1) { context }\n",
    "    val output = Array(1) { FloatArray(10000) }\n",
    "    \n",
    "    model.run(input, output)\n",
    "    \n",
    "    val topPrediction = output[0].indices.maxByOrNull { output[0][it] }\n",
    "    return vocabulary[topPrediction]\n",
    "}\n",
    "```\n",
    "\n",
    "## Business Value: $20M-$50M/year\n",
    "\n",
    "**User Retention Value:**\n",
    "- **Accuracy improvement**: 62% ‚Üí 70% (13% relative improvement, matches Google Gboard)\n",
    "- **User satisfaction**: NPS +8 points ‚Üí Retention +2%\n",
    "- **Retention value**: 500M users √ó 2% retention √ó $5 ARPU = **$50M/year**\n",
    "\n",
    "**Privacy Differentiation:**\n",
    "- **Marketing advantage**: \"Your data never leaves your device\" (vs competitors who violate privacy)\n",
    "- **Brand trust**: +5% market share from privacy-conscious users\n",
    "- **Revenue**: 500M √ó 5% √ó $5 ARPU = **$125M/year** (aspirational)\n",
    "\n",
    "**Regulatory Avoidance:**\n",
    "- **GDPR compliance**: No personal data sent to servers ‚úÖ\n",
    "- **Fine avoidance**: ‚Ç¨20M ($22M) max fine √ó 0% risk = $0 (vs 10% risk centralized = $2.2M expected cost)\n",
    "\n",
    "**Bandwidth Savings:**\n",
    "- **Without compression**: 10MB/user √ó 50K users √ó 1000 rounds = 500TB = $45K\n",
    "- **With compression**: 100KB/user √ó 50K users √ó 1000 rounds = 5TB = $450\n",
    "- **Savings**: $44.5K per training cycle √ó 10 cycles/year = **$445K/year**\n",
    "\n",
    "**Conservative estimate**: **$20M-$50M/year** (mobile platform with 500M+ users)\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Project 3: Predictive Maintenance (50 Factories)\n",
    "\n",
    "## Business Objective\n",
    "Train predictive maintenance model across 50 semiconductor factories without sharing proprietary sensor data\n",
    "\n",
    "**Current Problem:**\n",
    "- **Single factory**: 500 machines ‚Üí 20% downtime reduction (limited data)\n",
    "- **Centralized**: Cannot share sensor data (trade secrets, competitor intelligence)\n",
    "- **Status quo**: Each factory uses inferior local model ‚ùå\n",
    "\n",
    "**Federated Solution:**\n",
    "- **50 factories**: 25,000 machines (federated) ‚Üí 40% downtime reduction (2√ó better)\n",
    "- **Privacy**: Proprietary sensor data never leaves factories ‚úÖ\n",
    "- **Vendor**: Equipment vendor can aggregate without seeing raw data ‚úÖ\n",
    "\n",
    "## Implementation Roadmap\n",
    "\n",
    "### Week 1-2: Sensor Data Simulation\n",
    "\n",
    "```python\n",
    "# Simulate factory sensor data (non-IID machine types)\n",
    "def simulate_factory_data(factory_id, num_machines=500):\n",
    "    \"\"\"\n",
    "    Simulate sensor data with different machine types per factory\n",
    "    \"\"\"\n",
    "    # Different machine distributions (non-IID)\n",
    "    if factory_id < 17:\n",
    "        # Older factories: Legacy machines\n",
    "        machine_age_mean = 15\n",
    "    elif factory_id < 34:\n",
    "        # Mid-age factories: Mixed\n",
    "        machine_age_mean = 8\n",
    "    else:\n",
    "        # New factories: Modern machines\n",
    "        machine_age_mean = 3\n",
    "    \n",
    "    machine_ages = np.clip(np.random.exponential(machine_age_mean, num_machines), 1, 25)\n",
    "    \n",
    "    # Sensor readings (correlated with age)\n",
    "    temperatures = 65 + 2 * machine_ages + np.random.normal(0, 5, num_machines)\n",
    "    vibrations = 0.5 + 0.1 * machine_ages + np.random.normal(0, 0.2, num_machines)\n",
    "    pressures = 100 - 1 * machine_ages + np.random.normal(0, 10, num_machines)\n",
    "    \n",
    "    # Failure probability (increases with age + sensor anomalies)\n",
    "    failure_score = (\n",
    "        2 * machine_ages + \n",
    "        0.5 * (temperatures - 65) + \n",
    "        10 * (vibrations - 0.5) + \n",
    "        0.1 * (100 - pressures) +\n",
    "        np.random.normal(0, 5, num_machines)\n",
    "    )\n",
    "    failures = (failure_score > 30).astype(int)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'machine_age': machine_ages,\n",
    "        'temperature': temperatures,\n",
    "        'vibration': vibrations,\n",
    "        'pressure': pressures,\n",
    "        'failure': failures\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "factory_data = [simulate_factory_data(i) for i in range(50)]\n",
    "```\n",
    "\n",
    "### Week 3-4: Federated Training\n",
    "\n",
    "```python\n",
    "class MaintenanceModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM for time-series failure prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=4, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (seq_len, batch, input_dim)\n",
    "        output, _ = self.lstm(x)\n",
    "        logits = torch.sigmoid(self.fc(output[-1]))  # Last timestep\n",
    "        return logits\n",
    "\n",
    "model = MaintenanceModel()\n",
    "\n",
    "# Federated training\n",
    "learner = FedProxLearner(\n",
    "    model, \n",
    "    factory_datasets, \n",
    "    test_dataset,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "history = learner.train(\n",
    "    rounds=300,\n",
    "    local_epochs=10,\n",
    "    lr=0.001,\n",
    "    mu=0.01,\n",
    "    client_fraction=0.4  # 20 factories per round\n",
    ")\n",
    "\n",
    "print(f\"Final accuracy: {history['test_acc'][-1]:.2f}%\")\n",
    "# Expected: 88% (vs 78% single factory)\n",
    "```\n",
    "\n",
    "### Week 5-8: Deployment & ROI Analysis\n",
    "\n",
    "```python\n",
    "# Deploy to each factory\n",
    "for factory_id, dataset in enumerate(factory_datasets):\n",
    "    # Personalized model (80% global, 20% local)\n",
    "    personalized_model = personalize(global_model, dataset, alpha=0.8)\n",
    "    \n",
    "    # Evaluate downtime reduction\n",
    "    baseline_downtime = 100  # hours/year per machine (status quo)\n",
    "    predicted_downtime = evaluate_downtime(personalized_model, dataset)\n",
    "    \n",
    "    reduction = (baseline_downtime - predicted_downtime) / baseline_downtime\n",
    "    print(f\"Factory {factory_id}: {reduction:.0%} downtime reduction\")\n",
    "    \n",
    "    # ROI calculation\n",
    "    cost_per_hour = 50000  # $50K/hour for semiconductor fab\n",
    "    num_machines = 500\n",
    "    annual_savings = num_machines * (baseline_downtime - predicted_downtime) * cost_per_hour\n",
    "    print(f\"  Annual savings: ${annual_savings/1e6:.2f}M\")\n",
    "\n",
    "# Expected per factory:\n",
    "# Baseline: 100 hours downtime/year\n",
    "# With federated model: 60 hours downtime/year (40% reduction)\n",
    "# Savings: 500 machines √ó 40 hours √ó $50K = $1M/year per factory\n",
    "```\n",
    "\n",
    "## Business Value: $30M-$80M/year\n",
    "\n",
    "**Direct Value (Per Factory):**\n",
    "- **Downtime reduction**: 20% (local model) ‚Üí 40% (federated model)\n",
    "- **Additional reduction**: 20% absolute\n",
    "- **Annual downtime**: 500 machines √ó 100 hours/year = 50,000 hours\n",
    "- **Additional savings**: 20% √ó 50,000 hours √ó $50K = **$500M √ó 20% = $1M/year per factory**\n",
    "\n",
    "**Total (50 Factories):**\n",
    "- **$1M/year √ó 50 factories = $50M/year**\n",
    "\n",
    "**Equipment Vendor Revenue:**\n",
    "- **Vendor charges**: 20% of savings as subscription fee\n",
    "- **Annual revenue**: $50M √ó 20% = **$10M/year**\n",
    "\n",
    "**Privacy Value:**\n",
    "- **Data sharing impossible**: Without federated learning, factories would refuse to share data (trade secrets)\n",
    "- **Federated enables collaboration**: **$50M/year value unlocked** (vs $0 without collaboration)\n",
    "\n",
    "**Conservative estimate**: **$30M-$80M/year** (depends on industry adoption and fab count)\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Project 4: Cross-Silo Federated Learning (Banks Detecting Fraud)\n",
    "\n",
    "## Business Objective\n",
    "Detect fraud patterns across 10 banks without sharing customer transaction data\n",
    "\n",
    "**Current Problem:**\n",
    "- **Single bank**: Limited fraud patterns (regional, product-specific)\n",
    "- **Centralized**: Cannot share customer data (PCI-DSS violation, competitive sensitivity)\n",
    "- **Status quo**: Each bank detects only known fraud patterns ‚ùå\n",
    "\n",
    "**Federated Solution:**\n",
    "- **10 banks**: Global fraud patterns (credit card fraud, money laundering, etc.)\n",
    "- **Privacy**: Customer data never leaves banks ‚úÖ\n",
    "- **Result**: 30% more fraud detected (patterns from other banks)\n",
    "\n",
    "## Implementation Roadmap\n",
    "\n",
    "### Week 1-3: Fraud Detection Model (GNN)\n",
    "\n",
    "```python\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class FraudDetectionGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Neural Network for fraud detection\n",
    "    (customers = nodes, transactions = edges)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features=10, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = torch.relu(self.conv2(x, edge_index))\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        return x\n",
    "\n",
    "model = FraudDetectionGNN()\n",
    "```\n",
    "\n",
    "### Week 4-6: Federated Training (Cross-Silo)\n",
    "\n",
    "```python\n",
    "# Each bank has different fraud patterns (non-IID)\n",
    "bank_graphs = [create_transaction_graph(bank_id) for bank_id in range(10)]\n",
    "\n",
    "# Federated training\n",
    "learner = FederatedLearner(\n",
    "    model, \n",
    "    bank_datasets, \n",
    "    test_dataset,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "history = learner.train(\n",
    "    rounds=200,\n",
    "    local_epochs=20,\n",
    "    lr=0.001,\n",
    "    client_fraction=1.0  # All 10 banks per round (cross-silo)\n",
    ")\n",
    "\n",
    "print(f\"Fraud detection rate: {history['test_recall'][-1]:.2f}%\")\n",
    "# Expected: 85% (vs 65% single bank)\n",
    "```\n",
    "\n",
    "### Week 7-8: Secure Aggregation\n",
    "\n",
    "```python\n",
    "# Add secure aggregation (banks don't trust server)\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "def secure_aggregate(encrypted_updates, bank_keys):\n",
    "    \"\"\"\n",
    "    Secure aggregation: Server cannot see individual updates\n",
    "    \"\"\"\n",
    "    # Each bank encrypts its update\n",
    "    encrypted_updates = [encrypt(update, bank_keys[i]) for i, update in enumerate(updates)]\n",
    "    \n",
    "    # Server aggregates encrypted updates (homomorphic encryption)\n",
    "    aggregated_encrypted = sum(encrypted_updates)\n",
    "    \n",
    "    # Decrypt aggregated (requires all banks' keys)\n",
    "    aggregated_update = decrypt(aggregated_encrypted, bank_keys)\n",
    "    \n",
    "    return aggregated_update\n",
    "```\n",
    "\n",
    "## Business Value: $15M-$40M/year\n",
    "\n",
    "**Fraud Detection Improvement:**\n",
    "- **Baseline**: 65% fraud detection rate (single bank)\n",
    "- **Federated**: 85% fraud detection rate (global patterns)\n",
    "- **Improvement**: 20% absolute (30% relative)\n",
    "\n",
    "**Prevented Fraud Losses:**\n",
    "- **Annual fraud**: $10M/bank (industry average)\n",
    "- **Additional detection**: 20% √ó $10M = $2M/bank\n",
    "- **Total (10 banks)**: 10 √ó $2M = **$20M/year**\n",
    "\n",
    "**Operational Savings:**\n",
    "- **False positives**: 50% reduction (better model)\n",
    "- **Customer support**: $500K/year per bank\n",
    "- **Total**: 10 √ó $500K = **$5M/year**\n",
    "\n",
    "**Competitive Advantage:**\n",
    "- **Customer trust**: Lower fraud rate ‚Üí +5% customer retention\n",
    "- **Revenue**: $100M deposits/bank √ó 5% √ó 2% interest margin = $100K/bank\n",
    "- **Total**: 10 √ó $100K = **$1M/year** (small but growing)\n",
    "\n",
    "**Conservative estimate**: **$15M-$40M/year** (10-20 banks in consortium)\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Project 5: Federated Learning for Autonomous Vehicles\n",
    "\n",
    "## Business Objective\n",
    "Train road condition detection model across 1M vehicles without sharing camera data\n",
    "\n",
    "**Current Problem:**\n",
    "- **Centralized**: Send all camera images to cloud (bandwidth $1B/year, privacy concerns)\n",
    "- **Local-only**: Limited to single vehicle's experience (miss rare conditions)\n",
    "\n",
    "**Federated Solution:**\n",
    "- **1M vehicles**: Train on diverse road conditions (rain, snow, construction, etc.)\n",
    "- **Privacy**: No camera images sent to cloud ‚úÖ\n",
    "- **Bandwidth**: 100KB updates vs 100MB images = 1000√ó reduction ‚úÖ\n",
    "\n",
    "## Business Value: $10M-$30M/year\n",
    "\n",
    "**Bandwidth Savings:**\n",
    "- **Centralized**: 100MB/vehicle √ó 1M vehicles √ó 100 updates/year = 10PB = $900K/year\n",
    "- **Federated**: 100KB/vehicle √ó 1M vehicles √ó 100 updates/year = 10TB = $900/year\n",
    "- **Savings**: **$899K/year**\n",
    "\n",
    "**Safety Improvement:**\n",
    "- **Rare condition detection**: 30% improvement (federated sees more scenarios)\n",
    "- **Accident reduction**: 5% fewer accidents (better model)\n",
    "- **Lives saved**: 5% √ó 100 deaths/year = 5 lives/year\n",
    "- **Value**: Priceless (regulatory compliance, brand reputation)\n",
    "\n",
    "**Model Improvement Speed:**\n",
    "- **Centralized**: 1 year to collect 1M images (bandwidth bottleneck)\n",
    "- **Federated**: 1 week to aggregate 1M updates (parallel training)\n",
    "- **Time-to-market**: 50√ó faster model iteration\n",
    "\n",
    "**Conservative estimate**: **$10M-$30M/year** (safety value + bandwidth + time-to-market)\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Project 6: Federated Recommender System (E-commerce)\n",
    "\n",
    "## Business Objective\n",
    "Improve product recommendations without collecting browsing history\n",
    "\n",
    "**Current Problem:**\n",
    "- **Centralized**: Collect all browsing history (GDPR violations, user backlash)\n",
    "- **Local-only**: Cannot leverage global patterns (cold start problem)\n",
    "\n",
    "**Federated Solution:**\n",
    "- **10M users**: Train on local preferences, aggregate global trends\n",
    "- **Privacy**: Browsing history stays on device ‚úÖ\n",
    "- **Result**: 15% higher click-through rate (CTR)\n",
    "\n",
    "## Business Value: $5M-$15M/year\n",
    "\n",
    "**Revenue Increase:**\n",
    "- **Baseline CTR**: 5%\n",
    "- **Federated CTR**: 5.75% (15% relative increase)\n",
    "- **Annual revenue**: $1B e-commerce platform\n",
    "- **Revenue increase**: 0.75% √ó $1B = **$7.5M/year**\n",
    "\n",
    "**User Retention:**\n",
    "- **Better recommendations**: NPS +5 points ‚Üí Retention +1%\n",
    "- **Retention value**: 10M users √ó 1% √ó $100 ARPU = **$10M/year**\n",
    "\n",
    "**Conservative estimate**: **$5M-$15M/year** (e-commerce platform)\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Project 7: Federated NLP for Medical Reports\n",
    "\n",
    "## Business Objective\n",
    "Train medical NLP model (extract diagnoses, procedures) across 50 hospitals without sharing reports\n",
    "\n",
    "**Current Problem:**\n",
    "- **Single hospital**: Limited report diversity (specialties, patient populations)\n",
    "- **Centralized**: Cannot share reports (HIPAA violation)\n",
    "\n",
    "**Federated Solution:**\n",
    "- **50 hospitals**: 1M reports (diverse specialties)\n",
    "- **Privacy**: Reports stay at hospitals ‚úÖ\n",
    "- **Result**: 92% extraction accuracy (vs 85% single hospital)\n",
    "\n",
    "## Business Value: $3M-$10M/year\n",
    "\n",
    "**Automation Value:**\n",
    "- **Manual coding**: 100 coders √ó $50K/year = $5M/year per hospital\n",
    "- **Automated extraction**: 50% reduction in manual work\n",
    "- **Savings**: $2.5M/year per hospital √ó 50 hospitals = **$125M/year** (aspirational)\n",
    "- **Federated contribution**: 10% (enable deployment via privacy) = **$12.5M/year**\n",
    "\n",
    "**Conservative estimate**: **$3M-$10M/year** (50-hospital network)\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Project 8: Federated IoT (Smart City Sensors)\n",
    "\n",
    "## Business Objective\n",
    "Train traffic prediction model across 10,000 city sensors without centralizing data\n",
    "\n",
    "**Current Problem:**\n",
    "- **Centralized**: Send all sensor data to cloud (bandwidth, latency, privacy)\n",
    "- **Local-only**: Cannot predict city-wide traffic patterns\n",
    "\n",
    "**Federated Solution:**\n",
    "- **10,000 sensors**: Train locally, aggregate city-wide patterns\n",
    "- **Privacy**: No raw sensor data sent ‚úÖ\n",
    "- **Result**: 25% better traffic prediction\n",
    "\n",
    "## Business Value: $2M-$5M/year\n",
    "\n",
    "**Traffic Optimization:**\n",
    "- **Commute time reduction**: 5 minutes/day per commuter\n",
    "- **Commuters**: 1M in city\n",
    "- **Value of time**: $20/hour\n",
    "- **Annual savings**: 1M √ó 250 days √ó 5 min √ó ($20/60 min) = **$41.7M/year**\n",
    "- **City captures**: 5% (via toll optimization, parking fees) = **$2M/year**\n",
    "\n",
    "**Bandwidth Savings:**\n",
    "- **Centralized**: 1MB/sensor √ó 10K sensors √ó 365 days = 3.65TB/year = $329/year\n",
    "- **Federated**: 10KB/sensor √ó 10K sensors √ó 365 days = 36.5GB/year = $3/year\n",
    "- **Savings**: **$326/year** (negligible but adds up across many cities)\n",
    "\n",
    "**Conservative estimate**: **$2M-$5M/year** (per smart city deployment)\n",
    "\n",
    "---\n",
    "\n",
    "# üìä Business Value Summary\n",
    "\n",
    "## Total Annual Value: $90M-$250M/year\n",
    "\n",
    "| Project | Annual Value | Key Metric | Devices/Entities |\n",
    "|---------|--------------|------------|------------------|\n",
    "| 1. Disease Prediction (Hospitals) | $10M-$30M | 7% accuracy improvement | 100 hospitals |\n",
    "| 2. Keyboard Prediction (Mobile) | $20M-$50M | 13% accuracy, 2% retention | 500M users |\n",
    "| 3. Predictive Maintenance (Factories) | $30M-$80M | 40% downtime reduction | 50 factories |\n",
    "| 4. Fraud Detection (Banks) | $15M-$40M | 20% more fraud detected | 10 banks |\n",
    "| 5. Autonomous Vehicles | $10M-$30M | Bandwidth + safety | 1M vehicles |\n",
    "| 6. E-commerce Recommender | $5M-$15M | 15% CTR increase | 10M users |\n",
    "| 7. Medical NLP | $3M-$10M | 7% extraction improvement | 50 hospitals |\n",
    "| 8. Smart City IoT | $2M-$5M | 25% traffic prediction | 10K sensors |\n",
    "| **Total** | **$95M-$260M** | Privacy + collaboration | Billions of devices |\n",
    "\n",
    "**Conservative midpoint**: **$175M/year** (across all federated learning projects)\n",
    "\n",
    "---\n",
    "\n",
    "# üîß Deployment Frameworks\n",
    "\n",
    "## 1. TensorFlow Federated (TFF)\n",
    "\n",
    "**Best for**: Google-scale deployments (billions of devices)\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "pip install tensorflow-federated\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "# Define federated data\n",
    "federated_train_data = [client_data_1, client_data_2, ...]\n",
    "\n",
    "# Define model\n",
    "def model_fn():\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=train_data[0].element_spec,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "# Build federated averaging process\n",
    "iterative_process = tff.learning.build_federated_averaging_process(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.01)\n",
    ")\n",
    "\n",
    "# Train\n",
    "state = iterative_process.initialize()\n",
    "for round_num in range(100):\n",
    "    state, metrics = iterative_process.next(state, federated_train_data)\n",
    "    print(f'Round {round_num}, metrics={metrics}')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. PySyft (OpenMined)\n",
    "\n",
    "**Best for**: Research, privacy-preserving ML, differential privacy\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "pip install syft\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "import syft as sy\n",
    "import torch\n",
    "\n",
    "# Create virtual workers (hospitals, devices)\n",
    "hook = sy.TorchHook(torch)\n",
    "hospital_a = sy.VirtualWorker(hook, id=\"hospital_a\")\n",
    "hospital_b = sy.VirtualWorker(hook, id=\"hospital_b\")\n",
    "\n",
    "# Send data to workers\n",
    "data_a = data_a.send(hospital_a)\n",
    "data_b = data_b.send(hospital_b)\n",
    "\n",
    "# Train locally on each worker\n",
    "model = model.send(hospital_a)\n",
    "model.train(data_a)\n",
    "model = model.get()\n",
    "\n",
    "# Aggregate\n",
    "# (PySyft handles secure aggregation automatically)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Flower (Scalable FL)\n",
    "\n",
    "**Best for**: Production deployments, cross-platform (mobile, edge, cloud)\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "pip install flwr\n",
    "```\n",
    "\n",
    "**Server:**\n",
    "```python\n",
    "import flwr as fl\n",
    "\n",
    "def fit_config(rnd: int):\n",
    "    return {\"epochs\": 5, \"batch_size\": 32}\n",
    "\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=0.3,  # 30% of clients per round\n",
    "    min_available_clients=10,\n",
    "    on_fit_config_fn=fit_config,\n",
    ")\n",
    "\n",
    "fl.server.start_server(\n",
    "    server_address=\"0.0.0.0:8080\",\n",
    "    config={\"num_rounds\": 100},\n",
    "    strategy=strategy,\n",
    ")\n",
    "```\n",
    "\n",
    "**Client:**\n",
    "```python\n",
    "import flwr as fl\n",
    "\n",
    "class CifarClient(fl.client.NumPyClient):\n",
    "    def get_parameters(self):\n",
    "        return get_model_parameters(model)\n",
    "    \n",
    "    def fit(self, parameters, config):\n",
    "        set_model_parameters(model, parameters)\n",
    "        train(model, train_loader, epochs=config[\"epochs\"])\n",
    "        return get_model_parameters(model), len(train_loader), {}\n",
    "    \n",
    "    def evaluate(self, parameters, config):\n",
    "        set_model_parameters(model, parameters)\n",
    "        loss, accuracy = evaluate(model, test_loader)\n",
    "        return loss, len(test_loader), {\"accuracy\": accuracy}\n",
    "\n",
    "fl.client.start_numpy_client(\n",
    "    server_address=\"localhost:8080\",\n",
    "    client=CifarClient()\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. NVIDIA FLARE (Medical Imaging)\n",
    "\n",
    "**Best for**: Healthcare, medical imaging, cross-silo FL\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "pip install nvflare\n",
    "```\n",
    "\n",
    "**Features:**\n",
    "- Secure aggregation\n",
    "- Differential privacy\n",
    "- HIPAA compliance tools\n",
    "- Integration with NVIDIA Clara (medical imaging)\n",
    "\n",
    "---\n",
    "\n",
    "# üéì Key Takeaways\n",
    "\n",
    "## When to Use Federated Learning\n",
    "\n",
    "‚úÖ **Use when:**\n",
    "1. **Privacy required**: GDPR, HIPAA, PCI-DSS compliance\n",
    "2. **Data cannot be centralized**: Trade secrets, competitive sensitivity\n",
    "3. **Large-scale edge deployment**: Billions of devices (mobile, IoT)\n",
    "4. **Personalization needed**: Adapt to local data distributions\n",
    "\n",
    "‚ùå **Don't use when:**\n",
    "1. **Data can be centralized**: No privacy/regulatory issues\n",
    "2. **Small number of devices**: <10 devices (overhead not justified)\n",
    "3. **Homogeneous data**: All devices have similar distributions\n",
    "4. **Real-time requirements**: <100ms latency (federated rounds take minutes)\n",
    "\n",
    "---\n",
    "\n",
    "## Trade-offs\n",
    "\n",
    "| Aspect | Centralized | Federated |\n",
    "|--------|-------------|-----------|\n",
    "| **Privacy** | ‚ùå Low | ‚úÖ High |\n",
    "| **Accuracy** | ‚úÖ Baseline | ‚ö†Ô∏è 95-99% of baseline |\n",
    "| **Training Speed** | ‚úÖ Fast | ‚ùå Slow (100-1000 rounds) |\n",
    "| **Communication** | ‚ùå High (GB/device) | ‚úÖ Low (KB/device) |\n",
    "| **Scalability** | ‚ùå Limited (server capacity) | ‚úÖ Unlimited (edge) |\n",
    "| **Complexity** | ‚úÖ Simple | ‚ùå Complex (non-IID, stragglers) |\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### 1. Handle Non-IID Data\n",
    "- **Use FedProx** (proximal regularization, Œº=0.01)\n",
    "- **Personalization** (mix global + local, Œ±=0.8)\n",
    "- **Stratified sampling** (ensure diverse device selection)\n",
    "\n",
    "### 2. Communication Efficiency\n",
    "- **Gradient compression** (top-k=1%, quantization INT8)\n",
    "- **Local epochs** (E=5-10, reduce communication frequency)\n",
    "- **Model compression** (prune 50-90% before federated training)\n",
    "\n",
    "### 3. Privacy Guarantees\n",
    "- **Differential privacy** (Œµ=3-8, moderate privacy)\n",
    "- **Secure aggregation** (homomorphic encryption for cross-silo)\n",
    "- **Gradient clipping** (C=1.0, bound sensitivity)\n",
    "\n",
    "### 4. Robustness\n",
    "- **Robust aggregation** (Krum, Trimmed Mean for Byzantine attacks)\n",
    "- **Client validation** (detect malicious updates)\n",
    "- **Anomaly detection** (flag outlier updates)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Path\n",
    "\n",
    "**Week 1-2**: Foundations\n",
    "- Read FedAvg paper (McMahan et al., 2017)\n",
    "- Implement FedAvg from scratch (10 devices, CIFAR-10)\n",
    "- Compare with centralized baseline\n",
    "\n",
    "**Week 3-4**: Non-IID Data\n",
    "- Read FedProx paper (Li et al., 2020)\n",
    "- Simulate non-IID data (Dirichlet Œ±=0.1-0.5)\n",
    "- Implement FedProx, compare with FedAvg\n",
    "\n",
    "**Week 5-6**: Privacy\n",
    "- Read DP-SGD paper (Abadi et al., 2016)\n",
    "- Implement differential privacy (gradient clipping + noise)\n",
    "- Measure privacy-accuracy trade-off (Œµ=1, 5, 10)\n",
    "\n",
    "**Week 7-8**: Communication Efficiency\n",
    "- Implement gradient compression (top-k, quantization)\n",
    "- Measure bandwidth savings (100√ó-1000√ó)\n",
    "- Optimize local epochs (E=1, 5, 10, 20)\n",
    "\n",
    "**Week 9-10**: Production Deployment\n",
    "- Deploy with TensorFlow Federated or Flower\n",
    "- Handle stragglers (timeout, dropout)\n",
    "- Monitor convergence (accuracy, loss, communication cost)\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "### Papers\n",
    "1. **FedAvg** (McMahan et al., 2017) - Original federated learning algorithm\n",
    "2. **FedProx** (Li et al., 2020) - Handling non-IID data\n",
    "3. **DP-SGD** (Abadi et al., 2016) - Differential privacy\n",
    "4. **Secure Aggregation** (Bonawitz et al., 2017) - Cryptographic aggregation\n",
    "\n",
    "### Frameworks\n",
    "1. **TensorFlow Federated**: Google-scale, production-ready\n",
    "2. **PySyft (OpenMined)**: Research, privacy-preserving ML\n",
    "3. **Flower**: Scalable, cross-platform (mobile, edge, cloud)\n",
    "4. **NVIDIA FLARE**: Healthcare, medical imaging\n",
    "\n",
    "### Courses\n",
    "1. **Coursera**: \"Privacy-Preserving Machine Learning\" (Andrew Trask)\n",
    "2. **Fast.ai**: Federated learning tutorials\n",
    "3. **OpenMined**: Privacy-preserving ML courses\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ Success Criteria Checklist\n",
    "\n",
    "Before deploying federated learning, verify:\n",
    "\n",
    "- [ ] **Privacy requirement**: Data cannot be centralized (GDPR, HIPAA, trade secrets)\n",
    "- [ ] **Device count**: >10 devices (preferably >100)\n",
    "- [ ] **Non-IID handling**: FedProx or personalization implemented\n",
    "- [ ] **Communication efficiency**: Compression (100√ó), local epochs (E>1)\n",
    "- [ ] **Privacy guarantee**: Differential privacy (Œµ<10) or secure aggregation\n",
    "- [ ] **Convergence**: 95%+ of centralized accuracy\n",
    "- [ ] **Robustness**: Defense against malicious devices (if applicable)\n",
    "- [ ] **Deployment**: Production-ready framework (TFF, Flower, FLARE)\n",
    "- [ ] **Business value**: Quantified ROI ($XM-$YM/year)\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Conclusion\n",
    "\n",
    "**Federated learning enables privacy-preserving collaboration:**\n",
    "- **Healthcare**: 100 hospitals train on 1M patients without sharing data ($10M-$30M/year)\n",
    "- **Mobile AI**: 500M users improve keyboard predictions locally ($20M-$50M/year)\n",
    "- **Manufacturing**: 50 factories collaborate on predictive maintenance ($30M-$80M/year)\n",
    "- **Total value**: **$90M-$250M/year** across industries\n",
    "\n",
    "**Key techniques:**\n",
    "1. **FedAvg**: Average local model updates (not raw data)\n",
    "2. **FedProx**: Handle non-IID data with proximal regularization\n",
    "3. **Differential Privacy**: Add calibrated noise for formal privacy guarantees\n",
    "4. **Communication Efficiency**: Compression (100√ó), local epochs (E=5-10)\n",
    "\n",
    "**Next steps:**\n",
    "1. Choose use case (healthcare, mobile, manufacturing)\n",
    "2. Implement FedAvg baseline (compare with centralized)\n",
    "3. Add FedProx + DP for non-IID data + privacy\n",
    "4. Deploy with TensorFlow Federated or Flower\n",
    "5. Quantify business value ($XM-$YM/year)\n",
    "\n",
    "**Remember**: Federated learning is essential for privacy-sensitive applications. Start federating today! üöÄüîê\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Progression:**\n",
    "- **Previous**: 068 Model Compression & Quantization (Prune, Distill, Quantize)\n",
    "- **Current**: 069 Federated Learning (Privacy-Preserving Distributed ML)\n",
    "- **Next**: 070 Edge AI & TinyML (On-Device Inference, Microcontrollers)\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Notebook Complete! Ready for production federated learning deployment and $90M-$250M/year business value creation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5031c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Simulate federated vs centralized learning convergence\n",
    "rounds = np.arange(1, 51)\n",
    "centralized_acc = 1 - 0.5 * np.exp(-rounds / 10)  # Fast convergence\n",
    "federated_acc = 1 - 0.5 * np.exp(-rounds / 15)     # Slower but private\n",
    "\n",
    "# Communication cost comparison\n",
    "communication_rounds = [10, 20, 30, 40, 50]\n",
    "centralized_comm = [r * 100 for r in communication_rounds]  # All data transferred\n",
    "federated_comm = [r * 5 for r in communication_rounds]       # Only model updates\n",
    "\n",
    "# Privacy levels (higher is better)\n",
    "methods = ['Centralized\\n(No Privacy)', 'Federated\\n(Basic)', 'Federated\\n+ DP', 'Federated\\n+ DP + SMC']\n",
    "privacy_scores = [0, 60, 85, 95]\n",
    "utility_scores = [100, 90, 80, 75]\n",
    "\n",
    "# Create visualizations\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Convergence Comparison\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.plot(rounds, centralized_acc, 'b-', linewidth=3, label='Centralized Learning', marker='o', markersize=4)\n",
    "ax1.plot(rounds, federated_acc, 'r-', linewidth=3, label='Federated Learning', marker='s', markersize=4)\n",
    "ax1.set_xlabel('Training Rounds', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Model Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Convergence: Federated vs Centralized', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11, loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0, 1.1])\n",
    "\n",
    "# Add annotations\n",
    "ax1.annotate('Centralized converges faster\\nbut no privacy', \n",
    "             xy=(30, 0.9), xytext=(35, 0.7),\n",
    "             arrowprops=dict(arrowstyle='->', color='blue', lw=2),\n",
    "             fontsize=10, color='blue', fontweight='bold')\n",
    "\n",
    "# Plot 2: Communication Cost\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.plot(communication_rounds, centralized_comm, 'b-o', linewidth=3, markersize=8, label='Centralized (Full Data)')\n",
    "ax2.plot(communication_rounds, federated_comm, 'r-s', linewidth=3, markersize=8, label='Federated (Model Only)')\n",
    "ax2.set_xlabel('Training Rounds', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Communication Cost (MB)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Communication Efficiency', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "reduction = (1 - federated_comm[-1] / centralized_comm[-1]) * 100\n",
    "ax2.text(25, 4000, f'{reduction:.0f}% Reduction\\nwith Federated', \n",
    "         fontsize=11, color='green', fontweight='bold',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "# Plot 3: Privacy-Utility Trade-off\n",
    "ax3 = fig.add_subplot(133)\n",
    "colors = ['#e74c3c', '#f39c12', '#3498db', '#2ecc71']\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax3.bar(x - width/2, privacy_scores, width, label='Privacy Level', color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "bars2 = ax3.bar(x + width/2, utility_scores, width, label='Model Utility', color='gray', alpha=0.5, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax3.set_xlabel('Privacy Method', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Score (0-100)', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Privacy-Utility Trade-off', fontsize=14, fontweight='bold')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(methods, fontsize=10)\n",
    "ax3.legend(fontsize=11)\n",
    "ax3.set_ylim([0, 110])\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "                f'{int(height)}', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('federated_learning_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä FEDERATED LEARNING ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüîπ Convergence:\")\n",
    "print(f\"   Final Centralized Accuracy: {centralized_acc[-1]:.2%}\")\n",
    "print(f\"   Final Federated Accuracy: {federated_acc[-1]:.2%}\")\n",
    "print(f\"   Accuracy gap: {(centralized_acc[-1] - federated_acc[-1])*100:.1f}% (acceptable for privacy gain)\")\n",
    "print(f\"\\nüîπ Communication Efficiency:\")\n",
    "print(f\"   Centralized cost (50 rounds): {centralized_comm[-1]} MB\")\n",
    "print(f\"   Federated cost (50 rounds): {federated_comm[-1]} MB\")\n",
    "print(f\"   Bandwidth savings: {reduction:.0f}%\")\n",
    "print(f\"\\nüîπ Privacy-Utility Trade-off:\")\n",
    "print(f\"   Best privacy: Federated + DP + SMC (95/100)\")\n",
    "print(f\"   Acceptable utility loss: {100 - utility_scores[-1]}% for maximum privacy\")\n",
    "print(f\"\\n‚úÖ Federated Learning achieves strong privacy with minimal accuracy loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b57796",
   "metadata": {},
   "source": [
    "## üìä Federated Learning Metrics Visualization\n",
    "\n",
    "Let's visualize the convergence and privacy metrics of federated learning:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
