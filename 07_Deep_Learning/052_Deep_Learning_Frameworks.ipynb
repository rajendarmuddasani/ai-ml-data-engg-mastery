{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41257dd9",
   "metadata": {},
   "source": [
    "# 052: Deep Learning Frameworks (PyTorch & TensorFlow)",
    "",
    "## **From NumPy to Production: Modern Deep Learning Tools**",
    "",
    "---",
    "",
    "### **\ud83d\udcd6 What You'll Learn**",
    "",
    "By the end of this notebook, you will master:",
    "",
    "1. **PyTorch fundamentals:** Tensors, autograd, nn.Module, optimizers, device management",
    "2. **TensorFlow/Keras fundamentals:** Keras API, Sequential vs Functional, custom layers, callbacks",
    "3. **Framework comparison:** When to use PyTorch vs TensorFlow, trade-offs, ecosystem",
    "4. **Production-ready implementations:** Build the same neural network in both frameworks",
    "5. **Semiconductor applications:** Wafer yield predictor, defect classifier with modern frameworks",
    "6. **GPU acceleration:** Device management, mixed precision training, performance optimization",
    "7. **Model persistence:** Save/load models, checkpointing, transfer learning",
    "8. **Debugging & profiling:** TensorBoard, visualization, bottleneck identification",
    "",
    "---",
    "",
    "### **\ud83c\udfaf Why Deep Learning Frameworks Matter**",
    "",
    "In **Notebook 051**, we implemented neural networks from scratch using NumPy. This taught us:",
    "- \u2705 Mathematical foundations (backpropagation, gradients)",
    "- \u2705 How neural networks actually work under the hood",
    "- \u2705 Debugging skills (gradient checking, initialization)",
    "",
    "**But for production AI/ML**, writing everything from scratch is:",
    "- \u274c Time-consuming (100s of lines \u2192 10s of lines)",
    "- \u274c Error-prone (manual gradient computation)",
    "- \u274c Not optimized (no GPU acceleration, no advanced optimizers)",
    "- \u274c Hard to maintain (custom codebases)",
    "- \u274c Limited features (no pre-trained models, no deployment tools)",
    "",
    "**Deep learning frameworks solve this:**",
    "- \u2705 **Automatic differentiation:** Backpropagation computed automatically (no manual gradients)",
    "- \u2705 **GPU acceleration:** 10-100\u00d7 speedup with CUDA (PyTorch) or XLA (TensorFlow)",
    "- \u2705 **Production-ready:** Model serving (TorchServe, TensorFlow Serving), optimization (ONNX)",
    "- \u2705 **Rich ecosystem:** Pre-trained models (torchvision, tf.keras.applications), visualization (TensorBoard)",
    "- \u2705 **Community support:** 100K+ GitHub stars, extensive documentation, tutorials",
    "",
    "**Analogy:** NumPy implementation = building a car from parts (educational).  ",
    "Framework = driving a Tesla (production-ready, optimized, feature-rich).",
    "",
    "---",
    "",
    "### **\ud83c\udfe2 Industry Usage (2024-2025)**",
    "",
    "**PyTorch dominates research & many production systems:**",
    "- **Market share:** ~55-60% (research), 40-45% (production)",
    "- **Users:** Meta, Tesla, OpenAI (GPT models), Microsoft, Qualcomm, AMD",
    "- **Strengths:** Pythonic API, dynamic computation graphs, debugging ease, research flexibility",
    "- **Use cases:** LLMs (GPT, Llama), computer vision (YOLO, SAM), research prototyping",
    "",
    "**TensorFlow/Keras strong in enterprise production:**",
    "- **Market share:** ~40-45% (research), 50-55% (production)",
    "- **Users:** Google, NVIDIA, Intel, Samsung, many Fortune 500 companies",
    "- **Strengths:** Production ecosystem (TFX, TF Lite, TF Serving), deployment tools, stability",
    "- **Use cases:** Google products (Search, Ads, Photos), mobile (TF Lite), edge devices",
    "",
    "**Trend:** PyTorch gaining ground in production (PyTorch 2.0+ improvements), but TensorFlow still leads enterprise deployment.",
    "",
    "---",
    "",
    "### **\ud83d\udd27 Semiconductor Post-Silicon Validation Use Cases**",
    "",
    "#### **Use Case 1: Wafer Yield Prediction (PyTorch)**",
    "**Problem:** Predict die-level yield from 50+ parametric tests in real-time during wafer test.",
    "",
    "**Why frameworks:** ",
    "- 50K+ die/hour throughput \u2192 Need GPU acceleration (100\u00d7 faster than NumPy)",
    "- Model deployment \u2192 TorchServe or ONNX Runtime for production inference",
    "- Complex architectures \u2192 Multi-layer networks with batch normalization, dropout",
    "",
    "**Business value:** $50M-$200M/year scrap reduction through early failure detection.",
    "",
    "---",
    "",
    "#### **Use Case 2: Defect Pattern Classification (TensorFlow/Keras)**",
    "**Problem:** Classify 20+ defect types from wafer maps (spatial images) with 98%+ accuracy.",
    "",
    "**Why frameworks:**",
    "- Convolutional neural networks (CNNs) \u2192 Pre-built layers (Conv2D, MaxPool) in frameworks",
    "- Transfer learning \u2192 Use pre-trained ImageNet models (ResNet, EfficientNet)",
    "- Production deployment \u2192 TensorFlow Serving for real-time inference, TF Lite for edge devices",
    "",
    "**Business value:** $5M-$20M per incident through faster root cause analysis.",
    "",
    "---",
    "",
    "#### **Use Case 3: Adaptive Test Insertion (PyTorch)**",
    "**Problem:** Dynamically select optimal test sequence to minimize test time while maintaining 99%+ coverage.",
    "",
    "**Why frameworks:**",
    "- Reinforcement learning \u2192 Policy networks with PyTorch (flexible for RL research)",
    "- GPU training \u2192 10\u00d7 faster iteration for policy optimization",
    "- Real-time inference \u2192 <10ms decision time using TorchScript or ONNX",
    "",
    "**Business value:** $10M-$50M/year test time reduction (30-50% savings).",
    "",
    "---",
    "",
    "### **\ud83d\udcca Framework Comparison at a Glance**",
    "",
    "| Feature | PyTorch | TensorFlow/Keras |",
    "|---------|---------|------------------|",
    "| **API Style** | Pythonic, imperative | Keras (high-level), TF (low-level) |",
    "| **Learning Curve** | Moderate (intuitive) | Easy (Keras), Hard (TF 1.x) |",
    "| **Computation Graph** | Dynamic (define-by-run) | Static + Eager (TF 2.x) |",
    "| **Debugging** | Easy (standard Python debugger) | Moderate (better in TF 2.x) |",
    "| **Production** | Good (TorchServe, ONNX) | Excellent (TF Serving, TFX, TF Lite) |",
    "| **Mobile/Edge** | Moderate (PyTorch Mobile) | Excellent (TF Lite, TF.js) |",
    "| **Research** | **Dominant** (60%+ papers) | Strong (40% papers) |",
    "| **Pre-trained Models** | torchvision, timm, HF | tf.keras.applications, TF Hub |",
    "| **GPU Support** | CUDA (NVIDIA) | CUDA + XLA (better multi-GPU) |",
    "| **Community** | Very active (researchers) | Very active (enterprise) |",
    "| **Ecosystem** | HuggingFace, Lightning | TFX, Model Garden, Vertex AI |",
    "",
    "**Verdict:** ",
    "- **For research, prototyping, flexibility:** PyTorch (easier debugging, more intuitive)",
    "- **For production deployment, mobile, enterprise:** TensorFlow (better tooling, maturity)",
    "- **For most projects:** Learn both (PyTorch for training, convert to ONNX for deployment)",
    "",
    "---",
    "",
    "### **\ud83d\ude80 What We'll Build**",
    "",
    "In this notebook, we'll implement the **same neural network** in both PyTorch and TensorFlow:",
    "",
    "**Architecture:** Multi-layer perceptron for semiconductor yield prediction",
    "```",
    "Input (50 features) ",
    "  \u2192 Dense (128, ReLU) + BatchNorm + Dropout (0.3)",
    "  \u2192 Dense (64, ReLU) + BatchNorm + Dropout (0.2)",
    "  \u2192 Dense (32, ReLU)",
    "  \u2192 Output (1, Sigmoid)",
    "```",
    "",
    "**Training configuration:**",
    "- Optimizer: Adam (lr=0.001)",
    "- Loss: Binary cross-entropy",
    "- Regularization: L2 (\u03bb=0.01) + Dropout",
    "- Metrics: Accuracy, Precision, Recall, AUC-ROC",
    "- Hardware: GPU if available (otherwise CPU)",
    "",
    "**We'll demonstrate:**",
    "1. **Model definition:** Class-based (PyTorch) vs Sequential/Functional (Keras)",
    "2. **Training loop:** Manual (PyTorch) vs fit() (Keras)",
    "3. **Callbacks:** Early stopping, learning rate scheduling, checkpointing",
    "4. **Device management:** CPU vs GPU, mixed precision training",
    "5. **Model saving:** State dict (PyTorch), SavedModel (TensorFlow)",
    "6. **Inference:** Batch prediction, production deployment",
    "",
    "**Dataset:** Simulated semiconductor parametric test data (5,000 samples, 50 features, binary yield).",
    "",
    "---",
    "",
    "### **\ud83d\uddfa\ufe0f Notebook Roadmap**",
    "",
    "**Part 1: PyTorch Fundamentals**",
    "1. Tensors, operations, device management",
    "2. Autograd (automatic differentiation)",
    "3. Building models with nn.Module",
    "4. Training loop from scratch",
    "5. Optimizers, schedulers, callbacks",
    "",
    "**Part 2: TensorFlow/Keras Fundamentals**",
    "1. Tensors, operations, eager execution",
    "2. Sequential vs Functional API",
    "3. Custom layers and models",
    "4. Built-in training with fit()",
    "5. Callbacks and model checkpointing",
    "",
    "**Part 3: Side-by-Side Comparison**",
    "1. Same architecture in both frameworks",
    "2. Performance comparison (training time, inference speed)",
    "3. Model conversion (ONNX for interoperability)",
    "4. Production deployment options",
    "",
    "**Part 4: Advanced Topics**",
    "1. GPU acceleration and mixed precision",
    "2. Distributed training (multi-GPU)",
    "3. Hyperparameter tuning (Ray Tune, Optuna)",
    "4. Production best practices",
    "",
    "**Part 5: Real-World Projects**",
    "1. Wafer yield predictor (PyTorch + TorchServe)",
    "2. Defect image classifier (TensorFlow + TF Serving)",
    "3. Power anomaly detector (Autoencoder in both frameworks)",
    "4. General AI/ML projects (churn, fraud, medical imaging)",
    "",
    "---",
    "",
    "### **\ud83d\udd17 Architecture Diagram: Framework Workflow**",
    "",
    "```mermaid",
    "graph TB",
    "    A[Raw Data] --> B[Preprocessing]",
    "    B --> C{Framework Choice}",
    "    ",
    "    C -->|PyTorch| D[PyTorch Tensors]",
    "    C -->|TensorFlow| E[TF Tensors]",
    "    ",
    "    D --> F[nn.Module Model]",
    "    E --> G[Keras Model]",
    "    ",
    "    F --> H[Manual Training Loop]",
    "    G --> I[model.fit Training]",
    "    ",
    "    H --> J[torch.save]",
    "    I --> K[model.save]",
    "    ",
    "    J --> L{Deployment}",
    "    K --> L",
    "    ",
    "    L --> M[TorchServe]",
    "    L --> N[TF Serving]",
    "    L --> O[ONNX Runtime]",
    "    ",
    "    M --> P[Production API]",
    "    N --> P",
    "    O --> P",
    "    ",
    "    style C fill:#f9f,stroke:#333,stroke-width:2px",
    "    style L fill:#bbf,stroke:#333,stroke-width:2px",
    "    style P fill:#bfb,stroke:#333,stroke-width:2px",
    "```",
    "",
    "**Key differences:**",
    "- **PyTorch:** More manual control (custom training loop), better for research/experimentation",
    "- **TensorFlow/Keras:** Higher-level API (fit() handles training), better for quick prototyping and production",
    "- **ONNX:** Universal format for model exchange (train in PyTorch, deploy with ONNX Runtime)",
    "",
    "---",
    "",
    "### **\ud83d\udce6 Installation & Setup**",
    "",
    "**PyTorch:**",
    "```bash",
    "# CPU only",
    "pip install torch torchvision",
    "",
    "# CUDA 11.8 (NVIDIA GPU)",
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118",
    "",
    "# CUDA 12.1",
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121",
    "```",
    "",
    "**TensorFlow:**",
    "```bash",
    "# CPU + GPU (automatic GPU detection)",
    "pip install tensorflow",
    "",
    "# Specific version",
    "pip install tensorflow==2.15.0",
    "```",
    "",
    "**Supporting libraries:**",
    "```bash",
    "pip install numpy pandas matplotlib scikit-learn tensorboard onnx onnxruntime",
    "```",
    "",
    "**Check installation:**",
    "```python",
    "import torch",
    "print(f\"PyTorch version: {torch.__version__}\")",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")",
    "",
    "import tensorflow as tf",
    "print(f\"TensorFlow version: {tf.__version__}\")",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU'))}\")",
    "```",
    "",
    "---",
    "",
    "### **\ud83d\udca1 Learning Strategy**",
    "",
    "**If you're new to deep learning frameworks:**",
    "1. Start with **Keras** (easiest, high-level API)",
    "2. Learn **PyTorch basics** (intuitive, Pythonic)",
    "3. Compare implementations side-by-side",
    "4. Pick primary framework based on use case (research \u2192 PyTorch, production \u2192 TensorFlow)",
    "",
    "**If you have framework experience:**",
    "- Focus on production patterns (deployment, monitoring, optimization)",
    "- Learn the other framework (cross-framework skills valuable)",
    "- Master ONNX for framework interoperability",
    "",
    "**For semiconductor engineers:**",
    "- Both frameworks widely used in industry (Qualcomm, AMD use both)",
    "- PyTorch common for research/prototyping",
    "- TensorFlow common for production deployment",
    "- ONNX increasingly popular for edge devices",
    "",
    "---",
    "",
    "### **\ud83d\udcda Prerequisites**",
    "",
    "**Required:**",
    "- \u2705 Completed **Notebook 051** (Neural Networks Foundations)",
    "- \u2705 Understand backpropagation, gradient descent, regularization",
    "- \u2705 Python basics (classes, functions, decorators)",
    "- \u2705 NumPy fundamentals (arrays, broadcasting)",
    "",
    "**Helpful:**",
    "- Familiarity with object-oriented programming",
    "- Basic understanding of GPU computing concepts",
    "- Experience with any ML library (scikit-learn, XGBoost)",
    "",
    "---",
    "",
    "### **\u23f1\ufe0f Time Investment**",
    "",
    "- **Reading + code execution:** 3-4 hours",
    "- **Practice exercises:** 2-3 hours",
    "- **Real-world project:** 5-10 hours",
    "- **Total:** 10-17 hours for mastery",
    "",
    "**Recommendation:** Spread over 3-5 sessions, practice with your own datasets between sessions.",
    "",
    "---",
    "",
    "### **\ud83c\udf93 Learning Objectives**",
    "",
    "After completing this notebook, you will be able to:",
    "",
    "\u2705 **Build neural networks** in PyTorch and TensorFlow/Keras  ",
    "\u2705 **Train models efficiently** with automatic differentiation and GPU acceleration  ",
    "\u2705 **Deploy models to production** using TorchServe, TF Serving, or ONNX Runtime  ",
    "\u2705 **Debug training issues** using TensorBoard and framework-specific tools  ",
    "\u2705 **Optimize performance** with mixed precision, data parallelism, and profiling  ",
    "\u2705 **Choose the right framework** based on project requirements and constraints  ",
    "\u2705 **Convert between frameworks** using ONNX for interoperability  ",
    "\u2705 **Apply to semiconductor testing** with production-grade implementations",
    "",
    "---",
    "",
    "**Let's dive in!** \ud83d\ude80 We'll start with PyTorch fundamentals, then TensorFlow/Keras, and finally compare them side-by-side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f0070",
   "metadata": {},
   "source": [
    "## \ud83d\udd25 Part 1: PyTorch Fundamentals\n",
    "\n",
    "### **What is PyTorch?**\n",
    "\n",
    "**PyTorch** is an open-source deep learning framework developed by Meta AI (formerly Facebook AI Research). It provides:\n",
    "- **Tensors:** GPU-accelerated multi-dimensional arrays (like NumPy + GPU)\n",
    "- **Autograd:** Automatic differentiation for backpropagation\n",
    "- **nn.Module:** Building blocks for neural networks\n",
    "- **Optimizers:** SGD, Adam, RMSprop with learning rate scheduling\n",
    "- **Ecosystem:** torchvision (computer vision), torchaudio (audio), torchtext (NLP)\n",
    "\n",
    "**Philosophy:** \"Pythonic\" design - tensors behave like NumPy arrays, imperative programming style, easy debugging.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. PyTorch Tensors: The Foundation**\n",
    "\n",
    "**Tensors** are multi-dimensional arrays (like NumPy ndarray) with GPU acceleration.\n",
    "\n",
    "#### **A. Creating Tensors**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# From Python list\n",
    "x = torch.tensor([1, 2, 3, 4, 5])  # 1D tensor (vector)\n",
    "print(f\"1D tensor: {x}, shape: {x.shape}, dtype: {x.dtype}\")\n",
    "\n",
    "# From NumPy array\n",
    "import numpy as np\n",
    "arr = np.array([[1, 2], [3, 4]])\n",
    "x = torch.from_numpy(arr)  # Shares memory with NumPy\n",
    "print(f\"From NumPy: {x}\")\n",
    "\n",
    "# Special tensors\n",
    "zeros = torch.zeros(2, 3)           # 2\u00d73 matrix of zeros\n",
    "ones = torch.ones(3, 4)             # 3\u00d74 matrix of ones\n",
    "rand = torch.rand(2, 2)             # Uniform [0, 1)\n",
    "randn = torch.randn(3, 3)           # Normal N(0, 1)\n",
    "eye = torch.eye(4)                  # 4\u00d74 identity matrix\n",
    "arange = torch.arange(0, 10, 2)     # [0, 2, 4, 6, 8]\n",
    "\n",
    "print(f\"Zeros:\\n{zeros}\")\n",
    "print(f\"Random:\\n{rand}\")\n",
    "```\n",
    "\n",
    "#### **B. Tensor Operations**\n",
    "\n",
    "```python\n",
    "# Element-wise operations\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "print(f\"Addition: {a + b}\")           # [5, 7, 9]\n",
    "print(f\"Multiplication: {a * b}\")     # [4, 10, 18]\n",
    "print(f\"Power: {a ** 2}\")             # [1, 4, 9]\n",
    "\n",
    "# Matrix operations\n",
    "A = torch.randn(2, 3)\n",
    "B = torch.randn(3, 4)\n",
    "\n",
    "print(f\"Matrix multiply: {torch.mm(A, B).shape}\")  # (2, 4)\n",
    "print(f\"Transpose: {A.T.shape}\")                    # (3, 2)\n",
    "\n",
    "# Aggregations\n",
    "x = torch.randn(3, 4)\n",
    "print(f\"Sum: {x.sum()}\")              # Total sum\n",
    "print(f\"Mean: {x.mean()}\")            # Average\n",
    "print(f\"Max: {x.max()}\")              # Maximum value\n",
    "print(f\"Row sums: {x.sum(dim=1)}\")    # Sum per row\n",
    "```\n",
    "\n",
    "#### **C. Reshaping & Indexing**\n",
    "\n",
    "```python\n",
    "x = torch.arange(12)\n",
    "print(f\"Original: {x}\")\n",
    "\n",
    "# Reshape\n",
    "x_reshaped = x.view(3, 4)             # 3\u00d74 matrix (shares memory)\n",
    "x_copy = x.reshape(4, 3)              # 4\u00d73 matrix (may copy)\n",
    "print(f\"Reshaped:\\n{x_reshaped}\")\n",
    "\n",
    "# Indexing\n",
    "A = torch.randn(4, 5)\n",
    "print(f\"First row: {A[0]}\")\n",
    "print(f\"First column: {A[:, 0]}\")\n",
    "print(f\"Submatrix: {A[1:3, 2:4]}\")    # Rows 1-2, cols 2-3\n",
    "\n",
    "# Boolean indexing\n",
    "x = torch.tensor([1, 2, 3, 4, 5])\n",
    "mask = x > 3\n",
    "print(f\"Elements > 3: {x[mask]}\")     # [4, 5]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Device Management: CPU vs GPU**\n",
    "\n",
    "**Key concept:** Tensors live on a **device** (CPU or GPU). All operations must use tensors on the **same device**.\n",
    "\n",
    "#### **A. Check GPU Availability**\n",
    "\n",
    "```python\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "```\n",
    "\n",
    "#### **B. Moving Tensors Between Devices**\n",
    "\n",
    "```python\n",
    "# Create tensor on CPU\n",
    "x_cpu = torch.randn(3, 3)\n",
    "print(f\"x_cpu device: {x_cpu.device}\")\n",
    "\n",
    "# Move to GPU\n",
    "x_gpu = x_cpu.to(device)  # or x_cpu.cuda() if GPU available\n",
    "print(f\"x_gpu device: {x_gpu.device}\")\n",
    "\n",
    "# Operations on GPU (10-100\u00d7 faster for large tensors)\n",
    "y_gpu = torch.randn(3, 3, device=device)  # Create directly on GPU\n",
    "z_gpu = x_gpu + y_gpu                      # GPU computation\n",
    "\n",
    "# Move back to CPU (required for NumPy conversion)\n",
    "z_cpu = z_gpu.cpu()\n",
    "z_np = z_cpu.numpy()  # Convert to NumPy\n",
    "\n",
    "print(f\"Result: {z_cpu}\")\n",
    "```\n",
    "\n",
    "**Performance tip:** Keep tensors on GPU throughout computation, only move to CPU when necessary (visualization, saving).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Autograd: Automatic Differentiation**\n",
    "\n",
    "**Autograd** is PyTorch's automatic differentiation engine. It tracks operations and computes gradients automatically.\n",
    "\n",
    "#### **A. Basic Gradient Computation**\n",
    "\n",
    "```python\n",
    "# Create tensor with gradient tracking\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "print(f\"x: {x}, requires_grad: {x.requires_grad}\")\n",
    "\n",
    "# Compute function\n",
    "y = x ** 2 + 3 * x + 1  # y = x\u00b2 + 3x + 1\n",
    "print(f\"y: {y}\")\n",
    "\n",
    "# Compute gradient dy/dx\n",
    "y.backward()  # Computes gradients\n",
    "print(f\"dy/dx: {x.grad}\")  # Should be 2x + 3 = 7 at x=2\n",
    "```\n",
    "\n",
    "**Mathematical verification:**\n",
    "- $y = x^2 + 3x + 1$\n",
    "- $\\frac{dy}{dx} = 2x + 3$\n",
    "- At $x = 2$: $\\frac{dy}{dx} = 2(2) + 3 = 7$ \u2705\n",
    "\n",
    "#### **B. Multi-Variable Gradients**\n",
    "\n",
    "```python\n",
    "# Multiple inputs\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Function\n",
    "z = x**2 + y**3  # z = x\u00b2 + y\u00b3\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "print(f\"\u2202z/\u2202x: {x.grad}\")  # Should be 2x = 2\n",
    "print(f\"\u2202z/\u2202y: {y.grad}\")  # Should be 3y\u00b2 = 12\n",
    "```\n",
    "\n",
    "#### **C. Gradient Accumulation**\n",
    "\n",
    "**Important:** Gradients **accumulate** by default. Reset with `zero_grad()` between iterations.\n",
    "\n",
    "```python\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# First computation\n",
    "y1 = x ** 2\n",
    "y1.backward()\n",
    "print(f\"First gradient: {x.grad}\")  # 2x = 4\n",
    "\n",
    "# Second computation (without zero_grad)\n",
    "y2 = x ** 3\n",
    "y2.backward()\n",
    "print(f\"Accumulated gradient: {x.grad}\")  # 4 + 3x\u00b2 = 16 (wrong!)\n",
    "\n",
    "# Correct approach\n",
    "x.grad.zero_()  # Reset gradient\n",
    "y2.backward()\n",
    "print(f\"Correct gradient: {x.grad}\")  # 3x\u00b2 = 12 \u2705\n",
    "```\n",
    "\n",
    "#### **D. Detaching from Computation Graph**\n",
    "\n",
    "```python\n",
    "x = torch.randn(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "\n",
    "# Detach (stop gradient tracking)\n",
    "y_detached = y.detach()  # y_detached doesn't track gradients\n",
    "\n",
    "# Use in computation without affecting gradients\n",
    "z = y_detached * 3\n",
    "z.sum().backward()  # Error: y_detached doesn't require gradients\n",
    "\n",
    "# Correct: Use torch.no_grad() context\n",
    "with torch.no_grad():\n",
    "    z = y * 3  # No gradient tracking (useful for inference)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Building Neural Networks with nn.Module**\n",
    "\n",
    "**nn.Module** is the base class for all neural network models in PyTorch. It provides:\n",
    "- Parameter management (weights, biases)\n",
    "- Automatic device placement\n",
    "- Built-in save/load functionality\n",
    "- Forward pass abstraction\n",
    "\n",
    "#### **A. Simple Linear Layer**\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "# Single linear layer: y = Wx + b\n",
    "linear = nn.Linear(in_features=10, out_features=5)\n",
    "\n",
    "# Input: batch of 32 samples, each with 10 features\n",
    "x = torch.randn(32, 10)\n",
    "\n",
    "# Forward pass\n",
    "y = linear(x)  # Output shape: (32, 5)\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "\n",
    "# Inspect parameters\n",
    "print(f\"Weight shape: {linear.weight.shape}\")  # (5, 10)\n",
    "print(f\"Bias shape: {linear.bias.shape}\")      # (5,)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in linear.parameters())}\")  # 55\n",
    "```\n",
    "\n",
    "#### **B. Custom Model with nn.Module**\n",
    "\n",
    "```python\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple 2-layer neural network.\n",
    "    \n",
    "    Architecture: Input \u2192 Hidden (ReLU) \u2192 Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: defines computation\"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = SimpleNet(input_size=20, hidden_size=64, output_size=1)\n",
    "print(model)\n",
    "\n",
    "# Forward pass\n",
    "x = torch.randn(10, 20)  # Batch of 10 samples\n",
    "output = model(x)\n",
    "print(f\"Output shape: {output.shape}\")  # (10, 1)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "```\n",
    "\n",
    "#### **C. Common Layers**\n",
    "\n",
    "```python\n",
    "# Activation functions\n",
    "relu = nn.ReLU()\n",
    "sigmoid = nn.Sigmoid()\n",
    "tanh = nn.Tanh()\n",
    "leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "# Regularization\n",
    "dropout = nn.Dropout(p=0.5)              # Randomly zero 50% of inputs\n",
    "batch_norm = nn.BatchNorm1d(num_features=64)  # Normalize batch\n",
    "\n",
    "# Loss functions\n",
    "mse_loss = nn.MSELoss()                  # Mean squared error (regression)\n",
    "bce_loss = nn.BCELoss()                  # Binary cross-entropy (binary classification)\n",
    "ce_loss = nn.CrossEntropyLoss()          # Cross-entropy (multi-class)\n",
    "\n",
    "# Pooling (for CNNs)\n",
    "max_pool = nn.MaxPool2d(kernel_size=2)   # 2\u00d72 max pooling\n",
    "avg_pool = nn.AvgPool2d(kernel_size=2)   # 2\u00d72 average pooling\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Training Loop: The PyTorch Pattern**\n",
    "\n",
    "**PyTorch doesn't have a built-in `fit()` method.** You write the training loop manually (more control, better for research).\n",
    "\n",
    "**Standard training loop:**\n",
    "\n",
    "```python\n",
    "# 1. Setup\n",
    "model = SimpleNet(input_size=20, hidden_size=64, output_size=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()  # BCE with sigmoid built-in\n",
    "\n",
    "# 2. Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "    loss.backward()         # Compute gradients\n",
    "    optimizer.step()        # Update parameters\n",
    "    \n",
    "    # Logging\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "```\n",
    "\n",
    "**Key steps:**\n",
    "1. **Forward pass:** Compute predictions (`outputs = model(X)`)\n",
    "2. **Compute loss:** Compare predictions to targets (`loss = criterion(outputs, y)`)\n",
    "3. **Zero gradients:** Reset accumulated gradients (`optimizer.zero_grad()`)\n",
    "4. **Backward pass:** Compute gradients (`loss.backward()`)\n",
    "5. **Update weights:** Apply gradients (`optimizer.step()`)\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Optimizers**\n",
    "\n",
    "PyTorch provides common optimizers in `torch.optim`:\n",
    "\n",
    "```python\n",
    "# SGD with momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Adam (default choice)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "# RMSprop\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
    "\n",
    "# AdamW (Adam with decoupled weight decay)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduling\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "# Or cosine annealing\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "# Use in training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # ... training code ...\n",
    "    scheduler.step()  # Update learning rate\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Model Saving & Loading**\n",
    "\n",
    "#### **A. Save/Load State Dict (Recommended)**\n",
    "\n",
    "```python\n",
    "# Save model parameters\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "# Load parameters\n",
    "model = SimpleNet(input_size=20, hidden_size=64, output_size=1)\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()  # Set to evaluation mode\n",
    "```\n",
    "\n",
    "#### **B. Save Entire Model (Less Flexible)**\n",
    "\n",
    "```python\n",
    "# Save entire model\n",
    "torch.save(model, 'model_complete.pth')\n",
    "\n",
    "# Load\n",
    "model = torch.load('model_complete.pth')\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "#### **C. Checkpointing (Save Training State)**\n",
    "\n",
    "```python\n",
    "# Save checkpoint\n",
    "checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint.pth')\n",
    "\n",
    "# Resume training\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83c\udfaf Key PyTorch Concepts Summary**\n",
    "\n",
    "| Concept | Purpose | Key Methods |\n",
    "|---------|---------|-------------|\n",
    "| **Tensor** | Multi-dimensional array with GPU support | `.to(device)`, `.numpy()`, `.item()` |\n",
    "| **Autograd** | Automatic differentiation | `.backward()`, `.zero_grad()`, `.detach()` |\n",
    "| **nn.Module** | Base class for models | `forward()`, `.parameters()`, `.train()`, `.eval()` |\n",
    "| **Optimizer** | Parameter update algorithms | `.zero_grad()`, `.step()` |\n",
    "| **Loss Function** | Measure prediction error | `nn.MSELoss()`, `nn.CrossEntropyLoss()` |\n",
    "| **Device** | CPU or GPU placement | `torch.device('cuda')`, `.to(device)` |\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** We'll implement a complete PyTorch model for semiconductor yield prediction! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948310dc",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0661a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PyTorch Complete Example: Semiconductor Yield Prediction\n",
    "Architecture: Input(50) \u2192 Dense(128, ReLU) + BatchNorm + Dropout(0.3)\n",
    "                        \u2192 Dense(64, ReLU) + BatchNorm + Dropout(0.2)\n",
    "                        \u2192 Dense(32, ReLU)\n",
    "                        \u2192 Output(1, Sigmoid)\n",
    "Goal: Predict wafer-level yield (binary: pass/fail) from 50 parametric test features.\n",
    "Business value: $50M-$200M/year scrap reduction through early failure detection.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "import time\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "print(\"=\"*80)\n",
    "print(\"PyTorch Semiconductor Yield Prediction Model\")\n",
    "print(\"=\"*80)\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Device Setup\n",
    "# -----------------------------------------------------------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nDevice: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. Generate Simulated Semiconductor Data\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. GENERATING SIMULATED SEMICONDUCTOR DATA\")\n",
    "print(\"=\"*80)\n",
    "n_samples = 5000\n",
    "n_features = 50\n",
    "# Simulate 50 parametric test features\n",
    "# Features: voltage, current, frequency, power, temperature measurements\n",
    "# Real-world: extracted from STDF files (wafer test + final test)\n",
    "np.random.seed(42)\n",
    "# Generate correlated features (semiconductor tests are often correlated)\n",
    "# Good devices: mean=0, std=1, high yield\n",
    "# Bad devices: shifted distributions, low yield\n",
    "def generate_semiconductor_data(n_samples, n_features):\n",
    "    \"\"\"\n",
    "    Generate simulated semiconductor parametric test data.\n",
    "    \n",
    "    Features represent:\n",
    "    - Voltage measurements (Vdd, Vss) - features 0-9\n",
    "    - Current measurements (Idd, leakage) - features 10-19\n",
    "    - Frequency measurements (clock, PLL) - features 20-29\n",
    "    - Power measurements (dynamic, static) - features 30-39\n",
    "    - Temperature coefficients - features 40-49\n",
    "    \n",
    "    Target: 1 = pass (good die), 0 = fail (bad die)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate features with realistic correlations\n",
    "    X = np.zeros((n_samples, n_features))\n",
    "    \n",
    "    # Create base latent factors (simulates underlying process variation)\n",
    "    n_factors = 5\n",
    "    latent_factors = np.random.randn(n_samples, n_factors)\n",
    "    \n",
    "    # Each feature is a linear combination of latent factors + noise\n",
    "    feature_weights = np.random.randn(n_factors, n_features) * 0.5\n",
    "    X = latent_factors @ feature_weights + np.random.randn(n_samples, n_features) * 0.3\n",
    "    \n",
    "    # Generate target (yield) based on feature patterns\n",
    "    # Good devices: sum of certain features is positive\n",
    "    # Bad devices: sum is negative\n",
    "    \n",
    "    # Critical features (indices 0, 10, 20, 30, 40 - one from each category)\n",
    "    critical_features = [0, 10, 20, 30, 40]\n",
    "    device_score = X[:, critical_features].sum(axis=1)\n",
    "    \n",
    "    # Add nonlinearity\n",
    "    device_score += 0.1 * (X[:, 5] * X[:, 15])  # Interaction term\n",
    "    device_score -= 0.2 * np.abs(X[:, 25])       # Nonlinear dependency\n",
    "    \n",
    "    # Convert to binary (threshold at median)\n",
    "    threshold = np.median(device_score)\n",
    "    y = (device_score > threshold).astype(int)\n",
    "    \n",
    "    # Add label noise (realistic: ~2% mislabeling)\n",
    "    flip_indices = np.random.choice(n_samples, size=int(0.02 * n_samples), replace=False)\n",
    "    y[flip_indices] = 1 - y[flip_indices]\n",
    "    \n",
    "    return X, y\n",
    "X, y = generate_semiconductor_data(n_samples, n_features)\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target distribution: {np.bincount(y)} (0=fail, 1=pass)\")\n",
    "print(f\"Yield rate: {y.mean()*100:.2f}%\")\n",
    "# Create feature names (for interpretability)\n",
    "feature_names = (\n",
    "    [f\"Vdd_{i}\" for i in range(10)] +\n",
    "    [f\"Idd_{i}\" for i in range(10)] +\n",
    "    [f\"Freq_{i}\" for i in range(10)] +\n",
    "    [f\"Power_{i}\" for i in range(10)] +\n",
    "    [f\"Temp_{i}\" for i in range(10)]\n",
    ")\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['yield'] = y\n",
    "print(\"\\nFirst few samples:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4843c0",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b077def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 3. Data Preprocessing\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. DATA PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "# Standardize features (critical for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(f\"Feature means (after scaling): {X_train_scaled.mean(axis=0)[:5]}\")  # Should be ~0\n",
    "print(f\"Feature stds (after scaling): {X_train_scaled.std(axis=0)[:5]}\")    # Should be ~1\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device)  # Shape: (n, 1)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1).to(device)\n",
    "print(f\"\\nTensor shapes:\")\n",
    "print(f\"X_train: {X_train_tensor.shape}, device: {X_train_tensor.device}\")\n",
    "print(f\"y_train: {y_train_tensor.shape}, device: {y_train_tensor.device}\")\n",
    "# Create DataLoader for batch training\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(f\"\\nNumber of batches per epoch: {len(train_loader)}\")\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. Define Model Architecture\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. MODEL ARCHITECTURE\")\n",
    "print(\"=\"*80)\n",
    "class SemiconductorYieldPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron for semiconductor yield prediction.\n",
    "    \n",
    "    Architecture:\n",
    "        Input(50) \u2192 Dense(128, ReLU) + BatchNorm + Dropout(0.3)\n",
    "                  \u2192 Dense(64, ReLU) + BatchNorm + Dropout(0.2)\n",
    "                  \u2192 Dense(32, ReLU)\n",
    "                  \u2192 Output(1, Sigmoid)\n",
    "    \n",
    "    Features:\n",
    "    - BatchNorm: Stabilizes training, reduces internal covariate shift\n",
    "    - Dropout: Prevents overfitting by randomly zeroing activations\n",
    "    - ReLU: Faster training, mitigates vanishing gradients\n",
    "    - Sigmoid: Outputs probability [0, 1] for binary classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=50, hidden1=128, hidden2=64, hidden3=32, dropout1=0.3, dropout2=0.2):\n",
    "        super(SemiconductorYieldPredictor, self).__init__()\n",
    "        \n",
    "        # Layer 1: Input \u2192 Hidden1\n",
    "        self.fc1 = nn.Linear(input_size, hidden1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden1)\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        \n",
    "        # Layer 2: Hidden1 \u2192 Hidden2\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden2)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        \n",
    "        # Layer 3: Hidden2 \u2192 Hidden3\n",
    "        self.fc3 = nn.Linear(hidden2, hidden3)\n",
    "        \n",
    "        # Output layer: Hidden3 \u2192 Output\n",
    "        self.fc4 = nn.Linear(hidden3, 1)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Initialize weights (Xavier/Glorot initialization)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier uniform initialization.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, 50)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, 1) with values in [0, 1]\n",
    "        \"\"\"\n",
    "        # Layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "# Create model\n",
    "model = SemiconductorYieldPredictor(\n",
    "    input_size=50,\n",
    "    hidden1=128,\n",
    "    hidden2=64,\n",
    "    hidden3=32,\n",
    "    dropout1=0.3,\n",
    "    dropout2=0.2\n",
    ").to(device)\n",
    "print(model)\n",
    "print(f\"\\nModel device: {next(model.parameters()).device}\")\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "# Breakdown by layer\n",
    "print(\"\\nParameter breakdown:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name:30s} {str(param.shape):20s} {param.numel():>8,} params\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f496b10",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00959e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 5. Define Loss Function and Optimizer\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. LOSS FUNCTION & OPTIMIZER\")\n",
    "print(\"=\"*80)\n",
    "# Loss: Binary Cross-Entropy (BCE)\n",
    "# Note: Using BCELoss (requires sigmoid in model) instead of BCEWithLogitsLoss\n",
    "criterion = nn.BCELoss()\n",
    "# Optimizer: Adam with weight decay (L2 regularization)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "# Learning rate scheduler: Reduce LR on plateau\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "print(f\"Loss function: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "print(f\"Scheduler: ReduceLROnPlateau (factor=0.5, patience=5)\")\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. Training Loop\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. TRAINING MODEL\")\n",
    "print(\"=\"*80)\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "early_stop_patience = 10\n",
    "print(f\"Training for {num_epochs} epochs...\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")\n",
    "print()\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    # -------------------\n",
    "    # Training phase\n",
    "    # -------------------\n",
    "    model.train()  # Set model to training mode (enables dropout, batchnorm updates)\n",
    "    \n",
    "    epoch_train_loss = 0.0\n",
    "    train_preds_all = []\n",
    "    train_targets_all = []\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        epoch_train_loss += loss.item() * batch_X.size(0)\n",
    "        \n",
    "        # Store predictions for accuracy\n",
    "        train_preds_all.append((outputs > 0.5).float())\n",
    "        train_targets_all.append(batch_y)\n",
    "    \n",
    "    # Average training loss\n",
    "    epoch_train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    \n",
    "    # Training accuracy\n",
    "    train_preds_all = torch.cat(train_preds_all).cpu().numpy()\n",
    "    train_targets_all = torch.cat(train_targets_all).cpu().numpy()\n",
    "    train_acc = accuracy_score(train_targets_all, train_preds_all)\n",
    "    train_accuracies.append(train_acc)\n",
    "    \n",
    "    # -------------------\n",
    "    # Validation phase\n",
    "    # -------------------\n",
    "    model.eval()  # Set model to evaluation mode (disables dropout, batchnorm uses running stats)\n",
    "    \n",
    "    epoch_val_loss = 0.0\n",
    "    val_preds_all = []\n",
    "    val_targets_all = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            epoch_val_loss += loss.item() * batch_X.size(0)\n",
    "            \n",
    "            val_preds_all.append((outputs > 0.5).float())\n",
    "            val_targets_all.append(batch_y)\n",
    "    \n",
    "    # Average validation loss\n",
    "    epoch_val_loss /= len(test_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    \n",
    "    # Validation accuracy\n",
    "    val_preds_all = torch.cat(val_preds_all).cpu().numpy()\n",
    "    val_targets_all = torch.cat(val_targets_all).cpu().numpy()\n",
    "    val_acc = accuracy_score(val_targets_all, val_preds_all)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Update learning rate based on validation loss\n",
    "    scheduler.step(epoch_val_loss)\n",
    "    \n",
    "    # Print progress every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1:3d}/{num_epochs}] \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'best_model_pytorch.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model_pytorch.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b58510",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4196fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 7. Evaluation\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_preds = (test_outputs > 0.5).float().cpu().numpy()\n",
    "    test_probs = test_outputs.cpu().numpy()\n",
    "    test_targets = y_test_tensor.cpu().numpy()\n",
    "# Metrics\n",
    "test_acc = accuracy_score(test_targets, test_preds)\n",
    "test_precision = precision_score(test_targets, test_preds)\n",
    "test_recall = recall_score(test_targets, test_preds)\n",
    "test_auc = roc_auc_score(test_targets, test_probs)\n",
    "print(f\"Test Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall:    {test_recall:.4f}\")\n",
    "print(f\"Test AUC-ROC:   {test_auc:.4f}\")\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_targets, test_preds)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"              Fail   Pass\")\n",
    "print(f\"Actual Fail   {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
    "print(f\"       Pass   {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
    "# Business metrics\n",
    "false_positives = cm[0, 1]  # Predicted pass, actually fail (bad dies shipped)\n",
    "false_negatives = cm[1, 0]  # Predicted fail, actually pass (good dies scrapped)\n",
    "print(f\"\\nBusiness Impact:\")\n",
    "print(f\"  False Positives (bad dies shipped): {false_positives} (~${false_positives * 50_000:,} potential loss)\")\n",
    "print(f\"  False Negatives (good dies scrapped): {false_negatives} (~${false_negatives * 1_000:,} loss)\")\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8. Visualizations\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"8. VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "# Training curves\n",
    "axes[0].plot(train_losses, label='Train Loss', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss (PyTorch)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "# Accuracy curves\n",
    "axes[1].plot(train_accuracies, label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(val_accuracies, label='Val Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy (PyTorch)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pytorch_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(\"Saved: pytorch_training_curves.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96163ed",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 5\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c544d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 9. Inference Example\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"9. PRODUCTION INFERENCE EXAMPLE\")\n",
    "print(\"=\"*80)\n",
    "# Simulate new test data (5 devices)\n",
    "new_devices = np.random.randn(5, 50)\n",
    "new_devices_scaled = scaler.transform(new_devices)\n",
    "new_devices_tensor = torch.FloatTensor(new_devices_scaled).to(device)\n",
    "# Inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(new_devices_tensor)\n",
    "    predictions = predictions.cpu().numpy()\n",
    "print(\"New device predictions (yield probability):\")\n",
    "for i, pred in enumerate(predictions):\n",
    "    status = \"PASS\" if pred[0] > 0.5 else \"FAIL\"\n",
    "    print(f\"  Device {i+1}: {pred[0]:.4f} \u2192 {status}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PyTorch Model Training Complete!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812d5914",
   "metadata": {},
   "source": [
    "## \ud83d\udd25 Part 2: TensorFlow/Keras Fundamentals\n",
    "\n",
    "### **What is TensorFlow/Keras?**\n",
    "\n",
    "**TensorFlow** is Google's open-source machine learning framework. **Keras** is its high-level API (integrated since TF 2.0).\n",
    "\n",
    "**Key features:**\n",
    "- **High-level API (Keras):** Simple, intuitive model building (`Sequential`, `Functional`)\n",
    "- **Production-ready:** TensorFlow Serving, TF Lite (mobile), TF.js (browser)\n",
    "- **Ecosystem:** TensorFlow Extended (TFX) for production ML pipelines\n",
    "- **Eager execution:** TF 2.x runs operations immediately (like PyTorch)\n",
    "- **Graph mode:** Can compile models for production (faster inference)\n",
    "\n",
    "**Philosophy:** Easy for beginners (Keras), powerful for production (TensorFlow).\n",
    "\n",
    "---\n",
    "\n",
    "### **1. TensorFlow Tensors**\n",
    "\n",
    "TensorFlow has its own tensor implementation (similar to PyTorch).\n",
    "\n",
    "#### **A. Creating Tensors**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# From Python list\n",
    "x = tf.constant([1, 2, 3, 4, 5])\n",
    "print(f\"TF tensor: {x}, dtype: {x.dtype}, shape: {x.shape}\")\n",
    "\n",
    "# From NumPy array\n",
    "arr = np.array([[1, 2], [3, 4]])\n",
    "x = tf.constant(arr)\n",
    "print(f\"From NumPy:\\n{x}\")\n",
    "\n",
    "# Special tensors\n",
    "zeros = tf.zeros((2, 3))\n",
    "ones = tf.ones((3, 4))\n",
    "rand = tf.random.uniform((2, 2), minval=0, maxval=1)  # Uniform [0, 1)\n",
    "randn = tf.random.normal((3, 3), mean=0, stddev=1)    # Normal N(0, 1)\n",
    "eye = tf.eye(4)\n",
    "arange = tf.range(0, 10, 2)\n",
    "\n",
    "print(f\"Zeros:\\n{zeros}\")\n",
    "print(f\"Random:\\n{rand}\")\n",
    "```\n",
    "\n",
    "#### **B. Tensor Operations**\n",
    "\n",
    "```python\n",
    "# Element-wise operations\n",
    "a = tf.constant([1.0, 2.0, 3.0])\n",
    "b = tf.constant([4.0, 5.0, 6.0])\n",
    "\n",
    "print(f\"Addition: {a + b}\")\n",
    "print(f\"Multiplication: {a * b}\")\n",
    "print(f\"Power: {tf.pow(a, 2)}\")\n",
    "\n",
    "# Matrix operations\n",
    "A = tf.random.normal((2, 3))\n",
    "B = tf.random.normal((3, 4))\n",
    "\n",
    "print(f\"Matrix multiply: {tf.matmul(A, B).shape}\")  # (2, 4)\n",
    "print(f\"Transpose: {tf.transpose(A).shape}\")        # (3, 2)\n",
    "\n",
    "# Aggregations\n",
    "x = tf.random.normal((3, 4))\n",
    "print(f\"Sum: {tf.reduce_sum(x)}\")\n",
    "print(f\"Mean: {tf.reduce_mean(x)}\")\n",
    "print(f\"Max: {tf.reduce_max(x)}\")\n",
    "print(f\"Row sums: {tf.reduce_sum(x, axis=1)}\")\n",
    "```\n",
    "\n",
    "#### **C. Converting to NumPy**\n",
    "\n",
    "```python\n",
    "x = tf.constant([[1, 2], [3, 4]])\n",
    "x_np = x.numpy()  # Convert to NumPy array\n",
    "print(f\"NumPy array:\\n{x_np}, type: {type(x_np)}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Automatic Differentiation with GradientTape**\n",
    "\n",
    "TensorFlow uses **`GradientTape`** to track operations for automatic differentiation.\n",
    "\n",
    "```python\n",
    "# Create variable (trainable tensor)\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "# Record operations\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2 + 3*x + 1  # y = x\u00b2 + 3x + 1\n",
    "\n",
    "# Compute gradient\n",
    "dy_dx = tape.gradient(y, x)  # dy/dx = 2x + 3 = 7 at x=2\n",
    "print(f\"dy/dx: {dy_dx.numpy()}\")\n",
    "```\n",
    "\n",
    "**Note:** PyTorch autograd is always on (if `requires_grad=True`), TensorFlow requires explicit `GradientTape` context.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Building Models: Sequential API**\n",
    "\n",
    "**Sequential API:** Simplest way to build models (linear stack of layers).\n",
    "\n",
    "```python\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(50,)),  # Input \u2192 128\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),                      # 128 \u2192 64\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation='relu'),                      # 64 \u2192 32\n",
    "    layers.Dense(1, activation='sigmoid')                     # 32 \u2192 1\n",
    "], name='yield_predictor')\n",
    "\n",
    "# Summary\n",
    "model.summary()\n",
    "\n",
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- \u2705 Concise and readable\n",
    "- \u2705 Easy to understand for beginners\n",
    "- \u2705 Automatic input shape inference\n",
    "\n",
    "**Limitations:**\n",
    "- \u274c No branching or multiple inputs/outputs\n",
    "- \u274c No skip connections (ResNet-style)\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Building Models: Functional API**\n",
    "\n",
    "**Functional API:** More flexible (multiple inputs/outputs, branching, skip connections).\n",
    "\n",
    "```python\n",
    "# Define input\n",
    "inputs = keras.Input(shape=(50,), name='input_features')\n",
    "\n",
    "# Layer 1\n",
    "x = layers.Dense(128, activation='relu', name='dense1')(inputs)\n",
    "x = layers.BatchNormalization(name='bn1')(x)\n",
    "x = layers.Dropout(0.3, name='dropout1')(x)\n",
    "\n",
    "# Layer 2\n",
    "x = layers.Dense(64, activation='relu', name='dense2')(x)\n",
    "x = layers.BatchNormalization(name='bn2')(x)\n",
    "x = layers.Dropout(0.2, name='dropout2')(x)\n",
    "\n",
    "# Layer 3\n",
    "x = layers.Dense(32, activation='relu', name='dense3')(x)\n",
    "\n",
    "# Output layer\n",
    "outputs = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "# Create model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='yield_predictor_functional')\n",
    "\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- \u2705 Flexible architecture (multiple inputs/outputs)\n",
    "- \u2705 Supports skip connections (ResNet, U-Net)\n",
    "- \u2705 Can extract intermediate layers\n",
    "\n",
    "**Use cases:**\n",
    "- Multi-input models (image + text)\n",
    "- Multi-output models (classification + regression)\n",
    "- Complex architectures (ResNet, Inception)\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Custom Layers and Models**\n",
    "\n",
    "For advanced use cases, create custom layers by subclassing `keras.layers.Layer`.\n",
    "\n",
    "```python\n",
    "class CustomDense(keras.layers.Layer):\n",
    "    \"\"\"Custom dense layer with L2 regularization.\"\"\"\n",
    "    \n",
    "    def __init__(self, units, l2_reg=0.01, **kwargs):\n",
    "        super(CustomDense, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.l2_reg = l2_reg\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Create layer weights.\"\"\"\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "            name='kernel',\n",
    "            regularizer=keras.regularizers.l2(self.l2_reg)\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "            name='bias'\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "# Use custom layer\n",
    "custom_layer = CustomDense(64, l2_reg=0.01)\n",
    "```\n",
    "\n",
    "**Custom Model (subclass `keras.Model`):**\n",
    "\n",
    "```python\n",
    "class YieldPredictor(keras.Model):\n",
    "    \"\"\"Custom model with manual forward pass.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(YieldPredictor, self).__init__()\n",
    "        self.dense1 = layers.Dense(128, activation='relu')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.dropout1 = layers.Dropout(0.3)\n",
    "        self.dense2 = layers.Dense(64, activation='relu')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.dropout2 = layers.Dropout(0.2)\n",
    "        self.dense3 = layers.Dense(32, activation='relu')\n",
    "        self.output_layer = layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Forward pass (training flag controls dropout/batchnorm).\"\"\"\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.dense3(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "model = YieldPredictor()\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- \u2705 Research: Custom training loops, complex architectures\n",
    "- \u2705 Non-standard forward passes (e.g., residual connections)\n",
    "- \u274c Simple models: Use Sequential/Functional API instead\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Training with model.fit()**\n",
    "\n",
    "**Keras provides a high-level `fit()` method** (unlike PyTorch, which requires manual training loop).\n",
    "\n",
    "```python\n",
    "# Compile model (define optimizer, loss, metrics)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,       # Use 20% of training data for validation\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5),\n",
    "        keras.callbacks.ModelCheckpoint('best_model_keras.h5', save_best_only=True)\n",
    "    ],\n",
    "    verbose=1  # Print progress\n",
    ")\n",
    "```\n",
    "\n",
    "**What `fit()` does automatically:**\n",
    "- \u2705 Batching data\n",
    "- \u2705 Forward/backward pass\n",
    "- \u2705 Gradient computation\n",
    "- \u2705 Parameter updates\n",
    "- \u2705 Validation evaluation\n",
    "- \u2705 Progress logging\n",
    "\n",
    "**Compare to PyTorch:** PyTorch requires manual implementation of all these steps.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Callbacks**\n",
    "\n",
    "**Callbacks** are functions executed during training (monitoring, checkpointing, early stopping).\n",
    "\n",
    "```python\n",
    "# EarlyStopping: Stop when validation loss stops improving\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# ModelCheckpoint: Save best model\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau: Reduce learning rate when loss plateaus\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "# TensorBoard: Logging for visualization\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir='./logs',\n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "# LearningRateScheduler: Custom LR schedule\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 10:\n",
    "        lr *= 0.9\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Use in training\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop, checkpoint, reduce_lr, tensorboard]\n",
    ")\n",
    "```\n",
    "\n",
    "**Common callbacks:**\n",
    "- `EarlyStopping`: Prevent overfitting\n",
    "- `ModelCheckpoint`: Save best model\n",
    "- `ReduceLROnPlateau`: Adaptive learning rate\n",
    "- `TensorBoard`: Visualization (training curves, histograms)\n",
    "- `CSVLogger`: Save training logs to CSV\n",
    "- `LambdaCallback`: Custom callback function\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Model Evaluation and Prediction**\n",
    "\n",
    "```python\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_precision, test_recall, test_auc = model.evaluate(\n",
    "    X_test_scaled, y_test, verbose=0\n",
    ")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "# Predict on new data\n",
    "predictions = model.predict(X_test_scaled)  # Returns probabilities\n",
    "pred_classes = (predictions > 0.5).astype(int)  # Convert to binary\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Model Saving and Loading**\n",
    "\n",
    "#### **A. SavedModel Format (Recommended for Production)**\n",
    "\n",
    "```python\n",
    "# Save entire model (architecture + weights + optimizer state)\n",
    "model.save('my_model')  # Creates directory with all model files\n",
    "\n",
    "# Load\n",
    "loaded_model = keras.models.load_model('my_model')\n",
    "```\n",
    "\n",
    "#### **B. HDF5 Format (Legacy)**\n",
    "\n",
    "```python\n",
    "# Save\n",
    "model.save('my_model.h5')\n",
    "\n",
    "# Load\n",
    "loaded_model = keras.models.load_model('my_model.h5')\n",
    "```\n",
    "\n",
    "#### **C. Save Only Weights**\n",
    "\n",
    "```python\n",
    "# Save weights\n",
    "model.save_weights('model_weights.h5')\n",
    "\n",
    "# Load weights (requires model to be built first)\n",
    "model = create_model()  # Define architecture\n",
    "model.load_weights('model_weights.h5')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83c\udfaf Keras vs PyTorch: Key Differences**\n",
    "\n",
    "| Feature | PyTorch | TensorFlow/Keras |\n",
    "|---------|---------|------------------|\n",
    "| **Training** | Manual loop | `model.fit()` (automatic) |\n",
    "| **API Style** | Imperative (define-by-run) | Declarative (define-then-run in graph mode) |\n",
    "| **Debugging** | Easier (standard Python debugger) | Harder (graph mode), easier in eager mode |\n",
    "| **Production** | TorchServe, ONNX | TensorFlow Serving (mature), TF Lite (mobile) |\n",
    "| **Learning Curve** | Moderate | Easy (Keras), hard (TF 1.x) |\n",
    "| **Flexibility** | High (custom training loops) | High (Functional API, subclassing) |\n",
    "| **Callbacks** | Manual implementation | Built-in (EarlyStopping, Checkpoints) |\n",
    "| **Device Management** | `.to(device)` | Automatic (CPU/GPU detection) |\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83d\ude80 When to Use Keras vs PyTorch?**\n",
    "\n",
    "**Use Keras/TensorFlow:**\n",
    "- \u2705 Quick prototyping with `fit()` API\n",
    "- \u2705 Production deployment (TF Serving, TF Lite)\n",
    "- \u2705 Mobile/Edge devices (TF Lite, TF.js)\n",
    "- \u2705 Enterprise adoption (mature ecosystem)\n",
    "- \u2705 Beginners (simpler API)\n",
    "\n",
    "**Use PyTorch:**\n",
    "- \u2705 Research and experimentation\n",
    "- \u2705 Custom training loops (reinforcement learning, GANs)\n",
    "- \u2705 Debugging-heavy workflows\n",
    "- \u2705 Dynamic architectures (RNNs with variable length)\n",
    "- \u2705 Pythonic coding style\n",
    "\n",
    "**Best practice:** Learn both, use PyTorch for research, convert to ONNX for production.\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** We'll implement the **same semiconductor yield predictor** in TensorFlow/Keras and compare with PyTorch! \ud83d\udd25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e162633a",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e42909",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TensorFlow/Keras Complete Example: Semiconductor Yield Prediction\n",
    "Same architecture as PyTorch for comparison:\n",
    "    Input(50) \u2192 Dense(128, ReLU) + BatchNorm + Dropout(0.3)\n",
    "              \u2192 Dense(64, ReLU) + BatchNorm + Dropout(0.2)\n",
    "              \u2192 Dense(32, ReLU)\n",
    "              \u2192 Output(1, Sigmoid)\n",
    "Goal: Compare Keras's high-level API to PyTorch's manual training loop.\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "import time\n",
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "print(\"=\"*80)\n",
    "print(\"TensorFlow/Keras Semiconductor Yield Prediction Model\")\n",
    "print(\"=\"*80)\n",
    "# Check GPU\n",
    "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(\"GPU will be used automatically\")\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Data Generation (Same as PyTorch)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. GENERATING DATA (Same as PyTorch)\")\n",
    "print(\"=\"*80)\n",
    "def generate_semiconductor_data(n_samples, n_features):\n",
    "    \"\"\"Same data generation function as PyTorch example.\"\"\"\n",
    "    X = np.zeros((n_samples, n_features))\n",
    "    n_factors = 5\n",
    "    latent_factors = np.random.randn(n_samples, n_factors)\n",
    "    feature_weights = np.random.randn(n_factors, n_features) * 0.5\n",
    "    X = latent_factors @ feature_weights + np.random.randn(n_samples, n_features) * 0.3\n",
    "    \n",
    "    critical_features = [0, 10, 20, 30, 40]\n",
    "    device_score = X[:, critical_features].sum(axis=1)\n",
    "    device_score += 0.1 * (X[:, 5] * X[:, 15])\n",
    "    device_score -= 0.2 * np.abs(X[:, 25])\n",
    "    \n",
    "    threshold = np.median(device_score)\n",
    "    y = (device_score > threshold).astype(int)\n",
    "    \n",
    "    flip_indices = np.random.choice(n_samples, size=int(0.02 * n_samples), replace=False)\n",
    "    y[flip_indices] = 1 - y[flip_indices]\n",
    "    \n",
    "    return X, y\n",
    "n_samples = 5000\n",
    "n_features = 50\n",
    "X, y = generate_semiconductor_data(n_samples, n_features)\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Target distribution: {np.bincount(y)} (0=fail, 1=pass)\")\n",
    "print(f\"Yield rate: {y.mean()*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220bcdfb",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d7f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 2. Data Preprocessing\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. DATA PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# Note: No need to convert to tensors or move to GPU (Keras handles automatically)\n",
    "print(f\"Data ready for Keras (NumPy arrays)\")\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. Define Model Architecture (Sequential API)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. MODEL ARCHITECTURE (Sequential API)\")\n",
    "print(\"=\"*80)\n",
    "model_sequential = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(50,), name='dense1'),\n",
    "    layers.BatchNormalization(name='bn1'),\n",
    "    layers.Dropout(0.3, name='dropout1'),\n",
    "    \n",
    "    layers.Dense(64, activation='relu', name='dense2'),\n",
    "    layers.BatchNormalization(name='bn2'),\n",
    "    layers.Dropout(0.2, name='dropout2'),\n",
    "    \n",
    "    layers.Dense(32, activation='relu', name='dense3'),\n",
    "    \n",
    "    layers.Dense(1, activation='sigmoid', name='output')\n",
    "], name='semiconductor_yield_predictor')\n",
    "print(model_sequential.summary())\n",
    "total_params = model_sequential.count_params()\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. Alternative: Functional API (More Flexible)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. ALTERNATIVE: FUNCTIONAL API\")\n",
    "print(\"=\"*80)\n",
    "# Define input\n",
    "inputs = keras.Input(shape=(50,), name='input_features')\n",
    "# Layer 1\n",
    "x = layers.Dense(128, activation='relu', name='dense1_func')(inputs)\n",
    "x = layers.BatchNormalization(name='bn1_func')(x)\n",
    "x = layers.Dropout(0.3, name='dropout1_func')(x)\n",
    "# Layer 2\n",
    "x = layers.Dense(64, activation='relu', name='dense2_func')(x)\n",
    "x = layers.BatchNormalization(name='bn2_func')(x)\n",
    "x = layers.Dropout(0.2, name='dropout2_func')(x)\n",
    "# Layer 3\n",
    "x = layers.Dense(32, activation='relu', name='dense3_func')(x)\n",
    "# Output\n",
    "outputs = layers.Dense(1, activation='sigmoid', name='output_func')(x)\n",
    "# Create model\n",
    "model_functional = keras.Model(inputs=inputs, outputs=outputs, name='yield_predictor_functional')\n",
    "print(model_functional.summary())\n",
    "# Use Functional API for rest of the example (more flexible)\n",
    "model = model_functional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15aed7",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5330340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 5. Compile Model (Define Optimizer, Loss, Metrics)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. COMPILE MODEL\")\n",
    "print(\"=\"*80)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "print(\"Model compiled with:\")\n",
    "print(\"  Optimizer: Adam (lr=0.001)\")\n",
    "print(\"  Loss: binary_crossentropy\")\n",
    "print(\"  Metrics: accuracy, precision, recall, AUC\")\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. Define Callbacks\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"6. DEFINE CALLBACKS\")\n",
    "print(\"=\"*80)\n",
    "# Early stopping\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "# Model checkpoint\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    'best_model_keras.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=0\n",
    ")\n",
    "# Reduce learning rate on plateau\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "# TensorBoard (optional)\n",
    "# tensorboard_cb = callbacks.TensorBoard(log_dir='./logs', histogram_freq=1)\n",
    "callback_list = [early_stop, checkpoint, reduce_lr]\n",
    "print(f\"Callbacks: EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\")\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7. Train Model (One Line!)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"7. TRAINING MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(\"Training for up to 50 epochs (with early stopping)...\")\n",
    "print(\"Compare to PyTorch: Keras fit() handles batching, forward/backward pass, optimization automatically\\n\")\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,  # Use 20% of training data for validation\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=callback_list,\n",
    "    verbose=1  # Print progress bar\n",
    ")\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09debfc9",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acde6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 8. Evaluation\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"8. MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "# Evaluate on test set\n",
    "test_results = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "test_loss = test_results[0]\n",
    "test_acc = test_results[1]\n",
    "test_precision = test_results[2]\n",
    "test_recall = test_results[3]\n",
    "test_auc = test_results[4]\n",
    "print(f\"Test Loss:      {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall:    {test_recall:.4f}\")\n",
    "print(f\"Test AUC-ROC:   {test_auc:.4f}\")\n",
    "# Predictions\n",
    "test_probs = model.predict(X_test_scaled, verbose=0)\n",
    "test_preds = (test_probs > 0.5).astype(int)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, test_preds)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"              Fail   Pass\")\n",
    "print(f\"Actual Fail   {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
    "print(f\"       Pass   {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
    "# Business metrics\n",
    "false_positives = cm[0, 1]\n",
    "false_negatives = cm[1, 0]\n",
    "print(f\"\\nBusiness Impact:\")\n",
    "print(f\"  False Positives (bad dies shipped): {false_positives} (~${false_positives * 50_000:,} potential loss)\")\n",
    "print(f\"  False Negatives (good dies scrapped): {false_negatives} (~${false_negatives * 1_000:,} loss)\")\n",
    "# -----------------------------------------------------------------------------\n",
    "# 9. Visualizations\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"9. VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "# Training curves (loss)\n",
    "axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss (Keras)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "# Training curves (accuracy)\n",
    "axes[1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy (Keras)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('keras_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(\"Saved: keras_training_curves.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b1721e",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 5\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4084b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 10. Model Saving and Loading\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"10. MODEL SAVING & LOADING\")\n",
    "print(\"=\"*80)\n",
    "# Save entire model (architecture + weights + optimizer state)\n",
    "model.save('semiconductor_yield_model_keras')\n",
    "print(\"Saved model to: semiconductor_yield_model_keras/\")\n",
    "# Load model\n",
    "loaded_model = keras.models.load_model('semiconductor_yield_model_keras')\n",
    "print(\"Loaded model successfully\")\n",
    "# Verify loaded model works\n",
    "loaded_predictions = loaded_model.predict(X_test_scaled[:5], verbose=0)\n",
    "print(f\"\\nPredictions from loaded model (first 5 samples):\")\n",
    "for i, pred in enumerate(loaded_predictions):\n",
    "    status = \"PASS\" if pred[0] > 0.5 else \"FAIL\"\n",
    "    print(f\"  Device {i+1}: {pred[0]:.4f} \u2192 {status}\")\n",
    "# -----------------------------------------------------------------------------\n",
    "# 11. Production Inference Example\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"11. PRODUCTION INFERENCE\")\n",
    "print(\"=\"*80)\n",
    "# Simulate new test data (5 devices)\n",
    "new_devices = np.random.randn(5, 50)\n",
    "new_devices_scaled = scaler.transform(new_devices)\n",
    "# Inference (batch prediction)\n",
    "predictions = model.predict(new_devices_scaled, verbose=0)\n",
    "print(\"New device predictions (yield probability):\")\n",
    "for i, pred in enumerate(predictions):\n",
    "    status = \"PASS\" if pred[0] > 0.5 else \"FAIL\"\n",
    "    confidence = pred[0] if pred[0] > 0.5 else 1 - pred[0]\n",
    "    print(f\"  Device {i+1}: {pred[0]:.4f} \u2192 {status} (confidence: {confidence:.2%})\")\n",
    "# -----------------------------------------------------------------------------\n",
    "# 12. Extract Intermediate Layer Outputs (Feature Extraction)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"12. FEATURE EXTRACTION (Intermediate Layers)\")\n",
    "print(\"=\"*80)\n",
    "# Create model that outputs intermediate layer\n",
    "layer_name = 'dense3_func'  # Third hidden layer\n",
    "intermediate_model = keras.Model(\n",
    "    inputs=model.input,\n",
    "    outputs=model.get_layer(layer_name).output\n",
    ")\n",
    "# Extract features\n",
    "features = intermediate_model.predict(X_test_scaled[:10], verbose=0)\n",
    "print(f\"Extracted features from layer '{layer_name}':\")\n",
    "print(f\"Shape: {features.shape}\")  # (10, 32) - 32 neurons in dense3\n",
    "print(f\"First sample features (first 10 values):\\n{features[0, :10]}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Keras Model Training Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"  \u2705 Model definition: ~20 lines (vs ~80 in PyTorch)\")\n",
    "print(\"  \u2705 Training: Single fit() call (vs manual loop in PyTorch)\")\n",
    "print(\"  \u2705 Callbacks: Built-in (vs manual implementation in PyTorch)\")\n",
    "print(\"  \u2705 GPU: Automatic detection (vs explicit .to(device) in PyTorch)\")\n",
    "print(\"  \u2705 Metrics: Tracked automatically (vs manual computation in PyTorch)\")\n",
    "print(\"\\n  Trade-off: Less flexibility for custom training logic\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9ec59e",
   "metadata": {},
   "source": [
    "## \ud83d\udd25 Part 3: Framework Comparison & ONNX Conversion\n",
    "\n",
    "### **Side-by-Side Comparison: PyTorch vs Keras**\n",
    "\n",
    "Now that we've built the **same model** in both frameworks, let's compare them systematically.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Code Comparison**\n",
    "\n",
    "#### **Model Definition**\n",
    "\n",
    "**PyTorch:**\n",
    "```python\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(50, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        # ... more layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "**Keras:**\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, input_shape=(50,)),\n",
    "    layers.BatchNormalization(),\n",
    "    # ... more layers\n",
    "])\n",
    "```\n",
    "\n",
    "**Winner:** Keras (simpler, less boilerplate)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Training**\n",
    "\n",
    "**PyTorch:**\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "**Keras:**\n",
    "```python\n",
    "model.fit(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[...])\n",
    "```\n",
    "\n",
    "**Winner:** Keras (automatic batching, validation, metrics)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Callbacks**\n",
    "\n",
    "**PyTorch:** Manual implementation required\n",
    "**Keras:** Built-in (`EarlyStopping`, `ModelCheckpoint`, `ReduceLROnPlateau`)\n",
    "\n",
    "**Winner:** Keras\n",
    "\n",
    "---\n",
    "\n",
    "#### **Device Management**\n",
    "\n",
    "**PyTorch:**\n",
    "```python\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "X.to(device)\n",
    "```\n",
    "\n",
    "**Keras:**\n",
    "```python\n",
    "# Automatic GPU detection\n",
    "```\n",
    "\n",
    "**Winner:** Keras (automatic)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Flexibility**\n",
    "\n",
    "**PyTorch:** Full control over training loop (better for research)  \n",
    "**Keras:** Less flexibility, but can use custom training loops if needed\n",
    "\n",
    "**Winner:** PyTorch (for research), Keras (for production)\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Performance Comparison**\n",
    "\n",
    "Let's benchmark both frameworks on the same task:\n",
    "\n",
    "| Metric | PyTorch | Keras | Notes |\n",
    "|--------|---------|-------|-------|\n",
    "| **Training Time** | ~15-20s | ~12-18s | Keras slightly faster (optimized C++ backend) |\n",
    "| **Inference Time (CPU)** | ~2-3ms/sample | ~2-4ms/sample | Similar |\n",
    "| **Inference Time (GPU)** | ~0.1ms/sample | ~0.1ms/sample | Similar |\n",
    "| **Memory Usage** | ~150MB | ~180MB | PyTorch slightly more efficient |\n",
    "| **Model Size** | ~500KB | ~520KB | Similar |\n",
    "\n",
    "**Verdict:** Performance is comparable for most tasks. Differences are negligible for production.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. ONNX: Universal Model Format**\n",
    "\n",
    "**ONNX (Open Neural Network Exchange)** allows converting models between frameworks.\n",
    "\n",
    "**Use case:** Train in PyTorch (research), deploy with ONNX Runtime (production).\n",
    "\n",
    "#### **A. Export PyTorch to ONNX**\n",
    "\n",
    "```python\n",
    "import torch.onnx\n",
    "\n",
    "# Export PyTorch model\n",
    "dummy_input = torch.randn(1, 50).to(device)\n",
    "torch.onnx.export(\n",
    "    model,                          # PyTorch model\n",
    "    dummy_input,                    # Sample input\n",
    "    \"model.onnx\",                   # Output file\n",
    "    export_params=True,             # Include weights\n",
    "    opset_version=13,               # ONNX version\n",
    "    input_names=['input'],          # Input name\n",
    "    output_names=['output'],        # Output name\n",
    "    dynamic_axes={                  # Variable batch size\n",
    "        'input': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "#### **B. Export Keras to ONNX**\n",
    "\n",
    "```python\n",
    "import tf2onnx\n",
    "\n",
    "# Export Keras model\n",
    "spec = (tf.TensorSpec((None, 50), tf.float32, name=\"input\"),)\n",
    "output_path = \"model_keras.onnx\"\n",
    "\n",
    "model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)\n",
    "with open(output_path, \"wb\") as f:\n",
    "    f.write(model_proto.SerializeToString())\n",
    "```\n",
    "\n",
    "#### **C. Load and Infer with ONNX Runtime**\n",
    "\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Load ONNX model\n",
    "session = ort.InferenceSession(\"model.onnx\")\n",
    "\n",
    "# Inference\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "X_sample = np.random.randn(10, 50).astype(np.float32)\n",
    "predictions = session.run([output_name], {input_name: X_sample})[0]\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "```\n",
    "\n",
    "**ONNX Advantages:**\n",
    "- \u2705 Framework-agnostic deployment\n",
    "- \u2705 Optimized inference (5-10\u00d7 faster than native frameworks)\n",
    "- \u2705 Supports edge devices (mobile, IoT)\n",
    "- \u2705 Cloud deployment (Azure ML, AWS SageMaker)\n",
    "\n",
    "---\n",
    "\n",
    "### **4. When to Use Which Framework?**\n",
    "\n",
    "#### **Use PyTorch If:**\n",
    "- \u2705 Research and experimentation (60% of ML papers use PyTorch)\n",
    "- \u2705 Custom training loops (GANs, reinforcement learning)\n",
    "- \u2705 Dynamic architectures (variable-length sequences)\n",
    "- \u2705 Debugging is critical (Pythonic, easier to debug)\n",
    "- \u2705 Working with HuggingFace Transformers\n",
    "\n",
    "#### **Use TensorFlow/Keras If:**\n",
    "- \u2705 Production deployment (TF Serving, TF Lite)\n",
    "- \u2705 Mobile/Edge devices (TF Lite superior to PyTorch Mobile)\n",
    "- \u2705 Quick prototyping (fit() API faster development)\n",
    "- \u2705 Enterprise adoption (Google ecosystem, Vertex AI)\n",
    "- \u2705 Team prefers high-level API\n",
    "\n",
    "#### **Best Practice:**\n",
    "1. **Prototype in PyTorch** (faster iteration, easier debugging)\n",
    "2. **Convert to ONNX** (framework-agnostic)\n",
    "3. **Deploy with ONNX Runtime** (optimized inference)\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Production Deployment Options**\n",
    "\n",
    "#### **PyTorch Deployment**\n",
    "\n",
    "**TorchServe:**\n",
    "```bash\n",
    "# Install TorchServe\n",
    "pip install torchserve torch-model-archiver\n",
    "\n",
    "# Archive model\n",
    "torch-model-archiver --model-name yield_predictor \\\n",
    "    --version 1.0 \\\n",
    "    --model-file model.py \\\n",
    "    --serialized-file model.pth \\\n",
    "    --handler custom_handler.py\n",
    "\n",
    "# Start server\n",
    "torchserve --start --model-store model_store --models yield_predictor=yield_predictor.mar\n",
    "```\n",
    "\n",
    "**Docker:**\n",
    "```dockerfile\n",
    "FROM pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime\n",
    "COPY model.pth /app/\n",
    "COPY inference.py /app/\n",
    "CMD [\"python\", \"/app/inference.py\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **TensorFlow Deployment**\n",
    "\n",
    "**TensorFlow Serving:**\n",
    "```bash\n",
    "# Save model in SavedModel format\n",
    "model.save('saved_model/1/')\n",
    "\n",
    "# Run TF Serving\n",
    "docker run -p 8501:8501 \\\n",
    "    --mount type=bind,source=/path/to/saved_model,target=/models/yield_predictor \\\n",
    "    -e MODEL_NAME=yield_predictor \\\n",
    "    -t tensorflow/serving\n",
    "```\n",
    "\n",
    "**TF Lite (Mobile):**\n",
    "```python\n",
    "# Convert to TF Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Quantization\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **ONNX Runtime (Universal)**\n",
    "\n",
    "```python\n",
    "# Deploy on any platform\n",
    "import onnxruntime as ort\n",
    "\n",
    "session = ort.InferenceSession(\"model.onnx\")\n",
    "predictions = session.run(None, {\"input\": X_test})[0]\n",
    "```\n",
    "\n",
    "**Deployment targets:**\n",
    "- Azure ML, AWS SageMaker, Google Vertex AI\n",
    "- Mobile (iOS, Android via ONNX Mobile)\n",
    "- Edge devices (NVIDIA Jetson, Raspberry Pi)\n",
    "- Web browsers (ONNX.js)\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Framework Ecosystem Comparison**\n",
    "\n",
    "| Tool/Library | PyTorch | TensorFlow | Purpose |\n",
    "|--------------|---------|------------|---------|\n",
    "| **Pre-trained Models** | torchvision, timm, HuggingFace | tf.keras.applications, TF Hub | Transfer learning |\n",
    "| **Visualization** | TensorBoard (PyTorch 1.2+) | TensorBoard (native) | Training monitoring |\n",
    "| **Serving** | TorchServe | TensorFlow Serving | Production inference |\n",
    "| **Mobile** | PyTorch Mobile | TF Lite | On-device inference |\n",
    "| **Distributed Training** | PyTorch DDP, Lightning | tf.distribute | Multi-GPU/multi-node |\n",
    "| **AutoML** | Ray Tune, Optuna | TensorFlow AutoML, Keras Tuner | Hyperparameter tuning |\n",
    "| **Model Zoo** | PyTorch Hub | TensorFlow Model Garden | Pre-trained models |\n",
    "| **Edge Deployment** | ONNX Runtime | TF Lite, Edge TPU | IoT, embedded systems |\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83c\udfaf Key Takeaways: Framework Choice**\n",
    "\n",
    "**For Semiconductor Testing:**\n",
    "- **Research phase:** PyTorch (flexibility for novel architectures)\n",
    "- **Production phase:** Convert to ONNX, deploy with ONNX Runtime\n",
    "- **Edge devices:** TensorFlow Lite (better support for custom hardware)\n",
    "- **Cloud deployment:** Both work well (choose based on team expertise)\n",
    "\n",
    "**General guideline:**\n",
    "- **Learning:** Start with Keras (easier), then learn PyTorch (flexibility)\n",
    "- **Career:** Learn both (most companies use both)\n",
    "- **Projects:** Choose based on deployment target, not training preferences\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** We'll explore GPU acceleration, mixed precision training, and distributed training techniques! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296025e",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Part 4: GPU Acceleration & Advanced Training\n",
    "\n",
    "### **1. GPU Acceleration Fundamentals**\n",
    "\n",
    "**Why GPUs matter for deep learning:**\n",
    "- **Parallelism:** GPUs have 1000s of cores vs CPUs with ~10 cores\n",
    "- **Speedup:** 10-100\u00d7 faster for large models\n",
    "- **Matrix operations:** Optimized for neural network computations\n",
    "\n",
    "**When GPU helps most:**\n",
    "- Large batch sizes (\u226532)\n",
    "- Large models (\u22651M parameters)\n",
    "- CNNs, RNNs, Transformers (matrix-heavy operations)\n",
    "\n",
    "**When GPU doesn't help:**\n",
    "- Small models (<100K parameters)\n",
    "- Small datasets (<1000 samples)\n",
    "- CPU-bound operations (data loading, preprocessing)\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Mixed Precision Training**\n",
    "\n",
    "**Mixed precision** uses both FP16 (16-bit) and FP32 (32-bit) floating point to:\n",
    "- **Speed up training:** 2-3\u00d7 faster (Tensor Cores on NVIDIA GPUs)\n",
    "- **Reduce memory:** 2\u00d7 less GPU memory\n",
    "- **Maintain accuracy:** Critical operations stay in FP32\n",
    "\n",
    "#### **PyTorch Mixed Precision**\n",
    "\n",
    "```python\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "model = Model().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scaler = GradScaler()  # Gradient scaling to prevent underflow\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with autocast\n",
    "        with autocast():\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "```\n",
    "\n",
    "**Key components:**\n",
    "- `autocast()`: Automatically casts operations to FP16 where safe\n",
    "- `GradScaler`: Scales gradients to prevent underflow in FP16\n",
    "\n",
    "---\n",
    "\n",
    "#### **TensorFlow Mixed Precision**\n",
    "\n",
    "```python\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Enable mixed precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Build model (automatically uses FP16 where appropriate)\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', dtype='float32'),  # Input in FP32\n",
    "    layers.Dense(64, activation='relu'),                    # Auto FP16\n",
    "    layers.Dense(1, activation='sigmoid', dtype='float32')  # Output in FP32\n",
    "])\n",
    "\n",
    "# Compile with loss scaling\n",
    "optimizer = keras.optimizers.Adam()\n",
    "optimizer = mixed_precision.LossScaleOptimizer(optimizer)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "model.fit(X_train, y_train, epochs=50)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- 2-3\u00d7 faster training on modern GPUs (V100, A100)\n",
    "- 50% less memory usage\n",
    "- Negligible accuracy loss (<0.1% typical)\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Distributed Training (Multi-GPU)**\n",
    "\n",
    "When one GPU isn't enough, train across multiple GPUs or machines.\n",
    "\n",
    "#### **Data Parallelism**\n",
    "\n",
    "Split batch across GPUs, each GPU processes subset, gradients are averaged.\n",
    "\n",
    "**PyTorch Distributed Data Parallel (DDP):**\n",
    "\n",
    "```python\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# Initialize process group (one process per GPU)\n",
    "dist.init_process_group(backend='nccl')\n",
    "local_rank = int(os.environ['LOCAL_RANK'])\n",
    "torch.cuda.set_device(local_rank)\n",
    "\n",
    "# Wrap model with DDP\n",
    "model = Model().to(local_rank)\n",
    "model = DDP(model, device_ids=[local_rank])\n",
    "\n",
    "# Training loop (same as single GPU)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "**Launch command:**\n",
    "```bash\n",
    "torchrun --nproc_per_node=4 train.py  # 4 GPUs on one machine\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**TensorFlow Distributed Strategy:**\n",
    "\n",
    "```python\n",
    "# Create strategy\n",
    "strategy = tf.distribute.MirroredStrategy()  # Synchronous multi-GPU\n",
    "\n",
    "# Build model within strategy scope\n",
    "with strategy.scope():\n",
    "    model = keras.Sequential([...])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train (automatically distributed)\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64)\n",
    "```\n",
    "\n",
    "**Multi-machine (multi-node):**\n",
    "```python\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Model Parallelism**\n",
    "\n",
    "Split model across GPUs when model is too large for one GPU.\n",
    "\n",
    "**PyTorch:**\n",
    "```python\n",
    "class ParallelModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1000, 5000).to('cuda:0')  # GPU 0\n",
    "        self.layer2 = nn.Linear(5000, 5000).to('cuda:1')  # GPU 1\n",
    "        self.layer3 = nn.Linear(5000, 10).to('cuda:2')    # GPU 2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x.to('cuda:0'))\n",
    "        x = self.layer2(x.to('cuda:1'))\n",
    "        x = self.layer3(x.to('cuda:2'))\n",
    "        return x\n",
    "```\n",
    "\n",
    "**Use case:** Very large models (GPT-3, BERT-Large) that don't fit on one GPU.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Hyperparameter Tuning**\n",
    "\n",
    "Finding optimal hyperparameters (learning rate, batch size, architecture).\n",
    "\n",
    "#### **PyTorch + Ray Tune**\n",
    "\n",
    "```python\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "def train_model(config):\n",
    "    model = Model(hidden_size=config['hidden_size'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        # Training loop\n",
    "        train_loss = train_epoch(model, optimizer, train_loader)\n",
    "        val_loss = validate(model, val_loader)\n",
    "        \n",
    "        # Report to Ray Tune\n",
    "        tune.report(loss=val_loss)\n",
    "\n",
    "# Define search space\n",
    "config = {\n",
    "    'lr': tune.loguniform(1e-5, 1e-2),\n",
    "    'hidden_size': tune.choice([64, 128, 256]),\n",
    "    'batch_size': tune.choice([32, 64, 128])\n",
    "}\n",
    "\n",
    "# Run tuning\n",
    "analysis = tune.run(\n",
    "    train_model,\n",
    "    config=config,\n",
    "    num_samples=20,\n",
    "    scheduler=ASHAScheduler()\n",
    ")\n",
    "\n",
    "best_config = analysis.get_best_config(metric='loss', mode='min')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **TensorFlow + Keras Tuner**\n",
    "\n",
    "```python\n",
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(\n",
    "            units=hp.Int('units1', min_value=64, max_value=256, step=64),\n",
    "            activation='relu'\n",
    "        ),\n",
    "        layers.Dropout(hp.Float('dropout', 0.1, 0.5, step=0.1)),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Float('lr', 1e-5, 1e-2, sampling='log')\n",
    "        ),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Create tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=50,\n",
    "    directory='tuner_results'\n",
    ")\n",
    "\n",
    "# Search\n",
    "tuner.search(X_train, y_train, validation_split=0.2, epochs=50)\n",
    "\n",
    "# Best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Performance Optimization Checklist**\n",
    "\n",
    "#### **Data Loading**\n",
    "- \u2705 Use `DataLoader` (PyTorch) or `tf.data` (TensorFlow) with `num_workers > 0`\n",
    "- \u2705 Preload data to RAM if possible\n",
    "- \u2705 Use `pin_memory=True` (PyTorch) for faster CPU\u2192GPU transfer\n",
    "\n",
    "#### **Training**\n",
    "- \u2705 Use mixed precision training (2-3\u00d7 speedup)\n",
    "- \u2705 Increase batch size to saturate GPU (monitor memory)\n",
    "- \u2705 Use gradient accumulation if GPU memory is limited\n",
    "- \u2705 Enable cuDNN autotuner: `torch.backends.cudnn.benchmark = True`\n",
    "\n",
    "#### **Model Architecture**\n",
    "- \u2705 Use fused operations (BatchNorm + ReLU in one layer)\n",
    "- \u2705 Avoid frequent CPU\u2194GPU transfers\n",
    "- \u2705 Use `torch.jit.script` (PyTorch) or `tf.function` (TensorFlow) for graph compilation\n",
    "\n",
    "#### **Profiling**\n",
    "- \u2705 Use PyTorch Profiler or TensorFlow Profiler to identify bottlenecks\n",
    "- \u2705 Monitor GPU utilization (`nvidia-smi`)\n",
    "- \u2705 Profile data loading separately from training\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Semiconductor-Specific Optimizations**\n",
    "\n",
    "#### **Real-Time Inference Requirements**\n",
    "\n",
    "**Challenge:** Wafer test needs <10ms inference time for 50K+ die/hour throughput.\n",
    "\n",
    "**Solutions:**\n",
    "1. **Model quantization:** INT8 inference (4\u00d7 faster, same accuracy)\n",
    "   ```python\n",
    "   # PyTorch quantization\n",
    "   model_int8 = torch.quantization.quantize_dynamic(\n",
    "       model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **TorchScript compilation:**\n",
    "   ```python\n",
    "   scripted_model = torch.jit.script(model)\n",
    "   scripted_model.save('model_scripted.pt')\n",
    "   ```\n",
    "\n",
    "3. **ONNX Runtime with GPU:**\n",
    "   ```python\n",
    "   providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "   session = ort.InferenceSession('model.onnx', providers=providers)\n",
    "   ```\n",
    "\n",
    "4. **Batch inference:** Process 1000 die at once (10\u00d7 faster than one-by-one)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Memory-Efficient Training for Large Datasets**\n",
    "\n",
    "**Challenge:** 1 million die \u00d7 50 features = 200MB (manageable), but 100M+ samples common.\n",
    "\n",
    "**Solutions:**\n",
    "1. **Streaming from disk:**\n",
    "   ```python\n",
    "   class STDFDataset(torch.utils.data.Dataset):\n",
    "       def __init__(self, file_list):\n",
    "           self.file_list = file_list\n",
    "       \n",
    "       def __getitem__(self, idx):\n",
    "           # Load single sample from STDF file on-the-fly\n",
    "           return load_stdf_sample(self.file_list[idx])\n",
    "   ```\n",
    "\n",
    "2. **Memory-mapped arrays:**\n",
    "   ```python\n",
    "   X_mmap = np.memmap('data.npy', dtype='float32', mode='r', shape=(1e6, 50))\n",
    "   ```\n",
    "\n",
    "3. **Gradient checkpointing:** Trade compute for memory (2\u00d7 less memory, 20% slower)\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83c\udfaf Performance Summary**\n",
    "\n",
    "| Technique | Speedup | Memory Reduction | Accuracy Impact |\n",
    "|-----------|---------|------------------|-----------------|\n",
    "| **Mixed Precision** | 2-3\u00d7 | 50% | <0.1% |\n",
    "| **Multi-GPU (4 GPUs)** | 3.5\u00d7 | - | None |\n",
    "| **Quantization (INT8)** | 4\u00d7 | 75% | ~1% |\n",
    "| **TorchScript** | 1.5\u00d7 | - | None |\n",
    "| **ONNX Runtime** | 2-5\u00d7 | - | None |\n",
    "| **Batch Inference** | 10\u00d7 | - | None |\n",
    "\n",
    "**For semiconductor testing:**\n",
    "- Training: Mixed precision + multi-GPU \u2192 6-9\u00d7 faster\n",
    "- Inference: ONNX Runtime + INT8 quantization \u2192 8-20\u00d7 faster\n",
    "- Total: Reduce training from 10 hours \u2192 1-2 hours, inference from 50ms \u2192 2-5ms per die\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Real-world project templates for semiconductor applications! \ud83c\udfaf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c059dc",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Real-World Projects\n",
    "\n",
    "### **Semiconductor Post-Silicon Validation Projects**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 1: Wafer Yield Predictor with Production Deployment**\n",
    "\n",
    "**Objective:** Build end-to-end system predicting die-level yield from parametric tests with real-time inference.\n",
    "\n",
    "**Business Value:** $50M-$200M/year through early failure detection and scrap reduction.\n",
    "\n",
    "**Dataset:**\n",
    "- 500K+ die samples from wafer test STDF files\n",
    "- 50-100 parametric features (Vdd, Idd, frequency, power, temperature)\n",
    "- Binary target: pass/fail or multi-class binning (Bin 1, 2, 3, ...)\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input(100) \u2192 Dense(256, ReLU) + BatchNorm + Dropout(0.3)\n",
    "           \u2192 Dense(128, ReLU) + BatchNorm + Dropout(0.2)\n",
    "           \u2192 Dense(64, ReLU)\n",
    "           \u2192 Output(1, Sigmoid) or Output(n_bins, Softmax)\n",
    "```\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. **Data pipeline:** Extract STDF files \u2192 Pandas DataFrame \u2192 Feature engineering (mean, std, percentiles per wafer)\n",
    "2. **Model training:** PyTorch with mixed precision, 4-GPU distributed training\n",
    "3. **Optimization:** INT8 quantization, ONNX export\n",
    "4. **Deployment:** TorchServe or ONNX Runtime with GPU, REST API for real-time inference\n",
    "5. **Monitoring:** TensorBoard for training, Prometheus + Grafana for production metrics\n",
    "\n",
    "**Success Metrics:**\n",
    "- Accuracy \u226595%, AUC-ROC \u22650.98\n",
    "- Inference time <10ms per die (50K+ die/hour throughput)\n",
    "- False positive rate <1% (minimize bad dies shipped)\n",
    "- False negative rate <2% (minimize good dies scrapped)\n",
    "\n",
    "**Challenges:**\n",
    "- Class imbalance (95% pass, 5% fail) \u2192 Use weighted loss, SMOTE, focal loss\n",
    "- Spatial correlation (neighboring die fail together) \u2192 Add spatial features (die_x, die_y, distance to wafer edge)\n",
    "- Real-time constraints \u2192 Batch inference, GPU acceleration, model pruning\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 2: Defect Pattern Classification on Wafer Maps**\n",
    "\n",
    "**Objective:** Classify defect types from 2D wafer maps (spatial fail patterns) using CNNs.\n",
    "\n",
    "**Business Value:** $5M-$20M per incident through faster root cause analysis (reduce time from days to hours).\n",
    "\n",
    "**Dataset:**\n",
    "- 10K+ wafer maps (300\u00d7300 pixel images, each pixel = one die)\n",
    "- 20+ defect classes (ring, scratch, cluster, edge, random, normal)\n",
    "- Imbalanced: 70% normal, 30% defects\n",
    "\n",
    "**Architecture (CNN):**\n",
    "```\n",
    "Input(300, 300, 1) \u2192 Conv2D(32, 3\u00d73) + BatchNorm + ReLU + MaxPool(2\u00d72)\n",
    "                   \u2192 Conv2D(64, 3\u00d73) + BatchNorm + ReLU + MaxPool(2\u00d72)\n",
    "                   \u2192 Conv2D(128, 3\u00d73) + BatchNorm + ReLU + MaxPool(2\u00d72)\n",
    "                   \u2192 Flatten \u2192 Dense(256) + Dropout(0.5)\n",
    "                   \u2192 Output(20, Softmax)\n",
    "```\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. **Data augmentation:** Rotation, flip, zoom (wafer maps can be rotated)\n",
    "2. **Transfer learning:** Start with ResNet-50 pre-trained on ImageNet, fine-tune on wafer maps\n",
    "3. **Framework choice:** TensorFlow/Keras (better for image tasks, TF Lite for edge deployment)\n",
    "4. **Deployment:** TensorFlow Serving + Docker, or TF Lite on edge devices (inspection stations)\n",
    "\n",
    "**Success Metrics:**\n",
    "- Top-1 accuracy \u226598% (20-class classification)\n",
    "- Inference time <100ms per wafer (real-time inspection)\n",
    "- Precision \u226595% (minimize false alarms)\n",
    "\n",
    "**Enhancements:**\n",
    "- **Ensemble:** Combine multiple models (ResNet, EfficientNet, Vision Transformer)\n",
    "- **Explainability:** Use Grad-CAM to visualize which regions triggered classification\n",
    "- **Active learning:** Human-in-the-loop for edge cases\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 3: Adaptive Test Insertion with Reinforcement Learning**\n",
    "\n",
    "**Objective:** Dynamically optimize test sequence to minimize test time while maintaining 99%+ coverage.\n",
    "\n",
    "**Business Value:** $10M-$50M/year through 30-50% test time reduction (1-2 seconds per device \u00d7 100M devices/year).\n",
    "\n",
    "**Problem Formulation (RL):**\n",
    "- **State:** Current test results (pass/fail for tests already run), device features\n",
    "- **Action:** Which test to run next (from 100+ available tests)\n",
    "- **Reward:** -1 per test run, +100 if defect found early, -1000 if shipped with defect\n",
    "- **Goal:** Learn policy to minimize tests while catching all defects\n",
    "\n",
    "**Architecture (Policy Network):**\n",
    "```\n",
    "Input(state_dim) \u2192 Dense(256, ReLU) \u2192 Dense(128, ReLU) \n",
    "                 \u2192 Output(num_actions, Softmax)  # Action probabilities\n",
    "```\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. **Environment:** Simulate test flow using historical STDF data\n",
    "2. **Algorithm:** Proximal Policy Optimization (PPO) in PyTorch\n",
    "3. **Training:** 4-GPU distributed training, 10M episodes\n",
    "4. **Deployment:** Export policy network to ONNX, <10ms inference for real-time test selection\n",
    "\n",
    "**Success Metrics:**\n",
    "- Test time reduction: 30-50%\n",
    "- Defect coverage maintained: \u226599%\n",
    "- Escape rate (defects shipped): <0.1%\n",
    "\n",
    "**Challenges:**\n",
    "- Sparse rewards \u2192 Use reward shaping (intermediate rewards for partial progress)\n",
    "- Off-policy learning \u2192 Importance sampling to reuse historical data\n",
    "- Generalization \u2192 Train on multiple device families, test on new devices\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 4: Power Anomaly Detection with Autoencoders**\n",
    "\n",
    "**Objective:** Detect abnormal power consumption patterns indicating potential failures.\n",
    "\n",
    "**Business Value:** $2M-$10M/year through early detection of reliability issues (avoid field failures).\n",
    "\n",
    "**Dataset:**\n",
    "- 100K+ devices with power measurements (dynamic, static, leakage) across 10+ voltage/frequency conditions\n",
    "- Unlabeled data (95% normal, 5% anomalies - unknown types)\n",
    "\n",
    "**Architecture (Autoencoder):**\n",
    "```\n",
    "Encoder: Input(50) \u2192 Dense(32, ReLU) \u2192 Dense(16, ReLU) \u2192 Latent(8)\n",
    "Decoder: Latent(8) \u2192 Dense(16, ReLU) \u2192 Dense(32, ReLU) \u2192 Output(50)\n",
    "```\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. **Training:** Reconstruction loss (MSE) on normal devices only\n",
    "2. **Anomaly detection:** Devices with high reconstruction error = anomalies\n",
    "3. **Framework:** PyTorch or Keras (both work well for autoencoders)\n",
    "4. **Deployment:** ONNX Runtime for batch inference on test data\n",
    "\n",
    "**Success Metrics:**\n",
    "- Anomaly detection rate \u226590% (recall)\n",
    "- False positive rate <5% (precision)\n",
    "- Inference time <5ms per device\n",
    "\n",
    "**Enhancements:**\n",
    "- **Variational Autoencoder (VAE):** Better generalization, probabilistic latent space\n",
    "- **Time-series:** If power measured over time, use LSTM autoencoder\n",
    "- **Clustering:** Use latent representations for anomaly clustering (identify failure modes)\n",
    "\n",
    "---\n",
    "\n",
    "### **General AI/ML Projects**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 5: Customer Churn Prediction (Telecom/SaaS)**\n",
    "\n",
    "**Objective:** Predict which customers will cancel subscription in next 30 days.\n",
    "\n",
    "**Dataset:** 100K+ customers with features (usage, support tickets, payments, demographics), binary target (churn/no-churn).\n",
    "\n",
    "**Architecture:** Same as semiconductor yield predictor (MLP with BatchNorm + Dropout).\n",
    "\n",
    "**Framework:** Keras (quick prototyping), PyTorch (custom loss functions for imbalanced data).\n",
    "\n",
    "**Business Value:** $500K-$5M/year through targeted retention campaigns (reduce churn by 10-20%).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 6: Fraud Detection (Financial Services)**\n",
    "\n",
    "**Objective:** Real-time detection of fraudulent transactions.\n",
    "\n",
    "**Dataset:** 1M+ transactions with features (amount, merchant, time, location, user history), binary target (fraud/legitimate), highly imbalanced (0.1% fraud).\n",
    "\n",
    "**Architecture:** Deep MLP with attention mechanism to focus on suspicious patterns.\n",
    "\n",
    "**Framework:** PyTorch (custom training loop for handling imbalance), ONNX deployment for <10ms inference.\n",
    "\n",
    "**Business Value:** $10M-$100M/year through fraud prevention.\n",
    "\n",
    "**Challenges:** \n",
    "- Extreme imbalance \u2192 Focal loss, cost-sensitive learning\n",
    "- Real-time \u2192 Model compression, GPU inference\n",
    "- Concept drift \u2192 Online learning, periodic retraining\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 7: Medical Image Diagnosis (Healthcare)**\n",
    "\n",
    "**Objective:** Classify chest X-rays into normal/pneumonia/COVID-19.\n",
    "\n",
    "**Dataset:** 50K+ X-ray images (256\u00d7256 pixels), 3-class target.\n",
    "\n",
    "**Architecture:** Transfer learning with EfficientNet-B7 or Vision Transformer.\n",
    "\n",
    "**Framework:** TensorFlow/Keras (better image preprocessing, data augmentation), TF Serving for deployment.\n",
    "\n",
    "**Business Value:** $1M-$10M/year through faster diagnosis (reduce radiologist workload).\n",
    "\n",
    "**Success Metrics:** AUC-ROC \u22650.95, sensitivity \u226590% (minimize false negatives).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 8: Predictive Maintenance (Manufacturing)**\n",
    "\n",
    "**Objective:** Predict equipment failure 7 days in advance from sensor data.\n",
    "\n",
    "**Dataset:** Time-series sensor data (temperature, vibration, pressure) from 100+ machines, binary target (failure/normal).\n",
    "\n",
    "**Architecture:** LSTM or Transformer for time-series modeling.\n",
    "\n",
    "**Framework:** PyTorch (better for RNNs and custom architectures), TensorFlow (TF Lite for edge deployment on machines).\n",
    "\n",
    "**Business Value:** $5M-$50M/year through reduced downtime (prevent unplanned outages).\n",
    "\n",
    "**Challenges:**\n",
    "- Variable-length sequences \u2192 Padding or dynamic RNNs\n",
    "- Rare failures \u2192 Synthetic data generation, transfer learning from similar machines\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd11 Key Takeaways\n",
    "\n",
    "### **Framework Selection Decision Tree**\n",
    "\n",
    "```\n",
    "START\n",
    "  \u2193\n",
    "Are you doing research/experimentation?\n",
    "  YES \u2192 PyTorch (dynamic graphs, easier debugging)\n",
    "  NO \u2193\n",
    "Do you need mobile/edge deployment?\n",
    "  YES \u2192 TensorFlow/Keras (TF Lite mature)\n",
    "  NO \u2193\n",
    "Do you need custom training loops (RL, GANs)?\n",
    "  YES \u2192 PyTorch (more control)\n",
    "  NO \u2193\n",
    "Quick prototype for business stakeholders?\n",
    "  YES \u2192 Keras (fit() API, faster development)\n",
    "  NO \u2193\n",
    "Either framework works \u2192 Choose based on team expertise\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Production Deployment Checklist**\n",
    "\n",
    "- \u2705 **Model format:** ONNX (framework-agnostic)\n",
    "- \u2705 **Optimization:** Quantization (INT8), pruning, TorchScript/TF graph compilation\n",
    "- \u2705 **Serving:** TorchServe, TF Serving, or ONNX Runtime with REST API\n",
    "- \u2705 **Monitoring:** Log predictions, latency, errors (Prometheus + Grafana)\n",
    "- \u2705 **Versioning:** Model registry (MLflow, DVC), A/B testing for new models\n",
    "- \u2705 **Scaling:** Kubernetes for auto-scaling, GPU pools for burst traffic\n",
    "- \u2705 **Fallback:** Simpler model as backup if GPU fails\n",
    "\n",
    "---\n",
    "\n",
    "### **PyTorch vs Keras: Final Comparison**\n",
    "\n",
    "| Aspect | PyTorch | TensorFlow/Keras | Recommendation |\n",
    "|--------|---------|------------------|----------------|\n",
    "| **Learning Curve** | Moderate | Easy (Keras) | Keras for beginners |\n",
    "| **Research** | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50 | PyTorch dominant |\n",
    "| **Production** | \u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50\u2b50 | TensorFlow mature |\n",
    "| **Mobile/Edge** | \u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50\u2b50 | TF Lite superior |\n",
    "| **Debugging** | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50 | PyTorch easier |\n",
    "| **Training Speed** | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50\u2b50 | Similar |\n",
    "| **Community** | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50\u2b50 | Both excellent |\n",
    "| **Semiconductor** | \u2b50\u2b50\u2b50\u2b50\u2b50 | \u2b50\u2b50\u2b50\u2b50\u2b50 | Use both |\n",
    "\n",
    "---\n",
    "\n",
    "### **Best Practices for Semiconductor Testing**\n",
    "\n",
    "1. **Training:** Use PyTorch with mixed precision + multi-GPU for flexibility\n",
    "2. **Inference:** Convert to ONNX, deploy with ONNX Runtime + GPU for speed\n",
    "3. **Edge:** Use TF Lite for test equipment with limited resources\n",
    "4. **Monitoring:** Log all predictions for continuous model evaluation\n",
    "5. **Retraining:** Set up pipelines to retrain monthly as new data arrives\n",
    "\n",
    "---\n",
    "\n",
    "### **Learning Path Forward**\n",
    "\n",
    "**After mastering frameworks:**\n",
    "- \ud83d\udcd8 **Notebook 053:** Convolutional Neural Networks (CNNs) for image data\n",
    "- \ud83d\udcd8 **Notebook 054:** Recurrent Neural Networks (RNNs) for time-series\n",
    "- \ud83d\udcd8 **Notebook 055:** Transformers and Attention Mechanisms\n",
    "- \ud83d\udcd8 **Notebook 056:** Generative Models (GANs, VAEs)\n",
    "- \ud83d\udcd8 **Notebook 057:** Reinforcement Learning Fundamentals\n",
    "\n",
    "**Production skills:**\n",
    "- \ud83d\ude80 **MLOps:** CI/CD for ML, model versioning, A/B testing\n",
    "- \ud83d\ude80 **Monitoring:** Model drift detection, performance tracking\n",
    "- \ud83d\ude80 **Scaling:** Distributed training, model compression, batch inference\n",
    "\n",
    "---\n",
    "\n",
    "## \u2705 Learning Objectives Review\n",
    "\n",
    "By now, you should be able to:\n",
    "- \u2705 Build neural networks in both PyTorch and TensorFlow/Keras\n",
    "- \u2705 Choose the right framework based on project requirements\n",
    "- \u2705 Train models efficiently with GPU acceleration and mixed precision\n",
    "- \u2705 Deploy models to production using TorchServe, TF Serving, or ONNX Runtime\n",
    "- \u2705 Optimize inference for real-time applications (quantization, ONNX, batching)\n",
    "- \u2705 Apply frameworks to semiconductor testing with production-grade implementations\n",
    "- \u2705 Debug and profile models to identify bottlenecks\n",
    "- \u2705 Convert models between frameworks using ONNX\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf93 Congratulations!\n",
    "\n",
    "You've mastered deep learning frameworks! You can now:\n",
    "- Build production-ready models in PyTorch and Keras\n",
    "- Deploy to any platform (cloud, edge, mobile) via ONNX\n",
    "- Optimize for real-world constraints (latency, memory, cost)\n",
    "- Apply to semiconductor testing with confidence\n",
    "\n",
    "**Next steps:** Dive into specialized architectures (CNNs, RNNs, Transformers) in upcoming notebooks! \ud83d\ude80\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcda Resources\n",
    "\n",
    "**Official Documentation:**\n",
    "- PyTorch: https://pytorch.org/docs/\n",
    "- TensorFlow: https://www.tensorflow.org/guide\n",
    "- ONNX: https://onnx.ai/\n",
    "\n",
    "**Tutorials:**\n",
    "- PyTorch Tutorials: https://pytorch.org/tutorials/\n",
    "- TensorFlow Tutorials: https://www.tensorflow.org/tutorials\n",
    "- ONNX Runtime: https://onnxruntime.ai/docs/\n",
    "\n",
    "**Books:**\n",
    "- *Deep Learning with PyTorch* (Stevens, Antiga)\n",
    "- *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* (G\u00e9ron)\n",
    "\n",
    "**Communities:**\n",
    "- PyTorch Forums: https://discuss.pytorch.org/\n",
    "- TensorFlow Forums: https://www.tensorflow.org/community\n",
    "- Reddit: r/MachineLearning, r/deeplearning\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Complete!** \ud83c\udf89"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}