{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a5b8bfd",
   "metadata": {},
   "source": [
    "# 054: Transfer Learning & Fine-Tuning",
    "",
    "## \ud83d\udcda Learning Objectives",
    "",
    "By the end of this notebook, you will master:",
    "",
    "1. **Transfer Learning Theory** - Mathematical foundations, feature hierarchy, domain adaptation",
    "2. **Pre-trained Model Zoo** - ImageNet models (ResNet, EfficientNet, ViT), when to use each",
    "3. **Fine-Tuning Strategies** - Layer-wise learning rates, gradual unfreezing, discriminative training",
    "4. **Learning Rate Policies** - Warm-up, cyclical LR, cosine annealing for transfer learning",
    "5. **Feature Extraction vs Fine-Tuning** - When to freeze, when to train, computational tradeoffs",
    "6. **Domain Adaptation** - Handling distribution shift between source (ImageNet) and target (semiconductor)",
    "7. **Multi-Task Transfer Learning** - Leveraging multiple pre-trained models simultaneously",
    "8. **Production Deployment** - Model compression, quantization, ONNX export for transfer learned models",
    "",
    "---",
    "",
    "## \ud83c\udfaf Why Transfer Learning Matters",
    "",
    "### The Core Problem",
    "Training deep neural networks from scratch requires:",
    "- **Massive datasets** (millions of labeled examples like ImageNet's 14M images)",
    "- **Computational resources** (hundreds of GPU-hours, $1000s in cloud costs)",
    "- **Time** (days to weeks of training)",
    "- **Expertise** (hyperparameter tuning, regularization, debugging)",
    "",
    "### The Transfer Learning Solution",
    "Leverage **pre-trained models** trained on large datasets (ImageNet, COCO, etc.) and adapt them to your specific task:",
    "- **10-100\u00d7 less data** needed (1000s instead of millions)",
    "- **10-100\u00d7 faster training** (minutes to hours instead of days)",
    "- **Better generalization** (pre-trained features capture universal patterns)",
    "- **Lower costs** (10\u00d7 reduction in compute requirements)",
    "",
    "---",
    "",
    "## \ud83d\udcbc Business Value for Semiconductor Industry",
    "",
    "### Use Case 1: Wafer Defect Classification with Limited Data",
    "**Problem:** New fab produces novel defect patterns. Only 500 labeled wafer maps available (insufficient for training from scratch).",
    "",
    "**Solution:** Transfer learning from ImageNet \u2192 Fine-tune on 500 wafer maps",
    "- **Result:** 92% accuracy (vs 65% training from scratch)",
    "- **Business Impact:** $5M-$10M/year in yield improvement",
    "- **Time Saved:** 2 weeks training \u2192 4 hours fine-tuning",
    "",
    "### Use Case 2: Multi-Product Test Adaptation",
    "**Problem:** Company produces 50+ IC products. Need separate yield models for each product family.",
    "",
    "**Solution:** Train base model on Product A (largest dataset) \u2192 Transfer to Products B-Z",
    "- **Result:** 15% improvement in cross-product generalization",
    "- **Business Impact:** $20M-$50M/year from optimized test flows across portfolio",
    "- **Cost Reduction:** Train 1 base model instead of 50 separate models",
    "",
    "### Use Case 3: SEM Image Defect Detection",
    "**Problem:** High-resolution SEM images (4096\u00d74096) of die defects. Limited labeled data due to expert annotation cost.",
    "",
    "**Solution:** EfficientNet-B7 (ImageNet pre-trained) \u2192 Fine-tune on 2000 SEM images",
    "- **Result:** 96% mAP for 30 defect classes",
    "- **Business Impact:** $2M-$8M/year in faster defect root-cause analysis",
    "- **Annotation Savings:** $50K-$200K (need 10\u00d7 fewer labeled images)",
    "",
    "---",
    "",
    "## \ud83c\udfd7\ufe0f What We'll Build",
    "",
    "### 1. **Wafer Map Multi-Class Classifier** (Semiconductor Focus)",
    "- **Task:** Classify 20 defect patterns (center, edge, scratch, ring, cluster, etc.)",
    "- **Approach:** Compare 3 strategies:",
    "  1. Feature extraction (freeze all layers)",
    "  2. Fine-tuning last N layers",
    "  3. Gradual unfreezing (progressive layer-wise training)",
    "- **Models:** ResNet-50, EfficientNet-B3, Vision Transformer (ViT-B/16)",
    "- **Metrics:** Test accuracy, training time, parameter efficiency",
    "",
    "### 2. **Domain Adaptation Experiment** (Transfer ImageNet \u2192 Grayscale Wafer Maps)",
    "- **Challenge:** ImageNet = RGB natural images, Wafer maps = grayscale spatial patterns",
    "- **Solution:** Domain-specific preprocessing, adaptive batch normalization",
    "- **Analysis:** Quantify domain gap, measure adaptation effectiveness",
    "",
    "### 3. **Production Deployment Pipeline**",
    "- **Model compression:** Quantization (FP32 \u2192 INT8), pruning",
    "- **Export:** ONNX format for multi-framework compatibility",
    "- **Inference:** TensorRT optimization, batch processing",
    "- **Monitoring:** Track prediction confidence, detect distribution drift",
    "",
    "---",
    "",
    "## \ud83d\udcca Transfer Learning Workflow",
    "",
    "```mermaid",
    "graph TD",
    "    A[Pre-trained Model<br/>ImageNet 1000 classes] --> B{Transfer Strategy}",
    "    B -->|Feature Extraction| C[Freeze all layers<br/>Train only classifier]",
    "    B -->|Fine-Tuning| D[Unfreeze last N layers<br/>Train with low LR]",
    "    B -->|Gradual Unfreezing| E[Progressive layer training<br/>Start from top, unfreeze downward]",
    "    ",
    "    C --> F[Target Dataset<br/>Wafer Maps 20 classes]",
    "    D --> F",
    "    E --> F",
    "    ",
    "    F --> G[Validation]",
    "    G -->|Poor Performance| H{Diagnosis}",
    "    H -->|Underfitting| I[Unfreeze more layers<br/>Increase model capacity]",
    "    H -->|Overfitting| J[Freeze more layers<br/>Add regularization]",
    "    H -->|Domain Gap| K[Domain adaptation<br/>Data augmentation]",
    "    ",
    "    I --> F",
    "    J --> F",
    "    K --> F",
    "    ",
    "    G -->|Good Performance| L[Production Deployment]",
    "    L --> M[ONNX Export]",
    "    L --> N[INT8 Quantization]",
    "    L --> O[TensorRT Optimization]",
    "    ",
    "    M --> P[Inference Server<br/>TorchServe/TF Serving]",
    "    N --> P",
    "    O --> P",
    "    ",
    "    P --> Q[Monitoring & Feedback]",
    "    Q -->|Distribution Drift| R[Retrain/Adapt]",
    "    R --> F",
    "    ",
    "    style A fill:#e1f5ff",
    "    style F fill:#fff4e1",
    "    style L fill:#e8f5e9",
    "    style P fill:#f3e5f5",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb599f65",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udee0\ufe0f Notebook Structure\n",
    "\n",
    "1. **Mathematical Foundations** - Why transfer learning works, feature hierarchy theory\n",
    "2. **Pre-trained Model Comparison** - ResNet vs EfficientNet vs Vision Transformer\n",
    "3. **Strategy 1: Feature Extraction** - Freeze backbone, train classifier only\n",
    "4. **Strategy 2: Full Fine-Tuning** - Unfreeze all layers with differential learning rates\n",
    "5. **Strategy 3: Gradual Unfreezing** - Progressive layer-wise training (best practice)\n",
    "6. **Domain Adaptation Techniques** - Handle ImageNet \u2192 Semiconductor distribution shift\n",
    "7. **Production Deployment** - Model compression, ONNX export, TensorRT optimization\n",
    "8. **Real-World Projects** - 8 semiconductor + general AI/ML transfer learning applications\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udce6 Prerequisites\n",
    "\n",
    "**Libraries:**\n",
    "```bash\n",
    "pip install torch torchvision timm  # PyTorch + model zoo (timm = PyTorch Image Models)\n",
    "pip install tensorflow tensorflow-hub  # TensorFlow + TF Hub\n",
    "pip install onnx onnxruntime tensorrt  # Model export & optimization\n",
    "pip install matplotlib seaborn scikit-learn  # Visualization & metrics\n",
    "pip install grad-cam  # Explainability for transfer learned models\n",
    "```\n",
    "\n",
    "**Prior Knowledge:**\n",
    "- Notebook 052: Deep Learning Frameworks (PyTorch/Keras basics)\n",
    "- Notebook 053: CNN Architectures (convolution, ResNet, VGG concepts)\n",
    "- Basic understanding of overfitting, regularization\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcca Dataset Overview\n",
    "\n",
    "### Synthetic Wafer Map Dataset (20 Defect Classes)\n",
    "We'll generate **10,000 wafer maps** (128\u00d7128 grayscale images) with these defect patterns:\n",
    "\n",
    "| Class | Pattern | Frequency | Business Impact |\n",
    "|-------|---------|-----------|-----------------|\n",
    "| 0 | Normal (no defects) | 20% | Baseline |\n",
    "| 1 | Center cluster | 8% | Process contamination ($2M-$5M/incident) |\n",
    "| 2 | Edge defects | 10% | Chuck/vacuum issues ($500K-$2M) |\n",
    "| 3 | Vertical scratch | 6% | Handling damage ($1M-$3M) |\n",
    "| 4 | Horizontal scratch | 6% | Robot arm misalignment ($1M-$3M) |\n",
    "| 5 | Ring pattern | 5% | Plasma etching non-uniformity ($3M-$8M) |\n",
    "| 6 | Random clusters | 7% | Particle contamination ($1M-$4M) |\n",
    "| 7 | Localized defects | 6% | Lithography hotspot ($2M-$6M) |\n",
    "| 8 | Near-full wafer | 4% | Catastrophic process failure ($10M-$30M) |\n",
    "| 9 | Donut pattern | 5% | Temperature gradient ($2M-$5M) |\n",
    "| 10-19 | Mixed/complex patterns | 23% | Various root causes |\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Class imbalance:** Mimics real production (normal wafers most common, catastrophic failures rare)\n",
    "- **Spatial features:** Defects have geometric structure (vs random noise)\n",
    "- **Grayscale:** Unlike ImageNet (RGB), tests domain adaptation capability\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf93 Learning Strategy\n",
    "\n",
    "### Progressive Complexity\n",
    "1. **Start simple:** Feature extraction (easiest, fastest)\n",
    "2. **Add complexity:** Fine-tune last layers (moderate difficulty)\n",
    "3. **Optimize:** Gradual unfreezing with discriminative LR (advanced, best results)\n",
    "\n",
    "### Experimentation Framework\n",
    "For each strategy, we'll measure:\n",
    "- **Test accuracy** (primary metric)\n",
    "- **Training time** (efficiency)\n",
    "- **Number of trainable parameters** (computational cost)\n",
    "- **Overfitting behavior** (train vs validation curves)\n",
    "- **Inference speed** (production readiness)\n",
    "\n",
    "### Success Criteria\n",
    "- **Baseline (train from scratch):** ~75% accuracy, 2+ hours training\n",
    "- **Target (transfer learning):** \u226592% accuracy, <30 minutes training\n",
    "- **Production requirement:** <50ms inference per wafer map\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd17 How This Fits in the Learning Path\n",
    "\n",
    "**Previous Notebooks:**\n",
    "- 052: Deep Learning Frameworks \u2192 PyTorch/Keras fundamentals\n",
    "- 053: CNN Architectures \u2192 Convolution, ResNet, basic transfer learning\n",
    "\n",
    "**This Notebook (054):**\n",
    "- **Advanced transfer learning strategies** (layer-wise LR, gradual unfreezing)\n",
    "- **Multiple model families** (ResNet, EfficientNet, Vision Transformer)\n",
    "- **Domain adaptation** (ImageNet \u2192 semiconductor)\n",
    "- **Production deployment** (compression, optimization)\n",
    "\n",
    "**Next Notebooks:**\n",
    "- 055: Object Detection (YOLO, R-CNN) \u2192 Localize defects, not just classify\n",
    "- 056: RNN/LSTM \u2192 Sequential test pattern analysis\n",
    "- 057: Seq2Seq & Attention \u2192 Foundation for Transformers\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\ude80 Let's Begin!\n",
    "\n",
    "We'll start with the mathematical foundations of transfer learning, then systematically compare three fine-tuning strategies on our wafer map dataset.\n",
    "\n",
    "---\n",
    "\n",
    "# \ud83d\udcd0 Part 1: Mathematical Foundations of Transfer Learning\n",
    "\n",
    "## \ud83e\uddee Why Does Transfer Learning Work?\n",
    "\n",
    "### The Feature Hierarchy Hypothesis\n",
    "\n",
    "Deep neural networks learn a **hierarchy of features** from low-level to high-level:\n",
    "\n",
    "```\n",
    "Layer 1 (Early):  Edge detectors, Gabor filters, color blobs\n",
    "                  \u2193 (Generic, universal patterns)\n",
    "Layer 2-3:        Textures, simple shapes, gradients\n",
    "                  \u2193 (Semi-generic, useful across domains)\n",
    "Layer 4-5:        Object parts (wheels, eyes, corners)\n",
    "                  \u2193 (Domain-specific but transferable)\n",
    "Final Layers:     Complete objects (cats, dogs, cars)\n",
    "                  \u2193 (Task-specific, must be retrained)\n",
    "Classification:   1000 ImageNet classes \u2192 20 wafer defect classes\n",
    "```\n",
    "\n",
    "**Key Insight:** Early layers learn **universal features** (edges, textures) that transfer across domains. Only final layers need retraining for new tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "**Source Domain (ImageNet):**\n",
    "- Input: $X_s \\in \\mathbb{R}^{224 \\times 224 \\times 3}$ (RGB images)\n",
    "- Labels: $Y_s \\in \\{1, 2, \\ldots, 1000\\}$ (1000 ImageNet classes)\n",
    "- Distribution: $P_s(X_s, Y_s)$ (natural images of animals, objects, scenes)\n",
    "\n",
    "**Target Domain (Semiconductor Wafer Maps):**\n",
    "- Input: $X_t \\in \\mathbb{R}^{128 \\times 128 \\times 1}$ (grayscale wafer maps)\n",
    "- Labels: $Y_t \\in \\{1, 2, \\ldots, 20\\}$ (20 defect patterns)\n",
    "- Distribution: $P_t(X_t, Y_t)$ (spatial defect patterns)\n",
    "\n",
    "**Transfer Learning Goal:**\n",
    "Leverage knowledge from $P_s$ to improve performance on $P_t$ despite:\n",
    "1. **Input distribution shift:** $P_s(X_s) \\neq P_t(X_t)$ (RGB vs grayscale, natural vs spatial)\n",
    "2. **Label space mismatch:** $Y_s \\neq Y_t$ (1000 classes vs 20 classes)\n",
    "3. **Limited target data:** $|D_t| \\ll |D_s|$ (10K wafer maps vs 14M ImageNet images)\n",
    "\n",
    "---\n",
    "\n",
    "### The Transfer Learning Assumption\n",
    "\n",
    "**Assumption:** There exists a shared feature representation $\\phi: X \\to \\mathbb{R}^d$ such that:\n",
    "\n",
    "$$\n",
    "\\phi(X_s) \\approx \\phi(X_t)\n",
    "$$\n",
    "\n",
    "In other words, the **intermediate feature representations** learned on ImageNet are useful for wafer map classification.\n",
    "\n",
    "**Validation of Assumption:**\n",
    "- \u2705 Both tasks involve 2D spatial pattern recognition\n",
    "- \u2705 Low-level features (edges, corners) are universal\n",
    "- \u2705 Mid-level features (textures, shapes) transfer across domains\n",
    "- \u26a0\ufe0f High-level features (object semantics) differ \u2192 **must retrain final layers**\n",
    "\n",
    "---\n",
    "\n",
    "### Three Transfer Learning Strategies\n",
    "\n",
    "Let's denote the pre-trained model as $f_\\theta = g_{\\theta_{\\text{head}}} \\circ h_{\\theta_{\\text{backbone}}}$:\n",
    "- $h_{\\theta_{\\text{backbone}}}$: Feature extractor (convolutional layers)\n",
    "- $g_{\\theta_{\\text{head}}}$: Classification head (fully-connected layers)\n",
    "\n",
    "#### **Strategy 1: Feature Extraction (Freeze Backbone)**\n",
    "\n",
    "**Approach:** Use pre-trained $h_{\\theta_{\\text{backbone}}}$ as fixed feature extractor, train only $g_{\\theta_{\\text{head}}}$.\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{backbone}} \\leftarrow \\text{frozen (no gradients)} \\\\\n",
    "\\theta_{\\text{head}} \\leftarrow \\text{trainable}\n",
    "$$\n",
    "\n",
    "**Optimization:**\n",
    "$$\n",
    "\\min_{\\theta_{\\text{head}}} \\sum_{(x,y) \\in D_t} \\mathcal{L}\\left(g_{\\theta_{\\text{head}}}(h_{\\theta_{\\text{backbone}}}(x)), y\\right)\n",
    "$$\n",
    "\n",
    "**Pros:**\n",
    "- \u2705 **Fastest training** (only 1-5% of parameters trainable)\n",
    "- \u2705 **No overfitting** (backbone parameters fixed)\n",
    "- \u2705 **Low memory** (no gradients for backbone)\n",
    "\n",
    "**Cons:**\n",
    "- \u274c **Limited adaptability** (backbone never adjusts to target domain)\n",
    "- \u274c **Suboptimal for large domain shift** (ImageNet features may not align with wafer maps)\n",
    "\n",
    "**When to Use:**\n",
    "- Target dataset very small (<1000 samples)\n",
    "- Target domain similar to source (e.g., ImageNet \u2192 other natural images)\n",
    "- Computational resources limited\n",
    "\n",
    "---\n",
    "\n",
    "#### **Strategy 2: Full Fine-Tuning (Unfreeze All Layers)**\n",
    "\n",
    "**Approach:** Unfreeze entire model, train all parameters with smaller learning rate.\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{backbone}} \\leftarrow \\text{trainable (low LR)} \\\\\n",
    "\\theta_{\\text{head}} \\leftarrow \\text{trainable (high LR)}\n",
    "$$\n",
    "\n",
    "**Discriminative Learning Rates:**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta_{\\text{head}}^{(t+1)} &\\leftarrow \\theta_{\\text{head}}^{(t)} - \\eta_{\\text{head}} \\cdot \\nabla_{\\theta_{\\text{head}}} \\mathcal{L} \\\\\n",
    "\\theta_{\\text{backbone}}^{(t+1)} &\\leftarrow \\theta_{\\text{backbone}}^{(t)} - \\eta_{\\text{backbone}} \\cdot \\nabla_{\\theta_{\\text{backbone}}} \\mathcal{L}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\eta_{\\text{head}} = 10 \\times \\eta_{\\text{backbone}}$ (head learns faster, backbone fine-tunes slowly).\n",
    "\n",
    "**Pros:**\n",
    "- \u2705 **Best accuracy** (model fully adapts to target domain)\n",
    "- \u2705 **Handles domain shift** (backbone adjusts to wafer map statistics)\n",
    "\n",
    "**Cons:**\n",
    "- \u274c **Overfitting risk** (especially with small datasets)\n",
    "- \u274c **Slower training** (all parameters update)\n",
    "- \u274c **High memory** (full backpropagation through entire network)\n",
    "\n",
    "**When to Use:**\n",
    "- Target dataset sufficiently large (>5000 samples)\n",
    "- Large domain shift (e.g., natural images \u2192 medical/industrial images)\n",
    "- Computational resources available\n",
    "\n",
    "---\n",
    "\n",
    "#### **Strategy 3: Gradual Unfreezing (Progressive Layer-Wise Training)**\n",
    "\n",
    "**Approach:** Start with feature extraction, then progressively unfreeze layers from top to bottom.\n",
    "\n",
    "**Phase 1 (Epochs 1-5):** Train classifier head only\n",
    "$$\n",
    "\\theta_{\\text{backbone}} \\leftarrow \\text{frozen}\n",
    "$$\n",
    "\n",
    "**Phase 2 (Epochs 6-10):** Unfreeze last block of backbone\n",
    "$$\n",
    "\\theta_{\\text{backbone, last block}} \\leftarrow \\text{trainable (low LR)}\n",
    "$$\n",
    "\n",
    "**Phase 3 (Epochs 11-15):** Unfreeze all backbone\n",
    "$$\n",
    "\\theta_{\\text{backbone, all}} \\leftarrow \\text{trainable (very low LR)}\n",
    "$$\n",
    "\n",
    "**Layer-Wise Learning Rates (Discriminative Fine-Tuning):**\n",
    "$$\n",
    "\\eta_{\\text{layer } i} = \\eta_{\\text{base}} \\times \\alpha^{L-i}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $L$: Total number of layers\n",
    "- $i$: Layer index (0 = first layer, $L$ = last layer)\n",
    "- $\\alpha \\in [0.5, 0.95]$: Decay factor (typical: 0.8)\n",
    "- Result: Early layers learn slower (preserve pre-trained features), late layers learn faster (adapt to new task)\n",
    "\n",
    "**Example (ResNet-50 with 50 layers, $\\eta_{\\text{base}} = 10^{-4}$, $\\alpha = 0.8$):**\n",
    "- Layer 1 (early): $\\eta_1 = 10^{-4} \\times 0.8^{49} \\approx 10^{-8}$ (nearly frozen)\n",
    "- Layer 25 (middle): $\\eta_{25} = 10^{-4} \\times 0.8^{25} \\approx 10^{-6}$\n",
    "- Layer 50 (classifier): $\\eta_{50} = 10^{-4} \\times 0.8^{0} = 10^{-4}$ (full learning rate)\n",
    "\n",
    "**Pros:**\n",
    "- \u2705 **Best of both worlds** (feature extraction stability + fine-tuning adaptability)\n",
    "- \u2705 **Reduced overfitting** (gradual adaptation prevents catastrophic forgetting)\n",
    "- \u2705 **Faster convergence** (each phase uses optimal LR for layer depth)\n",
    "\n",
    "**Cons:**\n",
    "- \u274c **Complex implementation** (requires layer-wise LR scheduling)\n",
    "- \u274c **Longer training** (multiple phases)\n",
    "\n",
    "**When to Use:**\n",
    "- **Best practice for most scenarios** (balances accuracy, stability, efficiency)\n",
    "- Target dataset moderate size (1000-10000 samples)\n",
    "- Production deployments (minimizes overfitting risk)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcca Feature Transferability Analysis\n",
    "\n",
    "**Question:** Which layers transfer well from ImageNet to semiconductor wafer maps?\n",
    "\n",
    "**Experimental Setup:**\n",
    "1. Train ResNet-50 on ImageNet (standard pre-training)\n",
    "2. Freeze layer $i$, train layers $i+1$ to $L$ on wafer maps\n",
    "3. Measure test accuracy for each $i$\n",
    "\n",
    "**Expected Results:**\n",
    "```\n",
    "Freeze Layer 0 (input):        ~75% accuracy (train from scratch)\n",
    "Freeze Layers 0-10 (early):    ~85% accuracy (low-level features transfer)\n",
    "Freeze Layers 0-30 (middle):   ~90% accuracy (mid-level textures transfer)\n",
    "Freeze Layers 0-45 (late):     ~92% accuracy (optimal balance)\n",
    "Freeze Layers 0-49 (all):      ~88% accuracy (backbone too rigid)\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "- **Layers 1-30:** Highly transferable (edges, textures, shapes)\n",
    "- **Layers 31-45:** Moderately transferable (need fine-tuning for wafer map specifics)\n",
    "- **Layers 46-50:** Task-specific (must retrain for 20 defect classes)\n",
    "\n",
    "**Takeaway:** Freeze early layers (preserve universal features), fine-tune late layers (adapt to semiconductor domain).\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Domain Adaptation: ImageNet \u2192 Semiconductor\n",
    "\n",
    "### Challenge: Distribution Shift\n",
    "\n",
    "**ImageNet Statistics:**\n",
    "- Mean: $\\mu_s = [0.485, 0.456, 0.406]$ (RGB channels)\n",
    "- Std: $\\sigma_s = [0.229, 0.224, 0.225]$\n",
    "- Color: Rich RGB information (natural scenes)\n",
    "\n",
    "**Wafer Map Statistics:**\n",
    "- Mean: $\\mu_t \\approx [0.15]$ (grayscale, mostly background/passing dies)\n",
    "- Std: $\\sigma_t \\approx [0.25]$ (defects = bright spots)\n",
    "- Color: Single-channel (no color information)\n",
    "\n",
    "**Problem:** Pre-trained BatchNorm layers have statistics from ImageNet distribution. Direct application causes **internal covariate shift**.\n",
    "\n",
    "### Solution 1: Input Preprocessing\n",
    "\n",
    "**Replicate Grayscale to 3 Channels:**\n",
    "$$\n",
    "X_{\\text{RGB}} = \\text{stack}(X_{\\text{gray}}, X_{\\text{gray}}, X_{\\text{gray}})\n",
    "$$\n",
    "\n",
    "**Normalize with ImageNet Statistics:**\n",
    "$$\n",
    "X_{\\text{normalized}} = \\frac{X_{\\text{RGB}} - \\mu_s}{\\sigma_s}\n",
    "$$\n",
    "\n",
    "**Pros:** Simple, preserves pre-trained weights exactly  \n",
    "**Cons:** Wastes computation (3 identical channels), doesn't fix BatchNorm statistics\n",
    "\n",
    "### Solution 2: Adaptive Batch Normalization\n",
    "\n",
    "**Freeze model, compute BatchNorm statistics on target domain:**\n",
    "\n",
    "```python\n",
    "model.train()  # Enable BatchNorm statistics update\n",
    "with torch.no_grad():  # Don't update weights\n",
    "    for x, _ in target_dataloader:\n",
    "        _ = model(x)  # Forward pass updates running_mean/running_var\n",
    "```\n",
    "\n",
    "**Effect:** BatchNorm layers adapt to $\\mu_t, \\sigma_t$ without changing convolutional filters.\n",
    "\n",
    "**Pros:** Fast (one pass through dataset), no weight updates needed  \n",
    "**Cons:** Limited improvement (BatchNorm alone doesn't fix feature misalignment)\n",
    "\n",
    "### Solution 3: Domain-Specific Data Augmentation\n",
    "\n",
    "**Standard ImageNet Augmentations (DON'T USE for wafer maps):**\n",
    "- Color jitter \u274c (wafer maps are grayscale)\n",
    "- Hue shifts \u274c (no color information)\n",
    "- Vertical flips \u274c (breaks top-edge vs bottom-edge defect distinction)\n",
    "\n",
    "**Wafer-Map-Specific Augmentations (RECOMMENDED):**\n",
    "- **Rotation 0-360\u00b0** \u2705 (wafers have rotational symmetry)\n",
    "- **Horizontal flips** \u2705 (left-edge = right-edge defects)\n",
    "- **Gaussian noise** \u2705 (mimics sensor noise)\n",
    "- **Brightness scaling** \u2705 (different test equipment sensitivities)\n",
    "- **Elastic deformation** \u2705 (mimics wafer warping)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcda Pre-Trained Model Zoo Comparison\n",
    "\n",
    "### Models We'll Use\n",
    "\n",
    "| Model | Parameters | ImageNet Top-1 | Speed (GPU) | When to Use |\n",
    "|-------|------------|----------------|-------------|-------------|\n",
    "| **ResNet-50** | 25.6M | 76.2% | 60 FPS | Baseline, proven architecture, semiconductor standard |\n",
    "| **EfficientNet-B3** | 12.0M | 81.6% | 45 FPS | **Best accuracy/params tradeoff**, mobile-friendly |\n",
    "| **Vision Transformer (ViT-B/16)** | 86.6M | 84.5% | 30 FPS | Cutting-edge, attention-based, data-hungry |\n",
    "\n",
    "**Source:** [PyTorch Image Models (timm)](https://github.com/huggingface/pytorch-image-models) - 700+ pre-trained models\n",
    "\n",
    "---\n",
    "\n",
    "### ResNet-50 (Residual Networks)\n",
    "\n",
    "**Architecture:**\n",
    "- 50 layers (48 convolutional + 1 max pool + 1 avg pool)\n",
    "- **Skip connections:** $y = F(x) + x$ (solve vanishing gradient problem)\n",
    "- 4 residual blocks (conv3_x, conv4_x, conv5_x, conv6_x)\n",
    "\n",
    "**Transfer Learning Strategy:**\n",
    "- Freeze: Blocks 1-3 (early feature extraction)\n",
    "- Fine-tune: Block 4 + classifier (task-specific adaptation)\n",
    "\n",
    "**Semiconductor Use Case:** Industry standard for wafer map classification (proven reliability).\n",
    "\n",
    "---\n",
    "\n",
    "### EfficientNet-B3 (Compound Scaling)\n",
    "\n",
    "**Architecture:**\n",
    "- Inverted residual blocks (MobileNetV2-style)\n",
    "- **Squeeze-and-Excitation (SE) blocks:** Channel attention ($y = x \\odot \\sigma(W_2 \\text{ReLU}(W_1 \\text{GAP}(x)))$)\n",
    "- Compound scaling: Simultaneously scale depth, width, resolution\n",
    "\n",
    "**Transfer Learning Strategy:**\n",
    "- Freeze: Blocks 1-5 (efficient feature pyramid)\n",
    "- Fine-tune: Blocks 6-7 + classifier\n",
    "\n",
    "**Semiconductor Use Case:** **Recommended for production** (2\u00d7 fewer parameters than ResNet, 5% higher accuracy).\n",
    "\n",
    "---\n",
    "\n",
    "### Vision Transformer (ViT-B/16)\n",
    "\n",
    "**Architecture:**\n",
    "- Patch embedding: Split image into 16\u00d716 patches \u2192 Flatten \u2192 Linear projection\n",
    "- Transformer encoder: 12 layers of multi-head self-attention\n",
    "- No convolution (purely attention-based)\n",
    "\n",
    "**Transfer Learning Strategy:**\n",
    "- Freeze: Patch embedding + first 10 transformer blocks\n",
    "- Fine-tune: Last 2 transformer blocks + classifier\n",
    "\n",
    "**Semiconductor Use Case:** **Experimental** (requires more data, slower inference, but captures long-range spatial dependencies).\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison on Wafer Map Task (Predicted Results)\n",
    "\n",
    "| Metric | ResNet-50 | EfficientNet-B3 | ViT-B/16 |\n",
    "|--------|-----------|-----------------|----------|\n",
    "| **Test Accuracy** | 91.5% | **93.2%** | 92.8% |\n",
    "| **Training Time (20 epochs)** | 25 min | **18 min** | 45 min |\n",
    "| **Inference Speed (batch=32)** | 60 FPS | **80 FPS** | 30 FPS |\n",
    "| **GPU Memory** | 4.2 GB | **2.8 GB** | 8.5 GB |\n",
    "| **Model Size (FP32)** | 98 MB | **46 MB** | 330 MB |\n",
    "\n",
    "**Winner for Semiconductor:** **EfficientNet-B3** (best accuracy, fastest inference, smallest model size).\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Selection Guide\n",
    "\n",
    "**Choose ResNet-50 if:**\n",
    "- \u2705 First time implementing transfer learning (simplest architecture)\n",
    "- \u2705 Need industry-standard baseline (proven in semiconductor)\n",
    "- \u2705 Debugging model (easier to interpret residual blocks)\n",
    "\n",
    "**Choose EfficientNet-B3 if:**\n",
    "- \u2705 **Production deployment** (best efficiency)\n",
    "- \u2705 Edge device inference (smallest model size)\n",
    "- \u2705 Limited GPU memory (2-3\u00d7 less memory than ResNet)\n",
    "\n",
    "**Choose Vision Transformer if:**\n",
    "- \u2705 Research project (cutting-edge architecture)\n",
    "- \u2705 Very large dataset (>50K wafer maps, ViT needs more data)\n",
    "- \u2705 Interpretability needed (attention maps show \"what model looks at\")\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd2c Learning Rate Policies for Transfer Learning\n",
    "\n",
    "### Standard Learning Rate (NOT OPTIMAL for Transfer Learning)\n",
    "\n",
    "**Constant LR:**\n",
    "$$\n",
    "\\eta(t) = \\eta_0 = 10^{-3}\n",
    "$$\n",
    "\n",
    "**Problem:** \n",
    "- Pre-trained weights already near optimal \u2192 High LR causes **catastrophic forgetting**\n",
    "- Random classifier head needs higher LR to converge \u2192 Low LR too slow\n",
    "\n",
    "---\n",
    "\n",
    "### Discriminative Learning Rates (RECOMMENDED)\n",
    "\n",
    "**Layer-wise LR scaling:**\n",
    "$$\n",
    "\\eta_{\\text{layer } i} = \\eta_{\\text{base}} \\times \\text{scale}^{i}\n",
    "$$\n",
    "\n",
    "**PyTorch Implementation:**\n",
    "```python\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.backbone.layer1.parameters(), 'lr': 1e-5},  # Early layers: very low LR\n",
    "    {'params': model.backbone.layer2.parameters(), 'lr': 5e-5},\n",
    "    {'params': model.backbone.layer3.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.backbone.layer4.parameters(), 'lr': 5e-4},  # Late layers: moderate LR\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-3}        # Classifier: high LR\n",
    "], lr=1e-4)  # Base LR (overridden by param group LRs)\n",
    "```\n",
    "\n",
    "**Effect:** Early layers preserve ImageNet features (small updates), late layers adapt to wafer maps (large updates).\n",
    "\n",
    "---\n",
    "\n",
    "### Warm-Up + Cosine Annealing (BEST PRACTICE)\n",
    "\n",
    "**Phase 1: Linear Warm-Up (Epochs 0-2)**\n",
    "$$\n",
    "\\eta(t) = \\eta_{\\text{max}} \\times \\frac{t}{T_{\\text{warmup}}}\n",
    "$$\n",
    "\n",
    "**Phase 2: Cosine Annealing (Epochs 3-20)**\n",
    "$$\n",
    "\\eta(t) = \\eta_{\\text{min}} + \\frac{1}{2}(\\eta_{\\text{max}} - \\eta_{\\text{min}})\\left(1 + \\cos\\left(\\frac{t - T_{\\text{warmup}}}{T_{\\text{max}} - T_{\\text{warmup}}} \\pi\\right)\\right)\n",
    "$$\n",
    "\n",
    "**PyTorch Implementation:**\n",
    "```python\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "\n",
    "warmup_scheduler = LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=2)\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=18, eta_min=1e-6)\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[2])\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- **Warm-up:** Prevents early divergence (random classifier head needs gentle start)\n",
    "- **Cosine decay:** Smooth convergence to optimum (avoids oscillations near minimum)\n",
    "\n",
    "**Results:**\n",
    "- +2-3% accuracy vs constant LR\n",
    "- Faster convergence (reaches 90% in 10 epochs vs 15 epochs)\n",
    "\n",
    "---\n",
    "\n",
    "### Cyclical Learning Rates (Alternative)\n",
    "\n",
    "**Triangular cycle:**\n",
    "$$\n",
    "\\eta(t) = \\eta_{\\text{min}} + (\\eta_{\\text{max}} - \\eta_{\\text{min}}) \\times \\max\\left(0, 1 - \\frac{|t \\mod (2 \\times \\text{cycle}\\_\\text{length}) - \\text{cycle}\\_\\text{length}|}{\\text{cycle}\\_\\text{length}}\\right)\n",
    "$$\n",
    "\n",
    "**PyTorch Implementation:**\n",
    "```python\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "scheduler = CyclicLR(optimizer, base_lr=1e-5, max_lr=1e-3, step_size_up=500, mode='triangular2')\n",
    "```\n",
    "\n",
    "**Use Case:** Helps escape local minima, useful when fine-tuning gets stuck.\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83e\uddea Experimental Framework Summary\n",
    "\n",
    "### What We'll Implement Next\n",
    "\n",
    "1. **Generate 10K wafer maps** (20 defect classes, 80/10/10 train/val/test split)\n",
    "2. **Load 3 pre-trained models** (ResNet-50, EfficientNet-B3, ViT-B/16)\n",
    "3. **Apply 3 fine-tuning strategies:**\n",
    "   - Feature extraction (freeze backbone)\n",
    "   - Full fine-tuning (unfreeze all, discriminative LR)\n",
    "   - Gradual unfreezing (progressive 3-phase training)\n",
    "4. **Compare results:** Accuracy, training time, parameters, overfitting\n",
    "5. **Production pipeline:** ONNX export, INT8 quantization, TensorRT optimization\n",
    "\n",
    "---\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "| Strategy | ResNet-50 Accuracy | EfficientNet-B3 Accuracy | Training Time |\n",
    "|----------|-------------------|--------------------------|---------------|\n",
    "| **Train from Scratch** | 75.2% | 76.8% | 120 min |\n",
    "| **Feature Extraction** | 87.5% | 89.2% | **12 min** |\n",
    "| **Full Fine-Tuning** | 91.5% | 93.2% | 25 min |\n",
    "| **Gradual Unfreezing** | **92.8%** | **94.1%** | 30 min |\n",
    "\n",
    "**Key Insight:** Gradual unfreezing achieves **best accuracy** (94.1%) with **minimal overfitting risk**.\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Next Steps\n",
    "\n",
    "Let's implement these strategies in code! We'll start with data generation, then systematically compare the three fine-tuning approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74aaf3",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae972445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Part 2: Data Generation & Preparation\n",
    "# ========================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "# ========================================\n",
    "# Synthetic Wafer Map Generation\n",
    "# ========================================\n",
    "def generate_wafer_map(defect_type, size=128):\n",
    "    \"\"\"\n",
    "    Generate synthetic wafer map with specified defect pattern.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    defect_type : int\n",
    "        Defect class (0-19)\n",
    "    size : int\n",
    "        Image size (default 128x128)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    wafer_map : np.ndarray\n",
    "        Grayscale wafer map (0=background, 1=passing die, 2=failing die)\n",
    "    \"\"\"\n",
    "    wafer = np.ones((size, size), dtype=np.float32)  # All passing\n",
    "    center_x, center_y = size // 2, size // 2\n",
    "    \n",
    "    # Create circular wafer boundary\n",
    "    y, x = np.ogrid[:size, :size]\n",
    "    mask = (x - center_x)**2 + (y - center_y)**2 <= (size // 2 - 2)**2\n",
    "    wafer[~mask] = 0  # Background outside wafer\n",
    "    \n",
    "    # Generate defect patterns based on type\n",
    "    if defect_type == 0:\n",
    "        # Normal (no defects)\n",
    "        pass\n",
    "    \n",
    "    elif defect_type == 1:\n",
    "        # Center cluster\n",
    "        cluster_size = np.random.randint(10, 20)\n",
    "        cluster_x = np.random.randint(center_x - 15, center_x + 15)\n",
    "        cluster_y = np.random.randint(center_y - 15, center_y + 15)\n",
    "        for _ in range(cluster_size):\n",
    "            dx = np.random.randint(-8, 8)\n",
    "            dy = np.random.randint(-8, 8)\n",
    "            px, py = cluster_x + dx, cluster_y + dy\n",
    "            if 0 <= px < size and 0 <= py < size and mask[py, px]:\n",
    "                wafer[py, px] = 0  # Failing die\n",
    "    \n",
    "    elif defect_type == 2:\n",
    "        # Edge defects\n",
    "        edge_type = np.random.choice(['top', 'bottom', 'left', 'right'])\n",
    "        num_defects = np.random.randint(15, 30)\n",
    "        if edge_type == 'top':\n",
    "            for _ in range(num_defects):\n",
    "                x = np.random.randint(0, size)\n",
    "                y = np.random.randint(0, center_y // 2)\n",
    "                if mask[y, x]:\n",
    "                    wafer[y, x] = 0\n",
    "        elif edge_type == 'bottom':\n",
    "            for _ in range(num_defects):\n",
    "                x = np.random.randint(0, size)\n",
    "                y = np.random.randint(center_y + center_y // 2, size)\n",
    "                if mask[y, x]:\n",
    "                    wafer[y, x] = 0\n",
    "        elif edge_type == 'left':\n",
    "            for _ in range(num_defects):\n",
    "                x = np.random.randint(0, center_x // 2)\n",
    "                y = np.random.randint(0, size)\n",
    "                if mask[y, x]:\n",
    "                    wafer[y, x] = 0\n",
    "        else:  # right\n",
    "            for _ in range(num_defects):\n",
    "                x = np.random.randint(center_x + center_x // 2, size)\n",
    "                y = np.random.randint(0, size)\n",
    "                if mask[y, x]:\n",
    "                    wafer[y, x] = 0\n",
    "    \n",
    "    elif defect_type == 3:\n",
    "        # Vertical scratch\n",
    "        scratch_x = np.random.randint(center_x - 20, center_x + 20)\n",
    "        scratch_width = np.random.randint(2, 4)\n",
    "        for y in range(size):\n",
    "            for dx in range(scratch_width):\n",
    "                if 0 <= scratch_x + dx < size and mask[y, scratch_x + dx]:\n",
    "                    wafer[y, scratch_x + dx] = 0\n",
    "    \n",
    "    elif defect_type == 4:\n",
    "        # Horizontal scratch\n",
    "        scratch_y = np.random.randint(center_y - 20, center_y + 20)\n",
    "        scratch_width = np.random.randint(2, 4)\n",
    "        for x in range(size):\n",
    "            for dy in range(scratch_width):\n",
    "                if 0 <= scratch_y + dy < size and mask[scratch_y + dy, x]:\n",
    "                    wafer[scratch_y + dy, x] = 0\n",
    "    \n",
    "    elif defect_type == 5:\n",
    "        # Ring pattern (concentric defects)\n",
    "        ring_radius = np.random.randint(25, 40)\n",
    "        ring_width = np.random.randint(3, 6)\n",
    "        for y in range(size):\n",
    "            for x in range(size):\n",
    "                dist = np.sqrt((x - center_x)**2 + (y - center_y)**2)\n",
    "                if ring_radius <= dist <= ring_radius + ring_width and mask[y, x]:\n",
    "                    if np.random.rand() > 0.3:  # 70% defect density in ring\n",
    "                        wafer[y, x] = 0\n",
    "    \n",
    "    elif defect_type == 6:\n",
    "        # Random clusters (multiple small clusters)\n",
    "        num_clusters = np.random.randint(3, 6)\n",
    "        for _ in range(num_clusters):\n",
    "            cluster_x = np.random.randint(10, size - 10)\n",
    "            cluster_y = np.random.randint(10, size - 10)\n",
    "            cluster_size = np.random.randint(5, 10)\n",
    "            for __ in range(cluster_size):\n",
    "                dx = np.random.randint(-5, 5)\n",
    "                dy = np.random.randint(-5, 5)\n",
    "                px, py = cluster_x + dx, cluster_y + dy\n",
    "                if 0 <= px < size and 0 <= py < size and mask[py, px]:\n",
    "                    wafer[py, px] = 0\n",
    "    \n",
    "    elif defect_type == 7:\n",
    "        # Localized defects (single region)\n",
    "        region_x = np.random.randint(center_x - 25, center_x + 25)\n",
    "        region_y = np.random.randint(center_y - 25, center_y + 25)\n",
    "        region_size = np.random.randint(15, 25)\n",
    "        for y in range(region_y - region_size, region_y + region_size):\n",
    "            for x in range(region_x - region_size, region_x + region_size):\n",
    "                if 0 <= x < size and 0 <= y < size and mask[y, x]:\n",
    "                    if np.random.rand() > 0.5:  # 50% defect density\n",
    "                        wafer[y, x] = 0\n",
    "    \n",
    "    elif defect_type == 8:\n",
    "        # Near-full wafer failure (catastrophic)\n",
    "        for y in range(size):\n",
    "            for x in range(size):\n",
    "                if mask[y, x] and np.random.rand() > 0.2:  # 80% defect rate\n",
    "                    wafer[y, x] = 0\n",
    "    \n",
    "    elif defect_type == 9:\n",
    "        # Donut pattern (center good, ring defective)\n",
    "        inner_radius = np.random.randint(15, 25)\n",
    "        outer_radius = np.random.randint(35, 45)\n",
    "        for y in range(size):\n",
    "            for x in range(size):\n",
    "                dist = np.sqrt((x - center_x)**2 + (y - center_y)**2)\n",
    "                if inner_radius <= dist <= outer_radius and mask[y, x]:\n",
    "                    if np.random.rand() > 0.3:\n",
    "                        wafer[y, x] = 0\n",
    "    \n",
    "    else:\n",
    "        # Mixed patterns for classes 10-19\n",
    "        # Combine 2-3 random patterns from above\n",
    "        patterns = np.random.choice(range(1, 10), size=2, replace=False)\n",
    "        for pattern in patterns:\n",
    "            temp_wafer = generate_wafer_map(pattern, size)\n",
    "            wafer = np.minimum(wafer, temp_wafer)  # Combine defects\n",
    "    \n",
    "    return wafer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83977d",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ee8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Generate Dataset\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING SYNTHETIC WAFER MAP DATASET\")\n",
    "print(\"=\"*60)\n",
    "# Class distribution (mimics real production imbalance)\n",
    "class_distribution = {\n",
    "    0: 2000,   # Normal (20%)\n",
    "    1: 800,    # Center cluster (8%)\n",
    "    2: 1000,   # Edge defects (10%)\n",
    "    3: 600,    # Vertical scratch (6%)\n",
    "    4: 600,    # Horizontal scratch (6%)\n",
    "    5: 500,    # Ring pattern (5%)\n",
    "    6: 700,    # Random clusters (7%)\n",
    "    7: 600,    # Localized defects (6%)\n",
    "    8: 400,    # Near-full failure (4%)\n",
    "    9: 500,    # Donut pattern (5%)\n",
    "}\n",
    "# Generate remaining classes (10-19) with 10-400 samples each\n",
    "for cls in range(10, 20):\n",
    "    class_distribution[cls] = np.random.randint(200, 400)\n",
    "# Total samples\n",
    "total_samples = sum(class_distribution.values())\n",
    "print(f\"\\nTotal samples: {total_samples}\")\n",
    "print(f\"Number of classes: 20\")\n",
    "print(f\"Image size: 128x128 grayscale\")\n",
    "# Generate data\n",
    "X_data = []\n",
    "y_data = []\n",
    "start_time = time.time()\n",
    "for defect_class, num_samples in class_distribution.items():\n",
    "    for _ in range(num_samples):\n",
    "        wafer_map = generate_wafer_map(defect_class, size=128)\n",
    "        X_data.append(wafer_map)\n",
    "        y_data.append(defect_class)\n",
    "    if (defect_class + 1) % 5 == 0:\n",
    "        print(f\"Generated classes 0-{defect_class}: {sum([class_distribution[i] for i in range(defect_class+1)])} samples\")\n",
    "X_data = np.array(X_data, dtype=np.float32)\n",
    "y_data = np.array(y_data, dtype=np.int64)\n",
    "generation_time = time.time() - start_time\n",
    "print(f\"\\n\u2713 Dataset generated in {generation_time:.2f} seconds\")\n",
    "print(f\"  Shape: X={X_data.shape}, y={y_data.shape}\")\n",
    "# ========================================\n",
    "# Train/Val/Test Split\n",
    "# ========================================\n",
    "# Split: 80% train, 10% val, 10% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_data, y_data, test_size=0.2, random_state=42, stratify=y_data)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "print(f\"\\nDataset Split:\")\n",
    "print(f\"  Train: {X_train.shape[0]} samples ({X_train.shape[0]/total_samples*100:.1f}%)\")\n",
    "print(f\"  Val:   {X_val.shape[0]} samples ({X_val.shape[0]/total_samples*100:.1f}%)\")\n",
    "print(f\"  Test:  {X_test.shape[0]} samples ({X_test.shape[0]/total_samples*100:.1f}%)\")\n",
    "# ========================================\n",
    "# Visualize Sample Wafer Maps\n",
    "# ========================================\n",
    "print(\"\\nVisualizing sample wafer maps from each class...\")\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "for cls in range(20):\n",
    "    # Find first sample of this class\n",
    "    idx = np.where(y_train == cls)[0][0]\n",
    "    wafer_map = X_train[idx]\n",
    "    \n",
    "    axes[cls].imshow(wafer_map, cmap='viridis', vmin=0, vmax=1)\n",
    "    axes[cls].set_title(f'Class {cls}\\n({class_distribution[cls]} samples)', fontsize=10)\n",
    "    axes[cls].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('wafer_map_samples.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\u2713 Saved visualization to 'wafer_map_samples.png'\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27412b1f",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9392f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# PyTorch Dataset Class\n",
    "# ========================================\n",
    "class WaferMapDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for wafer maps with preprocessing for transfer learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, transform=None, replicate_channels=True):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray\n",
    "            Wafer maps (N, H, W)\n",
    "        y : np.ndarray\n",
    "            Labels (N,)\n",
    "        transform : torchvision.transforms\n",
    "            Data augmentation pipeline\n",
    "        replicate_channels : bool\n",
    "            If True, replicate grayscale to 3 channels for ImageNet models\n",
    "        \"\"\"\n",
    "        self.X = torch.from_numpy(X).float().unsqueeze(1)  # (N, 1, H, W)\n",
    "        self.y = torch.from_numpy(y).long()\n",
    "        self.transform = transform\n",
    "        self.replicate_channels = replicate_channels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx]  # (1, H, W)\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        # Replicate to 3 channels (grayscale \u2192 RGB) for ImageNet models\n",
    "        if self.replicate_channels:\n",
    "            image = image.repeat(3, 1, 1)  # (3, H, W)\n",
    "        \n",
    "        # Apply transforms (augmentation)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "# ========================================\n",
    "# Data Augmentation & Preprocessing\n",
    "# ========================================\n",
    "# ImageNet normalization statistics (IMPORTANT for transfer learning)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "# Training augmentation (wafer-specific)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet/EfficientNet input size\n",
    "    transforms.RandomRotation(180),  # Wafers have rotational symmetry\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Left-right symmetry\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Simulate different sensors\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "# Validation/Test (no augmentation, only resize + normalize)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "# Create datasets\n",
    "train_dataset = WaferMapDataset(X_train, y_train, transform=train_transform)\n",
    "val_dataset = WaferMapDataset(X_val, y_val, transform=val_transform)\n",
    "test_dataset = WaferMapDataset(X_test, y_test, transform=val_transform)\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "print(f\"\\nData Loaders Created:\")\n",
    "print(f\"  Train batches: {len(train_loader)} (batch_size={batch_size})\")\n",
    "print(f\"  Val batches:   {len(val_loader)}\")\n",
    "print(f\"  Test batches:  {len(test_loader)}\")\n",
    "# Verify data shape\n",
    "sample_batch, sample_labels = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shape: {sample_batch.shape}\")  # Should be (32, 3, 224, 224)\n",
    "print(f\"Sample labels shape: {sample_labels.shape}\")  # Should be (32,)\n",
    "print(f\"Label range: {sample_labels.min()}-{sample_labels.max()}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e75a4e",
   "metadata": {},
   "source": [
    "# \ud83d\udcd0 Part 3: Strategy 1 - Feature Extraction (Freeze Backbone)\n",
    "\n",
    "## \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Use pre-trained ResNet-50 as a **fixed feature extractor**, training only the final classification layer.\n",
    "\n",
    "**Key Points:**\n",
    "- **Freeze backbone:** Set `requires_grad=False` for all convolutional layers (preserve ImageNet features)\n",
    "- **Replace classifier:** Swap 1000-class ImageNet head with 20-class semiconductor head\n",
    "- **Fast training:** Only ~2M parameters trainable (vs 25M total)\n",
    "- **Low overfitting risk:** Pre-trained features fixed, only linear classifier learns\n",
    "\n",
    "**Strategy 1 Architecture:**\n",
    "```\n",
    "Input (224\u00d7224\u00d73 grayscale replicated) \n",
    "    \u2193\n",
    "[FROZEN] ResNet-50 Backbone (23.5M params)\n",
    "    \u251c\u2500 Conv1 + MaxPool\n",
    "    \u251c\u2500 Layer1 (Residual blocks 1-3)\n",
    "    \u251c\u2500 Layer2 (Residual blocks 4-7)\n",
    "    \u251c\u2500 Layer3 (Residual blocks 8-13)\n",
    "    \u2514\u2500 Layer4 (Residual blocks 14-16)\n",
    "    \u2193\n",
    "Global Average Pooling \u2192 2048-dim feature vector\n",
    "    \u2193\n",
    "[TRAINABLE] Classifier (2M params)\n",
    "    \u251c\u2500 Dropout(0.3)\n",
    "    \u251c\u2500 Linear(2048 \u2192 512)\n",
    "    \u251c\u2500 ReLU + Dropout(0.3)\n",
    "    \u2514\u2500 Linear(512 \u2192 20)\n",
    "    \u2193\n",
    "Output: 20 defect classes\n",
    "```\n",
    "\n",
    "**Training Details:**\n",
    "- **Optimizer:** Adam with LR=1e-3 (can use higher LR since only classifier trains)\n",
    "- **Loss:** CrossEntropyLoss with class weights (handle imbalance)\n",
    "- **Epochs:** 15 (converges quickly)\n",
    "- **Early stopping:** Patience=5 epochs\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Baseline approach:** Simplest transfer learning strategy, proves value of pre-training\n",
    "- **Production use case:** When dataset too small (<1000 samples) to fine-tune safely\n",
    "- **Semiconductor application:** Quick prototyping for new defect types with limited data\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd27 Implementation: Feature Extraction with ResNet-50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4077a2",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Strategy 1: Feature Extraction (Freeze Backbone)\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STRATEGY 1: FEATURE EXTRACTION (FREEZE BACKBONE)\")\n",
    "print(\"=\"*70)\n",
    "# Load pre-trained ResNet-50\n",
    "resnet50_frozen = models.resnet50(pretrained=True)\n",
    "# Freeze all parameters in backbone\n",
    "for param in resnet50_frozen.parameters():\n",
    "    param.requires_grad = False\n",
    "# Replace final classifier\n",
    "# Original: Linear(2048 \u2192 1000) for ImageNet\n",
    "# New: Linear(2048 \u2192 512 \u2192 20) for semiconductor\n",
    "num_features = resnet50_frozen.fc.in_features  # 2048 for ResNet-50\n",
    "resnet50_frozen.fc = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(num_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, 20)  # 20 defect classes\n",
    ")\n",
    "# Move model to GPU\n",
    "resnet50_frozen = resnet50_frozen.to(device)\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in resnet50_frozen.parameters())\n",
    "trainable_params = sum(p.numel() for p in resnet50_frozen.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "print(f\"\\nModel: ResNet-50 (Feature Extraction)\")\n",
    "print(f\"  Total parameters:     {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n",
    "print(f\"  Frozen parameters:    {frozen_params:,} ({frozen_params/1e6:.2f}M)\")\n",
    "print(f\"  % Trainable:          {trainable_params/total_params*100:.2f}%\")\n",
    "# ========================================\n",
    "# Training Setup\n",
    "# ========================================\n",
    "# Class weights (handle imbalance)\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum() * len(class_counts)  # Normalize\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "# Loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# Optimizer (only trainable parameters)\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, resnet50_frozen.parameters()),\n",
    "    lr=1e-3,  # Can use higher LR since only classifier trains\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "# Learning rate scheduler (ReduceLROnPlateau)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "# ========================================\n",
    "# Training Function\n",
    "# ========================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b27e19",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Function: train_epoch\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Progress (every 50 batches)\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"  Batch {batch_idx+1}/{len(dataloader)}: \"\n",
    "                  f\"Loss={loss.item():.4f}, Acc={100.*correct/total:.2f}%\")\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "# ========================================\n",
    "# Training Loop\n",
    "# ========================================\n",
    "num_epochs = 15\n",
    "best_val_acc = 0.0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []\n",
    "print(f\"\\nTraining for {num_epochs} epochs...\")\n",
    "print(f\"Device: {device}\")\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(resnet50_frozen, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_epoch(resnet50_frozen, val_loader, criterion, device)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    # Save metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f\"\\n  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f}, Val Acc:   {val_acc:.2f}%\")\n",
    "    print(f\"  Current LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(resnet50_frozen.state_dict(), 'resnet50_frozen_best.pth')\n",
    "        print(f\"  \u2713 New best validation accuracy! Model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n  Early stopping triggered (patience={patience})\")\n",
    "            break\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\n\u2713 Training completed in {training_time/60:.2f} minutes\")\n",
    "print(f\"  Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "# Load best model\n",
    "resnet50_frozen.load_state_dict(torch.load('resnet50_frozen_best.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d028389e",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9abf3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Test Evaluation\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = validate_epoch(resnet50_frozen, test_loader, criterion, device)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "# Detailed metrics\n",
    "resnet50_frozen.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = resnet50_frozen(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "# Classification report\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"-\"*70)\n",
    "print(classification_report(y_true, y_pred, target_names=[f'Class {i}' for i in range(20)], digits=4))\n",
    "# Precision, Recall, F1 (weighted)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "print(f\"\\nWeighted Metrics:\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "# ========================================\n",
    "# Visualize Training Curves\n",
    "# ========================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# Loss curves\n",
    "axes[0].plot(train_losses, label='Train Loss', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Strategy 1: Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "# Accuracy curves\n",
    "axes[1].plot(train_accs, label='Train Acc', linewidth=2)\n",
    "axes[1].plot(val_accs, label='Val Acc', linewidth=2)\n",
    "axes[1].axhline(y=test_acc, color='red', linestyle='--', label=f'Test Acc ({test_acc:.2f}%)', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Strategy 1: Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('strategy1_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n\u2713 Saved training curves to 'strategy1_training_curves.png'\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55299d26",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Confusion Matrix\n",
    "# ========================================\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=[f'C{i}' for i in range(20)],\n",
    "            yticklabels=[f'C{i}' for i in range(20)])\n",
    "plt.xlabel('Predicted Class', fontsize=12)\n",
    "plt.ylabel('True Class', fontsize=12)\n",
    "plt.title('Strategy 1: Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('strategy1_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\u2713 Saved confusion matrix to 'strategy1_confusion_matrix.png'\")\n",
    "plt.show()\n",
    "# ========================================\n",
    "# Summary Statistics\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STRATEGY 1 SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: ResNet-50 (Feature Extraction)\")\n",
    "print(f\"  Total parameters:     {total_params/1e6:.2f}M\")\n",
    "print(f\"  Trainable parameters: {trainable_params/1e6:.2f}M ({trainable_params/total_params*100:.2f}%)\")\n",
    "print(f\"  Training time:        {training_time/60:.2f} minutes\")\n",
    "print(f\"  Best val accuracy:    {best_val_acc:.2f}%\")\n",
    "print(f\"  Test accuracy:        {test_acc:.2f}%\")\n",
    "print(f\"  Test F1-score:        {f1:.4f}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7010e3",
   "metadata": {},
   "source": [
    "# \ud83d\udd25 Part 4: Strategy 2 - Full Fine-Tuning (Discriminative Learning Rates)\n",
    "\n",
    "## \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Unfreeze **all layers** of ResNet-50 and train with **discriminative learning rates** (early layers learn slower, late layers learn faster).\n",
    "\n",
    "**Key Points:**\n",
    "- **Unfreeze backbone:** All 25M parameters trainable (vs 2M in Strategy 1)\n",
    "- **Discriminative LR:** Early layers use LR=1e-6, late layers use LR=1e-3 (1000\u00d7 difference!)\n",
    "- **Prevents catastrophic forgetting:** Low LR in early layers preserves ImageNet edge/texture detectors\n",
    "- **Adapts to domain:** High LR in late layers adjusts to semiconductor-specific patterns\n",
    "\n",
    "**Strategy 2 Architecture:**\n",
    "```\n",
    "Input (224\u00d7224\u00d73)\n",
    "    \u2193\n",
    "[TRAINABLE - LR=1e-6] Layer1 (Conv + Residual blocks 1-3)\n",
    "    \u2193\n",
    "[TRAINABLE - LR=5e-6] Layer2 (Residual blocks 4-7)\n",
    "    \u2193\n",
    "[TRAINABLE - LR=1e-5] Layer3 (Residual blocks 8-13)\n",
    "    \u2193\n",
    "[TRAINABLE - LR=1e-4] Layer4 (Residual blocks 14-16)\n",
    "    \u2193\n",
    "Global Average Pooling \u2192 2048-dim feature vector\n",
    "    \u2193\n",
    "[TRAINABLE - LR=1e-3] Classifier (512 \u2192 20)\n",
    "    \u2193\n",
    "Output: 20 defect classes\n",
    "```\n",
    "\n",
    "**Discriminative LR Formula:**\n",
    "$$\n",
    "\\eta_{\\text{layer } i} = \\eta_{\\text{base}} \\times \\alpha^{L-i}\n",
    "$$\n",
    "\n",
    "where $\\eta_{\\text{base}} = 10^{-3}$, $\\alpha = 0.1$, $L = 5$ (5 layer groups).\n",
    "\n",
    "**Example:**\n",
    "- Layer 1: $\\eta_1 = 10^{-3} \\times 0.1^{4} = 10^{-7}$ (nearly frozen)\n",
    "- Layer 2: $\\eta_2 = 10^{-3} \\times 0.1^{3} = 10^{-6}$\n",
    "- Layer 3: $\\eta_3 = 10^{-3} \\times 0.1^{2} = 10^{-5}$\n",
    "- Layer 4: $\\eta_4 = 10^{-3} \\times 0.1^{1} = 10^{-4}$\n",
    "- Classifier: $\\eta_5 = 10^{-3} \\times 0.1^{0} = 10^{-3}$ (full LR)\n",
    "\n",
    "**Training Details:**\n",
    "- **Optimizer:** Adam with parameter groups (each layer group = different LR)\n",
    "- **Scheduler:** Cosine annealing with warm-up (2 epochs warm-up, then cosine decay)\n",
    "- **Epochs:** 20 (needs more training since all layers update)\n",
    "- **Expected improvement:** +3-5% accuracy vs Strategy 1\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Maximizes accuracy:** Full model adaptation to semiconductor domain\n",
    "- **Production standard:** Industry best practice for transfer learning\n",
    "- **Handles domain shift:** Early layers preserve universal features, late layers specialize\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd27 Implementation: Full Fine-Tuning with Discriminative LR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16756ae9",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6429f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Strategy 2: Full Fine-Tuning (Discriminative LR)\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STRATEGY 2: FULL FINE-TUNING (DISCRIMINATIVE LEARNING RATES)\")\n",
    "print(\"=\"*70)\n",
    "# Load pre-trained ResNet-50 (fresh copy)\n",
    "resnet50_finetune = models.resnet50(pretrained=True)\n",
    "# Replace final classifier (same as Strategy 1)\n",
    "num_features = resnet50_finetune.fc.in_features\n",
    "resnet50_finetune.fc = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(num_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, 20)\n",
    ")\n",
    "# Move to GPU\n",
    "resnet50_finetune = resnet50_finetune.to(device)\n",
    "# Count parameters (ALL trainable now)\n",
    "total_params_ft = sum(p.numel() for p in resnet50_finetune.parameters())\n",
    "trainable_params_ft = sum(p.numel() for p in resnet50_finetune.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel: ResNet-50 (Full Fine-Tuning)\")\n",
    "print(f\"  Total parameters:     {total_params_ft:,} ({total_params_ft/1e6:.2f}M)\")\n",
    "print(f\"  Trainable parameters: {trainable_params_ft:,} ({trainable_params_ft/1e6:.2f}M)\")\n",
    "print(f\"  % Trainable:          100.00%\")\n",
    "# ========================================\n",
    "# Discriminative Learning Rates Setup\n",
    "# ========================================\n",
    "# Define parameter groups with different LRs\n",
    "base_lr = 1e-3\n",
    "lr_decay = 0.1\n",
    "param_groups = [\n",
    "    {'params': resnet50_finetune.conv1.parameters(), 'lr': base_lr * (lr_decay ** 4)},         # LR = 1e-7\n",
    "    {'params': resnet50_finetune.layer1.parameters(), 'lr': base_lr * (lr_decay ** 3)},        # LR = 1e-6\n",
    "    {'params': resnet50_finetune.layer2.parameters(), 'lr': base_lr * (lr_decay ** 2)},        # LR = 1e-5\n",
    "    {'params': resnet50_finetune.layer3.parameters(), 'lr': base_lr * lr_decay},               # LR = 1e-4\n",
    "    {'params': resnet50_finetune.layer4.parameters(), 'lr': base_lr * (lr_decay ** 0.5)},      # LR = 3e-4\n",
    "    {'params': resnet50_finetune.fc.parameters(), 'lr': base_lr}                               # LR = 1e-3\n",
    "]\n",
    "print(f\"\\nDiscriminative Learning Rates:\")\n",
    "layer_names = ['conv1', 'layer1', 'layer2', 'layer3', 'layer4', 'fc (classifier)']\n",
    "for i, (layer_name, pg) in enumerate(zip(layer_names, param_groups)):\n",
    "    print(f\"  {layer_name:20s}: LR = {pg['lr']:.2e}\")\n",
    "# Optimizer with parameter groups\n",
    "optimizer_ft = torch.optim.Adam(param_groups, weight_decay=1e-4)\n",
    "# Cosine annealing scheduler with warm-up\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "warmup_epochs = 2\n",
    "total_epochs_ft = 20\n",
    "warmup_scheduler = LinearLR(optimizer_ft, start_factor=0.1, end_factor=1.0, total_iters=warmup_epochs)\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer_ft, T_max=total_epochs_ft - warmup_epochs, eta_min=1e-7)\n",
    "scheduler_ft = SequentialLR(optimizer_ft, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_epochs])\n",
    "print(f\"\\nLearning Rate Scheduler:\")\n",
    "print(f\"  Warm-up: Epochs 1-{warmup_epochs} (linear 0.1\u00d7 \u2192 1.0\u00d7)\")\n",
    "print(f\"  Cosine annealing: Epochs {warmup_epochs+1}-{total_epochs_ft} (\u2192 1e-7)\")\n",
    "# Loss function (reuse from Strategy 1)\n",
    "criterion_ft = nn.CrossEntropyLoss(weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f587579f",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8762f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Training Loop (Strategy 2)\n",
    "# ========================================\n",
    "best_val_acc_ft = 0.0\n",
    "patience_ft = 5\n",
    "patience_counter_ft = 0\n",
    "train_losses_ft, train_accs_ft = [], []\n",
    "val_losses_ft, val_accs_ft = [], []\n",
    "print(f\"\\nTraining for {total_epochs_ft} epochs...\")\n",
    "start_time_ft = time.time()\n",
    "for epoch in range(total_epochs_ft):\n",
    "    print(f\"\\nEpoch {epoch+1}/{total_epochs_ft}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Train\n",
    "    train_loss_ft, train_acc_ft = train_epoch(resnet50_finetune, train_loader, criterion_ft, optimizer_ft, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss_ft, val_acc_ft = validate_epoch(resnet50_finetune, val_loader, criterion_ft, device)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler_ft.step()\n",
    "    \n",
    "    # Save metrics\n",
    "    train_losses_ft.append(train_loss_ft)\n",
    "    train_accs_ft.append(train_acc_ft)\n",
    "    val_losses_ft.append(val_loss_ft)\n",
    "    val_accs_ft.append(val_acc_ft)\n",
    "    \n",
    "    # Get current LRs for each group\n",
    "    current_lrs = [pg['lr'] for pg in optimizer_ft.param_groups]\n",
    "    \n",
    "    print(f\"\\n  Train Loss: {train_loss_ft:.4f}, Train Acc: {train_acc_ft:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss_ft:.4f}, Val Acc:   {val_acc_ft:.2f}%\")\n",
    "    print(f\"  LR Range:   {min(current_lrs):.2e} - {max(current_lrs):.2e}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc_ft > best_val_acc_ft:\n",
    "        best_val_acc_ft = val_acc_ft\n",
    "        patience_counter_ft = 0\n",
    "        torch.save(resnet50_finetune.state_dict(), 'resnet50_finetune_best.pth')\n",
    "        print(f\"  \u2713 New best validation accuracy! Model saved.\")\n",
    "    else:\n",
    "        patience_counter_ft += 1\n",
    "        if patience_counter_ft >= patience_ft:\n",
    "            print(f\"\\n  Early stopping triggered (patience={patience_ft})\")\n",
    "            break\n",
    "training_time_ft = time.time() - start_time_ft\n",
    "print(f\"\\n\u2713 Training completed in {training_time_ft/60:.2f} minutes\")\n",
    "print(f\"  Best validation accuracy: {best_val_acc_ft:.2f}%\")\n",
    "# Load best model\n",
    "resnet50_finetune.load_state_dict(torch.load('resnet50_finetune_best.pth'))\n",
    "# ========================================\n",
    "# Test Evaluation (Strategy 2)\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SET EVALUATION (STRATEGY 2)\")\n",
    "print(\"=\"*70)\n",
    "test_loss_ft, test_acc_ft = validate_epoch(resnet50_finetune, test_loader, criterion_ft, device)\n",
    "print(f\"\\nTest Loss: {test_loss_ft:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_ft:.2f}%\")\n",
    "# Detailed metrics\n",
    "resnet50_finetune.eval()\n",
    "y_true_ft = []\n",
    "y_pred_ft = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = resnet50_finetune(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        y_true_ft.extend(labels.cpu().numpy())\n",
    "        y_pred_ft.extend(predicted.cpu().numpy())\n",
    "precision_ft, recall_ft, f1_ft, _ = precision_recall_fscore_support(y_true_ft, y_pred_ft, average='weighted')\n",
    "print(f\"\\nWeighted Metrics:\")\n",
    "print(f\"  Precision: {precision_ft:.4f}\")\n",
    "print(f\"  Recall:    {recall_ft:.4f}\")\n",
    "print(f\"  F1-Score:  {f1_ft:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea45234",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8707ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Visualize Training Curves (Strategy 2)\n",
    "# ========================================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(train_losses_ft, label='Train Loss', linewidth=2)\n",
    "axes[0].plot(val_losses_ft, label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Strategy 2: Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[1].plot(train_accs_ft, label='Train Acc', linewidth=2)\n",
    "axes[1].plot(val_accs_ft, label='Val Acc', linewidth=2)\n",
    "axes[1].axhline(y=test_acc_ft, color='red', linestyle='--', label=f'Test Acc ({test_acc_ft:.2f}%)', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Strategy 2: Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('strategy2_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n\u2713 Saved training curves to 'strategy2_training_curves.png'\")\n",
    "plt.show()\n",
    "# ========================================\n",
    "# Comparison: Strategy 1 vs Strategy 2\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: STRATEGY 1 vs STRATEGY 2\")\n",
    "print(\"=\"*70)\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Trainable Params (M)',\n",
    "        'Training Time (min)',\n",
    "        'Best Val Acc (%)',\n",
    "        'Test Acc (%)',\n",
    "        'Test F1-Score',\n",
    "        'Overfitting Gap (Train-Val %)'\n",
    "    ],\n",
    "    'Strategy 1 (Frozen)': [\n",
    "        f'{trainable_params/1e6:.2f}',\n",
    "        f'{training_time/60:.2f}',\n",
    "        f'{best_val_acc:.2f}',\n",
    "        f'{test_acc:.2f}',\n",
    "        f'{f1:.4f}',\n",
    "        f'{train_accs[-1] - val_accs[-1]:.2f}'\n",
    "    ],\n",
    "    'Strategy 2 (Fine-Tune)': [\n",
    "        f'{trainable_params_ft/1e6:.2f}',\n",
    "        f'{training_time_ft/60:.2f}',\n",
    "        f'{best_val_acc_ft:.2f}',\n",
    "        f'{test_acc_ft:.2f}',\n",
    "        f'{f1_ft:.4f}',\n",
    "        f'{train_accs_ft[-1] - val_accs_ft[-1]:.2f}'\n",
    "    ]\n",
    "}\n",
    "import pandas as pd\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(f\"\\n\ud83d\udcca Key Observations:\")\n",
    "print(f\"  \u2022 Fine-tuning improves test accuracy by {test_acc_ft - test_acc:.2f}%\")\n",
    "print(f\"  \u2022 Training time increases by {(training_time_ft - training_time)/60:.2f} min\")\n",
    "print(f\"  \u2022 Fine-tuning uses {trainable_params_ft/trainable_params:.1f}\u00d7 more trainable parameters\")\n",
    "if test_acc_ft > test_acc:\n",
    "    print(f\"  \u2713 Strategy 2 wins on accuracy!\")\n",
    "else:\n",
    "    print(f\"  \u2713 Strategy 1 wins on efficiency!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2870b47b",
   "metadata": {},
   "source": [
    "# \ud83c\udfaf Part 5: Strategy 3 - Gradual Unfreezing & EfficientNet Comparison\n",
    "\n",
    "## \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement **best-practice transfer learning** - gradual unfreezing combined with EfficientNet-B3 (most efficient architecture).\n",
    "\n",
    "**Key Points:**\n",
    "- **Phase 1 (Epochs 1-5):** Train classifier only (feature extraction baseline)\n",
    "- **Phase 2 (Epochs 6-10):** Unfreeze layer4 (late features adapt to semiconductor patterns)\n",
    "- **Phase 3 (Epochs 11-15):** Unfreeze layer3 (mid-level features fine-tune)\n",
    "- **EfficientNet-B3:** 2\u00d7 fewer parameters than ResNet-50, 3-5% higher accuracy\n",
    "\n",
    "**Strategy 3: Progressive Layer Unfreezing**\n",
    "\n",
    "```\n",
    "Phase 1 (Epochs 1-5): Feature Extraction\n",
    "    [FROZEN] Backbone \u2192 [TRAINABLE] Classifier\n",
    "    Goal: Train classifier to convergence\n",
    "    LR: 1e-3 for classifier\n",
    "\n",
    "Phase 2 (Epochs 6-10): Unfreeze Top Layers\n",
    "    [FROZEN] Layers 1-3 \u2192 [TRAINABLE] Layer4 + Classifier\n",
    "    Goal: Adapt high-level features to wafer maps\n",
    "    LR: 1e-4 for layer4, 1e-3 for classifier\n",
    "\n",
    "Phase 3 (Epochs 11-15): Full Fine-Tuning\n",
    "    [TRAINABLE] All layers\n",
    "    Goal: End-to-end optimization for semiconductor domain\n",
    "    LR: Discriminative (1e-6 for layer1 \u2192 1e-3 for classifier)\n",
    "```\n",
    "\n",
    "**Why Gradual Unfreezing Works:**\n",
    "- **Prevents catastrophic forgetting:** Early layers preserve ImageNet features (edges, textures)\n",
    "- **Stable training:** Classifier converges first, then backbone adapts gradually\n",
    "- **Best accuracy:** Balances feature extraction (Strategy 1) + full fine-tuning (Strategy 2)\n",
    "\n",
    "**EfficientNet-B3 Advantages:**\n",
    "- **Compound scaling:** Optimally scales depth, width, and resolution\n",
    "- **Fewer parameters:** 12M vs ResNet-50's 25.6M (2\u00d7 smaller)\n",
    "- **Higher accuracy:** ImageNet Top-1: 81.6% vs 76.2% (5.4% better)\n",
    "- **Faster inference:** 2\u00d7 faster on edge devices\n",
    "- **Semiconductor use case:** **Recommended for production deployment**\n",
    "\n",
    "**Expected Results:**\n",
    "- **Strategy 3 (Gradual):** 93-94% accuracy, best generalization\n",
    "- **EfficientNet-B3:** 94-95% accuracy, fastest inference, smallest model size\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd27 Implementation: Gradual Unfreezing + EfficientNet-B3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9f4d54",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b639991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Strategy 3: Gradual Unfreezing with EfficientNet-B3\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STRATEGY 3: GRADUAL UNFREEZING with EfficientNet-B3\")\n",
    "print(\"=\"*70)\n",
    "# Install timm if not available (PyTorch Image Models - 700+ pre-trained models)\n",
    "try:\n",
    "    import timm\n",
    "except ImportError:\n",
    "    print(\"Installing timm (PyTorch Image Models)...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'timm', '-q'])\n",
    "    import timm\n",
    "print(f\"timm version: {timm.__version__}\")\n",
    "# Load pre-trained EfficientNet-B3\n",
    "efficientnet = timm.create_model('efficientnet_b3', pretrained=True, num_classes=20)\n",
    "# Move to GPU\n",
    "efficientnet = efficientnet.to(device)\n",
    "# Model info\n",
    "total_params_eff = sum(p.numel() for p in efficientnet.parameters())\n",
    "print(f\"\\nModel: EfficientNet-B3\")\n",
    "print(f\"  Total parameters: {total_params_eff:,} ({total_params_eff/1e6:.2f}M)\")\n",
    "print(f\"  Input size: 224\u00d7224\u00d73\")\n",
    "print(f\"  ImageNet Top-1 accuracy: 81.6%\")\n",
    "# ========================================\n",
    "# Phase 1: Feature Extraction (Freeze Backbone)\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 1: FEATURE EXTRACTION (Epochs 1-5)\")\n",
    "print(\"=\"*70)\n",
    "# Freeze all parameters except classifier\n",
    "for name, param in efficientnet.named_parameters():\n",
    "    if 'classifier' not in name:  # EfficientNet uses 'classifier' instead of 'fc'\n",
    "        param.requires_grad = False\n",
    "trainable_phase1 = sum(p.numel() for p in efficientnet.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_phase1:,} ({trainable_phase1/total_params_eff*100:.2f}%)\")\n",
    "# Optimizer (classifier only)\n",
    "optimizer_phase1 = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, efficientnet.parameters()),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "# Loss function\n",
    "criterion_eff = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# Train Phase 1\n",
    "phase1_epochs = 5\n",
    "train_losses_phase1, train_accs_phase1 = [], []\n",
    "val_losses_phase1, val_accs_phase1 = [], []\n",
    "start_time_phase1 = time.time()\n",
    "for epoch in range(phase1_epochs):\n",
    "    print(f\"\\nPhase 1 - Epoch {epoch+1}/{phase1_epochs}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    train_loss_p1, train_acc_p1 = train_epoch(efficientnet, train_loader, criterion_eff, optimizer_phase1, device)\n",
    "    val_loss_p1, val_acc_p1 = validate_epoch(efficientnet, val_loader, criterion_eff, device)\n",
    "    \n",
    "    train_losses_phase1.append(train_loss_p1)\n",
    "    train_accs_phase1.append(train_acc_p1)\n",
    "    val_losses_phase1.append(val_loss_p1)\n",
    "    val_accs_phase1.append(val_acc_p1)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss_p1:.4f}, Train Acc: {train_acc_p1:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss_p1:.4f}, Val Acc:   {val_acc_p1:.2f}%\")\n",
    "time_phase1 = time.time() - start_time_phase1\n",
    "print(f\"\\n\u2713 Phase 1 completed in {time_phase1/60:.2f} minutes\")\n",
    "print(f\"  Val accuracy: {val_accs_phase1[-1]:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304f4b69",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ccc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Phase 2: Unfreeze Last Block (Layers 6-7)\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 2: UNFREEZE TOP BLOCKS (Epochs 6-10)\")\n",
    "print(\"=\"*70)\n",
    "# Unfreeze blocks 6-7 (last two blocks before classifier)\n",
    "for name, param in efficientnet.named_parameters():\n",
    "    if 'blocks.6' in name or 'blocks.7' in name or 'classifier' in name:\n",
    "        param.requires_grad = True\n",
    "trainable_phase2 = sum(p.numel() for p in efficientnet.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_phase2:,} ({trainable_phase2/total_params_eff*100:.2f}%)\")\n",
    "# Optimizer with discriminative LR\n",
    "optimizer_phase2 = torch.optim.Adam([\n",
    "    {'params': [p for n, p in efficientnet.named_parameters() if 'blocks.6' in n or 'blocks.7' in n], 'lr': 1e-4},\n",
    "    {'params': [p for n, p in efficientnet.named_parameters() if 'classifier' in n], 'lr': 1e-3}\n",
    "], weight_decay=1e-4)\n",
    "print(f\"Learning rates:\")\n",
    "print(f\"  Blocks 6-7: 1e-4\")\n",
    "print(f\"  Classifier: 1e-3\")\n",
    "# Train Phase 2\n",
    "phase2_epochs = 5\n",
    "train_losses_phase2, train_accs_phase2 = [], []\n",
    "val_losses_phase2, val_accs_phase2 = [], []\n",
    "start_time_phase2 = time.time()\n",
    "for epoch in range(phase2_epochs):\n",
    "    print(f\"\\nPhase 2 - Epoch {epoch+1}/{phase2_epochs}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    train_loss_p2, train_acc_p2 = train_epoch(efficientnet, train_loader, criterion_eff, optimizer_phase2, device)\n",
    "    val_loss_p2, val_acc_p2 = validate_epoch(efficientnet, val_loader, criterion_eff, device)\n",
    "    \n",
    "    train_losses_phase2.append(train_loss_p2)\n",
    "    train_accs_phase2.append(train_acc_p2)\n",
    "    val_losses_phase2.append(val_loss_p2)\n",
    "    val_accs_phase2.append(val_acc_p2)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss_p2:.4f}, Train Acc: {train_acc_p2:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss_p2:.4f}, Val Acc:   {val_acc_p2:.2f}%\")\n",
    "time_phase2 = time.time() - start_time_phase2\n",
    "print(f\"\\n\u2713 Phase 2 completed in {time_phase2/60:.2f} minutes\")\n",
    "print(f\"  Val accuracy: {val_accs_phase2[-1]:.2f}%\")\n",
    "print(f\"  Improvement vs Phase 1: +{val_accs_phase2[-1] - val_accs_phase1[-1]:.2f}%\")\n",
    "# ========================================\n",
    "# Phase 3: Full Fine-Tuning (Unfreeze All)\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 3: FULL FINE-TUNING (Epochs 11-15)\")\n",
    "print(\"=\"*70)\n",
    "# Unfreeze all layers\n",
    "for param in efficientnet.parameters():\n",
    "    param.requires_grad = True\n",
    "trainable_phase3 = sum(p.numel() for p in efficientnet.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_phase3:,} ({trainable_phase3/total_params_eff*100:.2f}%)\")\n",
    "# Optimizer with full discriminative LR\n",
    "optimizer_phase3 = torch.optim.Adam([\n",
    "    {'params': [p for n, p in efficientnet.named_parameters() if 'blocks.0' in n or 'blocks.1' in n], 'lr': 1e-6},\n",
    "    {'params': [p for n, p in efficientnet.named_parameters() if 'blocks.2' in n or 'blocks.3' in n], 'lr': 5e-6},\n",
    "    {'params': [p for n, p in efficientnet.named_parameters() if 'blocks.4' in n or 'blocks.5' in n], 'lr': 1e-5},\n",
    "    {'params': [p for n, p in efficientnet.named_parameters() if 'blocks.6' in n or 'blocks.7' in n], 'lr': 1e-4},\n",
    "    {'params': [p for n, p in efficientnet.named_parameters() if 'classifier' in n], 'lr': 1e-3}\n",
    "], weight_decay=1e-4)\n",
    "print(f\"Discriminative learning rates:\")\n",
    "print(f\"  Blocks 0-1 (early):   1e-6\")\n",
    "print(f\"  Blocks 2-3:           5e-6\")\n",
    "print(f\"  Blocks 4-5:           1e-5\")\n",
    "print(f\"  Blocks 6-7 (late):    1e-4\")\n",
    "print(f\"  Classifier:           1e-3\")\n",
    "# Train Phase 3\n",
    "phase3_epochs = 5\n",
    "train_losses_phase3, train_accs_phase3 = [], []\n",
    "val_losses_phase3, val_accs_phase3 = [], []\n",
    "best_val_acc_eff = 0.0\n",
    "start_time_phase3 = time.time()\n",
    "for epoch in range(phase3_epochs):\n",
    "    print(f\"\\nPhase 3 - Epoch {epoch+1}/{phase3_epochs}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    train_loss_p3, train_acc_p3 = train_epoch(efficientnet, train_loader, criterion_eff, optimizer_phase3, device)\n",
    "    val_loss_p3, val_acc_p3 = validate_epoch(efficientnet, val_loader, criterion_eff, device)\n",
    "    \n",
    "    train_losses_phase3.append(train_loss_p3)\n",
    "    train_accs_phase3.append(train_acc_p3)\n",
    "    val_losses_phase3.append(val_loss_p3)\n",
    "    val_accs_phase3.append(val_acc_p3)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss_p3:.4f}, Train Acc: {train_acc_p3:.2f}%\")\n",
    "    print(f\"  Val Loss:   {val_loss_p3:.4f}, Val Acc:   {val_acc_p3:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc_p3 > best_val_acc_eff:\n",
    "        best_val_acc_eff = val_acc_p3\n",
    "        torch.save(efficientnet.state_dict(), 'efficientnet_gradual_best.pth')\n",
    "        print(f\"  \u2713 New best model saved!\")\n",
    "time_phase3 = time.time() - start_time_phase3\n",
    "total_training_time_eff = time_phase1 + time_phase2 + time_phase3\n",
    "print(f\"\\n\u2713 Phase 3 completed in {time_phase3/60:.2f} minutes\")\n",
    "print(f\"  Val accuracy: {val_accs_phase3[-1]:.2f}%\")\n",
    "print(f\"  Improvement vs Phase 2: +{val_accs_phase3[-1] - val_accs_phase2[-1]:.2f}%\")\n",
    "print(f\"\\n\u2713 Total training time (all 3 phases): {total_training_time_eff/60:.2f} minutes\")\n",
    "print(f\"  Best validation accuracy: {best_val_acc_eff:.2f}%\")\n",
    "# Load best model\n",
    "efficientnet.load_state_dict(torch.load('efficientnet_gradual_best.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0958f",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91274c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Test Evaluation (Strategy 3)\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SET EVALUATION (STRATEGY 3 - EfficientNet-B3)\")\n",
    "print(\"=\"*70)\n",
    "test_loss_eff, test_acc_eff = validate_epoch(efficientnet, test_loader, criterion_eff, device)\n",
    "print(f\"\\nTest Loss: {test_loss_eff:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_eff:.2f}%\")\n",
    "# Detailed metrics\n",
    "efficientnet.eval()\n",
    "y_true_eff = []\n",
    "y_pred_eff = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = efficientnet(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        y_true_eff.extend(labels.cpu().numpy())\n",
    "        y_pred_eff.extend(predicted.cpu().numpy())\n",
    "precision_eff, recall_eff, f1_eff, _ = precision_recall_fscore_support(y_true_eff, y_pred_eff, average='weighted')\n",
    "print(f\"\\nWeighted Metrics:\")\n",
    "print(f\"  Precision: {precision_eff:.4f}\")\n",
    "print(f\"  Recall:    {recall_eff:.4f}\")\n",
    "print(f\"  F1-Score:  {f1_eff:.4f}\")\n",
    "# ========================================\n",
    "# Visualize 3-Phase Training\n",
    "# ========================================\n",
    "# Combine all phases\n",
    "all_train_losses = train_losses_phase1 + train_losses_phase2 + train_losses_phase3\n",
    "all_train_accs = train_accs_phase1 + train_accs_phase2 + train_accs_phase3\n",
    "all_val_losses = val_losses_phase1 + val_losses_phase2 + val_losses_phase3\n",
    "all_val_accs = val_accs_phase1 + val_accs_phase2 + val_accs_phase3\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# Loss curves with phase markers\n",
    "epochs_range = range(1, len(all_train_losses) + 1)\n",
    "axes[0].plot(epochs_range, all_train_losses, label='Train Loss', linewidth=2)\n",
    "axes[0].plot(epochs_range, all_val_losses, label='Val Loss', linewidth=2)\n",
    "axes[0].axvline(x=5, color='red', linestyle='--', alpha=0.7, label='Phase 1\u21922')\n",
    "axes[0].axvline(x=10, color='green', linestyle='--', alpha=0.7, label='Phase 2\u21923')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Strategy 3: Gradual Unfreezing - Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "# Accuracy curves with phase markers\n",
    "axes[1].plot(epochs_range, all_train_accs, label='Train Acc', linewidth=2)\n",
    "axes[1].plot(epochs_range, all_val_accs, label='Val Acc', linewidth=2)\n",
    "axes[1].axvline(x=5, color='red', linestyle='--', alpha=0.7, label='Phase 1\u21922')\n",
    "axes[1].axvline(x=10, color='green', linestyle='--', alpha=0.7, label='Phase 2\u21923')\n",
    "axes[1].axhline(y=test_acc_eff, color='purple', linestyle='--', label=f'Test Acc ({test_acc_eff:.2f}%)', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Strategy 3: Gradual Unfreezing - Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('strategy3_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n\u2713 Saved training curves to 'strategy3_training_curves.png'\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef131e7",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020aa52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Final Comparison: All 3 Strategies\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL COMPARISON: ALL STRATEGIES\")\n",
    "print(\"=\"*70)\n",
    "final_comparison = {\n",
    "    'Metric': [\n",
    "        'Model',\n",
    "        'Total Params (M)',\n",
    "        'Trainable Params (M)',\n",
    "        'Training Time (min)',\n",
    "        'Test Accuracy (%)',\n",
    "        'Test F1-Score',\n",
    "        'Training Strategy'\n",
    "    ],\n",
    "    'Strategy 1': [\n",
    "        'ResNet-50',\n",
    "        f'{total_params/1e6:.2f}',\n",
    "        f'{trainable_params/1e6:.2f}',\n",
    "        f'{training_time/60:.2f}',\n",
    "        f'{test_acc:.2f}',\n",
    "        f'{f1:.4f}',\n",
    "        'Feature Extraction'\n",
    "    ],\n",
    "    'Strategy 2': [\n",
    "        'ResNet-50',\n",
    "        f'{total_params_ft/1e6:.2f}',\n",
    "        f'{trainable_params_ft/1e6:.2f}',\n",
    "        f'{training_time_ft/60:.2f}',\n",
    "        f'{test_acc_ft:.2f}',\n",
    "        f'{f1_ft:.4f}',\n",
    "        'Full Fine-Tuning'\n",
    "    ],\n",
    "    'Strategy 3': [\n",
    "        'EfficientNet-B3',\n",
    "        f'{total_params_eff/1e6:.2f}',\n",
    "        f'{total_params_eff/1e6:.2f}',\n",
    "        f'{total_training_time_eff/60:.2f}',\n",
    "        f'{test_acc_eff:.2f}',\n",
    "        f'{f1_eff:.4f}',\n",
    "        'Gradual Unfreezing'\n",
    "    ]\n",
    "}\n",
    "df_final = pd.DataFrame(final_comparison)\n",
    "print(df_final.to_string(index=False))\n",
    "print(f\"\\n\ud83c\udfc6 WINNER: Strategy 3 (EfficientNet-B3 + Gradual Unfreezing)\")\n",
    "print(f\"  \u2713 Highest test accuracy: {test_acc_eff:.2f}%\")\n",
    "print(f\"  \u2713 Smallest model size: {total_params_eff/1e6:.2f}M params (2\u00d7 smaller than ResNet-50)\")\n",
    "print(f\"  \u2713 Best F1-score: {f1_eff:.4f}\")\n",
    "print(f\"  \u2713 Balanced training time: {total_training_time_eff/60:.2f} minutes\")\n",
    "print(\"\\n\ud83d\udcca Key Insights:\")\n",
    "print(f\"  \u2022 Transfer learning improves accuracy by {test_acc_eff - 75:.2f}% over training from scratch\")\n",
    "print(f\"  \u2022 EfficientNet-B3 outperforms ResNet-50 by {test_acc_eff - test_acc_ft:.2f}% with 2\u00d7 fewer parameters\")\n",
    "print(f\"  \u2022 Gradual unfreezing provides best accuracy with minimal overfitting risk\")\n",
    "print(f\"  \u2022 Discriminative LR essential for preserving pre-trained features\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cd6c58",
   "metadata": {},
   "source": [
    "# \ud83d\ude80 Part 6: Real-World Projects & Production Deployment\n",
    "\n",
    "## \ud83d\udcca 8 Real-World Transfer Learning Projects\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83d\udd2c Semiconductor Projects (Post-Silicon Validation)**\n",
    "\n",
    "#### **Project 1: Production Wafer Yield Predictor with Multi-Site Data**\n",
    "\n",
    "**Objective:** Train base model on Fab A data, transfer to Fabs B-F with minimal data collection\n",
    "\n",
    "**Business Value:** $50M-$200M/year from cross-site yield optimization\n",
    "\n",
    "**Architecture:**\n",
    "- Base model: EfficientNet-B4 trained on 100K wafer maps from Fab A (primary production site)\n",
    "- Transfer strategy: Gradual unfreezing with domain adaptation for Fabs B-F\n",
    "- Input: 256\u00d7256 wafer maps (higher resolution for fine defect patterns)\n",
    "- Output: Continuous yield% prediction (0-100%)\n",
    "\n",
    "**Implementation Approach:**\n",
    "```python\n",
    "# Pseudocode\n",
    "base_model = train_base_efficientnet_b4(fab_a_data_100k)  # 3 days training\n",
    "\n",
    "for fab in [fab_b, fab_c, fab_d, fab_e, fab_f]:\n",
    "    # Collect only 2K wafer maps per fab (vs 100K from scratch)\n",
    "    small_dataset = collect_wafer_data(fab, n_samples=2000)\n",
    "    \n",
    "    # Transfer with gradual unfreezing\n",
    "    fab_model = copy.deepcopy(base_model)\n",
    "    fine_tune_gradual(fab_model, small_dataset, epochs=15)  # 4 hours\n",
    "    \n",
    "    deploy_to_production(fab, fab_model)\n",
    "```\n",
    "\n",
    "**Success Metrics:**\n",
    "- **Accuracy:** R\u00b2 \u2265 0.90 for yield prediction\n",
    "- **Data efficiency:** 50\u00d7 less data per fab (2K vs 100K wafer maps)\n",
    "- **Training time:** 4 hours fine-tuning vs 3 days from scratch\n",
    "- **Cost savings:** $10M-$40M per fab from optimized test flows\n",
    "\n",
    "**Key Techniques:**\n",
    "- Domain adaptation (handle fab-to-fab process variations)\n",
    "- Multi-task learning (predict yield + top-3 defect patterns simultaneously)\n",
    "- Active learning (prioritize labeling most informative wafer maps)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 2: SEM Image Defect Classification with Few-Shot Learning**\n",
    "\n",
    "**Objective:** Classify 50+ defect types from high-resolution SEM images with <100 samples per class\n",
    "\n",
    "**Business Value:** $2M-$8M/year in faster root-cause analysis, reduce expert annotation cost by $100K-$500K\n",
    "\n",
    "**Architecture:**\n",
    "- Pre-trained: Vision Transformer (ViT-L/16) on ImageNet + fine-tuned on 10K general SEM images\n",
    "- Few-shot approach: Prototypical networks (learn metric space where similar defects cluster)\n",
    "- Input: 4096\u00d74096 SEM images \u2192 512\u00d7512 crops (sliding window)\n",
    "- Output: 50 defect classes (scratches, pits, voids, contaminants, etc.)\n",
    "\n",
    "**Implementation Approach:**\n",
    "```python\n",
    "# Step 1: Pre-train ViT on large general SEM dataset (10K images, 20 classes)\n",
    "vit_base = train_vit_on_general_sem(sem_dataset_10k)\n",
    "\n",
    "# Step 2: Meta-learning for few-shot adaptation\n",
    "prototypical_network = build_prototypical_head(vit_base.features)\n",
    "\n",
    "# Step 3: Fine-tune with few samples (5-50 per new defect class)\n",
    "for new_defect_class in novel_defects_50_classes:\n",
    "    support_set = get_labeled_samples(new_defect_class, n_shot=10)\n",
    "    query_set = get_test_samples(new_defect_class, n=100)\n",
    "    \n",
    "    # Episodic training\n",
    "    prototypical_network.meta_learn(support_set, query_set, episodes=500)\n",
    "\n",
    "# Step 4: Deploy for production defect detection\n",
    "deploy_sem_classifier(prototypical_network)\n",
    "```\n",
    "\n",
    "**Success Metrics:**\n",
    "- **Accuracy:** \u226585% with 10-shot (10 samples per class)\n",
    "- **mAP:** \u22650.88 for multi-label detection\n",
    "- **Inference speed:** <500ms per 4K\u00d74K image (with sliding window)\n",
    "- **Annotation savings:** $100K-$500K (need 50 samples vs 5000 per class)\n",
    "\n",
    "**Key Techniques:**\n",
    "- Prototypical networks (metric learning for few-shot)\n",
    "- Data augmentation specific to SEM (rotation, elastic deformation, Gaussian noise)\n",
    "- Attention visualization (Grad-CAM to show defect locations for engineer trust)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 3: Adaptive Test Program with RL + Transfer Learning**\n",
    "\n",
    "**Objective:** Optimize test sequence dynamically using reinforcement learning agent guided by CNN-extracted wafer features\n",
    "\n",
    "**Business Value:** $20M-$80M/year from reduced test time (15-30% reduction) + improved binning accuracy\n",
    "\n",
    "**Architecture:**\n",
    "- CNN feature extractor: EfficientNet-B3 (frozen, pre-trained on 50K wafer maps)\n",
    "- RL policy network: PPO (Proximal Policy Optimization) actor-critic\n",
    "- State: CNN features (2048-dim) + current test results (128-dim) \u2192 2176-dim\n",
    "- Action: Choose next test from 150 parametric tests + binning decision\n",
    "- Reward: \u2212(test_time_cost) + (binning_accuracy_reward) \u2212 (misclassification_penalty)\n",
    "\n",
    "**Implementation Approach:**\n",
    "```python\n",
    "# Step 1: Pre-train CNN on large wafer dataset (frozen feature extractor)\n",
    "cnn_features = pretrained_efficientnet_b3.forward_features(wafer_map)  # (2048,)\n",
    "\n",
    "# Step 2: RL training\n",
    "class TestSequenceEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.state = concat(cnn_features, current_test_results)\n",
    "        self.action_space = Discrete(151)  # 150 tests + 1 binning action\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action < 150:  # Perform test\n",
    "            test_result, test_time = execute_test(action)\n",
    "            reward = -test_time * 0.1  # Cost of time\n",
    "        else:  # Binning decision\n",
    "            binning_accuracy = evaluate_binning()\n",
    "            reward = binning_accuracy * 100 - misclassification_penalty * 50\n",
    "        return new_state, reward, done, info\n",
    "\n",
    "# Step 3: Train PPO agent\n",
    "ppo_agent = PPO(policy='MlpPolicy', env=TestSequenceEnv(), learning_rate=3e-4)\n",
    "ppo_agent.learn(total_timesteps=1_000_000)  # 2-3 days training\n",
    "\n",
    "# Step 4: Deploy RL-guided test sequence\n",
    "deploy_adaptive_test_program(ppo_agent, cnn_features)\n",
    "```\n",
    "\n",
    "**Success Metrics:**\n",
    "- **Test time reduction:** 15-30% (60 sec \u2192 45 sec average per device)\n",
    "- **Binning accuracy:** \u226599.5% (match expert human decisions)\n",
    "- **Throughput increase:** 20-40% more devices tested per hour\n",
    "- **ROI:** $20M-$80M/year from faster time-to-market + yield improvement\n",
    "\n",
    "**Key Techniques:**\n",
    "- Transfer learning (CNN provides spatial awareness to RL agent)\n",
    "- Curriculum learning (start with easy devices, progress to complex failure modes)\n",
    "- Safe RL (constrain policy to prevent catastrophic misclassifications)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 4: Cross-Product Wafer Map Synthesis with Conditional GANs**\n",
    "\n",
    "**Objective:** Generate synthetic wafer maps for rare defect patterns to augment training data (solve class imbalance)\n",
    "\n",
    "**Business Value:** $5M-$15M/year from improved detection of rare but catastrophic defects (e.g., near-full wafer failures)\n",
    "\n",
    "**Architecture:**\n",
    "- Generator: StyleGAN2 conditioned on defect class label\n",
    "- Discriminator: EfficientNet-B1 (transfer learned from ImageNet)\n",
    "- Input: Noise vector (512-dim) + class label (one-hot 20-dim)\n",
    "- Output: Synthetic 128\u00d7128 wafer map\n",
    "\n",
    "**Implementation Approach:**\n",
    "```python\n",
    "# Step 1: Train conditional GAN\n",
    "generator = StyleGAN2Generator(latent_dim=512, num_classes=20)\n",
    "discriminator = EfficientNetB1Discriminator(num_classes=20)  # Pre-trained features\n",
    "\n",
    "for epoch in range(500):\n",
    "    # Generate fake wafer maps\n",
    "    z = torch.randn(batch_size, 512)\n",
    "    labels = torch.randint(0, 20, (batch_size,))\n",
    "    fake_wafers = generator(z, labels)\n",
    "    \n",
    "    # Train discriminator (real vs fake + classify defect type)\n",
    "    d_loss_real = discriminator(real_wafers, real_labels)\n",
    "    d_loss_fake = discriminator(fake_wafers.detach(), labels)\n",
    "    \n",
    "    # Train generator (fool discriminator + match defect statistics)\n",
    "    g_loss = discriminator(fake_wafers, labels)\n",
    "\n",
    "# Step 2: Augment training data\n",
    "for rare_class in [8, 15, 17, 19]:  # Classes with <500 samples\n",
    "    synthetic_wafers = generator.generate(n=5000, class=rare_class)\n",
    "    training_data.add(synthetic_wafers)\n",
    "\n",
    "# Step 3: Train final classifier on augmented data\n",
    "classifier = train_efficientnet_b3(training_data_augmented)\n",
    "```\n",
    "\n",
    "**Success Metrics:**\n",
    "- **FID score:** \u226450 (synthetic wafers indistinguishable from real)\n",
    "- **Classifier accuracy on rare classes:** +10-15% improvement (75% \u2192 88%)\n",
    "- **Data augmentation ratio:** 5:1 synthetic:real for rare classes\n",
    "- **Cost avoidance:** $2M-$5M (avoid collecting 50K+ real rare-defect wafers)\n",
    "\n",
    "**Key Techniques:**\n",
    "- Conditional GAN (control defect pattern generation)\n",
    "- Transfer learning in discriminator (EfficientNet features improve GAN training stability)\n",
    "- Fr\u00e9chet Inception Distance (FID) for quality evaluation\n",
    "\n",
    "---\n",
    "\n",
    "### **\ud83c\udf10 General AI/ML Projects**\n",
    "\n",
    "#### **Project 5: Medical Image Diagnosis Transfer Learning**\n",
    "\n",
    "**Objective:** Classify chest X-rays (Normal, Pneumonia, COVID-19, Tuberculosis) with limited labeled data\n",
    "\n",
    "**Business Value:** Clinical decision support, reduce radiologist workload by 40%, faster triage\n",
    "\n",
    "**Architecture:**\n",
    "- Pre-trained: DenseNet-121 (ImageNet) \u2192 Fine-tuned on ChestX-ray14 (100K images)\n",
    "- Transfer: Gradual unfreezing for COVID-19 detection (5K labeled images)\n",
    "- Input: 224\u00d7224 grayscale X-ray (replicated to 3 channels)\n",
    "- Output: 4 classes (Normal, Pneumonia, COVID, TB)\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "densenet121 = models.densenet121(pretrained=True)\n",
    "\n",
    "# Phase 1: Train on ChestX-ray14 (general lung pathologies, 100K images)\n",
    "densenet121 = train_on_chestxray14(densenet121, epochs=50)  # 2 days\n",
    "\n",
    "# Phase 2: Transfer to COVID-19 dataset (5K images)\n",
    "densenet121_covid = gradual_unfreeze(densenet121, covid_dataset, epochs=15)  # 3 hours\n",
    "\n",
    "# Ensemble with multiple pre-trained models\n",
    "efficientnet = fine_tune_efficientnet_b4(covid_dataset)\n",
    "resnet50 = fine_tune_resnet50(covid_dataset)\n",
    "ensemble = weighted_average([densenet121_covid, efficientnet, resnet50])\n",
    "```\n",
    "\n",
    "**Success Metrics:**\n",
    "- **AUC-ROC:** \u22650.95 for COVID-19 detection\n",
    "- **Sensitivity:** \u226592% (catch most positive cases)\n",
    "- **Specificity:** \u226588% (minimize false alarms)\n",
    "- **Inference time:** <100ms per X-ray (real-time triage)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 6: Autonomous Vehicle Object Detection with Domain Adaptation**\n",
    "\n",
    "**Objective:** Transfer object detection from COCO dataset (natural images) to automotive cameras (different lighting, angles, weather)\n",
    "\n",
    "**Business Value:** Autonomous driving, reduce annotation cost by $500K-$2M, faster deployment\n",
    "\n",
    "**Architecture:**\n",
    "- Pre-trained: YOLOv8-X (COCO dataset, 80 classes)\n",
    "- Transfer: Fine-tune on autonomous driving dataset (BDD100K, 10 classes: car, truck, pedestrian, cyclist, etc.)\n",
    "- Domain adaptation: Adapt to different weather (rain, fog, night)\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "yolov8 = YOLO('yolov8x.pt')  # Pre-trained on COCO\n",
    "\n",
    "# Fine-tune on BDD100K (autonomous driving dataset)\n",
    "yolov8.train(data='bdd100k.yaml', epochs=100, imgsz=640, batch=16)\n",
    "\n",
    "# Domain adaptation for adverse weather\n",
    "yolov8_rain = domain_adapt(yolov8, rain_images, method='CycleGAN')\n",
    "yolov8_fog = domain_adapt(yolov8, fog_images, method='CycleGAN')\n",
    "yolov8_night = domain_adapt(yolov8, night_images, method='adversarial')\n",
    "\n",
    "# Deploy ensemble\n",
    "ensemble_yolo = MultiDomainYOLO([yolov8, yolov8_rain, yolov8_fog, yolov8_night])\n",
    "```\n",
    "\n",
    "**Success Metrics:**\n",
    "- **mAP@0.5:** \u22650.65 (COCO-style evaluation)\n",
    "- **Inference speed:** <30ms per frame (real-time at 30 FPS)\n",
    "- **Robustness:** \u226585% accuracy in rain/fog/night (vs 95% in clear day)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 7: Satellite Image Change Detection**\n",
    "\n",
    "**Objective:** Detect changes in satellite imagery (buildings, deforestation, floods) using pre-trained models\n",
    "\n",
    "**Business Value:** Disaster response, urban planning, environmental monitoring\n",
    "\n",
    "**Architecture:**\n",
    "- Pre-trained: ResNet-101 (ImageNet) \u2192 Fine-tuned on xView (1M labeled buildings)\n",
    "- Transfer: Siamese network for change detection (compare before/after images)\n",
    "- Input: Pair of 256\u00d7256 RGB satellite images (t1, t2)\n",
    "- Output: Change map (pixel-wise classification: no change, new building, demolished, flooded, etc.)\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "# Siamese architecture\n",
    "resnet101_backbone = models.resnet101(pretrained=True)\n",
    "resnet101_backbone.fc = nn.Identity()  # Remove classifier, use features only\n",
    "\n",
    "class ChangeDetectionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.backbone = resnet101_backbone  # Shared weights\n",
    "        self.fusion = nn.Conv2d(4096, 512, 1)  # Fuse t1 + t2 features\n",
    "        self.decoder = UNetDecoder(512, num_classes=5)  # Upsample to change map\n",
    "    \n",
    "    def forward(self, img_t1, img_t2):\n",
    "        feat_t1 = self.backbone(img_t1)  # (B, 2048, 8, 8)\n",
    "        feat_t2 = self.backbone(img_t2)  # (B, 2048, 8, 8)\n",
    "        concat = torch.cat([feat_t1, feat_t2], dim=1)  # (B, 4096, 8, 8)\n",
    "        fused = self.fusion(concat)  # (B, 512, 8, 8)\n",
    "        change_map = self.decoder(fused)  # (B, 5, 256, 256)\n",
    "        return change_map\n",
    "\n",
    "model = ChangeDetectionNet()\n",
    "train_on_change_detection_dataset(model, epochs=50)\n",
    "```\n",
    "\n",
    "**Success Metrics:**\n",
    "- **IoU:** \u22650.75 for building change detection\n",
    "- **Precision:** \u226588% (minimize false alarms for disaster response)\n",
    "- **Processing speed:** <2 sec per 10 km\u00b2 satellite tile\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 8: Product Recommendation with Visual Features**\n",
    "\n",
    "**Objective:** Extract visual features from product images to improve recommendation system (e.g., \"similar items\")\n",
    "\n",
    "**Business Value:** E-commerce, increase click-through rate by 15-25%, boost sales by $10M-$50M\n",
    "\n",
    "**Architecture:**\n",
    "- Pre-trained: EfficientNet-B5 (ImageNet) \u2192 Feature extractor (no fine-tuning)\n",
    "- Transfer: Use 2048-dim image embeddings for similarity search\n",
    "- Recommendation: Combine visual features + user behavior + product metadata\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "efficientnet_b5 = timm.create_model('efficientnet_b5', pretrained=True, num_classes=0)  # No classifier\n",
    "efficientnet_b5.eval()\n",
    "\n",
    "# Extract features for entire product catalog\n",
    "product_catalog = load_product_images()  # 1M products\n",
    "embeddings = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for product_id, image in tqdm(product_catalog.items()):\n",
    "        image_tensor = preprocess(image)\n",
    "        embedding = efficientnet_b5(image_tensor)  # (2048,)\n",
    "        embeddings[product_id] = embedding.cpu().numpy()\n",
    "\n",
    "# Build FAISS index for fast similarity search\n",
    "import faiss\n",
    "index = faiss.IndexFlatL2(2048)\n",
    "index.add(np.array(list(embeddings.values())))\n",
    "\n",
    "# Find similar products\n",
    "query_embedding = embeddings['product_12345']\n",
    "distances, indices = index.search(query_embedding.reshape(1, -1), k=10)\n",
    "similar_products = [product_ids[i] for i in indices[0]]\n",
    "\n",
    "# Hybrid recommendation (visual + collaborative filtering)\n",
    "recommendations = combine_visual_and_cf(similar_products, user_history)\n",
    "```\n",
    "\n",
    "**Success Metrics:**\n",
    "- **Click-through rate:** +15-25% on \"similar items\" section\n",
    "- **Conversion rate:** +8-12% from visual recommendations\n",
    "- **Inference speed:** <50ms per query (sub-second response)\n",
    "- **Revenue impact:** $10M-$50M/year for large e-commerce platform\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf93 Key Takeaways & Best Practices\n",
    "\n",
    "### **When to Use Each Transfer Learning Strategy**\n",
    "\n",
    "| Strategy | Use Case | Data Size | Domain Similarity | Computational Budget |\n",
    "|----------|----------|-----------|-------------------|----------------------|\n",
    "| **Feature Extraction** | Quick prototyping, very small dataset | <1K | High (similar to ImageNet) | Low (1\u00d7 baseline) |\n",
    "| **Full Fine-Tuning** | Maximum accuracy, sufficient data | >5K | Medium-Low (domain shift) | High (10\u00d7 baseline) |\n",
    "| **Gradual Unfreezing** | **Production standard**, balanced approach | 1K-10K | Any | Medium (3-5\u00d7 baseline) |\n",
    "\n",
    "**Recommendation:** **Start with gradual unfreezing** (Strategy 3) for most projects.\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Selection Guide**\n",
    "\n",
    "| Model | Parameters | Speed | Accuracy | When to Use |\n",
    "|-------|------------|-------|----------|-------------|\n",
    "| **ResNet-50** | 25.6M | Medium | Good | Industry baseline, debugging, interpretability |\n",
    "| **EfficientNet-B3** | 12.0M | **Fast** | **Best** | **Production (recommended)**, edge devices, cost-sensitive |\n",
    "| **Vision Transformer** | 86.6M | Slow | Excellent | Research, very large datasets (>50K), interpretability (attention maps) |\n",
    "\n",
    "**Recommendation:** **EfficientNet-B3** for semiconductor production (best accuracy/efficiency tradeoff).\n",
    "\n",
    "---\n",
    "\n",
    "### **Learning Rate Strategies**\n",
    "\n",
    "1. **Discriminative LR (ESSENTIAL):**\n",
    "   - Early layers: 1e-6 to 1e-5 (preserve ImageNet features)\n",
    "   - Late layers: 1e-4 to 1e-3 (adapt to target domain)\n",
    "   - Formula: $\\eta_{\\text{layer } i} = \\eta_{\\text{base}} \\times \\text{decay}^{L-i}$\n",
    "\n",
    "2. **Warm-up + Cosine Annealing (RECOMMENDED):**\n",
    "   - Warm-up: 2-5 epochs, linear increase 0.1\u00d7 \u2192 1.0\u00d7\n",
    "   - Cosine decay: Smooth convergence, prevents oscillations\n",
    "   - PyTorch: `SequentialLR(LinearLR, CosineAnnealingLR)`\n",
    "\n",
    "3. **Cyclical LR (ALTERNATIVE):**\n",
    "   - Good for escaping local minima\n",
    "   - Use when training plateaus\n",
    "\n",
    "---\n",
    "\n",
    "### **Domain Adaptation Checklist**\n",
    "\n",
    "For semiconductor (ImageNet \u2192 grayscale wafer maps):\n",
    "\n",
    "\u2705 **Data Preprocessing:**\n",
    "- Replicate grayscale to 3 channels: `image.repeat(3, 1, 1)`\n",
    "- Normalize with ImageNet stats: `mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]`\n",
    "- Resize to 224\u00d7224 (or 299\u00d7299 for Inception, 384\u00d7384 for ViT)\n",
    "\n",
    "\u2705 **Data Augmentation (Wafer-Specific):**\n",
    "- \u2705 Rotation (0-360\u00b0, wafers have rotational symmetry)\n",
    "- \u2705 Horizontal flip (left-right symmetry)\n",
    "- \u2705 Gaussian noise (sensor variability)\n",
    "- \u2705 Brightness/contrast (equipment differences)\n",
    "- \u274c Color jitter (grayscale only)\n",
    "- \u274c Vertical flip (breaks edge defect semantics)\n",
    "\n",
    "\u2705 **Batch Normalization Adaptation:**\n",
    "- Option 1: Adaptive BN (compute running stats on target domain, no weight updates)\n",
    "- Option 2: Fine-tune BN layers only (unfreeze `running_mean`, `running_var`)\n",
    "\n",
    "---\n",
    "\n",
    "### **Production Deployment Pipeline**\n",
    "\n",
    "**Step 1: Model Compression**\n",
    "```python\n",
    "# INT8 Quantization (4\u00d7 smaller, 2-3\u00d7 faster)\n",
    "from torch.quantization import quantize_dynamic\n",
    "quantized_model = quantize_dynamic(efficientnet, {nn.Linear, nn.Conv2d}, dtype=torch.qint8)\n",
    "\n",
    "# Pruning (remove 30-50% weights)\n",
    "from torch.nn.utils import prune\n",
    "prune.l1_unstructured(efficientnet.classifier, name='weight', amount=0.3)\n",
    "```\n",
    "\n",
    "**Step 2: ONNX Export**\n",
    "```python\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "torch.onnx.export(efficientnet, dummy_input, \"efficientnet_b3.onnx\",\n",
    "                  input_names=['input'], output_names=['output'],\n",
    "                  dynamic_axes={'input': {0: 'batch_size'}})\n",
    "```\n",
    "\n",
    "**Step 3: TensorRT Optimization** (NVIDIA GPUs)\n",
    "```bash\n",
    "trtexec --onnx=efficientnet_b3.onnx --saveEngine=efficientnet_b3.trt \\\n",
    "        --fp16 --workspace=4096  # Mixed precision, 4GB workspace\n",
    "```\n",
    "\n",
    "**Step 4: Inference Server** (TorchServe/TF Serving)\n",
    "```python\n",
    "# TorchServe deployment\n",
    "torch-model-archiver --model-name efficientnet_wafer_classifier \\\n",
    "                     --version 1.0 \\\n",
    "                     --serialized-file efficientnet_b3.onnx \\\n",
    "                     --handler image_classifier\n",
    "\n",
    "torchserve --start --model-store model_store --models efficientnet_wafer_classifier.mar\n",
    "```\n",
    "\n",
    "**Step 5: Monitoring & Retraining**\n",
    "- **Track inference metrics:** Prediction confidence, latency, throughput\n",
    "- **Detect distribution drift:** KL divergence on prediction distributions\n",
    "- **Active learning:** Flag low-confidence predictions for expert review\n",
    "- **Retraining schedule:** Monthly or when accuracy drops >2%\n",
    "\n",
    "---\n",
    "\n",
    "### **Semiconductor-Specific Best Practices**\n",
    "\n",
    "1. **Class Imbalance Handling:**\n",
    "   - Use weighted loss: `nn.CrossEntropyLoss(weight=class_weights)`\n",
    "   - Focal loss for hard examples: $FL(p_t) = -(1-p_t)^\\gamma \\log(p_t)$\n",
    "   - SMOTE (Synthetic Minority Over-sampling) for rare defects\n",
    "\n",
    "2. **Spatial Correlation:**\n",
    "   - CNNs naturally capture spatial patterns (wafer maps)\n",
    "   - Consider graph neural networks (GNNs) for die-to-die spatial dependencies\n",
    "\n",
    "3. **Explainability (CRITICAL for engineer trust):**\n",
    "   - Grad-CAM: Visualize which wafer regions drive prediction\n",
    "   - SHAP: Feature importance for parametric test data\n",
    "   - Sanity checks: Ensure model learns defect patterns, not spurious correlations\n",
    "\n",
    "4. **Multi-Task Learning:**\n",
    "   - Simultaneously predict: Yield% + Defect class + Severity + Root cause\n",
    "   - Shared backbone, multiple heads \u2192 Better feature utilization\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcda What's Next?\n",
    "\n",
    "**Upcoming Notebooks:**\n",
    "- **055: Object Detection (YOLO, R-CNN)** \u2192 Localize defects on wafer, not just classify\n",
    "- **056: RNN/LSTM/GRU** \u2192 Sequential test pattern analysis (time-series wafer data)\n",
    "- **057: Seq2Seq & Attention** \u2192 Test sequence optimization\n",
    "- **058: Transformers** \u2192 Self-attention for spatial wafer map features + BERT-style pre-training\n",
    "\n",
    "---\n",
    "\n",
    "## \u2705 Learning Objectives Review\n",
    "\n",
    "By completing this notebook, you've mastered:\n",
    "\n",
    "1. \u2705 **Transfer Learning Theory** - Feature hierarchy, domain adaptation, mathematical formulation\n",
    "2. \u2705 **Pre-trained Model Zoo** - ResNet-50, EfficientNet-B3, Vision Transformer comparison\n",
    "3. \u2705 **Fine-Tuning Strategies** - Feature extraction, full fine-tuning, gradual unfreezing (best practice)\n",
    "4. \u2705 **Learning Rate Policies** - Discriminative LR, warm-up, cosine annealing, cyclical LR\n",
    "5. \u2705 **Feature Extraction vs Fine-Tuning** - When to freeze, when to train, computational tradeoffs\n",
    "6. \u2705 **Domain Adaptation** - ImageNet \u2192 semiconductor (grayscale, augmentation, batch norm)\n",
    "7. \u2705 **Multi-Task Transfer Learning** - Leveraging multiple pre-trained models (ensemble)\n",
    "8. \u2705 **Production Deployment** - ONNX export, INT8 quantization, TensorRT optimization, inference serving\n",
    "\n",
    "**Key Skill Acquired:** You can now apply transfer learning to any image classification problem with confidence!\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcd6 Additional Resources\n",
    "\n",
    "**Must-Read Papers:**\n",
    "- \"Visualizing and Understanding Convolutional Networks\" (Zeiler & Fergus, 2013) - Why transfer learning works\n",
    "- \"How transferable are features in deep neural networks?\" (Yosinski et al., 2014) - Feature hierarchy analysis\n",
    "- \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\" (Tan & Le, 2019)\n",
    "- \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" (Dosovitskiy et al., 2020) - Vision Transformer\n",
    "\n",
    "**Courses:**\n",
    "- CS231n (Stanford) - Lecture 11: Transfer Learning & Fine-Tuning\n",
    "- Fast.ai Practical Deep Learning - Lesson 1 (transfer learning focus)\n",
    "\n",
    "**Libraries:**\n",
    "- **timm** (PyTorch Image Models): 700+ pre-trained models - https://github.com/huggingface/pytorch-image-models\n",
    "- **TensorFlow Hub**: Pre-trained models for TensorFlow - https://tfhub.dev\n",
    "- **ONNX Runtime**: Cross-framework inference - https://onnxruntime.ai\n",
    "\n",
    "**Deployment Tools:**\n",
    "- **TorchServe**: PyTorch model serving - https://pytorch.org/serve\n",
    "- **TensorRT**: NVIDIA GPU optimization - https://developer.nvidia.com/tensorrt\n",
    "- **TF Serving**: TensorFlow model serving - https://www.tensorflow.org/tfx/guide/serving\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Final Summary\n",
    "\n",
    "**Transfer Learning ROI:**\n",
    "- **10-100\u00d7 less data** needed (1K vs 100K samples)\n",
    "- **10-100\u00d7 faster training** (hours vs days)\n",
    "- **5-15% higher accuracy** than training from scratch\n",
    "- **$5M-$200M business value** for semiconductor applications\n",
    "\n",
    "**Best Practices:**\n",
    "1. **Always start with pre-trained models** (ImageNet baseline)\n",
    "2. **Use gradual unfreezing** (Strategy 3) for production\n",
    "3. **Discriminative LR is essential** (early layers slower, late layers faster)\n",
    "4. **EfficientNet-B3 recommended** for semiconductor (best accuracy/efficiency)\n",
    "5. **Monitor for distribution drift** (retrain when accuracy drops)\n",
    "\n",
    "**You're now ready to deploy production-grade transfer learning systems!** \ud83d\ude80\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations on completing Notebook 054!** \ud83c\udf89\n",
    "\n",
    "Next notebook: **055_Object_Detection_YOLO_RCNN.ipynb** - Learn to localize defects, not just classify wafer maps!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}