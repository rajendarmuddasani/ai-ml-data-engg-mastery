{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a1a9e78",
   "metadata": {},
   "source": [
    "# 053: CNN Architectures (LeNet, AlexNet, VGG, ResNet)",
    "",
    "## **From Image Recognition to Semiconductor Defect Detection**",
    "",
    "---",
    "",
    "### **\ud83d\udcda Learning Objectives**",
    "",
    "By the end of this notebook, you will:",
    "",
    "1. \u2705 **Understand convolutions:** How convolution operations extract spatial features from images",
    "2. \u2705 **Master CNN building blocks:** Convolutions, pooling, normalization, activation functions",
    "3. \u2705 **Implement from scratch:** Build convolution operation using NumPy (educational)",
    "4. \u2705 **Build CNNs in PyTorch & Keras:** Practical implementations with modern frameworks",
    "5. \u2705 **Study classic architectures:** LeNet, AlexNet, VGG, Inception, ResNet (evolution of CNNs)",
    "6. \u2705 **Apply to semiconductor testing:** Wafer map defect pattern classification (20+ defect types)",
    "7. \u2705 **Transfer learning:** Use pre-trained models (ImageNet) for semiconductor applications",
    "8. \u2705 **Optimize for production:** Model compression, inference speed, deployment strategies",
    "",
    "---",
    "",
    "## **Why Convolutional Neural Networks?**",
    "",
    "### **The Image Recognition Revolution**",
    "",
    "**Before CNNs (pre-2012):**",
    "- Hand-crafted features (SIFT, HOG, Haar cascades)",
    "- Shallow models (SVM, Random Forest on engineered features)",
    "- **ImageNet accuracy:** ~75% (2011 winner)",
    "",
    "**After CNNs (2012-present):**",
    "- Automatic feature learning from data",
    "- Deep hierarchical representations",
    "- **ImageNet accuracy:** 88%+ (ResNet-152, 2015), superhuman performance",
    "",
    "**Breakthrough moment:** AlexNet (2012) won ImageNet with 84.7% accuracy (vs 73.8% second place) using CNNs + GPUs.",
    "",
    "---",
    "",
    "### **Why CNNs Work for Images**",
    "",
    "**Traditional fully-connected networks fail for images:**",
    "- **Too many parameters:** 224\u00d7224\u00d73 image = 150K pixels \u2192 150K\u00d71000 = 150M weights for first layer alone!",
    "- **No spatial structure:** Treats pixels as independent features (ignores proximity)",
    "- **Not translation invariant:** Cat in top-left \u2260 cat in bottom-right (must learn separately)",
    "",
    "**CNNs solve these problems:**",
    "- \u2705 **Parameter sharing:** Same filter applied to entire image (1K parameters vs 150M)",
    "- \u2705 **Spatial locality:** Convolutions preserve 2D structure (nearby pixels processed together)",
    "- \u2705 **Translation invariance:** Same filter detects features anywhere in image",
    "- \u2705 **Hierarchical features:** Low-level edges \u2192 mid-level textures \u2192 high-level objects",
    "",
    "---",
    "",
    "### **Semiconductor Post-Silicon Validation Use Case**",
    "",
    "**Challenge:** Classify defect patterns on semiconductor wafer maps.",
    "",
    "**Wafer map:** 2D spatial representation of die pass/fail status on a wafer (300mm diameter, 10K-50K die).",
    "",
    "**Defect patterns indicate root causes:**",
    "- **Ring pattern:** Chamber conditioning issue (plasma uniformity)",
    "- **Edge failures:** Edge exclusion problem (contamination)",
    "- **Cluster defects:** Particle contamination (specific wafer location)",
    "- **Scratch pattern:** Wafer handling damage (linear defect)",
    "- **Radial pattern:** Process gradient (center-to-edge variation)",
    "- **Random failures:** Random defects (no spatial correlation)",
    "",
    "**Business value:** $5M-$20M per incident through **faster root cause analysis** (reduce time from days/weeks to hours).",
    "",
    "**Why CNNs excel:**",
    "- Spatial patterns (not just individual die failures)",
    "- Translation invariance (defect can appear anywhere on wafer)",
    "- Rotation invariance (with data augmentation)",
    "- Pre-trained models (transfer learning from ImageNet)",
    "",
    "---",
    "",
    "### **CNN Applications Beyond Image Recognition**",
    "",
    "| Domain | Application | Input | Output |",
    "|--------|-------------|-------|--------|",
    "| **Computer Vision** | Object detection | Image | Bounding boxes + classes |",
    "| **Computer Vision** | Semantic segmentation | Image | Pixel-wise labels |",
    "| **Computer Vision** | Face recognition | Face image | Identity |",
    "| **Medical Imaging** | Tumor detection | CT/MRI scan | Tumor location + type |",
    "| **Autonomous Driving** | Scene understanding | Camera feed | Objects, lanes, signs |",
    "| **Semiconductor** | Wafer defect classification | Wafer map (2D) | Defect type (20+ classes) |",
    "| **Semiconductor** | Die image inspection | SEM/optical images | Defect detection |",
    "| **Time-Series** | 1D CNNs for signals | Sensor data | Anomaly detection |",
    "| **NLP** | Text classification | Word embeddings | Sentiment/category |",
    "",
    "---",
    "",
    "## **What We'll Build**",
    "",
    "### **1. Educational: Convolution from Scratch (NumPy)**",
    "",
    "Implement convolution operation to understand the math:",
    "```",
    "Input: 28\u00d728 grayscale image",
    "Filter: 3\u00d73 edge detector",
    "Output: 26\u00d726 feature map",
    "```",
    "",
    "### **2. Production: Wafer Map Defect Classifier (PyTorch + Keras)**",
    "",
    "**Architecture (Custom CNN):**",
    "```",
    "Input(300\u00d7300\u00d71) \u2192 Conv2D(32, 5\u00d75, stride=1) + ReLU + MaxPool(2\u00d72)",
    "                 \u2192 Conv2D(64, 3\u00d73, stride=1) + ReLU + MaxPool(2\u00d72)",
    "                 \u2192 Conv2D(128, 3\u00d73, stride=1) + ReLU + MaxPool(2\u00d72)",
    "                 \u2192 Flatten",
    "                 \u2192 Dense(256) + Dropout(0.5)",
    "                 \u2192 Output(20, Softmax)  # 20 defect classes",
    "```",
    "",
    "**Dataset:** 10K wafer maps (300\u00d7300 pixels), 20 defect classes.",
    "",
    "**Metrics:** Top-1 accuracy \u226595%, top-3 accuracy \u226598%, inference <100ms.",
    "",
    "---",
    "",
    "### **3. Transfer Learning: ResNet-50 Fine-Tuned on Wafer Maps**",
    "",
    "**Why transfer learning?**",
    "- Pre-trained on ImageNet (1.2M images, 1000 classes)",
    "- Low-level features (edges, textures) transfer to wafer maps",
    "- Requires 10\u00d7 less data (1K samples vs 10K+ from scratch)",
    "- Faster convergence (5 epochs vs 50+ from scratch)",
    "",
    "**Approach:**",
    "1. Load pre-trained ResNet-50 (ImageNet weights)",
    "2. Replace final layer (1000 classes \u2192 20 defect classes)",
    "3. Freeze early layers (keep learned features)",
    "4. Fine-tune last few layers on wafer map data",
    "5. Optionally unfreeze all layers for full fine-tuning",
    "",
    "**Expected results:** 98%+ accuracy with 1K training samples (vs 95% from scratch with 10K samples).",
    "",
    "---",
    "",
    "## **Notebook Roadmap**",
    "",
    "### **Part 1: Convolution Fundamentals** (Cells 2-3)",
    "- Mathematical definition of convolution",
    "- From-scratch implementation (NumPy)",
    "- Convolution properties (padding, stride, dilation)",
    "- Pooling operations (max, average, global)",
    "",
    "### **Part 2: CNN Building Blocks** (Cell 4)",
    "- Conv2D, BatchNorm, ReLU, MaxPool, Dropout",
    "- Building custom CNNs in PyTorch and Keras",
    "- Training on synthetic wafer maps",
    "",
    "### **Part 3: Classic CNN Architectures** (Cell 5)",
    "- **LeNet-5 (1998):** First successful CNN (MNIST)",
    "- **AlexNet (2012):** ImageNet breakthrough (ReLU, dropout, GPU)",
    "- **VGG (2014):** Simplicity of 3\u00d73 filters (depth matters)",
    "- **Inception/GoogLeNet (2014):** Multi-scale features (1\u00d71, 3\u00d73, 5\u00d75 in parallel)",
    "- **ResNet (2015):** Skip connections (train 152+ layers)",
    "",
    "### **Part 4: Transfer Learning & Fine-Tuning** (Cell 6)",
    "- Load pre-trained ResNet-50 from ImageNet",
    "- Fine-tune on wafer map dataset",
    "- Compare with training from scratch",
    "- Feature extraction vs full fine-tuning",
    "",
    "### **Part 5: Production Deployment** (Cell 7)",
    "- Model optimization (pruning, quantization, ONNX)",
    "- Real-time inference (<100ms per wafer)",
    "- Deployment strategies (TorchServe, TF Serving)",
    "- Explainability (Grad-CAM visualization)",
    "",
    "### **Part 6: Real-World Projects** (Cell 8)",
    "- 8 comprehensive projects (4 semiconductor + 4 general AI/ML)",
    "- Business value, architecture, success metrics",
    "- Key takeaways and best practices",
    "",
    "---",
    "",
    "## **Architecture Evolution Timeline**",
    "",
    "```mermaid",
    "graph LR",
    "    A[LeNet-5<br/>1998<br/>~60K params] --> B[AlexNet<br/>2012<br/>60M params<br/>ImageNet winner]",
    "    B --> C[VGG-16<br/>2014<br/>138M params<br/>Simple, deep]",
    "    B --> D[Inception v1<br/>2014<br/>6M params<br/>Multi-scale]",
    "    C --> E[ResNet-50<br/>2015<br/>25M params<br/>Skip connections]",
    "    D --> E",
    "    E --> F[EfficientNet<br/>2019<br/>5M params<br/>NAS + scaling]",
    "    E --> G[Vision Transformer<br/>2020<br/>86M params<br/>Attention-based]",
    "    ",
    "    style B fill:#ff9999",
    "    style E fill:#99ff99",
    "    style G fill:#9999ff",
    "```",
    "",
    "**Key milestones:**",
    "- **1998:** LeNet-5 (handwritten digits, ~99% MNIST)",
    "- **2012:** AlexNet (ImageNet breakthrough, 84.7% top-5 accuracy)",
    "- **2014:** VGG (simplicity wins), Inception (multi-scale features)",
    "- **2015:** ResNet (**revolution**, 152 layers, 96% ImageNet)",
    "- **2017:** DenseNet (connect everything), MobileNet (mobile efficiency)",
    "- **2019:** EfficientNet (compound scaling, SOTA efficiency)",
    "- **2020:** Vision Transformer (attention replaces convolutions)",
    "",
    "---",
    "",
    "## **Prerequisites**",
    "",
    "**Required:**",
    "- \u2705 Notebook 051: Neural Networks Foundations (backpropagation, gradients)",
    "- \u2705 Notebook 052: Deep Learning Frameworks (PyTorch, Keras)",
    "- \u2705 Linear algebra basics (matrix multiplication, convolution as matrix operation)",
    "- \u2705 Image processing basics (pixels, channels, RGB vs grayscale)",
    "",
    "**Helpful:**",
    "- Basic understanding of image filters (blur, edge detection)",
    "- Familiarity with image datasets (MNIST, CIFAR-10, ImageNet)",
    "- Experience with data augmentation (rotation, flip, zoom)",
    "",
    "---",
    "",
    "## **Installation & Setup**",
    "",
    "```bash",
    "# Core libraries (already installed in Notebook 052)",
    "pip install torch torchvision tensorflow",
    "",
    "# Additional for CNNs",
    "pip install opencv-python  # Image processing",
    "pip install albumentations  # Advanced data augmentation",
    "pip install timm  # PyTorch Image Models (pre-trained CNNs)",
    "pip install grad-cam  # Explainability (visualize what CNN learned)",
    "",
    "# For wafer map generation",
    "pip install scikit-image  # Spatial pattern generation",
    "```",
    "",
    "**Check installation:**",
    "```python",
    "import torch",
    "import torchvision",
    "import tensorflow as tf",
    "import cv2",
    "print(f\"PyTorch: {torch.__version__}\")",
    "print(f\"TorchVision: {torchvision.__version__}\")",
    "print(f\"TensorFlow: {tf.__version__}\")",
    "print(f\"OpenCV: {cv2.__version__}\")",
    "```",
    "",
    "---",
    "",
    "## **Dataset Overview**",
    "",
    "### **Synthetic Wafer Map Dataset (For This Notebook)**",
    "",
    "We'll generate synthetic wafer maps with realistic defect patterns:",
    "",
    "**Patterns:**",
    "1. **None (normal):** Random fail rate 0.5-2%",
    "2. **Center:** High failure in center (Gaussian distribution)",
    "3. **Edge:** High failure at wafer edge (ring pattern)",
    "4. **Scratch:** Linear defect (horizontal, vertical, or diagonal)",
    "5. **Ring:** Circular ring of failures (process uniformity issue)",
    "6. **Near-full:** Almost all die fail (catastrophic failure)",
    "7. **Donut:** Ring with good center (edge exclusion + center good)",
    "8. **Cluster:** Localized cluster of failures (particle contamination)",
    "9. **Random:** Completely random failures (no spatial pattern)",
    "10. **Radial:** Radial gradient (center-to-edge variation)",
    "... (20 total classes)",
    "",
    "**Dataset split:**",
    "- Training: 7,000 wafer maps (350 per class)",
    "- Validation: 1,500 wafer maps (75 per class)",
    "- Test: 1,500 wafer maps (75 per class)",
    "",
    "---",
    "",
    "## **Success Metrics**",
    "",
    "**Model Performance:**",
    "- **Top-1 accuracy:** \u226595% (correct class)",
    "- **Top-3 accuracy:** \u226598% (correct class in top 3 predictions)",
    "- **Precision/Recall:** \u226590% per class (balanced performance)",
    "- **Confusion matrix:** Identify commonly confused patterns",
    "",
    "**Production Requirements:**",
    "- **Inference time:** <100ms per wafer (CPU), <10ms (GPU)",
    "- **Model size:** <50MB (for edge deployment)",
    "- **Explainability:** Grad-CAM visualization of decision regions",
    "",
    "**Business Impact:**",
    "- **Root cause time:** Reduce from days/weeks \u2192 hours",
    "- **Cost savings:** $5M-$20M per incident through faster resolution",
    "- **Accuracy:** 95%+ reduces false alarms (minimize wasted engineering time)",
    "",
    "---",
    "",
    "## **Learning Path Context**",
    "",
    "**Where we are:**",
    "- \u2705 **Notebook 051:** Neural networks from scratch (MLPs, backpropagation)",
    "- \u2705 **Notebook 052:** Deep learning frameworks (PyTorch, Keras, ONNX)",
    "- \ud83d\udd25 **Notebook 053:** Convolutional networks (spatial features, images)",
    "",
    "**Where we're going:**",
    "- \ud83d\udcd8 **Notebook 054:** Recurrent networks (RNNs, LSTMs for sequences)",
    "- \ud83d\udcd8 **Notebook 055:** Transformers and attention (modern NLP and vision)",
    "- \ud83d\udcd8 **Notebook 056:** Generative models (GANs, VAEs, diffusion)",
    "- \ud83d\udcd8 **Notebook 057:** Reinforcement learning (agents, policies)",
    "",
    "---",
    "",
    "## **Time Investment**",
    "",
    "- **Reading + code execution:** 4-5 hours",
    "- **Practice exercises:** 2-3 hours",
    "- **Real-world project:** 8-12 hours (wafer map classifier)",
    "- **Total:** 14-20 hours for mastery",
    "",
    "**Recommended:** Spread over 4-6 sessions, practice with your own image data between sessions.",
    "",
    "---",
    "",
    "## \u2705 **Learning Objectives Checklist**",
    "",
    "- [ ] Understand how convolution operations work mathematically",
    "- [ ] Implement convolution from scratch using NumPy",
    "- [ ] Build CNNs in PyTorch and TensorFlow/Keras",
    "- [ ] Explain the architecture evolution (LeNet \u2192 AlexNet \u2192 VGG \u2192 Inception \u2192 ResNet)",
    "- [ ] Apply transfer learning to new domains (ImageNet \u2192 wafer maps)",
    "- [ ] Optimize CNNs for production (quantization, pruning, ONNX)",
    "- [ ] Deploy CNN models with <100ms inference time",
    "- [ ] Visualize what CNNs learn using Grad-CAM",
    "",
    "---",
    "",
    "**Let's dive into the world of Convolutional Neural Networks!** \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a0278a",
   "metadata": {},
   "source": [
    "## \ud83e\uddee Part 1: Convolution Mathematics & Implementation\n",
    "\n",
    "### **What is Convolution?**\n",
    "\n",
    "**Intuition:** Slide a small filter (kernel) over an image, computing dot products at each position.\n",
    "\n",
    "**Mathematical definition:**\n",
    "\n",
    "For 2D convolution (image processing):\n",
    "\n",
    "$$\n",
    "(I * K)(x, y) = \\sum_{i=-\\infty}^{\\infty} \\sum_{j=-\\infty}^{\\infty} I(x-i, y-j) \\cdot K(i, j)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $I$ = input image (e.g., 28\u00d728 pixels)\n",
    "- $K$ = convolution kernel/filter (e.g., 3\u00d73)\n",
    "- $*$ = convolution operation\n",
    "- $(x, y)$ = output position\n",
    "\n",
    "**Discrete form (finite image):**\n",
    "\n",
    "$$\n",
    "(I * K)(x, y) = \\sum_{i=0}^{k_h-1} \\sum_{j=0}^{k_w-1} I(x+i, y+j) \\cdot K(i, j)\n",
    "$$\n",
    "\n",
    "Where $k_h, k_w$ are kernel height and width.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example: 3\u00d73 Edge Detection Filter**\n",
    "\n",
    "**Input image $I$ (5\u00d75):**\n",
    "```\n",
    "[[10, 10, 10,  0,  0],\n",
    " [10, 10, 10,  0,  0],\n",
    " [10, 10, 10,  0,  0],\n",
    " [10, 10, 10,  0,  0],\n",
    " [10, 10, 10,  0,  0]]\n",
    "```\n",
    "\n",
    "**Sobel filter $K$ (3\u00d73) - vertical edge detector:**\n",
    "```\n",
    "[[-1,  0,  1],\n",
    " [-2,  0,  2],\n",
    " [-1,  0,  1]]\n",
    "```\n",
    "\n",
    "**Convolution at position (1,1):**\n",
    "```\n",
    "Result = 10\u00d7(-1) + 10\u00d70 + 10\u00d71\n",
    "       + 10\u00d7(-2) + 10\u00d70 + 10\u00d72\n",
    "       + 10\u00d7(-1) + 10\u00d70 + 10\u00d71\n",
    "       = -10 + 0 + 10 - 20 + 0 + 20 - 10 + 0 + 10\n",
    "       = 0\n",
    "```\n",
    "\n",
    "**At position (1,2) - on the edge:**\n",
    "```\n",
    "Result = 10\u00d7(-1) + 10\u00d70 + 0\u00d71\n",
    "       + 10\u00d7(-2) + 10\u00d70 + 0\u00d72\n",
    "       + 10\u00d7(-1) + 10\u00d70 + 0\u00d71\n",
    "       = -10 + 0 + 0 - 20 + 0 + 0 - 10 + 0 + 0\n",
    "       = -40  (strong vertical edge detected!)\n",
    "```\n",
    "\n",
    "**Output:** High values where vertical edges exist.\n",
    "\n",
    "---\n",
    "\n",
    "### **Convolution Properties**\n",
    "\n",
    "#### **1. Padding**\n",
    "\n",
    "**Problem:** Convolution shrinks output size.\n",
    "- Input: 5\u00d75, Filter: 3\u00d73 \u2192 Output: 3\u00d73 (lost 2 pixels on each side)\n",
    "\n",
    "**Solution:** Add zeros around input (padding).\n",
    "\n",
    "**Types:**\n",
    "- **Valid (no padding):** Output size = $(n - k + 1) \\times (n - k + 1)$\n",
    "- **Same (zero padding):** Output size = $n \\times n$ (same as input)\n",
    "- **Full padding:** Pad with $k-1$ zeros on each side\n",
    "\n",
    "**Formula:** \n",
    "$$\n",
    "\\text{Output size} = \\frac{n + 2p - k}{s} + 1\n",
    "$$\n",
    "Where:\n",
    "- $n$ = input size\n",
    "- $p$ = padding\n",
    "- $k$ = kernel size\n",
    "- $s$ = stride\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Stride**\n",
    "\n",
    "**Definition:** Step size when sliding filter.\n",
    "\n",
    "- **Stride = 1:** Slide 1 pixel at a time (default)\n",
    "- **Stride = 2:** Slide 2 pixels at a time (reduce output size by 2\u00d7)\n",
    "\n",
    "**Effect:**\n",
    "- Larger stride \u2192 smaller output \u2192 fewer parameters \u2192 faster computation\n",
    "- Trade-off: May lose spatial information\n",
    "\n",
    "**Example:**\n",
    "- Input: 28\u00d728, Filter: 3\u00d73, Stride: 1, Padding: 0 \u2192 Output: 26\u00d726\n",
    "- Input: 28\u00d728, Filter: 3\u00d73, Stride: 2, Padding: 0 \u2192 Output: 13\u00d713\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Dilation (Atrous Convolution)**\n",
    "\n",
    "**Definition:** Insert gaps between kernel elements.\n",
    "\n",
    "**Purpose:** Increase receptive field without increasing parameters.\n",
    "\n",
    "**Dilation rate = 2:**\n",
    "```\n",
    "Original 3\u00d73 kernel:\n",
    "[a, b, c]\n",
    "[d, e, f]\n",
    "[g, h, i]\n",
    "\n",
    "Dilated 3\u00d73 kernel (effective 5\u00d75):\n",
    "[a, 0, b, 0, c]\n",
    "[0, 0, 0, 0, 0]\n",
    "[d, 0, e, 0, f]\n",
    "[0, 0, 0, 0, 0]\n",
    "[g, 0, h, 0, i]\n",
    "```\n",
    "\n",
    "**Use case:** Semantic segmentation (capture multi-scale context).\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Receptive Field**\n",
    "\n",
    "**Definition:** Region in input that influences one output pixel.\n",
    "\n",
    "**Example:**\n",
    "- 1 conv layer (3\u00d73): Receptive field = 3\u00d73\n",
    "- 2 conv layers (3\u00d73): Receptive field = 5\u00d75\n",
    "- 3 conv layers (3\u00d73): Receptive field = 7\u00d77\n",
    "\n",
    "**Formula (for $L$ layers):**\n",
    "$$\n",
    "RF = 1 + \\sum_{i=1}^{L} (k_i - 1) \\cdot \\prod_{j=1}^{i-1} s_j\n",
    "$$\n",
    "\n",
    "**Why it matters:** Deeper networks see larger context (better for complex patterns).\n",
    "\n",
    "---\n",
    "\n",
    "### **Pooling Operations**\n",
    "\n",
    "**Purpose:** Downsample feature maps (reduce spatial dimensions, increase efficiency).\n",
    "\n",
    "#### **Max Pooling**\n",
    "\n",
    "Take maximum value in each region.\n",
    "\n",
    "**Example (2\u00d72 max pooling, stride=2):**\n",
    "```\n",
    "Input (4\u00d74):\n",
    "[[1, 3, 2, 4],\n",
    " [5, 6, 1, 2],\n",
    " [7, 2, 8, 1],\n",
    " [0, 9, 3, 5]]\n",
    "\n",
    "Output (2\u00d72):\n",
    "[[6, 4],   # max(1,3,5,6)=6, max(2,4,1,2)=4\n",
    " [9, 8]]   # max(7,2,0,9)=9, max(8,1,3,5)=8\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- \u2705 Translation invariance (small shifts don't change max)\n",
    "- \u2705 Computational efficiency (reduce spatial size by 4\u00d7)\n",
    "- \u2705 Prevents overfitting (lossy compression)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Average Pooling**\n",
    "\n",
    "Take average value in each region.\n",
    "\n",
    "**Example (2\u00d72 average pooling):**\n",
    "```\n",
    "Input (4\u00d74):\n",
    "[[1, 3, 2, 4],\n",
    " [5, 6, 1, 2],\n",
    " [7, 2, 8, 1],\n",
    " [0, 9, 3, 5]]\n",
    "\n",
    "Output (2\u00d72):\n",
    "[[3.75, 2.25],  # avg(1,3,5,6)=3.75, avg(2,4,1,2)=2.25\n",
    " [4.5,  4.25]]  # avg(7,2,0,9)=4.5,  avg(8,1,3,5)=4.25\n",
    "```\n",
    "\n",
    "**Use case:** Final feature aggregation (e.g., Global Average Pooling before classification).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Global Average Pooling (GAP)**\n",
    "\n",
    "Average entire feature map into single value per channel.\n",
    "\n",
    "**Example:**\n",
    "- Input: 7\u00d77\u00d7512 (feature map with 512 channels)\n",
    "- Output: 1\u00d71\u00d7512 (512 values)\n",
    "\n",
    "**Advantages:**\n",
    "- \u2705 Eliminates fully connected layers (fewer parameters)\n",
    "- \u2705 Spatial invariance (works for any input size)\n",
    "- \u2705 Regularization effect (prevents overfitting)\n",
    "\n",
    "**Used in:** ResNet, Inception, MobileNet\n",
    "\n",
    "---\n",
    "\n",
    "### **From-Scratch Implementation (NumPy)**\n",
    "\n",
    "Now let's implement 2D convolution to understand the mechanics:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def convolve2d(image, kernel, padding=0, stride=1):\n",
    "    \"\"\"\n",
    "    2D convolution implementation from scratch.\n",
    "    \n",
    "    Args:\n",
    "        image: 2D array (H, W)\n",
    "        kernel: 2D array (kH, kW)\n",
    "        padding: int (zero padding on all sides)\n",
    "        stride: int (step size)\n",
    "    \n",
    "    Returns:\n",
    "        output: 2D array (convolved feature map)\n",
    "    \"\"\"\n",
    "    # Get dimensions\n",
    "    H, W = image.shape\n",
    "    kH, kW = kernel.shape\n",
    "    \n",
    "    # Add padding\n",
    "    if padding > 0:\n",
    "        image = np.pad(image, padding, mode='constant', constant_values=0)\n",
    "        H, W = image.shape\n",
    "    \n",
    "    # Calculate output dimensions\n",
    "    out_H = (H - kH) // stride + 1\n",
    "    out_W = (W - kW) // stride + 1\n",
    "    \n",
    "    # Initialize output\n",
    "    output = np.zeros((out_H, out_W))\n",
    "    \n",
    "    # Perform convolution\n",
    "    for i in range(0, out_H):\n",
    "        for j in range(0, out_W):\n",
    "            # Extract region\n",
    "            y_start = i * stride\n",
    "            x_start = j * stride\n",
    "            region = image[y_start:y_start+kH, x_start:x_start+kW]\n",
    "            \n",
    "            # Element-wise multiply and sum\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def max_pool2d(image, pool_size=2, stride=2):\n",
    "    \"\"\"\n",
    "    2D max pooling from scratch.\n",
    "    \n",
    "    Args:\n",
    "        image: 2D array (H, W)\n",
    "        pool_size: int (pooling window size)\n",
    "        stride: int (step size)\n",
    "    \n",
    "    Returns:\n",
    "        output: 2D array (pooled feature map)\n",
    "    \"\"\"\n",
    "    H, W = image.shape\n",
    "    \n",
    "    # Calculate output dimensions\n",
    "    out_H = (H - pool_size) // stride + 1\n",
    "    out_W = (W - pool_size) // stride + 1\n",
    "    \n",
    "    # Initialize output\n",
    "    output = np.zeros((out_H, out_W))\n",
    "    \n",
    "    # Perform max pooling\n",
    "    for i in range(out_H):\n",
    "        for j in range(out_W):\n",
    "            y_start = i * stride\n",
    "            x_start = j * stride\n",
    "            region = image[y_start:y_start+pool_size, x_start:x_start+pool_size]\n",
    "            output[i, j] = np.max(region)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# Example: Edge detection\n",
    "print(\"=\"*80)\n",
    "print(\"CONVOLUTION FROM SCRATCH - EDGE DETECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create simple test image (5\u00d75)\n",
    "image = np.array([\n",
    "    [10, 10, 10,  0,  0],\n",
    "    [10, 10, 10,  0,  0],\n",
    "    [10, 10, 10,  0,  0],\n",
    "    [10, 10, 10,  0,  0],\n",
    "    [10, 10, 10,  0,  0]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Sobel filter (vertical edge detector)\n",
    "sobel_vertical = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-2, 0, 2],\n",
    "    [-1, 0, 1]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Apply convolution\n",
    "output = convolve2d(image, sobel_vertical, padding=0, stride=1)\n",
    "\n",
    "print(f\"Input image shape: {image.shape}\")\n",
    "print(f\"Kernel shape: {sobel_vertical.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nOutput (edge detected):\\n{output}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(image, cmap='gray')\n",
    "axes[0].set_title('Input Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(sobel_vertical, cmap='gray')\n",
    "axes[1].set_title('Sobel Filter (Vertical)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(output, cmap='gray')\n",
    "axes[2].set_title('Convolution Output (Edge)')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Example: Max pooling\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MAX POOLING FROM SCRATCH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create test feature map\n",
    "feature_map = np.array([\n",
    "    [1, 3, 2, 4],\n",
    "    [5, 6, 1, 2],\n",
    "    [7, 2, 8, 1],\n",
    "    [0, 9, 3, 5]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Apply max pooling\n",
    "pooled = max_pool2d(feature_map, pool_size=2, stride=2)\n",
    "\n",
    "print(f\"Input feature map:\\n{feature_map}\")\n",
    "print(f\"\\nMax pooled (2\u00d72, stride=2):\\n{pooled}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axes[0].imshow(feature_map, cmap='viridis')\n",
    "axes[0].set_title('Feature Map (4\u00d74)')\n",
    "axes[0].axis('off')\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axes[0].text(j, i, f'{feature_map[i,j]:.0f}', \n",
    "                     ha='center', va='center', color='white', fontsize=14)\n",
    "\n",
    "axes[1].imshow(pooled, cmap='viridis')\n",
    "axes[1].set_title('Max Pooled (2\u00d72)')\n",
    "axes[1].axis('off')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[1].text(j, i, f'{pooled[i,j]:.0f}', \n",
    "                     ha='center', va='center', color='white', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"From-scratch implementation complete!\")\n",
    "print(\"Now we'll use PyTorch/Keras for efficient GPU-accelerated convolutions.\")\n",
    "print(\"=\"*80)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights from Implementation**\n",
    "\n",
    "1. **Convolution = Pattern Matching**\n",
    "   - Filter weights define what pattern to detect\n",
    "   - High output value = pattern found at that location\n",
    "   - Different filters detect different features (edges, corners, textures)\n",
    "\n",
    "2. **Computational Cost**\n",
    "   - For each output pixel: $k_h \\times k_w$ multiplications\n",
    "   - Total operations: $(n-k+1)^2 \\times k^2$ for $n \\times n$ input, $k \\times k$ filter\n",
    "   - Example: 28\u00d728 input, 3\u00d73 filter = 26\u00d726\u00d79 = 6,084 multiplications per filter\n",
    "\n",
    "3. **Why Use Frameworks (PyTorch/Keras)?**\n",
    "   - \u2705 GPU acceleration (100-1000\u00d7 faster)\n",
    "   - \u2705 Optimized implementations (cuDNN, MKL)\n",
    "   - \u2705 Automatic gradient computation (backpropagation through convolutions)\n",
    "   - \u2705 Batch processing (multiple images at once)\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** We'll build CNNs in PyTorch and Keras with GPU acceleration! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d532e10",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7330bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Synthetic Wafer Map Generator for CNN Training\n",
    "This module generates realistic semiconductor wafer map defect patterns for training\n",
    "defect classification models. Each wafer is 300\u00d7300 pixels representing die pass/fail.\n",
    "Defect Types (20 classes):\n",
    "    0: None (normal, random failures 0.5-2%)\n",
    "    1: Center (failures concentrated in wafer center)\n",
    "    2: Edge (failures at wafer edge)\n",
    "    3: Scratch (linear horizontal defect)\n",
    "    4: Ring (circular ring pattern)\n",
    "    5: Cluster (localized cluster of failures)\n",
    "    ... (15 more classes)\n",
    "Business value: $5M-$20M per incident through faster root cause analysis.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import time\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "print(\"=\"*80)\n",
    "print(\"WAFER MAP DEFECT PATTERN GENERATOR\")\n",
    "print(\"=\"*80)\n",
    "#------------------------------------------------------------------------------\n",
    "# 1. Wafer Map Generation Functions\n",
    "#------------------------------------------------------------------------------\n",
    "def generate_wafer_mask(size=300):\n",
    "    \"\"\"\n",
    "    Generate circular wafer mask (wafer is circular, not square).\n",
    "    \n",
    "    Returns:\n",
    "        mask: 2D array (size\u00d7size), 1=valid die, 0=outside wafer\n",
    "    \"\"\"\n",
    "    center = size // 2\n",
    "    y, x = np.ogrid[:size, :size]\n",
    "    distance = np.sqrt((x - center)**2 + (y - center)**2)\n",
    "    mask = (distance <= center * 0.95).astype(np.float32)  # 95% of radius\n",
    "    return mask\n",
    "def generate_normal_wafer(size=300, fail_rate=0.01):\n",
    "    \"\"\"Normal wafer: random failures (0.5-2%)\"\"\"\n",
    "    wafer = np.random.rand(size, size) < fail_rate\n",
    "    mask = generate_wafer_mask(size)\n",
    "    return wafer.astype(np.float32) * mask\n",
    "def generate_center_defect(size=300):\n",
    "    \"\"\"Center defect: high failure density in center\"\"\"\n",
    "    center = size // 2\n",
    "    y, x = np.ogrid[:size, :size]\n",
    "    distance = np.sqrt((x - center)**2 + (y - center)**2)\n",
    "    \n",
    "    # Gaussian peak at center\n",
    "    defect = np.exp(-(distance**2) / (2 * (size/8)**2))\n",
    "    wafer = (np.random.rand(size, size) < defect * 0.8).astype(np.float32)\n",
    "    \n",
    "    mask = generate_wafer_mask(size)\n",
    "    return wafer * mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb3bb8b",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Function: generate_edge_defect\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c195916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edge_defect(size=300):\n",
    "    \"\"\"Edge defect: high failure at wafer edge\"\"\"\n",
    "    center = size // 2\n",
    "    y, x = np.ogrid[:size, :size]\n",
    "    distance = np.sqrt((x - center)**2 + (y - center)**2)\n",
    "    \n",
    "    # Ring at edge\n",
    "    edge_distance = np.abs(distance - center * 0.85)\n",
    "    defect = np.exp(-(edge_distance**2) / (2 * (size/20)**2))\n",
    "    wafer = (np.random.rand(size, size) < defect * 0.7).astype(np.float32)\n",
    "    \n",
    "    mask = generate_wafer_mask(size)\n",
    "    return wafer * mask\n",
    "def generate_scratch_defect(size=300):\n",
    "    \"\"\"Scratch: linear defect (horizontal, vertical, or diagonal)\"\"\"\n",
    "    wafer = np.zeros((size, size), dtype=np.float32)\n",
    "    \n",
    "    # Random orientation\n",
    "    orientation = np.random.choice(['horizontal', 'vertical', 'diagonal'])\n",
    "    position = np.random.randint(size//4, 3*size//4)\n",
    "    width = np.random.randint(3, 8)\n",
    "    \n",
    "    if orientation == 'horizontal':\n",
    "        wafer[position-width:position+width, :] = 1\n",
    "    elif orientation == 'vertical':\n",
    "        wafer[:, position-width:position+width] = 1\n",
    "    else:  # diagonal\n",
    "        for i in range(size):\n",
    "            j = i + position - size//2\n",
    "            if 0 <= j < size:\n",
    "                wafer[max(0, i-width):min(size, i+width), \n",
    "                      max(0, j-width):min(size, j+width)] = 1\n",
    "    \n",
    "    mask = generate_wafer_mask(size)\n",
    "    return wafer * mask\n",
    "def generate_ring_defect(size=300):\n",
    "    \"\"\"Ring defect: circular ring of failures\"\"\"\n",
    "    center = size // 2\n",
    "    y, x = np.ogrid[:size, :size]\n",
    "    distance = np.sqrt((x - center)**2 + (y - center)**2)\n",
    "    \n",
    "    # Ring at random radius\n",
    "    ring_radius = np.random.uniform(0.4, 0.7) * center\n",
    "    ring_width = size / 15\n",
    "    ring_distance = np.abs(distance - ring_radius)\n",
    "    defect = np.exp(-(ring_distance**2) / (2 * ring_width**2))\n",
    "    \n",
    "    wafer = (np.random.rand(size, size) < defect * 0.8).astype(np.float32)\n",
    "    mask = generate_wafer_mask(size)\n",
    "    return wafer * mask\n",
    "def generate_cluster_defect(size=300):\n",
    "    \"\"\"Cluster: localized cluster of failures\"\"\"\n",
    "    wafer = np.zeros((size, size), dtype=np.float32)\n",
    "    \n",
    "    # Random cluster location\n",
    "    n_clusters = np.random.randint(1, 4)\n",
    "    for _ in range(n_clusters):\n",
    "        cx = np.random.randint(size//4, 3*size//4)\n",
    "        cy = np.random.randint(size//4, 3*size//4)\n",
    "        cluster_size = np.random.uniform(size/15, size/8)\n",
    "        \n",
    "        y, x = np.ogrid[:size, :size]\n",
    "        distance = np.sqrt((x - cx)**2 + (y - cy)**2)\n",
    "        cluster = np.exp(-(distance**2) / (2 * cluster_size**2))\n",
    "        wafer += (np.random.rand(size, size) < cluster * 0.9).astype(np.float32)\n",
    "    \n",
    "    wafer = np.clip(wafer, 0, 1)\n",
    "    mask = generate_wafer_mask(size)\n",
    "    return wafer * mask\n",
    "# Dictionary mapping class index to generation function\n",
    "DEFECT_GENERATORS = {\n",
    "    0: lambda size: generate_normal_wafer(size, fail_rate=0.01),\n",
    "    1: generate_center_defect,\n",
    "    2: generate_edge_defect,\n",
    "    3: generate_scratch_defect,\n",
    "    4: generate_ring_defect,\n",
    "    5: generate_cluster_defect,\n",
    "    # For simplicity, we'll use variations of above for other classes\n",
    "    6: lambda size: generate_normal_wafer(size, fail_rate=0.02),\n",
    "    7: lambda size: generate_edge_defect(size) + generate_center_defect(size) * 0.3,\n",
    "    8: lambda size: generate_ring_defect(size),\n",
    "    9: lambda size: generate_cluster_defect(size),\n",
    "}\n",
    "DEFECT_NAMES = {\n",
    "    0: \"Normal\",\n",
    "    1: \"Center\",\n",
    "    2: \"Edge\",\n",
    "    3: \"Scratch\",\n",
    "    4: \"Ring\",\n",
    "    5: \"Cluster\",\n",
    "    6: \"Normal-High\",\n",
    "    7: \"Edge+Center\",\n",
    "    8: \"Ring-2\",\n",
    "    9: \"Multi-Cluster\",\n",
    "}\n",
    "#------------------------------------------------------------------------------\n",
    "# 2. Generate Dataset\n",
    "#------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968247b",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Function: generate_wafer_dataset\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280df3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wafer_dataset(n_samples=1000, n_classes=10, size=300):\n",
    "    \"\"\"\n",
    "    Generate synthetic wafer map dataset.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: int (total samples)\n",
    "        n_classes: int (number of defect classes)\n",
    "        size: int (wafer map size, e.g., 300\u00d7300)\n",
    "    \n",
    "    Returns:\n",
    "        X: np.array (n_samples, size, size, 1)\n",
    "        y: np.array (n_samples,)\n",
    "    \"\"\"\n",
    "    samples_per_class = n_samples // n_classes\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for class_idx in range(n_classes):\n",
    "        generator = DEFECT_GENERATORS.get(class_idx, DEFECT_GENERATORS[0])\n",
    "        \n",
    "        for _ in range(samples_per_class):\n",
    "            wafer = generator(size)\n",
    "            X.append(wafer)\n",
    "            y.append(class_idx)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Add channel dimension (grayscale)\n",
    "    X = X[:, :, :, np.newaxis]\n",
    "    \n",
    "    return X, y\n",
    "print(\"\\nGenerating wafer map dataset...\")\n",
    "print(\"  - 7,000 training samples\")\n",
    "print(\"  - 1,500 validation samples\")\n",
    "print(\"  - 1,500 test samples\")\n",
    "print(\"  - 10 defect classes\")\n",
    "print(\"  - 300\u00d7300 pixel wafer maps\\n\")\n",
    "start_time = time.time()\n",
    "# Generate dataset\n",
    "X_all, y_all = generate_wafer_dataset(n_samples=10000, n_classes=10, size=128)  # Using 128 for faster training\n",
    "# Split into train/val/test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_all, y_all, test_size=0.3, random_state=42, stratify=y_all\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "print(f\"Dataset generated in {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"  Training:   {X_train.shape}, labels: {y_train.shape}\")\n",
    "print(f\"  Validation: {X_val.shape}, labels: {y_val.shape}\")\n",
    "print(f\"  Test:       {X_test.shape}, labels: {y_test.shape}\")\n",
    "# Visualize samples from each class\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.ravel()\n",
    "for class_idx in range(10):\n",
    "    idx = np.where(y_train == class_idx)[0][0]\n",
    "    axes[class_idx].imshow(X_train[idx, :, :, 0], cmap='RdYlGn_r', vmin=0, vmax=1)\n",
    "    axes[class_idx].set_title(f'Class {class_idx}: {DEFECT_NAMES[class_idx]}')\n",
    "    axes[class_idx].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Wafer Map Defect Patterns (Sample from Each Class)', y=1.02, fontsize=14)\n",
    "plt.savefig('wafer_map_samples.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\nSaved: wafer_map_samples.png\")\n",
    "plt.show()\n",
    "#------------------------------------------------------------------------------\n",
    "# 3. Build CNN in PyTorch\n",
    "#------------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUILDING CNN IN PYTORCH\")\n",
    "print(\"=\"*80)\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nDevice: {device}\")\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).permute(0, 3, 1, 2).to(device)  # (N, C, H, W)\n",
    "y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val).permute(0, 3, 1, 2).to(device)\n",
    "y_val_tensor = torch.LongTensor(y_val).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).permute(0, 3, 1, 2).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test).to(device)\n",
    "print(f\"Tensor shapes (PyTorch format):\")\n",
    "print(f\"  X_train: {X_train_tensor.shape} (N, C, H, W)\")\n",
    "print(f\"  y_train: {y_train_tensor.shape}\")\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3716e6",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Class: WaferCNN\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3582c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaferCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom CNN for wafer map defect classification.\n",
    "    \n",
    "    Architecture:\n",
    "        Input(128\u00d7128\u00d71) \u2192 Conv(32, 5\u00d75) + ReLU + MaxPool(2\u00d72)\n",
    "                         \u2192 Conv(64, 3\u00d73) + ReLU + MaxPool(2\u00d72)\n",
    "                         \u2192 Conv(128, 3\u00d73) + ReLU + MaxPool(2\u00d72)\n",
    "                         \u2192 Flatten\n",
    "                         \u2192 Dense(256) + Dropout(0.5)\n",
    "                         \u2192 Output(10)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(WaferCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Calculate flattened size: 128\u00d7128 \u2192 64\u00d764 \u2192 32\u00d732 \u2192 16\u00d716\n",
    "        self.flatten_size = 128 * 16 * 16\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv block 1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Conv block 2\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Conv block 3\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "# Create model\n",
    "model = WaferCNN(num_classes=10).to(device)\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "print(f\"\\nLoss: CrossEntropyLoss\")\n",
    "print(f\"Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"Scheduler: ReduceLROnPlateau\")\n",
    "#------------------------------------------------------------------------------\n",
    "# 4. Training Loop\n",
    "#------------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING CNN\")\n",
    "print(\"=\"*80)\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "best_val_acc = 0.0\n",
    "print(f\"\\nTraining for {num_epochs} epochs...\")\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        train_loss += loss.item() * batch_X.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += batch_y.size(0)\n",
    "        train_correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = train_correct / train_total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            val_loss += loss.item() * batch_X.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += batch_y.size(0)\n",
    "            val_correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = val_correct / val_total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1:2d}/{num_epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_wafer_cnn.pth')\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_wafer_cnn.pth'))\n",
    "# Visualize training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(val_losses, label='Val Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(val_accs, label='Val Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('cnn_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "print(\"Saved: cnn_training_curves.png\")\n",
    "plt.show()\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CNN trained successfully!\")\n",
    "print(\"Next: Evaluate on test set and study classic architectures\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1c5bc0",
   "metadata": {},
   "source": [
    "## \ud83c\udfdb\ufe0f Part 2: Classic CNN Architectures\n",
    "\n",
    "### **Evolution of CNN Architectures (1998-2020)**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. LeNet-5 (1998) - The Pioneer**\n",
    "\n",
    "**Author:** Yann LeCun  \n",
    "**Dataset:** MNIST (handwritten digits)  \n",
    "**Accuracy:** ~99% on MNIST\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input(32\u00d732\u00d71) \u2192 Conv(6, 5\u00d75) + tanh + AvgPool(2\u00d72)\n",
    "               \u2192 Conv(16, 5\u00d75) + tanh + AvgPool(2\u00d72)\n",
    "               \u2192 Flatten\n",
    "               \u2192 Dense(120) + tanh\n",
    "               \u2192 Dense(84) + tanh\n",
    "               \u2192 Output(10) + softmax\n",
    "```\n",
    "\n",
    "**Parameters:** ~60K\n",
    "\n",
    "**Key innovations:**\n",
    "- \u2705 Convolutional layers for local feature extraction\n",
    "- \u2705 Pooling for translation invariance\n",
    "- \u2705 End-to-end trainable (no hand-crafted features)\n",
    "\n",
    "**Limitations:**\n",
    "- \u274c Shallow (only 2 conv layers)\n",
    "- \u274c tanh activation (vanishing gradients)\n",
    "- \u274c Small dataset (60K MNIST samples)\n",
    "\n",
    "---\n",
    "\n",
    "### **2. AlexNet (2012) - The Revolution**\n",
    "\n",
    "**Authors:** Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton  \n",
    "**Dataset:** ImageNet (1.2M images, 1000 classes)  \n",
    "**Accuracy:** 84.7% top-5 (vs 73.8% second place)\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input(224\u00d7224\u00d73) \u2192 Conv(96, 11\u00d711, stride=4) + ReLU + MaxPool(3\u00d73, stride=2)\n",
    "                 \u2192 Conv(256, 5\u00d75) + ReLU + MaxPool(3\u00d73, stride=2)\n",
    "                 \u2192 Conv(384, 3\u00d73) + ReLU\n",
    "                 \u2192 Conv(384, 3\u00d73) + ReLU\n",
    "                 \u2192 Conv(256, 3\u00d73) + ReLU + MaxPool(3\u00d73, stride=2)\n",
    "                 \u2192 Flatten\n",
    "                 \u2192 Dense(4096) + ReLU + Dropout(0.5)\n",
    "                 \u2192 Dense(4096) + ReLU + Dropout(0.5)\n",
    "                 \u2192 Output(1000) + softmax\n",
    "```\n",
    "\n",
    "**Parameters:** ~60M\n",
    "\n",
    "**Key innovations:**\n",
    "- \u2705 **ReLU activation:** Faster training (6\u00d7 faster than tanh)\n",
    "- \u2705 **Dropout:** Prevents overfitting (50% dropout in FC layers)\n",
    "- \u2705 **GPU training:** Trained on 2 GTX 580 GPUs (split model across GPUs)\n",
    "- \u2705 **Data augmentation:** Random crops, flips, color jittering\n",
    "- \u2705 **Local Response Normalization (LRN):** Lateral inhibition\n",
    "\n",
    "**Impact:** Sparked the deep learning revolution. CNNs became dominant in computer vision.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. VGG (2014) - Simplicity Wins**\n",
    "\n",
    "**Authors:** Karen Simonyan, Andrew Zisserman (Oxford)  \n",
    "**Dataset:** ImageNet  \n",
    "**Accuracy:** 92.7% top-5 (VGG-16)\n",
    "\n",
    "**Architecture (VGG-16):**\n",
    "```\n",
    "Input(224\u00d7224\u00d73)\n",
    "\u2192 Conv(64, 3\u00d73) + ReLU \u2192 Conv(64, 3\u00d73) + ReLU \u2192 MaxPool(2\u00d72)\n",
    "\u2192 Conv(128, 3\u00d73) + ReLU \u2192 Conv(128, 3\u00d73) + ReLU \u2192 MaxPool(2\u00d72)\n",
    "\u2192 Conv(256, 3\u00d73) + ReLU \u2192 Conv(256, 3\u00d73) + ReLU \u2192 Conv(256, 3\u00d73) + ReLU \u2192 MaxPool(2\u00d72)\n",
    "\u2192 Conv(512, 3\u00d73) + ReLU \u2192 Conv(512, 3\u00d73) + ReLU \u2192 Conv(512, 3\u00d73) + ReLU \u2192 MaxPool(2\u00d72)\n",
    "\u2192 Conv(512, 3\u00d73) + ReLU \u2192 Conv(512, 3\u00d73) + ReLU \u2192 Conv(512, 3\u00d73) + ReLU \u2192 MaxPool(2\u00d72)\n",
    "\u2192 Flatten\n",
    "\u2192 Dense(4096) + ReLU + Dropout(0.5)\n",
    "\u2192 Dense(4096) + ReLU + Dropout(0.5)\n",
    "\u2192 Output(1000) + softmax\n",
    "```\n",
    "\n",
    "**Parameters:** 138M (VGG-16), 144M (VGG-19)\n",
    "\n",
    "**Key insights:**\n",
    "- \u2705 **Uniform architecture:** Only 3\u00d73 convolutions throughout (simple, easy to understand)\n",
    "- \u2705 **Depth matters:** Stacking 3\u00d73 filters = larger receptive field (two 3\u00d73 = one 5\u00d75, but fewer parameters)\n",
    "- \u2705 **Transfer learning:** VGG features transfer well to other tasks\n",
    "\n",
    "**Limitations:**\n",
    "- \u274c **Too many parameters:** 138M parameters (slow training, large memory)\n",
    "- \u274c **Computational cost:** ~16B FLOPs for single forward pass\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Inception v1 / GoogLeNet (2014) - Multi-Scale Features**\n",
    "\n",
    "**Authors:** Christian Szegedy et al. (Google)  \n",
    "**Dataset:** ImageNet  \n",
    "**Accuracy:** 93.3% top-5\n",
    "\n",
    "**Inception Module (core building block):**\n",
    "```\n",
    "Input feature map\n",
    "    \u2193\n",
    "    \u251c\u2192 Conv(1\u00d71) \u2192 ReLU \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u251c\u2192 Conv(1\u00d71) \u2192 ReLU \u2192 Conv(3\u00d73) \u2192 ReLU \u2500\u2524\n",
    "    \u251c\u2192 Conv(1\u00d71) \u2192 ReLU \u2192 Conv(5\u00d75) \u2192 ReLU \u2500\u2524\n",
    "    \u2514\u2192 MaxPool(3\u00d73) \u2192 Conv(1\u00d71) \u2192 ReLU \u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                                              \u2193\n",
    "                                       Concatenate\n",
    "                                              \u2193\n",
    "                                        Output feature map\n",
    "```\n",
    "\n",
    "**Key innovations:**\n",
    "- \u2705 **Multi-scale processing:** 1\u00d71, 3\u00d73, 5\u00d75 convolutions in parallel (capture features at different scales)\n",
    "- \u2705 **1\u00d71 convolutions:** Dimensionality reduction (reduce parameters and computation)\n",
    "  - Example: 256 channels \u2192 1\u00d71 conv(64) \u2192 3\u00d73 conv \u2192 fewer parameters than direct 3\u00d73 on 256 channels\n",
    "- \u2705 **Global Average Pooling:** Replace FC layers (fewer parameters, reduce overfitting)\n",
    "- \u2705 **Auxiliary classifiers:** Intermediate loss functions help gradient flow in deep networks\n",
    "\n",
    "**Parameters:** Only 6M (23\u00d7 fewer than VGG-16!)\n",
    "\n",
    "**Why it matters:**\n",
    "- Efficiency: Fewer parameters, faster inference\n",
    "- Multi-scale: Better feature extraction (objects at different scales)\n",
    "- Influenced later architectures (ResNeXt, EfficientNet)\n",
    "\n",
    "---\n",
    "\n",
    "### **5. ResNet (2015) - The Game Changer**\n",
    "\n",
    "**Authors:** Kaiming He et al. (Microsoft Research)  \n",
    "**Dataset:** ImageNet  \n",
    "**Accuracy:** 96.4% top-5 (ResNet-152)\n",
    "\n",
    "**Problem:** Deep networks are hard to train (vanishing gradients, degradation problem).\n",
    "\n",
    "**Solution:** **Residual connections (skip connections)** - allow gradients to flow directly through network.\n",
    "\n",
    "**Residual Block:**\n",
    "```\n",
    "Input x\n",
    "    \u2193\n",
    "    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 (identity shortcut)\n",
    "    \u2193                             \u2193\n",
    "Conv(3\u00d73) \u2192 ReLU \u2192 Conv(3\u00d73)     \u2193\n",
    "    \u2193                             \u2193\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Add \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                  \u2193\n",
    "                ReLU\n",
    "                  \u2193\n",
    "               Output\n",
    "```\n",
    "\n",
    "**Mathematical formulation:**\n",
    "$$\n",
    "\\text{Output} = F(x) + x\n",
    "$$\n",
    "\n",
    "Where $F(x)$ is the residual mapping (what the layers learn).\n",
    "\n",
    "**Why this works:**\n",
    "- If identity mapping is optimal, layers can learn $F(x) = 0$ (easy!)\n",
    "- Gradient flows directly through skip connection: $\\frac{\\partial y}{\\partial x} = \\frac{\\partial F}{\\partial x} + 1$\n",
    "- The \"+1\" ensures gradient always flows (solves vanishing gradient)\n",
    "\n",
    "**Architecture (ResNet-50):**\n",
    "```\n",
    "Input(224\u00d7224\u00d73)\n",
    "\u2192 Conv(64, 7\u00d77, stride=2) + BN + ReLU + MaxPool(3\u00d73, stride=2)\n",
    "\u2192 [Residual Block \u00d7 3]  (64 channels)\n",
    "\u2192 [Residual Block \u00d7 4]  (128 channels)\n",
    "\u2192 [Residual Block \u00d7 6]  (256 channels)\n",
    "\u2192 [Residual Block \u00d7 3]  (512 channels)\n",
    "\u2192 GlobalAvgPool \u2192 Dense(1000) + softmax\n",
    "```\n",
    "\n",
    "**Variants:**\n",
    "- ResNet-18, ResNet-34: Basic residual blocks (2 conv layers per block)\n",
    "- ResNet-50, ResNet-101, ResNet-152: Bottleneck blocks (1\u00d71 \u2192 3\u00d73 \u2192 1\u00d71, more efficient)\n",
    "\n",
    "**Parameters:** 25M (ResNet-50), 60M (ResNet-152)\n",
    "\n",
    "**Impact:**\n",
    "- \u2705 **Train very deep networks:** 152 layers (vs 19 in VGG)\n",
    "- \u2705 **Better accuracy:** 96.4% top-5 (superhuman on ImageNet)\n",
    "- \u2705 **Transfer learning:** ResNet features work amazingly well on other tasks\n",
    "- \u2705 **Foundation for modern architectures:** Most SOTA models use residual connections\n",
    "\n",
    "---\n",
    "\n",
    "### **Architecture Comparison Table**\n",
    "\n",
    "| Architecture | Year | Layers | Parameters | ImageNet Top-5 | Key Innovation |\n",
    "|--------------|------|--------|------------|----------------|----------------|\n",
    "| **LeNet-5** | 1998 | 7 | 60K | N/A (MNIST) | First successful CNN |\n",
    "| **AlexNet** | 2012 | 8 | 60M | 84.7% | ReLU, dropout, GPU training |\n",
    "| **VGG-16** | 2014 | 16 | 138M | 92.7% | Uniform 3\u00d73 filters |\n",
    "| **Inception v1** | 2014 | 22 | 6M | 93.3% | Multi-scale features, 1\u00d71 conv |\n",
    "| **ResNet-50** | 2015 | 50 | 25M | 96.4% | Residual connections (skip) |\n",
    "| **ResNet-152** | 2015 | 152 | 60M | 96.4% | Extremely deep networks |\n",
    "\n",
    "---\n",
    "\n",
    "### **Modern Architectures (2017-2020)**\n",
    "\n",
    "#### **DenseNet (2017)** - Connect Everything\n",
    "- Every layer connects to every other layer (dense connections)\n",
    "- Feature reuse, fewer parameters, better gradient flow\n",
    "- Parameters: 8M (DenseNet-121)\n",
    "\n",
    "#### **MobileNet (2017)** - Mobile Efficiency\n",
    "- Depthwise separable convolutions (reduce parameters by 8-9\u00d7)\n",
    "- Designed for mobile devices (latency <100ms)\n",
    "- Parameters: 4M\n",
    "\n",
    "#### **EfficientNet (2019)** - Compound Scaling\n",
    "- Neural Architecture Search (NAS) to find optimal architecture\n",
    "- Compound scaling: balance depth, width, and resolution\n",
    "- SOTA accuracy with fewer parameters\n",
    "- EfficientNet-B7: 84.4% top-1 accuracy (vs 80.9% ResNet-152)\n",
    "\n",
    "#### **Vision Transformer (2020)** - Attention Without Convolutions\n",
    "- Replaces convolutions with self-attention (inspired by NLP transformers)\n",
    "- Treats image as sequence of patches (16\u00d716 pixels)\n",
    "- SOTA on ImageNet: 88.5% top-1 (ViT-Huge)\n",
    "- Requires massive datasets (100M+ images)\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Which Architecture?**\n",
    "\n",
    "| Use Case | Recommended Architecture | Reason |\n",
    "|----------|-------------------------|---------|\n",
    "| **Research/Experimentation** | ResNet-50, EfficientNet | Good balance accuracy/speed |\n",
    "| **Production (accuracy critical)** | ResNet-152, EfficientNet-B7 | Best accuracy |\n",
    "| **Production (speed critical)** | MobileNet, EfficientNet-B0 | Fast inference |\n",
    "| **Transfer learning** | ResNet-50, VGG-16 | Well-studied, many pre-trained models |\n",
    "| **Mobile/Edge devices** | MobileNet, EfficientNet-B0-B2 | Designed for low latency |\n",
    "| **Semiconductor (wafer maps)** | ResNet-34/50, EfficientNet-B1 | Good for small datasets with transfer learning |\n",
    "| **Medical imaging** | DenseNet, ResNet | Feature reuse, high accuracy |\n",
    "| **Real-time video** | MobileNet, YOLO | Low latency (<50ms) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Implementing Classic Architectures**\n",
    "\n",
    "**PyTorch:** Available in `torchvision.models`\n",
    "```python\n",
    "import torchvision.models as models\n",
    "\n",
    "# Pre-trained on ImageNet\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "inception_v3 = models.inception_v3(pretrained=True)\n",
    "efficientnet_b0 = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "# Custom number of classes (transfer learning)\n",
    "resnet50.fc = nn.Linear(resnet50.fc.in_features, 20)  # 20 defect classes\n",
    "```\n",
    "\n",
    "**TensorFlow/Keras:** Available in `tf.keras.applications`\n",
    "```python\n",
    "from tensorflow.keras.applications import ResNet50, VGG16, InceptionV3, EfficientNetB0\n",
    "\n",
    "# Pre-trained on ImageNet\n",
    "resnet50 = ResNet50(weights='imagenet', include_top=False)\n",
    "vgg16 = VGG16(weights='imagenet', include_top=False)\n",
    "efficientnet = EfficientNetB0(weights='imagenet', include_top=False)\n",
    "\n",
    "# Add custom classifier\n",
    "x = resnet50.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "output = Dense(20, activation='softmax')(x)  # 20 defect classes\n",
    "model = Model(inputs=resnet50.input, outputs=output)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Transfer learning - fine-tune ResNet-50 on wafer maps! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70e4670",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e730585",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transfer Learning: Fine-Tune ResNet-50 on Wafer Maps\n",
    "Transfer learning leverages pre-trained models (ImageNet) for new tasks with limited data.\n",
    "Benefits:\n",
    "    - Require 10\u00d7 less training data (1K vs 10K+ from scratch)\n",
    "    - Converge faster (5 epochs vs 50+)\n",
    "    - Better accuracy (98%+ vs 95% from scratch)\n",
    "    - Lower computational cost\n",
    "Approach:\n",
    "    1. Load pre-trained ResNet-50 (trained on ImageNet 1.2M images)\n",
    "    2. Replace final classification layer (1000 classes \u2192 20 defect classes)\n",
    "    3. Freeze early layers (keep learned low-level features like edges)\n",
    "    4. Fine-tune last few layers on wafer map data\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "print(\"=\"*80)\n",
    "print(\"TRANSFER LEARNING: ResNet-50 on Wafer Maps\")\n",
    "print(\"=\"*80)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nDevice: {device}\")\n",
    "#------------------------------------------------------------------------------\n",
    "# 1. Load Pre-Trained ResNet-50\n",
    "#------------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. LOADING PRE-TRAINED ResNet-50\")\n",
    "print(\"=\"*80)\n",
    "# Load ResNet-50 pre-trained on ImageNet\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "print(f\"\\nOriginal ResNet-50:\")\n",
    "print(f\"  Input: 224\u00d7224\u00d73 (RGB images)\")\n",
    "print(f\"  Output: 1000 classes (ImageNet)\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in resnet50.parameters()):,}\")\n",
    "# Modify for grayscale input (1 channel instead of 3)\n",
    "# Option 1: Replicate grayscale to 3 channels (easier, we'll use this)\n",
    "# Option 2: Modify first conv layer to accept 1 channel\n",
    "# Modify final layer for 10 defect classes\n",
    "num_features = resnet50.fc.in_features  # 2048 for ResNet-50\n",
    "resnet50.fc = nn.Linear(num_features, 10)  # 10 defect classes\n",
    "resnet50 = resnet50.to(device)\n",
    "print(f\"\\nModified ResNet-50:\")\n",
    "print(f\"  Input: 224\u00d7224\u00d73 (will replicate grayscale)\")\n",
    "print(f\"  Output: 10 classes (defect types)\")\n",
    "print(f\"  New FC layer: {num_features} \u2192 10\")\n",
    "#------------------------------------------------------------------------------\n",
    "# 2. Freeze Early Layers (Feature Extraction)\n",
    "#------------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. FREEZING EARLY LAYERS\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab261c4",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67dbbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except final FC layer\n",
    "for name, param in resnet50.named_parameters():\n",
    "    if \"fc\" not in name:  # Freeze all except FC layer\n",
    "        param.requires_grad = False\n",
    "trainable_params = sum(p.numel() for p in resnet50.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in resnet50.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,} ({(1-trainable_params/total_params)*100:.1f}%)\")\n",
    "print(\"\\nStrategy: Feature extraction\")\n",
    "print(\"  - Early layers: FROZEN (keep ImageNet features)\")\n",
    "print(\"  - Final FC layer: TRAINABLE (learn defect-specific features)\")\n",
    "#------------------------------------------------------------------------------\n",
    "# 3. Prepare Data for ResNet (224\u00d7224\u00d73)\n",
    "#------------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. DATA PREPROCESSING FOR ResNet\")\n",
    "print(\"=\"*80)\n",
    "# Resize wafer maps from 128\u00d7128 to 224\u00d7224 (ResNet input size)\n",
    "# Replicate grayscale to 3 channels (RGB)\n",
    "import torch.nn.functional as F_resize\n",
    "# Resize and replicate channels\n",
    "X_train_resnet = F_resize.interpolate(X_train_tensor, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "X_train_resnet = X_train_resnet.repeat(1, 3, 1, 1)  # 1 channel \u2192 3 channels\n",
    "X_val_resnet = F_resize.interpolate(X_val_tensor, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "X_val_resnet = X_val_resnet.repeat(1, 3, 1, 1)\n",
    "X_test_resnet = F_resize.interpolate(X_test_tensor, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "X_test_resnet = X_test_resnet.repeat(1, 3, 1, 1)\n",
    "print(f\"Original: {X_train_tensor.shape} (N, C=1, H=128, W=128)\")\n",
    "print(f\"ResNet input: {X_train_resnet.shape} (N, C=3, H=224, W=224)\")\n",
    "# Create DataLoaders\n",
    "train_dataset_resnet = TensorDataset(X_train_resnet, y_train_tensor)\n",
    "train_loader_resnet = DataLoader(train_dataset_resnet, batch_size=16, shuffle=True)\n",
    "val_dataset_resnet = TensorDataset(X_val_resnet, y_val_tensor)\n",
    "val_loader_resnet = DataLoader(val_dataset_resnet, batch_size=16, shuffle=False)\n",
    "#------------------------------------------------------------------------------\n",
    "# 4. Training (Fine-Tuning)\n",
    "#------------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. FINE-TUNING ResNet-50\")\n",
    "print(\"=\"*80)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, resnet50.parameters()), lr=0.001)\n",
    "num_epochs = 10  # Fewer epochs needed with transfer learning\n",
    "train_losses_tl = []\n",
    "val_losses_tl = []\n",
    "train_accs_tl = []\n",
    "val_accs_tl = []\n",
    "print(f\"\\nTraining for {num_epochs} epochs (transfer learning)...\")\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a72fab",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2821127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    resnet50.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader_resnet:\n",
    "        outputs = resnet50(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * batch_X.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += batch_y.size(0)\n",
    "        train_correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_loader_resnet.dataset)\n",
    "    train_acc = train_correct / train_total\n",
    "    train_losses_tl.append(train_loss)\n",
    "    train_accs_tl.append(train_acc)\n",
    "    \n",
    "    # Validation\n",
    "    resnet50.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader_resnet:\n",
    "            outputs = resnet50(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            val_loss += loss.item() * batch_X.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += batch_y.size(0)\n",
    "            val_correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_loader_resnet.dataset)\n",
    "    val_acc = val_correct / val_total\n",
    "    val_losses_tl.append(val_loss)\n",
    "    val_accs_tl.append(val_acc)\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1:2d}/{num_epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "training_time_tl = time.time() - start_time\n",
    "print(f\"\\nTransfer learning training completed in {training_time_tl:.2f} seconds\")\n",
    "print(f\"Final validation accuracy: {val_accs_tl[-1]:.4f}\")\n",
    "#------------------------------------------------------------------------------\n",
    "# 5. Comparison: Custom CNN vs Transfer Learning\n",
    "#------------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. COMPARISON: Custom CNN vs Transfer Learning\")\n",
    "print(\"=\"*80)\n",
    "# Evaluate both models on test set\n",
    "# Custom CNN (from previous cell)\n",
    "model.eval()\n",
    "test_correct_custom = 0\n",
    "test_total_custom = 0\n",
    "with torch.no_grad():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94748f31",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19ec472",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total_custom += batch_y.size(0)\n",
    "        test_correct_custom += (predicted == batch_y).sum().item()\n",
    "test_acc_custom = test_correct_custom / test_total_custom\n",
    "# ResNet-50 transfer learning\n",
    "test_dataset_resnet = TensorDataset(X_test_resnet, y_test_tensor)\n",
    "test_loader_resnet = DataLoader(test_dataset_resnet, batch_size=16, shuffle=False)\n",
    "resnet50.eval()\n",
    "test_correct_tl = 0\n",
    "test_total_tl = 0\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader_resnet:\n",
    "        outputs = resnet50(batch_X)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total_tl += batch_y.size(0)\n",
    "        test_correct_tl += (predicted == batch_y).sum().item()\n",
    "test_acc_tl = test_correct_tl / test_total_tl\n",
    "print(f\"\\nTest Set Accuracy:\")\n",
    "print(f\"  Custom CNN (from scratch):      {test_acc_custom:.4f}\")\n",
    "print(f\"  ResNet-50 (transfer learning):  {test_acc_tl:.4f}\")\n",
    "print(f\"  Improvement: {(test_acc_tl - test_acc_custom)*100:.2f}%\")\n",
    "print(f\"\\nTraining Time:\")\n",
    "print(f\"  Custom CNN:   {training_time:.2f} seconds (20 epochs)\")\n",
    "print(f\"  ResNet-50:    {training_time_tl:.2f} seconds (10 epochs)\")\n",
    "print(f\"\\nParameters:\")\n",
    "print(f\"  Custom CNN:   {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  ResNet-50:    {sum(p.numel() for p in resnet50.parameters()):,} (only {trainable_params:,} trained)\")\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# Accuracy comparison\n",
    "axes[0].plot(range(1, len(train_accs)+1), train_accs, label='Custom CNN Train', linestyle='--')\n",
    "axes[0].plot(range(1, len(val_accs)+1), val_accs, label='Custom CNN Val', linestyle='--')\n",
    "axes[0].plot(range(1, len(train_accs_tl)+1), train_accs_tl, label='ResNet-50 Train', linewidth=2)\n",
    "axes[0].plot(range(1, len(val_accs_tl)+1), val_accs_tl, label='ResNet-50 Val', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Training Comparison: Custom CNN vs Transfer Learning')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "# Bar chart: Test accuracy comparison\n",
    "models_names = ['Custom CNN\\n(from scratch)', 'ResNet-50\\n(transfer learning)']\n",
    "test_accs = [test_acc_custom, test_acc_tl]\n",
    "colors = ['#3498db', '#2ecc71']\n",
    "axes[1].bar(models_names, test_accs, color=colors, alpha=0.8)\n",
    "axes[1].set_ylabel('Test Accuracy')\n",
    "axes[1].set_title('Test Set Performance')\n",
    "axes[1].set_ylim([0.8, 1.0])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "# Add accuracy values on bars\n",
    "for i, (name, acc) in enumerate(zip(models_names, test_accs)):\n",
    "    axes[1].text(i, acc + 0.01, f'{acc:.4f}', ha='center', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('transfer_learning_comparison.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\nSaved: transfer_learning_comparison.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a9b234",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 5\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1777b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Transfer Learning Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"  \u2705 Transfer learning achieves higher accuracy with less training\")\n",
    "print(\"  \u2705 Leverages ImageNet features (edges, textures) for wafer maps\")\n",
    "print(\"  \u2705 Requires only 10 epochs vs 20+ from scratch\")\n",
    "print(\"  \u2705 Production recommendation: Use transfer learning for semiconductor applications\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2068e6b",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Real-World Projects & Key Takeaways\n",
    "\n",
    "### **Semiconductor Post-Silicon Validation Projects**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 1: Production Wafer Map Defect Classifier**\n",
    "\n",
    "**Objective:** Deploy real-time defect pattern classification system for 300mm wafer fabs.\n",
    "\n",
    "**Business Value:** $5M-$20M per incident through automated root cause analysis (reduce time from days to hours).\n",
    "\n",
    "**Dataset:**\n",
    "- 50K+ real wafer maps from production STDF files\n",
    "- 300\u00d7300 pixel resolution (10K-50K die per wafer)\n",
    "- 25 defect classes (ring, scratch, cluster, edge, radial, random, + subclasses)\n",
    "- Highly imbalanced (80% normal, 20% defects, rare classes <1%)\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "EfficientNet-B3 (transfer learning from ImageNet)\n",
    "  \u2192 Replace final layer (1000 \u2192 25 classes)\n",
    "  \u2192 Fine-tune with class-weighted loss (handle imbalance)\n",
    "  \u2192 Data augmentation (rotation, flip, zoom - wafers can be oriented any way)\n",
    "```\n",
    "\n",
    "**Implementation Steps:**\n",
    "1. **Data pipeline:** Extract wafer maps from STDF \u2192 300\u00d7300 images \u2192 augmentation\n",
    "2. **Transfer learning:** Start with EfficientNet-B3 (pre-trained ImageNet)\n",
    "3. **Handle imbalance:** Focal loss + class weights + SMOTE oversampling\n",
    "4. **Ensemble:** Train 5 models with different seeds, average predictions (boost accuracy 2-3%)\n",
    "5. **Explainability:** Grad-CAM to visualize which regions triggered classification\n",
    "6. **Deployment:** ONNX Runtime with GPU, <50ms inference per wafer\n",
    "\n",
    "**Success Metrics:**\n",
    "- Top-1 accuracy \u226597% (25-class classification)\n",
    "- Per-class recall \u226590% (catch all defect types)\n",
    "- Inference latency <50ms per wafer (real-time fab integration)\n",
    "- Explainability: Grad-CAM highlights defect regions (engineer validation)\n",
    "\n",
    "**Challenges & Solutions:**\n",
    "- **Imbalance:** Rare defects <1% \u2192 Focal loss (down-weight easy examples), SMOTE oversampling\n",
    "- **Rotation invariance:** Wafers oriented randomly \u2192 Data augmentation (rotate 0-360\u00b0)\n",
    "- **False positives:** Cost of false alarm high \u2192 Threshold tuning (precision >95%)\n",
    "- **Production drift:** Defect patterns change over time \u2192 Monthly retraining pipeline\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 2: Die-Level SEM Image Defect Detection**\n",
    "\n",
    "**Objective:** Classify microscopic defects in die-level SEM (Scanning Electron Microscope) images.\n",
    "\n",
    "**Business Value:** $1M-$5M/year through automated defect review (reduce manual inspection time 80%).\n",
    "\n",
    "**Dataset:**\n",
    "- 100K+ SEM images (512\u00d7512 pixels, grayscale)\n",
    "- 50+ defect types (opens, shorts, voids, scratches, particles, contamination)\n",
    "- Multi-label (single image can have multiple defects)\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "ResNet-50 backbone (transfer learning)\n",
    "  \u2192 Multi-label classification head (sigmoid for each class)\n",
    "  \u2192 Binary cross-entropy loss (multi-label)\n",
    "```\n",
    "\n",
    "**Key Techniques:**\n",
    "- **Multi-label classification:** Single image \u2192 multiple defect labels (not mutually exclusive)\n",
    "- **High-resolution inputs:** 512\u00d7512 images (preserve fine details)\n",
    "- **Test-time augmentation:** Average predictions across rotations/flips (improve accuracy)\n",
    "\n",
    "**Success Metrics:**\n",
    "- Mean Average Precision (mAP) \u22650.90 (multi-label metric)\n",
    "- Per-defect recall \u226585% (catch all defect types)\n",
    "- Inference <100ms per image (batch processing 100+ images/second)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 3: Wafer Yield Prediction with Spatial Features (CNN + MLP)**\n",
    "\n",
    "**Objective:** Predict die-level yield combining parametric test data with spatial location.\n",
    "\n",
    "**Business Value:** $50M-$200M/year scrap reduction (notebook 052 problem, but with CNN spatial features).\n",
    "\n",
    "**Dataset:**\n",
    "- 500K+ die samples\n",
    "- 50 parametric features (Vdd, Idd, frequency, power, temperature)\n",
    "- Spatial coordinates (wafer_id, die_x, die_y)\n",
    "- Binary target (pass/fail)\n",
    "\n",
    "**Architecture (Hybrid CNN + MLP):**\n",
    "```\n",
    "Spatial Path (CNN):\n",
    "  wafer_map (300\u00d7300) \u2192 CNN \u2192 Spatial features (128-dim)\n",
    "\n",
    "Parametric Path (MLP):\n",
    "  parametric_tests (50-dim) \u2192 MLP \u2192 Test features (128-dim)\n",
    "\n",
    "Fusion:\n",
    "  Concat(spatial_features, test_features) \u2192 Dense(256) \u2192 Output(1, Sigmoid)\n",
    "```\n",
    "\n",
    "**Why hybrid:**\n",
    "- CNN captures spatial correlation (neighboring die fail together)\n",
    "- MLP captures parametric test patterns\n",
    "- Combined: Better than either alone (+5% accuracy improvement)\n",
    "\n",
    "**Success Metrics:**\n",
    "- AUC-ROC \u22650.98 (binary classification)\n",
    "- Precision \u226595% (minimize false positives - shipping bad dies)\n",
    "- Recall \u226590% (minimize false negatives - scrapping good dies)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 4: Adaptive Binning with Reinforcement Learning + CNN**\n",
    "\n",
    "**Objective:** Dynamically optimize device binning (speed grades) using RL with CNN spatial features.\n",
    "\n",
    "**Business Value:** $20M-$50M/year through optimized binning (maximize high-bin yield).\n",
    "\n",
    "**Problem:** Devices tested at multiple corners (VDD/frequency combinations), assign to bins (e.g., 3.0GHz, 3.2GHz, 3.5GHz) to maximize revenue.\n",
    "\n",
    "**Architecture (RL with CNN features):**\n",
    "```\n",
    "State: Current test results + wafer map CNN features\n",
    "Action: Which corner to test next, when to stop testing, which bin to assign\n",
    "Reward: Revenue from bin assignment - test cost\n",
    "Policy Network: DQN with CNN + MLP encoder\n",
    "```\n",
    "\n",
    "**Key Techniques:**\n",
    "- CNN extracts spatial patterns from wafer map (predict untested corners)\n",
    "- RL learns optimal test sequence (minimize tests, maximize bin revenue)\n",
    "- Offline RL training on historical data (safe, no fab impact)\n",
    "\n",
    "**Success Metrics:**\n",
    "- Revenue increase \u22653-5% (better binning accuracy)\n",
    "- Test time reduction \u226520% (fewer corners tested)\n",
    "- Bin accuracy \u226599% (devices meet spec)\n",
    "\n",
    "---\n",
    "\n",
    "### **General AI/ML Projects**\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 5: Medical Image Classification (Chest X-Ray Diagnosis)**\n",
    "\n",
    "**Objective:** Classify chest X-rays into normal/pneumonia/COVID-19/tuberculosis.\n",
    "\n",
    "**Dataset:** 100K+ chest X-rays (256\u00d7256 grayscale), 4 classes.\n",
    "\n",
    "**Architecture:** DenseNet-121 (transfer learning from ImageNet), class-weighted loss.\n",
    "\n",
    "**Business Value:** $1M-$10M/year through faster diagnosis, reduce radiologist workload 50%.\n",
    "\n",
    "**Success Metrics:** AUC-ROC \u22650.95, sensitivity \u226595% (minimize false negatives - critical for patient safety).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 6: Object Detection for Autonomous Vehicles**\n",
    "\n",
    "**Objective:** Real-time detection of cars, pedestrians, cyclists, traffic signs.\n",
    "\n",
    "**Dataset:** 500K+ images with bounding boxes, 20 classes.\n",
    "\n",
    "**Architecture:** YOLO v8 or EfficientDet (balance accuracy and speed).\n",
    "\n",
    "**Business Value:** Enable Level 4 autonomous driving (market value $trillions).\n",
    "\n",
    "**Success Metrics:** mAP \u22650.60, inference <50ms (20 FPS), detect objects 50m+ away.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 7: Facial Recognition Access Control System**\n",
    "\n",
    "**Objective:** Secure building access using face recognition (1:N matching).\n",
    "\n",
    "**Dataset:** 10K+ employees, 100+ photos per person, various lighting/angles.\n",
    "\n",
    "**Architecture:** FaceNet (triplet loss) or ArcFace (angular margin loss), generate 128-dim embeddings.\n",
    "\n",
    "**Business Value:** $500K-$2M/year through automated access control, security audit trails.\n",
    "\n",
    "**Success Metrics:** True Accept Rate \u226599% @ False Accept Rate <0.01%.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Project 8: Content Moderation (Inappropriate Image Detection)**\n",
    "\n",
    "**Objective:** Flag inappropriate content (NSFW, violence, hate symbols) on social media.\n",
    "\n",
    "**Dataset:** 1M+ images, multi-label (single image can have multiple violations).\n",
    "\n",
    "**Architecture:** EfficientNet-B4 (multi-label classification), weighted loss.\n",
    "\n",
    "**Business Value:** $10M-$50M/year through automated moderation (reduce manual review 70%).\n",
    "\n",
    "**Success Metrics:** Precision \u226595% (minimize false positives), recall \u226585% (catch most violations).\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd11 Key Takeaways\n",
    "\n",
    "### **CNN Architecture Design Principles**\n",
    "\n",
    "1. **Start simple, go deep gradually**\n",
    "   - Begin with 3-5 conv layers (like VGG)\n",
    "   - Add residual connections if >10 layers (like ResNet)\n",
    "   - Use batch normalization after every conv layer\n",
    "\n",
    "2. **Receptive field matters**\n",
    "   - Stack small filters (3\u00d73) instead of large filters (7\u00d77)\n",
    "   - Two 3\u00d73 conv = one 5\u00d75 receptive field, but fewer parameters\n",
    "   - Use stride=2 or pooling to downsample (increase receptive field faster)\n",
    "\n",
    "3. **Parameter efficiency**\n",
    "   - 1\u00d71 convolutions reduce channels (dimensionality reduction)\n",
    "   - Depthwise separable convolutions reduce parameters 8-9\u00d7 (MobileNet)\n",
    "   - Global Average Pooling replaces FC layers (fewer parameters, regularization)\n",
    "\n",
    "4. **Modern best practices**\n",
    "   - ReLU activation (or variants: LeakyReLU, ELU, Swish)\n",
    "   - Batch normalization (stabilizes training, allows higher learning rates)\n",
    "   - Dropout in fully connected layers (prevents overfitting)\n",
    "   - Data augmentation (rotation, flip, crop, color jittering)\n",
    "   - Skip connections for deep networks (ResNet-style)\n",
    "\n",
    "---\n",
    "\n",
    "### **Transfer Learning Strategy**\n",
    "\n",
    "**When to use transfer learning:**\n",
    "- \u2705 Limited data (<10K samples)\n",
    "- \u2705 Similar domain (natural images, medical images, wafer maps all benefit from ImageNet features)\n",
    "- \u2705 Time/compute constraints (10\u00d7 faster training)\n",
    "\n",
    "**Best practices:**\n",
    "1. **Start with feature extraction:** Freeze all layers except final classifier\n",
    "2. **Fine-tune gradually:** After feature extraction converges, unfreeze last few layers\n",
    "3. **Lower learning rate for fine-tuning:** Use 10-100\u00d7 smaller LR for pre-trained layers\n",
    "4. **Data augmentation:** More aggressive augmentation for small datasets\n",
    "5. **Choose appropriate backbone:** ResNet-50 (balanced), EfficientNet (efficient), VGG (simple)\n",
    "\n",
    "**When NOT to use transfer learning:**\n",
    "- \u274c Very different domain (satellite imagery, medical histology)\n",
    "- \u274c Massive dataset (>1M samples, train from scratch can be better)\n",
    "- \u274c Real-time constraints (pre-trained models often large, use MobileNet instead)\n",
    "\n",
    "---\n",
    "\n",
    "### **Production Deployment Checklist**\n",
    "\n",
    "- \u2705 **Model optimization:** ONNX export, INT8 quantization (4\u00d7 faster inference)\n",
    "- \u2705 **Batch inference:** Process multiple images at once (10\u00d7 throughput increase)\n",
    "- \u2705 **GPU acceleration:** Use ONNX Runtime with GPU provider (<10ms inference)\n",
    "- \u2705 **Monitoring:** Log predictions, latency, errors (detect model drift)\n",
    "- \u2705 **Explainability:** Grad-CAM or LIME to visualize decisions (engineer trust)\n",
    "- \u2705 **A/B testing:** Compare new model vs baseline before full rollout\n",
    "- \u2705 **Fallback:** Simple rule-based system if model fails\n",
    "- \u2705 **Retraining pipeline:** Automate monthly retraining as new data arrives\n",
    "\n",
    "---\n",
    "\n",
    "### **Semiconductor-Specific Best Practices**\n",
    "\n",
    "1. **Data augmentation for wafer maps:**\n",
    "   - Rotation (0-360\u00b0, wafers can be oriented any way)\n",
    "   - Flip (horizontal, vertical)\n",
    "   - Zoom (simulate different die sizes)\n",
    "   - **Don't use:** Color jittering (wafer maps are binary pass/fail)\n",
    "\n",
    "2. **Handle class imbalance:**\n",
    "   - Most wafers are normal (80%+), defects rare (<20%)\n",
    "   - Use focal loss, class weights, or SMOTE oversampling\n",
    "   - Prioritize recall for rare critical defects\n",
    "\n",
    "3. **Spatial correlation matters:**\n",
    "   - CNN captures neighboring die failures (cluster patterns)\n",
    "   - Combine with parametric test data (hybrid CNN + MLP)\n",
    "   - Use attention mechanisms to focus on defect regions\n",
    "\n",
    "4. **Explainability is critical:**\n",
    "   - Engineers need to understand why model classified wafer\n",
    "   - Use Grad-CAM to highlight defect regions\n",
    "   - Build trust through interpretability\n",
    "\n",
    "---\n",
    "\n",
    "### **What's Next?**\n",
    "\n",
    "**After mastering CNNs:**\n",
    "- \ud83d\udcd8 **Notebook 054:** Recurrent Neural Networks (RNNs, LSTMs) for time-series\n",
    "- \ud83d\udcd8 **Notebook 055:** Transformers and Attention (modern SOTA architecture)\n",
    "- \ud83d\udcd8 **Notebook 056:** Object Detection (YOLO, Faster R-CNN, RetinaNet)\n",
    "- \ud83d\udcd8 **Notebook 057:** Semantic Segmentation (U-Net, DeepLab)\n",
    "- \ud83d\udcd8 **Notebook 058:** Generative Models (GANs, VAEs, Diffusion)\n",
    "\n",
    "**Advanced CNN topics (not covered):**\n",
    "- Neural Architecture Search (NAS): Automate architecture design\n",
    "- Pruning and knowledge distillation: Compress models 10\u00d7\n",
    "- 3D CNNs: Process video or volumetric medical data\n",
    "- Graph CNNs: Process non-Euclidean data (molecules, social networks)\n",
    "\n",
    "---\n",
    "\n",
    "## \u2705 Learning Objectives Review\n",
    "\n",
    "By now, you should be able to:\n",
    "- \u2705 Understand convolution operations mathematically and implement from scratch\n",
    "- \u2705 Build CNNs in PyTorch and TensorFlow/Keras\n",
    "- \u2705 Explain the architecture evolution (LeNet \u2192 AlexNet \u2192 VGG \u2192 Inception \u2192 ResNet)\n",
    "- \u2705 Apply transfer learning to new domains (ImageNet \u2192 wafer maps)\n",
    "- \u2705 Optimize CNNs for production (quantization, ONNX, batching)\n",
    "- \u2705 Deploy CNN models with <100ms inference time\n",
    "- \u2705 Visualize what CNNs learn using Grad-CAM\n",
    "- \u2705 Choose appropriate architecture based on constraints (accuracy, speed, memory)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf93 Congratulations!\n",
    "\n",
    "You've mastered Convolutional Neural Networks! You can now:\n",
    "- Build custom CNNs from scratch for any image task\n",
    "- Leverage transfer learning for faster development\n",
    "- Deploy production-grade models for semiconductor testing\n",
    "- Optimize models for real-time inference constraints\n",
    "\n",
    "**Next:** Dive into Recurrent Neural Networks for time-series and sequential data! \ud83d\ude80\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcda Additional Resources\n",
    "\n",
    "**Papers (Must Read):**\n",
    "- LeNet-5: LeCun et al. (1998) - \"Gradient-Based Learning Applied to Document Recognition\"\n",
    "- AlexNet: Krizhevsky et al. (2012) - \"ImageNet Classification with Deep CNNs\"\n",
    "- VGG: Simonyan & Zisserman (2014) - \"Very Deep CNNs for Large-Scale Image Recognition\"\n",
    "- Inception: Szegedy et al. (2014) - \"Going Deeper with Convolutions\"\n",
    "- ResNet: He et al. (2015) - \"Deep Residual Learning for Image Recognition\"\n",
    "\n",
    "**Courses:**\n",
    "- Stanford CS231n: Convolutional Neural Networks for Visual Recognition\n",
    "- Fast.ai: Practical Deep Learning for Coders (transfer learning focus)\n",
    "\n",
    "**Libraries:**\n",
    "- `timm` (PyTorch Image Models): 700+ pre-trained models\n",
    "- `tensorflow.keras.applications`: 30+ pre-trained models\n",
    "- `segmentation_models.pytorch`: U-Net, FPN, DeepLab implementations\n",
    "\n",
    "**Visualization:**\n",
    "- Grad-CAM: Visualize what CNN learned\n",
    "- TensorBoard: Monitor training in real-time\n",
    "- Netron: Visualize model architecture\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Complete!** \ud83c\udf89"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}