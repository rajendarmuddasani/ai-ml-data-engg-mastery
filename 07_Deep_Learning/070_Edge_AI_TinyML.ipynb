{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9892e7d",
   "metadata": {},
   "source": [
    "# 070: Edge AI & TinyML - On-Device Inference\n",
    "\n",
    "## üìò Complete Guide to Deploying ML on Microcontrollers & Edge Devices\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Understand Edge AI fundamentals**: Why run models on-device (latency, privacy, cost)\n",
    "2. **Master TinyML**: Deploy ML models on microcontrollers (<1MB memory)\n",
    "3. **Implement model optimization**: Quantization (INT8, INT4), pruning, knowledge distillation\n",
    "4. **Deploy production systems**: TensorFlow Lite, ONNX Runtime, TensorRT, Arduino, ESP32\n",
    "5. **Build real-world projects**: Keyword spotting, gesture recognition, anomaly detection ($50M-$150M/year value)\n",
    "\n",
    "---\n",
    "\n",
    "## üìä What is Edge AI & TinyML?\n",
    "\n",
    "### Edge AI\n",
    "**Definition**: Running machine learning inference on edge devices (smartphones, IoT sensors, embedded systems) instead of cloud servers\n",
    "\n",
    "**Key characteristics:**\n",
    "- **Local inference**: No internet required (works offline)\n",
    "- **Low latency**: <10ms response time (vs 100-500ms cloud)\n",
    "- **Privacy**: Data never leaves device (GDPR/HIPAA compliant)\n",
    "- **Cost efficiency**: No cloud API fees ($0.001-$0.01 per inference)\n",
    "\n",
    "### TinyML\n",
    "**Definition**: Subset of Edge AI focused on ultra-constrained devices (microcontrollers with <1MB RAM, <1MHz CPU)\n",
    "\n",
    "**Target devices:**\n",
    "- **Microcontrollers**: Arduino Nano 33 BLE (256KB RAM), ESP32 (520KB RAM), STM32 (64-512KB RAM)\n",
    "- **AI accelerators**: Google Coral Edge TPU, NVIDIA Jetson Nano, Intel Movidius\n",
    "- **Wearables**: Smartwatches, fitness trackers, hearing aids\n",
    "- **IoT sensors**: Smart home, industrial sensors, environmental monitoring\n",
    "\n",
    "**Constraints:**\n",
    "- **Memory**: <1MB RAM (model must fit entirely in SRAM)\n",
    "- **Compute**: <100 MFLOPS (vs 10 TFLOPS GPU)\n",
    "- **Power**: <10mW (battery-powered, must last months/years)\n",
    "- **Storage**: <1MB flash (model + firmware)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Why Edge AI Matters: The Deployment Problem\n",
    "\n",
    "### The Cloud Inference Problem\n",
    "\n",
    "**Example: Image classification API**\n",
    "- **Input**: 224√ó224√ó3 RGB image = 150KB\n",
    "- **Upload time**: 150KB √∑ 10Mbps (typical 4G) = 120ms\n",
    "- **Inference time**: 50ms (cloud GPU)\n",
    "- **Download time**: 1KB result √∑ 10Mbps = 1ms\n",
    "- **Total latency**: 120 + 50 + 1 = **171ms** ‚ùå\n",
    "\n",
    "**Problems:**\n",
    "1. **Latency**: 171ms too slow for real-time (autonomous vehicles need <10ms)\n",
    "2. **Privacy**: User uploads personal images to cloud (GDPR violation risk)\n",
    "3. **Cost**: $0.001 per inference √ó 1B inferences/month = $1M/month\n",
    "4. **Reliability**: Requires internet connection (fails offline)\n",
    "\n",
    "### The Edge AI Solution\n",
    "\n",
    "**On-device inference:**\n",
    "- **No upload**: 0ms (data stays on device)\n",
    "- **Local inference**: 10ms (optimized model)\n",
    "- **No download**: 0ms (result computed locally)\n",
    "- **Total latency**: **10ms** ‚úÖ (17√ó faster)\n",
    "\n",
    "**Benefits:**\n",
    "1. **Latency**: 10ms (real-time capable)\n",
    "2. **Privacy**: Data never leaves device (GDPR compliant)\n",
    "3. **Cost**: $0 per inference after deployment (no API fees)\n",
    "4. **Reliability**: Works offline (no internet required)\n",
    "\n",
    "---\n",
    "\n",
    "## üí∞ Business Value: $50M-$150M/year\n",
    "\n",
    "Edge AI unlocks massive business value across three dimensions:\n",
    "\n",
    "### Use Case 1: Smart Home Voice Assistant ($15M-$40M/year)\n",
    "\n",
    "**Problem**: Cloud-based voice assistants (Alexa, Google Home)\n",
    "- **Latency**: 300-500ms (\"always listening\" but slow response)\n",
    "- **Privacy**: All audio sent to cloud (user concern, GDPR issues)\n",
    "- **Cost**: $0.005 per query √ó 10M users √ó 100 queries/day = $5M/month = $60M/year\n",
    "- **Dependency**: Requires internet (fails during outages)\n",
    "\n",
    "**Edge AI Solution**: On-device keyword spotting + local command processing\n",
    "- **Latency**: 10-20ms (\"instant\" response)\n",
    "- **Privacy**: Audio processed locally (only wake word triggers upload)\n",
    "- **Cost**: $0/query after deployment (savings: $60M/year)\n",
    "- **Reliability**: Works offline (critical for security cameras, door locks)\n",
    "\n",
    "**Implementation:**\n",
    "- **Wake word detection**: TinyML model on microcontroller (10KB model, <5mW power)\n",
    "- **Command classification**: Edge model on device (100KB model, 50ms inference)\n",
    "- **Only upload if needed**: Complex queries go to cloud (90% handled locally)\n",
    "\n",
    "**Business metrics:**\n",
    "- **Cost savings**: $60M/year ‚Üí $5M/year infrastructure = **$55M/year saved**\n",
    "- **User satisfaction**: NPS +12 (privacy + speed)\n",
    "- **Market differentiation**: \"Privacy-first assistant\" (vs cloud competitors)\n",
    "- **Total value**: **$15M-$40M/year** (10M users)\n",
    "\n",
    "---\n",
    "\n",
    "### Use Case 2: Manufacturing Defect Detection ($20M-$60M/year)\n",
    "\n",
    "**Problem**: Cloud-based visual inspection\n",
    "- **Latency**: 200ms (too slow for production line, 60 items/min max)\n",
    "- **Privacy**: Cannot send proprietary product images to cloud (trade secrets)\n",
    "- **Cost**: $0.01 per inspection √ó 1M inspections/day = $10K/day = $3.6M/year\n",
    "- **Bandwidth**: 500KB per image √ó 1M images/day = 500GB/day = $45/day = $16K/year\n",
    "\n",
    "**Edge AI Solution**: On-device defect detection (camera + edge device)\n",
    "- **Latency**: 10ms (600 items/min, 10√ó faster throughput)\n",
    "- **Privacy**: Images processed locally (no upload, trade secrets protected)\n",
    "- **Cost**: $0 per inspection (no API fees)\n",
    "- **Bandwidth**: 0 (no uploads)\n",
    "\n",
    "**Implementation:**\n",
    "- **Edge device**: NVIDIA Jetson Nano ($99, 128 CUDA cores)\n",
    "- **Model**: MobileNetV2 + INT8 quantization (5MB model, 10ms inference)\n",
    "- **Deployment**: 100 production lines √ó $99 = $9,900 one-time cost\n",
    "\n",
    "**Business metrics:**\n",
    "- **Throughput increase**: 60 ‚Üí 600 items/min (10√ó faster)\n",
    "- **Revenue impact**: $1M/year per line √ó 10√ó throughput = $10M/year (or avoid $10M capex for 10√ó more lines)\n",
    "- **Cost savings**: $3.6M/year API fees ‚Üí $0\n",
    "- **Privacy value**: Priceless (trade secret protection)\n",
    "- **Conservative value per factory**: **$5M-$15M/year**\n",
    "- **Total (4 factories)**: **$20M-$60M/year**\n",
    "\n",
    "---\n",
    "\n",
    "### Use Case 3: Wearable Health Monitoring ($15M-$50M/year)\n",
    "\n",
    "**Problem**: Cloud-based health monitoring (Apple Watch, Fitbit)\n",
    "- **Latency**: 500ms (ECG ‚Üí cloud ‚Üí analysis ‚Üí alert)\n",
    "- **Privacy**: Sensitive health data uploaded to cloud (HIPAA concerns)\n",
    "- **Cost**: $0.001 per heartbeat √ó 100 beats/min √ó 10M users √ó 60 min/hr √ó 24 hr/day = $14.4B/day ‚ùå (impossible)\n",
    "- **Battery**: Constant uploads drain battery (24-hour lifespan vs 7-day goal)\n",
    "\n",
    "**Edge AI Solution**: On-device health analytics\n",
    "- **Latency**: <10ms (real-time arrhythmia detection)\n",
    "- **Privacy**: Health data stays on device (HIPAA compliant)\n",
    "- **Cost**: $0 per inference (no uploads)\n",
    "- **Battery**: 7-day lifespan (only upload alerts, not raw data)\n",
    "\n",
    "**Implementation:**\n",
    "- **TinyML model**: Arrhythmia detection (50KB model, 5ms inference, <1mW power)\n",
    "- **Microcontroller**: ARM Cortex-M4 (built into smartwatch)\n",
    "- **Alert mechanism**: Only upload if anomaly detected (99.9% filtered locally)\n",
    "\n",
    "**Business metrics:**\n",
    "- **Cost avoidance**: $14.4B/day ‚Üí $0 (cloud inference impossible, edge AI enables feature)\n",
    "- **Battery life**: 24hr ‚Üí 7 days (7√ó improvement)\n",
    "- **Privacy**: HIPAA compliant (vs regulatory risk)\n",
    "- **Market differentiation**: \"Medical-grade on-device monitoring\"\n",
    "- **Revenue**: 10M users √ó $5/month subscription = $50M/month = **$600M/year** (aspirational)\n",
    "- **Conservative capture**: 2.5-8% of revenue attributed to edge AI = **$15M-$50M/year**\n",
    "\n",
    "---\n",
    "\n",
    "### Total Business Value Summary\n",
    "\n",
    "| Use Case | Annual Value | Key Metric | Deployment |\n",
    "|----------|--------------|------------|------------|\n",
    "| Smart Home Voice (10M users) | $15M-$40M | $55M cost savings | Microcontroller (10KB model) |\n",
    "| Manufacturing Defect (4 factories) | $20M-$60M | 10√ó throughput | Jetson Nano (5MB model) |\n",
    "| Wearable Health (10M users) | $15M-$50M | 7-day battery | ARM Cortex-M4 (50KB model) |\n",
    "| **Total** | **$50M-$150M** | Latency + Privacy + Cost | Edge/TinyML |\n",
    "\n",
    "**Conservative midpoint**: **$100M/year** across edge AI applications\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Edge AI Workflow: From Cloud Model to Microcontroller\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Train Large Model on Cloud<br/>ResNet-50, 98.5% accuracy, 25MB] --> B[Model Optimization<br/>Quantization + Pruning + Distillation]\n",
    "    B --> C[Compressed Model<br/>MobileNetV2, 96.2% accuracy, 5MB]\n",
    "    C --> D[Convert to Edge Format<br/>TensorFlow Lite / ONNX / TensorRT]\n",
    "    D --> E[Deploy to Edge Device<br/>Smartphone / Jetson / Microcontroller]\n",
    "    E --> F[On-Device Inference<br/>10ms latency, 0 cost, privacy-preserving]\n",
    "    \n",
    "    style A fill:#ffcccc\n",
    "    style C fill:#ccffcc\n",
    "    style F fill:#ccccff\n",
    "```\n",
    "\n",
    "**Key steps:**\n",
    "1. **Train large model**: ResNet-50, 25MB, 98.5% accuracy (cloud GPU)\n",
    "2. **Optimize model**: Quantize (INT8) + Prune (50%) + Distill ‚Üí 5MB, 96.2% accuracy\n",
    "3. **Convert to edge format**: TensorFlow Lite (.tflite), ONNX (.onnx), TensorRT (.engine)\n",
    "4. **Deploy to device**: Flash to microcontroller or install on mobile app\n",
    "5. **Inference**: 10ms latency, no internet, privacy-preserving\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Edge AI Architecture Spectrum\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Cloud Only<br/>500ms latency<br/>$1M/month cost<br/>Privacy risk] --> B[Hybrid<br/>Edge filtering + Cloud fallback<br/>50ms latency<br/>$100K/month]\n",
    "    B --> C[Edge Only<br/>Smartphone/Jetson<br/>10ms latency<br/>$1K/month]\n",
    "    C --> D[TinyML<br/>Microcontroller<br/>5ms latency<br/>$0/month]\n",
    "    \n",
    "    style A fill:#ffcccc\n",
    "    style D fill:#ccffcc\n",
    "```\n",
    "\n",
    "**Device spectrum:**\n",
    "1. **Cloud only**: All inference on server (500ms, $1M/month, privacy risk)\n",
    "2. **Hybrid**: Edge filtering + cloud fallback (50ms, $100K/month, 90% local)\n",
    "3. **Edge only**: Smartphone/Jetson (10ms, $1K/month, full offline)\n",
    "4. **TinyML**: Microcontroller (5ms, $0/month, ultra-low power)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ When to Use Edge AI vs Cloud\n",
    "\n",
    "### Use Edge AI when:\n",
    "‚úÖ **Latency critical**: <50ms required (autonomous vehicles, robotics)\n",
    "‚úÖ **Privacy sensitive**: Health data, personal photos, voice recordings\n",
    "‚úÖ **Offline capability**: No internet (remote sensors, submarines, aircraft)\n",
    "‚úÖ **High volume**: >1M inferences/day (cost prohibitive in cloud)\n",
    "‚úÖ **Bandwidth constrained**: Cannot upload large data (video, images)\n",
    "\n",
    "### Use Cloud when:\n",
    "‚úÖ **Complex models**: >100MB models (GPT-4, DALL-E, large ensembles)\n",
    "‚úÖ **Low volume**: <1K inferences/day (cloud cheaper than edge deployment)\n",
    "‚úÖ **Continuous learning**: Model updates daily (edge deployment friction)\n",
    "‚úÖ **Heterogeneous devices**: Many device types (cloud API simpler)\n",
    "\n",
    "### Hybrid approach (best of both):\n",
    "- **Edge**: Wake word detection, face detection, sensor anomaly filtering (99% of data)\n",
    "- **Cloud**: Complex analysis, speech recognition, image captioning (1% of data)\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Edge AI Technology Stack\n",
    "\n",
    "### Model Optimization Techniques\n",
    "\n",
    "| Technique | Compression | Accuracy Loss | Compute Speedup |\n",
    "|-----------|-------------|---------------|-----------------|\n",
    "| **Quantization (INT8)** | 4√ó | 0.5-2% | 2-4√ó |\n",
    "| **Pruning (structured)** | 2-5√ó | 1-3% | 1.5-3√ó |\n",
    "| **Knowledge Distillation** | 10-100√ó | 2-5% | 5-20√ó |\n",
    "| **Neural Architecture Search** | 5-20√ó | 0-2% | 3-10√ó |\n",
    "| **Combined (all above)** | 50-200√ó | 3-8% | 10-50√ó |\n",
    "\n",
    "### Deployment Frameworks\n",
    "\n",
    "| Framework | Target Devices | Model Size | Use Case |\n",
    "|-----------|----------------|------------|----------|\n",
    "| **TensorFlow Lite** | Android/iOS/Linux | 1MB-100MB | Mobile apps, edge devices |\n",
    "| **TensorFlow Lite Micro** | Microcontrollers | 10KB-500KB | TinyML, ultra-low power |\n",
    "| **ONNX Runtime** | Cross-platform | 1MB-1GB | Server, edge, mobile |\n",
    "| **TensorRT** | NVIDIA GPUs | 1MB-10GB | Jetson, autonomous vehicles |\n",
    "| **Core ML** | Apple devices | 1MB-100MB | iPhone, iPad, Mac |\n",
    "| **OpenVINO** | Intel CPUs/VPUs | 1MB-1GB | Intel NUC, RealSense |\n",
    "\n",
    "### Edge Hardware Comparison\n",
    "\n",
    "| Device | RAM | Compute | Power | Cost | Use Case |\n",
    "|--------|-----|---------|-------|------|----------|\n",
    "| **Arduino Nano 33 BLE** | 256KB | 64 MHz | 5mW | $25 | TinyML, keyword spotting |\n",
    "| **ESP32** | 520KB | 240 MHz | 10mW | $5 | IoT sensors, gesture recognition |\n",
    "| **STM32H7** | 1MB | 480 MHz | 20mW | $15 | Industrial sensors, audio processing |\n",
    "| **Raspberry Pi 4** | 4GB | 1.5 GHz | 3W | $55 | Edge server, multi-model inference |\n",
    "| **Google Coral Edge TPU** | 8GB | 4 TOPS | 2W | $75 | Vision applications, 400 FPS |\n",
    "| **NVIDIA Jetson Nano** | 4GB | 472 GFLOPS | 10W | $99 | Autonomous robots, manufacturing |\n",
    "| **NVIDIA Jetson Xavier NX** | 8GB | 21 TOPS | 15W | $399 | Autonomous vehicles, drones |\n",
    "| **iPhone 14 (A16 Bionic)** | 6GB | 17 TOPS | 5W | $799 | Mobile AI, AR/VR |\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Historical Context: The Edge AI Revolution\n",
    "\n",
    "### 2010-2015: Cloud-Only Era\n",
    "- All ML inference in cloud (AlexNet, VGG, ResNet on server GPUs)\n",
    "- Mobile devices only captured data and displayed results\n",
    "- Latency: 500ms, Cost: $0.01/inference\n",
    "\n",
    "### 2016-2017: Mobile AI Emerges\n",
    "- **SqueezeNet (2016)**: 50√ó smaller than AlexNet, same accuracy\n",
    "- **MobileNets (2017)**: Designed for mobile devices (4.2MB model)\n",
    "- **Core ML (2017)**: Apple enables on-device ML on iPhone\n",
    "\n",
    "### 2018-2019: Edge AI Accelerates\n",
    "- **TensorFlow Lite (2018)**: Google's mobile/edge ML framework\n",
    "- **EfficientNet (2019)**: State-of-the-art efficiency (10√ó better than ResNet)\n",
    "- **NVIDIA Jetson Nano (2019)**: $99 edge AI computer (472 GFLOPS)\n",
    "\n",
    "### 2019-2020: TinyML Born\n",
    "- **TensorFlow Lite Micro (2019)**: ML on microcontrollers (<1MB RAM)\n",
    "- **TinyML Summit (2019)**: First conference on ultra-low-power ML\n",
    "- **Google Coral (2019)**: Edge TPU ($75, 4 TOPS)\n",
    "\n",
    "### 2021-Present: Edge AI Mainstream\n",
    "- **Apple Neural Engine**: 15.8 TOPS on iPhone 13 (2021)\n",
    "- **Qualcomm AI Engine**: 15 TOPS on Snapdragon 8 Gen 2 (2022)\n",
    "- **Edge AI market**: $15B (2023) ‚Üí $75B projected (2028)\n",
    "- **TinyML deployments**: 1B+ devices worldwide\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Learning Path Context\n",
    "\n",
    "**Where we are:**\n",
    "- **Completed**: 066 Attention ‚Üí 067 NAS ‚Üí 068 Model Compression ‚Üí 069 Federated Learning\n",
    "- **Current**: 070 Edge AI & TinyML (practical deployment)\n",
    "- **Next**: 071 Transformers & BERT (large language models)\n",
    "\n",
    "**Why Edge AI matters:**\n",
    "- **Practical deployment**: Models useless if too large/slow for production\n",
    "- **Real-world constraints**: 99% of ML applications need edge deployment\n",
    "- **Business value**: $50M-$150M/year from latency + privacy + cost savings\n",
    "\n",
    "---\n",
    "\n",
    "## üîç What Makes Edge AI Challenging?\n",
    "\n",
    "### Challenge 1: Model Size Constraints\n",
    "- **Cloud**: 25MB ResNet-50 (no problem)\n",
    "- **Mobile**: 5MB MobileNetV2 (acceptable)\n",
    "- **Microcontroller**: 50KB model (must fit in SRAM) ‚ùå\n",
    "\n",
    "**Solution**: Aggressive compression (quantization + pruning + distillation)\n",
    "\n",
    "### Challenge 2: Compute Constraints\n",
    "- **Cloud GPU**: 10 TFLOPS (bfloat16), 50ms inference\n",
    "- **Mobile CPU**: 100 GFLOPS (float32), 100ms inference\n",
    "- **Microcontroller**: 10 MFLOPS (int8), 10ms inference ‚ùå\n",
    "\n",
    "**Solution**: Optimize operations (depthwise conv, skip connections, INT8 quantization)\n",
    "\n",
    "### Challenge 3: Power Constraints\n",
    "- **Edge server**: 300W (plugged in, no constraint)\n",
    "- **Mobile phone**: 5W (battery 1 day, acceptable)\n",
    "- **Wearable**: 10mW (battery 1 week, critical) ‚ùå\n",
    "\n",
    "**Solution**: Ultra-low-power operations (sparse inference, event-driven, duty cycling)\n",
    "\n",
    "### Challenge 4: Memory Constraints\n",
    "- **Cloud**: 32GB GPU memory (no constraint)\n",
    "- **Mobile**: 4GB RAM (acceptable)\n",
    "- **Microcontroller**: 256KB SRAM (model + activations + stack must fit) ‚ùå\n",
    "\n",
    "**Solution**: In-place operations, memory reuse, streaming inference\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Questions This Notebook Answers\n",
    "\n",
    "1. **How to compress 25MB model ‚Üí 50KB model** (500√ó compression, <5% accuracy loss)\n",
    "2. **How to quantize float32 ‚Üí INT8** (4√ó smaller, 2-4√ó faster, <2% accuracy loss)\n",
    "3. **How to deploy to TensorFlow Lite** (mobile/edge devices)\n",
    "4. **How to deploy to TensorFlow Lite Micro** (microcontrollers, <1MB RAM)\n",
    "5. **How to deploy to NVIDIA Jetson** (edge AI computer, 472 GFLOPS)\n",
    "6. **When to use Edge AI vs Cloud** (latency, privacy, cost trade-offs)\n",
    "7. **How to build production edge AI systems** (8 real-world projects, $50M-$150M/year)\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Notebook Structure\n",
    "\n",
    "1. **Introduction** (this cell): Why Edge AI, business value, historical context\n",
    "2. **Mathematical Foundations**: Quantization theory, pruning theory, distillation theory, efficiency metrics\n",
    "3. **Implementation**: TensorFlow Lite, TFLite Micro, Arduino deployment, Jetson deployment\n",
    "4. **Production Projects**: 8 real-world projects (smart home, manufacturing, wearable, autonomous vehicles, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Let's Build Edge AI Systems!\n",
    "\n",
    "In the next cells, we'll:\n",
    "1. **Derive the math**: Quantization (symmetric, asymmetric), pruning (magnitude, structured), distillation (Hinton 2015)\n",
    "2. **Implement from scratch**: INT8 quantization, structured pruning, KD loss\n",
    "3. **Deploy to production**: TensorFlow Lite (mobile), TFLite Micro (Arduino), TensorRT (Jetson)\n",
    "4. **Build 8 projects**: Smart home voice ($15M-$40M), manufacturing defect ($20M-$60M), wearable health ($15M-$50M), etc.\n",
    "\n",
    "**Total business value**: $50M-$150M/year from edge AI deployment\n",
    "\n",
    "Ready? Let's make ML models run anywhere! üöÄüîßüì±\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Progression:**\n",
    "- **Previous**: 069 Federated Learning (Privacy-Preserving Distributed ML)\n",
    "- **Current**: 070 Edge AI & TinyML (On-Device Inference)\n",
    "- **Next**: 071 Transformers & BERT (Self-Attention, Pre-training, Transfer Learning)\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Introduction complete! Next: Mathematical foundations of model compression for edge deployment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad20737",
   "metadata": {},
   "source": [
    "# üìê Mathematical Foundations: Model Optimization for Edge Deployment\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "To deploy ML models on edge devices (smartphones, microcontrollers, IoT sensors), we need **3 fundamental techniques**:\n",
    "\n",
    "1. **Quantization**: Convert float32 (32 bits) ‚Üí INT8 (8 bits) = 4√ó smaller, 2-4√ó faster\n",
    "2. **Pruning**: Remove unimportant weights (50-90% sparsity) = 2-10√ó smaller\n",
    "3. **Knowledge Distillation**: Train small \"student\" model to mimic large \"teacher\" = 10-100√ó smaller\n",
    "\n",
    "**Combined effect**: 25MB ResNet-50 ‚Üí 50KB MobileNetV2 (500√ó compression, <5% accuracy loss)\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Quantization: Float32 ‚Üí INT8\n",
    "\n",
    "## What is Quantization?\n",
    "\n",
    "**Definition**: Map high-precision floating-point values to low-precision integers\n",
    "\n",
    "**Motivation**:\n",
    "- **Memory**: float32 (4 bytes) ‚Üí INT8 (1 byte) = **4√ó smaller**\n",
    "- **Compute**: INT8 operations 2-4√ó faster than float32 (hardware optimized)\n",
    "- **Power**: INT8 uses 5-10√ó less energy than float32\n",
    "\n",
    "**Trade-off**: Slight accuracy loss (typically 0.5-2%)\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Symmetric Quantization\n",
    "\n",
    "**Idea**: Map float range [-Œ±, Œ±] to INT8 range [-127, 127]\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$\n",
    "q = \\text{round}\\left(\\frac{r}{s}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $r$ = real-valued float32 number\n",
    "- $q$ = quantized INT8 integer\n",
    "- $s$ = scale factor = $\\frac{\\alpha}{127}$\n",
    "\n",
    "### Dequantization (INT8 ‚Üí float32)\n",
    "\n",
    "$$\n",
    "r = s \\cdot q\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "**Quantize weight matrix:**\n",
    "\n",
    "Original weights (float32):\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "0.8 & -0.5 & 0.3 \\\\\n",
    "-0.2 & 0.9 & -0.7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 1**: Find maximum absolute value\n",
    "$$\n",
    "\\alpha = \\max(|W|) = 0.9\n",
    "$$\n",
    "\n",
    "**Step 2**: Calculate scale factor\n",
    "$$\n",
    "s = \\frac{\\alpha}{127} = \\frac{0.9}{127} = 0.00709\n",
    "$$\n",
    "\n",
    "**Step 3**: Quantize each weight\n",
    "$$\n",
    "q_{ij} = \\text{round}\\left(\\frac{w_{ij}}{s}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "q_{11} = \\text{round}\\left(\\frac{0.8}{0.00709}\\right) = \\text{round}(112.8) = 113\n",
    "$$\n",
    "\n",
    "$$\n",
    "q_{12} = \\text{round}\\left(\\frac{-0.5}{0.00709}\\right) = \\text{round}(-70.5) = -71\n",
    "$$\n",
    "\n",
    "$$\n",
    "q_{13} = \\text{round}\\left(\\frac{0.3}{0.00709}\\right) = \\text{round}(42.3) = 42\n",
    "$$\n",
    "\n",
    "Similarly for second row:\n",
    "\n",
    "$$\n",
    "Q = \\begin{bmatrix}\n",
    "113 & -71 & 42 \\\\\n",
    "-28 & 127 & -99\n",
    "\\end{bmatrix} \\quad \\text{(INT8)}\n",
    "$$\n",
    "\n",
    "**Dequantization** (to verify):\n",
    "$$\n",
    "w_{11} = 0.00709 \\times 113 = 0.801 \\approx 0.8 \\quad ‚úÖ\n",
    "$$\n",
    "\n",
    "**Quantization error**:\n",
    "$$\n",
    "\\text{Error} = |0.801 - 0.8| = 0.001 \\quad \\text{(0.125% relative error)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 Asymmetric Quantization\n",
    "\n",
    "**Motivation**: Symmetric quantization wastes range if distribution is not centered at 0\n",
    "\n",
    "**Example**: ReLU activations (always non-negative)\n",
    "- **Symmetric**: Maps [0, 1.0] to [-127, 127] ‚Üí Wastes negative range ‚ùå\n",
    "- **Asymmetric**: Maps [0, 1.0] to [0, 255] ‚Üí Uses full range ‚úÖ\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$\n",
    "q = \\text{round}\\left(\\frac{r}{s}\\right) + z\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $s$ = scale factor = $\\frac{r_{\\max} - r_{\\min}}{q_{\\max} - q_{\\min}}$\n",
    "- $z$ = zero-point (INT8 integer representing 0.0)\n",
    "\n",
    "For UINT8: $q_{\\min} = 0$, $q_{\\max} = 255$\n",
    "\n",
    "### Dequantization\n",
    "\n",
    "$$\n",
    "r = s \\cdot (q - z)\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "**Quantize ReLU activations (always ‚â• 0):**\n",
    "\n",
    "$$\n",
    "A = [0.0, 0.3, 0.6, 0.9, 1.2]\n",
    "$$\n",
    "\n",
    "**Step 1**: Find range\n",
    "$$\n",
    "r_{\\min} = 0.0, \\quad r_{\\max} = 1.2\n",
    "$$\n",
    "\n",
    "**Step 2**: Calculate scale and zero-point\n",
    "$$\n",
    "s = \\frac{1.2 - 0.0}{255 - 0} = \\frac{1.2}{255} = 0.00471\n",
    "$$\n",
    "\n",
    "$$\n",
    "z = \\text{round}\\left(-\\frac{r_{\\min}}{s}\\right) = \\text{round}\\left(-\\frac{0.0}{0.00471}\\right) = 0\n",
    "$$\n",
    "\n",
    "**Step 3**: Quantize\n",
    "$$\n",
    "q_i = \\text{round}\\left(\\frac{a_i}{s}\\right) + z\n",
    "$$\n",
    "\n",
    "$$\n",
    "q_1 = \\text{round}\\left(\\frac{0.0}{0.00471}\\right) + 0 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "q_2 = \\text{round}\\left(\\frac{0.3}{0.00471}\\right) + 0 = 64\n",
    "$$\n",
    "\n",
    "$$\n",
    "q_3 = \\text{round}\\left(\\frac{0.6}{0.00471}\\right) + 0 = 127\n",
    "$$\n",
    "\n",
    "$$\n",
    "q_4 = \\text{round}\\left(\\frac{0.9}{0.00471}\\right) + 0 = 191\n",
    "$$\n",
    "\n",
    "$$\n",
    "q_5 = \\text{round}\\left(\\frac{1.2}{0.00471}\\right) + 0 = 255\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q = [0, 64, 127, 191, 255] \\quad \\text{(UINT8)}\n",
    "$$\n",
    "\n",
    "**Full range utilized**: 0-255 ‚úÖ (vs symmetric would only use 0-127)\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3 Per-Channel Quantization\n",
    "\n",
    "**Problem**: Different channels have different value ranges\n",
    "\n",
    "**Example**: Conv layer with 3 output channels\n",
    "\n",
    "$$\n",
    "W_1 \\in [-0.1, 0.1], \\quad W_2 \\in [-0.5, 0.5], \\quad W_3 \\in [-1.0, 1.0]\n",
    "$$\n",
    "\n",
    "**Per-tensor quantization** (single scale for all channels):\n",
    "$$\n",
    "s = \\frac{1.0}{127} = 0.00787\n",
    "$$\n",
    "\n",
    "Channel 1 uses only [-13, 13] out of [-127, 127] ‚Üí **Wastes 90% of range** ‚ùå\n",
    "\n",
    "**Per-channel quantization** (separate scale per channel):\n",
    "$$\n",
    "s_1 = \\frac{0.1}{127} = 0.00079, \\quad s_2 = \\frac{0.5}{127} = 0.00394, \\quad s_3 = \\frac{1.0}{127} = 0.00787\n",
    "$$\n",
    "\n",
    "Each channel uses full [-127, 127] range ‚Üí **10√ó better precision** ‚úÖ\n",
    "\n",
    "### Formula\n",
    "\n",
    "For output channel $c$:\n",
    "$$\n",
    "q_{i,c} = \\text{round}\\left(\\frac{w_{i,c}}{s_c}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "s_c = \\frac{\\max(|W_c|)}{127}\n",
    "$$\n",
    "\n",
    "**Accuracy improvement**: Per-channel quantization typically recovers 1-2% accuracy vs per-tensor\n",
    "\n",
    "---\n",
    "\n",
    "## 1.4 Quantization-Aware Training (QAT)\n",
    "\n",
    "**Problem**: Post-training quantization (PTQ) loses 1-3% accuracy\n",
    "\n",
    "**Solution**: Train model with quantization simulation (fake quantization)\n",
    "\n",
    "### Fake Quantization\n",
    "\n",
    "During forward pass, simulate quantization:\n",
    "$$\n",
    "w_{\\text{fake}} = s \\cdot \\text{round}\\left(\\frac{w}{s}\\right)\n",
    "$$\n",
    "\n",
    "During backward pass, use straight-through estimator (STE):\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} \\approx \\frac{\\partial L}{\\partial w_{\\text{fake}}}\n",
    "$$\n",
    "\n",
    "**Effect**: Model learns to be robust to quantization noise\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "```\n",
    "# Pseudo-code for QAT\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        # Forward pass with fake quantization\n",
    "        weights_float = model.weights\n",
    "        weights_quantized = fake_quantize(weights_float)\n",
    "        \n",
    "        # Compute loss with quantized weights\n",
    "        output = forward(input, weights_quantized)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass (gradients w.r.t. float weights)\n",
    "        loss.backward()  # STE: gradient flows through round()\n",
    "        \n",
    "        # Update float weights\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "**Accuracy improvement**: QAT typically recovers 0.5-1.5% accuracy vs PTQ\n",
    "\n",
    "---\n",
    "\n",
    "## 1.5 Quantization for Different Layers\n",
    "\n",
    "### Convolution Layer\n",
    "\n",
    "**Operation**: $Y = W * X + b$\n",
    "\n",
    "**Quantization**:\n",
    "- **Weights**: INT8 symmetric, per-channel\n",
    "- **Activations**: UINT8 asymmetric (ReLU outputs)\n",
    "- **Bias**: INT32 (higher precision to avoid accumulation error)\n",
    "\n",
    "**Quantized operation**:\n",
    "$$\n",
    "Y_q = \\text{round}\\left(\\frac{1}{s_y}\\left(s_w \\cdot s_x \\cdot (W_q * X_q) + b\\right)\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $W_q$, $X_q$, $Y_q$ are INT8 tensors\n",
    "- $s_w$, $s_x$, $s_y$ are scale factors\n",
    "- Multiplication $s_w \\cdot s_x$ is folded into runtime\n",
    "\n",
    "### Fully Connected Layer\n",
    "\n",
    "**Operation**: $Y = WX + b$\n",
    "\n",
    "**Same quantization as convolution**\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "**Problem**: BN requires float32 mean/variance (expensive)\n",
    "\n",
    "**Solution**: Fuse BN into previous conv layer\n",
    "\n",
    "$$\n",
    "Y = \\text{Conv}(X, W, b) \\rightarrow Y' = \\gamma \\frac{Y - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "**Fused operation**:\n",
    "$$\n",
    "Y' = \\text{Conv}\\left(X, \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}} W, \\frac{\\gamma(b - \\mu)}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\\right)\n",
    "$$\n",
    "\n",
    "**Result**: No runtime BN computation, just conv with modified weights/bias ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "## 1.6 Quantization Accuracy Analysis\n",
    "\n",
    "### Theoretical Error Bound\n",
    "\n",
    "**Quantization error per weight**:\n",
    "$$\n",
    "\\epsilon = \\left|w - s \\cdot \\text{round}\\left(\\frac{w}{s}\\right)\\right| \\leq \\frac{s}{2}\n",
    "$$\n",
    "\n",
    "**For symmetric INT8** ($s = \\frac{\\alpha}{127}$):\n",
    "$$\n",
    "\\epsilon \\leq \\frac{\\alpha}{254}\n",
    "$$\n",
    "\n",
    "**Example**: $\\alpha = 1.0$\n",
    "$$\n",
    "\\epsilon \\leq \\frac{1.0}{254} = 0.0039 \\quad \\text{(0.39% max error per weight)}\n",
    "$$\n",
    "\n",
    "### Layer-wise Error Propagation\n",
    "\n",
    "For $L$-layer network:\n",
    "$$\n",
    "\\text{Total error} \\approx \\sum_{l=1}^{L} \\epsilon_l \\cdot \\left|\\frac{\\partial \\text{loss}}{\\partial w_l}\\right|\n",
    "$$\n",
    "\n",
    "**Typical results** (image classification):\n",
    "- **1 layer**: 0.1% accuracy loss\n",
    "- **10 layers**: 1.0% accuracy loss (errors compound)\n",
    "- **50 layers**: 3-5% accuracy loss (deep networks more sensitive)\n",
    "\n",
    "**Mitigation**: Use mixed precision (sensitive layers in float32, others in INT8)\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Pruning: Remove Unimportant Weights\n",
    "\n",
    "## What is Pruning?\n",
    "\n",
    "**Definition**: Set unimportant weights to zero to reduce model size and computation\n",
    "\n",
    "**Motivation**:\n",
    "- **Sparsity**: 50-90% of weights can be pruned with <3% accuracy loss\n",
    "- **Lottery ticket hypothesis** (Frankle & Carbin 2019): Sparse subnetworks exist that match full network performance\n",
    "\n",
    "---\n",
    "\n",
    "## 2.1 Magnitude-based Pruning\n",
    "\n",
    "**Idea**: Prune weights with smallest absolute values (least important)\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "**Step 1**: Compute importance score for each weight\n",
    "$$\n",
    "I_i = |w_i|\n",
    "$$\n",
    "\n",
    "**Step 2**: Sort weights by importance\n",
    "$$\n",
    "|w_{(1)}| \\leq |w_{(2)}| \\leq \\cdots \\leq |w_{(n)}|\n",
    "$$\n",
    "\n",
    "**Step 3**: Prune bottom $p\\%$ of weights\n",
    "$$\n",
    "w_i = \\begin{cases}\n",
    "w_i & \\text{if } |w_i| > \\text{threshold} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where threshold is $(1-p)$-th percentile of $|w_i|$\n",
    "\n",
    "### Example\n",
    "\n",
    "**Original weights**:\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "0.8 & -0.1 & 0.3 \\\\\n",
    "-0.05 & 0.9 & -0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Sorted by magnitude**:\n",
    "$$\n",
    "|-0.05| < |-0.1| < |-0.2| < |0.3| < |0.8| < |0.9|\n",
    "$$\n",
    "\n",
    "**Prune 50%** (bottom 3 weights):\n",
    "$$\n",
    "W_{\\text{pruned}} = \\begin{bmatrix}\n",
    "0.8 & 0 & 0.3 \\\\\n",
    "0 & 0.9 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Sparsity**: 3/6 = 50% weights pruned ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 Structured Pruning\n",
    "\n",
    "**Problem**: Unstructured (magnitude-based) pruning creates irregular sparsity ‚Üí Hard to accelerate on hardware\n",
    "\n",
    "**Solution**: Prune entire channels, filters, or layers (regular sparsity)\n",
    "\n",
    "### Channel Pruning\n",
    "\n",
    "**Idea**: Remove entire output channels from conv layer\n",
    "\n",
    "**Importance score** for channel $c$:\n",
    "$$\n",
    "I_c = \\sum_{i,j,k} |w_{i,j,k,c}| \\quad \\text{(sum of absolute weights in channel)}\n",
    "$$\n",
    "\n",
    "**Prune channels with lowest** $I_c$\n",
    "\n",
    "**Example**: Conv layer (64 channels ‚Üí 32 channels)\n",
    "- **Original**: 3√ó3√ó3√ó64 = 1,728 parameters\n",
    "- **After pruning**: 3√ó3√ó3√ó32 = 864 parameters (50% reduction)\n",
    "- **Speedup**: 2√ó faster (regular sparsity, hardware-friendly) ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "## 2.3 Iterative Pruning with Fine-tuning\n",
    "\n",
    "**Problem**: Pruning 90% in one shot loses >10% accuracy ‚ùå\n",
    "\n",
    "**Solution**: Prune gradually (10-20% per iteration) + fine-tune\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "```\n",
    "# Pseudo-code for iterative pruning\n",
    "model = train_model()  # Train baseline\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # Prune 20% of remaining weights\n",
    "    prune_percentage = 0.2\n",
    "    prune_weights(model, prune_percentage)\n",
    "    \n",
    "    # Fine-tune for 10 epochs\n",
    "    fine_tune(model, epochs=10)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = evaluate(model, test_loader)\n",
    "    print(f\"Iteration {iteration}, Sparsity: {get_sparsity(model):.0%}, Acc: {accuracy:.2%}\")\n",
    "```\n",
    "\n",
    "**Typical results** (ResNet-50 on ImageNet):\n",
    "- **Baseline**: 0% sparsity, 76.1% accuracy\n",
    "- **Iteration 1**: 20% sparsity, 75.8% accuracy (0.3% loss)\n",
    "- **Iteration 2**: 36% sparsity, 75.3% accuracy (0.8% loss)\n",
    "- **Iteration 3**: 49% sparsity, 74.5% accuracy (1.6% loss)\n",
    "- **Iteration 4**: 59% sparsity, 73.2% accuracy (2.9% loss)\n",
    "- **Iteration 5**: 67% sparsity, 71.5% accuracy (4.6% loss) ‚ö†Ô∏è\n",
    "\n",
    "**Optimal**: 50-60% sparsity (2-3% accuracy loss, acceptable)\n",
    "\n",
    "---\n",
    "\n",
    "## 2.4 Lottery Ticket Hypothesis\n",
    "\n",
    "**Discovery** (Frankle & Carbin 2019): Dense networks contain sparse \"winning tickets\" that can match full accuracy\n",
    "\n",
    "### Finding Winning Tickets\n",
    "\n",
    "**Algorithm**:\n",
    "1. **Train full network** to convergence\n",
    "2. **Prune** smallest-magnitude weights (e.g., 90%)\n",
    "3. **Reset** remaining weights to initial values\n",
    "4. **Retrain** sparse network from scratch\n",
    "\n",
    "**Result**: Sparse network matches or exceeds full network accuracy! üéØ\n",
    "\n",
    "**Why it works**: Initialization matters more than we thought (lucky lottery tickets exist)\n",
    "\n",
    "---\n",
    "\n",
    "## 2.5 Pruning + Quantization = Deep Compression\n",
    "\n",
    "**Deep Compression** (Han et al. 2016): Combine pruning + quantization + Huffman coding\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "**Step 1**: Magnitude-based pruning (10√ó reduction)\n",
    "- AlexNet: 61M ‚Üí 6.7M parameters (90% pruned)\n",
    "\n",
    "**Step 2**: Quantization (4√ó reduction)\n",
    "- INT8 per-channel quantization\n",
    "\n",
    "**Step 3**: Huffman coding (2√ó reduction)\n",
    "- Encode quantized values with variable-length codes\n",
    "\n",
    "**Total compression**: 10 √ó 4 √ó 2 = **80√ó compression** üöÄ\n",
    "\n",
    "**Example**: AlexNet\n",
    "- **Original**: 240MB (float32)\n",
    "- **After pruning**: 24MB (90% sparsity)\n",
    "- **After quantization**: 6MB (INT8)\n",
    "- **After Huffman**: 3MB (variable-length encoding)\n",
    "- **Final**: **240MB ‚Üí 3MB = 80√ó smaller** ‚úÖ\n",
    "\n",
    "**Accuracy**: 57.2% ‚Üí 57.0% (0.2% loss, negligible)\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Knowledge Distillation: Train Small Model to Mimic Large Model\n",
    "\n",
    "## What is Knowledge Distillation?\n",
    "\n",
    "**Definition**: Transfer knowledge from large \"teacher\" model to small \"student\" model\n",
    "\n",
    "**Motivation**:\n",
    "- **Teacher**: ResNet-50 (25MB, 98.5% accuracy) ‚Üí Too large for edge ‚ùå\n",
    "- **Student**: MobileNetV2 (5MB, 95.0% accuracy) ‚Üí Fits on edge but lower accuracy ‚ùå\n",
    "- **Distilled student**: MobileNetV2 (5MB, 97.2% accuracy) ‚Üí Learns from teacher ‚úÖ\n",
    "\n",
    "**Gain**: 2.2% accuracy improvement (95.0% ‚Üí 97.2%) from distillation\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Hinton's Distillation (2015)\n",
    "\n",
    "### Soft Targets\n",
    "\n",
    "**Problem**: Hard labels (one-hot) lose information\n",
    "\n",
    "**Example**: Dog vs Wolf classification\n",
    "- **Hard label**: Dog = [1, 0] (binary)\n",
    "- **Soft label**: Dog = [0.9, 0.1] (teacher thinks 10% wolf-like)\n",
    "\n",
    "**Soft label captures similarity** between classes (dog closer to wolf than to airplane)\n",
    "\n",
    "### Temperature Scaling\n",
    "\n",
    "**Standard softmax** (temperature T=1):\n",
    "$$\n",
    "p_i = \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)}\n",
    "$$\n",
    "\n",
    "**Softmax with temperature**:\n",
    "$$\n",
    "p_i^T = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}\n",
    "$$\n",
    "\n",
    "**Effect of temperature**:\n",
    "- **T = 1**: Standard softmax (sharp distribution)\n",
    "- **T = 5**: Softer distribution (reveals class similarities)\n",
    "- **T = 10**: Very soft (more information for student)\n",
    "\n",
    "**Example**: Logits $z = [5.0, 2.0, 1.0]$\n",
    "\n",
    "**T = 1** (standard):\n",
    "$$\n",
    "p = [0.935, 0.047, 0.017] \\quad \\text{(very confident)}\n",
    "$$\n",
    "\n",
    "**T = 5** (soft):\n",
    "$$\n",
    "p^5 = [0.585, 0.237, 0.178] \\quad \\text{(class 2 and 3 have meaningful probabilities)}\n",
    "$$\n",
    "\n",
    "### Distillation Loss\n",
    "\n",
    "**Total loss** (weighted combination):\n",
    "$$\n",
    "L = \\alpha \\cdot L_{\\text{hard}} + (1 - \\alpha) \\cdot L_{\\text{soft}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **Hard loss**: Cross-entropy with true labels\n",
    "$$\n",
    "L_{\\text{hard}} = -\\sum_i y_i \\log(p_i^{\\text{student}})\n",
    "$$\n",
    "\n",
    "- **Soft loss**: KL divergence between teacher and student soft outputs\n",
    "$$\n",
    "L_{\\text{soft}} = T^2 \\cdot \\text{KL}\\left(p^{\\text{teacher}}_T \\parallel p^{\\text{student}}_T\\right)\n",
    "$$\n",
    "\n",
    "- **Œ±**: Weight (typically Œ±=0.1, more weight on soft loss)\n",
    "- **T¬≤**: Compensate for magnitude change with temperature\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "**Teacher outputs** (T=5):\n",
    "$$\n",
    "p^{\\text{teacher}}_5 = [0.585, 0.237, 0.178]\n",
    "$$\n",
    "\n",
    "**Student outputs** (T=5):\n",
    "$$\n",
    "p^{\\text{student}}_5 = [0.550, 0.300, 0.150]\n",
    "$$\n",
    "\n",
    "**KL divergence**:\n",
    "$$\n",
    "\\text{KL} = \\sum_i p_i^{\\text{teacher}} \\log\\left(\\frac{p_i^{\\text{teacher}}}{p_i^{\\text{student}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.585 \\log\\left(\\frac{0.585}{0.550}\\right) + 0.237 \\log\\left(\\frac{0.237}{0.300}\\right) + 0.178 \\log\\left(\\frac{0.178}{0.150}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.585 \\times 0.0614 + 0.237 \\times (-0.2364) + 0.178 \\times 0.1718\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.0359 - 0.0560 + 0.0306 = 0.0105\n",
    "$$\n",
    "\n",
    "**Soft loss**:\n",
    "$$\n",
    "L_{\\text{soft}} = T^2 \\cdot \\text{KL} = 5^2 \\times 0.0105 = 0.2625\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 Feature-based Distillation\n",
    "\n",
    "**Idea**: Match intermediate feature maps (not just final outputs)\n",
    "\n",
    "### FitNet (Romero et al. 2015)\n",
    "\n",
    "**Match intermediate layers**:\n",
    "$$\n",
    "L_{\\text{feature}} = \\frac{1}{2}\\left\\|F^{\\text{student}} - W_r F^{\\text{teacher}}\\right\\|^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $F^{\\text{teacher}}$ = Teacher's intermediate feature map (e.g., 512 channels)\n",
    "- $F^{\\text{student}}$ = Student's intermediate feature map (e.g., 256 channels)\n",
    "- $W_r$ = Linear projection matrix (512 ‚Üí 256 to match dimensions)\n",
    "\n",
    "**Effect**: Student learns better internal representations (not just final outputs)\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 Relation-based Distillation\n",
    "\n",
    "**Idea**: Match relationships between feature pairs (not absolute values)\n",
    "\n",
    "### RKD (Park et al. 2019)\n",
    "\n",
    "**Distance-wise relation**:\n",
    "$$\n",
    "L_{\\text{distance}} = \\sum_{i,j} \\left\\|\\frac{d_{ij}^{\\text{student}}}{\\mu_s} - \\frac{d_{ij}^{\\text{teacher}}}{\\mu_t}\\right\\|^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $d_{ij} = \\|f_i - f_j\\|$ (distance between features of samples i and j)\n",
    "- $\\mu$ = mean distance (normalization)\n",
    "\n",
    "**Angle-wise relation**:\n",
    "$$\n",
    "L_{\\text{angle}} = \\sum_{i,j,k} \\left|\\cos\\theta_{ijk}^{\\text{student}} - \\cos\\theta_{ijk}^{\\text{teacher}}\\right|^2\n",
    "$$\n",
    "\n",
    "**Effect**: Student learns geometric structure of feature space (robust to feature magnitude differences)\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 Self-Distillation\n",
    "\n",
    "**Idea**: No external teacher, use model's own predictions\n",
    "\n",
    "### Born-Again Networks (Furlanello et al. 2018)\n",
    "\n",
    "**Algorithm**:\n",
    "1. Train model M1 (student becomes teacher)\n",
    "2. Train model M2 using M1 as teacher\n",
    "3. Train model M3 using M2 as teacher\n",
    "4. ...\n",
    "\n",
    "**Result**: M2 > M1, M3 > M2 (accuracy improves through self-distillation!)\n",
    "\n",
    "**Why it works**: Ensemble effect (each generation learns from predecessor's mistakes)\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Efficiency Metrics for Edge AI\n",
    "\n",
    "## 4.1 Model Size\n",
    "\n",
    "**Total parameters**:\n",
    "$$\n",
    "\\text{Size} = \\sum_{\\text{layers}} n_{\\text{params}} \\times \\text{bits per param}\n",
    "$$\n",
    "\n",
    "**Example**: ResNet-50\n",
    "- **Parameters**: 25.6M\n",
    "- **Precision**: float32 (32 bits = 4 bytes)\n",
    "- **Size**: 25.6M √ó 4 = **102.4 MB**\n",
    "\n",
    "**After INT8 quantization**:\n",
    "- **Size**: 25.6M √ó 1 = **25.6 MB** (4√ó smaller)\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2 FLOPs (Floating-Point Operations)\n",
    "\n",
    "**Conv layer FLOPs**:\n",
    "$$\n",
    "\\text{FLOPs} = 2 \\times H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{in}} \\times C_{\\text{out}} \\times K \\times K\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $H_{\\text{out}}, W_{\\text{out}}$ = Output height/width\n",
    "- $C_{\\text{in}}, C_{\\text{out}}$ = Input/output channels\n",
    "- $K \\times K$ = Kernel size\n",
    "- Factor of 2 = Multiply + Add\n",
    "\n",
    "**Example**: Standard conv\n",
    "- Input: 56√ó56√ó64\n",
    "- Output: 56√ó56√ó128\n",
    "- Kernel: 3√ó3\n",
    "- **FLOPs**: $2 \\times 56 \\times 56 \\times 64 \\times 128 \\times 3 \\times 3 = 924M$ ‚ùå (expensive)\n",
    "\n",
    "**Depthwise separable conv** (MobileNet):\n",
    "- **Depthwise**: $2 \\times 56 \\times 56 \\times 64 \\times 1 \\times 3 \\times 3 = 3.6M$\n",
    "- **Pointwise**: $2 \\times 56 \\times 56 \\times 64 \\times 128 \\times 1 \\times 1 = 51M$\n",
    "- **Total**: 3.6M + 51M = **54.6M** ‚úÖ (17√ó fewer FLOPs)\n",
    "\n",
    "---\n",
    "\n",
    "## 4.3 Latency\n",
    "\n",
    "**Total latency**:\n",
    "$$\n",
    "\\text{Latency} = \\sum_{\\text{layers}} \\frac{\\text{FLOPs}}{\\text{Device throughput}} + \\text{Memory overhead}\n",
    "$$\n",
    "\n",
    "**Example**: ResNet-50 on different devices\n",
    "- **Cloud GPU** (NVIDIA V100, 125 TFLOPS): 4 billion FLOPs √∑ 125 TFLOPS = 32ms\n",
    "- **Edge GPU** (Jetson Nano, 472 GFLOPS): 4 billion FLOPs √∑ 472 GFLOPS = 8,500ms ‚ùå (too slow)\n",
    "- **Optimized mobile** (MobileNetV2, 300M FLOPs): 300M √∑ 472 GFLOPS = 635ms (better)\n",
    "\n",
    "**INT8 quantization speedup**: 2-4√ó ‚Üí 635ms √∑ 3 = **212ms** ‚úÖ (acceptable for mobile)\n",
    "\n",
    "---\n",
    "\n",
    "## 4.4 Energy Consumption\n",
    "\n",
    "**Energy per operation** (Horowitz 2014):\n",
    "- **INT8 ADD**: 0.03 pJ\n",
    "- **INT8 MULT**: 0.2 pJ\n",
    "- **FP32 ADD**: 0.9 pJ (30√ó more than INT8)\n",
    "- **FP32 MULT**: 3.7 pJ (18√ó more than INT8)\n",
    "- **DRAM access**: 640 pJ (3,200√ó more than INT8 mult!)\n",
    "\n",
    "**Insight**: Memory access dominates energy (not compute) ‚Üí Optimize for memory locality\n",
    "\n",
    "**Example**: 1 billion FP32 multiplications\n",
    "- **Energy**: 1B √ó 3.7 pJ = 3.7 mJ\n",
    "- **Battery**: 3.7Wh typical smartphone battery\n",
    "- **Battery drain**: 3.7 mJ √∑ 3.7 Wh = 0.0003% per inference (negligible)\n",
    "\n",
    "But DRAM access:\n",
    "- **Energy**: 1B √ó 640 pJ = 640 mJ (173√ó more!)\n",
    "- **Battery drain**: 0.05% per inference (significant for always-on applications)\n",
    "\n",
    "**Optimization**: Use on-chip SRAM (5 pJ, 128√ó less than DRAM)\n",
    "\n",
    "---\n",
    "\n",
    "## 4.5 MAC (Multiply-Accumulate) Operations\n",
    "\n",
    "**Definition**: Core operation in neural networks\n",
    "\n",
    "$$\n",
    "y = \\sum_{i=1}^{n} w_i x_i + b \\quad \\text{(n MACs + 1 ADD)}\n",
    "$$\n",
    "\n",
    "**Efficiency comparison**:\n",
    "\n",
    "| Device | MACs/second | Power | MACs/Watt |\n",
    "|--------|-------------|-------|-----------|\n",
    "| Cloud GPU (V100) | 125 TMAC | 300W | 417 GMAC/W |\n",
    "| Edge GPU (Jetson Nano) | 472 GMAC | 10W | 47 GMAC/W |\n",
    "| Mobile CPU (A16 Bionic) | 17 TMAC | 5W | 3.4 TMAC/W |\n",
    "| Microcontroller (Cortex-M4) | 10 MMAC | 0.01W | 1 GMAC/W |\n",
    "\n",
    "**Insight**: Cloud GPU has highest absolute performance, but mobile/edge have better energy efficiency\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Key Formulas Summary\n",
    "\n",
    "## Quantization\n",
    "\n",
    "**Symmetric**:\n",
    "$$\n",
    "q = \\text{round}\\left(\\frac{r}{s}\\right), \\quad s = \\frac{\\alpha}{127}\n",
    "$$\n",
    "\n",
    "**Asymmetric**:\n",
    "$$\n",
    "q = \\text{round}\\left(\\frac{r}{s}\\right) + z, \\quad s = \\frac{r_{\\max} - r_{\\min}}{255}\n",
    "$$\n",
    "\n",
    "**Dequantization**:\n",
    "$$\n",
    "r = s \\cdot (q - z)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Pruning\n",
    "\n",
    "**Magnitude importance**:\n",
    "$$\n",
    "I_i = |w_i|\n",
    "$$\n",
    "\n",
    "**Prune**:\n",
    "$$\n",
    "w_i = \\begin{cases}\n",
    "w_i & \\text{if } |w_i| > \\text{threshold} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Knowledge Distillation\n",
    "\n",
    "**Soft targets**:\n",
    "$$\n",
    "p_i^T = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}\n",
    "$$\n",
    "\n",
    "**Distillation loss**:\n",
    "$$\n",
    "L = \\alpha \\cdot L_{\\text{hard}} + (1 - \\alpha) \\cdot T^2 \\cdot \\text{KL}(p^{\\text{teacher}}_T \\parallel p^{\\text{student}}_T)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Efficiency Metrics\n",
    "\n",
    "**Model size** (MB):\n",
    "$$\n",
    "\\text{Size} = \\frac{\\text{Parameters} \\times \\text{Bits per param}}{8 \\times 10^6}\n",
    "$$\n",
    "\n",
    "**FLOPs** (conv layer):\n",
    "$$\n",
    "\\text{FLOPs} = 2 \\times H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{in}} \\times C_{\\text{out}} \\times K^2\n",
    "$$\n",
    "\n",
    "**Latency** (ms):\n",
    "$$\n",
    "\\text{Latency} = \\frac{\\text{FLOPs}}{\\text{Device throughput}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# üìä Compression Comparison Table\n",
    "\n",
    "| Technique | Compression | Accuracy Loss | Speedup | Energy Saving |\n",
    "|-----------|-------------|---------------|---------|---------------|\n",
    "| **INT8 Quantization** | 4√ó | 0.5-2% | 2-4√ó | 5-10√ó |\n",
    "| **Pruning (50%)** | 2√ó | 1-3% | 1.5√ó | 2√ó |\n",
    "| **Pruning (90%)** | 10√ó | 3-8% | 3√ó | 5√ó |\n",
    "| **Knowledge Distillation** | 10-100√ó | 2-5% | 10-50√ó | 20-100√ó |\n",
    "| **Combined (all)** | 50-500√ó | 3-10% | 20-100√ó | 50-200√ó |\n",
    "\n",
    "**Optimal strategy**: Combine all three techniques for maximum compression\n",
    "\n",
    "**Example**: ResNet-50 ‚Üí MobileNetV2\n",
    "- **Original**: 25MB, 4B FLOPs, 98.5% accuracy\n",
    "- **Distillation**: 5MB (5√ó smaller), 300M FLOPs (13√ó fewer), 96.0% accuracy\n",
    "- **+ Quantization**: 1.25MB (20√ó smaller), 150M FLOPs (27√ó fewer), 95.5% accuracy\n",
    "- **+ Pruning**: 0.5MB (50√ó smaller), 75M FLOPs (53√ó fewer), 94.2% accuracy\n",
    "\n",
    "**Final**: **50√ó compression, 53√ó speedup, 4.3% accuracy loss** ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "# üéì Takeaways\n",
    "\n",
    "1. **Quantization (INT8)**: 4√ó smaller, 2-4√ó faster, <2% accuracy loss (essential for edge)\n",
    "2. **Pruning (50-90%)**: 2-10√ó smaller, 1.5-3√ó faster, 1-8% accuracy loss (hardware-dependent)\n",
    "3. **Knowledge Distillation**: 10-100√ó smaller, 10-50√ó faster, 2-5% accuracy loss (train small model)\n",
    "4. **Combined**: 50-500√ó compression possible with 3-10% accuracy loss (production edge AI)\n",
    "5. **Energy is key**: DRAM access 100√ó more expensive than compute ‚Üí Optimize memory first\n",
    "\n",
    "**Next**: Implementation (TensorFlow Lite, TFLite Micro, Arduino, Jetson deployment)\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Mathematical foundations complete! Next: Production implementation and deployment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf07431",
   "metadata": {},
   "source": [
    "### üìù Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Edge AI & TinyML Implementation\n",
    "# Complete production-ready code for deploying ML models on edge devices\n",
    "# ===========================\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "# ===========================\n",
    "# Part 1: INT8 Quantization Implementation\n",
    "# ===========================\n",
    "class SymmetricQuantizer:\n",
    "    \"\"\"\n",
    "    Symmetric INT8 quantization: Maps [-Œ±, Œ±] ‚Üí [-127, 127]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.scale = None\n",
    "        self.alpha = None\n",
    "    \n",
    "    def calibrate(self, weights):\n",
    "        \"\"\"\n",
    "        Calibrate quantization parameters from weights\n",
    "        \n",
    "        Args:\n",
    "            weights: numpy array of float32 weights\n",
    "        \"\"\"\n",
    "        self.alpha = np.max(np.abs(weights))\n",
    "        self.scale = self.alpha / 127.0\n",
    "    \n",
    "    def quantize(self, weights):\n",
    "        \"\"\"\n",
    "        Quantize float32 weights to INT8\n",
    "        \n",
    "        Args:\n",
    "            weights: numpy array of float32 weights\n",
    "        \n",
    "        Returns:\n",
    "            quantized: INT8 weights\n",
    "        \"\"\"\n",
    "        if self.scale is None:\n",
    "            self.calibrate(weights)\n",
    "        \n",
    "        quantized = np.round(weights / self.scale)\n",
    "        quantized = np.clip(quantized, -127, 127).astype(np.int8)\n",
    "        return quantized\n",
    "    \n",
    "    def dequantize(self, quantized):\n",
    "        \"\"\"\n",
    "        Dequantize INT8 weights back to float32\n",
    "        \n",
    "        Args:\n",
    "            quantized: INT8 weights\n",
    "        \n",
    "        Returns:\n",
    "            dequantized: float32 weights (approximately original)\n",
    "        \"\"\"\n",
    "        return self.scale * quantized.astype(np.float32)\n",
    "class AsymmetricQuantizer:\n",
    "    \"\"\"\n",
    "    Asymmetric UINT8 quantization: Maps [min, max] ‚Üí [0, 255]\n",
    "    Used for activations (e.g., ReLU outputs always non-negative)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.scale = None\n",
    "        self.zero_point = None\n",
    "        self.r_min = None\n",
    "        self.r_max = None\n",
    "    \n",
    "    def calibrate(self, activations):\n",
    "        \"\"\"\n",
    "        Calibrate quantization parameters from activations\n",
    "        \n",
    "        Args:\n",
    "            activations: numpy array of float32 activations\n",
    "        \"\"\"\n",
    "        self.r_min = np.min(activations)\n",
    "        self.r_max = np.max(activations)\n",
    "        \n",
    "        # Ensure range includes 0\n",
    "        self.r_min = min(self.r_min, 0.0)\n",
    "        self.r_max = max(self.r_max, 0.0)\n",
    "        \n",
    "        self.scale = (self.r_max - self.r_min) / 255.0\n",
    "        self.zero_point = int(np.round(-self.r_min / self.scale))\n",
    "        self.zero_point = np.clip(self.zero_point, 0, 255)\n",
    "    \n",
    "    def quantize(self, activations):\n",
    "        \"\"\"\n",
    "        Quantize float32 activations to UINT8\n",
    "        \n",
    "        Args:\n",
    "            activations: numpy array of float32 activations\n",
    "        \n",
    "        Returns:\n",
    "            quantized: UINT8 activations\n",
    "        \"\"\"\n",
    "        if self.scale is None:\n",
    "            self.calibrate(activations)\n",
    "        \n",
    "        quantized = np.round(activations / self.scale) + self.zero_point\n",
    "        quantized = np.clip(quantized, 0, 255).astype(np.uint8)\n",
    "        return quantized\n",
    "    \n",
    "    def dequantize(self, quantized):\n",
    "        \"\"\"\n",
    "        Dequantize UINT8 activations back to float32\n",
    "        \n",
    "        Args:\n",
    "            quantized: UINT8 activations\n",
    "        \n",
    "        Returns:\n",
    "            dequantized: float32 activations (approximately original)\n",
    "        \"\"\"\n",
    "        return self.scale * (quantized.astype(np.float32) - self.zero_point)\n",
    "# Demo: Quantization accuracy\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Demo 1: INT8 Quantization\")\n",
    "print(\"=\"*60)\n",
    "# Simulate weight matrix\n",
    "weights = np.random.randn(100, 100).astype(np.float32) * 0.5\n",
    "# Symmetric quantization\n",
    "quantizer = SymmetricQuantizer()\n",
    "quantized_weights = quantizer.quantize(weights)\n",
    "dequantized_weights = quantizer.dequantize(quantized_weights)\n",
    "# Calculate error\n",
    "mse = np.mean((weights - dequantized_weights) ** 2)\n",
    "relative_error = np.mean(np.abs(weights - dequantized_weights) / (np.abs(weights) + 1e-8)) * 100\n",
    "print(f\"Original weights: mean={weights.mean():.4f}, std={weights.std():.4f}, range=[{weights.min():.4f}, {weights.max():.4f}]\")\n",
    "print(f\"Quantized weights: min={quantized_weights.min()}, max={quantized_weights.max()}\")\n",
    "print(f\"Quantization scale: {quantizer.scale:.6f}\")\n",
    "print(f\"Mean Squared Error: {mse:.6f}\")\n",
    "print(f\"Relative Error: {relative_error:.2f}%\")\n",
    "print(f\"Compression: float32 (4 bytes) ‚Üí INT8 (1 byte) = 4√ó smaller\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5355da8",
   "metadata": {},
   "source": [
    "### üìù Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c27b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Part 2: Build and Train Mobile Model\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Part 2: Train MobileNetV2-style Model\")\n",
    "print(\"=\"*60)\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "# Normalize\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "# One-hot encode labels\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "print(f\"Training data: {x_train.shape}, Labels: {y_train.shape}\")\n",
    "print(f\"Test data: {x_test.shape}, Labels: {y_test.shape}\")\n",
    "def build_mobile_model(input_shape=(32, 32, 3), num_classes=10):\n",
    "    \"\"\"\n",
    "    Build lightweight mobile-friendly CNN (inspired by MobileNetV2)\n",
    "    \n",
    "    Uses depthwise separable convolutions for efficiency:\n",
    "    - Depthwise conv: 3√ó3 spatial filtering per channel\n",
    "    - Pointwise conv: 1√ó1 conv to mix channels\n",
    "    \n",
    "    Efficiency: 9√ó fewer parameters than standard conv\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Initial conv\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Depthwise separable block 1\n",
    "    x = layers.DepthwiseConv2D(3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(64, 1, padding='same')(x)  # Pointwise\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(2)(x)  # 32√ó32 ‚Üí 16√ó16\n",
    "    \n",
    "    # Depthwise separable block 2\n",
    "    x = layers.DepthwiseConv2D(3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(128, 1, padding='same')(x)  # Pointwise\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(2)(x)  # 16√ó16 ‚Üí 8√ó8\n",
    "    \n",
    "    # Depthwise separable block 3\n",
    "    x = layers.DepthwiseConv2D(3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(256, 1, padding='same')(x)  # Pointwise\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    # Global average pooling\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Classifier\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs, name='mobile_cnn')\n",
    "    return model\n",
    "# Build model\n",
    "model = build_mobile_model()\n",
    "model.summary()\n",
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Model size (float32): {total_params * 4 / 1e6:.2f} MB\")\n",
    "print(f\"Model size (INT8): {total_params / 1e6:.2f} MB (after quantization)\")\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# Train (quick training for demo, only 5 epochs)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training mobile model (5 epochs for demo)...\")\n",
    "print(\"=\"*60)\n",
    "history = model.fit(\n",
    "    x_train[:10000], y_train[:10000],  # Use subset for faster demo\n",
    "    batch_size=128,\n",
    "    epochs=5,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"\\nTest accuracy (float32 model): {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a464983b",
   "metadata": {},
   "source": [
    "### üìù Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c414235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Part 3: Post-Training Quantization (TensorFlow Lite)\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Part 3: Post-Training Quantization (PTQ)\")\n",
    "print(\"=\"*60)\n",
    "# Convert to TensorFlow Lite (float32 baseline)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model_float = converter.convert()\n",
    "# Save float32 model\n",
    "with open('/tmp/mobile_model_float32.tflite', 'wb') as f:\n",
    "    f.write(tflite_model_float)\n",
    "float_size = len(tflite_model_float)\n",
    "print(f\"Float32 TFLite model size: {float_size / 1e6:.2f} MB\")\n",
    "# Convert to INT8 (post-training quantization)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Representative dataset for calibration\n",
    "def representative_dataset():\n",
    "    \"\"\"\n",
    "    Provide representative data for quantization calibration\n",
    "    \"\"\"\n",
    "    for i in range(100):\n",
    "        yield [x_train[i:i+1]]\n",
    "converter.representative_dataset = representative_dataset\n",
    "# Full integer quantization (INT8 weights + INT8 activations)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "tflite_model_int8 = converter.convert()\n",
    "# Save INT8 model\n",
    "with open('/tmp/mobile_model_int8.tflite', 'wb') as f:\n",
    "    f.write(tflite_model_int8)\n",
    "int8_size = len(tflite_model_int8)\n",
    "print(f\"INT8 TFLite model size: {int8_size / 1e6:.2f} MB\")\n",
    "print(f\"Compression ratio: {float_size / int8_size:.2f}√ó\")\n",
    "# Evaluate INT8 model\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_int8)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "# Test on subset (TFLite inference is slower in Python)\n",
    "num_test_samples = 100\n",
    "correct = 0\n",
    "for i in range(num_test_samples):\n",
    "    # Prepare input (UINT8)\n",
    "    input_data = (x_test[i:i+1] * 255).astype(np.uint8)\n",
    "    \n",
    "    # Run inference\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get output (UINT8)\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    # Dequantize output\n",
    "    output_scale = output_details[0]['quantization'][0]\n",
    "    output_zero_point = output_details[0]['quantization'][1]\n",
    "    output_float = output_scale * (output_data.astype(np.float32) - output_zero_point)\n",
    "    \n",
    "    # Predict\n",
    "    pred = np.argmax(output_float)\n",
    "    true = np.argmax(y_test[i])\n",
    "    \n",
    "    if pred == true:\n",
    "        correct += 1\n",
    "int8_accuracy = correct / num_test_samples\n",
    "print(f\"\\nTest accuracy (INT8 model): {int8_accuracy:.4f}\")\n",
    "print(f\"Accuracy loss from quantization: {(test_acc - int8_accuracy) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0204c172",
   "metadata": {},
   "source": [
    "### üìù Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Part 4: Magnitude-based Pruning\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Part 4: Magnitude-based Pruning\")\n",
    "print(\"=\"*60)\n",
    "def prune_weights(weights, sparsity=0.5):\n",
    "    \"\"\"\n",
    "    Prune weights by magnitude (set smallest |w| to zero)\n",
    "    \n",
    "    Args:\n",
    "        weights: numpy array\n",
    "        sparsity: fraction of weights to prune (0.5 = 50%)\n",
    "    \n",
    "    Returns:\n",
    "        pruned_weights: weights with smallest values set to zero\n",
    "    \"\"\"\n",
    "    threshold = np.percentile(np.abs(weights), sparsity * 100)\n",
    "    mask = np.abs(weights) > threshold\n",
    "    pruned_weights = weights * mask\n",
    "    return pruned_weights\n",
    "# Get first conv layer weights\n",
    "first_conv_weights = model.layers[1].get_weights()[0]  # Shape: (3, 3, 3, 32)\n",
    "print(f\"Original weights shape: {first_conv_weights.shape}\")\n",
    "print(f\"Original non-zero weights: {np.count_nonzero(first_conv_weights)}\")\n",
    "# Prune 50%\n",
    "pruned_weights = prune_weights(first_conv_weights, sparsity=0.5)\n",
    "print(f\"After 50% pruning: {np.count_nonzero(pruned_weights)} non-zero weights\")\n",
    "print(f\"Actual sparsity: {1 - np.count_nonzero(pruned_weights) / first_conv_weights.size:.2%}\")\n",
    "# Visualize pruning effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].hist(first_conv_weights.flatten(), bins=50, alpha=0.7)\n",
    "axes[0].set_title('Original Weight Distribution')\n",
    "axes[0].set_xlabel('Weight value')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(0, color='red', linestyle='--', label='Zero')\n",
    "axes[0].legend()\n",
    "axes[1].hist(pruned_weights.flatten(), bins=50, alpha=0.7)\n",
    "axes[1].set_title('Pruned Weight Distribution (50% sparsity)')\n",
    "axes[1].set_xlabel('Weight value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(0, color='red', linestyle='--', label='Zero (many)')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/pruning_demo.png', dpi=150, bbox_inches='tight')\n",
    "print(\"Saved pruning visualization to /tmp/pruning_demo.png\")\n",
    "# ===========================\n",
    "# Part 5: Knowledge Distillation\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Part 5: Knowledge Distillation\")\n",
    "print(\"=\"*60)\n",
    "class Distiller(keras.Model):\n",
    "    \"\"\"\n",
    "    Knowledge distillation wrapper\n",
    "    \n",
    "    Trains student model to match teacher's soft targets\n",
    "    \"\"\"\n",
    "    def __init__(self, student, teacher, temperature=5.0, alpha=0.1):\n",
    "        super().__init__()\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def compile(self, optimizer, metrics):\n",
    "        super().compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_tracker = keras.metrics.Mean(name=\"student_loss\")\n",
    "        self.distillation_loss_tracker = keras.metrics.Mean(name=\"distillation_loss\")\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.student_loss_tracker, self.distillation_loss_tracker]\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        \n",
    "        # Teacher predictions (soft targets)\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Student predictions\n",
    "            student_predictions = self.student(x, training=True)\n",
    "            \n",
    "            # Hard loss (cross-entropy with true labels)\n",
    "            student_loss = keras.losses.categorical_crossentropy(y, student_predictions)\n",
    "            \n",
    "            # Soft loss (KL divergence with teacher)\n",
    "            # Apply temperature scaling\n",
    "            teacher_soft = tf.nn.softmax(teacher_predictions / self.temperature)\n",
    "            student_soft = tf.nn.softmax(student_predictions / self.temperature)\n",
    "            \n",
    "            distillation_loss = keras.losses.kl_divergence(teacher_soft, student_soft)\n",
    "            \n",
    "            # Combined loss (weighted)\n",
    "            # Note: temperature^2 scales KL divergence magnitude\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss * (self.temperature ** 2)\n",
    "        \n",
    "        # Update student weights\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        # Update metrics\n",
    "        self.student_loss_tracker.update_state(student_loss)\n",
    "        self.distillation_loss_tracker.update_state(distillation_loss)\n",
    "        \n",
    "        return {\n",
    "            \"student_loss\": self.student_loss_tracker.result(),\n",
    "            \"distillation_loss\": self.distillation_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        \n",
    "        # Student predictions\n",
    "        student_predictions = self.student(x, training=False)\n",
    "        \n",
    "        # Hard loss\n",
    "        student_loss = keras.losses.categorical_crossentropy(y, student_predictions)\n",
    "        \n",
    "        # Update metrics\n",
    "        self.student_loss_tracker.update_state(student_loss)\n",
    "        \n",
    "        return {\"student_loss\": self.student_loss_tracker.result()}\n",
    "# Create teacher (larger model, assume already trained)\n",
    "teacher = model  # Use our trained model as teacher\n",
    "# Create student (smaller model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a56f3b",
   "metadata": {},
   "source": [
    "### üìù Function: build_small_student\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c2355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_small_student(input_shape=(32, 32, 3), num_classes=10):\n",
    "    \"\"\"\n",
    "    Build smaller student model (half the channels of teacher)\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv2D(16, 3, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.DepthwiseConv2D(3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(32, 1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    \n",
    "    x = layers.DepthwiseConv2D(3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(64, 1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs, name='student_cnn')\n",
    "    return model\n",
    "student = build_small_student()\n",
    "student_params = student.count_params()\n",
    "teacher_params = teacher.count_params()\n",
    "print(f\"Teacher parameters: {teacher_params:,}\")\n",
    "print(f\"Student parameters: {student_params:,}\")\n",
    "print(f\"Compression: {teacher_params / student_params:.2f}√ó fewer parameters\")\n",
    "# Create distiller\n",
    "distiller = Distiller(student=student, teacher=teacher, temperature=5.0, alpha=0.1)\n",
    "distiller.compile(\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# Train student with distillation (quick demo)\n",
    "print(\"\\nTraining student with knowledge distillation...\")\n",
    "distiller.fit(\n",
    "    x_train[:5000], y_train[:5000],\n",
    "    batch_size=128,\n",
    "    epochs=3,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "# Evaluate student\n",
    "student_loss, student_acc = student.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"\\nTeacher accuracy: {test_acc:.4f}\")\n",
    "print(f\"Student accuracy (with distillation): {student_acc:.4f}\")\n",
    "print(f\"Accuracy gap: {(test_acc - student_acc) * 100:.2f}%\")\n",
    "# ===========================\n",
    "# Part 6: TensorFlow Lite Micro (Arduino Deployment)\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Part 6: TensorFlow Lite Micro (TinyML)\")\n",
    "print(\"=\"*60)\n",
    "# Convert to TFLite Micro-compatible format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(student)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_micro_model = converter.convert()\n",
    "# Save\n",
    "with open('/tmp/student_tflite_micro.tflite', 'wb') as f:\n",
    "    f.write(tflite_micro_model)\n",
    "micro_size = len(tflite_micro_model)\n",
    "print(f\"TFLite Micro model size: {micro_size / 1024:.2f} KB\")\n",
    "# Check if fits on Arduino Nano 33 BLE (256KB RAM)\n",
    "arduino_ram = 256 * 1024\n",
    "if micro_size < arduino_ram:\n",
    "    print(f\"‚úÖ Fits on Arduino Nano 33 BLE (256KB RAM)\")\n",
    "    print(f\"   Remaining RAM: {(arduino_ram - micro_size) / 1024:.2f} KB\")\n",
    "else:\n",
    "    print(f\"‚ùå Too large for Arduino Nano 33 BLE\")\n",
    "    print(f\"   Need to reduce by: {(micro_size - arduino_ram) / 1024:.2f} KB\")\n",
    "# Generate C array for Arduino\n",
    "print(\"\\nGenerating C array for Arduino deployment...\")\n",
    "with open('/tmp/model.h', 'w') as f:\n",
    "    f.write(\"// Generated model for Arduino deployment\\n\")\n",
    "    f.write(\"// Include this file in your Arduino sketch\\n\\n\")\n",
    "    f.write(\"#ifndef MODEL_H\\n\")\n",
    "    f.write(\"#define MODEL_H\\n\\n\")\n",
    "    f.write(f\"const unsigned int model_size = {micro_size};\\n\")\n",
    "    f.write(\"const unsigned char model_data[] = {\\n\")\n",
    "    \n",
    "    # Convert bytes to C array\n",
    "    for i, byte in enumerate(tflite_micro_model):\n",
    "        if i % 12 == 0:\n",
    "            f.write(\"  \")\n",
    "        f.write(f\"0x{byte:02x}\")\n",
    "        if i < len(tflite_micro_model) - 1:\n",
    "            f.write(\", \")\n",
    "        if (i + 1) % 12 == 0:\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"\\n};\\n\\n\")\n",
    "    f.write(\"#endif  // MODEL_H\\n\")\n",
    "print(\"Saved Arduino header to /tmp/model.h\")\n",
    "# Generate Arduino sketch template\n",
    "arduino_sketch = \"\"\"\n",
    "// TensorFlow Lite Micro Arduino Example\n",
    "// Deploy quantized model on microcontroller\n",
    "#include <TensorFlowLite.h>\n",
    "#include \"model.h\"\n",
    "// TFLite globals\n",
    "namespace {\n",
    "  const tflite::Model* model = nullptr;\n",
    "  tflite::MicroInterpreter* interpreter = nullptr;\n",
    "  TfLiteTensor* input = nullptr;\n",
    "  TfLiteTensor* output = nullptr;\n",
    "  \n",
    "  // Tensor arena (adjust size based on model)\n",
    "  constexpr int kTensorArenaSize = 60 * 1024;  // 60KB\n",
    "  uint8_t tensor_arena[kTensorArenaSize];\n",
    "}\n",
    "void setup() {\n",
    "  Serial.begin(115200);\n",
    "  while (!Serial) {}\n",
    "  \n",
    "  // Load model\n",
    "  model = tflite::GetModel(model_data);\n",
    "  if (model->version() != TFLITE_SCHEMA_VERSION) {\n",
    "    Serial.println(\"Model schema mismatch!\");\n",
    "    return;\n",
    "  }\n",
    "  \n",
    "  // Set up interpreter\n",
    "  static tflite::MicroMutableOpResolver<5> resolver;\n",
    "  resolver.AddConv2D();\n",
    "  resolver.AddDepthwiseConv2D();\n",
    "  resolver.AddFullyConnected();\n",
    "  resolver.AddReshape();\n",
    "  resolver.AddSoftmax();\n",
    "  \n",
    "  static tflite::MicroInterpreter static_interpreter(\n",
    "      model, resolver, tensor_arena, kTensorArenaSize);\n",
    "  interpreter = &static_interpreter;\n",
    "  \n",
    "  // Allocate tensors\n",
    "  TfLiteStatus allocate_status = interpreter->AllocateTensors();\n",
    "  if (allocate_status != kTfLiteOk) {\n",
    "    Serial.println(\"AllocateTensors() failed!\");\n",
    "    return;\n",
    "  }\n",
    "  \n",
    "  // Get input/output tensors\n",
    "  input = interpreter->input(0);\n",
    "  output = interpreter->output(0);\n",
    "  \n",
    "  Serial.println(\"Model loaded successfully!\");\n",
    "  Serial.print(\"Input shape: \");\n",
    "  Serial.print(input->dims->data[1]);\n",
    "  Serial.print(\"x\");\n",
    "  Serial.println(input->dims->data[2]);\n",
    "}\n",
    "void loop() {\n",
    "  // Read sensor data (e.g., camera, microphone)\n",
    "  // For demo, use dummy data\n",
    "  for (int i = 0; i < input->bytes; i++) {\n",
    "    input->data.uint8[i] = random(0, 256);\n",
    "  }\n",
    "  \n",
    "  // Run inference\n",
    "  unsigned long start = micros();\n",
    "  TfLiteStatus invoke_status = interpreter->Invoke();\n",
    "  unsigned long end = micros();\n",
    "  \n",
    "  if (invoke_status != kTfLiteOk) {\n",
    "    Serial.println(\"Invoke failed!\");\n",
    "    return;\n",
    "  }\n",
    "  \n",
    "  // Get results\n",
    "  int max_idx = 0;\n",
    "  uint8_t max_val = 0;\n",
    "  for (int i = 0; i < 10; i++) {\n",
    "    if (output->data.uint8[i] > max_val) {\n",
    "      max_val = output->data.uint8[i];\n",
    "      max_idx = i;\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  Serial.print(\"Prediction: \");\n",
    "  Serial.print(max_idx);\n",
    "  Serial.print(\", Confidence: \");\n",
    "  Serial.print(max_val);\n",
    "  Serial.print(\", Latency: \");\n",
    "  Serial.print(end - start);\n",
    "  Serial.println(\" us\");\n",
    "  \n",
    "  delay(1000);  // Run once per second\n",
    "}\n",
    "\"\"\"\n",
    "with open('/tmp/tflite_micro_arduino.ino', 'w') as f:\n",
    "    f.write(arduino_sketch)\n",
    "print(\"Saved Arduino sketch template to /tmp/tflite_micro_arduino.ino\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d2cdf1",
   "metadata": {},
   "source": [
    "### üìù Implementation Part 6\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e795cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Part 7: Efficiency Analysis\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Part 7: Efficiency Analysis\")\n",
    "print(\"=\"*60)\n",
    "# Model comparison table\n",
    "models = {\n",
    "    'Teacher (float32)': {\n",
    "        'params': teacher_params,\n",
    "        'size_mb': teacher_params * 4 / 1e6,\n",
    "        'accuracy': test_acc,\n",
    "    },\n",
    "    'Teacher (INT8)': {\n",
    "        'params': teacher_params,\n",
    "        'size_mb': int8_size / 1e6,\n",
    "        'accuracy': int8_accuracy,\n",
    "    },\n",
    "    'Student (float32)': {\n",
    "        'params': student_params,\n",
    "        'size_mb': student_params * 4 / 1e6,\n",
    "        'accuracy': student_acc,\n",
    "    },\n",
    "    'Student (INT8)': {\n",
    "        'params': student_params,\n",
    "        'size_mb': micro_size / 1e6,\n",
    "        'accuracy': student_acc * 0.98,  # Estimate (slight loss from INT8)\n",
    "    }\n",
    "}\n",
    "print(f\"\\n{'Model':<20} {'Params':>12} {'Size (MB)':>12} {'Accuracy':>12} {'Compression':>12}\")\n",
    "print(\"-\" * 80)\n",
    "baseline_size = models['Teacher (float32)']['size_mb']\n",
    "baseline_acc = models['Teacher (float32)']['accuracy']\n",
    "for name, metrics in models.items():\n",
    "    compression = baseline_size / metrics['size_mb']\n",
    "    acc_loss = (baseline_acc - metrics['accuracy']) * 100\n",
    "    \n",
    "    print(f\"{name:<20} {metrics['params']:>12,} {metrics['size_mb']:>12.2f} \"\n",
    "          f\"{metrics['accuracy']:>12.4f} {compression:>11.1f}√ó\")\n",
    "# ===========================\n",
    "# Summary\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: Edge AI Model Optimization\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "‚úÖ Quantization (INT8):\n",
    "   - Size: {baseline_size:.2f}MB ‚Üí {int8_size/1e6:.2f}MB ({baseline_size/(int8_size/1e6):.1f}√ó smaller)\n",
    "   - Accuracy loss: {(test_acc - int8_accuracy)*100:.2f}%\n",
    "   - Speedup: 2-4√ó (on hardware with INT8 support)\n",
    "‚úÖ Knowledge Distillation:\n",
    "   - Size: {teacher_params:,} ‚Üí {student_params:,} params ({teacher_params/student_params:.1f}√ó fewer)\n",
    "   - Accuracy: {test_acc:.4f} ‚Üí {student_acc:.4f} ({(test_acc-student_acc)*100:.2f}% loss)\n",
    "   - Model size: {baseline_size:.2f}MB ‚Üí {student_params*4/1e6:.2f}MB ({baseline_size/(student_params*4/1e6):.1f}√ó smaller)\n",
    "‚úÖ Combined (Distillation + Quantization):\n",
    "   - Total compression: {baseline_size/(micro_size/1e6):.1f}√ó smaller\n",
    "   - Final size: {micro_size/1024:.2f}KB\n",
    "   - Fits on microcontroller: {'Yes ‚úÖ' if micro_size < arduino_ram else 'No ‚ùå'}\n",
    "‚úÖ Deployment Ready:\n",
    "   - TensorFlow Lite model: /tmp/mobile_model_int8.tflite\n",
    "   - TFLite Micro model: /tmp/student_tflite_micro.tflite\n",
    "   - Arduino header: /tmp/model.h\n",
    "   - Arduino sketch: /tmp/tflite_micro_arduino.ino\n",
    "\"\"\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Next: Deploy to production edge devices!\")\n",
    "print(\"Part 8: Real-world projects (smart home, manufacturing, wearables)\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91784e13",
   "metadata": {},
   "source": [
    "# üöÄ Production Projects & Deployment\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This section presents **8 production-grade Edge AI & TinyML projects** with complete implementation roadmaps, deployment strategies, and business value quantification.\n",
    "\n",
    "**Total business value**: $50M-$150M/year across use cases\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Project 1: Smart Home Voice Assistant (Keyword Spotting)\n",
    "\n",
    "## Business Objective\n",
    "Deploy wake word detection on microcontroller for privacy-preserving, always-on voice assistant\n",
    "\n",
    "**Problem**: Cloud-based assistants violate privacy and have 300-500ms latency\n",
    "\n",
    "**Edge Solution**: On-device keyword spotting (10ms latency, zero privacy risk)\n",
    "\n",
    "## Technical Implementation\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "Microphone ‚Üí MFCC Feature Extraction ‚Üí TinyML Model (50KB) ‚Üí Wake Word Detection\n",
    "```\n",
    "\n",
    "**Model**: 1D CNN for audio classification\n",
    "- Input: 40 MFCC features √ó 49 frames = 1,960 features\n",
    "- Architecture: Conv1D (32) ‚Üí Conv1D (64) ‚Üí Dense (128) ‚Üí Softmax (12 classes)\n",
    "- Size: 50KB (INT8 quantized)\n",
    "- Latency: 10ms on ARM Cortex-M4\n",
    "- Power: 5mW (battery lasts 6 months)\n",
    "\n",
    "### Week-by-Week Roadmap\n",
    "\n",
    "**Week 1-2: Data Collection & Preparation**\n",
    "```python\n",
    "# Collect wake word dataset\n",
    "# - 10 keywords: \"Hey Assistant\", \"OK Computer\", \"Wake Up\", etc.\n",
    "# - 2000 samples per keyword\n",
    "# - Background noise samples (TV, music, traffic)\n",
    "\n",
    "import librosa\n",
    "\n",
    "def extract_mfcc(audio_file, n_mfcc=40, n_frames=49):\n",
    "    \"\"\"Extract MFCC features from audio\"\"\"\n",
    "    y, sr = librosa.load(audio_file, sr=16000)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    \n",
    "    # Pad or truncate to fixed length\n",
    "    if mfcc.shape[1] < n_frames:\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, n_frames - mfcc.shape[1])))\n",
    "    else:\n",
    "        mfcc = mfcc[:, :n_frames]\n",
    "    \n",
    "    return mfcc.T  # Shape: (49, 40)\n",
    "```\n",
    "\n",
    "**Week 3-4: Model Training**\n",
    "```python\n",
    "def build_keyword_spotting_model():\n",
    "    \"\"\"Build 1D CNN for keyword spotting\"\"\"\n",
    "    inputs = layers.Input(shape=(49, 40))\n",
    "    \n",
    "    x = layers.Conv1D(32, 3, activation='relu')(inputs)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(64, 3, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(12, activation='softmax')(x)  # 10 keywords + silence + unknown\n",
    "    \n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "# Train with data augmentation (time shift, pitch shift, noise addition)\n",
    "# Target: 95%+ accuracy\n",
    "```\n",
    "\n",
    "**Week 5-6: Quantization & Optimization**\n",
    "```python\n",
    "# INT8 quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Result: 200KB ‚Üí 50KB (4√ó compression)\n",
    "# Accuracy: 95.2% ‚Üí 94.8% (0.4% loss, acceptable)\n",
    "```\n",
    "\n",
    "**Week 7-8: Deploy to Arduino Nano 33 BLE**\n",
    "```cpp\n",
    "// Arduino deployment code\n",
    "#include <TensorFlowLite.h>\n",
    "#include <PDM.h>  // Microphone library\n",
    "\n",
    "// Inference every 1 second\n",
    "void loop() {\n",
    "  // Read audio from microphone\n",
    "  PDM.read(audio_buffer, BUFFER_SIZE);\n",
    "  \n",
    "  // Extract MFCC\n",
    "  extract_mfcc(audio_buffer, mfcc_features);\n",
    "  \n",
    "  // Run inference (10ms)\n",
    "  interpreter->Invoke();\n",
    "  \n",
    "  // Check if wake word detected\n",
    "  float max_prob = 0;\n",
    "  int max_idx = 0;\n",
    "  for (int i = 0; i < 10; i++) {\n",
    "    if (output[i] > max_prob) {\n",
    "      max_prob = output[i];\n",
    "      max_idx = i;\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  if (max_prob > 0.8) {  // Confidence threshold\n",
    "    Serial.println(\"Wake word detected!\");\n",
    "    // Trigger full assistant (send to cloud for complex queries)\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## Business Value: $15M-$40M/year\n",
    "\n",
    "**Cost Savings**:\n",
    "- **API fees**: $60M/year ‚Üí $5M/year = **$55M saved**\n",
    "- **Bandwidth**: 500GB/day ‚Üí 5GB/day = $16K/year saved\n",
    "\n",
    "**Privacy Value**:\n",
    "- GDPR compliant (no audio upload)\n",
    "- User trust +12 NPS points\n",
    "- Market differentiation\n",
    "\n",
    "**Conservative estimate**: **$15M-$40M/year** (10M users)\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Project 2: Manufacturing Defect Detection (Edge Vision)\n",
    "\n",
    "## Business Objective\n",
    "Real-time visual inspection on production line using edge AI camera\n",
    "\n",
    "**Problem**: Cloud inference 200ms latency limits throughput to 60 items/min\n",
    "\n",
    "**Edge Solution**: On-device inference 10ms ‚Üí 600 items/min (10√ó throughput)\n",
    "\n",
    "## Technical Implementation\n",
    "\n",
    "### Hardware\n",
    "- **NVIDIA Jetson Nano**: $99, 472 GFLOPS, 4GB RAM\n",
    "- **Industrial camera**: 1920√ó1080, 60 FPS\n",
    "- **Deployment**: 100 production lines\n",
    "\n",
    "### Model Architecture\n",
    "```python\n",
    "# MobileNetV2 + Custom defect classifier\n",
    "base = keras.applications.MobileNetV2(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Freeze base\n",
    "base.trainable = False\n",
    "\n",
    "# Add defect classification head\n",
    "x = base.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "outputs = layers.Dense(5, activation='softmax')(x)  # 5 defect types\n",
    "\n",
    "model = keras.Model(base.input, outputs)\n",
    "\n",
    "# Train on factory data\n",
    "# Target: 98%+ defect detection rate\n",
    "```\n",
    "\n",
    "### TensorRT Optimization\n",
    "```python\n",
    "# Convert to TensorRT for maximum speed\n",
    "import tensorrt as trt\n",
    "\n",
    "# Build TensorRT engine\n",
    "builder = trt.Builder(TRT_LOGGER)\n",
    "network = builder.create_network()\n",
    "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "\n",
    "# Load ONNX model\n",
    "with open('defect_detector.onnx', 'rb') as f:\n",
    "    parser.parse(f.read())\n",
    "\n",
    "# Build with INT8 precision\n",
    "config = builder.create_builder_config()\n",
    "config.set_flag(trt.BuilderFlag.INT8)\n",
    "config.int8_calibrator = EntropyCalibrator(calibration_data)\n",
    "\n",
    "# Build engine\n",
    "engine = builder.build_engine(network, config)\n",
    "\n",
    "# Result: 50ms ‚Üí 10ms inference (5√ó speedup)\n",
    "```\n",
    "\n",
    "### Week-by-Week Roadmap\n",
    "\n",
    "**Week 1-2**: Data collection (10K defect images per line)\n",
    "\n",
    "**Week 3-4**: Train MobileNetV2 classifier (98% accuracy)\n",
    "\n",
    "**Week 5-6**: Optimize with TensorRT INT8 (10ms inference)\n",
    "\n",
    "**Week 7-8**: Deploy to 100 lines, validate throughput\n",
    "\n",
    "## Business Value: $20M-$60M/year\n",
    "\n",
    "**Throughput Increase**:\n",
    "- 60 items/min ‚Üí 600 items/min (10√ó faster)\n",
    "- Revenue: $10M/year per line √ó 10√ó = **$100M/year** (or avoid $100M capex for 10√ó more lines)\n",
    "\n",
    "**Cost Savings**:\n",
    "- API fees: $3.6M/year ‚Üí $0\n",
    "- Bandwidth: $16K/year ‚Üí $0\n",
    "\n",
    "**Per Factory**: $5M-$15M/year  \n",
    "**Total (4 factories)**: **$20M-$60M/year**\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Project 3: Wearable Health Monitoring (Arrhythmia Detection)\n",
    "\n",
    "## Business Objective\n",
    "Real-time ECG analysis on smartwatch for arrhythmia detection\n",
    "\n",
    "**Problem**: Sending 100 beats/min to cloud costs $14.4B/day (impossible)\n",
    "\n",
    "**Edge Solution**: On-device ECG analysis, only upload anomalies\n",
    "\n",
    "## Technical Implementation\n",
    "\n",
    "### Model Architecture\n",
    "```python\n",
    "# 1D CNN for ECG classification\n",
    "def build_ecg_model():\n",
    "    inputs = layers.Input(shape=(300, 1))  # 300 samples, 1 channel\n",
    "    \n",
    "    x = layers.Conv1D(32, 5, activation='relu')(inputs)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(64, 5, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(128, 5, activation='relu')(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    outputs = layers.Dense(5, activation='softmax')(x)  # Normal, AFib, VTach, VFib, PVC\n",
    "    \n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "# Optimize for ARM Cortex-M4 (smartwatch)\n",
    "# Target: 50KB model, 5ms inference, <1mW power\n",
    "```\n",
    "\n",
    "### Power Optimization\n",
    "```python\n",
    "# Duty cycling for battery life\n",
    "import time\n",
    "\n",
    "def run_ecg_monitoring():\n",
    "    while True:\n",
    "        # Read ECG (10ms)\n",
    "        ecg_data = read_ecg_sensor()\n",
    "        \n",
    "        # Inference (5ms)\n",
    "        prediction = model.predict(ecg_data)\n",
    "        \n",
    "        # Check anomaly\n",
    "        if is_anomaly(prediction):\n",
    "            send_alert_to_cloud()\n",
    "        \n",
    "        # Sleep for 950ms (95% duty cycle)\n",
    "        time.sleep(0.95)\n",
    "\n",
    "# Power consumption:\n",
    "# - Active (15ms): 10mW\n",
    "# - Sleep (985ms): 0.1mW\n",
    "# - Average: (10 * 15 + 0.1 * 985) / 1000 = 0.25mW\n",
    "# - Battery life: 200mAh √∑ 0.25mW = 7 days ‚úÖ\n",
    "```\n",
    "\n",
    "### Week-by-Week Roadmap\n",
    "\n",
    "**Week 1-2**: Collect ECG dataset (MIT-BIH, PTB-XL)\n",
    "\n",
    "**Week 3-4**: Train 1D CNN (98% accuracy)\n",
    "\n",
    "**Week 5-6**: Quantize to INT8, optimize for ARM\n",
    "\n",
    "**Week 7-8**: Deploy to smartwatch, validate battery life\n",
    "\n",
    "## Business Value: $15M-$50M/year\n",
    "\n",
    "**Feature Enablement**:\n",
    "- Cloud inference impossible ($14B/day cost)\n",
    "- Edge AI enables real-time monitoring ($0 cost)\n",
    "\n",
    "**Market Value**:\n",
    "- 10M users √ó $5/month subscription = **$600M/year**\n",
    "- Conservative attribution to edge AI: 2.5-8% = **$15M-$50M/year**\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Project 4: Autonomous Vehicle (Perception)\n",
    "\n",
    "## Business Objective\n",
    "Real-time object detection for autonomous driving\n",
    "\n",
    "**Latency requirement**: <10ms (safety-critical)\n",
    "\n",
    "## Technical Implementation\n",
    "\n",
    "### Model: YOLOv5-Nano + TensorRT\n",
    "```python\n",
    "# Optimized for NVIDIA Jetson Xavier NX\n",
    "# Input: 640√ó640√ó3 camera image\n",
    "# Output: Bounding boxes (cars, pedestrians, cyclists, traffic signs)\n",
    "# Latency: 8ms @ 30 FPS\n",
    "# Power: 15W\n",
    "\n",
    "# INT8 quantization + TensorRT optimization\n",
    "# Accuracy: 88% mAP (vs 90% float32, acceptable for redundancy)\n",
    "```\n",
    "\n",
    "## Business Value: $10M-$30M/year\n",
    "\n",
    "**Bandwidth Savings**: $900K/year (no image upload)\n",
    "\n",
    "**Safety Value**: Priceless (enables autonomous driving)\n",
    "\n",
    "**Time-to-Market**: 50√ó faster model iteration\n",
    "\n",
    "**Conservative**: **$10M-$30M/year**\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Project 5: Smart Agriculture (Crop Disease Detection)\n",
    "\n",
    "## Business Objective\n",
    "On-device plant disease detection for farmers\n",
    "\n",
    "## Technical Implementation\n",
    "- **Device**: Smartphone app with TensorFlow Lite\n",
    "- **Model**: MobileNetV2 (5MB, 100ms inference)\n",
    "- **Dataset**: PlantVillage (38 disease classes)\n",
    "\n",
    "## Business Value: $5M-$15M/year\n",
    "\n",
    "**Farmer Value**: Early detection saves 20-30% crop loss\n",
    "\n",
    "**App Revenue**: 1M farmers √ó $5/month = **$60M/year**\n",
    "\n",
    "**Conservative attribution**: 8-25% = **$5M-$15M/year**\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Project 6: Industrial IoT (Predictive Maintenance)\n",
    "\n",
    "## Business Objective\n",
    "On-sensor anomaly detection for industrial equipment\n",
    "\n",
    "## Technical Implementation\n",
    "- **Device**: ESP32 microcontroller on each machine\n",
    "- **Model**: Autoencoder (50KB, 10ms inference)\n",
    "- **Sensors**: Vibration, temperature, pressure\n",
    "\n",
    "## Business Value: $10M-$30M/year\n",
    "\n",
    "**Bandwidth Savings**: 10,000 sensors √ó 1MB/day ‚Üí 10KB/day = $500K/year\n",
    "\n",
    "**Downtime Reduction**: 30% improvement = **$30M/year**\n",
    "\n",
    "**Conservative**: **$10M-$30M/year**\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Project 7: Retail (Cashierless Checkout)\n",
    "\n",
    "## Business Objective\n",
    "Real-time product recognition for automated checkout\n",
    "\n",
    "## Technical Implementation\n",
    "- **Device**: Edge camera + Jetson Nano\n",
    "- **Model**: EfficientNet-Lite (3MB, 20ms)\n",
    "- **Products**: 500 SKUs\n",
    "\n",
    "## Business Value: $5M-$20M/year\n",
    "\n",
    "**Labor Savings**: 10 stores √ó $200K/year = **$2M/year**\n",
    "\n",
    "**Customer Experience**: Faster checkout, +10% sales = **$5M/year**\n",
    "\n",
    "**Total**: **$5M-$20M/year**\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Project 8: Semiconductor (Wafer Defect Detection)\n",
    "\n",
    "## Business Objective\n",
    "Real-time wafer defect detection during fabrication\n",
    "\n",
    "## Technical Implementation\n",
    "- **Device**: Custom ASIC vision processor\n",
    "- **Model**: Custom CNN (1MB, 5ms)\n",
    "- **Resolution**: 10,000√ó10,000 pixels (full wafer scan)\n",
    "\n",
    "## Business Value: $20M-$50M/year\n",
    "\n",
    "**Yield Improvement**: 1% yield increase = **$50M/year** (300mm fab)\n",
    "\n",
    "**Throughput**: Real-time vs batch = 2√ó faster\n",
    "\n",
    "**Conservative**: **$20M-$50M/year**\n",
    "\n",
    "---\n",
    "\n",
    "# üìä Business Value Summary\n",
    "\n",
    "| Project | Annual Value | Key Metric | Device |\n",
    "|---------|--------------|------------|--------|\n",
    "| 1. Smart Home Voice | $15M-$40M | $55M cost savings | Arduino Nano 33 BLE |\n",
    "| 2. Manufacturing Defect | $20M-$60M | 10√ó throughput | Jetson Nano |\n",
    "| 3. Wearable Health | $15M-$50M | 7-day battery | ARM Cortex-M4 |\n",
    "| 4. Autonomous Vehicle | $10M-$30M | <10ms latency | Jetson Xavier NX |\n",
    "| 5. Smart Agriculture | $5M-$15M | 20-30% crop loss reduction | Smartphone |\n",
    "| 6. Industrial IoT | $10M-$30M | 30% downtime reduction | ESP32 |\n",
    "| 7. Retail Checkout | $5M-$20M | Labor + sales increase | Jetson Nano |\n",
    "| 8. Semiconductor Inspection | $20M-$50M | 1% yield improvement | Custom ASIC |\n",
    "| **Total** | **$100M-$295M** | Latency + Privacy + Cost | Edge/TinyML |\n",
    "\n",
    "**Conservative midpoint**: **$200M/year** across all edge AI applications\n",
    "\n",
    "---\n",
    "\n",
    "# üîß Deployment Framework Comparison\n",
    "\n",
    "## TensorFlow Lite (Mobile/Edge)\n",
    "**Best for**: Android, iOS, Raspberry Pi  \n",
    "**Model size**: 1MB-100MB  \n",
    "**Latency**: 10-100ms  \n",
    "**Deployment**: Simple (one-click export from TensorFlow)\n",
    "\n",
    "## TensorFlow Lite Micro (TinyML)\n",
    "**Best for**: Microcontrollers (Arduino, ESP32, STM32)  \n",
    "**Model size**: 10KB-500KB  \n",
    "**Latency**: 1-50ms  \n",
    "**Power**: <10mW  \n",
    "**Deployment**: C++ library, compile with Arduino IDE\n",
    "\n",
    "## TensorRT (NVIDIA)\n",
    "**Best for**: Jetson, autonomous vehicles, data centers  \n",
    "**Model size**: 1MB-10GB  \n",
    "**Latency**: 1-10ms  \n",
    "**Optimization**: INT8, FP16, custom kernels  \n",
    "**Speedup**: 5-10√ó faster than TensorFlow Lite\n",
    "\n",
    "## Core ML (Apple)\n",
    "**Best for**: iPhone, iPad, Mac  \n",
    "**Model size**: 1MB-100MB  \n",
    "**Latency**: 5-50ms  \n",
    "**Integration**: Native iOS/macOS APIs  \n",
    "**Hardware**: Neural Engine (15.8 TOPS on A16 Bionic)\n",
    "\n",
    "---\n",
    "\n",
    "# üéì Key Takeaways\n",
    "\n",
    "## When to Use Edge AI\n",
    "\n",
    "‚úÖ **Use Edge AI when**:\n",
    "1. **Latency critical**: <50ms required\n",
    "2. **Privacy sensitive**: Data cannot leave device\n",
    "3. **High volume**: >1M inferences/day (cloud expensive)\n",
    "4. **Offline capability**: No internet connection\n",
    "5. **Bandwidth constrained**: Cannot upload large data\n",
    "\n",
    "‚ùå **Don't use Edge AI when**:\n",
    "1. **Complex models**: >1GB models (GPT-4, DALL-E)\n",
    "2. **Low volume**: <1K inferences/day\n",
    "3. **Continuous learning**: Model updates daily\n",
    "4. **Heterogeneous devices**: Many device types\n",
    "\n",
    "## Optimization Strategy\n",
    "\n",
    "**Step 1**: Train large model on cloud (maximize accuracy)  \n",
    "**Step 2**: Knowledge distillation (10-100√ó compression, 2-5% accuracy loss)  \n",
    "**Step 3**: INT8 quantization (4√ó compression, 0.5-2% accuracy loss)  \n",
    "**Step 4**: Pruning (2-5√ó compression, 1-3% accuracy loss)  \n",
    "**Step 5**: Deploy to target device  \n",
    "**Result**: 50-500√ó total compression, 3-10% accuracy loss\n",
    "\n",
    "## Hardware Selection\n",
    "\n",
    "| Use Case | Device | Cost | Power | Latency |\n",
    "|----------|--------|------|-------|---------|\n",
    "| **Always-on wake word** | Arduino Nano 33 BLE | $25 | 5mW | 10ms |\n",
    "| **Wearable health** | ARM Cortex-M4 | Built-in | <1mW | 5ms |\n",
    "| **Smart home camera** | Raspberry Pi 4 | $55 | 3W | 50ms |\n",
    "| **Manufacturing vision** | Jetson Nano | $99 | 10W | 10ms |\n",
    "| **Autonomous vehicle** | Jetson Xavier NX | $399 | 15W | 5ms |\n",
    "| **Mobile app** | Smartphone | User-owned | 5W | 20ms |\n",
    "\n",
    "---\n",
    "\n",
    "# üìö Resources & Next Steps\n",
    "\n",
    "## Frameworks\n",
    "1. **TensorFlow Lite**: https://tensorflow.org/lite\n",
    "2. **TensorFlow Lite Micro**: https://tensorflow.org/lite/microcontrollers\n",
    "3. **TensorRT**: https://developer.nvidia.com/tensorrt\n",
    "4. **Core ML**: https://developer.apple.com/machine-learning/core-ml/\n",
    "\n",
    "## Hardware\n",
    "1. **Arduino Nano 33 BLE**: $25, 256KB RAM\n",
    "2. **ESP32**: $5, 520KB RAM\n",
    "3. **Raspberry Pi 4**: $55, 4GB RAM\n",
    "4. **NVIDIA Jetson Nano**: $99, 472 GFLOPS\n",
    "5. **Google Coral Dev Board**: $150, 4 TOPS\n",
    "\n",
    "## Courses\n",
    "1. **TinyML (edX)**: Harvard CS249r\n",
    "2. **Edge AI (Coursera)**: TensorFlow Lite deployment\n",
    "3. **NVIDIA Deep Learning Institute**: Jetson AI courses\n",
    "\n",
    "## Papers\n",
    "1. **MobileNets** (Howard et al., 2017): Efficient mobile architectures\n",
    "2. **EfficientNet** (Tan & Le, 2019): State-of-the-art efficiency\n",
    "3. **TinyML** (Banbury et al., 2020): Machine learning on microcontrollers\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ Success Criteria Checklist\n",
    "\n",
    "Before deploying edge AI, verify:\n",
    "\n",
    "- [ ] **Latency requirement**: <50ms achieved\n",
    "- [ ] **Model size**: Fits on target device (RAM + Flash)\n",
    "- [ ] **Accuracy**: Within 3-5% of cloud model\n",
    "- [ ] **Power consumption**: Meets battery life goals (<10mW for wearables)\n",
    "- [ ] **Quantization**: INT8 working (4√ó compression)\n",
    "- [ ] **Deployment tested**: Model runs on actual hardware\n",
    "- [ ] **Business value**: ROI quantified ($XM-$YM/year)\n",
    "- [ ] **Fallback strategy**: Hybrid edge+cloud for complex cases\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Conclusion\n",
    "\n",
    "**Edge AI enables $100M-$300M/year business value**:\n",
    "- **Smart home**: $15M-$40M (privacy + cost savings)\n",
    "- **Manufacturing**: $20M-$60M (10√ó throughput)\n",
    "- **Wearables**: $15M-$50M (7-day battery, feature enablement)\n",
    "- **Autonomous vehicles**: $10M-$30M (safety + bandwidth)\n",
    "- **Total**: **$100M-$295M/year** across 8 use cases\n",
    "\n",
    "**Key techniques**:\n",
    "1. **Quantization** (INT8): 4√ó smaller, 2-4√ó faster, <2% accuracy loss\n",
    "2. **Knowledge distillation**: 10-100√ó smaller, 2-5% accuracy loss\n",
    "3. **Pruning**: 2-10√ó smaller, 1-8% accuracy loss\n",
    "4. **Combined**: 50-500√ó compression, 3-10% accuracy loss\n",
    "\n",
    "**Deployment platforms**:\n",
    "- **TensorFlow Lite**: Mobile/edge (1MB-100MB models)\n",
    "- **TFLite Micro**: Microcontrollers (10KB-500KB models)\n",
    "- **TensorRT**: NVIDIA Jetson (maximum speed)\n",
    "- **Core ML**: Apple devices (native integration)\n",
    "\n",
    "**Next steps**:\n",
    "1. Choose use case (voice, vision, sensor)\n",
    "2. Train baseline model on cloud\n",
    "3. Optimize (quantization + distillation + pruning)\n",
    "4. Deploy to target device (Arduino, Jetson, mobile)\n",
    "5. Validate latency, accuracy, power consumption\n",
    "6. Quantify business value ($XM-$YM/year)\n",
    "\n",
    "**Remember**: Edge AI is essential for real-time, privacy-sensitive, high-volume applications. Start deploying today! üöÄüì±üîß\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Progression:**\n",
    "- **Previous**: 069 Federated Learning (Privacy-Preserving Distributed ML)\n",
    "- **Current**: 070 Edge AI & TinyML (On-Device Inference, Microcontrollers)\n",
    "- **Next**: 071 Transformers & BERT (Self-Attention, Pre-training, Transfer Learning)\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Notebook Complete! Ready for production edge AI deployment and $100M-$300M/year business value creation.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
