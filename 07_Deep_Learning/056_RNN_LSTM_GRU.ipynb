{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc4b981",
   "metadata": {},
   "source": [
    "# 056: RNN, LSTM, GRU",
    "",
    "## \ud83d\udcda Learning Objectives",
    "",
    "By the end of this notebook, you will master:",
    "",
    "1. **Sequential Data Processing** - Understand why CNNs fail on temporal sequences",
    "2. **RNN Architecture** - Hidden states, recurrent connections, backpropagation through time",
    "3. **Vanishing Gradient Problem** - Why vanilla RNNs fail on long sequences",
    "4. **LSTM Networks** - Cell states, gates (forget, input, output), gradient flow",
    "5. **GRU Networks** - Simplified gating (reset, update), computational efficiency",
    "6. **Bidirectional RNNs** - Process sequences forward and backward simultaneously",
    "7. **Semiconductor Applications** - Sequential test pattern analysis, time-series yield prediction",
    "8. **Production Deployment** - Model optimization, inference strategies, real-time processing",
    "",
    "---",
    "",
    "## \ud83c\udfaf Why Sequential Models Matter",
    "",
    "### **The Temporal Challenge**",
    "",
    "**CNNs assume spatial independence:**",
    "- Each pixel processed independently (with local context via filters)",
    "- No concept of \"before\" or \"after\"",
    "- Works for images (spatial data) but fails for sequences (temporal data)",
    "",
    "**Sequential data requires memory:**",
    "- Test results at time $t$ depend on previous measurements at $t-1, t-2, ...$",
    "- Wafer yield patterns evolve over production days",
    "- Device parametric drift accumulates over time",
    "",
    "**Example: Test Pattern Analysis**",
    "",
    "```",
    "Parametric Test Sequence (20 measurements over time):",
    "Time:    t1    t2    t3    t4    t5    ...  t20",
    "Vdd:    1.05  1.06  1.04  1.03  1.02  ...  0.98  \u2190 Voltage drift detected",
    "Idd:    250   252   248   245   240   ...  210   \u2190 Current decreasing",
    "Freq:   2.4   2.4   2.3   2.3   2.2   ...  2.0   \u2190 Frequency degradation",
    "",
    "Pattern: Gradual performance degradation \u2192 Predict failure at t25",
    "```",
    "",
    "**CNN would analyze each timestep independently (wrong!)**  ",
    "**RNN captures temporal dependencies (correct!)**",
    "",
    "---",
    "",
    "## \ud83c\udfed Semiconductor Use Case: Sequential Test Analysis",
    "",
    "### **Problem Statement**",
    "",
    "**Objective:** Predict device failure 5 test cycles ahead based on parametric trend analysis",
    "",
    "**Business Value:**",
    "- **Proactive binning:** $20M-$80M/year from early failure detection",
    "- **Test time reduction:** Skip remaining tests if failure predicted (30% time savings)",
    "- **Yield optimization:** Identify slow degradation patterns for process tuning",
    "",
    "**Data:**",
    "- 50,000 devices tested over 20 cycles (1 week of production)",
    "- 15 parametric measurements per cycle (Vdd, Idd, freq, power, temp, etc.)",
    "- Sequential pattern: Each device has time-series: $(x_1, x_2, ..., x_{20}) \\rightarrow y_{failure}$",
    "",
    "**Challenge:**",
    "- Long sequences (20 timesteps)",
    "- Multiple features (15 parameters)",
    "- Temporal dependencies (drift accumulates over time)",
    "- Need to predict 5 cycles ahead (early warning)",
    "",
    "---",
    "",
    "## \ud83d\udcca What We'll Build",
    "",
    "```mermaid",
    "graph LR",
    "    A[Sequential Test Data<br/>20 cycles \u00d7 15 params] --> B[Data Preprocessing<br/>Normalization + Windowing]",
    "    B --> C[Vanilla RNN<br/>Baseline Model]",
    "    B --> D[LSTM Network<br/>Long-term Dependencies]",
    "    B --> E[GRU Network<br/>Efficient Alternative]",
    "    B --> F[Bidirectional LSTM<br/>Forward + Backward]",
    "    ",
    "    C --> G[Comparison<br/>Accuracy + Speed]",
    "    D --> G",
    "    E --> G",
    "    F --> G",
    "    ",
    "    G --> H[Best Model Selection<br/>Deploy to Production]",
    "    ",
    "    H --> I[Real-time Inference<br/>Predict at Cycle 15]",
    "    ",
    "    style A fill:#e1f5ff",
    "    style H fill:#d4edda",
    "    style I fill:#fff3cd",
    "```",
    "",
    "**Architecture Comparison:**",
    "1. **Vanilla RNN:** Simple recurrence, fails on long sequences (vanishing gradient)",
    "2. **LSTM:** Gated architecture, excellent long-term memory, 4\u00d7 parameters of RNN",
    "3. **GRU:** Simplified gates, 75% of LSTM parameters, competitive accuracy",
    "4. **Bidirectional LSTM:** Process both directions, best accuracy, 2\u00d7 inference time",
    "",
    "---",
    "",
    "## \ud83d\udd27 Prerequisites",
    "",
    "```python",
    "# Core libraries",
    "import numpy as np",
    "import pandas as pd",
    "import matplotlib.pyplot as plt",
    "import seaborn as sns",
    "",
    "# PyTorch for neural networks",
    "import torch",
    "import torch.nn as nn",
    "import torch.optim as optim",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset",
    "",
    "# Scikit-learn utilities",
    "from sklearn.preprocessing import StandardScaler",
    "from sklearn.model_selection import train_test_split",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix",
    "",
    "# Visualization",
    "import warnings",
    "warnings.filterwarnings('ignore')",
    "",
    "# Set random seeds for reproducibility",
    "np.random.seed(42)",
    "torch.manual_seed(42)",
    "```",
    "",
    "**Installation:**",
    "```bash",
    "pip install torch numpy pandas matplotlib seaborn scikit-learn",
    "```",
    "",
    "---",
    "",
    "## \ud83d\udcc8 Success Metrics",
    "",
    "**Model Performance:**",
    "- **Accuracy:** \u226585% on failure prediction (5 cycles ahead)",
    "- **Recall:** \u226590% (critical: don't miss failures)",
    "- **Precision:** \u226580% (minimize false alarms)",
    "- **Early detection:** Predict failure at cycle 15 (5 cycles early)",
    "",
    "**Computational Efficiency:**",
    "- **Training time:** <10 min on CPU for 50K devices",
    "- **Inference time:** <1ms per device (real-time capable)",
    "- **Model size:** <5MB (edge deployable)",
    "",
    "**Business Impact:**",
    "- **Cost savings:** $20M-$80M/year from early detection",
    "- **Test time reduction:** 30% (skip remaining tests)",
    "- **False alarm rate:** <10% (avoid unnecessary quarantine)",
    "",
    "---",
    "",
    "## \ud83d\uddc2\ufe0f Notebook Structure",
    "",
    "1. **Mathematical Foundations** - RNN equations, LSTM gates, GRU mechanics",
    "2. **Data Generation** - Synthetic sequential test data with realistic patterns",
    "3. **Vanilla RNN Implementation** - From scratch + PyTorch comparison",
    "4. **LSTM Network** - Gated recurrence for long sequences",
    "5. **GRU Network** - Efficient alternative to LSTM",
    "6. **Bidirectional LSTM** - Forward + backward processing",
    "7. **Model Comparison** - Accuracy, speed, memory analysis",
    "8. **Real-World Projects** - 8 production-ready applications",
    "9. **Key Takeaways** - When to use each architecture, best practices",
    "",
    "Let's start! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cb0d48",
   "metadata": {},
   "source": [
    "# \ud83d\udcd0 Part 1: Mathematical Foundations\n",
    "\n",
    "## \ud83d\udd04 Vanilla RNN Architecture\n",
    "\n",
    "### **Core Concept: Hidden State as Memory**\n",
    "\n",
    "**Feedforward networks (CNNs, MLPs):** No memory between inputs  \n",
    "**Recurrent networks:** Hidden state $h_t$ carries information from previous timesteps\n",
    "\n",
    "### **RNN Equations**\n",
    "\n",
    "At each timestep $t$, given input $x_t$ and previous hidden state $h_{t-1}$:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_t = W_{hy} h_t + b_y\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "- $x_t \\in \\mathbb{R}^{d}$ : Input at time $t$ (e.g., 15 parametric measurements)\n",
    "- $h_t \\in \\mathbb{R}^{h}$ : Hidden state at time $t$ (e.g., 128 units)\n",
    "- $y_t \\in \\mathbb{R}^{c}$ : Output at time $t$ (e.g., 2 classes: pass/fail)\n",
    "- $W_{xh} \\in \\mathbb{R}^{h \\times d}$ : Input-to-hidden weights\n",
    "- $W_{hh} \\in \\mathbb{R}^{h \\times h}$ : Hidden-to-hidden (recurrent) weights\n",
    "- $W_{hy} \\in \\mathbb{R}^{c \\times h}$ : Hidden-to-output weights\n",
    "- $b_h, b_y$ : Bias terms\n",
    "- $\\tanh$ : Activation function (outputs in range $[-1, 1]$)\n",
    "\n",
    "### **Unrolled RNN Visualization**\n",
    "\n",
    "```\n",
    "Input sequence:  x\u2081    x\u2082    x\u2083    ...   x\u2082\u2080\n",
    "                  \u2193     \u2193     \u2193           \u2193\n",
    "Hidden states:   h\u2081 \u2192 h\u2082 \u2192 h\u2083 \u2192 ... \u2192 h\u2082\u2080\n",
    "                  \u2193     \u2193     \u2193           \u2193\n",
    "Outputs:         y\u2081    y\u2082    y\u2083    ...   y\u2082\u2080\n",
    "\n",
    "At timestep t=3:\n",
    "  h\u2083 = tanh(W_hh\u00b7h\u2082 + W_xh\u00b7x\u2083 + b_h)\n",
    "  y\u2083 = W_hy\u00b7h\u2083 + b_y\n",
    "\n",
    "h\u2083 contains information from x\u2081, x\u2082, x\u2083 (memory!)\n",
    "```\n",
    "\n",
    "### **Example Calculation**\n",
    "\n",
    "**Setup:**\n",
    "- Input dimension $d=15$ (15 parametric measurements)\n",
    "- Hidden dimension $h=128$\n",
    "- Output dimension $c=2$ (binary classification: pass/fail)\n",
    "\n",
    "**Timestep t=1:**\n",
    "\n",
    "$$\n",
    "h_1 = \\tanh(W_{hh} h_0 + W_{xh} x_1 + b_h)\n",
    "$$\n",
    "\n",
    "where $h_0 = \\mathbf{0}$ (initial hidden state is zeros)\n",
    "\n",
    "$$\n",
    "h_1 = \\tanh\\left(\\begin{bmatrix} 128 \\times 128 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 128 \\times 15 \\end{bmatrix} \\begin{bmatrix} 1.05 \\\\ 250 \\\\ \\vdots \\\\ 2.4 \\end{bmatrix} + \\begin{bmatrix} 128 \\times 1 \\end{bmatrix}\\right)\n",
    "$$\n",
    "\n",
    "Result: $h_1 \\in \\mathbb{R}^{128}$ (values in range $[-1, 1]$)\n",
    "\n",
    "**Timestep t=2:**\n",
    "\n",
    "$$\n",
    "h_2 = \\tanh(W_{hh} h_1 + W_{xh} x_2 + b_h)\n",
    "$$\n",
    "\n",
    "Now $h_1$ from previous step is used! $h_2$ contains information from both $x_1$ and $x_2$.\n",
    "\n",
    "**Final output (at t=20):**\n",
    "\n",
    "$$\n",
    "y_{20} = W_{hy} h_{20} + b_y = \\begin{bmatrix} 2 \\times 128 \\end{bmatrix} \\begin{bmatrix} h_{20} \\end{bmatrix} + \\begin{bmatrix} 2 \\times 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_{20} = \\begin{bmatrix} \\text{logit}_{\\text{pass}} \\\\ \\text{logit}_{\\text{fail}} \\end{bmatrix} \\rightarrow \\text{softmax} \\rightarrow \\begin{bmatrix} P(\\text{pass}) \\\\ P(\\text{fail}) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## \u26a0\ufe0f The Vanishing Gradient Problem\n",
    "\n",
    "### **Backpropagation Through Time (BPTT)**\n",
    "\n",
    "Training RNNs requires computing gradients with respect to $W_{hh}$ across all timesteps:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_{hh}} = \\sum_{t=1}^{T} \\frac{\\partial L_t}{\\partial W_{hh}}\n",
    "$$\n",
    "\n",
    "For each timestep $t$, gradient flows backward through time:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_t}{\\partial W_{hh}} = \\frac{\\partial L_t}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial h_{t-1}} \\cdot \\frac{\\partial h_{t-1}}{\\partial h_{t-2}} \\cdots \\frac{\\partial h_1}{\\partial W_{hh}}\n",
    "$$\n",
    "\n",
    "### **Chain Rule Explosion**\n",
    "\n",
    "Each $\\frac{\\partial h_t}{\\partial h_{t-1}}$ term involves:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_t}{\\partial h_{t-1}} = W_{hh}^T \\cdot \\text{diag}(\\tanh'(h_t))\n",
    "$$\n",
    "\n",
    "For long sequences ($T=20$), gradient becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_{20}}{\\partial W_{hh}} \\propto \\left(W_{hh}^T\\right)^{20} \\cdot \\prod_{t=1}^{20} \\tanh'(h_t)\n",
    "$$\n",
    "\n",
    "**Problem 1: $\\tanh'(x) \\leq 1$** (derivative saturates)\n",
    "\n",
    "$$\n",
    "\\tanh'(x) = 1 - \\tanh^2(x) \\leq 1\n",
    "$$\n",
    "\n",
    "For $|x| > 2$: $\\tanh'(x) \\approx 0$ (gradient vanishes)\n",
    "\n",
    "**Problem 2: Matrix power $(W_{hh}^T)^{20}$**\n",
    "\n",
    "If eigenvalues of $W_{hh}$ are:\n",
    "- $\\lambda < 1$ : $(W_{hh})^{20} \\rightarrow 0$ (vanishing gradient)\n",
    "- $\\lambda > 1$ : $(W_{hh})^{20} \\rightarrow \\infty$ (exploding gradient)\n",
    "\n",
    "### **Numerical Example**\n",
    "\n",
    "Assume $W_{hh}$ has largest eigenvalue $\\lambda = 0.9$:\n",
    "\n",
    "$$\n",
    "\\text{Gradient contribution from timestep } t=1 \\text{ to } t=20:\n",
    "$$\n",
    "\n",
    "$$\n",
    "(0.9)^{20} \\approx 0.12 \\quad \\text{(12% of original gradient)}\n",
    "$$\n",
    "\n",
    "If $\\lambda = 0.8$:\n",
    "\n",
    "$$\n",
    "(0.8)^{20} \\approx 0.01 \\quad \\text{(1% survives!)}\n",
    "$$\n",
    "\n",
    "**Consequence:** Early timesteps ($t=1, 2, 3$) contribute negligibly to gradient \u2192 RNN forgets long-term dependencies!\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udee1\ufe0f LSTM: Long Short-Term Memory\n",
    "\n",
    "### **Key Innovation: Cell State $C_t$ as Protected Memory**\n",
    "\n",
    "LSTM adds a **cell state** $C_t$ that flows through time with minimal modifications:\n",
    "\n",
    "$$\n",
    "C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t\n",
    "$$\n",
    "\n",
    "**Why this solves vanishing gradient:**\n",
    "- Gradient flows directly through $C_t$ without multiplicative $W_{hh}$ terms\n",
    "- Gates $f_t, i_t$ control information flow (learned, not fixed)\n",
    "- If $f_t \\approx 1$: $C_t \\approx C_{t-1}$ (gradient flows unchanged!)\n",
    "\n",
    "### **LSTM Equations (Complete Architecture)**\n",
    "\n",
    "At each timestep $t$:\n",
    "\n",
    "**1. Forget Gate** (decide what to forget from $C_{t-1}$):\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "\n",
    "- $f_t \\in [0, 1]^h$ : Elementwise forget weights\n",
    "- $f_t = 1$ : Keep everything from $C_{t-1}$\n",
    "- $f_t = 0$ : Completely forget $C_{t-1}$\n",
    "\n",
    "**2. Input Gate** (decide what new information to add):\n",
    "\n",
    "$$\n",
    "i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "$$\n",
    "\n",
    "- $i_t \\in [0, 1]^h$ : Input gate weights\n",
    "- $\\tilde{C}_t \\in [-1, 1]^h$ : Candidate cell state\n",
    "\n",
    "**3. Cell State Update** (combine forget and input):\n",
    "\n",
    "$$\n",
    "C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t\n",
    "$$\n",
    "\n",
    "**4. Output Gate** (decide what to output from $C_t$):\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_t = o_t \\odot \\tanh(C_t)\n",
    "$$\n",
    "\n",
    "**5. Final Output:**\n",
    "\n",
    "$$\n",
    "y_t = W_y h_t + b_y\n",
    "$$\n",
    "\n",
    "### **LSTM Dimensions**\n",
    "\n",
    "For our semiconductor example:\n",
    "- Input: $x_t \\in \\mathbb{R}^{15}$ (15 parameters)\n",
    "- Hidden state: $h_t \\in \\mathbb{R}^{128}$\n",
    "- Cell state: $C_t \\in \\mathbb{R}^{128}$\n",
    "- Concatenated input: $[h_{t-1}, x_t] \\in \\mathbb{R}^{143}$\n",
    "\n",
    "Weight matrices:\n",
    "- $W_f, W_i, W_o, W_C \\in \\mathbb{R}^{128 \\times 143}$\n",
    "- Total parameters: $4 \\times (128 \\times 143) = 73,216$ (per LSTM layer)\n",
    "\n",
    "**4\u00d7 more parameters than vanilla RNN** (which has only $W_{hh} \\in \\mathbb{R}^{128 \\times 128}$)\n",
    "\n",
    "### **Why LSTM Works: Gradient Flow Analysis**\n",
    "\n",
    "Gradient of loss with respect to $C_{t-1}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial C_{t-1}} = \\frac{\\partial L}{\\partial C_t} \\cdot \\frac{\\partial C_t}{\\partial C_{t-1}} = \\frac{\\partial L}{\\partial C_t} \\cdot f_t\n",
    "$$\n",
    "\n",
    "**Key insight:** $f_t$ is learned (not fixed)!\n",
    "- If long-term dependency needed: $f_t \\approx 1$ (gradient flows unchanged)\n",
    "- If short-term needed: $f_t \\approx 0$ (gradient blocked)\n",
    "\n",
    "Compare to vanilla RNN:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_t}{\\partial h_{t-1}} = W_{hh}^T \\cdot \\tanh'(h_t)\n",
    "$$\n",
    "\n",
    "Fixed matrix $W_{hh}$ (not adaptive!) \u2192 Cannot learn to preserve gradients selectively.\n",
    "\n",
    "---\n",
    "\n",
    "## \u26a1 GRU: Gated Recurrent Unit\n",
    "\n",
    "### **Simplification of LSTM**\n",
    "\n",
    "GRU combines forget and input gates into a single **update gate** $z_t$:\n",
    "- No separate cell state $C_t$ (uses $h_t$ directly)\n",
    "- Fewer parameters (3 gates vs 4 in LSTM)\n",
    "- Faster training and inference\n",
    "\n",
    "### **GRU Equations**\n",
    "\n",
    "**1. Update Gate** (how much to update hidden state):\n",
    "\n",
    "$$\n",
    "z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "$$\n",
    "\n",
    "**2. Reset Gate** (how much past to forget when computing candidate):\n",
    "\n",
    "$$\n",
    "r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "$$\n",
    "\n",
    "**3. Candidate Hidden State:**\n",
    "\n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t] + b_h)\n",
    "$$\n",
    "\n",
    "**4. Hidden State Update:**\n",
    "\n",
    "$$\n",
    "h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- $z_t \\approx 0$ : Keep old hidden state $h_{t-1}$ (like LSTM forget gate $f_t=1$)\n",
    "- $z_t \\approx 1$ : Use new candidate $\\tilde{h}_t$ (like LSTM input gate $i_t=1$)\n",
    "- $r_t \\approx 0$ : Ignore past when computing $\\tilde{h}_t$ (fresh start)\n",
    "- $r_t \\approx 1$ : Use past fully (retain memory)\n",
    "\n",
    "### **GRU vs LSTM: Parameter Comparison**\n",
    "\n",
    "For hidden size $h=128$, input size $d=15$:\n",
    "\n",
    "**LSTM:**\n",
    "- 4 gates: $f_t, i_t, o_t, \\tilde{C}_t$\n",
    "- Parameters: $4 \\times (128 \\times 143) = 73,216$\n",
    "\n",
    "**GRU:**\n",
    "- 3 gates: $z_t, r_t, \\tilde{h}_t$\n",
    "- Parameters: $3 \\times (128 \\times 143) = 54,912$\n",
    "\n",
    "**GRU has 75% of LSTM parameters** (25% reduction!)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd04 Bidirectional RNNs\n",
    "\n",
    "### **Forward + Backward Processing**\n",
    "\n",
    "Standard RNN processes sequence left-to-right: $x_1 \\rightarrow x_2 \\rightarrow \\cdots \\rightarrow x_{20}$\n",
    "\n",
    "**Bidirectional RNN (BiRNN):** Process both directions simultaneously\n",
    "\n",
    "$$\n",
    "\\overrightarrow{h}_t = \\text{RNN}_{\\text{forward}}(x_t, \\overrightarrow{h}_{t-1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\overleftarrow{h}_t = \\text{RNN}_{\\text{backward}}(x_t, \\overleftarrow{h}_{t+1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_t = [\\overrightarrow{h}_t; \\overleftarrow{h}_t] \\quad \\text{(concatenate)}\n",
    "$$\n",
    "\n",
    "### **Why Bidirectional Helps**\n",
    "\n",
    "**Example:** Predict failure at $t=15$ based on full sequence $x_1, ..., x_{20}$\n",
    "\n",
    "Standard LSTM at $t=15$:\n",
    "- Sees $x_1, ..., x_{15}$ (forward context)\n",
    "- Misses $x_{16}, ..., x_{20}$ (future context)\n",
    "\n",
    "Bidirectional LSTM at $t=15$:\n",
    "- $\\overrightarrow{h}_{15}$ : Information from $x_1, ..., x_{15}$\n",
    "- $\\overleftarrow{h}_{15}$ : Information from $x_{20}, ..., x_{16}$\n",
    "- Combined $h_{15}$ : Full sequence context!\n",
    "\n",
    "**Trade-off:** 2\u00d7 parameters, 2\u00d7 inference time (but higher accuracy)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcca Architecture Comparison Summary\n",
    "\n",
    "| Architecture | Parameters | Gradient Flow | Long-term Memory | Speed | Use Case |\n",
    "|--------------|-----------|---------------|------------------|-------|----------|\n",
    "| **Vanilla RNN** | $h^2 + h \\cdot d$ | Poor (vanishing) | \u2717 (5-10 steps) | Fast | Short sequences |\n",
    "| **LSTM** | $4(h^2 + h \\cdot d)$ | Excellent | \u2713 (100+ steps) | Slow | Long sequences |\n",
    "| **GRU** | $3(h^2 + h \\cdot d)$ | Very good | \u2713 (50+ steps) | Medium | Balanced |\n",
    "| **Bidirectional LSTM** | $2 \\times 4(h^2 + h \\cdot d)$ | Excellent | \u2713 (100+ steps) | Very slow | Best accuracy |\n",
    "\n",
    "**For $h=128, d=15$:**\n",
    "- Vanilla RNN: ~18K parameters\n",
    "- LSTM: ~73K parameters\n",
    "- GRU: ~55K parameters\n",
    "- BiLSTM: ~146K parameters\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Next Steps\n",
    "\n",
    "Now that we understand the theory, let's implement:\n",
    "1. Generate synthetic sequential test data\n",
    "2. Build all 4 architectures from scratch\n",
    "3. Compare performance on failure prediction task\n",
    "4. Deploy best model to production\n",
    "\n",
    "Let's code! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c2bac7",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac0a84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# PART 2: DATA GENERATION\n",
    "# Sequential Parametric Test Data with Realistic Patterns\n",
    "# ========================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n",
    "print(\"=\" * 60)\n",
    "print(\"SEQUENTIAL TEST DATA GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "# ========================================\n",
    "# CONFIGURATION\n",
    "# ========================================\n",
    "NUM_DEVICES = 50000      # Total devices to simulate\n",
    "SEQ_LENGTH = 20          # Test cycles per device\n",
    "NUM_FEATURES = 15        # Parametric measurements per cycle\n",
    "FAILURE_RATE = 0.20      # 20% of devices will fail\n",
    "# Feature names (semiconductor parametric tests)\n",
    "FEATURE_NAMES = [\n",
    "    'Vdd_voltage',        # Supply voltage (V)\n",
    "    'Idd_current',        # Supply current (mA)\n",
    "    'Frequency',          # Operating frequency (GHz)\n",
    "    'Power',              # Power consumption (W)\n",
    "    'Temperature',        # Junction temperature (\u00b0C)\n",
    "    'Leakage_current',    # Standby current (\u00b5A)\n",
    "    'Rise_time',          # Signal rise time (ps)\n",
    "    'Fall_time',          # Signal fall time (ps)\n",
    "    'Setup_time',         # Setup time margin (ps)\n",
    "    'Hold_time',          # Hold time margin (ps)\n",
    "    'Jitter',             # Clock jitter (ps)\n",
    "    'Phase_noise',        # PLL phase noise (dBc/Hz)\n",
    "    'THD',                # Total harmonic distortion (%)\n",
    "    'SNR',                # Signal-to-noise ratio (dB)\n",
    "    'BER'                 # Bit error rate (log scale)\n",
    "]\n",
    "print(f\"\\nDataset Configuration:\")\n",
    "print(f\"  Devices:          {NUM_DEVICES:,}\")\n",
    "print(f\"  Sequence length:  {SEQ_LENGTH} cycles\")\n",
    "print(f\"  Features:         {NUM_FEATURES}\")\n",
    "print(f\"  Failure rate:     {FAILURE_RATE*100:.0f}%\")\n",
    "# ========================================\n",
    "# GENERATE SEQUENTIAL DATA\n",
    "# ========================================\n",
    "def generate_device_sequence(device_id, will_fail):\n",
    "    \"\"\"\n",
    "    Generate sequential test data for one device\n",
    "    \n",
    "    Patterns:\n",
    "    - Passing devices: Stable parameters with minor noise\n",
    "    - Failing devices: Gradual drift + accelerated degradation in last 5 cycles\n",
    "    \"\"\"\n",
    "    \n",
    "    # Base values (nominal operating conditions)\n",
    "    base_values = np.array([\n",
    "        1.05,   # Vdd (V)\n",
    "        250,    # Idd (mA)\n",
    "        2.4,    # Freq (GHz)\n",
    "        0.6,    # Power (W)\n",
    "        75,     # Temp (\u00b0C)\n",
    "        10,     # Leakage (\u00b5A)\n",
    "        50,     # Rise time (ps)\n",
    "        50,     # Fall time (ps)\n",
    "        100,    # Setup time (ps)\n",
    "        100,    # Hold time (ps)\n",
    "        20,     # Jitter (ps)\n",
    "        -80,    # Phase noise (dBc/Hz)\n",
    "        1.0,    # THD (%)\n",
    "        40,     # SNR (dB)\n",
    "        -9      # BER (log10)\n",
    "    ])\n",
    "    \n",
    "    sequence = np.zeros((SEQ_LENGTH, NUM_FEATURES))\n",
    "    \n",
    "    if will_fail:\n",
    "        # Failing device: Gradual degradation\n",
    "        for t in range(SEQ_LENGTH):\n",
    "            # Drift rate increases over time\n",
    "            drift_factor = (t / SEQ_LENGTH) ** 2  # Quadratic acceleration\n",
    "            \n",
    "            # Different parameters degrade differently\n",
    "            drift = np.array([\n",
    "                -0.01 * drift_factor,   # Vdd decreases\n",
    "                -2.0 * drift_factor,    # Idd decreases (less current)\n",
    "                -0.05 * drift_factor,   # Frequency drops\n",
    "                -0.02 * drift_factor,   # Power decreases\n",
    "                +2.0 * drift_factor,    # Temperature increases (bad!)\n",
    "                +1.5 * drift_factor,    # Leakage increases (bad!)\n",
    "                +3.0 * drift_factor,    # Rise time increases (slower)\n",
    "                +3.0 * drift_factor,    # Fall time increases (slower)\n",
    "                -5.0 * drift_factor,    # Setup time decreases (margin loss)\n",
    "                -5.0 * drift_factor,    # Hold time decreases (margin loss)\n",
    "                +2.0 * drift_factor,    # Jitter increases (bad!)\n",
    "                +5.0 * drift_factor,    # Phase noise increases (worse)\n",
    "                +0.5 * drift_factor,    # THD increases (distortion)\n",
    "                -2.0 * drift_factor,    # SNR decreases (noise)\n",
    "                +0.5 * drift_factor     # BER increases (errors)\n",
    "            ])\n",
    "            \n",
    "            # Add random noise\n",
    "            noise = np.random.normal(0, 0.02, NUM_FEATURES)\n",
    "            \n",
    "            sequence[t] = base_values + drift + noise\n",
    "    \n",
    "    else:\n",
    "        # Passing device: Stable with minor variations\n",
    "        for t in range(SEQ_LENGTH):\n",
    "            # Small random walk\n",
    "            noise = np.random.normal(0, 0.01, NUM_FEATURES)\n",
    "            sequence[t] = base_values + noise\n",
    "    \n",
    "    return sequence\n",
    "# Generate dataset\n",
    "print(\"\\nGenerating sequential test data...\")\n",
    "X = np.zeros((NUM_DEVICES, SEQ_LENGTH, NUM_FEATURES))\n",
    "y = np.zeros(NUM_DEVICES, dtype=int)\n",
    "num_fail = int(NUM_DEVICES * FAILURE_RATE)\n",
    "num_pass = NUM_DEVICES - num_fail\n",
    "# Generate failing devices\n",
    "for i in range(num_fail):\n",
    "    X[i] = generate_device_sequence(i, will_fail=True)\n",
    "    y[i] = 1  # Failure\n",
    "# Generate passing devices\n",
    "for i in range(num_fail, NUM_DEVICES):\n",
    "    X[i] = generate_device_sequence(i, will_fail=False)\n",
    "    y[i] = 0  # Pass\n",
    "print(f\"  Passing devices:  {num_pass:,} ({num_pass/NUM_DEVICES*100:.1f}%)\")\n",
    "print(f\"  Failing devices:  {num_fail:,} ({num_fail/NUM_DEVICES*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bb2fc4",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41751f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# VISUALIZATION: EXAMPLE SEQUENCES\n",
    "# ========================================\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle(\"Sequential Test Patterns: Passing vs Failing Devices\", fontsize=16, fontweight='bold')\n",
    "# Select 3 features to visualize\n",
    "features_to_plot = [0, 1, 4]  # Vdd, Idd, Temperature\n",
    "feature_labels = ['Vdd (V)', 'Idd (mA)', 'Temperature (\u00b0C)']\n",
    "for col, (feat_idx, feat_label) in enumerate(zip(features_to_plot, feature_labels)):\n",
    "    # Passing device (row 0)\n",
    "    ax_pass = axes[0, col]\n",
    "    pass_device_idx = num_fail + 0  # First passing device\n",
    "    ax_pass.plot(range(SEQ_LENGTH), X[pass_device_idx, :, feat_idx], \n",
    "                marker='o', linewidth=2, color='green', alpha=0.7)\n",
    "    ax_pass.set_title(f\"Passing Device: {feat_label}\", fontweight='bold')\n",
    "    ax_pass.set_xlabel(\"Test Cycle\")\n",
    "    ax_pass.set_ylabel(feat_label)\n",
    "    ax_pass.grid(True, alpha=0.3)\n",
    "    ax_pass.axhline(y=X[pass_device_idx, 0, feat_idx], color='blue', \n",
    "                   linestyle='--', alpha=0.5, label='Baseline')\n",
    "    ax_pass.legend()\n",
    "    \n",
    "    # Failing device (row 1)\n",
    "    ax_fail = axes[1, col]\n",
    "    fail_device_idx = 0  # First failing device\n",
    "    ax_fail.plot(range(SEQ_LENGTH), X[fail_device_idx, :, feat_idx], \n",
    "                marker='o', linewidth=2, color='red', alpha=0.7)\n",
    "    ax_fail.set_title(f\"Failing Device: {feat_label}\", fontweight='bold')\n",
    "    ax_fail.set_xlabel(\"Test Cycle\")\n",
    "    ax_fail.set_ylabel(feat_label)\n",
    "    ax_fail.grid(True, alpha=0.3)\n",
    "    ax_fail.axhline(y=X[fail_device_idx, 0, feat_idx], color='blue', \n",
    "                   linestyle='--', alpha=0.5, label='Baseline')\n",
    "    ax_fail.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('sequential_test_patterns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n\u2713 Visualization saved to sequential_test_patterns.png\")\n",
    "# ========================================\n",
    "# FEATURE SCALING\n",
    "# ========================================\n",
    "print(\"\\nApplying feature scaling...\")\n",
    "# Reshape for scaling: (num_devices * seq_length, num_features)\n",
    "X_reshaped = X.reshape(-1, NUM_FEATURES)\n",
    "scaler = StandardScaler()\n",
    "X_scaled_reshaped = scaler.fit_transform(X_reshaped)\n",
    "# Reshape back: (num_devices, seq_length, num_features)\n",
    "X_scaled = X_scaled_reshaped.reshape(NUM_DEVICES, SEQ_LENGTH, NUM_FEATURES)\n",
    "print(f\"  Original shape:  {X.shape}\")\n",
    "print(f\"  Scaled shape:    {X_scaled.shape}\")\n",
    "# Check scaling\n",
    "print(f\"\\n  Mean (should be ~0):  {X_scaled_reshaped.mean(axis=0)[:3]}\")\n",
    "print(f\"  Std (should be ~1):   {X_scaled_reshaped.std(axis=0)[:3]}\")\n",
    "# ========================================\n",
    "# TRAIN/VAL/TEST SPLIT\n",
    "# ========================================\n",
    "print(\"\\nSplitting dataset...\")\n",
    "# First split: 80% train+val, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "# Second split: 80% train, 20% val (of the 80%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.20, random_state=42, stratify=y_temp\n",
    ")\n",
    "print(f\"  Train set:  {X_train.shape[0]:,} devices ({X_train.shape[0]/NUM_DEVICES*100:.1f}%)\")\n",
    "print(f\"    Pass:     {(y_train == 0).sum():,}\")\n",
    "print(f\"    Fail:     {(y_train == 1).sum():,}\")\n",
    "print(f\"\\n  Val set:    {X_val.shape[0]:,} devices ({X_val.shape[0]/NUM_DEVICES*100:.1f}%)\")\n",
    "print(f\"    Pass:     {(y_val == 0).sum():,}\")\n",
    "print(f\"    Fail:     {(y_val == 1).sum():,}\")\n",
    "print(f\"\\n  Test set:   {X_test.shape[0]:,} devices ({X_test.shape[0]/NUM_DEVICES*100:.1f}%)\")\n",
    "print(f\"    Pass:     {(y_test == 0).sum():,}\")\n",
    "print(f\"    Fail:     {(y_test == 1).sum():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3356ba99",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f882a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CONVERT TO PYTORCH TENSORS\n",
    "# ========================================\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "y_val_tensor = torch.LongTensor(y_val)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(f\"\\nData loaders created:\")\n",
    "print(f\"  Batch size:      {BATCH_SIZE}\")\n",
    "print(f\"  Train batches:   {len(train_loader)}\")\n",
    "print(f\"  Val batches:     {len(val_loader)}\")\n",
    "print(f\"  Test batches:    {len(test_loader)}\")\n",
    "# ========================================\n",
    "# EARLY WARNING SIMULATION\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EARLY WARNING SCENARIO\")\n",
    "print(\"=\" * 60)\n",
    "# Goal: Predict failure at cycle 15 (5 cycles early)\n",
    "PREDICTION_CYCLE = 15\n",
    "# Create early prediction dataset (only first 15 cycles)\n",
    "X_test_early = X_test[:, :PREDICTION_CYCLE, :]\n",
    "X_test_early_tensor = torch.FloatTensor(X_test_early)\n",
    "print(f\"\\nEarly prediction setup:\")\n",
    "print(f\"  Full sequence:     {SEQ_LENGTH} cycles\")\n",
    "print(f\"  Prediction point:  Cycle {PREDICTION_CYCLE}\")\n",
    "print(f\"  Early warning:     {SEQ_LENGTH - PREDICTION_CYCLE} cycles ahead\")\n",
    "print(f\"  Test samples:      {X_test_early.shape[0]:,}\")\n",
    "# ========================================\n",
    "# SUMMARY STATISTICS\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "summary_df = pd.DataFrame({\n",
    "    'Split': ['Train', 'Validation', 'Test', 'Total'],\n",
    "    'Total': [len(y_train), len(y_val), len(y_test), NUM_DEVICES],\n",
    "    'Pass': [(y_train==0).sum(), (y_val==0).sum(), (y_test==0).sum(), (y==0).sum()],\n",
    "    'Fail': [(y_train==1).sum(), (y_val==1).sum(), (y_test==1).sum(), (y==1).sum()],\n",
    "    'Fail %': [\n",
    "        f\"{(y_train==1).sum()/len(y_train)*100:.1f}%\",\n",
    "        f\"{(y_val==1).sum()/len(y_val)*100:.1f}%\",\n",
    "        f\"{(y_test==1).sum()/len(y_test)*100:.1f}%\",\n",
    "        f\"{(y==1).sum()/NUM_DEVICES*100:.1f}%\"\n",
    "    ]\n",
    "})\n",
    "print(\"\\n\", summary_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2713 Data preparation complete!\")\n",
    "print(\"=\" * 60)\n",
    "# Save data for later use\n",
    "print(\"\\nSaving processed data...\")\n",
    "torch.save({\n",
    "    'X_train': X_train_tensor,\n",
    "    'y_train': y_train_tensor,\n",
    "    'X_val': X_val_tensor,\n",
    "    'y_val': y_val_tensor,\n",
    "    'X_test': X_test_tensor,\n",
    "    'y_test': y_test_tensor,\n",
    "    'X_test_early': X_test_early_tensor,\n",
    "    'scaler': scaler,\n",
    "    'feature_names': FEATURE_NAMES\n",
    "}, 'sequential_test_data.pt')\n",
    "print(\"\u2713 Data saved to sequential_test_data.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac4a08",
   "metadata": {},
   "source": [
    "# \ud83e\udde0 Part 3: Model Implementations\n",
    "\n",
    "## \ud83d\udcdd What We'll Build\n",
    "\n",
    "We'll implement 4 architectures and compare them:\n",
    "1. **Vanilla RNN** - Baseline (will struggle with 20-step sequences)\n",
    "2. **LSTM** - Gold standard for long sequences\n",
    "3. **GRU** - Efficient alternative (75% of LSTM parameters)\n",
    "4. **Bidirectional LSTM** - Best accuracy (2\u00d7 parameters)\n",
    "\n",
    "**Training Strategy:**\n",
    "- Optimizer: Adam (learning rate = 0.001)\n",
    "- Loss: CrossEntropyLoss (binary classification)\n",
    "- Epochs: 20\n",
    "- Early stopping: Stop if validation loss doesn't improve for 5 epochs\n",
    "- Metrics: Accuracy, Precision, Recall, F1-Score\n",
    "\n",
    "**Model Architecture Pattern:**\n",
    "```\n",
    "Input (batch_size, seq_length=20, input_size=15)\n",
    "    \u2193\n",
    "RNN/LSTM/GRU Layer (hidden_size=128)\n",
    "    \u2193\n",
    "Take final hidden state h_20\n",
    "    \u2193\n",
    "Fully Connected Layer (128 \u2192 2)\n",
    "    \u2193\n",
    "Softmax \u2192 [P(pass), P(fail)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea40b2",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fca4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# PART 3: MODEL IMPLEMENTATIONS\n",
    "# RNN, LSTM, GRU, Bidirectional LSTM\n",
    "# ========================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "# ========================================\n",
    "# MODEL 1: VANILLA RNN\n",
    "# ========================================\n",
    "class VanillaRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RNN for sequence classification\n",
    "    Expected to struggle with long sequences (vanishing gradient)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=1):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True  # Input shape: (batch, seq, features)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        # out: (batch, seq_length, hidden_size)\n",
    "        # hn: (num_layers, batch, hidden_size)\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        # Take output from last time step\n",
    "        out = out[:, -1, :]  # (batch, hidden_size)\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        out = self.fc(out)  # (batch, num_classes)\n",
    "        \n",
    "        return out\n",
    "# ========================================\n",
    "# MODEL 2: LSTM\n",
    "# ========================================\n",
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM for sequence classification\n",
    "    Handles long-term dependencies via cell state\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        # out: (batch, seq_length, hidden_size)\n",
    "        # hn, cn: (num_layers, batch, hidden_size)\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Take output from last time step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a78bd",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# MODEL 3: GRU\n",
    "# ========================================\n",
    "class GRUModel(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU for sequence classification\n",
    "    Simplified gating mechanism (fewer parameters than LSTM)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=1):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate GRU\n",
    "        out, hn = self.gru(x, h0)\n",
    "        \n",
    "        # Take output from last time step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "# ========================================\n",
    "# MODEL 4: BIDIRECTIONAL LSTM\n",
    "# ========================================\n",
    "class BiLSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM for sequence classification\n",
    "    Processes sequence in both directions (forward + backward)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=1):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True  # Key difference!\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer (input = 2*hidden_size because bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states (2*num_layers for bidirectional)\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate bidirectional LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Take output from last time step\n",
    "        # out[:, -1, :] contains [forward_hidden; backward_hidden]\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Pass through fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64462484",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bcbe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# TRAINING FUNCTION\n",
    "# ========================================\n",
    "def train_model(model, train_loader, val_loader, num_epochs=20, learning_rate=0.001, patience=5):\n",
    "    \"\"\"\n",
    "    Train RNN/LSTM/GRU model with early stopping\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        num_epochs: Maximum epochs\n",
    "        learning_rate: Learning rate for Adam optimizer\n",
    "        patience: Early stopping patience (epochs without improvement)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    # Early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"\\nTraining {model.__class__.__name__}...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(train_accuracy)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "            print(f\"  Val Loss:   {avg_val_loss:.4f}, Val Acc:   {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), f'best_{model.__class__.__name__}.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1} (patience={patience})\")\n",
    "                break\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nTraining completed in {training_time:.2f} sec ({training_time/60:.2f} min)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(f'best_{model.__class__.__name__}.pth'))\n",
    "    \n",
    "    history['training_time'] = training_time\n",
    "    history['best_val_loss'] = best_val_loss\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e1685d",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 4\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb5c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# EVALUATION FUNCTION\n",
    "# ========================================\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with metrics\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_predictions, average='binary'\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'inference_time': inference_time,\n",
    "        'samples': len(all_labels)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "# ========================================\n",
    "# INITIALIZE ALL MODELS\n",
    "# ========================================\n",
    "INPUT_SIZE = 15      # 15 parametric features\n",
    "HIDDEN_SIZE = 128    # Hidden units\n",
    "NUM_CLASSES = 2      # Binary classification (pass/fail)\n",
    "NUM_LAYERS = 1       # Single layer RNN/LSTM/GRU\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INITIALIZING MODELS\")\n",
    "print(\"=\" * 60)\n",
    "models = {\n",
    "    'VanillaRNN': VanillaRNN(INPUT_SIZE, HIDDEN_SIZE, NUM_CLASSES, NUM_LAYERS),\n",
    "    'LSTM': LSTMModel(INPUT_SIZE, HIDDEN_SIZE, NUM_CLASSES, NUM_LAYERS),\n",
    "    'GRU': GRUModel(INPUT_SIZE, HIDDEN_SIZE, NUM_CLASSES, NUM_LAYERS),\n",
    "    'BiLSTM': BiLSTMModel(INPUT_SIZE, HIDDEN_SIZE, NUM_CLASSES, NUM_LAYERS)\n",
    "}\n",
    "# Print parameter counts\n",
    "for name, model in models.items():\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Parameters: {num_params:,}\")\n",
    "    print(f\"  Architecture: {model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe29a6",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 5\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b42ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# TRAIN ALL MODELS\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING ALL MODELS\")\n",
    "print(\"=\" * 60)\n",
    "training_histories = {}\n",
    "for name, model in models.items():\n",
    "    history = train_model(model, train_loader, val_loader, num_epochs=20, learning_rate=0.001)\n",
    "    training_histories[name] = history\n",
    "# ========================================\n",
    "# EVALUATE ALL MODELS\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATING ALL MODELS ON TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "evaluation_results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    metrics = evaluate_model(model, test_loader)\n",
    "    evaluation_results[name] = metrics\n",
    "    \n",
    "    print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:  {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  Inference: {metrics['inference_time']:.2f} sec ({metrics['inference_time']/metrics['samples']*1000:.2f} ms/sample)\")\n",
    "# ========================================\n",
    "# VISUALIZATION: TRAINING CURVES\n",
    "# ========================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle(\"Training Curves: All Models\", fontsize=16, fontweight='bold')\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "# Loss curves\n",
    "ax = axes[0, 0]\n",
    "for (name, history), color in zip(training_histories.items(), colors):\n",
    "    ax.plot(history['train_loss'], label=f'{name} Train', color=color, linestyle='-', alpha=0.7)\n",
    "    ax.plot(history['val_loss'], label=f'{name} Val', color=color, linestyle='--', alpha=0.7)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Training and Validation Loss\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "# Accuracy curves\n",
    "ax = axes[0, 1]\n",
    "for (name, history), color in zip(training_histories.items(), colors):\n",
    "    ax.plot(history['train_acc'], label=f'{name} Train', color=color, linestyle='-', alpha=0.7)\n",
    "    ax.plot(history['val_acc'], label=f'{name} Val', color=color, linestyle='--', alpha=0.7)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Training and Validation Accuracy\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "# Final test accuracy comparison\n",
    "ax = axes[1, 0]\n",
    "names = list(evaluation_results.keys())\n",
    "accuracies = [evaluation_results[name]['accuracy'] for name in names]\n",
    "bars = ax.bar(names, accuracies, color=colors, alpha=0.7)\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Test Set Accuracy Comparison\")\n",
    "ax.set_ylim(0.7, 1.0)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "           f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "# Inference speed comparison\n",
    "ax = axes[1, 1]\n",
    "names = list(evaluation_results.keys())\n",
    "speeds = [evaluation_results[name]['inference_time']/evaluation_results[name]['samples']*1000 \n",
    "          for name in names]\n",
    "bars = ax.bar(names, speeds, color=colors, alpha=0.7)\n",
    "ax.set_ylabel(\"Time (ms/sample)\")\n",
    "ax.set_title(\"Inference Speed Comparison\")\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "# Add value labels\n",
    "for bar, speed in zip(bars, speeds):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "           f'{speed:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('rnn_lstm_gru_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n\u2713 Training curves saved to rnn_lstm_gru_comparison.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378cccef",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 6\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CONFUSION MATRICES\n",
    "# ========================================\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "fig.suptitle(\"Confusion Matrices: All Models\", fontsize=16, fontweight='bold')\n",
    "for ax, (name, metrics) in zip(axes, evaluation_results.items()):\n",
    "    cm = metrics['confusion_matrix']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, \n",
    "                xticklabels=['Pass', 'Fail'], yticklabels=['Pass', 'Fail'])\n",
    "    ax.set_title(f\"{name}\\nAcc={metrics['accuracy']:.3f}\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\u2713 Confusion matrices saved to confusion_matrices.png\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2713 All models trained and evaluated!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19754940",
   "metadata": {},
   "source": [
    "# \ud83d\ude80 Part 4: Real-World Projects & Best Practices\n",
    "\n",
    "## \ud83d\udd2c Semiconductor Projects (Post-Silicon Validation)\n",
    "\n",
    "### **Project 1: Adaptive Test Sequence Optimization with LSTM**\n",
    "\n",
    "**Objective:** Dynamically optimize test sequence order based on real-time parametric trends\n",
    "\n",
    "**Business Value:** $30M-$100M/year from 40% test time reduction + early failure detection\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Sequential Test Data (20 cycles \u00d7 15 params)\n",
    "    \u2193\n",
    "Bidirectional LSTM (256 hidden units, 2 layers)\n",
    "    \u2193\n",
    "Attention Mechanism (weight important timesteps)\n",
    "    \u2193\n",
    "Multi-task Output:\n",
    "    \u251c\u2500 Failure prediction (classification)\n",
    "    \u251c\u2500 Time-to-failure estimation (regression)\n",
    "    \u2514\u2500 Next optimal test recommendation (ranking)\n",
    "```\n",
    "\n",
    "**Implementation Strategy:**\n",
    "```python\n",
    "class AdaptiveTestOptimizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Bidirectional LSTM encoder\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=15, hidden_size=256, num_layers=2,\n",
    "            batch_first=True, bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=512, num_heads=8\n",
    "        )\n",
    "        \n",
    "        # Multi-task heads\n",
    "        self.failure_classifier = nn.Linear(512, 2)  # Pass/Fail\n",
    "        self.ttf_regressor = nn.Linear(512, 1)       # Time-to-failure\n",
    "        self.test_ranker = nn.Linear(512, 20)        # Next test priority\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encode sequence\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_out, attn_weights = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Global pooling\n",
    "        pooled = attn_out.mean(dim=1)\n",
    "        \n",
    "        # Multi-task predictions\n",
    "        failure_pred = self.failure_classifier(pooled)\n",
    "        ttf_pred = self.ttf_regressor(pooled)\n",
    "        test_ranking = self.test_ranker(pooled)\n",
    "        \n",
    "        return failure_pred, ttf_pred, test_ranking, attn_weights\n",
    "\n",
    "# Training with multi-task loss\n",
    "def multi_task_loss(failure_pred, ttf_pred, test_ranking, \n",
    "                    failure_label, ttf_label, test_order_label):\n",
    "    # Classification loss\n",
    "    loss_class = nn.CrossEntropyLoss()(failure_pred, failure_label)\n",
    "    \n",
    "    # Regression loss (only for failing devices)\n",
    "    mask = (failure_label == 1).float()\n",
    "    loss_reg = (nn.MSELoss(reduction='none')(ttf_pred.squeeze(), ttf_label) * mask).mean()\n",
    "    \n",
    "    # Ranking loss (learn optimal test order)\n",
    "    loss_rank = nn.BCEWithLogitsLoss()(test_ranking, test_order_label)\n",
    "    \n",
    "    # Combined loss\n",
    "    total_loss = loss_class + 0.5 * loss_reg + 0.3 * loss_rank\n",
    "    return total_loss\n",
    "\n",
    "# Deploy in production\n",
    "def adaptive_test_flow(device_data):\n",
    "    \"\"\"\n",
    "    Real-time test optimization during production\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Run first 5 tests (baseline measurements)\n",
    "    2. LSTM predicts failure probability\n",
    "    3. If P(failure) > 0.8: Skip remaining tests \u2192 Bin as fail (save 75% test time)\n",
    "    4. If P(failure) < 0.2: Run only critical tests (save 50% time)\n",
    "    5. If 0.2 \u2264 P(failure) \u2264 0.8: Run full test suite\n",
    "    6. Update model weekly with new data (active learning)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Run initial tests\n",
    "    initial_data = run_tests(device_data, test_indices=[0, 1, 2, 3, 4])\n",
    "    \n",
    "    # Predict failure\n",
    "    failure_prob, ttf, test_priority, _ = model(initial_data)\n",
    "    \n",
    "    if failure_prob[1] > 0.8:  # High confidence of failure\n",
    "        decision = \"FAIL\"\n",
    "        remaining_tests = []  # Skip all\n",
    "        saved_time = 0.75\n",
    "    elif failure_prob[0] > 0.8:  # High confidence of pass\n",
    "        # Run only top-5 critical tests (ranked by model)\n",
    "        critical_tests = test_priority.argsort(descending=True)[:5]\n",
    "        remaining_tests = critical_tests.tolist()\n",
    "        saved_time = 0.50\n",
    "    else:  # Uncertain \u2192 Run full suite\n",
    "        remaining_tests = list(range(5, 20))\n",
    "        saved_time = 0.0\n",
    "    \n",
    "    # Execute remaining tests\n",
    "    final_data = run_tests(device_data, test_indices=remaining_tests)\n",
    "    \n",
    "    return decision, saved_time\n",
    "```\n",
    "\n",
    "**Success Metrics:**\n",
    "- **Test time reduction:** 40% average (60% for clear failures, 20% for passes)\n",
    "- **Early detection accuracy:** \u226595% (5 cycles ahead)\n",
    "- **False skip rate:** <2% (avoid missing marginal failures)\n",
    "- **ROI:** $30M-$100M/year from faster test throughput\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 2: Wafer-Level Yield Forecasting with GRU**\n",
    "\n",
    "**Objective:** Predict daily wafer yield based on 30-day process parameter time series\n",
    "\n",
    "**Business Value:** $50M-$200M/year from proactive process adjustments\n",
    "\n",
    "**Data:**\n",
    "- 30-day rolling window of wafer fabrication parameters\n",
    "- 50 process variables (temperature, pressure, gas flow, etch rate, etc.)\n",
    "- Daily wafer yield (% passing devices)\n",
    "\n",
    "**Architecture:**\n",
    "```python\n",
    "class YieldForecaster(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-layer GRU (faster than LSTM, sufficient for 30-day sequences)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=50,    # 50 process variables\n",
    "            hidden_size=256,\n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # Forecast head (predict next 7 days)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 7)  # 7-day forecast\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 30 days, 50 features)\n",
    "        gru_out, _ = self.gru(x)\n",
    "        \n",
    "        # Take last hidden state\n",
    "        last_hidden = gru_out[:, -1, :]\n",
    "        \n",
    "        # Forecast\n",
    "        x = torch.relu(self.fc1(last_hidden))\n",
    "        forecast = self.fc2(x)  # (batch, 7) - 7-day yield forecast\n",
    "        \n",
    "        return forecast\n",
    "\n",
    "# Anomaly detection (flag unusual patterns)\n",
    "def detect_yield_anomalies(forecast, historical_yield):\n",
    "    \"\"\"\n",
    "    Alert if forecasted yield deviates significantly from historical baseline\n",
    "    \"\"\"\n",
    "    mean_yield = historical_yield.mean()\n",
    "    std_yield = historical_yield.std()\n",
    "    \n",
    "    # Z-score for each forecast day\n",
    "    z_scores = (forecast - mean_yield) / std_yield\n",
    "    \n",
    "    # Flag if |z| > 2 (beyond 2 standard deviations)\n",
    "    anomalies = (z_scores.abs() > 2).any(dim=1)\n",
    "    \n",
    "    return anomalies, z_scores\n",
    "\n",
    "# Production deployment\n",
    "def daily_yield_monitoring():\n",
    "    \"\"\"\n",
    "    Run every morning to forecast yield and trigger alerts\n",
    "    \"\"\"\n",
    "    # Fetch last 30 days of process data\n",
    "    process_data = fetch_fab_data(days=30)\n",
    "    \n",
    "    # Preprocess\n",
    "    X = preprocess_process_data(process_data)\n",
    "    \n",
    "    # Forecast next 7 days\n",
    "    forecast = model(X)\n",
    "    \n",
    "    # Check for anomalies\n",
    "    historical = fetch_historical_yield(days=365)\n",
    "    anomalies, z_scores = detect_yield_anomalies(forecast, historical)\n",
    "    \n",
    "    if anomalies.any():\n",
    "        # Alert fab engineers\n",
    "        send_alert(\n",
    "            subject=\"Yield Anomaly Detected\",\n",
    "            message=f\"Forecasted yield drop: {forecast.min():.2f}% (z={z_scores.min():.2f})\",\n",
    "            recipients=[\"fab_engineer@company.com\", \"process_manager@company.com\"]\n",
    "        )\n",
    "        \n",
    "        # Recommend root cause investigation\n",
    "        feature_importance = compute_shap_values(model, X)\n",
    "        top_features = feature_importance.argsort(descending=True)[:5]\n",
    "        \n",
    "        print(\"Investigate these process parameters:\")\n",
    "        for i, feat_idx in enumerate(top_features):\n",
    "            print(f\"  {i+1}. {FEATURE_NAMES[feat_idx]}: {feature_importance[feat_idx]:.3f}\")\n",
    "    \n",
    "    return forecast\n",
    "```\n",
    "\n",
    "**Success Metrics:**\n",
    "- **Forecast accuracy:** RMSE < 2% (absolute yield percentage)\n",
    "- **Anomaly detection:** 90% recall (catch yield drops), 85% precision (minimize false alarms)\n",
    "- **Lead time:** 7-day advance warning (vs 1-day with reactive monitoring)\n",
    "- **Cost savings:** $50M-$200M/year from proactive process tuning\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 3: Device Lifetime Prediction from Reliability Test Sequences**\n",
    "\n",
    "**Objective:** Predict device lifetime (MTTF) from accelerated stress test time series\n",
    "\n",
    "**Business Value:** $10M-$40M/year from improved reliability binning + reduced RMA costs\n",
    "\n",
    "**Architecture:** LSTM with attention (focus on degradation inflection points)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 4: Multi-Fab Process Drift Detection**\n",
    "\n",
    "**Objective:** Detect process drift across 5 fabs using synchronized parametric time series\n",
    "\n",
    "**Architecture:** Bidirectional GRU with contrastive learning (learn fab-invariant features)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf10 General AI/ML Projects\n",
    "\n",
    "### **Project 5: Stock Price Prediction with LSTM**\n",
    "\n",
    "**Objective:** Forecast next 7-day stock prices from 60-day historical data\n",
    "\n",
    "**Architecture:** Stacked LSTM (3 layers, 256 hidden units) + Dropout (0.3)\n",
    "\n",
    "**Challenges:** Non-stationary data (use differencing), high noise (ensemble 5 models)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 6: Natural Language Generation (Text Completion)**\n",
    "\n",
    "**Objective:** Character-level language model (predict next character given sequence)\n",
    "\n",
    "**Architecture:** GRU (512 hidden, 2 layers) trained on Wikipedia text corpus\n",
    "\n",
    "**Application:** Auto-complete, code generation, creative writing\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 7: Video Activity Recognition**\n",
    "\n",
    "**Objective:** Classify human actions from video sequences (30 FPS, 5-sec clips)\n",
    "\n",
    "**Architecture:** \n",
    "- CNN (ResNet-50) extracts spatial features from each frame\n",
    "- LSTM (256 hidden) models temporal dynamics across frames\n",
    "- Final FC layer classifies 101 action classes\n",
    "\n",
    "**Dataset:** UCF-101 (13K videos, 101 actions)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 8: Music Generation with LSTM**\n",
    "\n",
    "**Objective:** Generate piano music conditioned on genre/mood\n",
    "\n",
    "**Architecture:** Bidirectional LSTM (512 hidden, 3 layers) trained on MIDI sequences\n",
    "\n",
    "**Output:** Note-by-note music generation (pitch + duration + velocity)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf93 Key Takeaways & Best Practices\n",
    "\n",
    "### **Architecture Selection Guide**\n",
    "\n",
    "| Sequence Length | Memory Needs | Parameters | Recommendation | Rationale |\n",
    "|-----------------|--------------|------------|----------------|-----------|\n",
    "| **Short (\u226410 steps)** | Low | Any | Vanilla RNN or GRU | Vanishing gradient not critical |\n",
    "| **Medium (10-50)** | Medium | Budget-conscious | **GRU** | 75% of LSTM params, 90% accuracy |\n",
    "| **Medium (10-50)** | Medium | Best accuracy | **LSTM** | Gold standard, proven |\n",
    "| **Long (50-200)** | High | Any | **LSTM or Transformer** | LSTM cell state prevents vanishing |\n",
    "| **Very long (>200)** | High | Any | **Transformer** | Self-attention, no sequential bottleneck |\n",
    "| **Bidirectional OK** | Any | 2\u00d7 budget | **BiLSTM/BiGRU** | Best accuracy (+2-5% over unidirectional) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Best Practices**\n",
    "\n",
    "**1. Gradient Clipping** (prevent exploding gradients):\n",
    "```python\n",
    "# Clip gradients to max norm = 1.0\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "```\n",
    "\n",
    "**2. Layer Normalization** (stabilize training):\n",
    "```python\n",
    "class LSTMWithLayerNorm(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        normalized_out = self.layer_norm(lstm_out)\n",
    "        return normalized_out, (hn, cn)\n",
    "```\n",
    "\n",
    "**3. Learning Rate Scheduling** (reduce LR when validation loss plateaus):\n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "# After each epoch\n",
    "scheduler.step(val_loss)\n",
    "```\n",
    "\n",
    "**4. Dropout for Regularization**:\n",
    "```python\n",
    "self.lstm = nn.LSTM(input_size, hidden_size, num_layers=2, dropout=0.2)\n",
    "```\n",
    "\n",
    "**5. Batch Size Tuning**:\n",
    "- Small batches (16-32): Better generalization, slower training\n",
    "- Large batches (128-256): Faster training, may overfit\n",
    "- **Recommendation:** Start with 64, tune based on validation loss\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Pitfalls & Solutions**\n",
    "\n",
    "| Problem | Symptom | Solution |\n",
    "|---------|---------|----------|\n",
    "| **Vanishing gradient** | Training stalls, loss doesn't decrease | Use LSTM/GRU, gradient clipping, reduce sequence length |\n",
    "| **Exploding gradient** | NaN loss after few iterations | Gradient clipping (max_norm=1.0), lower LR |\n",
    "| **Overfitting** | Train acc=95%, Val acc=70% | Dropout (0.2-0.5), L2 regularization, more data |\n",
    "| **Slow convergence** | Loss decreases slowly | Increase LR (0.001 \u2192 0.01), use Adam optimizer, batch normalization |\n",
    "| **Mode collapse (generation)** | Model outputs repetitive text | Temperature sampling, nucleus sampling, beam search |\n",
    "| **Memory errors** | CUDA OOM | Reduce batch size, use gradient checkpointing, smaller hidden size |\n",
    "\n",
    "---\n",
    "\n",
    "### **Production Deployment Checklist**\n",
    "\n",
    "\u2705 **Model Optimization:**\n",
    "- [ ] Convert to ONNX for framework independence\n",
    "- [ ] Quantize to INT8 (3-4\u00d7 smaller, 2\u00d7 faster, <1% accuracy loss)\n",
    "- [ ] Prune redundant connections (remove 30-50% weights)\n",
    "- [ ] Use optimized LSTM implementations (cuDNN, MKL-DNN)\n",
    "\n",
    "\u2705 **Inference Optimization:**\n",
    "- [ ] Batch multiple sequences together (process 32-64 at once)\n",
    "- [ ] Use stateful LSTMs (preserve hidden state across batches for streaming)\n",
    "- [ ] Cache hidden states for sequential predictions\n",
    "- [ ] Compile model with TorchScript/TensorRT\n",
    "\n",
    "\u2705 **Monitoring & Maintenance:**\n",
    "- [ ] Log prediction confidence distributions (detect distribution drift)\n",
    "- [ ] Track sequence lengths (ensure within training range)\n",
    "- [ ] A/B test new model versions (gradual rollout)\n",
    "- [ ] Active learning: flag low-confidence predictions for labeling\n",
    "\n",
    "\u2705 **Edge Deployment:**\n",
    "- [ ] Use GRU instead of LSTM (25% fewer parameters)\n",
    "- [ ] Reduce hidden size (256 \u2192 128, minimal accuracy loss)\n",
    "- [ ] Quantize to FP16 or INT8\n",
    "- [ ] Target <10 MB model size for mobile/IoT devices\n",
    "\n",
    "---\n",
    "\n",
    "### **Performance Optimization: Speed vs Accuracy**\n",
    "\n",
    "| Configuration | Parameters | Speed (samples/sec) | Accuracy | Use Case |\n",
    "|---------------|------------|---------------------|----------|----------|\n",
    "| **GRU-64** | 15K | 1200 | 82% | Edge devices, real-time |\n",
    "| **GRU-128** | 55K | 800 | 87% | Balanced (recommended) |\n",
    "| **LSTM-128** | 73K | 600 | 88% | Standard production |\n",
    "| **LSTM-256** | 289K | 200 | 90% | High-accuracy applications |\n",
    "| **BiLSTM-256** | 578K | 100 | 92% | Best accuracy (offline) |\n",
    "\n",
    "**Recommendation for production:**\n",
    "- Start with **GRU-128** (good accuracy, fast)\n",
    "- Upgrade to **LSTM-128** if accuracy critical (+1%)\n",
    "- Use **BiLSTM** only for offline batch processing\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcda What's Next?\n",
    "\n",
    "**Upcoming Notebooks:**\n",
    "- **057: Sequence-to-Sequence Models** \u2192 Encoder-decoder architecture, attention mechanism\n",
    "- **058: Transformers & Attention** \u2192 Self-attention, multi-head attention, BERT/GPT foundations\n",
    "- **059: Time Series Forecasting** \u2192 ARIMA, Prophet, N-BEATS, Temporal Fusion Transformers\n",
    "- **060: Generative RNNs** \u2192 Text generation, music generation, variational RNNs\n",
    "\n",
    "---\n",
    "\n",
    "## \u2705 Learning Objectives Review\n",
    "\n",
    "1. \u2705 **Sequential Data Processing** - Temporal dependencies, memory mechanism\n",
    "2. \u2705 **RNN Architecture** - Hidden state recurrence, BPTT\n",
    "3. \u2705 **Vanishing Gradient** - Why vanilla RNNs fail, $(W_{hh})^{20} \\rightarrow 0$\n",
    "4. \u2705 **LSTM Networks** - Cell state, forget/input/output gates, gradient flow\n",
    "5. \u2705 **GRU Networks** - Reset/update gates, 75% parameters of LSTM\n",
    "6. \u2705 **Bidirectional RNNs** - Forward + backward context, 2\u00d7 parameters\n",
    "7. \u2705 **Semiconductor Applications** - Test sequence analysis, yield forecasting\n",
    "8. \u2705 **Production Deployment** - ONNX export, quantization, edge optimization\n",
    "\n",
    "**Key Skill Acquired:** Build production-grade RNN/LSTM/GRU models for sequential data analysis!\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcd6 Additional Resources\n",
    "\n",
    "**Must-Read Papers:**\n",
    "- \"Long Short-Term Memory\" (Hochreiter & Schmidhuber, 1997) - Original LSTM paper\n",
    "- \"Learning Phrase Representations using RNN Encoder-Decoder\" (Cho et al., 2014) - GRU introduction\n",
    "- \"Sequence to Sequence Learning with Neural Networks\" (Sutskever et al., 2014) - Seq2seq architecture\n",
    "\n",
    "**Courses & Tutorials:**\n",
    "- CS224n (Stanford) - NLP with Deep Learning (RNN lectures)\n",
    "- Fast.ai Deep Learning - RNN for text classification\n",
    "- PyTorch LSTM Tutorial - https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "\n",
    "**Libraries & Tools:**\n",
    "- **PyTorch** - https://pytorch.org/docs/stable/nn.html#recurrent-layers\n",
    "- **TensorFlow** - https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
    "- **Hugging Face** - https://huggingface.co/transformers (Transformer-based models)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Final Summary\n",
    "\n",
    "**RNN Family Mastery:**\n",
    "- **Vanilla RNN:** Simple but struggles with long sequences (vanishing gradient)\n",
    "- **LSTM:** Gold standard (cell state bypasses vanishing gradient)\n",
    "- **GRU:** Efficient alternative (75% parameters, 90% accuracy of LSTM)\n",
    "- **Bidirectional:** Best accuracy (forward + backward context)\n",
    "\n",
    "**Semiconductor Impact:**\n",
    "- **Test optimization:** $30M-$100M/year from adaptive test sequencing\n",
    "- **Yield forecasting:** $50M-$200M/year from proactive process tuning\n",
    "- **Reliability prediction:** $10M-$40M/year from improved lifetime estimation\n",
    "\n",
    "**You're now ready to build sequential models for time-series analysis!** \ud83d\ude80\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations on completing Notebook 056!** \ud83c\udf89\n",
    "\n",
    "Next notebook: **057_Seq2Seq_Attention.ipynb** - Encoder-decoder architectures for sequence transformation!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}