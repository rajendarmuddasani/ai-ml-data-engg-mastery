{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 079: RAG Fundamentals\n\n",
    "# Introduction\n",
    "\n",
    "In this notebook, we will explore the fundamentals of Retrieval-Augmented Generation (RAG), a powerful architecture that combines retrieval mechanisms with generative models. RAG is particularly useful for tasks that require precise information retrieval followed by generative responses, such as technical documentation search, failure analysis, and test parameter recommendations.\n",
    "\n",
    "## Workflow Overview\n",
    "The RAG architecture involves several key components:\n",
    "- **Document Chunking**: Breaking down documents into manageable pieces.\n",
    "- **Embedding Generation**: Creating vector representations of chunks.\n",
    "- **Semantic Search**: Using vector embeddings to find relevant information.\n",
    "- **Generative Response**: Using a language model to generate responses based on retrieved information.\n",
    "\n",
    "```mermaid\n",
    "graph TD;\n",
    "    A[Input Query] -->|Embedding| B(Vector Database);\n",
    "    B -->|Retrieve| C[Relevant Documents];\n",
    "    C -->|Context| D[Generative Model];\n",
    "    D --> E[Output]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Foundation\n",
    "\n",
    "The core of RAG involves vector embeddings and semantic similarity. Here, we define the mathematical foundation for these concepts.\n",
    "\n",
    "## Vector Embeddings\n",
    "A vector embedding \\( \\mathbf{v} \\in \\mathbb{R}^d \\) is a representation of a document or query in a high-dimensional space.\n",
    "\n",
    "## Semantic Similarity\n",
    "The similarity between two vectors \\( \\mathbf{v}_1 \\) and \\( \\mathbf{v}_2 \\) can be computed using cosine similarity:\n",
    "\n",
    "\\[\n",
    "\\text{similarity}(\\mathbf{v}_1, \\mathbf{v}_2) = \\frac{\\mathbf{v}_1 \\cdot \\mathbf{v}_2}{\\|\\mathbf{v}_1\\| \\|\\mathbf{v}_2\\|}\n",
    "\\]\n",
    "\n",
    "This measures the cosine of the angle between two vectors, providing a measure of similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Scratch Implementation: NumPy Embeddings\n",
    "import numpy as np\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"Technical documentation on Vdd specifications.\",\n",
    "    \"Failure analysis report for recent tests.\",\n",
    "    \"Design verification processes and protocols.\"\n",
    "]\n",
    "\n",
    "# Simple word embedding using character count vectorization\n",
    "def simple_embedding(doc):\n",
    "    vector = np.zeros(26)\n",
    "    for char in doc.lower():\n",
    "        if char.isalpha():\n",
    "            vector[ord(char) - ord('a')] += 1\n",
    "    return vector\n",
    "\n",
    "# Create embeddings for all documents\n",
    "document_embeddings = np.array([simple_embedding(doc) for doc in documents])\n",
    "print(\"Document Embeddings:\", document_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Implementation: Semantic Search with FAISS\n",
    "\n",
    "For production systems, we use libraries like FAISS for efficient similarity search. FAISS is optimized for high-dimensional vector searches and is widely used in semantic search applications.\n",
    "\n",
    "We will demonstrate how to index and query document embeddings using FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS Implementation\n",
    "import faiss\n",
    "\n",
    "# Convert document embeddings to float32\n",
    "document_embeddings = document_embeddings.astype('float32')\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(document_embeddings.shape[1])\n",
    "index.add(document_embeddings)\n",
    "\n",
    "# Query embedding\n",
    "query = \"What are the Vdd specifications?\"\n",
    "query_embedding = simple_embedding(query).astype('float32').reshape(1, -1)\n",
    "\n",
    "# Search\n",
    "D, I = index.search(query_embedding, k=1)\n",
    "print(\"Closest document index:\", I)\n",
    "print(\"Closest document:\", documents[I[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Silicon Validation Examples\n",
    "\n",
    "## Technical Documentation Search\n",
    "The RAG system can be used to search for technical specifications from datasheets, enabling engineers to quickly find relevant information based on test parameters.\n",
    "\n",
    "## Failure Analysis Report Retrieval\n",
    "Retrieve relevant failure analysis reports to diagnose issues during semiconductor testing.\n",
    "\n",
    "## Test Parameter Recommendation Engine\n",
    "Based on historical test data and outcomes, recommend optimal test parameters for new silicon wafers.\n",
    "\n",
    "## Design Verification Query Assistant\n",
    "Assist engineers in verifying design specifications against test results by retrieving pertinent documents and reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General AI/ML Examples\n",
    "\n",
    "Beyond post-silicon applications, RAG architectures can be utilized in various domains:\n",
    "- **Customer Support**: Retrieve relevant FAQs and generate responses.\n",
    "- **Legal Document Analysis**: Extract case law or statutes relevant to a legal query.\n",
    "- **Academic Research**: Find related research papers and generate summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation & Diagnostics\n",
    "\n",
    "To evaluate the performance of a RAG system, we consider metrics such as:\n",
    "- **Precision and Recall**: Measure the accuracy of retrieved documents.\n",
    "- **Response Coherence**: Evaluate the quality of generated responses.\n",
    "- **Latency**: Assess the time taken to retrieve and generate responses.\n",
    "\n",
    "Visualization tools such as confusion matrices and response timing charts can provide insights into system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Evaluation: Precision and Recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Assume ground truth and predictions\n",
    "ground_truth = [1, 0, 1]\n",
    "predictions = [1, 0, 1]\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(ground_truth, predictions)\n",
    "recall = recall_score(ground_truth, predictions)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-World Projects\n",
    "\n",
    "Here are several project ideas that leverage RAG architecture:\n",
    "\n",
    "1. **Automated Datasheet Analyzer**: Develop a system to parse and retrieve specific parameters from extensive datasheets.\n",
    "\n",
    "2. **Failure Analysis Assistant**: Create a tool that uses RAG to assist engineers in finding relevant failure reports and solutions.\n",
    "\n",
    "3. **Test Optimization Tool**: Implement a RAG-based engine that recommends test parameters to optimize yield.\n",
    "\n",
    "4. **Design Verification Helper**: Build a query assistant that verifies design parameters against test results.\n",
    "\n",
    "5. **Legal Document Retrieval System**: Use RAG to enhance legal research by retrieving and summarizing relevant case laws.\n",
    "\n",
    "6. **Academic Paper Finder**: Develop a system that locates and summarizes academic research papers based on user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices & Takeaways\n",
    "\n",
    "- **Efficient Chunking**: Ensure documents are chunked in a way that preserves context without overwhelming the embedding model.\n",
    "- **Embedding Quality**: Use high-quality embeddings to improve retrieval accuracy.\n",
    "- **Performance Monitoring**: Continuously evaluate system performance using appropriate metrics.\n",
    "- **Scalability**: Design the RAG system to handle increasing volumes of data and queries efficiently.\n",
    "\n",
    "By following these best practices, you can build robust RAG systems that effectively combine retrieval and generation capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 RAG System Components Deep Dive\n",
    "\n",
    "### 1. Document Chunking Strategies\n",
    "\n",
    "**Fixed-Size Chunking:**\n",
    "```python\n",
    "chunk_size = 512  # tokens\n",
    "overlap = 50      # token overlap between chunks\n",
    "```\n",
    "- \u2705 Simple, predictable\n",
    "- \u274c May break semantic units\n",
    "\n",
    "**Semantic Chunking:**\n",
    "- Split by paragraphs, sections, or topics\n",
    "- \u2705 Preserves context\n",
    "- \u2705 Better retrieval quality\n",
    "\n",
    "**Sliding Window:**\n",
    "- Overlap helps with context preservation\n",
    "- Prevents information loss at boundaries\n",
    "\n",
    "**Post-Silicon Example:**\n",
    "- Test spec: Chunk by test category (voltage, timing, power)\n",
    "- Debug logs: Chunk by failure mode or error code\n",
    "- Equipment manuals: Chunk by section/subsection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Embedding Models Comparison\n",
    "\n",
    "| Model | Dimensions | Max Tokens | Use Case |\n",
    "|-------|------------|------------|----------|\n",
    "| **text-embedding-ada-002** | 1536 | 8191 | General purpose (OpenAI) |\n",
    "| **all-MiniLM-L6-v2** | 384 | 256 | Fast, lightweight (local) |\n",
    "| **BGE-large-en-v1.5** | 1024 | 512 | High quality (open-source) |\n",
    "| **E5-large-v2** | 1024 | 512 | Multilingual support |\n",
    "\n",
    "**Performance Metrics:**\n",
    "- **Retrieval Recall**: How many relevant docs are found\n",
    "- **Precision**: How many found docs are relevant\n",
    "- **Latency**: Embedding generation time\n",
    "\n",
    "**Cost Considerations:**\n",
    "- OpenAI embeddings: $0.0001 per 1K tokens\n",
    "- Local models: Free but require GPU/compute\n",
    "- Trade-off: Quality vs. cost vs. latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare embedding models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load different models\n",
    "model_small = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model_large = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "\n",
    "query = \"What causes voltage droops during stress tests?\"\n",
    "\n",
    "# Generate embeddings\n",
    "emb_small = model_small.encode(query)\n",
    "emb_large = model_large.encode(query)\n",
    "\n",
    "print(f\"Small model: {len(emb_small)} dimensions\")\n",
    "print(f\"Large model: {len(emb_large)} dimensions\")\n",
    "print(f\"\\nMemory: {emb_small.nbytes / 1024:.2f} KB vs {emb_large.nbytes / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Vector Database Selection\n",
    "\n",
    "**Key Factors:**\n",
    "\n",
    "1. **Scale Requirements**\n",
    "   - <1M vectors: Chroma, FAISS (local)\n",
    "   - 1M-100M: Pinecone, Weaviate (cloud)\n",
    "   - >100M: Milvus, Qdrant (distributed)\n",
    "\n",
    "2. **Query Latency**\n",
    "   - HNSW algorithm: <10ms for 1M vectors\n",
    "   - IVF algorithm: <50ms for 10M vectors\n",
    "   - Quantization: 4x faster, slight accuracy loss\n",
    "\n",
    "3. **Features Needed**\n",
    "   - Metadata filtering: All support\n",
    "   - Hybrid search (vector + keyword): Weaviate, Qdrant\n",
    "   - Multi-tenancy: Pinecone, Weaviate\n",
    "   - On-premise deployment: Milvus, Qdrant\n",
    "\n",
    "**Cost Comparison (1M vectors, 1536 dims):**\n",
    "- FAISS (local): Free, ~6GB RAM\n",
    "- Chroma (local): Free, ~8GB RAM\n",
    "- Pinecone: ~$70/month\n",
    "- Weaviate Cloud: ~$25/month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Key Takeaways\n",
    "\n",
    "**RAG solves the LLM knowledge problem:**\n",
    "- \u2705 Up-to-date information (no training required)\n",
    "- \u2705 Domain-specific knowledge integration\n",
    "- \u2705 Reduced hallucinations (grounded in sources)\n",
    "- \u2705 Explainable answers (citations provided)\n",
    "\n",
    "**Core RAG Pipeline:**\n",
    "1. Document chunking \u2192 2. Embedding \u2192 3. Vector storage\n",
    "4. Query embedding \u2192 5. Similarity search \u2192 6. Context + LLM\n",
    "\n",
    "**Performance Benchmarks:**\n",
    "- Generic LLM: 45-55% accuracy on domain questions\n",
    "- RAG system: 75-85% accuracy (30-40% improvement)\n",
    "- Production latency: 1-3 seconds end-to-end\n",
    "\n",
    "**Business Value:**\n",
    "- Reduces expert dependency by 60%\n",
    "- Accelerates onboarding by 3-4 months\n",
    "- Typical ROI: $2-5M annually for engineering teams\n",
    "\n",
    "**Next Steps:**\n",
    "- 080: Advanced RAG (re-ranking, query expansion)\n",
    "- 081: Conversational RAG (chat memory)\n",
    "- 084: Domain-specific RAG (fine-tuning)\n",
    "\n",
    "---\n",
    "Ready to build production RAG systems! \ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}