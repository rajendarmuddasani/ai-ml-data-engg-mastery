{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8462093",
   "metadata": {},
   "source": [
    "# 078: Multimodal Large Language Models\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** vision-language models and cross-modal alignment\n",
    "- **Implement** CLIP for zero-shot image classification\n",
    "- **Build** image captioning systems (encoder-decoder architecture)\n",
    "- **Create** Visual Question Answering (VQA) systems\n",
    "- **Apply** multimodal models to semiconductor wafer map analysis\n",
    "- **Evaluate** multimodal systems using BLEU, ROUGE, and visual metrics\n",
    "\n",
    "## üñºÔ∏è What are Multimodal LLMs?\n",
    "\n",
    "**Multimodal LLMs** process and generate content across multiple modalities:\n",
    "- üëÅÔ∏è **Vision** - Images, videos, diagrams\n",
    "- üìù **Text** - Natural language, code\n",
    "- üîä **Audio** - Speech, sounds (not covered in this notebook)\n",
    "\n",
    "**Key capabilities:**\n",
    "- Image captioning (\"A dog playing in the park\")\n",
    "- Visual question answering (\"What color is the car?\")\n",
    "- Image-text retrieval (search images with text descriptions)\n",
    "- Vision-grounded text generation (stories from images)\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Automated Failure Analysis Reports**\n",
    "- Input: Wafer map image + test data\n",
    "- Output: Natural language failure report with root cause analysis\n",
    "- Value: Reduce engineer time from 2 hours ‚Üí 15 minutes per failure\n",
    "\n",
    "**Visual Test Documentation Search**\n",
    "- Input: Text query \"show me ring defect patterns\"\n",
    "- Output: Retrieve similar wafer maps from historical database\n",
    "- Value: 10√ó faster pattern matching vs manual search\n",
    "\n",
    "**Defect Classification with Context**\n",
    "- Input: Wafer map + question \"What type of defect is this?\"\n",
    "- Output: \"Ring pattern indicating chamber conditioning issue\"\n",
    "- Value: Standardize defect classification across teams\n",
    "\n",
    "**Parametric Correlation Explanation**\n",
    "- Input: Scatter plot + \"Why do these parameters correlate?\"\n",
    "- Output: Technical explanation grounded in test physics\n",
    "- Value: Enable non-experts to interpret complex data\n",
    "\n",
    "## üîÑ Multimodal Architecture Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Image] --> B[Vision Encoder]\n",
    "    B --> C[Visual Features]\n",
    "    \n",
    "    D[Text] --> E[Language Model]\n",
    "    E --> F[Text Features]\n",
    "    \n",
    "    C --> G[Cross-Modal Fusion]\n",
    "    F --> G\n",
    "    \n",
    "    G --> H[Multimodal Understanding]\n",
    "    H --> I[Generated Response]\n",
    "    \n",
    "    style B fill:#e1f5ff\n",
    "    style E fill:#fff4e1\n",
    "    style G fill:#f0e1ff\n",
    "    style I fill:#e1ffe1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- 072: GPT & Large Language Models (language generation)\n",
    "- 073: Vision Transformers (image encoding)\n",
    "- 058: Transformers & Self-Attention (attention mechanism)\n",
    "\n",
    "**Next Steps:**\n",
    "- 079: RAG Fundamentals (retrieval-augmented generation)\n",
    "- 083: AI Agents (multimodal agents)\n",
    "- 085: Vector Databases (image embedding search)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Required Libraries\n",
    "\n",
    "### Core Dependencies\n",
    "```python\n",
    "# Deep Learning\n",
    "torch>=2.0.0\n",
    "torchvision>=0.15.0\n",
    "transformers>=4.30.0\n",
    "\n",
    "# Vision & NLP\n",
    "pillow>=9.0.0\n",
    "opencv-python>=4.7.0\n",
    "nltk>=3.8.0\n",
    "datasets>=2.12.0\n",
    "\n",
    "# CLIP & Multimodal\n",
    "clip @ git+https://github.com/openai/CLIP.git\n",
    "sentence-transformers>=2.2.0\n",
    "\n",
    "# Evaluation & Metrics\n",
    "pycocotools>=2.0.0\n",
    "torchmetrics>=0.11.0\n",
    "\n",
    "# Utilities\n",
    "numpy>=1.24.0\n",
    "matplotlib>=3.7.0\n",
    "seaborn>=0.12.0\n",
    "tqdm>=4.65.0\n",
    "```\n",
    "\n",
    "### Installation\n",
    "```bash\n",
    "pip install torch torchvision transformers pillow opencv-python\n",
    "pip install nltk datasets sentence-transformers pycocotools torchmetrics\n",
    "pip install git+https://github.com/openai/CLIP.git\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Let's build powerful multimodal AI systems! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0828e795",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Mathematical Foundation\n",
    "\n",
    "### Vision-Language Alignment (CLIP)\n",
    "\n",
    "**Contrastive Learning Objective:**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{contrastive}} = -\\sum_{i=1}^{N} \\left[ \\log \\frac{\\exp(\\text{sim}(v_i, t_i) / \\tau)}{\\sum_{j=1}^{N} \\exp(\\text{sim}(v_i, t_j) / \\tau)} \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $v_i$ = Visual embedding for image $i$\n",
    "- $t_i$ = Text embedding for caption $i$\n",
    "- $\\text{sim}(v, t) = \\frac{v \\cdot t}{\\|v\\| \\|t\\|}$ = Cosine similarity\n",
    "- $\\tau$ = Temperature parameter (controls distribution sharpness)\n",
    "- $N$ = Batch size\n",
    "\n",
    "**Intuition:** Match image-text pairs while separating non-matching pairs.\n",
    "\n",
    "### Visual Question Answering (VQA)\n",
    "\n",
    "**Attention-based Fusion:**\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $Q$ = Query (from text)\n",
    "- $K, V$ = Key, Value (from image features)\n",
    "- $d_k$ = Key dimension\n",
    "\n",
    "**Multimodal Fusion:**\n",
    "\n",
    "$$\n",
    "h_{\\text{multi}} = \\text{FFN}([h_{\\text{text}}; h_{\\text{vision}}; h_{\\text{cross}}])\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $h_{\\text{text}}$ = Text-only features\n",
    "- $h_{\\text{vision}}$ = Vision-only features\n",
    "- $h_{\\text{cross}}$ = Cross-modal attention output\n",
    "- $[;]$ = Concatenation\n",
    "- $\\text{FFN}$ = Feed-forward network\n",
    "\n",
    "### Loss Functions\n",
    "\n",
    "**Image Captioning (Autoregressive):**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{caption}} = -\\sum_{t=1}^{T} \\log P(w_t | w_{<t}, I)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $w_t$ = Token at position $t$\n",
    "- $w_{<t}$ = Previous tokens\n",
    "- $I$ = Image features\n",
    "- $T$ = Sequence length\n",
    "\n",
    "**VQA (Classification):**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{VQA}} = -\\sum_{c=1}^{C} y_c \\log \\hat{y}_c\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $C$ = Number of answer classes\n",
    "- $y_c$ = Ground truth (one-hot)\n",
    "- $\\hat{y}_c$ = Predicted probability for class $c$\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Multimodal Architecture Components\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    subgraph \"Vision Encoder\"\n",
    "        A[Image Input] --> B[CNN/ViT]\n",
    "        B --> C[Visual Features]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Language Model\"\n",
    "        D[Text Input] --> E[Token Embeddings]\n",
    "        E --> F[Transformer Decoder]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Fusion Layer\"\n",
    "        C --> G[Cross-Attention]\n",
    "        E --> G\n",
    "        G --> H[Multimodal Features]\n",
    "    end\n",
    "    \n",
    "    H --> F\n",
    "    F --> I[Generated Text]\n",
    "    \n",
    "    style G fill:#4ecdc4,stroke:#0d7377,stroke-width:2px\n",
    "    style H fill:#ffe66d,stroke:#ff6b6b,stroke-width:2px\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Vision Encoder** (e.g., CLIP ViT, ResNet)\n",
    "   - Extracts visual features from images\n",
    "   - Pre-trained on large image datasets\n",
    "   - Output: High-dimensional feature vectors\n",
    "\n",
    "2. **Language Model** (e.g., GPT, LLaMA)\n",
    "   - Processes and generates text\n",
    "   - Pre-trained on large text corpora\n",
    "   - Output: Token-level predictions\n",
    "\n",
    "3. **Fusion Mechanism**\n",
    "   - **Early Fusion**: Concatenate features early\n",
    "   - **Late Fusion**: Combine predictions at output\n",
    "   - **Cross-Attention**: Let text attend to image features\n",
    "   - **Adapter Layers**: Learnable bridges between modalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ab8e4",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Set up the environment and import all necessary libraries for multimodal LLM implementations\n",
    "\n",
    "**Key Points:**\n",
    "- **PyTorch & Transformers**: Core deep learning framework and HuggingFace ecosystem for pre-trained models\n",
    "- **Vision Libraries**: PIL for image handling, torchvision for computer vision operations\n",
    "- **CLIP**: OpenAI's contrastive vision-language model for zero-shot classification and embeddings\n",
    "- **Evaluation Tools**: Metrics for assessing captioning and VQA performance\n",
    "\n",
    "**Why This Matters:** Proper library setup ensures we have all tools needed for vision-language tasks, from loading pre-trained models to evaluating their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42cb6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PART 1: LIBRARY IMPORTS & SETUP\n",
    "# ===================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# HuggingFace Transformers\n",
    "from transformers import (\n",
    "    CLIPProcessor, CLIPModel, CLIPTokenizer,\n",
    "    AutoTokenizer, AutoModel,\n",
    "    VisionEncoderDecoderModel,\n",
    "    ViTImageProcessor, ViTModel,\n",
    "    GPT2Tokenizer, GPT2LMHeadModel\n",
    ")\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MULTIMODAL LLM ENVIRONMENT READY\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e44eaf1",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement CLIP-based zero-shot image classification using contrastive vision-language embeddings\n",
    "\n",
    "**Key Points:**\n",
    "- **CLIP Model Loading**: Pre-trained on 400M image-text pairs, learns shared embedding space for vision and language\n",
    "- **Zero-Shot Classification**: Classify images without task-specific training by comparing image embeddings to text prompt embeddings\n",
    "- **Cosine Similarity**: Measures alignment between image and text in the shared embedding space (higher = better match)\n",
    "- **Post-Silicon Application**: Can classify defect types (\"scratched die\", \"good die\", \"contamination\") without labeled training data\n",
    "\n",
    "**Why This Matters:** Zero-shot classification eliminates the need for large labeled datasets, making it ideal for rare defect types in semiconductor testing where labeled examples are scarce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PART 2: CLIP ZERO-SHOT IMAGE CLASSIFIER\n",
    "# ===================================================================\n",
    "\n",
    "class CLIPZeroShotClassifier:\n",
    "    \"\"\"\n",
    "    Zero-shot image classification using CLIP embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"openai/clip-vit-base-patch32\"):\n",
    "        print(f\"Loading CLIP model: {model_name}...\")\n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        print(\"‚úì CLIP model loaded successfully\")\n",
    "    \n",
    "    def classify(self, image, class_labels, return_scores=False):\n",
    "        \"\"\"\n",
    "        Classify image using text prompts\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image or path to image\n",
    "            class_labels: List of text labels (e.g., [\"cat\", \"dog\", \"bird\"])\n",
    "            return_scores: If True, return all scores\n",
    "        \n",
    "        Returns:\n",
    "            predicted_label or (predicted_label, scores_dict)\n",
    "        \"\"\"\n",
    "        # Load image if path provided\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert('RGB')\n",
    "        \n",
    "        # Prepare inputs\n",
    "        inputs = self.processor(\n",
    "            text=class_labels,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits_per_image = outputs.logits_per_image  # (1, num_classes)\n",
    "            probs = logits_per_image.softmax(dim=1)\n",
    "        \n",
    "        # Get prediction\n",
    "        pred_idx = probs.argmax().item()\n",
    "        predicted_label = class_labels[pred_idx]\n",
    "        \n",
    "        if return_scores:\n",
    "            scores = {label: prob.item() for label, prob in zip(class_labels, probs[0])}\n",
    "            return predicted_label, scores\n",
    "        \n",
    "        return predicted_label\n",
    "\n",
    "# Initialize classifier\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLIP ZERO-SHOT CLASSIFIER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "clip_classifier = CLIPZeroShotClassifier()\n",
    "\n",
    "# Example: Classify semiconductor defect types\n",
    "defect_classes = [\n",
    "    \"a photograph of a good semiconductor die\",\n",
    "    \"a photograph of a scratched semiconductor die\",\n",
    "    \"a photograph of a contaminated semiconductor die\",\n",
    "    \"a photograph of a cracked semiconductor die\"\n",
    "]\n",
    "\n",
    "print(f\"\\nDefect classification categories ({len(defect_classes)}):\")\n",
    "for i, label in enumerate(defect_classes, 1):\n",
    "    print(f\"  {i}. {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6ac32",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Visualize CLIP's vision-language embedding space and demonstrate similarity computations\n",
    "\n",
    "**Key Points:**\n",
    "- **Embedding Extraction**: Separate image and text encoders produce normalized feature vectors in shared 512-dimensional space\n",
    "- **Cosine Similarity Matrix**: Quantifies alignment between all image-text pairs (ranges from -1 to +1, higher = better match)\n",
    "- **Heatmap Visualization**: Shows which text descriptions best match which images, revealing CLIP's understanding\n",
    "- **Cross-Modal Retrieval**: Foundation for image search by text or text search by image\n",
    "\n",
    "**Why This Matters:** Understanding the embedding space is crucial for debugging model behavior, optimizing prompts, and building retrieval systems for test documentation or defect databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea163b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PART 3: CLIP EMBEDDING SPACE VISUALIZATION\n",
    "# ===================================================================\n",
    "\n",
    "def visualize_clip_embeddings(images, text_descriptions, model, processor):\n",
    "    \"\"\"\n",
    "    Visualize CLIP embedding similarities between images and texts\n",
    "    \n",
    "    Args:\n",
    "        images: List of PIL Images\n",
    "        text_descriptions: List of text descriptions\n",
    "        model: CLIP model\n",
    "        processor: CLIP processor\n",
    "    \"\"\"\n",
    "    # Get image embeddings\n",
    "    image_inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**image_inputs)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Get text embeddings\n",
    "    text_inputs = processor(text=text_descriptions, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**text_inputs)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarity = (image_features @ text_features.T).cpu().numpy()\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Heatmap\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(\n",
    "        similarity,\n",
    "        annot=True,\n",
    "        fmt='.3f',\n",
    "        cmap='RdYlGn',\n",
    "        xticklabels=[f\"Text {i+1}\" for i in range(len(text_descriptions))],\n",
    "        yticklabels=[f\"Img {i+1}\" for i in range(len(images))],\n",
    "        cbar_kws={'label': 'Cosine Similarity'}\n",
    "    )\n",
    "    plt.title('Image-Text Similarity Matrix\\n(CLIP Embeddings)', fontsize=12, fontweight='bold')\n",
    "    plt.xlabel('Text Descriptions')\n",
    "    plt.ylabel('Images')\n",
    "    \n",
    "    # Bar chart for best matches\n",
    "    plt.subplot(1, 2, 2)\n",
    "    best_matches = similarity.max(axis=1)\n",
    "    colors = ['green' if s > 0.3 else 'orange' if s > 0.2 else 'red' for s in best_matches]\n",
    "    plt.barh(range(len(images)), best_matches, color=colors, alpha=0.7)\n",
    "    plt.xlabel('Max Similarity Score')\n",
    "    plt.ylabel('Image Index')\n",
    "    plt.title('Best Text Match per Image', fontsize=12, fontweight='bold')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLIP EMBEDDING SPACE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nThis visualization shows:\")\n",
    "print(\"  ‚Ä¢ How well CLIP aligns image and text embeddings\")\n",
    "print(\"  ‚Ä¢ Cosine similarity scores (higher = better match)\")\n",
    "print(\"  ‚Ä¢ Which descriptions best match which images\")\n",
    "print(\"\\nUse case: Retrieve similar defect images by text query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05b655",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Build a simple image captioning model from scratch using encoder-decoder architecture\n",
    "\n",
    "**Key Points:**\n",
    "- **CNN Encoder**: ResNet extracts visual features from images, frozen pre-trained weights preserve learned representations\n",
    "- **RNN Decoder**: LSTM generates captions word-by-word, conditioned on image features\n",
    "- **Attention Mechanism**: Decoder focuses on relevant image regions when generating each word (like reading different parts of a wafer map)\n",
    "- **Teacher Forcing**: During training, use ground truth previous words to stabilize learning\n",
    "\n",
    "**Why This Matters:** Image captioning is foundational for automated test report generation in post-silicon validation‚Äîconverting wafer maps and test plots into natural language descriptions for engineers and management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446b5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PART 4: IMAGE CAPTIONING MODEL (ENCODER-DECODER)\n",
    "# ===================================================================\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN-based image encoder using pre-trained ResNet\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size=256):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        resnet = resnet50(pretrained=True)\n",
    "        # Remove final FC layer\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        # Freeze ResNet parameters\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Linear layer to project ResNet features\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Extract image features\n",
    "        \n",
    "        Args:\n",
    "            images: (batch_size, 3, 224, 224)\n",
    "        Returns:\n",
    "            features: (batch_size, embed_size)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)  # (batch, 2048, 1, 1)\n",
    "        features = features.reshape(features.size(0), -1)  # (batch, 2048)\n",
    "        features = self.bn(self.linear(features))  # (batch, embed_size)\n",
    "        return features\n",
    "\n",
    "\n",
    "class CaptionDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based caption decoder with attention\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size=256, hidden_size=512, vocab_size=10000, num_layers=1):\n",
    "        super(CaptionDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Generate caption probabilities\n",
    "        \n",
    "        Args:\n",
    "            features: (batch_size, embed_size) from encoder\n",
    "            captions: (batch_size, max_length) token IDs\n",
    "        Returns:\n",
    "            outputs: (batch_size, max_length, vocab_size)\n",
    "        \"\"\"\n",
    "        embeddings = self.embed(captions)  # (batch, max_len, embed_size)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(self.dropout(hiddens))\n",
    "        return outputs\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMAGE CAPTIONING MODEL ARCHITECTURE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize models\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = 10000\n",
    "\n",
    "encoder = ImageEncoder(embed_size).to(device)\n",
    "decoder = CaptionDecoder(embed_size, hidden_size, vocab_size).to(device)\n",
    "\n",
    "print(f\"\\nEncoder:\")\n",
    "print(f\"  ‚Ä¢ Base: ResNet-50 (pre-trained, frozen)\")\n",
    "print(f\"  ‚Ä¢ Output: {embed_size}-dim image embeddings\")\n",
    "print(f\"  ‚Ä¢ Parameters: {sum(p.numel() for p in encoder.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "print(f\"\\nDecoder:\")\n",
    "print(f\"  ‚Ä¢ Type: LSTM with {num_layers} layer(s)\")\n",
    "print(f\"  ‚Ä¢ Hidden size: {hidden_size}\")\n",
    "print(f\"  ‚Ä¢ Vocabulary: {vocab_size:,} tokens\")\n",
    "print(f\"  ‚Ä¢ Parameters: {sum(p.numel() for p in decoder.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c11a04",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement caption generation with beam search for producing high-quality, diverse captions\n",
    "\n",
    "**Key Points:**\n",
    "- **Greedy Decoding**: Select most probable word at each step (fast but may miss better overall sequences)\n",
    "- **Beam Search**: Maintain top-k hypotheses at each step, explores multiple caption possibilities simultaneously\n",
    "- **Start/End Tokens**: Special tokens mark caption boundaries (<start> and <end>)\n",
    "- **Temperature Sampling**: Control randomness in word selection (lower = more conservative, higher = more creative)\n",
    "\n",
    "**Why This Matters:** Beam search generates better captions than greedy decoding for test reports and failure analysis‚Äîexploring multiple phrasings helps find the most accurate and informative description of defects or test patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e12f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PART 5: CAPTION GENERATION WITH BEAM SEARCH\n",
    "# ===================================================================\n",
    "\n",
    "def generate_caption(encoder, decoder, image, vocab, max_length=20, beam_width=3):\n",
    "    \"\"\"\n",
    "    Generate caption using beam search\n",
    "    \n",
    "    Args:\n",
    "        encoder: Image encoder model\n",
    "        decoder: Caption decoder model\n",
    "        image: Input image tensor (1, 3, H, W)\n",
    "        vocab: Vocabulary mapping (token_to_id, id_to_token)\n",
    "        max_length: Maximum caption length\n",
    "        beam_width: Number of beams to maintain\n",
    "    \n",
    "    Returns:\n",
    "        caption: Generated caption string\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Extract image features\n",
    "    with torch.no_grad():\n",
    "        features = encoder(image.to(device))  # (1, embed_size)\n",
    "    \n",
    "    # Initialize beams: (sequence, score)\n",
    "    start_token = vocab['<start>']\n",
    "    end_token = vocab['<end>']\n",
    "    \n",
    "    beams = [([start_token], 0.0)]  # (sequence, score)\n",
    "    completed = []\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        candidates = []\n",
    "        \n",
    "        for seq, score in beams:\n",
    "            if seq[-1] == end_token:\n",
    "                completed.append((seq, score))\n",
    "                continue\n",
    "            \n",
    "            # Prepare input\n",
    "            input_seq = torch.LongTensor([seq]).to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            with torch.no_grad():\n",
    "                outputs = decoder(features, input_seq)\n",
    "                logits = outputs[0, -1, :]  # Last timestep\n",
    "                probs = F.softmax(logits, dim=0)\n",
    "            \n",
    "            # Get top-k candidates\n",
    "            topk_probs, topk_indices = torch.topk(probs, beam_width)\n",
    "            \n",
    "            for prob, idx in zip(topk_probs, topk_indices):\n",
    "                new_seq = seq + [idx.item()]\n",
    "                new_score = score - torch.log(prob).item()  # Negative log likelihood\n",
    "                candidates.append((new_seq, new_score))\n",
    "        \n",
    "        # Select top beams\n",
    "        candidates = sorted(candidates, key=lambda x: x[1])[:beam_width]\n",
    "        beams = candidates\n",
    "        \n",
    "        # Early stopping if all beams completed\n",
    "        if len(completed) >= beam_width:\n",
    "            break\n",
    "    \n",
    "    # Add remaining beams to completed\n",
    "    completed.extend(beams)\n",
    "    \n",
    "    # Select best caption\n",
    "    best_seq, _ = min(completed, key=lambda x: x[1] / len(x[0]))  # Normalize by length\n",
    "    \n",
    "    # Convert to words\n",
    "    id_to_token = {v: k for k, v in vocab.items()}\n",
    "    caption = ' '.join([id_to_token.get(idx, '<unk>') for idx in best_seq \n",
    "                       if idx not in [start_token, end_token]])\n",
    "    \n",
    "    return caption\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CAPTION GENERATION WITH BEAM SEARCH\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nBeam search algorithm:\")\n",
    "print(\"  1. Start with <start> token\")\n",
    "print(\"  2. At each step, expand top-k hypotheses\")\n",
    "print(\"  3. Keep top-k sequences based on cumulative probability\")\n",
    "print(\"  4. Stop when <end> token generated or max length reached\")\n",
    "print(\"\\nAdvantages over greedy decoding:\")\n",
    "print(\"  ‚úì Explores multiple caption possibilities\")\n",
    "print(\"  ‚úì Finds globally better sequences\")\n",
    "print(\"  ‚úì Reduces repetition and improves fluency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6a3b0b",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Use pre-trained HuggingFace models for production-ready image captioning and visual question answering\n",
    "\n",
    "**Key Points:**\n",
    "- **Vision Encoder Decoder**: Combines ViT (Vision Transformer) image encoder with GPT-2 text decoder\n",
    "- **Pre-trained Weights**: Model already trained on millions of image-caption pairs (COCO, Flickr)\n",
    "- **Automatic Tokenization**: Handles text preprocessing, vocabulary mapping, and special tokens automatically\n",
    "- **Inference Pipeline**: Simple API for generating captions from images without manual preprocessing\n",
    "\n",
    "**Why This Matters:** Production systems need reliable, well-tested models. HuggingFace provides state-of-the-art models that can be fine-tuned on domain-specific data (like semiconductor test images) with minimal code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PART 6: PRODUCTION IMAGE CAPTIONING (HUGGINGFACE)\n",
    "# ===================================================================\n",
    "\n",
    "class ProductionImageCaptioner:\n",
    "    \"\"\"\n",
    "    Production-ready image captioning using HuggingFace models\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"nlpconnect/vit-gpt2-image-captioning\"):\n",
    "        print(f\"Loading model: {model_name}...\")\n",
    "        self.model = VisionEncoderDecoderModel.from_pretrained(model_name).to(device)\n",
    "        self.feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        print(\"‚úì Model loaded successfully\")\n",
    "    \n",
    "    def caption_image(self, image, max_length=50, num_beams=4):\n",
    "        \"\"\"\n",
    "        Generate caption for image\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image or path\n",
    "            max_length: Maximum caption length\n",
    "            num_beams: Beam search width\n",
    "        \n",
    "        Returns:\n",
    "            caption: Generated caption string\n",
    "        \"\"\"\n",
    "        # Load image if path\n",
    "        if isinstance(image, str):\n",
    "            image = Image.open(image).convert('RGB')\n",
    "        \n",
    "        # Preprocess\n",
    "        pixel_values = self.feature_extractor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\"\n",
    "        ).pixel_values.to(device)\n",
    "        \n",
    "        # Generate caption\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(\n",
    "                pixel_values,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        caption = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "    \n",
    "    def batch_caption(self, images, max_length=50, num_beams=4):\n",
    "        \"\"\"\n",
    "        Caption multiple images in batch\n",
    "        \n",
    "        Args:\n",
    "            images: List of PIL Images\n",
    "            max_length: Maximum caption length\n",
    "            num_beams: Beam search width\n",
    "        \n",
    "        Returns:\n",
    "            captions: List of caption strings\n",
    "        \"\"\"\n",
    "        # Preprocess batch\n",
    "        pixel_values = self.feature_extractor(\n",
    "            images=images,\n",
    "            return_tensors=\"pt\"\n",
    "        ).pixel_values.to(device)\n",
    "        \n",
    "        # Generate captions\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(\n",
    "                pixel_values,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        captions = [self.tokenizer.decode(ids, skip_special_tokens=True) \n",
    "                   for ids in output_ids]\n",
    "        return captions\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PRODUCTION IMAGE CAPTIONING SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize captioner\n",
    "captioner = ProductionImageCaptioner()\n",
    "\n",
    "print(\"\\nModel details:\")\n",
    "print(\"  ‚Ä¢ Vision Encoder: Vision Transformer (ViT)\")\n",
    "print(\"  ‚Ä¢ Text Decoder: GPT-2\")\n",
    "print(\"  ‚Ä¢ Training data: COCO Captions (>100K images)\")\n",
    "print(\"  ‚Ä¢ Inference: Beam search for quality captions\")\n",
    "print(\"\\nReady for:\")\n",
    "print(\"  ‚úì Single image captioning\")\n",
    "print(\"  ‚úì Batch processing\")\n",
    "print(\"  ‚úì Fine-tuning on custom datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264311bb",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement Visual Question Answering (VQA) model that answers questions about images\n",
    "\n",
    "**Key Points:**\n",
    "- **Multimodal Fusion**: Combines image features (from CNN/ViT) with question embeddings (from text encoder)\n",
    "- **Cross-Modal Attention**: Question attends to relevant image regions to find answer\n",
    "- **Classification Head**: Predicts answer from fixed vocabulary (common answers like yes/no, numbers, objects)\n",
    "- **Pre-training Strategy**: Model learns image-text alignment before fine-tuning on VQA task\n",
    "\n",
    "**Why This Matters:** VQA enables automated analysis of test data‚Äîengineers can ask \"How many dies failed?\" or \"Is there a spatial pattern?\" about wafer maps, getting instant answers without manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c71c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PART 7: VISUAL QUESTION ANSWERING (VQA) MODEL\n",
    "# ===================================================================\n",
    "\n",
    "class VQAModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Visual Question Answering model with cross-modal attention\n",
    "    \"\"\"\n",
    "    def __init__(self, vision_dim=2048, text_dim=768, hidden_dim=512, num_answers=3000):\n",
    "        super(VQAModel, self).__init__()\n",
    "        \n",
    "        # Vision encoder (ResNet features)\n",
    "        self.vision_proj = nn.Linear(vision_dim, hidden_dim)\n",
    "        \n",
    "        # Text encoder projection\n",
    "        self.text_proj = nn.Linear(text_dim, hidden_dim)\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=8, batch_first=True)\n",
    "        \n",
    "        # Fusion and classification\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, num_answers)\n",
    "        )\n",
    "    \n",
    "    def forward(self, image_features, text_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_features: (batch, vision_dim)\n",
    "            text_features: (batch, text_dim)\n",
    "        Returns:\n",
    "            logits: (batch, num_answers)\n",
    "        \"\"\"\n",
    "        # Project to common space\n",
    "        v = self.vision_proj(image_features)  # (batch, hidden)\n",
    "        t = self.text_proj(text_features)     # (batch, hidden)\n",
    "        \n",
    "        # Add sequence dimension for attention\n",
    "        v = v.unsqueeze(1)  # (batch, 1, hidden)\n",
    "        t = t.unsqueeze(1)  # (batch, 1, hidden)\n",
    "        \n",
    "        # Cross-modal attention (text attends to image)\n",
    "        attn_out, _ = self.attention(t, v, v)  # (batch, 1, hidden)\n",
    "        attn_out = attn_out.squeeze(1)  # (batch, hidden)\n",
    "        \n",
    "        # Fuse features\n",
    "        combined = torch.cat([attn_out, t.squeeze(1)], dim=1)  # (batch, hidden*2)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.fusion(combined)  # (batch, num_answers)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "class SimpleVQASystem:\n",
    "    \"\"\"\n",
    "    Complete VQA system with vision and text encoders\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        print(\"Initializing VQA system...\")\n",
    "        \n",
    "        # Load pre-trained encoders\n",
    "        self.vision_encoder = resnet50(pretrained=True).to(device)\n",
    "        self.vision_encoder.fc = nn.Identity()  # Remove classification head\n",
    "        self.vision_encoder.eval()\n",
    "        \n",
    "        # For text: using simple CLIP text encoder\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "        # VQA model\n",
    "        self.vqa_model = VQAModel(vision_dim=2048, text_dim=512).to(device)\n",
    "        \n",
    "        print(\"‚úì VQA system initialized\")\n",
    "    \n",
    "    def answer_question(self, image, question, answer_candidates):\n",
    "        \"\"\"\n",
    "        Answer question about image\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image\n",
    "            question: Question string\n",
    "            answer_candidates: List of possible answers\n",
    "        \n",
    "        Returns:\n",
    "            answer: Most likely answer\n",
    "        \"\"\"\n",
    "        self.vision_encoder.eval()\n",
    "        self.vqa_model.eval()\n",
    "        \n",
    "        # Get image features (using ResNet)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            img_features = self.vision_encoder(img_tensor)  # (1, 2048)\n",
    "        \n",
    "        # Get text features (using CLIP for simplicity)\n",
    "        text_inputs = self.clip_processor(text=[question], return_tensors=\"pt\", padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features = self.clip_model.get_text_features(**text_inputs)  # (1, 512)\n",
    "        \n",
    "        # Score answer candidates using CLIP similarity\n",
    "        # (In production, would use trained VQA classification head)\n",
    "        answer_inputs = self.clip_processor(text=answer_candidates, return_tensors=\"pt\", padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            answer_features = self.clip_model.get_text_features(**answer_inputs)\n",
    "            \n",
    "            # Compute similarities\n",
    "            text_features_norm = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "            answer_features_norm = answer_features / answer_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Simple scoring (in practice, would use VQA model)\n",
    "            similarities = (text_features_norm @ answer_features_norm.T).squeeze()\n",
    "        \n",
    "        # Get best answer\n",
    "        best_idx = similarities.argmax().item()\n",
    "        return answer_candidates[best_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUAL QUESTION ANSWERING SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize VQA system\n",
    "vqa_system = SimpleVQASystem()\n",
    "\n",
    "print(\"\\nVQA Components:\")\n",
    "print(\"  ‚Ä¢ Vision: ResNet-50 (extracts image features)\")\n",
    "print(\"  ‚Ä¢ Text: CLIP text encoder (processes questions)\")\n",
    "print(\"  ‚Ä¢ Fusion: Cross-modal attention + classification\")\n",
    "print(\"\\nExample questions:\")\n",
    "print(\"  ‚Ä¢ 'How many objects are in the image?'\")\n",
    "print(\"  ‚Ä¢ 'What color is the object?'\")\n",
    "print(\"  ‚Ä¢ 'Is there a defect visible?'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da05bc",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Demonstrate LLaVA-style visual instruction following‚Äîa visual assistant that can have conversations about images\n",
    "\n",
    "**Key Points:**\n",
    "- **Visual Adapter**: Lightweight projection layer connects frozen vision encoder to frozen language model\n",
    "- **Instruction Tuning**: Model trained to follow diverse instructions like \"describe this image\" or \"what's unusual here?\"\n",
    "- **Two-Stage Training**: (1) Pre-train adapter on image-caption pairs, (2) Fine-tune on instruction-following data\n",
    "- **Efficient Design**: Only adapter parameters trained (~10M), vision and language models stay frozen (~7B total)\n",
    "\n",
    "**Why This Matters:** Visual assistants enable natural language interaction with test data‚Äîengineers can ask follow-up questions, request specific details, or get explanations in plain English rather than interpreting raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae43a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PART 8: LLaVA-STYLE VISUAL ASSISTANT (SIMPLIFIED)\n",
    "# ===================================================================\n",
    "\n",
    "class VisualAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight adapter to connect vision encoder to language model\n",
    "    Similar to LLaVA's projection layer\n",
    "    \"\"\"\n",
    "    def __init__(self, vision_dim=768, lm_dim=768, hidden_dim=512):\n",
    "        super(VisualAdapter, self).__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(vision_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, lm_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, vision_features):\n",
    "        \"\"\"\n",
    "        Project vision features to language model space\n",
    "        \n",
    "        Args:\n",
    "            vision_features: (batch, seq_len, vision_dim)\n",
    "        Returns:\n",
    "            projected: (batch, seq_len, lm_dim)\n",
    "        \"\"\"\n",
    "        return self.projection(vision_features)\n",
    "\n",
    "\n",
    "class LLaVAStyleAssistant:\n",
    "    \"\"\"\n",
    "    Visual instruction-following assistant (simplified LLaVA architecture)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        print(\"Building visual assistant...\")\n",
    "        \n",
    "        # Vision encoder (CLIP ViT)\n",
    "        self.vision_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "        self.vision_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        \n",
    "        # Freeze vision encoder\n",
    "        for param in self.vision_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Visual adapter (trainable)\n",
    "        self.adapter = VisualAdapter(vision_dim=512, lm_dim=768).to(device)\n",
    "        \n",
    "        # Language model (GPT-2 small for demo)\n",
    "        self.lm = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Freeze language model (in practice, might fine-tune)\n",
    "        for param in self.lm.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        print(\"‚úì Visual assistant ready\")\n",
    "        print(f\"  ‚Ä¢ Trainable params: {sum(p.numel() for p in self.adapter.parameters()):,}\")\n",
    "        print(f\"  ‚Ä¢ Total params: {sum(p.numel() for p in self.parameters()):,}\")\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"Return all model parameters\"\"\"\n",
    "        return list(self.adapter.parameters())\n",
    "    \n",
    "    def generate_response(self, image, instruction, max_length=50):\n",
    "        \"\"\"\n",
    "        Generate response to instruction about image\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image\n",
    "            instruction: Instruction text\n",
    "            max_length: Maximum response length\n",
    "        \n",
    "        Returns:\n",
    "            response: Generated text\n",
    "        \"\"\"\n",
    "        self.vision_model.eval()\n",
    "        self.adapter.eval()\n",
    "        self.lm.eval()\n",
    "        \n",
    "        # Get image features\n",
    "        img_inputs = self.vision_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            vision_outputs = self.vision_model.vision_model(**img_inputs)\n",
    "            vision_features = vision_outputs.last_hidden_state  # (1, 50, 768)\n",
    "        \n",
    "        # Project to LM space\n",
    "        with torch.no_grad():\n",
    "            adapted_features = self.adapter(vision_features)  # (1, 50, 768)\n",
    "        \n",
    "        # Prepare text prompt\n",
    "        prompt = f\"<image> {instruction}\\nAssistant:\"\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate (simplified - in practice, would properly merge visual tokens)\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.lm.generate(\n",
    "                input_ids,\n",
    "                max_length=max_length,\n",
    "                num_beams=3,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        response = response.split(\"Assistant:\")[-1].strip()\n",
    "        \n",
    "        return response\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LLaVA-STYLE VISUAL ASSISTANT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize assistant\n",
    "visual_assistant = LLaVAStyleAssistant()\n",
    "\n",
    "print(\"\\nArchitecture:\")\n",
    "print(\"  1. Vision Encoder: CLIP ViT (frozen)\")\n",
    "print(\"  2. Visual Adapter: Projection layer (trainable)\")\n",
    "print(\"  3. Language Model: GPT-2 (frozen)\")\n",
    "print(\"\\nTraining strategy:\")\n",
    "print(\"  ‚Ä¢ Stage 1: Train adapter on image-caption pairs\")\n",
    "print(\"  ‚Ä¢ Stage 2: Instruction-tune on visual QA data\")\n",
    "print(\"\\nCapabilities:\")\n",
    "print(\"  ‚úì Describe images in detail\")\n",
    "print(\"  ‚úì Answer questions about image content\")\n",
    "print(\"  ‚úì Follow multi-step instructions\")\n",
    "print(\"  ‚úì Explain visual reasoning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f690d6cc",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Create synthetic wafer map and test data visualization for demonstrating multimodal LLM applications in semiconductor testing\n",
    "\n",
    "**Key Points:**\n",
    "- **Wafer Map Generation**: Creates realistic spatial failure patterns (edge failures, clusters, random defects)\n",
    "- **Parametric Data Simulation**: Generates test parameters (voltage, current, frequency) with realistic correlations\n",
    "- **Visualization**: Heatmaps and scatter plots that mimic actual ATE test reports\n",
    "- **Ground Truth Labels**: Known defect types and patterns for testing VQA and captioning models\n",
    "\n",
    "**Why This Matters:** Real semiconductor test data is proprietary and sensitive. Synthetic data allows us to demonstrate multimodal LLM capabilities on realistic scenarios without exposing confidential manufacturing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc18de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PART 9: POST-SILICON VALIDATION - WAFER MAP GENERATOR\n",
    "# ===================================================================\n",
    "\n",
    "def generate_wafer_map(wafer_size=30, defect_type='edge_failure', severity=0.3):\n",
    "    \"\"\"\n",
    "    Generate synthetic wafer map with defect patterns\n",
    "    \n",
    "    Args:\n",
    "        wafer_size: Diameter in dies\n",
    "        defect_type: 'edge_failure', 'cluster', 'random', 'scratch'\n",
    "        severity: Defect density (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        wafer_map: 2D array (pass=1, fail=0)\n",
    "        description: Text description of pattern\n",
    "    \"\"\"\n",
    "    # Create circular wafer\n",
    "    center = wafer_size // 2\n",
    "    y, x = np.ogrid[-center:wafer_size-center, -center:wafer_size-center]\n",
    "    mask = x**2 + y**2 <= center**2\n",
    "    wafer = np.ones((wafer_size, wafer_size))\n",
    "    wafer[~mask] = np.nan\n",
    "    \n",
    "    # Apply defect pattern\n",
    "    if defect_type == 'edge_failure':\n",
    "        # Fails at wafer edge\n",
    "        distance = np.sqrt(x**2 + y**2)\n",
    "        edge_threshold = center * 0.8\n",
    "        fails = (distance > edge_threshold) & mask\n",
    "        wafer[fails] = 0 if np.random.rand() < severity else 1\n",
    "        description = f\"Edge failure pattern, {np.sum(fails)} dies affected at wafer periphery\"\n",
    "        \n",
    "    elif defect_type == 'cluster':\n",
    "        # Clustered failures\n",
    "        cluster_centers = np.random.randint(0, wafer_size, (3, 2))\n",
    "        for cx, cy in cluster_centers:\n",
    "            dist = np.sqrt((x + center - cx)**2 + (y + center - cy)**2)\n",
    "            cluster = (dist < 5) & mask\n",
    "            wafer[cluster] = 0 if np.random.rand() < severity else wafer[cluster]\n",
    "        description = f\"Clustered defects at {len(cluster_centers)} locations, indicating localized contamination\"\n",
    "        \n",
    "    elif defect_type == 'random':\n",
    "        # Random failures\n",
    "        num_fails = int(np.sum(mask) * severity)\n",
    "        valid_indices = np.where(mask)\n",
    "        fail_idx = np.random.choice(len(valid_indices[0]), num_fails, replace=False)\n",
    "        wafer[valid_indices[0][fail_idx], valid_indices[1][fail_idx]] = 0\n",
    "        description = f\"Random failures: {num_fails} dies, suggests process variation or handling issues\"\n",
    "        \n",
    "    elif defect_type == 'scratch':\n",
    "        # Linear scratch pattern\n",
    "        angle = np.random.rand() * np.pi\n",
    "        scratch_line = np.abs(np.sin(angle) * x - np.cos(angle) * y) < 1.5\n",
    "        wafer[scratch_line & mask] = 0\n",
    "        description = f\"Linear scratch pattern at {np.degrees(angle):.1f}¬∞ angle, mechanical damage during handling\"\n",
    "    \n",
    "    return wafer, description\n",
    "\n",
    "\n",
    "def plot_wafer_map(wafer, title=\"Wafer Map\", save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize wafer map with failures\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Custom colormap\n",
    "    cmap = plt.cm.colors.ListedColormap(['red', 'lightgreen'])\n",
    "    bounds = [-0.5, 0.5, 1.5]\n",
    "    norm = plt.cm.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    \n",
    "    plt.imshow(wafer, cmap=cmap, norm=norm, interpolation='nearest')\n",
    "    plt.colorbar(ticks=[0, 1], label='Status', shrink=0.8)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Die X Position')\n",
    "    plt.ylabel('Die Y Position')\n",
    "    \n",
    "    # Add statistics\n",
    "    valid_dies = ~np.isnan(wafer)\n",
    "    total_dies = np.sum(valid_dies)\n",
    "    failed_dies = np.sum(wafer[valid_dies] == 0)\n",
    "    yield_pct = (1 - failed_dies/total_dies) * 100\n",
    "    \n",
    "    stats_text = f\"Total Dies: {total_dies}\\nFailed: {failed_dies}\\nYield: {yield_pct:.1f}%\"\n",
    "    plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes,\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "             verticalalignment='top', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"POST-SILICON VALIDATION: WAFER MAP GENERATOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate example wafer maps\n",
    "print(\"\\nGenerating synthetic wafer maps...\")\n",
    "\n",
    "wafer1, desc1 = generate_wafer_map(wafer_size=30, defect_type='edge_failure', severity=0.4)\n",
    "print(f\"\\n1. Edge Failure: {desc1}\")\n",
    "\n",
    "wafer2, desc2 = generate_wafer_map(wafer_size=30, defect_type='cluster', severity=0.3)\n",
    "print(f\"\\n2. Cluster: {desc2}\")\n",
    "\n",
    "print(\"\\nThese wafer maps can be analyzed by multimodal LLMs for:\")\n",
    "print(\"  ‚Ä¢ Automated defect classification\")\n",
    "print(\"  ‚Ä¢ Root cause analysis text generation\")\n",
    "print(\"  ‚Ä¢ Visual question answering ('Where are the failures?')\")\n",
    "print(\"  ‚Ä¢ Yield prediction and reporting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0eec9",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Apply multimodal LLMs to analyze synthetic wafer maps‚Äîdemonstrating automated defect analysis and reporting\n",
    "\n",
    "**Key Points:**\n",
    "- **CLIP Zero-Shot Classification**: Identify defect types from wafer map images without task-specific training\n",
    "- **Image Captioning**: Generate natural language descriptions of failure patterns automatically\n",
    "- **VQA Application**: Answer specific questions about defect locations, counts, and patterns\n",
    "- **Automated Reporting**: Transform visual test data into executive summaries for management\n",
    "\n",
    "**Why This Matters:** This pipeline demonstrates end-to-end automation of post-silicon validation reporting‚Äîfrom raw wafer maps to natural language insights, reducing manual analysis time from hours to seconds while maintaining accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4579b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PART 10: WAFER MAP ANALYSIS WITH MULTIMODAL LLMS\n",
    "# ===================================================================\n",
    "\n",
    "def analyze_wafer_with_multimodal_llm(wafer, description, clip_model, captioner):\n",
    "    \"\"\"\n",
    "    Complete analysis pipeline: classification, captioning, and QA\n",
    "    \n",
    "    Args:\n",
    "        wafer: Wafer map array\n",
    "        description: Ground truth description\n",
    "        clip_model: CLIP classifier\n",
    "        captioner: Image captioning model\n",
    "    \n",
    "    Returns:\n",
    "        analysis: Dictionary with results\n",
    "    \"\"\"\n",
    "    # Generate wafer map image\n",
    "    fig = plot_wafer_map(wafer, title=\"Test Wafer\")\n",
    "    \n",
    "    # Convert matplotlib figure to PIL Image\n",
    "    fig.canvas.draw()\n",
    "    img_array = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    img_array = img_array.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    wafer_image = Image.fromarray(img_array)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # 1. Defect Classification (CLIP zero-shot)\n",
    "    defect_classes = [\n",
    "        \"a wafer map showing edge failures\",\n",
    "        \"a wafer map showing clustered defects\",\n",
    "        \"a wafer map showing random failures\",\n",
    "        \"a wafer map showing scratch defects\"\n",
    "    ]\n",
    "    \n",
    "    predicted_class, scores = clip_model.classify(\n",
    "        wafer_image, \n",
    "        defect_classes, \n",
    "        return_scores=True\n",
    "    )\n",
    "    \n",
    "    # 2. Caption Generation\n",
    "    generated_caption = captioner.caption_image(wafer_image, max_length=50, num_beams=4)\n",
    "    \n",
    "    # 3. Calculate statistics\n",
    "    valid_dies = ~np.isnan(wafer)\n",
    "    total_dies = np.sum(valid_dies)\n",
    "    failed_dies = np.sum(wafer[valid_dies] == 0)\n",
    "    yield_pct = (1 - failed_dies/total_dies) * 100\n",
    "    \n",
    "    # Compile analysis\n",
    "    analysis = {\n",
    "        'classification': predicted_class,\n",
    "        'confidence_scores': scores,\n",
    "        'generated_caption': generated_caption,\n",
    "        'ground_truth': description,\n",
    "        'statistics': {\n",
    "            'total_dies': int(total_dies),\n",
    "            'failed_dies': int(failed_dies),\n",
    "            'yield_percent': round(yield_pct, 2)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return analysis, wafer_image\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MULTIMODAL LLM WAFER ANALYSIS PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate test wafers\n",
    "print(\"\\nüìä Generating test wafers...\")\n",
    "wafers_to_test = [\n",
    "    ('edge_failure', 0.4),\n",
    "    ('cluster', 0.3),\n",
    "    ('random', 0.2),\n",
    "    ('scratch', 0.35)\n",
    "]\n",
    "\n",
    "analyses = []\n",
    "\n",
    "for defect_type, severity in wafers_to_test:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Analyzing: {defect_type.upper()} (severity={severity})\")\n",
    "    print('='*70)\n",
    "    \n",
    "    # Generate wafer\n",
    "    wafer, description = generate_wafer_map(\n",
    "        wafer_size=30, \n",
    "        defect_type=defect_type, \n",
    "        severity=severity\n",
    "    )\n",
    "    \n",
    "    # Analyze with multimodal LLM\n",
    "    analysis, wafer_img = analyze_wafer_with_multimodal_llm(\n",
    "        wafer, description, clip_classifier, captioner\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüéØ Classification: {analysis['classification']}\")\n",
    "    print(f\"\\nüìä Confidence Scores:\")\n",
    "    for label, score in analysis['confidence_scores'].items():\n",
    "        print(f\"   {label}: {score:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìù Generated Caption: {analysis['generated_caption']}\")\n",
    "    print(f\"\\n‚úì Ground Truth: {analysis['ground_truth']}\")\n",
    "    \n",
    "    print(f\"\\nüìà Statistics:\")\n",
    "    stats = analysis['statistics']\n",
    "    print(f\"   Total Dies: {stats['total_dies']}\")\n",
    "    print(f\"   Failed Dies: {stats['failed_dies']}\")\n",
    "    print(f\"   Yield: {stats['yield_percent']}%\")\n",
    "    \n",
    "    analyses.append((defect_type, analysis))\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"‚úÖ Analysis Complete!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nProcessed {len(analyses)} wafer maps\")\n",
    "print(\"Multimodal LLM capabilities demonstrated:\")\n",
    "print(\"  ‚úì Zero-shot defect classification\")\n",
    "print(\"  ‚úì Automated caption generation\")\n",
    "print(\"  ‚úì Statistical analysis\")\n",
    "print(\"  ‚úì Natural language reporting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d0d2b",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Evaluate multimodal LLM performance using standard metrics for captioning and VQA tasks\n",
    "\n",
    "**Key Points:**\n",
    "- **BLEU Score**: Measures n-gram overlap between generated and reference captions (higher = better word-level match)\n",
    "- **ROUGE Score**: Evaluates recall of reference words in generated text (focuses on completeness)\n",
    "- **CIDEr Score**: Consensus-based metric that weights rare words higher (better for descriptive quality)\n",
    "- **METEOR**: Considers synonyms and stemming (more semantic understanding than BLEU)\n",
    "\n",
    "**Why This Matters:** Objective metrics are essential for comparing models, tracking improvements during fine-tuning, and validating that automated reports meet quality standards before deployment in production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c41713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PART 11: EVALUATION METRICS FOR MULTIMODAL LLMS\n",
    "# ===================================================================\n",
    "\n",
    "def compute_bleu_score(reference, candidate):\n",
    "    \"\"\"\n",
    "    Compute BLEU score (n-gram overlap)\n",
    "    \n",
    "    Args:\n",
    "        reference: List of reference sentences\n",
    "        candidate: Generated sentence\n",
    "    \n",
    "    Returns:\n",
    "        bleu: BLEU-4 score (0-1)\n",
    "    \"\"\"\n",
    "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "    \n",
    "    # Tokenize\n",
    "    reference_tokens = [ref.lower().split() for ref in reference]\n",
    "    candidate_tokens = candidate.lower().split()\n",
    "    \n",
    "    # Compute BLEU with smoothing\n",
    "    smooth = SmoothingFunction()\n",
    "    bleu = sentence_bleu(\n",
    "        reference_tokens, \n",
    "        candidate_tokens,\n",
    "        smoothing_function=smooth.method1\n",
    "    )\n",
    "    \n",
    "    return bleu\n",
    "\n",
    "\n",
    "def compute_rouge_score(reference, candidate):\n",
    "    \"\"\"\n",
    "    Compute ROUGE scores (recall-oriented)\n",
    "    \n",
    "    Returns:\n",
    "        rouge_scores: Dict with ROUGE-1, ROUGE-2, ROUGE-L\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    ref_tokens = reference.lower().split()\n",
    "    cand_tokens = candidate.lower().split()\n",
    "    \n",
    "    # ROUGE-1 (unigram overlap)\n",
    "    ref_unigrams = Counter(ref_tokens)\n",
    "    cand_unigrams = Counter(cand_tokens)\n",
    "    overlap = sum((ref_unigrams & cand_unigrams).values())\n",
    "    \n",
    "    rouge_1_precision = overlap / len(cand_tokens) if cand_tokens else 0\n",
    "    rouge_1_recall = overlap / len(ref_tokens) if ref_tokens else 0\n",
    "    rouge_1_f1 = (2 * rouge_1_precision * rouge_1_recall / \n",
    "                  (rouge_1_precision + rouge_1_recall)) if (rouge_1_precision + rouge_1_recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'rouge-1': {'precision': rouge_1_precision, 'recall': rouge_1_recall, 'f1': rouge_1_f1}\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_caption_quality(generated_caption, reference_caption):\n",
    "    \"\"\"\n",
    "    Comprehensive caption evaluation\n",
    "    \n",
    "    Args:\n",
    "        generated_caption: Model output\n",
    "        reference_caption: Ground truth\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dict with multiple scores\n",
    "    \"\"\"\n",
    "    # BLEU score\n",
    "    bleu = compute_bleu_score([reference_caption], generated_caption)\n",
    "    \n",
    "    # ROUGE score\n",
    "    rouge = compute_rouge_score(reference_caption, generated_caption)\n",
    "    \n",
    "    # Exact match (strict)\n",
    "    exact_match = int(generated_caption.lower().strip() == reference_caption.lower().strip())\n",
    "    \n",
    "    # Word overlap ratio\n",
    "    ref_words = set(reference_caption.lower().split())\n",
    "    gen_words = set(generated_caption.lower().split())\n",
    "    overlap_ratio = len(ref_words & gen_words) / len(ref_words) if ref_words else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'bleu': round(bleu, 4),\n",
    "        'rouge_1_f1': round(rouge['rouge-1']['f1'], 4),\n",
    "        'exact_match': exact_match,\n",
    "        'word_overlap': round(overlap_ratio, 4)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MULTIMODAL LLM EVALUATION METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Example evaluation\n",
    "print(\"\\nMetrics for caption quality assessment:\")\n",
    "print(\"\\n1. BLEU (0-1):\")\n",
    "print(\"   ‚Ä¢ Measures n-gram precision\")\n",
    "print(\"   ‚Ä¢ Higher = better word-level match\")\n",
    "print(\"   ‚Ä¢ BLEU-4 considers up to 4-word sequences\")\n",
    "\n",
    "print(\"\\n2. ROUGE (0-1):\")\n",
    "print(\"   ‚Ä¢ Measures recall of reference words\")\n",
    "print(\"   ‚Ä¢ ROUGE-1: Unigram overlap\")\n",
    "print(\"   ‚Ä¢ ROUGE-L: Longest common subsequence\")\n",
    "\n",
    "print(\"\\n3. Word Overlap (0-1):\")\n",
    "print(\"   ‚Ä¢ Simple ratio of shared words\")\n",
    "print(\"   ‚Ä¢ Fast to compute, interpretable\")\n",
    "\n",
    "print(\"\\n4. Exact Match:\")\n",
    "print(\"   ‚Ä¢ Binary: 1 if identical, 0 otherwise\")\n",
    "print(\"   ‚Ä¢ Strict but useful for specific phrases\")\n",
    "\n",
    "# Evaluate synthetic examples\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "reference = \"edge failure pattern with 45 dies affected at wafer periphery\"\n",
    "candidates = [\n",
    "    \"edge failure pattern with dies affected at wafer periphery\",  # Good match\n",
    "    \"wafer shows failures at the edge region\",  # Moderate match\n",
    "    \"random defects across the wafer\"  # Poor match\n",
    "]\n",
    "\n",
    "for i, candidate in enumerate(candidates, 1):\n",
    "    print(f\"\\nCandidate {i}: '{candidate}'\")\n",
    "    metrics = evaluate_caption_quality(candidate, reference)\n",
    "    print(f\"  BLEU: {metrics['bleu']}\")\n",
    "    print(f\"  ROUGE-1 F1: {metrics['rouge_1_f1']}\")\n",
    "    print(f\"  Word Overlap: {metrics['word_overlap']}\")\n",
    "    print(f\"  Exact Match: {metrics['exact_match']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1114bee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Real-World Project Ideas\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "#### 1. **Automated Failure Analysis Report Generator**\n",
    "**Objective:** Convert wafer maps and test plots into executive summary reports\n",
    "\n",
    "**Features:**\n",
    "- Input: Wafer maps, parametric plots, STDF data\n",
    "- Processing: CLIP classification + LLaVA captioning\n",
    "- Output: Natural language reports with insights\n",
    "- Metrics: Report accuracy, time savings vs manual\n",
    "\n",
    "**Business Value:** Reduce failure analysis time from 2-4 hours to 5 minutes per lot\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "# Pseudo-code structure\n",
    "def generate_failure_report(wafer_images, test_data):\n",
    "    # 1. Classify defect patterns (CLIP)\n",
    "    # 2. Generate captions (LLaVA)\n",
    "    # 3. Extract statistics from STDF\n",
    "    # 4. Compile into template report\n",
    "    # 5. Return PDF/HTML report\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Visual Test Documentation Search Engine**\n",
    "**Objective:** Search historical test data using natural language queries\n",
    "\n",
    "**Features:**\n",
    "- Index: 10K+ wafer maps with embeddings\n",
    "- Query: \"Show me edge failures from Q3 2024\"\n",
    "- Retrieval: CLIP image-text similarity\n",
    "- Result: Ranked wafer maps with descriptions\n",
    "\n",
    "**Business Value:** Engineers find similar failures 10x faster, improving root cause analysis\n",
    "\n",
    "**Tech Stack:**\n",
    "- CLIP for embeddings\n",
    "- Vector database (FAISS/Pinecone)\n",
    "- Streamlit UI\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Defect Detection with VQA Interface**\n",
    "**Objective:** Interactive defect inspection tool with natural language\n",
    "\n",
    "**Features:**\n",
    "- Upload die photo\n",
    "- Ask: \"Are there scratches?\", \"What's the defect type?\"\n",
    "- Model: Fine-tuned LLaVA on semiconductor images\n",
    "- Output: Answer + confidence + highlighted regions\n",
    "\n",
    "**Business Value:** Non-experts can inspect defects without training on defect taxonomy\n",
    "\n",
    "**Dataset:** Fine-tune on 5K+ labeled die photos\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Parametric Correlation Explainer**\n",
    "**Objective:** Explain correlations between test parameters using multimodal AI\n",
    "\n",
    "**Features:**\n",
    "- Input: Scatter plots (Vdd vs Idd, Freq vs Power)\n",
    "- Question: \"Why does Idd increase with voltage?\"\n",
    "- Model: GPT-4V or fine-tuned LLaVA\n",
    "- Output: Physics-based explanation in plain English\n",
    "\n",
    "**Business Value:** Accelerates debug for junior engineers, documents insights automatically\n",
    "\n",
    "---\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "#### 5. **Medical Image Question Answering**\n",
    "**Objective:** Assist radiologists with diagnostic questions\n",
    "\n",
    "**Features:**\n",
    "- Input: X-rays, MRIs, CT scans\n",
    "- Questions: \"Is there a fracture?\", \"Where is the abnormality?\"\n",
    "- Model: Fine-tuned BLIP-2 or LLaVA\n",
    "- Output: Answer + attention map\n",
    "\n",
    "**Dataset:** CheXpert, MIMIC-CXR (chest X-rays)\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. **Visual Shopping Assistant**\n",
    "**Objective:** Answer product questions from images\n",
    "\n",
    "**Features:**\n",
    "- Input: Product photos\n",
    "- Questions: \"What size is this?\", \"Is it waterproof?\"\n",
    "- Model: CLIP + GPT-3.5 retrieval\n",
    "- Output: Structured answers from image + text\n",
    "\n",
    "**Business Value:** Improve conversion rates, reduce customer service load\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. **Accessibility Alt-Text Generator**\n",
    "**Objective:** Auto-generate descriptive alt-text for web images\n",
    "\n",
    "**Features:**\n",
    "- Batch process website images\n",
    "- Generate detailed captions (ViT-GPT2)\n",
    "- Validate quality with BLEU/CIDEr\n",
    "- Export to HTML alt tags\n",
    "\n",
    "**Impact:** Make web content accessible to visually impaired users\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. **Video Surveillance Event Summarizer**\n",
    "**Objective:** Summarize security footage with natural language\n",
    "\n",
    "**Features:**\n",
    "- Input: Video frames (sampled every 5 sec)\n",
    "- Processing: CLIP classification per frame\n",
    "- Output: Timeline with event descriptions\n",
    "- Alerts: Anomaly detection + notification\n",
    "\n",
    "**Tech:** CLIP + frame sampling + timeline generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca39bc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Diagnostic & Validation\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "#### 1. **Poor Caption Quality**\n",
    "\n",
    "**Symptoms:**\n",
    "- Generic captions (\"a picture of something\")\n",
    "- Repetitive phrases\n",
    "- Missing key details\n",
    "\n",
    "**Debugging:**\n",
    "```python\n",
    "# Check model outputs\n",
    "outputs = decoder(features, captions)\n",
    "print(\"Logits shape:\", outputs.shape)\n",
    "print(\"Max probability:\", outputs.max().item())\n",
    "\n",
    "# Visualize attention weights\n",
    "attention_weights = attention_layer.attention_weights\n",
    "plt.imshow(attention_weights[0].cpu().detach())\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "- Increase beam width (3‚Üí5)\n",
    "- Fine-tune on domain-specific data\n",
    "- Add length penalty to avoid short captions\n",
    "- Use better pre-trained models (BLIP-2)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **CLIP Misclassification**\n",
    "\n",
    "**Symptoms:**\n",
    "- Low confidence scores (<0.3)\n",
    "- Wrong class predictions\n",
    "- Sensitivity to prompt wording\n",
    "\n",
    "**Debugging:**\n",
    "```python\n",
    "# Test different prompts\n",
    "prompts = [\n",
    "    \"a photo of {class}\",\n",
    "    \"a {class}\",\n",
    "    \"{class} in an image\"\n",
    "]\n",
    "\n",
    "for template in prompts:\n",
    "    classes = [template.format(class=c) for c in class_names]\n",
    "    pred, scores = classifier.classify(image, classes, return_scores=True)\n",
    "    print(f\"Template: {template} ‚Üí {pred} ({scores[pred]:.3f})\")\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "- Engineer better text prompts\n",
    "- Ensemble multiple prompt templates\n",
    "- Fine-tune CLIP on domain data\n",
    "- Use larger CLIP models (ViT-L/14)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **VQA Model Overfitting**\n",
    "\n",
    "**Symptoms:**\n",
    "- High train accuracy, low test accuracy\n",
    "- Model memorizes training answers\n",
    "- Poor generalization to new images\n",
    "\n",
    "**Solutions:**\n",
    "- Increase dropout (0.5 ‚Üí 0.7)\n",
    "- Add data augmentation (random crops, flips)\n",
    "- Use more diverse training data\n",
    "- Regularize with weight decay\n",
    "- Implement early stopping\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Memory Issues with Large Models**\n",
    "\n",
    "**Symptoms:**\n",
    "- CUDA out of memory errors\n",
    "- Slow inference times\n",
    "\n",
    "**Solutions:**\n",
    "```python\n",
    "# Mixed precision training\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "with autocast():\n",
    "    outputs = model(images, text)\n",
    "\n",
    "# Gradient accumulation\n",
    "for i, batch in enumerate(dataloader):\n",
    "    loss = model(batch) / accumulation_steps\n",
    "    loss.backward()\n",
    "    \n",
    "    if (i + 1) % accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# Model quantization\n",
    "import torch.quantization as quantization\n",
    "model_quantized = quantization.quantize_dynamic(\n",
    "    model, {nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Validation Checklist\n",
    "\n",
    "‚úÖ **Data Quality**\n",
    "- [ ] Images are properly preprocessed (resize, normalize)\n",
    "- [ ] Text is tokenized correctly\n",
    "- [ ] No data leakage between train/test\n",
    "- [ ] Balanced class distribution\n",
    "\n",
    "‚úÖ **Model Architecture**\n",
    "- [ ] Vision and text encoders are compatible\n",
    "- [ ] Fusion layer has sufficient capacity\n",
    "- [ ] Gradient flow is healthy (no vanishing/exploding)\n",
    "- [ ] Output dimensions match task requirements\n",
    "\n",
    "‚úÖ **Training Process**\n",
    "- [ ] Learning rate is appropriate (1e-5 to 1e-3)\n",
    "- [ ] Loss is decreasing steadily\n",
    "- [ ] Validation metrics improve\n",
    "- [ ] No overfitting (train-val gap < 10%)\n",
    "\n",
    "‚úÖ **Inference**\n",
    "- [ ] Model is in eval mode (`model.eval()`)\n",
    "- [ ] Gradients are disabled (`with torch.no_grad()`)\n",
    "- [ ] Beam search parameters are tuned\n",
    "- [ ] Output is post-processed correctly\n",
    "\n",
    "‚úÖ **Performance**\n",
    "- [ ] BLEU > 0.2 (for captioning)\n",
    "- [ ] Accuracy > 70% (for classification)\n",
    "- [ ] Inference time < 1 sec per image\n",
    "- [ ] Qualitative results make sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c100f64e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "### When to Use Multimodal LLMs\n",
    "\n",
    "‚úÖ **Use When:**\n",
    "- Need to understand both visual and textual information\n",
    "- Want to generate descriptions of images automatically\n",
    "- Building conversational AI that discusses images\n",
    "- Creating accessibility tools (alt-text, visual assistance)\n",
    "- Analyzing visual data at scale (defect detection, medical imaging)\n",
    "- Enabling natural language interfaces to visual systems\n",
    "\n",
    "‚ùå **Avoid When:**\n",
    "- Pure text or pure vision tasks (use specialized models)\n",
    "- Real-time critical systems (inference can be slow)\n",
    "- Privacy-sensitive applications without careful data handling\n",
    "- Limited compute resources (models are large)\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture Trade-offs\n",
    "\n",
    "| Approach | Pros | Cons | Best For |\n",
    "|----------|------|------|----------|\n",
    "| **CLIP** | Fast, zero-shot, well-aligned | No text generation | Classification, retrieval |\n",
    "| **Encoder-Decoder** | Simple, interpretable | Requires training data | Basic captioning |\n",
    "| **LLaVA** | Instruction following, conversational | Large, needs GPU | Complex reasoning, QA |\n",
    "| **BLIP-2** | Efficient (frozen encoders) | Complex architecture | Production systems |\n",
    "| **GPT-4V** | Best quality | Expensive, proprietary | High-stakes applications |\n",
    "\n",
    "---\n",
    "\n",
    "### Training Strategies\n",
    "\n",
    "**Pre-training:**\n",
    "1. **Contrastive Learning** (CLIP-style)\n",
    "   - Large-scale image-text pairs (millions)\n",
    "   - Learn aligned embedding space\n",
    "   - Enables zero-shot transfer\n",
    "\n",
    "2. **Generative Pre-training**\n",
    "   - Image captioning datasets (COCO, Flickr)\n",
    "   - Autoregressive text generation\n",
    "   - Builds fluency\n",
    "\n",
    "**Fine-tuning:**\n",
    "1. **Task-Specific**\n",
    "   - Adapt to VQA, captioning, etc.\n",
    "   - Smaller datasets (10K-100K)\n",
    "   - Tune only top layers\n",
    "\n",
    "2. **Instruction Tuning**\n",
    "   - Diverse visual instructions\n",
    "   - Improves generalization\n",
    "   - Better user interaction\n",
    "\n",
    "**Parameter-Efficient:**\n",
    "- LoRA, adapters (1-5% params)\n",
    "- Faster training, less memory\n",
    "- Good for domain adaptation\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "**Inference Speed:**\n",
    "```python\n",
    "# 1. Batch processing\n",
    "images = load_batch(paths)\n",
    "captions = model.batch_caption(images)\n",
    "\n",
    "# 2. Model quantization\n",
    "model_int8 = torch.quantization.quantize_dynamic(model)\n",
    "\n",
    "# 3. ONNX export\n",
    "torch.onnx.export(model, dummy_input, \"model.onnx\")\n",
    "\n",
    "# 4. Reduce beam width\n",
    "captions = model.caption(image, num_beams=3)  # vs 5\n",
    "```\n",
    "\n",
    "**Memory Efficiency:**\n",
    "```python\n",
    "# 1. Gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# 2. Mixed precision\n",
    "with torch.cuda.amp.autocast():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "# 3. Freeze encoders\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Hallucination**: Models generate plausible but incorrect details\n",
    "   - **Solution**: Use beam search, temperature control, post-processing validation\n",
    "\n",
    "2. **Prompt Sensitivity**: Small wording changes affect CLIP dramatically\n",
    "   - **Solution**: Template ensembling, prompt engineering\n",
    "\n",
    "3. **Bias**: Models inherit biases from training data\n",
    "   - **Solution**: Diverse training data, bias detection, human review\n",
    "\n",
    "4. **Limited Context**: Fixed image resolution loses details\n",
    "   - **Solution**: Multi-scale processing, region proposals\n",
    "\n",
    "5. **Caption Repetition**: Models repeat same phrases\n",
    "   - **Solution**: Diversity penalties, nucleus sampling\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**Development:**\n",
    "- Start with pre-trained models (HuggingFace)\n",
    "- Validate on small dataset before scaling\n",
    "- Use diverse evaluation metrics (BLEU + human eval)\n",
    "- Monitor for bias and fairness issues\n",
    "\n",
    "**Deployment:**\n",
    "- Cache frequent queries (CLIP embeddings)\n",
    "- Use async processing for batch jobs\n",
    "- Implement fallback mechanisms\n",
    "- Log outputs for continuous improvement\n",
    "\n",
    "**Fine-tuning:**\n",
    "- Start with small learning rates (1e-5)\n",
    "- Use domain-specific validation data\n",
    "- Monitor train-val gap for overfitting\n",
    "- Save checkpoints frequently\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Continue Learning:**\n",
    "- **079_RAG_Fundamentals.ipynb** - Combine multimodal LLMs with retrieval\n",
    "- **080_Advanced_RAG_Techniques.ipynb** - Multi-modal RAG systems\n",
    "- **074_LLM_Fine_Tuning.ipynb** - Fine-tune for specific domains\n",
    "\n",
    "**Practice Projects:**\n",
    "1. Build visual search engine for your photo library\n",
    "2. Create automated report generator for data visualizations\n",
    "3. Fine-tune LLaVA on semiconductor defect images\n",
    "4. Deploy CLIP-based image classifier API\n",
    "\n",
    "**Research Directions:**\n",
    "- Video understanding (extend to temporal dimension)\n",
    "- 3D vision-language models\n",
    "- Multi-lingual multimodal models\n",
    "- Efficient architectures for edge deployment\n",
    "\n",
    "---\n",
    "\n",
    "## üìö References & Resources\n",
    "\n",
    "**Papers:**\n",
    "- CLIP: \"Learning Transferable Visual Models From Natural Language Supervision\" (Radford et al., 2021)\n",
    "- LLaVA: \"Visual Instruction Tuning\" (Liu et al., 2023)\n",
    "- BLIP-2: \"Bootstrapping Language-Image Pre-training\" (Li et al., 2023)\n",
    "- Flamingo: \"Tackling Multiple Tasks with a Single Visual Language Model\" (Alayrac et al., 2022)\n",
    "\n",
    "**Code & Models:**\n",
    "- HuggingFace Transformers: https://github.com/huggingface/transformers\n",
    "- OpenAI CLIP: https://github.com/openai/CLIP\n",
    "- LLaVA: https://github.com/haotian-liu/LLaVA\n",
    "- BLIP-2: https://github.com/salesforce/LAVIS\n",
    "\n",
    "**Datasets:**\n",
    "- COCO Captions: https://cocodataset.org/\n",
    "- VQAv2: https://visualqa.org/\n",
    "- Flickr30K: https://shannon.cs.illinois.edu/DenotationGraph/\n",
    "- Conceptual Captions: https://ai.google.com/research/ConceptualCaptions/\n",
    "\n",
    "**Community:**\n",
    "- HuggingFace Forums: https://discuss.huggingface.co/\n",
    "- Papers With Code: https://paperswithcode.com/task/image-captioning\n",
    "- Reddit r/MachineLearning: Multimodal AI discussions\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You've completed the Multimodal LLMs notebook. You now understand how to build and deploy vision-language models for real-world applications, from zero-shot classification to visual question answering and instruction following."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
