{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e9229a4",
   "metadata": {},
   "source": [
    "# 115: Anomaly Detection\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** anomaly types: point, contextual, collective\n",
    "- **Implement** statistical methods: Z-score, IQR, Grubbs' test\n",
    "- **Build** machine learning detectors: Isolation Forest, One-Class SVM\n",
    "- **Apply** time series methods: ARIMA residuals, seasonal decomposition\n",
    "- **Use** deep learning: autoencoders for high-dimensional anomalies\n",
    "- **Design** anomaly detection systems for test escapes, parametric outliers, and fraud detection\n",
    "\n",
    "## üìö What is Anomaly Detection?\n",
    "\n",
    "**Anomaly detection** identifies rare observations that deviate significantly from normal patterns. Unlike classification (where labels exist), anomaly detection often works with **unlabeled data**, assuming most observations are normal.\n",
    "\n",
    "**Core concepts:**\n",
    "- **Point Anomaly**: Individual data point deviates (e.g., one device with extreme Vdd)\n",
    "- **Contextual Anomaly**: Abnormal in specific context (e.g., high yield on weekend is suspicious)\n",
    "- **Collective Anomaly**: Collection of points abnormal together (e.g., sequence of test failures)\n",
    "\n",
    "**Why Anomaly Detection?**\n",
    "- ‚úÖ **Rare Event Discovery**: Finds <1% outliers without manual labeling\n",
    "- ‚úÖ **Early Warning**: Detects issues before they cascade (process drift, equipment failure)\n",
    "- ‚úÖ **Quality Control**: Identifies defective units, test escapes\n",
    "- ‚úÖ **Fraud/Security**: Credit card fraud, network intrusions\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Test Escape Detection**\n",
    "- Input: Parametric test data (Vdd, Idd, freq) for 100K devices\n",
    "- Anomaly: Devices passing test but with unusual parameter combinations\n",
    "- Output: Flag 0.5% devices as potential test escapes ‚Üí send to extended test\n",
    "- Value: Reduce field failures, improve test coverage\n",
    "\n",
    "**Wafer Map Outlier Detection**\n",
    "- Input: Spatial yield data (die_x, die_y, pass/fail) for 500 wafers\n",
    "- Anomaly: Wafers with unusual spatial patterns (edge fails, clusters)\n",
    "- Output: Identify 3-5 wafers with anomalous signatures ‚Üí root cause analysis\n",
    "- Value: Detect lithography issues, contamination, equipment problems\n",
    "\n",
    "**Parametric Drift Monitoring**\n",
    "- Input: Daily average Vdd/Idd per lot over 6 months\n",
    "- Anomaly: Sudden shift in mean or increased variance\n",
    "- Output: Alert when drift exceeds 3œÉ from historical baseline\n",
    "- Value: Early process issue detection, prevent yield loss\n",
    "\n",
    "**Equipment Health Monitoring**\n",
    "- Input: Tester sensor data (temperature, power, test time) hourly\n",
    "- Anomaly: Unusual sensor readings indicating impending failure\n",
    "- Output: Predict equipment failure 24-48 hours in advance\n",
    "- Value: Preventive maintenance, minimize downtime\n",
    "\n",
    "## üîÑ Anomaly Detection Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Collect Data] --> B[Explore Distribution]\n",
    "    B --> C{Data Type?}\n",
    "    C -->|Univariate| D[Statistical Methods]\n",
    "    C -->|Multivariate| E[ML Methods]\n",
    "    C -->|Time Series| F[Temporal Methods]\n",
    "    D --> G[Z-score, IQR, Grubbs]\n",
    "    E --> H[Isolation Forest, One-Class SVM]\n",
    "    F --> I[ARIMA Residuals, Decomposition]\n",
    "    G --> J[Score Anomalies]\n",
    "    H --> J\n",
    "    I --> J\n",
    "    J --> K{Threshold?}\n",
    "    K -->|Manual| L[Domain Expertise]\n",
    "    K -->|Adaptive| M[Percentile-based]\n",
    "    L --> N[Flag Anomalies]\n",
    "    M --> N\n",
    "    N --> O[Investigate Root Cause]\n",
    "    O --> P[Feedback Loop]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style N fill:#ffe1e1\n",
    "    style O fill:#fffacd\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- 010: Linear Regression (statistical foundations)\n",
    "- 114: Time Series Forecasting (temporal patterns)\n",
    "\n",
    "**Next Steps:**\n",
    "- 051: Autoencoders (deep learning for anomaly detection)\n",
    "- 131: MLOps (deploying anomaly detectors)\n",
    "\n",
    "---\n",
    "\n",
    "Let's detect the unusual! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6084715",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38158cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"‚úÖ Libraries loaded successfully!\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"Scikit-learn ready for anomaly detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa678be",
   "metadata": {},
   "source": [
    "## 2. Statistical Anomaly Detection: Z-Score & IQR\n",
    "\n",
    "**Purpose:** Detect univariate outliers using statistical thresholds.\n",
    "\n",
    "**Key Points:**\n",
    "- **Z-Score**: $z = \\frac{x - \\mu}{\\sigma}$, flag if $|z| > 3$ (99.7% coverage)\n",
    "- **IQR Method**: Outliers outside $[Q_1 - 1.5 \\times IQR, Q_3 + 1.5 \\times IQR]$\n",
    "- **Grubbs' Test**: Statistical hypothesis test for single outlier (assumes normality)\n",
    "- **Assumptions**: Z-score needs normal distribution, IQR is distribution-free\n",
    "\n",
    "**Why This Matters:** Simple, interpretable, fast for real-time monitoring. Post-silicon: flag devices with Vdd/Idd outside 3œÉ limits for extended testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6555865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate device Vdd measurements (mostly normal, few outliers)\n",
    "np.random.seed(100)\n",
    "n_devices = 1000\n",
    "\n",
    "# Normal devices: Vdd ~ N(1.05, 0.01)\n",
    "normal_vdd = np.random.normal(1.05, 0.01, int(n_devices * 0.98))\n",
    "\n",
    "# Anomalous devices: extreme Vdd values\n",
    "anomaly_vdd = np.array([0.92, 0.94, 1.18, 1.20, 1.22])  # 5 outliers\n",
    "\n",
    "# Combine\n",
    "vdd_data = np.concatenate([normal_vdd, anomaly_vdd])\n",
    "np.random.shuffle(vdd_data)\n",
    "\n",
    "# True labels for evaluation (0 = normal, 1 = anomaly)\n",
    "true_labels = np.array([1 if v < 0.95 or v > 1.15 else 0 for v in vdd_data])\n",
    "\n",
    "print(\"Device Vdd Data Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total devices: {len(vdd_data)}\")\n",
    "print(f\"Mean Vdd: {vdd_data.mean():.4f} V\")\n",
    "print(f\"Std Dev: {vdd_data.std():.4f} V\")\n",
    "print(f\"Min: {vdd_data.min():.4f} V\")\n",
    "print(f\"Max: {vdd_data.max():.4f} V\")\n",
    "print(f\"True anomalies: {true_labels.sum()} ({true_labels.sum()/len(vdd_data)*100:.2f}%)\")\n",
    "\n",
    "# Method 1: Z-Score\n",
    "z_scores = np.abs((vdd_data - vdd_data.mean()) / vdd_data.std())\n",
    "z_threshold = 3.0\n",
    "z_anomalies = z_scores > z_threshold\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Z-Score Method (threshold = 3.0):\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Anomalies detected: {z_anomalies.sum()}\")\n",
    "print(f\"Detection rate: {z_anomalies.sum()/len(vdd_data)*100:.2f}%\")\n",
    "\n",
    "# Confusion matrix for Z-score\n",
    "z_tp = np.sum((z_anomalies == 1) & (true_labels == 1))  # True positives\n",
    "z_fp = np.sum((z_anomalies == 1) & (true_labels == 0))  # False positives\n",
    "z_tn = np.sum((z_anomalies == 0) & (true_labels == 0))  # True negatives\n",
    "z_fn = np.sum((z_anomalies == 0) & (true_labels == 1))  # False negatives\n",
    "\n",
    "z_precision = z_tp / (z_tp + z_fp) if (z_tp + z_fp) > 0 else 0\n",
    "z_recall = z_tp / (z_tp + z_fn) if (z_tp + z_fn) > 0 else 0\n",
    "z_f1 = 2 * z_precision * z_recall / (z_precision + z_recall) if (z_precision + z_recall) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {z_precision:.3f} (of detected, how many are true anomalies)\")\n",
    "print(f\"Recall: {z_recall:.3f} (of true anomalies, how many detected)\")\n",
    "print(f\"F1-Score: {z_f1:.3f}\")\n",
    "\n",
    "# Method 2: IQR\n",
    "Q1 = np.percentile(vdd_data, 25)\n",
    "Q3 = np.percentile(vdd_data, 75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "iqr_anomalies = (vdd_data < lower_bound) | (vdd_data > upper_bound)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"IQR Method (1.5 √ó IQR rule):\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Q1: {Q1:.4f} V, Q3: {Q3:.4f} V, IQR: {IQR:.4f} V\")\n",
    "print(f\"Lower bound: {lower_bound:.4f} V\")\n",
    "print(f\"Upper bound: {upper_bound:.4f} V\")\n",
    "print(f\"Anomalies detected: {iqr_anomalies.sum()}\")\n",
    "print(f\"Detection rate: {iqr_anomalies.sum()/len(vdd_data)*100:.2f}%\")\n",
    "\n",
    "# Confusion matrix for IQR\n",
    "iqr_tp = np.sum((iqr_anomalies == 1) & (true_labels == 1))\n",
    "iqr_fp = np.sum((iqr_anomalies == 1) & (true_labels == 0))\n",
    "iqr_precision = iqr_tp / (iqr_tp + iqr_fp) if (iqr_tp + iqr_fp) > 0 else 0\n",
    "iqr_recall = iqr_tp / (z_tp + z_fn) if (z_tp + z_fn) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {iqr_precision:.3f}\")\n",
    "print(f\"Recall: {iqr_recall:.3f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Distribution with anomalies\n",
    "axes[0, 0].hist(vdd_data, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(vdd_data.mean(), color='blue', linestyle='--', linewidth=2, label='Mean')\n",
    "axes[0, 0].axvline(vdd_data.mean() + 3*vdd_data.std(), color='red', linestyle=':', linewidth=2, label='¬±3œÉ')\n",
    "axes[0, 0].axvline(vdd_data.mean() - 3*vdd_data.std(), color='red', linestyle=':', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Vdd (V)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Device Vdd Distribution')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Z-scores\n",
    "axes[0, 1].scatter(range(len(vdd_data)), z_scores, c=z_anomalies, cmap='coolwarm', alpha=0.6, s=20)\n",
    "axes[0, 1].axhline(z_threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold = {z_threshold}')\n",
    "axes[0, 1].set_xlabel('Device Index')\n",
    "axes[0, 1].set_ylabel('|Z-Score|')\n",
    "axes[0, 1].set_title(f'Z-Score Anomaly Detection ({z_anomalies.sum()} detected)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. IQR boxplot\n",
    "axes[1, 0].boxplot(vdd_data, vert=True, patch_artist=True, \n",
    "                    boxprops=dict(facecolor='lightblue'))\n",
    "axes[1, 0].scatter(np.ones(iqr_anomalies.sum()), vdd_data[iqr_anomalies], \n",
    "                   color='red', s=50, zorder=3, label=f'Anomalies ({iqr_anomalies.sum()})')\n",
    "axes[1, 0].set_ylabel('Vdd (V)')\n",
    "axes[1, 0].set_title('IQR Method: Boxplot with Outliers')\n",
    "axes[1, 0].set_xticks([])\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. Method comparison\n",
    "methods = ['Z-Score', 'IQR']\n",
    "precisions = [z_precision, iqr_precision]\n",
    "recalls = [z_recall, iqr_recall]\n",
    "f1_scores = [z_f1, 2*iqr_precision*iqr_recall/(iqr_precision+iqr_recall) if (iqr_precision+iqr_recall) > 0 else 0]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.25\n",
    "\n",
    "axes[1, 1].bar(x - width, precisions, width, label='Precision', color='skyblue')\n",
    "axes[1, 1].bar(x, recalls, width, label='Recall', color='lightgreen')\n",
    "axes[1, 1].bar(x + width, f1_scores, width, label='F1-Score', color='salmon')\n",
    "axes[1, 1].set_xlabel('Method')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title('Method Comparison')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(methods)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "axes[1, 1].set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "print(f\"   Z-score: Good for normal distributions, sensitive to extreme values\")\n",
    "print(f\"   IQR: Robust to outliers, doesn't assume distribution\")\n",
    "print(f\"   Both methods detected {z_tp} of {true_labels.sum()} true anomalies\")\n",
    "print(f\"   Z-score more aggressive (higher recall), IQR more conservative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d23e214",
   "metadata": {},
   "source": [
    "## 3. Isolation Forest: Multivariate Anomaly Detection\n",
    "\n",
    "**Purpose:** Detect anomalies in high-dimensional data using tree-based isolation.\n",
    "\n",
    "**Key Points:**\n",
    "- **Concept**: Anomalies are easier to \"isolate\" (require fewer splits in decision tree)\n",
    "- **Algorithm**: Build random trees, measure path length to isolate each point\n",
    "- **Anomaly Score**: Shorter average path ‚Üí anomaly (isolated quickly)\n",
    "- **Advantages**: Handles high dimensions, no distance metrics, fast\n",
    "\n",
    "**Why This Matters:** Detects complex multivariate outliers that univariate methods miss. Post-silicon: find devices with unusual *combinations* of Vdd, Idd, freq (even if each individually looks normal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14563406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multivariate device test data: Vdd, Idd, frequency\n",
    "np.random.seed(200)\n",
    "n_devices = 1000\n",
    "\n",
    "# Normal devices: correlated Vdd and Idd (higher Vdd ‚Üí higher Idd)\n",
    "normal_vdd = np.random.normal(1.05, 0.01, int(n_devices * 0.97))\n",
    "normal_idd = 50 + 30 * (normal_vdd - 1.05) + np.random.normal(0, 2, int(n_devices * 0.97))  # Correlation\n",
    "normal_freq = np.random.normal(2400, 50, int(n_devices * 0.97))\n",
    "\n",
    "# Anomalous devices: unusual combinations\n",
    "anomaly_vdd = np.array([1.08, 1.09, 1.02, 1.03, 0.98, 0.97, 1.12, 1.13, 1.01, 1.00,\n",
    "                         1.06, 1.07, 1.04, 1.05, 1.05, 1.06, 1.04, 1.03, 1.07, 1.08,\n",
    "                         1.02, 1.03, 1.08, 1.09, 1.01, 1.02, 1.09, 1.10, 1.03, 1.04])\n",
    "anomaly_idd = np.array([30, 32, 70, 72, 75, 73, 35, 33, 80, 78,  # Unusual Vdd-Idd combinations\n",
    "                         25, 27, 85, 82, 90, 88, 20, 22, 95, 92,\n",
    "                         15, 18, 100, 98, 12, 14, 28, 26, 105, 102])\n",
    "anomaly_freq = np.array([2200, 2180, 2150, 2160, 2100, 2120, 2600, 2620, 2080, 2090,\n",
    "                          2050, 2070, 2650, 2630, 2700, 2680, 2000, 2020, 2750, 2720,\n",
    "                          1950, 1970, 2800, 2780, 1900, 1920, 2220, 2240, 2850, 2820])\n",
    "\n",
    "# Combine\n",
    "vdd_multi = np.concatenate([normal_vdd, anomaly_vdd])\n",
    "idd_multi = np.concatenate([normal_idd, anomaly_idd])\n",
    "freq_multi = np.concatenate([normal_freq, anomaly_freq])\n",
    "\n",
    "# Create dataframe\n",
    "df_devices = pd.DataFrame({\n",
    "    'vdd': vdd_multi,\n",
    "    'idd': idd_multi,\n",
    "    'freq': freq_multi\n",
    "})\n",
    "\n",
    "# Shuffle\n",
    "df_devices = df_devices.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# True labels (last 30 were anomalies before shuffling - need to track)\n",
    "true_labels_multi = np.array([0] * int(n_devices * 0.97) + [1] * 30)\n",
    "true_labels_multi = true_labels_multi[df_devices.index.values]  # Match shuffle\n",
    "\n",
    "print(\"Multivariate Device Test Data:\")\n",
    "print(\"=\" * 60)\n",
    "print(df_devices.describe())\n",
    "print(f\"\\nTrue anomalies: {true_labels_multi.sum()} ({true_labels_multi.sum()/len(df_devices)*100:.2f}%)\")\n",
    "\n",
    "# Fit Isolation Forest\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.03,  # Expected proportion of anomalies\n",
    "    random_state=42,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "# Predict (-1 = anomaly, 1 = normal)\n",
    "iso_predictions = iso_forest.fit_predict(df_devices)\n",
    "iso_anomalies = iso_predictions == -1\n",
    "\n",
    "# Anomaly scores (lower = more anomalous)\n",
    "iso_scores = iso_forest.score_samples(df_devices)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Isolation Forest Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Anomalies detected: {iso_anomalies.sum()}\")\n",
    "print(f\"Detection rate: {iso_anomalies.sum()/len(df_devices)*100:.2f}%\")\n",
    "\n",
    "# Confusion matrix\n",
    "iso_tp = np.sum((iso_anomalies == 1) & (true_labels_multi == 1))\n",
    "iso_fp = np.sum((iso_anomalies == 1) & (true_labels_multi == 0))\n",
    "iso_tn = np.sum((iso_anomalies == 0) & (true_labels_multi == 0))\n",
    "iso_fn = np.sum((iso_anomalies == 0) & (true_labels_multi == 1))\n",
    "\n",
    "iso_precision = iso_tp / (iso_tp + iso_fp) if (iso_tp + iso_fp) > 0 else 0\n",
    "iso_recall = iso_tp / (iso_tp + iso_fn) if (iso_tp + iso_fn) > 0 else 0\n",
    "iso_f1 = 2 * iso_precision * iso_recall / (iso_precision + iso_recall) if (iso_precision + iso_recall) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {iso_precision:.3f}\")\n",
    "print(f\"Recall: {iso_recall:.3f}\")\n",
    "print(f\"F1-Score: {iso_f1:.3f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Vdd vs Idd (2D projection)\n",
    "scatter1 = axes[0, 0].scatter(df_devices['vdd'], df_devices['idd'], \n",
    "                              c=iso_scores, cmap='RdYlGn', alpha=0.6, s=30)\n",
    "axes[0, 0].scatter(df_devices[iso_anomalies]['vdd'], df_devices[iso_anomalies]['idd'],\n",
    "                   edgecolors='red', facecolors='none', s=100, linewidths=2, label='Detected Anomalies')\n",
    "axes[0, 0].set_xlabel('Vdd (V)')\n",
    "axes[0, 0].set_ylabel('Idd (mA)')\n",
    "axes[0, 0].set_title('Isolation Forest: Vdd vs Idd')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0, 0], label='Anomaly Score')\n",
    "\n",
    "# 2. Vdd vs Freq\n",
    "scatter2 = axes[0, 1].scatter(df_devices['vdd'], df_devices['freq'], \n",
    "                              c=iso_scores, cmap='RdYlGn', alpha=0.6, s=30)\n",
    "axes[0, 1].scatter(df_devices[iso_anomalies]['vdd'], df_devices[iso_anomalies]['freq'],\n",
    "                   edgecolors='red', facecolors='none', s=100, linewidths=2, label='Detected Anomalies')\n",
    "axes[0, 1].set_xlabel('Vdd (V)')\n",
    "axes[0, 1].set_ylabel('Frequency (MHz)')\n",
    "axes[0, 1].set_title('Isolation Forest: Vdd vs Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[0, 1], label='Anomaly Score')\n",
    "\n",
    "# 3. Anomaly score distribution\n",
    "axes[1, 0].hist(iso_scores[~iso_anomalies], bins=50, alpha=0.7, label='Normal', color='green', edgecolor='black')\n",
    "axes[1, 0].hist(iso_scores[iso_anomalies], bins=20, alpha=0.7, label='Anomaly', color='red', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Anomaly Score')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Anomaly Score Distribution')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. PCA visualization (3D ‚Üí 2D)\n",
    "pca = PCA(n_components=2)\n",
    "pca_features = pca.fit_transform(df_devices)\n",
    "\n",
    "axes[1, 1].scatter(pca_features[:, 0], pca_features[:, 1], \n",
    "                   c=iso_scores, cmap='RdYlGn', alpha=0.6, s=30)\n",
    "axes[1, 1].scatter(pca_features[iso_anomalies, 0], pca_features[iso_anomalies, 1],\n",
    "                   edgecolors='red', facecolors='none', s=100, linewidths=2, label='Detected Anomalies')\n",
    "axes[1, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
    "axes[1, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
    "axes[1, 1].set_title('PCA Projection with Anomalies')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "print(f\"   Isolation Forest detects multivariate outliers (unusual combinations)\")\n",
    "print(f\"   Captured {iso_tp} of {true_labels_multi.sum()} true anomalies ({iso_recall*100:.1f}% recall)\")\n",
    "print(f\"   Some anomalies have normal Vdd/Idd individually but abnormal together\")\n",
    "print(f\"   Anomaly score < -0.1 typically indicates strong outlier\")\n",
    "print(f\"   PCA helps visualize high-D anomalies in 2D space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a7366",
   "metadata": {},
   "source": [
    "## üöÄ Real-World Project Templates\n",
    "\n",
    "Build production anomaly detection systems:\n",
    "\n",
    "### 1Ô∏è‚É£ **Post-Silicon Test Escape Detection**\n",
    "- **Objective**: Flag 0.1-0.5% devices with unusual parametric combinations before shipment  \n",
    "- **Data**: 1M devices/month, 50+ parametric tests (Vdd, Idd, freq, power, timing)  \n",
    "- **Success Metric**: Reduce field failure rate by 30%, precision > 20% (avoid false alarms)  \n",
    "- **Method**: Isolation Forest on normalized parameters, ensemble with Mahalanobis distance  \n",
    "- **Tech Stack**: Python, Spark for scale, real-time scoring API, feedback loop from field failures\n",
    "\n",
    "### 2Ô∏è‚É£ **Credit Card Fraud Detection**\n",
    "- **Objective**: Detect fraudulent transactions in real-time (<100ms latency)  \n",
    "- **Data**: 100M transactions/month, features: amount, merchant, time, location, user history  \n",
    "- **Success Metric**: Recall > 80% (catch fraud), precision > 40% (minimize false declines)  \n",
    "- **Method**: Isolation Forest + One-Class SVM ensemble, time-based anomaly scoring  \n",
    "- **Tech Stack**: Python, Kafka streaming, Redis caching, model serving (TensorFlow Serving)\n",
    "\n",
    "### 3Ô∏è‚É£ **Network Intrusion Detection**\n",
    "- **Objective**: Identify malicious network traffic patterns  \n",
    "- **Data**: 10B packets/day, features: packet size, protocol, source/dest IPs, timing  \n",
    "- **Success Metric**: Detect >90% attacks, false alarm rate < 1%  \n",
    "- **Method**: Autoencoder (learn normal traffic patterns), threshold reconstruction error  \n",
    "- **Tech Stack**: PyTorch, LSTM autoencoder, packet capture (pcap), SIEM integration\n",
    "\n",
    "### 4Ô∏è‚É£ **Manufacturing: Equipment Failure Prediction**\n",
    "- **Objective**: Detect anomalous sensor readings 24-48 hours before equipment failure  \n",
    "- **Data**: Sensor streams (vibration, temperature, pressure) at 1Hz from 500 machines  \n",
    "- **Success Metric**: 75% of failures predicted with 48hr lead time, uptime improvement 15%  \n",
    "- **Method**: Time series anomaly (STL decomposition + Z-score on residuals), LSTM autoencoder  \n",
    "- **Tech Stack**: Python, InfluxDB (time series DB), Grafana alerts, PySpark for batch processing\n",
    "\n",
    "### 5Ô∏è‚É£ **Healthcare: Sepsis Early Detection**\n",
    "- **Objective**: Identify patients at risk of sepsis 6-12 hours before clinical diagnosis  \n",
    "- **Data**: EHR vitals every 15min (heart rate, BP, temp, lab results), 50K patients/year  \n",
    "- **Success Metric**: AUC > 0.85, sensitivity > 80%, alert physicians with >6hr lead time  \n",
    "- **Method**: One-Class SVM on normal patient trajectories, Isolation Forest on lab anomalies  \n",
    "- **Tech Stack**: R/Python, EHR integration (FHIR), real-time dashboard, clinical workflow integration\n",
    "\n",
    "### 6Ô∏è‚É£ **E-Commerce: Fake Review Detection**\n",
    "- **Objective**: Identify suspicious product reviews (bots, incentivized, malicious)  \n",
    "- **Data**: 1M reviews/month, features: text, rating, timing, user history, product category  \n",
    "- **Success Metric**: Precision > 60% (manual review cost), catch 70% of fake reviews  \n",
    "- **Method**: Text embeddings (BERT) + behavioral features ‚Üí Isolation Forest, clustering  \n",
    "- **Tech Stack**: Python, Transformers, Elasticsearch, human-in-the-loop labeling\n",
    "\n",
    "### 7Ô∏è‚É£ **Cybersecurity: Insider Threat Detection**\n",
    "- **Objective**: Detect employees with anomalous access patterns (data exfiltration risk)  \n",
    "- **Data**: Login logs, file access, email patterns, network activity for 10K employees  \n",
    "- **Success Metric**: Detect >60% insider threats (rare event), investigation rate < 5 cases/week  \n",
    "- **Method**: User behavior profiling (Isolation Forest), graph anomaly (unusual connections)  \n",
    "- **Tech Stack**: Python, Neo4j (graph DB), Active Directory logs, UEBA platform\n",
    "\n",
    "### 8Ô∏è‚É£ **Energy: Smart Grid Anomaly Detection**\n",
    "- **Objective**: Detect electricity theft and meter malfunctions  \n",
    "- **Data**: Smart meter readings (15min intervals) for 1M households, weather data  \n",
    "- **Success Metric**: Detect 80% of theft/malfunctions, reduce false positives by 50% vs baseline  \n",
    "- **Method**: Time series clustering (normal consumption profiles), distance-based anomalies  \n",
    "- **Tech Stack**: Python, Hadoop for scale, time series DB, GIS visualization for spatial patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20c2ef2",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### What is Anomaly Detection?\n",
    "\n",
    "Identifying rare observations that deviate significantly from normal patterns, typically working with **unlabeled data** under the assumption that most observations are normal.\n",
    "\n",
    "### Anomaly Types\n",
    "\n",
    "| **Type** | **Definition** | **Example** | **Detection Method** |\n",
    "|----------|---------------|-------------|---------------------|\n",
    "| **Point Anomaly** | Individual data point unusual | Device Vdd = 1.20V (normal ~1.05V) | Z-score, IQR, Isolation Forest |\n",
    "| **Contextual Anomaly** | Abnormal in specific context | High yield on weekend (unusual timing) | Time series methods, conditional modeling |\n",
    "| **Collective Anomaly** | Sequence/group abnormal together | 10 consecutive test failures | Sequential pattern mining, HMM |\n",
    "\n",
    "### Statistical Methods\n",
    "\n",
    "**Z-Score:**\n",
    "- Formula: $z = \\frac{x - \\mu}{\\sigma}$\n",
    "- Threshold: $|z| > 3$ (99.7% of normal data within ¬±3œÉ)\n",
    "- **Pros**: Simple, interpretable, fast\n",
    "- **Cons**: Assumes normal distribution, sensitive to outliers in training\n",
    "- **Use When**: Univariate data, known distribution\n",
    "\n",
    "**IQR (Interquartile Range):**\n",
    "- Bounds: $[Q_1 - 1.5 \\times IQR, Q_3 + 1.5 \\times IQR]$ where $IQR = Q_3 - Q_1$\n",
    "- **Pros**: Robust to outliers, distribution-free\n",
    "- **Cons**: Fixed threshold (1.5√ó may not fit all domains)\n",
    "- **Use When**: Skewed distributions, presence of outliers\n",
    "\n",
    "**Grubbs' Test:**\n",
    "- Statistical hypothesis test for single outlier\n",
    "- Null hypothesis: No outliers present\n",
    "- **Pros**: Principled statistical test, p-value for confidence\n",
    "- **Cons**: Only detects one outlier at a time, requires normality\n",
    "\n",
    "### Machine Learning Methods\n",
    "\n",
    "**Isolation Forest:**\n",
    "- **Concept**: Anomalies are easier to isolate (shorter tree path lengths)\n",
    "- **Algorithm**: Build ensemble of random trees, measure average path length\n",
    "- **Anomaly Score**: Shorter path ‚Üí more anomalous\n",
    "- **Pros**: Handles high dimensions, fast (linear time), no distance metrics\n",
    "- **Cons**: Contamination parameter must be set, black box\n",
    "- **Use When**: Multivariate data, high dimensions (>10 features), large datasets\n",
    "\n",
    "**One-Class SVM:**\n",
    "- **Concept**: Learn decision boundary around normal data in high-D space\n",
    "- **Algorithm**: Map data to high-D space (kernel trick), find separating hyperplane\n",
    "- **Pros**: Flexible (kernel choice), theoretical foundation\n",
    "- **Cons**: Slow for large datasets, parameter tuning (ŒΩ, Œ≥), not interpretable\n",
    "- **Use When**: Small-medium datasets (<10K samples), need tight boundary\n",
    "\n",
    "**Autoencoder (Deep Learning):**\n",
    "- **Concept**: Neural network learns to compress and reconstruct normal data\n",
    "- **Anomaly Score**: Reconstruction error (MSE between input and output)\n",
    "- **Pros**: Learns complex patterns, handles images/sequences, nonlinear\n",
    "- **Cons**: Requires tuning, needs more data, can overfit to anomalies\n",
    "- **Use When**: High-dimensional data (images, time series), sufficient training data (>10K)\n",
    "\n",
    "### Time Series Anomaly Detection\n",
    "\n",
    "**ARIMA Residuals:**\n",
    "1. Fit ARIMA model on historical data\n",
    "2. Forecast expected values\n",
    "3. Flag if $|actual - forecast| > k \\times \\sigma_{residual}$\n",
    "- **Use**: Detect sudden shifts, spikes in temporal data\n",
    "\n",
    "**Seasonal Decomposition (STL):**\n",
    "1. Decompose: $Y_t = Trend_t + Seasonal_t + Residual_t$\n",
    "2. Apply anomaly detection to residuals\n",
    "- **Use**: Separate seasonal patterns from true anomalies\n",
    "\n",
    "**Prophet (Facebook):**\n",
    "- Robust trend + multiple seasonalities + holidays\n",
    "- Built-in anomaly detection via prediction intervals\n",
    "- **Use**: Business metrics with strong seasonality\n",
    "\n",
    "### Method Selection Guide\n",
    "\n",
    "```\n",
    "Data Characteristics:\n",
    "‚îú‚îÄ Univariate, normal distribution ‚Üí Z-Score\n",
    "‚îú‚îÄ Univariate, skewed/unknown ‚Üí IQR\n",
    "‚îú‚îÄ Multivariate, <10 features ‚Üí One-Class SVM, Mahalanobis\n",
    "‚îú‚îÄ Multivariate, >10 features ‚Üí Isolation Forest, Autoencoder\n",
    "‚îú‚îÄ Time series with trend ‚Üí ARIMA residuals, Prophet\n",
    "‚îú‚îÄ Time series with seasonality ‚Üí STL decomposition\n",
    "‚îî‚îÄ High-dimensional (images, text) ‚Üí Autoencoder, VAE\n",
    "\n",
    "Sample Size:\n",
    "‚îú‚îÄ < 1K samples ‚Üí Statistical methods, One-Class SVM\n",
    "‚îú‚îÄ 1K - 100K ‚Üí Isolation Forest\n",
    "‚îî‚îÄ > 100K ‚Üí Isolation Forest (parallel), Autoencoder\n",
    "\n",
    "Interpretability Needs:\n",
    "‚îú‚îÄ High (must explain) ‚Üí Z-score, IQR, decision trees\n",
    "‚îî‚îÄ Low (black box OK) ‚Üí Isolation Forest, Autoencoder\n",
    "\n",
    "Real-Time Requirements:\n",
    "‚îú‚îÄ <10ms ‚Üí Precomputed thresholds (Z-score, IQR)\n",
    "‚îú‚îÄ <100ms ‚Üí Isolation Forest (CPU)\n",
    "‚îî‚îÄ >100ms ‚Üí Autoencoder (GPU)\n",
    "```\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "**Without True Labels:**\n",
    "- **Contamination Rate**: Proportion flagged as anomalies (should match domain expectation)\n",
    "- **Silhouette Score**: How well anomalies separate from normal\n",
    "- **Manual Inspection**: Sample flagged cases for validation\n",
    "\n",
    "**With True Labels (rare):**\n",
    "- **Precision**: $\\frac{TP}{TP + FP}$ (of detected, how many are truly anomalous)\n",
    "- **Recall**: $\\frac{TP}{TP + FN}$ (of true anomalies, how many detected)\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **AUC-ROC**: Area under receiver operating characteristic curve\n",
    "- **Precision at K**: Precision in top K most anomalous predictions\n",
    "\n",
    "**Business Metrics:**\n",
    "- **Alert Fatigue**: False positive rate (too many false alarms ‚Üí ignored alerts)\n",
    "- **Lead Time**: How early anomalies detected before failure\n",
    "- **Cost Savings**: Value of prevented issues - investigation cost\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "- ‚ùå **Training on Contaminated Data**: If training set has anomalies, model learns them as normal\n",
    "- ‚ùå **Fixed Thresholds**: Static thresholds fail when data distribution shifts\n",
    "- ‚ùå **Ignoring Context**: 100 transactions/day is normal weekday, anomalous on Sunday\n",
    "- ‚ùå **Curse of Dimensionality**: Distance metrics break in high dimensions (>20 features)\n",
    "- ‚ùå **Imbalanced Evaluation**: 99% accuracy meaningless if only 0.1% are anomalies\n",
    "- ‚ùå **No Feedback Loop**: Anomalies change over time, models need retraining\n",
    "\n",
    "### Post-Silicon Applications\n",
    "\n",
    "**Test Escape Detection:**\n",
    "- **Input**: Vdd, Idd, freq, power, timing for 100K devices\n",
    "- **Method**: Isolation Forest on parametric space\n",
    "- **Threshold**: Top 0.5% anomaly scores ‚Üí extended test\n",
    "- **Value**: Reduce field failures 20-40%\n",
    "\n",
    "**Wafer Map Analysis:**\n",
    "- **Input**: Spatial yield patterns (die_x, die_y, pass/fail)\n",
    "- **Method**: Spatial clustering + distance from normal patterns\n",
    "- **Anomaly**: Unusual edge fails, center clusters, random patterns\n",
    "- **Value**: Detect lithography, contamination issues early\n",
    "\n",
    "**Parametric Drift:**\n",
    "- **Input**: Daily average Vdd/Idd per lot over 6 months\n",
    "- **Method**: ARIMA residuals, CUSUM charts\n",
    "- **Alert**: When forecast error exceeds 3œÉ for 3 consecutive days\n",
    "- **Value**: Early process issue detection, prevent yield loss\n",
    "\n",
    "**Equipment Health:**\n",
    "- **Input**: Tester sensor data (temp, power, vibration) hourly\n",
    "- **Method**: Autoencoder on sensor time series, threshold reconstruction error\n",
    "- **Predict**: Equipment failure 24-48 hours in advance\n",
    "- **Value**: Preventive maintenance, reduce downtime 15-25%\n",
    "\n",
    "### Advanced Topics (Not Covered)\n",
    "\n",
    "- **GANs for Anomaly Detection**: Train generator to produce normal data, flag what it can't generate\n",
    "- **Variational Autoencoders (VAE)**: Probabilistic autoencoder with better generalization\n",
    "- **Local Outlier Factor (LOF)**: Density-based anomaly detection\n",
    "- **DBSCAN**: Clustering-based outlier detection\n",
    "- **Time Series Discord**: Finding most unusual subsequence in time series\n",
    "- **Anomaly Detection in Graphs**: Detecting anomalous nodes/edges in networks\n",
    "\n",
    "### Tool Ecosystem\n",
    "\n",
    "**Python:**\n",
    "- **scikit-learn**: Isolation Forest, One-Class SVM, Elliptic Envelope, LOF\n",
    "- **PyOD**: Python Outlier Detection library (20+ algorithms)\n",
    "- **statsmodels**: Time series methods (ARIMA, STL)\n",
    "- **Prophet**: Facebook's forecasting library with anomaly detection\n",
    "- **TensorFlow/PyTorch**: Autoencoders, GANs for deep learning methods\n",
    "\n",
    "**R:**\n",
    "- **anomalize**: Time series anomaly detection (tidyverse-style)\n",
    "- **AnomalyDetection**: Twitter's breakout detection\n",
    "- **outliers**: Statistical tests (Grubbs, Dixon, etc.)\n",
    "\n",
    "**Commercial:**\n",
    "- **AWS SageMaker**: Built-in anomaly detection algorithms (Random Cut Forest)\n",
    "- **Azure Anomaly Detector**: Time series anomaly API\n",
    "- **Datadog**: Infrastructure monitoring with anomaly alerts\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start Simple**: Z-score/IQR before complex ML (often sufficient)\n",
    "2. **Validate Assumptions**: Check distribution before using Z-score\n",
    "3. **Domain Knowledge**: Use expert thresholds when available (spec limits, safety margins)\n",
    "4. **Adaptive Thresholds**: Update thresholds as data evolves (rolling window)\n",
    "5. **Human-in-Loop**: Present top anomalies for expert review, capture feedback\n",
    "6. **Monitor Performance**: Track false positive rate, lead time, business impact\n",
    "7. **Explainability**: For detected anomalies, show which features drove the score\n",
    "8. **Ensemble Methods**: Combine multiple detectors (vote, average scores)\n",
    "\n",
    "### Next Steps\n",
    "- **Notebook 051**: Autoencoders (deep learning for anomaly detection)\n",
    "- **Notebook 131**: MLOps (deploying anomaly detection systems)\n",
    "- **Advanced**: Graph anomaly detection, causal anomaly detection, explainable AI for outliers\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: *\"One person's noise is another person's signal!\"* üîç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95993d9",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "**When to Use**: High-stakes monitoring (fraud, manufacturing defects, network intrusion), unlabeled data, real-time detection  \n",
    "**Limitations**: High false positives, threshold tuning, concept drift over time  \n",
    "**Alternatives**: Supervised classification (if labels available), rule-based systems, statistical process control  \n",
    "**Best Practices**: Ensemble methods (Isolation Forest + LOF), validate with domain experts, adaptive thresholds, explainability (SHAP)  \n",
    "\n",
    "## üîç Diagnostic & Mastery\n",
    "\n",
    "**Post-Silicon**: Detect parametric test outliers (80 features), wafer spatial anomalies, ATE tester health drift ‚Üí save $2.8M-$10.5M/year\n",
    "\n",
    "‚úÖ Master Isolation Forest, LOF, Autoencoders, One-Class SVM  \n",
    "‚úÖ Apply to semiconductor test outlier detection and equipment monitoring\n",
    "\n",
    "**Next Steps**: 160_Multi_Variate_Anomaly_Detection, 036_Isolation_Forest\n",
    "\n",
    "## üìà Progress\n",
    "\n",
    "‚úÖ 30 notebooks complete | ~83.4% done (146/175) | Next: 9-cell batch continues"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
