{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93334c5b",
   "metadata": {},
   "source": [
    "# 110: Experimental Design & A/B Testing\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** statistical hypothesis testing and experimental design principles\n",
    "- **Calculate** sample sizes using power analysis for reliable experiments\n",
    "- **Implement** A/B tests with proper statistical rigor (t-tests, chi-square, Bayesian)\n",
    "- **Apply** multi-armed bandit strategies for adaptive experimentation\n",
    "- **Evaluate** experiment validity using p-values, confidence intervals, and effect sizes\n",
    "- **Design** production-ready A/B testing frameworks for post-silicon and product development\n",
    "\n",
    "## üìö What is Experimental Design?\n",
    "\n",
    "**Experimental design** is the systematic planning of experiments to answer specific questions while controlling for confounding variables. It ensures that observed effects are due to treatments (interventions) rather than chance or bias.\n",
    "\n",
    "**A/B testing** (also called split testing) is the most common experimental design in tech, comparing two variants (A = control, B = treatment) to determine which performs better on a key metric. It's the foundation of data-driven decision making in product development, marketing, and engineering.\n",
    "\n",
    "**Why Experimental Design?**\n",
    "- ‚úÖ **Causal Inference**: Proves causation (not just correlation) through randomization\n",
    "- ‚úÖ **Risk Mitigation**: Test changes on small groups before full rollout\n",
    "- ‚úÖ **Quantified Impact**: Measure effect size with statistical confidence\n",
    "- ‚úÖ **Optimization**: Continuously improve products/processes through iteration\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Test Program Optimization**\n",
    "- Input: Test flow variants (e.g., parallel vs sequential test insertion)\n",
    "- Output: Test time reduction % with maintained defect coverage\n",
    "- Value: $500K+ annual savings per tester (faster throughput)\n",
    "\n",
    "**Device Binning Strategy**\n",
    "- Input: Alternative voltage/frequency bin thresholds\n",
    "- Output: Yield improvement % vs product quality trade-off\n",
    "- Value: 2-5% yield gain = millions in revenue for high-volume products\n",
    "\n",
    "**Burn-In Process Effectiveness**\n",
    "- Input: Burn-in duration variants (24hr vs 48hr vs 72hr)\n",
    "- Output: Infant mortality reduction % vs cost increase\n",
    "- Value: Reduced field failures, warranty costs\n",
    "\n",
    "**Parametric Limit Tuning**\n",
    "- Input: Tighter vs relaxed test limits for Vdd, Idd, frequency\n",
    "- Output: Defect escape rate vs yield loss\n",
    "- Value: Balance quality (customer satisfaction) with profitability\n",
    "\n",
    "## üîÑ Experimental Design Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Define Hypothesis] --> B[Calculate Sample Size]\n",
    "    B --> C[Randomize Assignment]\n",
    "    C --> D[Run Experiment]\n",
    "    D --> E[Collect Data]\n",
    "    E --> F[Statistical Test]\n",
    "    F --> G{Significant?}\n",
    "    G -->|Yes| H[Implement Winner]\n",
    "    G -->|No| I[No Change]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style H fill:#e1ffe1\n",
    "    style I fill:#ffe1e1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- 010: Linear Regression (hypothesis testing basics)\n",
    "- 106: A/B Testing ML Models (model comparison context)\n",
    "\n",
    "**Next Steps:**\n",
    "- 111: Causal Inference (advanced treatment effect estimation)\n",
    "- 112: Bayesian Statistics (Bayesian A/B testing)\n",
    "\n",
    "---\n",
    "\n",
    "Let's build rigorous experimentation systems! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53fa3a4",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2557fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency, ttest_ind, mannwhitneyu\n",
    "from statsmodels.stats.power import TTestIndPower, NormalIndPower\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully!\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7880c8",
   "metadata": {},
   "source": [
    "## 2. Statistical Power Analysis\n",
    "\n",
    "**Purpose:** Calculate required sample size to detect meaningful effects with high confidence.\n",
    "\n",
    "**Key Points:**\n",
    "- **Statistical Power (1 - Œ≤)**: Probability of detecting true effect (target: 80%+)\n",
    "- **Significance Level (Œ±)**: Probability of false positive (Type I error, typically 0.05)\n",
    "- **Effect Size (Cohen's d)**: Magnitude of difference (small=0.2, medium=0.5, large=0.8)\n",
    "- **Sample Size Trade-off**: Larger samples detect smaller effects but cost more\n",
    "\n",
    "**Why This Matters:** Underpowered experiments waste resources and miss real effects. Overpowered experiments waste money detecting trivial differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c268fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power analysis for t-test (continuous metrics)\n",
    "def calculate_sample_size_ttest(effect_size, alpha=0.05, power=0.8):\n",
    "    \"\"\"\n",
    "    Calculate required sample size per group for independent t-test.\n",
    "    \n",
    "    Parameters:\n",
    "    - effect_size: Cohen's d (standardized difference between means)\n",
    "    - alpha: Significance level (Type I error rate)\n",
    "    - power: Statistical power (1 - Type II error rate)\n",
    "    \n",
    "    Returns:\n",
    "    - Required sample size per group\n",
    "    \"\"\"\n",
    "    analysis = TTestIndPower()\n",
    "    sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, ratio=1.0)\n",
    "    return int(np.ceil(sample_size))\n",
    "\n",
    "# Example: Post-silicon test time reduction experiment\n",
    "# Current avg test time: 5.0s, New flow: 4.5s, Std: 0.8s\n",
    "# Effect size = (5.0 - 4.5) / 0.8 = 0.625 (medium-large effect)\n",
    "\n",
    "effect_sizes = [0.2, 0.5, 0.8]  # Small, medium, large\n",
    "labels = ['Small (0.2)', 'Medium (0.5)', 'Large (0.8)']\n",
    "\n",
    "print(\"Sample Size Requirements for t-test:\")\n",
    "print(\"=\" * 60)\n",
    "for effect, label in zip(effect_sizes, labels):\n",
    "    n = calculate_sample_size_ttest(effect)\n",
    "    print(f\"Effect Size {label:15s}: {n:4d} samples per group ({n*2} total)\")\n",
    "\n",
    "# Visualize power curves\n",
    "sample_sizes = np.arange(10, 500, 10)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for effect, label in zip(effect_sizes, labels):\n",
    "    analysis = TTestIndPower()\n",
    "    power_values = [analysis.solve_power(effect_size=effect, nobs1=n, alpha=0.05, ratio=1.0) for n in sample_sizes]\n",
    "    ax.plot(sample_sizes, power_values, label=label, linewidth=2)\n",
    "\n",
    "ax.axhline(y=0.8, color='red', linestyle='--', label='Target Power (80%)')\n",
    "ax.set_xlabel('Sample Size per Group')\n",
    "ax.set_ylabel('Statistical Power')\n",
    "ax.set_title('Power Curves for Different Effect Sizes (Œ± = 0.05)')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Interpretation: To detect a medium effect (0.5) with 80% power, need ~64 devices per variant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee184bf",
   "metadata": {},
   "source": [
    "## 3. A/B Test: Continuous Metrics (t-test)\n",
    "\n",
    "**Purpose:** Compare means of two groups when the metric is continuous (e.g., test time, voltage, revenue).\n",
    "\n",
    "**Key Points:**\n",
    "- **Null Hypothesis (H‚ÇÄ)**: Mean_A = Mean_B (no difference)\n",
    "- **Alternative Hypothesis (H‚ÇÅ)**: Mean_A ‚â† Mean_B (two-tailed test)\n",
    "- **T-statistic**: Measures how many standard errors the means differ by\n",
    "- **P-value**: Probability of observing this difference if H‚ÇÄ is true (p < 0.05 ‚Üí reject H‚ÇÄ)\n",
    "\n",
    "**Why This Matters:** T-tests are the workhorse of A/B testing for continuous outcomes. Post-silicon examples: test time, power consumption, frequency performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c30382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate A/B test: Test flow optimization\n",
    "# Control (A): Current test flow, avg = 5.0s, std = 0.8s\n",
    "# Treatment (B): Optimized flow, avg = 4.5s, std = 0.7s (10% faster)\n",
    "\n",
    "np.random.seed(123)\n",
    "n_samples = 100  # Per group (based on power analysis for effect size ~0.625)\n",
    "\n",
    "# Generate data\n",
    "test_time_A = np.random.normal(5.0, 0.8, n_samples)  # Control\n",
    "test_time_B = np.random.normal(4.5, 0.7, n_samples)  # Treatment\n",
    "\n",
    "# Create dataframe\n",
    "ab_data = pd.DataFrame({\n",
    "    'variant': ['A'] * n_samples + ['B'] * n_samples,\n",
    "    'test_time_sec': np.concatenate([test_time_A, test_time_B])\n",
    "})\n",
    "\n",
    "# Descriptive statistics\n",
    "summary_stats = ab_data.groupby('variant')['test_time_sec'].agg(['mean', 'std', 'count'])\n",
    "print(\"A/B Test Summary Statistics:\")\n",
    "print(summary_stats)\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = ttest_ind(test_time_A, test_time_B)\n",
    "mean_diff = test_time_A.mean() - test_time_B.mean()\n",
    "percent_improvement = (mean_diff / test_time_A.mean()) * 100\n",
    "\n",
    "# Cohen's d (effect size)\n",
    "pooled_std = np.sqrt(((n_samples - 1) * test_time_A.std()**2 + (n_samples - 1) * test_time_B.std()**2) / (2 * n_samples - 2))\n",
    "cohens_d = mean_diff / pooled_std\n",
    "\n",
    "# Confidence interval (95%)\n",
    "se_diff = np.sqrt(test_time_A.var()/n_samples + test_time_B.var()/n_samples)\n",
    "ci_lower = mean_diff - 1.96 * se_diff\n",
    "ci_upper = mean_diff + 1.96 * se_diff\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"T-Test Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Mean Difference (A - B): {mean_diff:.3f} seconds\")\n",
    "print(f\"Improvement: {percent_improvement:.2f}%\")\n",
    "print(f\"T-statistic: {t_stat:.3f}\")\n",
    "print(f\"P-value: {p_value:.6f}\")\n",
    "print(f\"Cohen's d (Effect Size): {cohens_d:.3f}\")\n",
    "print(f\"95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\n‚úÖ RESULT: Statistically significant (p < 0.05)\")\n",
    "    print(f\"   Decision: Adopt variant B (optimized test flow)\")\n",
    "    print(f\"   Expected savings: {percent_improvement:.1f}% test time reduction\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå RESULT: Not statistically significant (p ‚â• 0.05)\")\n",
    "    print(f\"   Decision: Insufficient evidence to change\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram comparison\n",
    "axes[0].hist(test_time_A, bins=20, alpha=0.6, label='Control (A)', color='blue', edgecolor='black')\n",
    "axes[0].hist(test_time_B, bins=20, alpha=0.6, label='Treatment (B)', color='green', edgecolor='black')\n",
    "axes[0].axvline(test_time_A.mean(), color='blue', linestyle='--', linewidth=2, label=f'Mean A: {test_time_A.mean():.2f}s')\n",
    "axes[0].axvline(test_time_B.mean(), color='green', linestyle='--', linewidth=2, label=f'Mean B: {test_time_B.mean():.2f}s')\n",
    "axes[0].set_xlabel('Test Time (seconds)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution Comparison')\n",
    "axes[0].legend()\n",
    "\n",
    "# Boxplot comparison\n",
    "ab_data.boxplot(column='test_time_sec', by='variant', ax=axes[1])\n",
    "axes[1].set_title('Test Time by Variant')\n",
    "axes[1].set_xlabel('Variant')\n",
    "axes[1].set_ylabel('Test Time (seconds)')\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bbc698",
   "metadata": {},
   "source": [
    "## 4. A/B Test: Proportion Metrics (Chi-Square Test)\n",
    "\n",
    "**Purpose:** Compare conversion rates or proportions between two groups (e.g., pass/fail rates, click-through rates).\n",
    "\n",
    "**Key Points:**\n",
    "- **Use Case**: Binary outcomes (pass/fail, click/no-click, buy/no-buy)\n",
    "- **Chi-Square Test**: Tests independence between categorical variables\n",
    "- **Expected vs Observed Counts**: Compares actual data to what's expected under H‚ÇÄ\n",
    "- **Contingency Table**: 2x2 table showing counts for each variant √ó outcome combination\n",
    "\n",
    "**Why This Matters:** Most product metrics are proportions (conversion rate, defect rate, yield%). Chi-square is the standard test for categorical A/B tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc44c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate A/B test: Device binning strategy\n",
    "# Control (A): Conservative limits, 88% pass rate\n",
    "# Treatment (B): Relaxed limits, 92% pass rate (4% yield improvement)\n",
    "\n",
    "np.random.seed(456)\n",
    "n_devices = 500  # Per variant\n",
    "\n",
    "# Generate data\n",
    "pass_rate_A = 0.88\n",
    "pass_rate_B = 0.92\n",
    "\n",
    "devices_A = np.random.binomial(1, pass_rate_A, n_devices)  # 1 = pass, 0 = fail\n",
    "devices_B = np.random.binomial(1, pass_rate_B, n_devices)\n",
    "\n",
    "# Create contingency table\n",
    "pass_A = devices_A.sum()\n",
    "fail_A = n_devices - pass_A\n",
    "pass_B = devices_B.sum()\n",
    "fail_B = n_devices - pass_B\n",
    "\n",
    "contingency_table = np.array([\n",
    "    [pass_A, fail_A],  # Control\n",
    "    [pass_B, fail_B]   # Treatment\n",
    "])\n",
    "\n",
    "# Chi-square test\n",
    "chi2, p_value_chi, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Effect size (Cramer's V)\n",
    "n_total = contingency_table.sum()\n",
    "cramers_v = np.sqrt(chi2 / n_total)\n",
    "\n",
    "# Observed rates\n",
    "observed_rate_A = pass_A / n_devices\n",
    "observed_rate_B = pass_B / n_devices\n",
    "rate_diff = observed_rate_B - observed_rate_A\n",
    "relative_lift = (rate_diff / observed_rate_A) * 100\n",
    "\n",
    "print(\"Contingency Table (Observed Counts):\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"{'Variant':<10} {'Pass':<10} {'Fail':<10} {'Total':<10}\")\n",
    "print(f\"{'A (Control)':<10} {pass_A:<10} {fail_A:<10} {n_devices:<10}\")\n",
    "print(f\"{'B (Treatment)':<10} {pass_B:<10} {fail_B:<10} {n_devices:<10}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Chi-Square Test Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Pass Rate A: {observed_rate_A:.4f} ({observed_rate_A*100:.2f}%)\")\n",
    "print(f\"Pass Rate B: {observed_rate_B:.4f} ({observed_rate_B*100:.2f}%)\")\n",
    "print(f\"Absolute Difference: {rate_diff:.4f} ({rate_diff*100:.2f} percentage points)\")\n",
    "print(f\"Relative Lift: {relative_lift:.2f}%\")\n",
    "print(f\"Chi-Square Statistic: {chi2:.3f}\")\n",
    "print(f\"P-value: {p_value_chi:.6f}\")\n",
    "print(f\"Cramer's V (Effect Size): {cramers_v:.3f}\")\n",
    "\n",
    "if p_value_chi < 0.05:\n",
    "    print(f\"\\n‚úÖ RESULT: Statistically significant (p < 0.05)\")\n",
    "    print(f\"   Decision: Adopt variant B (relaxed binning limits)\")\n",
    "    print(f\"   Expected yield gain: {rate_diff*100:.2f} percentage points\")\n",
    "    print(f\"   Revenue impact: ~${(rate_diff * n_devices * 2 * 10):,.0f} (assuming $10/device, 500K units/year)\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå RESULT: Not statistically significant (p ‚â• 0.05)\")\n",
    "    print(f\"   Decision: Insufficient evidence to change binning strategy\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart: Pass rates\n",
    "variants = ['A (Control)', 'B (Treatment)']\n",
    "pass_rates = [observed_rate_A, observed_rate_B]\n",
    "colors = ['blue', 'green']\n",
    "\n",
    "axes[0].bar(variants, pass_rates, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[0].axhline(y=0.88, color='red', linestyle='--', label='Baseline (88%)')\n",
    "axes[0].set_ylabel('Pass Rate')\n",
    "axes[0].set_title('Device Pass Rate by Variant')\n",
    "axes[0].set_ylim(0.8, 1.0)\n",
    "axes[0].legend()\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (variant, rate) in enumerate(zip(variants, pass_rates)):\n",
    "    axes[0].text(i, rate + 0.01, f'{rate*100:.2f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# Stacked bar: Pass/Fail counts\n",
    "pass_counts = [pass_A, pass_B]\n",
    "fail_counts = [fail_A, fail_B]\n",
    "\n",
    "axes[1].bar(variants, pass_counts, label='Pass', color='green', alpha=0.7, edgecolor='black')\n",
    "axes[1].bar(variants, fail_counts, bottom=pass_counts, label='Fail', color='red', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Pass/Fail Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3301edb5",
   "metadata": {},
   "source": [
    "## 5. Bayesian A/B Testing\n",
    "\n",
    "**Purpose:** Use Bayesian inference to estimate probability that variant B is better than A (more intuitive than p-values).\n",
    "\n",
    "**Key Points:**\n",
    "- **Prior Distribution**: Initial belief before seeing data (e.g., Beta(1,1) = uniform)\n",
    "- **Likelihood**: Observed data (successes/failures)\n",
    "- **Posterior Distribution**: Updated belief after seeing data (Beta distribution)\n",
    "- **Probability B > A**: Direct probability statement (\"95% chance B is better\")\n",
    "\n",
    "**Why This Matters:** Bayesian A/B testing provides intuitive probabilities instead of confusing p-values. Allows early stopping when posterior probability is convincing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "\n",
    "# Use same data from chi-square test\n",
    "# Prior: Beta(1, 1) = uniform (no prior knowledge)\n",
    "alpha_prior = 1\n",
    "beta_prior = 1\n",
    "\n",
    "# Posterior parameters (Beta distribution)\n",
    "# Posterior ~ Beta(alpha_prior + successes, beta_prior + failures)\n",
    "alpha_A = alpha_prior + pass_A\n",
    "beta_A = beta_prior + fail_A\n",
    "\n",
    "alpha_B = alpha_prior + pass_B\n",
    "beta_B = beta_prior + fail_B\n",
    "\n",
    "# Sample from posterior distributions\n",
    "n_samples_bayes = 100000\n",
    "samples_A = np.random.beta(alpha_A, beta_A, n_samples_bayes)\n",
    "samples_B = np.random.beta(alpha_B, beta_B, n_samples_bayes)\n",
    "\n",
    "# Probability that B > A\n",
    "prob_B_better = (samples_B > samples_A).mean()\n",
    "\n",
    "# Expected loss if we choose wrong variant\n",
    "loss_if_choose_A = np.maximum(samples_B - samples_A, 0).mean()  # Loss if we pick A but B is better\n",
    "loss_if_choose_B = np.maximum(samples_A - samples_B, 0).mean()  # Loss if we pick B but A is better\n",
    "\n",
    "# Credible intervals (Bayesian equivalent of confidence intervals)\n",
    "ci_A = np.percentile(samples_A, [2.5, 97.5])\n",
    "ci_B = np.percentile(samples_B, [2.5, 97.5])\n",
    "\n",
    "print(\"Bayesian A/B Test Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Posterior Mean (A): {samples_A.mean():.4f}\")\n",
    "print(f\"Posterior Mean (B): {samples_B.mean():.4f}\")\n",
    "print(f\"95% Credible Interval (A): [{ci_A[0]:.4f}, {ci_A[1]:.4f}]\")\n",
    "print(f\"95% Credible Interval (B): [{ci_B[0]:.4f}, {ci_B[1]:.4f}]\")\n",
    "print(f\"\\nProbability B > A: {prob_B_better:.4f} ({prob_B_better*100:.2f}%)\")\n",
    "print(f\"Expected Loss if Choose A: {loss_if_choose_A:.6f}\")\n",
    "print(f\"Expected Loss if Choose B: {loss_if_choose_B:.6f}\")\n",
    "\n",
    "if prob_B_better > 0.95:\n",
    "    print(f\"\\n‚úÖ DECISION: Choose variant B (>{prob_B_better*100:.0f}% probability of being better)\")\n",
    "elif prob_B_better < 0.05:\n",
    "    print(f\"\\n‚úÖ DECISION: Choose variant A ({(1-prob_B_better)*100:.0f}% probability of being better)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è DECISION: Inconclusive - continue testing or use business judgment\")\n",
    "\n",
    "# Visualization: Posterior distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Posterior distributions\n",
    "x = np.linspace(0.8, 1.0, 1000)\n",
    "posterior_A = beta.pdf(x, alpha_A, beta_A)\n",
    "posterior_B = beta.pdf(x, alpha_B, beta_B)\n",
    "\n",
    "axes[0].plot(x, posterior_A, label='A (Control)', color='blue', linewidth=2)\n",
    "axes[0].plot(x, posterior_B, label='B (Treatment)', color='green', linewidth=2)\n",
    "axes[0].fill_between(x, posterior_A, alpha=0.3, color='blue')\n",
    "axes[0].fill_between(x, posterior_B, alpha=0.3, color='green')\n",
    "axes[0].set_xlabel('Pass Rate')\n",
    "axes[0].set_ylabel('Probability Density')\n",
    "axes[0].set_title('Posterior Distributions')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Difference distribution (B - A)\n",
    "difference = samples_B - samples_A\n",
    "axes[1].hist(difference, bins=50, color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='No Difference')\n",
    "axes[1].axvline(x=difference.mean(), color='green', linestyle='-', linewidth=2, label=f'Mean: {difference.mean():.4f}')\n",
    "axes[1].set_xlabel('Pass Rate Difference (B - A)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Pass Rate Improvement')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Interpretation: With {prob_B_better*100:.1f}% probability, variant B has higher pass rate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ae5960",
   "metadata": {},
   "source": [
    "## 6. Multi-Armed Bandit (Thompson Sampling)\n",
    "\n",
    "**Purpose:** Adaptive experimentation that balances exploration (trying variants) and exploitation (using best variant).\n",
    "\n",
    "**Key Points:**\n",
    "- **Regret Minimization**: Reduce opportunity cost of using suboptimal variants during testing\n",
    "- **Thompson Sampling**: Bayesian algorithm that samples from posterior and picks best sample\n",
    "- **Dynamic Allocation**: Automatically shifts traffic to better-performing variants\n",
    "- **Continuous Learning**: No fixed test duration, keeps optimizing\n",
    "\n",
    "**Why This Matters:** Traditional A/B tests waste 50% of traffic on inferior variants. Bandits reduce this waste while still learning which is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81c8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multi-armed bandit for device binning optimization\n",
    "# 3 variants: Conservative (88%), Moderate (91%), Aggressive (93%)\n",
    "\n",
    "class ThompsonSamplingBandit:\n",
    "    def __init__(self, n_arms, true_rates):\n",
    "        self.n_arms = n_arms\n",
    "        self.true_rates = true_rates  # True pass rates (unknown to algorithm)\n",
    "        \n",
    "        # Prior: Beta(1, 1) for each arm\n",
    "        self.alpha = np.ones(n_arms)\n",
    "        self.beta = np.ones(n_arms)\n",
    "        \n",
    "        # Tracking\n",
    "        self.pulls = np.zeros(n_arms)\n",
    "        self.successes = np.zeros(n_arms)\n",
    "        self.cumulative_reward = 0\n",
    "        self.cumulative_regret = 0\n",
    "        self.history = []\n",
    "        \n",
    "    def select_arm(self):\n",
    "        \"\"\"Thompson Sampling: Sample from each arm's posterior and pick highest.\"\"\"\n",
    "        samples = [np.random.beta(self.alpha[i], self.beta[i]) for i in range(self.n_arms)]\n",
    "        return np.argmax(samples)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        \"\"\"Update posterior after observing reward (1 = pass, 0 = fail).\"\"\"\n",
    "        self.pulls[arm] += 1\n",
    "        self.successes[arm] += reward\n",
    "        \n",
    "        # Update Beta parameters\n",
    "        self.alpha[arm] += reward\n",
    "        self.beta[arm] += (1 - reward)\n",
    "        \n",
    "        # Track performance\n",
    "        self.cumulative_reward += reward\n",
    "        best_rate = max(self.true_rates)\n",
    "        self.cumulative_regret += (best_rate - self.true_rates[arm])\n",
    "        \n",
    "        self.history.append({\n",
    "            'arm': arm,\n",
    "            'reward': reward,\n",
    "            'cumulative_regret': self.cumulative_regret\n",
    "        })\n",
    "    \n",
    "    def run(self, n_rounds):\n",
    "        \"\"\"Run bandit for n_rounds.\"\"\"\n",
    "        for _ in range(n_rounds):\n",
    "            arm = self.select_arm()\n",
    "            reward = np.random.binomial(1, self.true_rates[arm])  # Simulate device test\n",
    "            self.update(arm, reward)\n",
    "\n",
    "# Run simulation\n",
    "np.random.seed(789)\n",
    "true_rates = [0.88, 0.91, 0.93]  # Conservative, Moderate, Aggressive\n",
    "n_rounds = 1000\n",
    "\n",
    "bandit = ThompsonSamplingBandit(n_arms=3, true_rates=true_rates)\n",
    "bandit.run(n_rounds)\n",
    "\n",
    "# Results\n",
    "print(\"Multi-Armed Bandit Results (Thompson Sampling):\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(3):\n",
    "    empirical_rate = bandit.successes[i] / bandit.pulls[i] if bandit.pulls[i] > 0 else 0\n",
    "    print(f\"Arm {i} (True Rate: {true_rates[i]:.2f}):\")\n",
    "    print(f\"  Pulls: {int(bandit.pulls[i])} ({bandit.pulls[i]/n_rounds*100:.1f}%)\")\n",
    "    print(f\"  Successes: {int(bandit.successes[i])}\")\n",
    "    print(f\"  Empirical Rate: {empirical_rate:.4f}\")\n",
    "\n",
    "best_arm = np.argmax(bandit.successes / (bandit.pulls + 1e-10))\n",
    "print(f\"\\n‚úÖ Best Arm: {best_arm} (Rate: {true_rates[best_arm]:.2f})\")\n",
    "print(f\"Cumulative Reward: {bandit.cumulative_reward:.0f} / {n_rounds} = {bandit.cumulative_reward/n_rounds:.4f}\")\n",
    "print(f\"Cumulative Regret: {bandit.cumulative_regret:.2f}\")\n",
    "\n",
    "# Compare to fixed A/B/C test (equal traffic split)\n",
    "fixed_reward = n_rounds * np.mean(true_rates)  # If we split traffic evenly\n",
    "optimal_reward = n_rounds * max(true_rates)  # If we knew best arm upfront\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Fixed A/B/C Test (equal split): {fixed_reward:.0f} successes\")\n",
    "print(f\"  Thompson Sampling: {bandit.cumulative_reward:.0f} successes\")\n",
    "print(f\"  Optimal (oracle): {optimal_reward:.0f} successes\")\n",
    "print(f\"  Bandit Gain vs Fixed: {bandit.cumulative_reward - fixed_reward:.0f} extra passes\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Arm selection over time\n",
    "history_df = pd.DataFrame(bandit.history)\n",
    "arm_counts = history_df.groupby([history_df.index // 50, 'arm']).size().unstack(fill_value=0)\n",
    "arm_counts.plot(kind='bar', stacked=True, ax=axes[0, 0], color=['blue', 'orange', 'green'])\n",
    "axes[0, 0].set_title('Arm Selection Over Time (50-round bins)')\n",
    "axes[0, 0].set_xlabel('Time Period')\n",
    "axes[0, 0].set_ylabel('Pulls')\n",
    "axes[0, 0].legend(['Arm 0 (88%)', 'Arm 1 (91%)', 'Arm 2 (93%)'])\n",
    "\n",
    "# 2. Cumulative regret\n",
    "axes[0, 1].plot(history_df['cumulative_regret'], color='red', linewidth=2)\n",
    "axes[0, 1].set_title('Cumulative Regret Over Time')\n",
    "axes[0, 1].set_xlabel('Round')\n",
    "axes[0, 1].set_ylabel('Cumulative Regret')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Final arm distribution\n",
    "pull_percentages = bandit.pulls / n_rounds * 100\n",
    "axes[1, 0].bar(['Arm 0\\n(88%)', 'Arm 1\\n(91%)', 'Arm 2\\n(93%)'], pull_percentages, \n",
    "               color=['blue', 'orange', 'green'], edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_ylabel('Percentage of Pulls')\n",
    "axes[1, 0].set_title('Final Traffic Allocation')\n",
    "axes[1, 0].axhline(y=33.33, color='red', linestyle='--', label='Equal Split (33.3%)')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Posterior distributions\n",
    "x_post = np.linspace(0.8, 1.0, 1000)\n",
    "for i in range(3):\n",
    "    posterior = beta.pdf(x_post, bandit.alpha[i], bandit.beta[i])\n",
    "    axes[1, 1].plot(x_post, posterior, label=f'Arm {i}', linewidth=2)\n",
    "    axes[1, 1].axvline(true_rates[i], color=f'C{i}', linestyle='--', alpha=0.5)\n",
    "\n",
    "axes[1, 1].set_xlabel('Pass Rate')\n",
    "axes[1, 1].set_ylabel('Posterior Density')\n",
    "axes[1, 1].set_title('Learned Posterior Distributions')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Interpretation: Bandit automatically converged to best arm (2) with ~{pull_percentages[2]:.0f}% traffic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea2d3e",
   "metadata": {},
   "source": [
    "## üöÄ Real-World Project Templates\n",
    "\n",
    "Build production experimentation systems using these frameworks:\n",
    "\n",
    "### 1Ô∏è‚É£ **Post-Silicon Test Program Optimizer**\n",
    "- **Objective**: A/B test different test insertion orders to minimize total test time  \n",
    "- **Data**: 10K+ devices, test times per block, defect coverage metrics  \n",
    "- **Success Metric**: Reduce test time by 10%+ while maintaining 99%+ defect coverage  \n",
    "- **Features**: Multi-objective optimization (time vs coverage), sequential testing, factorial design  \n",
    "- **Tech Stack**: Python, statsmodels, DOE (Design of Experiments), Monte Carlo simulation\n",
    "\n",
    "### 2Ô∏è‚É£ **Website Conversion Optimization Platform**\n",
    "- **Objective**: Test landing page variants to maximize sign-up conversion rate  \n",
    "- **Data**: 100K+ visitors/month, click-through, bounce rate, conversion events  \n",
    "- **Success Metric**: Increase conversion from 2.5% ‚Üí 3.0% (20% relative lift)  \n",
    "- **Features**: Bayesian sequential testing, multi-variant testing, segmentation analysis  \n",
    "- **Tech Stack**: Google Optimize, Python, BigQuery, Looker dashboards\n",
    "\n",
    "### 3Ô∏è‚É£ **Email Campaign A/B Testing Engine**\n",
    "- **Objective**: Test subject lines, send times, content to maximize open/click rates  \n",
    "- **Data**: 500K subscribers, open rates, click rates, unsubscribe rates  \n",
    "- **Success Metric**: Improve click-through rate from 3.2% ‚Üí 4.0%  \n",
    "- **Features**: Multi-armed bandit for subject lines, time-based segmentation, fatigue analysis  \n",
    "- **Tech Stack**: Mailchimp API, Python, Thompson Sampling, Redshift\n",
    "\n",
    "### 4Ô∏è‚É£ **Manufacturing Process Optimization**\n",
    "- **Objective**: Test burn-in durations to balance infant mortality vs throughput  \n",
    "- **Data**: 50K devices, field failure rates (0-90 days), burn-in costs  \n",
    "- **Success Metric**: Reduce infant mortality by 30% with < 5% throughput loss  \n",
    "- **Features**: Survival analysis, cost-benefit modeling, sequential experimentation  \n",
    "- **Tech Stack**: JMP/Minitab for DOE, Python (lifelines), Tableau\n",
    "\n",
    "### 5Ô∏è‚É£ **Recommendation Algorithm A/B Test**\n",
    "- **Objective**: Test collaborative filtering vs content-based recommendations  \n",
    "- **Data**: 1M+ users, click-through, watch time, purchase conversion  \n",
    "- **Success Metric**: Increase engagement time by 15% (avg session: 12 min ‚Üí 14 min)  \n",
    "- **Features**: Stratified randomization (by user tenure), long-term holdout, network effects correction  \n",
    "- **Tech Stack**: Spark, MLflow, custom experimentation framework, Kafka\n",
    "\n",
    "### 6Ô∏è‚É£ **Pricing Experimentation Platform**\n",
    "- **Objective**: Test pricing tiers to maximize revenue per user  \n",
    "- **Data**: 200K customers, price elasticity, churn rates, LTV  \n",
    "- **Success Metric**: Increase ARPU (average revenue per user) by $5/month  \n",
    "- **Features**: Conjoint analysis, demand curve estimation, competitive positioning  \n",
    "- **Tech Stack**: Optimizely, Python (econometrics), Stripe integration, Mixpanel\n",
    "\n",
    "### 7Ô∏è‚É£ **Mobile App Onboarding Flow Test**\n",
    "- **Objective**: Optimize tutorial flow to maximize Day-7 retention  \n",
    "- **Data**: 50K new users/week, onboarding completion, D1/D7/D30 retention  \n",
    "- **Success Metric**: Improve D7 retention from 40% ‚Üí 48%  \n",
    "- **Features**: Funnel analysis, sequential A/B testing, cohort comparison  \n",
    "- **Tech Stack**: Firebase A/B Testing, Amplitude, Python (survival analysis)\n",
    "\n",
    "### 8Ô∏è‚É£ **Search Ranking Algorithm Experiment**\n",
    "- **Objective**: Test BM25 vs neural ranking model for search relevance  \n",
    "- **Data**: 10M searches/month, click position, dwell time, conversion  \n",
    "- **Success Metric**: Reduce \"pogo-sticking\" (return to search) by 20%  \n",
    "- **Features**: Interleaving experiments, pairwise comparison, novelty/diversity metrics  \n",
    "- **Tech Stack**: Elasticsearch, custom interleaving framework, ClickHouse, Grafana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094aa0b9",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### What is Experimental Design?\n",
    "Systematic planning of controlled experiments to establish causal relationships between interventions (treatments) and outcomes while minimizing bias and confounding variables.\n",
    "\n",
    "### Why A/B Testing?\n",
    "- **Causal Evidence**: Randomization eliminates confounding ‚Üí proves causation\n",
    "- **Risk Management**: Test changes on small samples before full rollout\n",
    "- **Data-Driven Decisions**: Quantify impact with statistical confidence\n",
    "- **Continuous Improvement**: Iterate quickly with measured improvements\n",
    "\n",
    "### Core Statistical Concepts\n",
    "\n",
    "| **Concept** | **Definition** | **Typical Value** |\n",
    "|------------|---------------|------------------|\n",
    "| **Significance Level (Œ±)** | Probability of Type I error (false positive) | 0.05 (5%) |\n",
    "| **Statistical Power (1-Œ≤)** | Probability of detecting true effect | 0.80 (80%) |\n",
    "| **Effect Size** | Standardized magnitude of difference | Small: 0.2, Medium: 0.5, Large: 0.8 |\n",
    "| **P-value** | Probability of observing data if H‚ÇÄ is true | < 0.05 for significance |\n",
    "| **Confidence Interval** | Range likely containing true parameter | 95% CI common |\n",
    "\n",
    "### Sample Size Formulas\n",
    "\n",
    "**T-test (continuous metrics):**\n",
    "$$n = \\frac{2(Z_{\\alpha/2} + Z_{\\beta})^2 \\sigma^2}{\\delta^2}$$\n",
    "- $Z_{\\alpha/2}$ = 1.96 for Œ± = 0.05\n",
    "- $Z_{\\beta}$ = 0.84 for power = 0.8\n",
    "- $\\sigma$ = standard deviation\n",
    "- $\\delta$ = minimum detectable effect\n",
    "\n",
    "**Proportion test (conversion rates):**\n",
    "$$n = \\frac{(Z_{\\alpha/2} + Z_{\\beta})^2 [p_1(1-p_1) + p_2(1-p_2)]}{(p_1 - p_2)^2}$$\n",
    "- $p_1, p_2$ = baseline and treatment proportions\n",
    "\n",
    "### Test Selection Guide\n",
    "\n",
    "| **Metric Type** | **Test** | **When to Use** | **Example** |\n",
    "|----------------|---------|----------------|------------|\n",
    "| **Continuous (mean)** | T-test | Normal distribution, equal variance | Test time, revenue, temperature |\n",
    "| **Continuous (median)** | Mann-Whitney U | Non-normal, outliers | Skewed distributions |\n",
    "| **Proportion** | Chi-square / Z-test | Binary outcome | Pass/fail, click/no-click |\n",
    "| **Count data** | Poisson test | Rare events | Defects per wafer |\n",
    "| **Survival** | Log-rank test | Time-to-event | Device lifetime, churn |\n",
    "\n",
    "### Bayesian vs Frequentist A/B Testing\n",
    "\n",
    "**Frequentist (Classical):**\n",
    "- ‚úÖ Well-understood, industry standard\n",
    "- ‚úÖ Fixed sample size, clear stopping rule\n",
    "- ‚ùå P-values are confusing (\"probability of data given H‚ÇÄ\")\n",
    "- ‚ùå Can't peek at results (inflates Type I error)\n",
    "\n",
    "**Bayesian:**\n",
    "- ‚úÖ Intuitive probabilities (\"95% chance B is better\")\n",
    "- ‚úÖ Can update continuously, early stopping allowed\n",
    "- ‚úÖ Incorporates prior knowledge\n",
    "- ‚ùå Requires prior specification (can introduce bias)\n",
    "- ‚ùå Computationally intensive for complex models\n",
    "\n",
    "### Multi-Armed Bandits vs Fixed A/B\n",
    "\n",
    "**Fixed A/B Test:**\n",
    "- Allocate 50/50 traffic until reaching sample size\n",
    "- Regret: Wastes 50% traffic on inferior variant\n",
    "- Clear statistical guarantees\n",
    "\n",
    "**Multi-Armed Bandit (Thompson Sampling):**\n",
    "- Dynamically allocate more traffic to better variants\n",
    "- Regret: Grows logarithmically (much better)\n",
    "- No fixed stopping time (continuous optimization)\n",
    "- Trade-off: Slower to converge on true best with certainty\n",
    "\n",
    "**When to Use Bandits:**\n",
    "- High traffic (can learn quickly)\n",
    "- Short-term optimizations (email subject lines)\n",
    "- Cost of using suboptimal variant is high\n",
    "\n",
    "**When to Use Fixed A/B:**\n",
    "- Low traffic (need clean statistical test)\n",
    "- Long-term strategic decisions (product redesign)\n",
    "- Regulatory/compliance requirements (clear p-value needed)\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "- ‚ùå **Peeking at Results**: Checking p-values repeatedly inflates false positive rate ‚Üí Use sequential testing or Bayesian methods\n",
    "- ‚ùå **Underpowered Tests**: Small samples miss real effects ‚Üí Always do power analysis first\n",
    "- ‚ùå **Multiple Testing**: Running 20 tests ‚Üí expect 1 false positive at Œ±=0.05 ‚Üí Use Bonferroni correction\n",
    "- ‚ùå **Ignoring Novelty Effect**: New variants get temporary boost ‚Üí Run for full business cycle\n",
    "- ‚ùå **Selection Bias**: Non-random assignment ‚Üí Use proper randomization\n",
    "- ‚ùå **Network Effects**: User interactions affect each other ‚Üí Use cluster randomization\n",
    "\n",
    "### Post-Silicon Experimentation Best Practices\n",
    "\n",
    "**Device-Level Randomization:**\n",
    "- Randomize at wafer/lot level to avoid tester bias\n",
    "- Control for spatial effects (edge vs center dies)\n",
    "- Match on process node, foundry, vintage\n",
    "\n",
    "**Multi-Objective Optimization:**\n",
    "- Balance test time, defect coverage, yield\n",
    "- Use Pareto frontier for trade-off analysis\n",
    "- Weight objectives by business value\n",
    "\n",
    "**Sequential Testing:**\n",
    "- Test program changes incrementally (insert ‚Üí reorder ‚Üí remove)\n",
    "- Use Bonferroni correction for multiple comparisons\n",
    "- Document assumptions for regulatory audit\n",
    "\n",
    "### Production Implementation Checklist\n",
    "\n",
    "- ‚úÖ **Pre-Experiment:**\n",
    "  - Define primary metric and guardrail metrics\n",
    "  - Calculate sample size (power analysis)\n",
    "  - Set up randomization infrastructure\n",
    "  - Plan for outlier/anomaly handling\n",
    "\n",
    "- ‚úÖ **During Experiment:**\n",
    "  - Monitor sample ratio mismatch (50/50 assignment working?)\n",
    "  - Check guardrail metrics (no unexpected harm)\n",
    "  - Log all assignment decisions for reproducibility\n",
    "\n",
    "- ‚úÖ **Post-Experiment:**\n",
    "  - Calculate confidence intervals, not just p-values\n",
    "  - Segment analysis (does effect vary by user type?)\n",
    "  - Long-term holdout to measure sustained impact\n",
    "\n",
    "### Tool Ecosystem\n",
    "\n",
    "**Experimentation Platforms:**\n",
    "- **Optimizely, VWO**: Commercial A/B testing (web/mobile)\n",
    "- **Google Optimize**: Free for small-scale tests\n",
    "- **Statsig, Eppo**: Modern, Bayesian-focused platforms\n",
    "- **Custom**: Python + feature flags (LaunchDarkly) + analytics DB\n",
    "\n",
    "**Statistical Analysis:**\n",
    "- **statsmodels**: Power analysis, hypothesis tests\n",
    "- **scipy.stats**: T-tests, chi-square, distributions\n",
    "- **PyMC/Stan**: Bayesian inference\n",
    "- **JMP/Minitab**: Industrial DOE (design of experiments)\n",
    "\n",
    "### Next Steps\n",
    "- **Notebook 111**: Causal Inference (propensity scores, DiD, instrumental variables)\n",
    "- **Notebook 112**: Bayesian Statistics (PyMC, hierarchical models)\n",
    "- **Advanced**: Multi-armed bandits with contextual information, adaptive experimental design\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: *In God we trust, all others bring data... and proper experimental design!* üìä"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
