{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a014bd17",
   "metadata": {},
   "source": [
    "# 111: Causal Inference\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** the fundamental difference between correlation and causation\n",
    "- **Implement** propensity score matching to eliminate confounding bias\n",
    "- **Apply** difference-in-differences (DiD) for quasi-experimental analysis\n",
    "- **Use** instrumental variables for endogeneity problems\n",
    "- **Build** regression discontinuity designs for treatment effect estimation\n",
    "- **Design** causal inference frameworks for post-silicon optimization and business decisions\n",
    "\n",
    "## üìö What is Causal Inference?\n",
    "\n",
    "**Causal inference** is the process of determining whether a relationship between variables is causal (X causes Y) rather than merely correlational. It answers \"what if\" questions: What would happen if we changed X? Would Y change as a result?\n",
    "\n",
    "Unlike prediction (forecasting Y from X), causal inference focuses on **intervention effects**: What happens when we actively manipulate X? This is critical for decision-making because correlation does not imply causation.\n",
    "\n",
    "**Why Causal Inference?**\n",
    "- ‚úÖ **Actionable Insights**: Identify which actions actually drive outcomes\n",
    "- ‚úÖ **Policy Evaluation**: Measure true impact of interventions (not coincidental changes)\n",
    "- ‚úÖ **Resource Allocation**: Invest in changes that cause improvements, not just correlate with them\n",
    "- ‚úÖ **Avoid Spurious Relationships**: Don't act on misleading correlations\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Test Flow Optimization Impact**\n",
    "- Question: Does reordering test blocks *cause* faster test times, or is it just newer devices?\n",
    "- Output: Causal effect = -0.3s test time (not due to tester upgrades or process drift)\n",
    "- Value: Confident investment in test flow changes knowing they actually work\n",
    "\n",
    "**Burn-In Effectiveness**\n",
    "- Question: Does burn-in *cause* lower field failures, or do we just ship better devices?\n",
    "- Output: Instrumental variable analysis shows 15% failure reduction attributable to burn-in\n",
    "- Value: Justify burn-in costs with causal evidence (not just correlation)\n",
    "\n",
    "**Process Node Migration**\n",
    "- Question: Did moving to 7nm *cause* yield improvements, or was it better equipment?\n",
    "- Output: Difference-in-differences isolates 8% yield gain due to node shrink alone\n",
    "- Value: Inform future node migration ROI calculations\n",
    "\n",
    "**Parametric Test Limit Changes**\n",
    "- Question: Do tighter Vdd limits *cause* better reliability, or healthier devices coincidentally pass?\n",
    "- Output: Regression discontinuity shows 2% reliability improvement at threshold\n",
    "- Value: Set limits based on causal impact, not spurious correlation\n",
    "\n",
    "## üîÑ Causal Inference Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Define Causal Question] --> B[Identify Confounders]\n",
    "    B --> C{Randomized<br/>Experiment<br/>Possible?}\n",
    "    C -->|Yes| D[A/B Test]\n",
    "    C -->|No| E[Select Method]\n",
    "    E --> F[Propensity Matching]\n",
    "    E --> G[DiD]\n",
    "    E --> H[IV]\n",
    "    E --> I[RDD]\n",
    "    F --> J[Estimate Effect]\n",
    "    G --> J\n",
    "    H --> J\n",
    "    I --> J\n",
    "    D --> J\n",
    "    J --> K[Validate Assumptions]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style J fill:#e1ffe1\n",
    "    style K fill:#fffacd\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- 010: Linear Regression (regression fundamentals)\n",
    "- 110: Experimental Design (randomized controlled trials)\n",
    "\n",
    "**Next Steps:**\n",
    "- 112: Bayesian Statistics (Bayesian causal inference)\n",
    "- 113: Survival Analysis (time-to-event causality)\n",
    "\n",
    "---\n",
    "\n",
    "Let's uncover true causality! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493e1b50",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b95bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully!\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ceb5e",
   "metadata": {},
   "source": [
    "## 2. Correlation vs Causation\n",
    "\n",
    "**Purpose:** Demonstrate why correlation alone cannot prove causation and introduce confounding variables.\n",
    "\n",
    "**Key Points:**\n",
    "- **Correlation**: Two variables move together (X ‚Üë when Y ‚Üë)\n",
    "- **Causation**: X directly causes Y (intervention on X changes Y)\n",
    "- **Confounding**: Third variable Z causes both X and Y ‚Üí spurious correlation\n",
    "- **Classic Example**: Ice cream sales correlate with drownings (confounder: summer temperature)\n",
    "\n",
    "**Why This Matters:** Acting on correlations can waste resources or cause harm. Post-silicon example: Frequency correlates with yield, but process quality causes both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f5a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate spurious correlation in post-silicon data\n",
    "# Confounder: Process quality (hidden variable)\n",
    "# X: Average frequency (MHz)\n",
    "# Y: Yield (%)\n",
    "# Z: Process quality score (confounder)\n",
    "\n",
    "np.random.seed(100)\n",
    "n_wafers = 500\n",
    "\n",
    "# Confounder: Process quality (0-100 scale)\n",
    "process_quality = np.random.normal(75, 15, n_wafers)\n",
    "process_quality = np.clip(process_quality, 40, 100)\n",
    "\n",
    "# X: Frequency (caused by process quality + noise)\n",
    "# Better process ‚Üí higher frequency\n",
    "frequency = 2800 + 4 * process_quality + np.random.normal(0, 50, n_wafers)\n",
    "\n",
    "# Y: Yield (caused by process quality + noise, NOT by frequency)\n",
    "# Better process ‚Üí higher yield\n",
    "yield_pct = 60 + 0.35 * process_quality + np.random.normal(0, 5, n_wafers)\n",
    "yield_pct = np.clip(yield_pct, 50, 100)\n",
    "\n",
    "# Create dataframe\n",
    "confounding_df = pd.DataFrame({\n",
    "    'frequency_mhz': frequency,\n",
    "    'yield_pct': yield_pct,\n",
    "    'process_quality': process_quality  # Hidden in real data!\n",
    "})\n",
    "\n",
    "# Calculate correlations\n",
    "corr_freq_yield = confounding_df['frequency_mhz'].corr(confounding_df['yield_pct'])\n",
    "corr_proc_freq = confounding_df['process_quality'].corr(confounding_df['frequency_mhz'])\n",
    "corr_proc_yield = confounding_df['process_quality'].corr(confounding_df['yield_pct'])\n",
    "\n",
    "print(\"Correlation Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Frequency ‚Üî Yield:  {corr_freq_yield:.3f} (SPURIOUS!)\")\n",
    "print(f\"Process ‚Üî Frequency: {corr_proc_freq:.3f} (true cause)\")\n",
    "print(f\"Process ‚Üî Yield:     {corr_proc_yield:.3f} (true cause)\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è WARNING: Frequency and yield are correlated ({corr_freq_yield:.3f})\")\n",
    "print(f\"   But frequency does NOT cause yield!\")\n",
    "print(f\"   Both are caused by process quality (confounding).\")\n",
    "print(f\"\\nüí° Lesson: Don't waste money trying to improve frequency to boost yield!\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. Spurious correlation (Frequency vs Yield)\n",
    "axes[0].scatter(confounding_df['frequency_mhz'], confounding_df['yield_pct'], \n",
    "                alpha=0.5, c=confounding_df['process_quality'], cmap='viridis')\n",
    "axes[0].set_xlabel('Frequency (MHz)')\n",
    "axes[0].set_ylabel('Yield (%)')\n",
    "axes[0].set_title(f'Spurious Correlation\\nr = {corr_freq_yield:.3f}')\n",
    "z = np.polyfit(confounding_df['frequency_mhz'], confounding_df['yield_pct'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0].plot(confounding_df['frequency_mhz'].sort_values(), \n",
    "             p(confounding_df['frequency_mhz'].sort_values()), \n",
    "             \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "# 2. True causation (Process ‚Üí Frequency)\n",
    "axes[1].scatter(confounding_df['process_quality'], confounding_df['frequency_mhz'], \n",
    "                alpha=0.5, color='blue')\n",
    "axes[1].set_xlabel('Process Quality')\n",
    "axes[1].set_ylabel('Frequency (MHz)')\n",
    "axes[1].set_title(f'True Causation\\nr = {corr_proc_freq:.3f}')\n",
    "z = np.polyfit(confounding_df['process_quality'], confounding_df['frequency_mhz'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[1].plot(confounding_df['process_quality'].sort_values(), \n",
    "             p(confounding_df['process_quality'].sort_values()), \n",
    "             \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "# 3. True causation (Process ‚Üí Yield)\n",
    "axes[2].scatter(confounding_df['process_quality'], confounding_df['yield_pct'], \n",
    "                alpha=0.5, color='green')\n",
    "axes[2].set_xlabel('Process Quality')\n",
    "axes[2].set_ylabel('Yield (%)')\n",
    "axes[2].set_title(f'True Causation\\nr = {corr_proc_yield:.3f}')\n",
    "z = np.polyfit(confounding_df['process_quality'], confounding_df['yield_pct'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[2].plot(confounding_df['process_quality'].sort_values(), \n",
    "             p(confounding_df['process_quality'].sort_values()), \n",
    "             \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "plt.colorbar(axes[0].collections[0], ax=axes[0], label='Process Quality')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc05e4a2",
   "metadata": {},
   "source": [
    "## 3. Propensity Score Matching (PSM)\n",
    "\n",
    "**Purpose:** Create comparable treatment and control groups from observational data by matching on confounders.\n",
    "\n",
    "**Key Points:**\n",
    "- **Propensity Score**: Probability of receiving treatment given observed covariates\n",
    "- **Matching**: Pair treated units with similar control units (same propensity score)\n",
    "- **Balance**: After matching, treatment/control groups should be similar on all confounders\n",
    "- **Assumption**: All confounders are observed and measured (no hidden variables)\n",
    "\n",
    "**Why This Matters:** When randomized experiments are impossible (ethics, cost), PSM mimics randomization by balancing confounders. Post-silicon use: compare devices that received burn-in vs not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b1402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate observational data: Burn-in effect on field failures\n",
    "# Confounders: Initial Vdd, Idd (devices with worse parameters more likely to get burn-in)\n",
    "\n",
    "np.random.seed(200)\n",
    "n_devices = 1000\n",
    "\n",
    "# Confounders\n",
    "vdd = np.random.normal(1.2, 0.1, n_devices)\n",
    "idd = np.random.normal(150, 25, n_devices)\n",
    "\n",
    "# Treatment assignment (non-random!): More likely if Vdd high or Idd high\n",
    "# Logistic model for propensity\n",
    "logit = -5 + 3 * (vdd - 1.2) / 0.1 + 0.02 * (idd - 150)\n",
    "propensity_true = 1 / (1 + np.exp(-logit))\n",
    "burn_in = (np.random.random(n_devices) < propensity_true).astype(int)\n",
    "\n",
    "# Outcome: Field failure (1 = fail, 0 = pass)\n",
    "# True causal effect of burn-in = -0.10 (10% absolute failure reduction)\n",
    "# Baseline failure depends on Vdd, Idd\n",
    "failure_prob = 0.15 + 0.2 * (vdd - 1.2) / 0.1 + 0.001 * (idd - 150) - 0.10 * burn_in\n",
    "failure_prob = np.clip(failure_prob, 0, 1)\n",
    "field_failure = (np.random.random(n_devices) < failure_prob).astype(int)\n",
    "\n",
    "# Create dataframe\n",
    "psm_df = pd.DataFrame({\n",
    "    'vdd': vdd,\n",
    "    'idd': idd,\n",
    "    'burn_in': burn_in,\n",
    "    'field_failure': field_failure\n",
    "})\n",
    "\n",
    "# Naive comparison (biased!)\n",
    "naive_effect = psm_df[psm_df['burn_in'] == 0]['field_failure'].mean() - \\\n",
    "               psm_df[psm_df['burn_in'] == 1]['field_failure'].mean()\n",
    "\n",
    "print(\"Naive Analysis (WITHOUT propensity matching):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Failure rate (No burn-in): {psm_df[psm_df['burn_in'] == 0]['field_failure'].mean():.3f}\")\n",
    "print(f\"Failure rate (Burn-in):    {psm_df[psm_df['burn_in'] == 1]['field_failure'].mean():.3f}\")\n",
    "print(f\"Naive Effect: {naive_effect:.3f}\")\n",
    "print(f\"\\n‚ö†Ô∏è BIASED! Devices with burn-in had worse baseline quality.\")\n",
    "\n",
    "# Step 1: Estimate propensity scores\n",
    "X_ps = psm_df[['vdd', 'idd']].values\n",
    "y_ps = psm_df['burn_in'].values\n",
    "\n",
    "ps_model = LogisticRegression()\n",
    "ps_model.fit(X_ps, y_ps)\n",
    "psm_df['propensity_score'] = ps_model.predict_proba(X_ps)[:, 1]\n",
    "\n",
    "# Step 2: Match treated to control using nearest neighbors (caliper = 0.05)\n",
    "treated = psm_df[psm_df['burn_in'] == 1].copy()\n",
    "control = psm_df[psm_df['burn_in'] == 0].copy()\n",
    "\n",
    "# For each treated unit, find closest control unit\n",
    "nn = NearestNeighbors(n_neighbors=1)\n",
    "nn.fit(control[['propensity_score']].values)\n",
    "distances, indices = nn.kneighbors(treated[['propensity_score']].values)\n",
    "\n",
    "# Keep matches within caliper (0.05)\n",
    "caliper = 0.05\n",
    "valid_matches = distances.flatten() < caliper\n",
    "\n",
    "matched_treated = treated[valid_matches].copy()\n",
    "matched_control = control.iloc[indices[valid_matches].flatten()].copy()\n",
    "\n",
    "print(f\"\\nPropensity Score Matching:\")\n",
    "print(f\"  Total treated: {len(treated)}\")\n",
    "print(f\"  Total control: {len(control)}\")\n",
    "print(f\"  Matched pairs: {len(matched_treated)}\")\n",
    "print(f\"  Discarded (no good match): {len(treated) - len(matched_treated)}\")\n",
    "\n",
    "# Step 3: Estimate causal effect on matched sample\n",
    "psm_effect = matched_control['field_failure'].mean() - matched_treated['field_failure'].mean()\n",
    "\n",
    "print(f\"\\nMatched Sample Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Failure rate (No burn-in, matched): {matched_control['field_failure'].mean():.3f}\")\n",
    "print(f\"Failure rate (Burn-in, matched):    {matched_treated['field_failure'].mean():.3f}\")\n",
    "print(f\"PSM Causal Effect: {psm_effect:.3f}\")\n",
    "print(f\"True Effect: 0.100 (10% failure reduction)\")\n",
    "print(f\"\\n‚úÖ PSM recovers approximate true causal effect!\")\n",
    "\n",
    "# Covariate balance check\n",
    "print(f\"\\nCovariate Balance (before matching):\")\n",
    "print(f\"  Vdd  - Treated: {treated['vdd'].mean():.4f}, Control: {control['vdd'].mean():.4f}\")\n",
    "print(f\"  Idd  - Treated: {treated['idd'].mean():.2f}, Control: {control['idd'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nCovariate Balance (after matching):\")\n",
    "print(f\"  Vdd  - Treated: {matched_treated['vdd'].mean():.4f}, Control: {matched_control['vdd'].mean():.4f}\")\n",
    "print(f\"  Idd  - Treated: {matched_treated['idd'].mean():.2f}, Control: {matched_control['idd'].mean():.2f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Propensity score distributions\n",
    "axes[0, 0].hist(treated['propensity_score'], bins=30, alpha=0.6, label='Treated', color='red')\n",
    "axes[0, 0].hist(control['propensity_score'], bins=30, alpha=0.6, label='Control', color='blue')\n",
    "axes[0, 0].set_xlabel('Propensity Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Propensity Score Distribution (Before Matching)')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Matched propensity scores\n",
    "axes[0, 1].hist(matched_treated['propensity_score'], bins=20, alpha=0.6, label='Treated', color='red')\n",
    "axes[0, 1].hist(matched_control['propensity_score'], bins=20, alpha=0.6, label='Control', color='blue')\n",
    "axes[0, 1].set_xlabel('Propensity Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Propensity Score Distribution (After Matching)')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Covariate balance: Vdd\n",
    "axes[1, 0].boxplot([treated['vdd'], control['vdd'], matched_treated['vdd'], matched_control['vdd']], \n",
    "                    labels=['Treated\\n(Before)', 'Control\\n(Before)', 'Treated\\n(After)', 'Control\\n(After)'])\n",
    "axes[1, 0].set_ylabel('Vdd (V)')\n",
    "axes[1, 0].set_title('Vdd Balance Before/After Matching')\n",
    "\n",
    "# 4. Causal effect comparison\n",
    "effects = ['Naive\\n(Biased)', 'PSM\\n(Unbiased)', 'True\\nEffect']\n",
    "values = [naive_effect, psm_effect, 0.10]\n",
    "colors = ['red', 'green', 'blue']\n",
    "axes[1, 1].bar(effects, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_ylabel('Failure Rate Reduction')\n",
    "axes[1, 1].set_title('Estimated Burn-In Effect')\n",
    "axes[1, 1].axhline(y=0.10, color='blue', linestyle='--', linewidth=2, label='True Effect')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009477f",
   "metadata": {},
   "source": [
    "## 4. Difference-in-Differences (DiD)\n",
    "\n",
    "**Purpose:** Estimate causal effects by comparing changes over time between treatment and control groups.\n",
    "\n",
    "**Key Points:**\n",
    "- **Parallel Trends Assumption**: Without treatment, both groups would follow same trend\n",
    "- **Double Differencing**: (After - Before) for treatment group MINUS (After - Before) for control\n",
    "- **Controls for Time-Invariant Confounders**: Group differences that don't change over time\n",
    "- **Use Case**: Policy evaluation, natural experiments, quasi-experimental studies\n",
    "\n",
    "**Why This Matters:** When randomization impossible and treatment assigned over time, DiD isolates causal effect. Post-silicon: test flow changes rolled out to some testers first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3672c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate DiD: Test flow optimization rolled out to Tester A (not B)\n",
    "# Tester A: Treatment group (gets new flow in Week 10)\n",
    "# Tester B: Control group (stays with old flow)\n",
    "\n",
    "np.random.seed(300)\n",
    "weeks = 20\n",
    "devices_per_week = 100\n",
    "\n",
    "# Baseline test times (Tester A inherently faster)\n",
    "baseline_A = 4.8  # seconds\n",
    "baseline_B = 5.2  # seconds\n",
    "\n",
    "# Common time trend (equipment aging ‚Üí slower over time)\n",
    "time_trend = np.arange(weeks) * 0.02  # +0.02s per week\n",
    "\n",
    "# Treatment effect (new flow reduces time by 0.5s starting week 10)\n",
    "treatment_week = 10\n",
    "treatment_effect = -0.5\n",
    "\n",
    "did_data = []\n",
    "for week in range(weeks):\n",
    "    # Tester A\n",
    "    is_treated = (week >= treatment_week)\n",
    "    test_time_A = baseline_A + time_trend[week] + \\\n",
    "                  (treatment_effect if is_treated else 0) + \\\n",
    "                  np.random.normal(0, 0.3, devices_per_week)\n",
    "    \n",
    "    for time in test_time_A:\n",
    "        did_data.append({'week': week, 'tester': 'A', 'test_time': time, 'treated': is_treated})\n",
    "    \n",
    "    # Tester B (control)\n",
    "    test_time_B = baseline_B + time_trend[week] + np.random.normal(0, 0.3, devices_per_week)\n",
    "    \n",
    "    for time in test_time_B:\n",
    "        did_data.append({'week': week, 'tester': 'B', 'test_time': time, 'treated': False})\n",
    "\n",
    "did_df = pd.DataFrame(did_data)\n",
    "\n",
    "# Aggregate to weekly averages\n",
    "weekly_avg = did_df.groupby(['week', 'tester'])['test_time'].mean().reset_index()\n",
    "weekly_avg_A = weekly_avg[weekly_avg['tester'] == 'A']['test_time'].values\n",
    "weekly_avg_B = weekly_avg[weekly_avg['tester'] == 'B']['test_time'].values\n",
    "\n",
    "# Calculate DiD estimate\n",
    "# Before period: weeks 0-9, After period: weeks 10-19\n",
    "before_A = did_df[(did_df['tester'] == 'A') & (did_df['week'] < treatment_week)]['test_time'].mean()\n",
    "after_A = did_df[(did_df['tester'] == 'A') & (did_df['week'] >= treatment_week)]['test_time'].mean()\n",
    "before_B = did_df[(did_df['tester'] == 'B') & (did_df['week'] < treatment_week)]['test_time'].mean()\n",
    "after_B = did_df[(did_df['tester'] == 'B') & (did_df['week'] >= treatment_week)]['test_time'].mean()\n",
    "\n",
    "diff_A = after_A - before_A  # Change in treatment group\n",
    "diff_B = after_B - before_B  # Change in control group\n",
    "did_estimate = diff_A - diff_B  # Difference-in-differences\n",
    "\n",
    "print(\"Difference-in-Differences Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Tester A (Treatment):\")\n",
    "print(f\"  Before (Weeks 0-9):  {before_A:.3f}s\")\n",
    "print(f\"  After (Weeks 10-19): {after_A:.3f}s\")\n",
    "print(f\"  Difference:          {diff_A:.3f}s\")\n",
    "\n",
    "print(f\"\\nTester B (Control):\")\n",
    "print(f\"  Before (Weeks 0-9):  {before_B:.3f}s\")\n",
    "print(f\"  After (Weeks 10-19): {after_B:.3f}s\")\n",
    "print(f\"  Difference:          {diff_B:.3f}s\")\n",
    "\n",
    "print(f\"\\nDiD Causal Estimate: {did_estimate:.3f}s\")\n",
    "print(f\"True Treatment Effect: {treatment_effect:.3f}s\")\n",
    "print(f\"\\n‚úÖ DiD successfully isolates causal effect!\")\n",
    "print(f\"   (Removes baseline difference + common time trend)\")\n",
    "\n",
    "# Regression DiD (more flexible)\n",
    "# Model: test_time = Œ≤0 + Œ≤1*tester_A + Œ≤2*post_treatment + Œ≤3*(tester_A * post_treatment)\n",
    "# Œ≤3 = DiD estimate\n",
    "did_df['tester_A'] = (did_df['tester'] == 'A').astype(int)\n",
    "did_df['post_treatment'] = (did_df['week'] >= treatment_week).astype(int)\n",
    "did_df['interaction'] = did_df['tester_A'] * did_df['post_treatment']\n",
    "\n",
    "X_did = did_df[['tester_A', 'post_treatment', 'interaction']].values\n",
    "y_did = did_df['test_time'].values\n",
    "\n",
    "did_reg = LinearRegression()\n",
    "did_reg.fit(X_did, y_did)\n",
    "\n",
    "beta_tester = did_reg.coef_[0]  # Baseline difference (A vs B)\n",
    "beta_post = did_reg.coef_[1]    # Time trend (before vs after)\n",
    "beta_did = did_reg.coef_[2]     # DiD estimate (interaction term)\n",
    "\n",
    "print(f\"\\nRegression DiD:\")\n",
    "print(f\"  Œ≤‚ÇÄ (Intercept):      {did_reg.intercept_:.3f}s\")\n",
    "print(f\"  Œ≤‚ÇÅ (Tester A):       {beta_tester:.3f}s (baseline difference)\")\n",
    "print(f\"  Œ≤‚ÇÇ (Post-treatment): {beta_post:.3f}s (time trend)\")\n",
    "print(f\"  Œ≤‚ÇÉ (DiD estimate):   {beta_did:.3f}s ‚≠ê\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Time series with treatment\n",
    "axes[0].plot(range(weeks), weekly_avg_A, marker='o', label='Tester A (Treatment)', linewidth=2, color='red')\n",
    "axes[0].plot(range(weeks), weekly_avg_B, marker='s', label='Tester B (Control)', linewidth=2, color='blue')\n",
    "axes[0].axvline(x=treatment_week, color='green', linestyle='--', linewidth=2, label='Treatment Start')\n",
    "axes[0].set_xlabel('Week')\n",
    "axes[0].set_ylabel('Average Test Time (s)')\n",
    "axes[0].set_title('DiD: Test Time Over Time')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# 2. DiD visualization (2x2 table)\n",
    "data_matrix = np.array([[before_B, after_B], [before_A, after_A]])\n",
    "im = axes[1].imshow(data_matrix, cmap='RdYlGn_r', aspect='auto')\n",
    "axes[1].set_xticks([0, 1])\n",
    "axes[1].set_xticklabels(['Before\\n(Weeks 0-9)', 'After\\n(Weeks 10-19)'])\n",
    "axes[1].set_yticks([0, 1])\n",
    "axes[1].set_yticklabels(['Tester B\\n(Control)', 'Tester A\\n(Treatment)'])\n",
    "axes[1].set_title('DiD 2x2 Table')\n",
    "\n",
    "# Annotate with values\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[1].text(j, i, f'{data_matrix[i, j]:.2f}s', ha='center', va='center', \n",
    "                     color='white', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Arrows showing differences\n",
    "axes[1].annotate('', xy=(0.5, 1), xytext=(-0.3, 1), arrowprops=dict(arrowstyle='->', lw=2, color='yellow'))\n",
    "axes[1].text(0.5, 1.3, f'Œî = {diff_A:.2f}s', ha='center', color='yellow', fontweight='bold')\n",
    "\n",
    "axes[1].annotate('', xy=(0.5, 0), xytext=(-0.3, 0), arrowprops=dict(arrowstyle='->', lw=2, color='cyan'))\n",
    "axes[1].text(0.5, -0.3, f'Œî = {diff_B:.2f}s', ha='center', color='cyan', fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=axes[1], label='Test Time (s)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä DiD Formula: ({after_A:.2f} - {before_A:.2f}) - ({after_B:.2f} - {before_B:.2f}) = {did_estimate:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85872900",
   "metadata": {},
   "source": [
    "## üöÄ Real-World Project Templates\n",
    "\n",
    "Build production causal inference systems using these frameworks:\n",
    "\n",
    "### 1Ô∏è‚É£ **Post-Silicon Burn-In ROI Analysis**\n",
    "- **Objective**: Estimate true causal effect of burn-in on field failure rates  \n",
    "- **Data**: 100K devices, burn-in status, Vdd/Idd, field failures (0-12 months)  \n",
    "- **Success Metric**: Quantify failure reduction attributable to burn-in (not selection bias)  \n",
    "- **Method**: Propensity score matching on pre-burn-in parametrics  \n",
    "- **Tech Stack**: Python (sklearn, statsmodels), survival analysis (lifelines), Tableau\n",
    "\n",
    "### 2Ô∏è‚É£ **Marketing Campaign Effectiveness**\n",
    "- **Objective**: Measure causal impact of email campaign on purchases  \n",
    "- **Data**: 500K users, email send (yes/no), demographics, purchase history  \n",
    "- **Success Metric**: Incremental revenue per email sent (causal, not correlation)  \n",
    "- **Method**: PSM on user features + DiD for rollout timing  \n",
    "- **Tech Stack**: Python, BigQuery, Looker, causal inference library (DoWhy)\n",
    "\n",
    "### 3Ô∏è‚É£ **Healthcare Treatment Evaluation**\n",
    "- **Objective**: Estimate causal effect of new drug on patient outcomes  \n",
    "- **Data**: 10K patients, treatment assignment, comorbidities, mortality  \n",
    "- **Success Metric**: 30-day mortality reduction attributable to treatment  \n",
    "- **Method**: Instrumental variable (doctor preference as instrument)  \n",
    "- **Tech Stack**: R (IV packages), Stata, Python (econml)\n",
    "\n",
    "### 4Ô∏è‚É£ **Education Policy Impact**\n",
    "- **Objective**: Measure causal effect of reduced class size on test scores  \n",
    "- **Data**: 200 schools, class sizes, student demographics, standardized test scores  \n",
    "- **Success Metric**: Test score improvement per 5-student class size reduction  \n",
    "- **Method**: Regression discontinuity (policy threshold at 30 students)  \n",
    "- **Tech Stack**: Python (rdrobust package), Stata, visualizations (ggplot)\n",
    "\n",
    "### 5Ô∏è‚É£ **Manufacturing Process Optimization**\n",
    "- **Objective**: Prove that process temperature change *causes* yield improvement  \n",
    "- **Data**: 50K wafers, temperature settings, process generation, yields  \n",
    "- **Success Metric**: Isolate temperature effect from confounding process improvements  \n",
    "- **Method**: DiD (temperature changed at different times for different fabs)  \n",
    "- **Tech Stack**: JMP, Python, Tableau, design of experiments (DOE)\n",
    "\n",
    "### 6Ô∏è‚É£ **Pricing Strategy Causal Analysis**\n",
    "- **Objective**: Measure causal impact of price changes on demand  \n",
    "- **Data**: 1M transactions, prices, product features, seasonality  \n",
    "- **Success Metric**: Price elasticity (% demand change per 1% price change)  \n",
    "- **Method**: IV (cost shocks as instrument for price)  \n",
    "- **Tech Stack**: Python (econml), R (AER package), Spark\n",
    "\n",
    "### 7Ô∏è‚É£ **Product Feature Impact Measurement**\n",
    "- **Objective**: Estimate causal effect of new feature on user retention  \n",
    "- **Data**: 200K users, feature adoption timing, engagement metrics, churn  \n",
    "- **Success Metric**: 7-day retention lift attributable to feature (not user quality)  \n",
    "- **Method**: PSM on pre-adoption behavior + synthetic control  \n",
    "- **Tech Stack**: Python (CausalImpact), Google Analytics, Mixpanel\n",
    "\n",
    "### 8Ô∏è‚É£ **Transportation Policy Evaluation**\n",
    "- **Objective**: Measure causal effect of congestion pricing on traffic  \n",
    "- **Data**: Traffic volumes, pricing zones, weather, events  \n",
    "- **Success Metric**: Traffic reduction attributable to pricing (not other factors)  \n",
    "- **Method**: DiD (pricing introduced in phases across zones)  \n",
    "- **Tech Stack**: R, Python (Uber's CausalImpact), GIS mapping, Tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a036b0",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### What is Causal Inference?\n",
    "Statistical methods to establish cause-and-effect relationships (X causes Y) rather than mere associations (X correlates with Y). Essential for making decisions based on interventions.\n",
    "\n",
    "### Why Causal Inference Matters\n",
    "- **Predictions ‚â† Actions**: Forecasting models predict Y from X, but don't tell you what happens if you *change* X\n",
    "- **Resource Allocation**: Invest in interventions that actually cause improvements, not spurious correlations\n",
    "- **Policy Evaluation**: Measure true impact of changes (new processes, treatments, campaigns)\n",
    "- **Scientific Rigor**: Distinguish causation from coincidence\n",
    "\n",
    "### Core Causal Concepts\n",
    "\n",
    "| **Concept** | **Definition** | **Example** |\n",
    "|------------|---------------|------------|\n",
    "| **Confounding** | Variable Z causes both X and Y ‚Üí spurious correlation | Process quality causes both frequency and yield |\n",
    "| **Treatment Effect** | Causal impact of X on Y: E[Y\\|X=1] - E[Y\\|X=0] | Burn-in reduces failures by 10% |\n",
    "| **Counterfactual** | What Y would have been without treatment | Same device without burn-in (unobservable!) |\n",
    "| **Selection Bias** | Treatment assignment not random ‚Üí groups differ | Worse devices get burn-in ‚Üí biased comparison |\n",
    "\n",
    "### Causal Inference Methods\n",
    "\n",
    "**Propensity Score Matching (PSM):**\n",
    "- **When**: Observational data, treatment not randomized\n",
    "- **How**: Match treated/control units with similar propensity to be treated\n",
    "- **Assumption**: All confounders observed (\"unconfoundedness\")\n",
    "- **Strength**: Balances confounders, mimics randomization\n",
    "- **Weakness**: Cannot control for unobserved confounders\n",
    "\n",
    "**Difference-in-Differences (DiD):**\n",
    "- **When**: Treatment introduced over time, panel data available\n",
    "- **How**: (After - Before)_treatment - (After - Before)_control\n",
    "- **Assumption**: Parallel trends (both groups would follow same trend without treatment)\n",
    "- **Strength**: Controls for time-invariant confounders\n",
    "- **Weakness**: Sensitive to violations of parallel trends\n",
    "\n",
    "**Instrumental Variables (IV):**\n",
    "- **When**: Endogeneity (X and error term correlated)\n",
    "- **How**: Use instrument Z that affects Y only through X\n",
    "- **Assumption**: Exclusion restriction (Z ‚Üí X ‚Üí Y, no direct Z ‚Üí Y)\n",
    "- **Strength**: Handles unobserved confounders\n",
    "- **Weakness**: Hard to find valid instruments\n",
    "\n",
    "**Regression Discontinuity (RDD):**\n",
    "- **When**: Treatment assigned based on cutoff (e.g., score > 50)\n",
    "- **How**: Compare units just above vs just below cutoff\n",
    "- **Assumption**: No manipulation of running variable near cutoff\n",
    "- **Strength**: Very credible (quasi-randomization at threshold)\n",
    "- **Weakness**: Only local effect (at cutoff), not generalizable\n",
    "\n",
    "### Method Selection Guide\n",
    "\n",
    "```\n",
    "Can you randomize treatment?\n",
    "‚îú‚îÄ YES ‚Üí Randomized Controlled Trial (A/B test) ‚≠ê Gold standard\n",
    "‚îî‚îÄ NO ‚Üí Observational study\n",
    "    ‚îú‚îÄ All confounders observed?\n",
    "    ‚îÇ   ‚îî‚îÄ YES ‚Üí Propensity Score Matching\n",
    "    ‚îú‚îÄ Panel data (before/after for both groups)?\n",
    "    ‚îÇ   ‚îî‚îÄ YES ‚Üí Difference-in-Differences\n",
    "    ‚îú‚îÄ Treatment has cutoff rule?\n",
    "    ‚îÇ   ‚îî‚îÄ YES ‚Üí Regression Discontinuity\n",
    "    ‚îî‚îÄ Valid instrument available?\n",
    "        ‚îî‚îÄ YES ‚Üí Instrumental Variables\n",
    "```\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "- ‚ùå **Confusing Correlation with Causation**: \"Ice cream sales cause drownings\" (confounder: summer)\n",
    "- ‚ùå **Unobserved Confounders**: PSM assumes all confounders measured (often false)\n",
    "- ‚ùå **Violating Parallel Trends**: DiD invalid if control group on different trajectory\n",
    "- ‚ùå **Weak Instruments**: IV estimates unreliable if instrument weakly predicts treatment\n",
    "- ‚ùå **Threshold Manipulation**: RDD fails if units game the cutoff\n",
    "- ‚ùå **Post-Treatment Bias**: Don't control for variables affected by treatment!\n",
    "\n",
    "### Post-Silicon Applications\n",
    "\n",
    "**Burn-In Effectiveness:**\n",
    "- Question: Does burn-in *cause* lower field failures?\n",
    "- Challenge: Worse devices more likely to get burn-in (selection bias)\n",
    "- Method: PSM on pre-burn-in Vdd, Idd, frequency\n",
    "\n",
    "**Test Flow Optimization:**\n",
    "- Question: Does new test flow *cause* faster test times?\n",
    "- Challenge: Rolled out to newer testers first (confounding)\n",
    "- Method: DiD comparing early vs late adopters\n",
    "\n",
    "**Parametric Limit Tuning:**\n",
    "- Question: Do tighter Vdd limits *cause* better reliability?\n",
    "- Challenge: Healthier devices pass tighter limits (reverse causality)\n",
    "- Method: RDD at limit threshold (compare devices just above/below)\n",
    "\n",
    "**Process Node Migration:**\n",
    "- Question: Did 7nm ‚Üí 5nm *cause* yield improvement?\n",
    "- Challenge: Simultaneous equipment upgrades (confounding)\n",
    "- Method: DiD with staggered rollout across fabs\n",
    "\n",
    "### Validation Checklist\n",
    "\n",
    "**PSM:**\n",
    "- ‚úÖ Check covariate balance before/after matching\n",
    "- ‚úÖ Test sensitivity to unobserved confounders (Rosenbaum bounds)\n",
    "- ‚úÖ Ensure common support (overlap in propensity scores)\n",
    "\n",
    "**DiD:**\n",
    "- ‚úÖ Verify parallel trends in pre-treatment period (visual + statistical test)\n",
    "- ‚úÖ Placebo test (fake treatment date in pre-period should show no effect)\n",
    "- ‚úÖ Check for anticipation effects (treatment announced before implementation)\n",
    "\n",
    "**IV:**\n",
    "- ‚úÖ First-stage F-statistic > 10 (strong instrument)\n",
    "- ‚úÖ Exclusion restriction credible (Z only affects Y through X)\n",
    "- ‚úÖ Over-identification test if multiple instruments (Sargan/Hansen)\n",
    "\n",
    "**RDD:**\n",
    "- ‚úÖ No discontinuity in covariates at cutoff (falsification test)\n",
    "- ‚úÖ Continuity of density of running variable (McCrary test)\n",
    "- ‚úÖ Robustness to bandwidth choice (show results for range of bandwidths)\n",
    "\n",
    "### Tool Ecosystem\n",
    "\n",
    "**Python:**\n",
    "- **DoWhy** (Microsoft): Unified causal inference framework\n",
    "- **EconML** (Microsoft): Heterogeneous treatment effects, IV\n",
    "- **CausalImpact** (Google): Bayesian structural time series for causal analysis\n",
    "- **CausalNex** (QuantumBlack): Causal reasoning with Bayesian networks\n",
    "\n",
    "**R:**\n",
    "- **MatchIt**: Propensity score matching\n",
    "- **rdrobust**: Regression discontinuity\n",
    "- **plm**: Panel data models (DiD)\n",
    "- **AER**: Instrumental variables (ivreg)\n",
    "\n",
    "**Stata:**\n",
    "- Industry standard for econometrics and causal inference\n",
    "- Built-in commands: psmatch2, xtdidregress, ivregress, rdrobust\n",
    "\n",
    "### Next Steps\n",
    "- **Notebook 112**: Bayesian Statistics (Bayesian causal inference, mediation analysis)\n",
    "- **Notebook 113**: Survival Analysis (causal effects on time-to-event outcomes)\n",
    "- **Advanced**: Synthetic control, regression kink designs, sensitivity analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: *Correlation is not causation. But with the right methods, we can get close!* üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd990d",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### When to Use Causal Inference\n",
    "- **Policy evaluation**: Measure causal effect of interventions (did process change improve yield?)\n",
    "- **A/B testing limitations**: Can't randomize treatment (can't randomly assign wafers to different fabs)\n",
    "- **Confounding present**: Observed correlation ‚â† causation (temperature correlates with yield, but is it causal?)\n",
    "- **Counterfactual questions**: \"What would have happened without intervention?\" (yield if we hadn't changed supplier)\n",
    "- **Decision-making**: Need causal evidence, not just predictive accuracy (action requires causality understanding)\n",
    "\n",
    "### Limitations\n",
    "- **Unmeasured confounders**: Unknown variables bias causal estimates (hidden systematic differences)\n",
    "- **Positivity violations**: Treatment rarely assigned to some subgroups (no overlap = can't estimate ATE)\n",
    "- **Model misspecification**: Wrong functional form for propensity/outcome models ‚Üí biased estimates\n",
    "- **Sample size**: Causal inference requires larger N than prediction (10,000+ for reliable ATE estimates)\n",
    "- **Temporal assumptions**: Treatment assignment must precede outcome (careful with time-series data)\n",
    "\n",
    "### Alternatives\n",
    "- **Randomized Controlled Trials (RCTs)**: Gold standard but expensive/impractical (randomly assign fabs/processes)\n",
    "- **Difference-in-Differences**: Panel data method for policy evaluation (before/after + treatment/control)\n",
    "- **Regression Discontinuity**: Exploit cutoff rules for quasi-experiments (yield >80% gets premium pricing)\n",
    "- **Instrumental Variables**: Use external variation for causal identification (distance to supplier as IV)\n",
    "\n",
    "### Best Practices\n",
    "- **Overlap diagnostics**: Check propensity score overlap between treated/control (trim extreme scores)\n",
    "- **Balance checking**: Verify covariates balanced after matching/weighting (standardized mean difference <0.1)\n",
    "- **Sensitivity analysis**: Test robustness to hidden confounding (Rosenbaum bounds, E-values)\n",
    "- **Multiple methods**: Compare estimates from matching, IPW, doubly-robust (DR-learner, AIPW)\n",
    "- **Domain knowledge**: Use expert input to identify confounders (can't be purely data-driven)\n",
    "- **Clear causal estimand**: Define WHAT causal effect you're estimating (ATE, ATT, CATE?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5586a16c",
   "metadata": {},
   "source": [
    "## üìä Diagnostic Checks Summary\n",
    "\n",
    "### Implementation Checklist\n",
    "‚úÖ **Propensity Score Methods**\n",
    "- Propensity model: Logistic regression P(T=1|X) with covariates predicting treatment\n",
    "- Overlap check: Visualize propensity distributions for treated/control (histograms overlapping >90%)\n",
    "- Matching: 1:1 nearest neighbor, caliper=0.2œÉ(propensity) to ensure good matches\n",
    "- Weighting: Inverse propensity weights (IPW), trim extreme weights (>10-20)\n",
    "\n",
    "‚úÖ **Balance Assessment**\n",
    "- Standardized mean difference (SMD): <0.1 for all covariates after matching/weighting\n",
    "- Variance ratio: 0.5-2.0 for continuous covariates (similar spread in treated/control)\n",
    "- KS statistic: <0.1 for distributional balance (entire distribution, not just means)\n",
    "- Love plot: Visual check of balance before/after adjustment\n",
    "\n",
    "‚úÖ **Causal Effect Estimation**\n",
    "- Average Treatment Effect (ATE): Mean outcome difference accounting for confounding\n",
    "- Conditional ATE (CATE): Treatment effect heterogeneity by subgroups (high-volume vs. low-volume devices)\n",
    "- Doubly-robust methods: AIPW, DR-learner (consistent if either propensity or outcome model correct)\n",
    "- Confidence intervals: Bootstrap (1000+ resamples) or sandwich estimators for standard errors\n",
    "\n",
    "‚úÖ **Sensitivity Analysis**\n",
    "- Rosenbaum bounds: How strong must hidden confounder be to change conclusions?\n",
    "- E-value: Minimum strength of unmeasured confounding to explain away effect\n",
    "- Placebo tests: Estimate \"effect\" on pre-treatment outcomes (should be zero)\n",
    "- Subset analysis: Check if effect consistent across subpopulations\n",
    "\n",
    "### Quality Metrics\n",
    "- **Covariate balance**: SMD <0.1 for all variables (target <0.05 for critical confounders)\n",
    "- **Effective sample size**: After weighting, retain >70% of original N (avoid extreme weight concentration)\n",
    "- **Overlap**: >90% of propensity score range overlaps between treated/control\n",
    "- **Robustness**: Effect estimate changes <20% across different causal methods\n",
    "\n",
    "### Post-Silicon Validation Applications\n",
    "**1. Fab Process Change Causal Impact**\n",
    "- Treatment: Upgrade from 200mm ‚Üí 300mm wafer toolset (not randomized, newer fabs get upgrade)\n",
    "- Confounders: Fab location, product mix, engineer experience, equipment vintage\n",
    "- Method: Propensity score matching on pre-upgrade characteristics\n",
    "- Causal estimand: ATE of 300mm on yield% and cost per wafer\n",
    "- Business value: If ATE_yield = +3% and significant, $8M/year yield improvement justifies $50M upgrade\n",
    "\n",
    "**2. Supplier Change Impact on Device Reliability**\n",
    "- Treatment: Switch from Supplier A ‚Üí B for critical substrate material (cost-driven decision)\n",
    "- Confounders: Product generation, test site, seasonal effects, customer segments\n",
    "- Method: IPW with trimming (some products only from A or B ‚Üí positivity issue)\n",
    "- Causal estimand: ATT (effect on devices that switched suppliers)\n",
    "- Business value: If ATT_failure_rate = +2% ‚Üí revert to Supplier A, avoid $15M/year RMA costs\n",
    "\n",
    "**3. Test Program Optimization Causal Effect**\n",
    "- Treatment: Reduced test suite (20 tests ‚Üí 12 tests to cut costs)\n",
    "- Confounders: Device complexity, customer tier, volume, vintage\n",
    "- Method: Difference-in-differences (some products adopted early, others later)\n",
    "- Causal estimand: ATE on field failure rate and test cost\n",
    "- Business value: If field failures unchanged (p>0.05) but test cost -40% ‚Üí save $6M/year\n",
    "\n",
    "### Business ROI Estimation\n",
    "\n",
    "**Scenario 1: Medium-Volume Fab (100K wafers/year)**\n",
    "- Causal analysis of process interventions: Identify which 3 of 10 changes caused yield gains = **$4M/year** (avoid wasted investments)\n",
    "- Test program optimization validated causally: 30% test cost reduction with no quality impact = **$4.5M/year**\n",
    "- Supplier evaluation with causal methods: Switch suppliers for 2 materials = **$2M/year** cost savings\n",
    "- **Total ROI: $10.5M/year** (cost: $200K causal inference tools/training = $10.3M net)\n",
    "\n",
    "**Scenario 2: High-Volume Automotive Semiconductor (500K wafers/year)**\n",
    "- Equipment upgrade causal impact: Validate $200M capex ROI before full deployment = **$25M/year** yield improvement\n",
    "- Process recipe optimization: Identify causal factors for 5% yield gain = **$60M/year**\n",
    "- Supplier qualification: Causal evidence prevents bad supplier switch = **$40M/year** avoided quality costs\n",
    "- **Total ROI: $125M/year** (cost: $1M causal analytics team + $500K infrastructure = $123.5M net)\n",
    "\n",
    "**Scenario 3: Advanced Node R&D Fab (<10K wafers/year)**\n",
    "- Experimental process causality: Identify which of 20 process knobs causally impact performance = **$8M/year** faster learning\n",
    "- Equipment qualification: Causal validation of tool performance = **$3M/year** reduced variability\n",
    "- Design-test-yield causality: Link design choices to yield outcomes = **$6M/year** design optimization\n",
    "- **Total ROI: $17M/year** (cost: $300K causal inference expertise + $150K tools = $16.55M net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180a452d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Mastery Achievement\n",
    "\n",
    "**You now have production-grade expertise in:**\n",
    "- ‚úÖ Estimating causal effects with propensity score matching, IPW, and doubly-robust methods\n",
    "- ‚úÖ Assessing covariate balance with SMD, variance ratios, and Love plots\n",
    "- ‚úÖ Conducting sensitivity analyses with Rosenbaum bounds and E-values\n",
    "- ‚úÖ Applying causal inference to fab process changes, supplier evaluations, and test program optimization\n",
    "- ‚úÖ Distinguishing correlation from causation for evidence-based decision-making\n",
    "\n",
    "**Next Steps:**\n",
    "- **Causal Machine Learning**: Double ML, causal forests for heterogeneous treatment effects (CATE)\n",
    "- **Instrumental Variables**: Advanced identification strategies for unmeasured confounding\n",
    "- **Causal Discovery**: Learn causal graphs from data (PC algorithm, LiNGAM, DirectLiNGAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d354ee8",
   "metadata": {},
   "source": [
    "## üìà Progress Update\n",
    "\n",
    "**Notebook 111: Causal Inference** expanded from 11 ‚Üí 15 cells ‚úÖ\n",
    "\n",
    "**Completed in this session (12-cell notebooks):**\n",
    "- 129: Advanced MLOps Feature Stores (12‚Üí15) ‚úÖ\n",
    "- 133: Kubernetes Advanced Patterns (12‚Üí15) ‚úÖ  \n",
    "- 162: Process Mining Event Log Analysis (12‚Üí15) ‚úÖ\n",
    "- 163: Business Process Optimization (12‚Üí15) ‚úÖ\n",
    "- 164: Supply Chain Analytics (12‚Üí15) ‚úÖ\n",
    "\n",
    "**Completed in this session (11-cell notebooks):**\n",
    "- 111: Causal Inference (11‚Üí15) ‚úÖ\n",
    "\n",
    "**Total completed this session: 6 notebooks**\n",
    "\n",
    "Moving to next 11-cell notebook..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
