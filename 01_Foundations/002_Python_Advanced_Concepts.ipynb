{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b262241",
   "metadata": {},
   "source": [
    "# 002: Python Advanced Concepts",
    "",
    "Decorators are a powerful Python feature that allows you to modify or enhance functions and classes without changing their source code.",
    "",
    "### 1.1 Function Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331bfba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic decorator example\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def timing_decorator(func):\n",
    "    \"\"\"Measure execution time of a function\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"{func.__name__} took {end_time - start_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@timing_decorator\n",
    "def slow_function(n):\n",
    "    \"\"\"Simulate a slow operation\"\"\"\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += i ** 2\n",
    "    return total\n",
    "\n",
    "# Test the decorator\n",
    "result = slow_function(1000000)\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812d0c2f",
   "metadata": {},
   "source": [
    "### 1.2 Decorators with Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ddc8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat(times):\n",
    "    \"\"\"Decorator that repeats function execution\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            results = []\n",
    "            for _ in range(times):\n",
    "                results.append(func(*args, **kwargs))\n",
    "            return results\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@repeat(times=3)\n",
    "def greet(name):\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "# This will execute 3 times\n",
    "messages = greet(\"Data Scientist\")\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6957cb32",
   "metadata": {},
   "source": [
    "### 1.3 Class Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8045cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleton(cls):\n",
    "    \"\"\"Ensure only one instance of a class exists\"\"\"\n",
    "    instances = {}\n",
    "    \n",
    "    @wraps(cls)\n",
    "    def get_instance(*args, **kwargs):\n",
    "        if cls not in instances:\n",
    "            instances[cls] = cls(*args, **kwargs)\n",
    "        return instances[cls]\n",
    "    \n",
    "    return get_instance\n",
    "\n",
    "@singleton\n",
    "class DatabaseConnection:\n",
    "    def __init__(self, host, port):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        print(f\"Creating connection to {host}:{port}\")\n",
    "\n",
    "# Both will return the same instance\n",
    "db1 = DatabaseConnection(\"localhost\", 5432)\n",
    "db2 = DatabaseConnection(\"localhost\", 5432)\n",
    "print(f\"Same instance? {db1 is db2}\")  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f1ee86",
   "metadata": {},
   "source": [
    "### 1.4 Real-World Application: Memoization for ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2a7805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "\n",
    "class MLModelCache:\n",
    "    \"\"\"Cache expensive model predictions\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=128)\n",
    "    def expensive_prediction(feature_tuple):\n",
    "        \"\"\"Simulate expensive model inference\"\"\"\n",
    "        features = np.array(feature_tuple)\n",
    "        # Simulate complex computation\n",
    "        time.sleep(0.1)  # Pretend this takes time\n",
    "        return np.sum(features ** 2)\n",
    "    \n",
    "    @classmethod\n",
    "    def predict(cls, features):\n",
    "        \"\"\"Convert features to tuple for caching\"\"\"\n",
    "        return cls.expensive_prediction(tuple(features))\n",
    "\n",
    "# Test caching performance\n",
    "model = MLModelCache()\n",
    "\n",
    "# First call - slow\n",
    "start = time.time()\n",
    "result1 = model.predict([1, 2, 3, 4, 5])\n",
    "print(f\"First call: {time.time() - start:.4f}s, Result: {result1}\")\n",
    "\n",
    "# Second call with same input - fast (cached)\n",
    "start = time.time()\n",
    "result2 = model.predict([1, 2, 3, 4, 5])\n",
    "print(f\"Cached call: {time.time() - start:.4f}s, Result: {result2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a77b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Generators and Iterators\n",
    "\n",
    "Generators are memory-efficient tools for handling large datasets - crucial for big data processing.\n",
    "\n",
    "### 2.1 Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e00e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_stream_generator(n):\n",
    "    \"\"\"Simulate streaming data - memory efficient\"\"\"\n",
    "    for i in range(n):\n",
    "        # Yield instead of return - creates generator\n",
    "        yield {\"id\": i, \"value\": i ** 2, \"timestamp\": time.time()}\n",
    "\n",
    "# Process large dataset without loading all into memory\n",
    "print(\"Processing stream of 1 million records...\")\n",
    "total = 0\n",
    "for record in data_stream_generator(1_000_000):\n",
    "    total += record[\"value\"]\n",
    "    if record[\"id\"] % 200_000 == 0:\n",
    "        print(f\"Processed {record['id']} records...\")\n",
    "\n",
    "print(f\"Total sum: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84611da3",
   "metadata": {},
   "source": [
    "### 2.2 Generator Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ab14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory comparison: List vs Generator\n",
    "import sys\n",
    "\n",
    "# List comprehension - loads all in memory\n",
    "list_comp = [x ** 2 for x in range(10000)]\n",
    "print(f\"List size: {sys.getsizeof(list_comp)} bytes\")\n",
    "\n",
    "# Generator expression - evaluates lazily\n",
    "gen_exp = (x ** 2 for x in range(10000))\n",
    "print(f\"Generator size: {sys.getsizeof(gen_exp)} bytes\")\n",
    "\n",
    "# Both produce same results\n",
    "print(f\"Sum from list: {sum([x ** 2 for x in range(10000)])}\")\n",
    "print(f\"Sum from generator: {sum((x ** 2 for x in range(10000)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a53a5",
   "metadata": {},
   "source": [
    "### 2.3 Custom Iterator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDFFileReader:\n",
    "    \"\"\"Custom iterator for reading STDF test data files\"\"\"\n",
    "    \n",
    "    def __init__(self, filename, batch_size=1000):\n",
    "        self.filename = filename\n",
    "        self.batch_size = batch_size\n",
    "        self.current_batch = 0\n",
    "        self.total_records = 10000  # Simulated\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.current_batch = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.current_batch * self.batch_size >= self.total_records:\n",
    "            raise StopIteration\n",
    "        \n",
    "        # Simulate reading a batch\n",
    "        start = self.current_batch * self.batch_size\n",
    "        end = min(start + self.batch_size, self.total_records)\n",
    "        batch_data = [\n",
    "            {\"test_id\": i, \"result\": \"PASS\" if i % 10 != 0 else \"FAIL\"}\n",
    "            for i in range(start, end)\n",
    "        ]\n",
    "        \n",
    "        self.current_batch += 1\n",
    "        return batch_data\n",
    "\n",
    "# Use the iterator\n",
    "reader = STDFFileReader(\"test_data.stdf\", batch_size=2000)\n",
    "for batch_num, batch in enumerate(reader, 1):\n",
    "    fail_count = sum(1 for record in batch if record[\"result\"] == \"FAIL\")\n",
    "    print(f\"Batch {batch_num}: {len(batch)} records, {fail_count} failures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07111a20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Context Managers\n",
    "\n",
    "Context managers ensure proper resource management - critical for file operations, database connections, and locks.\n",
    "\n",
    "### 3.1 Using Context Managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional file handling (risky)\n",
    "def read_file_risky(filename):\n",
    "    f = open(filename, 'r')\n",
    "    data = f.read()\n",
    "    f.close()  # What if an exception occurs before this?\n",
    "    return data\n",
    "\n",
    "# Safe file handling with context manager\n",
    "def read_file_safe(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read()\n",
    "    # File automatically closed, even if exception occurs\n",
    "    return data\n",
    "\n",
    "# Multiple context managers\n",
    "with open('input.txt', 'r') as infile, open('output.txt', 'w') as outfile:\n",
    "    for line in infile:\n",
    "        outfile.write(line.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc98878",
   "metadata": {},
   "source": [
    "### 3.2 Creating Custom Context Managers (Class-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d2aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"Context manager for timing code execution\"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"Code block\"):\n",
    "        self.name = name\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        print(f\"Starting: {self.name}\")\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.end_time = time.time()\n",
    "        self.elapsed = self.end_time - self.start_time\n",
    "        print(f\"Finished: {self.name} in {self.elapsed:.4f}s\")\n",
    "        \n",
    "        # Return False to propagate exceptions\n",
    "        return False\n",
    "\n",
    "# Use the timer\n",
    "with Timer(\"Data processing\"):\n",
    "    data = [i ** 2 for i in range(1000000)]\n",
    "    result = sum(data)\n",
    "    print(f\"Processed {len(data)} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a6d1f",
   "metadata": {},
   "source": [
    "### 3.3 Creating Context Managers with contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b0d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def database_transaction(connection):\n",
    "    \"\"\"Manage database transaction with automatic rollback on error\"\"\"\n",
    "    print(\"Beginning transaction...\")\n",
    "    try:\n",
    "        yield connection\n",
    "        print(\"Committing transaction...\")\n",
    "        # connection.commit()  # Simulated\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        print(\"Rolling back transaction...\")\n",
    "        # connection.rollback()  # Simulated\n",
    "        raise\n",
    "    finally:\n",
    "        print(\"Closing connection...\")\n",
    "        # connection.close()  # Simulated\n",
    "\n",
    "# Simulate usage\n",
    "class FakeConnection:\n",
    "    pass\n",
    "\n",
    "conn = FakeConnection()\n",
    "with database_transaction(conn) as db:\n",
    "    print(\"Executing queries...\")\n",
    "    # Perform database operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74e2162",
   "metadata": {},
   "source": [
    "### 3.4 Real-World: Model Training Context Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def ml_training_session(model_name, log_dir=\"./logs\"):\n",
    "    \"\"\"Context manager for ML training with automatic cleanup\"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Setup\n",
    "    print(f\"Starting training session: {model_name}\")\n",
    "    start_time = time.time()\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    session_info = {\n",
    "        \"model\": model_name,\n",
    "        \"start_time\": start_time,\n",
    "        \"log_dir\": log_dir\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        yield session_info\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        # Save error state\n",
    "        raise\n",
    "    finally:\n",
    "        # Cleanup and logging\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"Training session completed in {duration:.2f}s\")\n",
    "        # Save training metadata\n",
    "        print(f\"Logs saved to {log_dir}\")\n",
    "\n",
    "# Use in training\n",
    "with ml_training_session(\"RandomForest_v1\") as session:\n",
    "    print(f\"Training {session['model']}...\")\n",
    "    time.sleep(0.5)  # Simulate training\n",
    "    print(\"Epoch 1 complete\")\n",
    "    print(\"Epoch 2 complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb11cc0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Magic Methods and Operator Overloading\n",
    "\n",
    "Magic methods (dunder methods) allow you to define how objects behave with Python operators.\n",
    "\n",
    "### 4.1 Common Magic Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee16065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vector:\n",
    "    \"\"\"2D Vector with operator overloading\"\"\"\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"String representation for developers\"\"\"\n",
    "        return f\"Vector({self.x}, {self.y})\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"String representation for users\"\"\"\n",
    "        return f\"<{self.x}, {self.y}>\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        \"\"\"Vector addition: v1 + v2\"\"\"\n",
    "        return Vector(self.x + other.x, self.y + other.y)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        \"\"\"Vector subtraction: v1 - v2\"\"\"\n",
    "        return Vector(self.x - other.x, self.y - other.y)\n",
    "    \n",
    "    def __mul__(self, scalar):\n",
    "        \"\"\"Scalar multiplication: v * scalar\"\"\"\n",
    "        return Vector(self.x * scalar, self.y * scalar)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Equality comparison: v1 == v2\"\"\"\n",
    "        return self.x == other.x and self.y == other.y\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Length (magnitude) of vector\"\"\"\n",
    "        return int((self.x ** 2 + self.y ** 2) ** 0.5)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Access by index: v[0], v[1]\"\"\"\n",
    "        if index == 0:\n",
    "            return self.x\n",
    "        elif index == 1:\n",
    "            return self.y\n",
    "        else:\n",
    "            raise IndexError(\"Vector index out of range\")\n",
    "\n",
    "# Test the vector class\n",
    "v1 = Vector(3, 4)\n",
    "v2 = Vector(1, 2)\n",
    "\n",
    "print(f\"v1: {v1}\")\n",
    "print(f\"v2: {v2}\")\n",
    "print(f\"v1 + v2: {v1 + v2}\")\n",
    "print(f\"v1 - v2: {v1 - v2}\")\n",
    "print(f\"v1 * 3: {v1 * 3}\")\n",
    "print(f\"v1 == v2: {v1 == v2}\")\n",
    "print(f\"Length of v1: {len(v1)}\")\n",
    "print(f\"v1[0]: {v1[0]}, v1[1]: {v1[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5eb298",
   "metadata": {},
   "source": [
    "### 4.2 Real-World: Custom ML Model Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33051e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEnsemble:\n",
    "    \"\"\"Ensemble of ML models with operator overloading\"\"\"\n",
    "    \n",
    "    def __init__(self, models=None):\n",
    "        self.models = models or []\n",
    "    \n",
    "    def __add__(self, model):\n",
    "        \"\"\"Add model to ensemble: ensemble + model\"\"\"\n",
    "        new_ensemble = ModelEnsemble(self.models.copy())\n",
    "        new_ensemble.models.append(model)\n",
    "        return new_ensemble\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of models in ensemble\"\"\"\n",
    "        return len(self.models)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Access model by index\"\"\"\n",
    "        return self.models[index]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate over models\"\"\"\n",
    "        return iter(self.models)\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        \"\"\"Make ensemble callable: ensemble(X)\"\"\"\n",
    "        predictions = [model(X) for model in self.models]\n",
    "        # Average predictions\n",
    "        return sum(predictions) / len(predictions)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ModelEnsemble(models={len(self.models)})\"\n",
    "\n",
    "# Simulate simple models\n",
    "class SimpleModel:\n",
    "    def __init__(self, name, bias):\n",
    "        self.name = name\n",
    "        self.bias = bias\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return X + self.bias\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Model({self.name})\"\n",
    "\n",
    "# Build ensemble using operator overloading\n",
    "ensemble = ModelEnsemble()\n",
    "ensemble = ensemble + SimpleModel(\"M1\", 1.0)\n",
    "ensemble = ensemble + SimpleModel(\"M2\", 2.0)\n",
    "ensemble = ensemble + SimpleModel(\"M3\", 1.5)\n",
    "\n",
    "print(f\"Ensemble: {ensemble}\")\n",
    "print(f\"Number of models: {len(ensemble)}\")\n",
    "print(f\"First model: {ensemble[0]}\")\n",
    "print(f\"Prediction for X=10: {ensemble(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b3b1d1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Meta-Programming\n",
    "\n",
    "Meta-programming allows you to write code that manipulates code - advanced but powerful.\n",
    "\n",
    "### 5.1 Dynamic Class Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c6064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_class(name, features):\n",
    "    \"\"\"Dynamically create a model class\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        for feature in features:\n",
    "            setattr(self, feature, kwargs.get(feature, None))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        attrs = \", \".join(f\"{k}={getattr(self, k)}\" for k in features)\n",
    "        return f\"{name}({attrs})\"\n",
    "    \n",
    "    # Create class dynamically\n",
    "    return type(name, (), {\n",
    "        '__init__': __init__,\n",
    "        '__repr__': __repr__,\n",
    "        'features': features\n",
    "    })\n",
    "\n",
    "# Create different model classes dynamically\n",
    "LinearModel = create_model_class('LinearModel', ['coef', 'intercept'])\n",
    "TreeModel = create_model_class('TreeModel', ['max_depth', 'n_estimators'])\n",
    "\n",
    "# Use the dynamically created classes\n",
    "lr = LinearModel(coef=[1.5, 2.3], intercept=0.5)\n",
    "rf = TreeModel(max_depth=10, n_estimators=100)\n",
    "\n",
    "print(lr)\n",
    "print(rf)\n",
    "print(f\"LinearModel features: {LinearModel.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85629123",
   "metadata": {},
   "source": [
    "### 5.2 Metaclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1159d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationMeta(type):\n",
    "    \"\"\"Metaclass that adds validation to all methods\"\"\"\n",
    "    \n",
    "    def __new__(cls, name, bases, attrs):\n",
    "        # Wrap all methods with validation\n",
    "        for key, value in attrs.items():\n",
    "            if callable(value) and not key.startswith('_'):\n",
    "                attrs[key] = cls.validate_wrapper(value)\n",
    "        \n",
    "        return super().__new__(cls, name, bases, attrs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_wrapper(func):\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "            print(f\"Validating call to {func.__name__}...\")\n",
    "            result = func(self, *args, **kwargs)\n",
    "            print(f\"Validation complete for {func.__name__}\")\n",
    "            return result\n",
    "        return wrapper\n",
    "\n",
    "class DataProcessor(metaclass=ValidationMeta):\n",
    "    \"\"\"All methods automatically get validation\"\"\"\n",
    "    \n",
    "    def clean_data(self, data):\n",
    "        return [x for x in data if x is not None]\n",
    "    \n",
    "    def transform_data(self, data):\n",
    "        return [x * 2 for x in data]\n",
    "\n",
    "# Test the metaclass\n",
    "processor = DataProcessor()\n",
    "clean = processor.clean_data([1, 2, None, 3])\n",
    "transformed = processor.transform_data(clean)\n",
    "print(f\"Result: {transformed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c80168",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Project: Advanced Python for STDF Analysis\n",
    "\n",
    "Let's apply all advanced concepts to build a sophisticated STDF data processor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from functools import lru_cache, wraps\n",
    "import time\n",
    "from typing import Iterator, Dict, Any\n",
    "class STDFAnalyzer:\n",
    "    \"\"\"Enterprise-grade STDF file analyzer using advanced Python\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_size=1000):\n",
    "        self._cache_size = cache_size\n",
    "        self._metrics = {'reads': 0, 'cache_hits': 0}\n",
    "    \n",
    "    # Decorator for timing\n",
    "    @staticmethod\n",
    "    def timed(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            duration = time.time() - start\n",
    "            print(f\"{func.__name__}: {duration:.4f}s\")\n",
    "            return result\n",
    "        return wrapper\n",
    "    \n",
    "    # Generator for memory-efficient reading\n",
    "    def read_stdf_records(self, filename: str, batch_size: int = 1000) -> Iterator[Dict]:\n",
    "        \"\"\"Generator that yields STDF records in batches\"\"\"\n",
    "        total_records = 50000  # Simulated\n",
    "        \n",
    "        for batch_start in range(0, total_records, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, total_records)\n",
    "            # Simulate reading records\n",
    "            yield {\n",
    "                'batch_id': batch_start // batch_size,\n",
    "                'records': [\n",
    "                    {\n",
    "                        'test_id': i,\n",
    "                        'voltage': 3.3 + (i % 10) * 0.01,\n",
    "                        'current': 0.5 + (i % 5) * 0.02,\n",
    "                        'result': 'PASS' if i % 15 != 0 else 'FAIL'\n",
    "                    }\n",
    "                    for i in range(batch_start, batch_end)\n",
    "                ]\n",
    "            }\n",
    "            self._metrics['reads'] += 1\n",
    "    \n",
    "    # Context manager for analysis session\n",
    "    @contextmanager\n",
    "    def analysis_session(self, filename: str):\n",
    "        \"\"\"Context manager for STDF analysis with cleanup\"\"\"\n",
    "        print(f\"Opening STDF file: {filename}\")\n",
    "        session_data = {\n",
    "            'filename': filename,\n",
    "            'start_time': time.time(),\n",
    "            'records_processed': 0\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            yield session_data\n",
    "        finally:\n",
    "            duration = time.time() - session_data['start_time']\n",
    "            print(f\"Analysis complete:\")\n",
    "            print(f\"  - File: {filename}\")\n",
    "            print(f\"  - Records: {session_data['records_processed']}\")\n",
    "            print(f\"  - Duration: {duration:.2f}s\")\n",
    "            print(f\"  - Cache hits: {self._metrics['cache_hits']}/{self._metrics['reads']}\")\n",
    "    \n",
    "    # Cached computation\n",
    "    @lru_cache(maxsize=128)\n",
    "    def compute_test_statistics(self, test_id: int, n_samples: int) -> Dict[str, float]:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\"Compute statistics for a test (cached for performance)\"\"\"\n",
    "        # Simulate expensive computation\n",
    "        time.sleep(0.01)\n",
    "        return {\n",
    "            'mean': test_id * 1.5,\n",
    "            'std': test_id * 0.1,\n",
    "            'n_samples': n_samples\n",
    "        }\n",
    "    \n",
    "    @timed\n",
    "    def analyze_file(self, filename: str) -> Dict[str, Any]:\n",
    "        \"\"\"Complete STDF file analysis using all advanced features\"\"\"\n",
    "        \n",
    "        with self.analysis_session(filename) as session:\n",
    "            results = {\n",
    "                'total_records': 0,\n",
    "                'pass_count': 0,\n",
    "                'fail_count': 0,\n",
    "                'test_stats': {}\n",
    "            }\n",
    "            \n",
    "            # Process in batches using generator\n",
    "            for batch in self.read_stdf_records(filename):\n",
    "                for record in batch['records']:\n",
    "                    results['total_records'] += 1\n",
    "                    \n",
    "                    if record['result'] == 'PASS':\n",
    "                        results['pass_count'] += 1\n",
    "                    else:\n",
    "                        results['fail_count'] += 1\n",
    "                    \n",
    "                    # Use cached statistics\n",
    "                    test_id = record['test_id'] % 10  # Group tests\n",
    "                    if test_id not in results['test_stats']:\n",
    "                        results['test_stats'][test_id] = self.compute_test_statistics(\n",
    "                            test_id, 100\n",
    "                        )\n",
    "                        self._metrics['cache_hits'] += 1\n",
    "            \n",
    "            session['records_processed'] = results['total_records']\n",
    "            \n",
    "            # Calculate yield\n",
    "            results['yield_percent'] = (\n",
    "                results['pass_count'] / results['total_records'] * 100\n",
    "            )\n",
    "            \n",
    "            return results\n",
    "# Demonstrate the advanced STDF analyzer\n",
    "print(\"=\"*60)\n",
    "print(\"Advanced STDF Analyzer Demo\")\n",
    "print(\"=\"*60)\n",
    "analyzer = STDFAnalyzer()\n",
    "results = analyzer.analyze_file(\"production_data.stdf\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Analysis Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Records: {results['total_records']}\")\n",
    "print(f\"Pass: {results['pass_count']}\")\n",
    "print(f\"Fail: {results['fail_count']}\")\n",
    "print(f\"Yield: {results['yield_percent']:.2f}%\")\n",
    "print(f\"Unique Tests: {len(results['test_stats'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a6c2d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Key Takeaways\n",
    "\n",
    "### When to Use Each Feature:\n",
    "\n",
    "1. **Decorators**\n",
    "   - \u2705 Cross-cutting concerns (logging, timing, caching)\n",
    "   - \u2705 Access control and authentication\n",
    "   - \u2705 Input validation\n",
    "\n",
    "2. **Generators**\n",
    "   - \u2705 Large datasets that don't fit in memory\n",
    "   - \u2705 Streaming data processing\n",
    "   - \u2705 Infinite sequences\n",
    "\n",
    "3. **Context Managers**\n",
    "   - \u2705 Resource management (files, connections)\n",
    "   - \u2705 Transaction handling\n",
    "   - \u2705 Setup/teardown operations\n",
    "\n",
    "4. **Magic Methods**\n",
    "   - \u2705 Custom data structures\n",
    "   - \u2705 Making objects behave like built-in types\n",
    "   - \u2705 Operator overloading\n",
    "\n",
    "5. **Meta-programming**\n",
    "   - \u2705 Frameworks and libraries\n",
    "   - \u2705 Dynamic code generation\n",
    "   - \u26a0\ufe0f Use sparingly - can reduce code readability\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Practice Exercises\n",
    "\n",
    "### Exercise 1: Create a retry decorator\n",
    "Write a decorator that retries a function up to N times if it raises an exception.\n",
    "\n",
    "### Exercise 2: Build a data pipeline generator\n",
    "Create a generator that simulates an ETL pipeline processing records one at a time.\n",
    "\n",
    "### Exercise 3: Custom context manager for logging\n",
    "Build a context manager that logs all function calls within its scope.\n",
    "\n",
    "### Exercise 4: Matrix class with operators\n",
    "Implement a Matrix class with +, -, *, and @ (matrix multiplication) operators.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- \u2705 Complete exercises above\n",
    "- \u2192 Continue to **003_Python_Concurrency_Parallelism.ipynb**\n",
    "- \u2192 Advanced topic: **044_Model_Interpretability_SHAP_LIME.ipynb** (uses decorators)\n",
    "- \u2192 Application: **092_Apache_Spark_PySpark.ipynb** (uses generators extensively)\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Complete! \ud83d\ude80**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}