{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "501604af",
   "metadata": {},
   "source": [
    "# 003: SQL Fundamentals\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** SQL database concepts and relational data models\n",
    "- **Master** fundamental SQL queries (SELECT, WHERE, JOIN, GROUP BY)\n",
    "- **Implement** data filtering, aggregation, and sorting operations\n",
    "- **Apply** SQL to post-silicon test data analysis and reporting\n",
    "- **Build** efficient queries for semiconductor manufacturing analytics\n",
    "\n",
    "## üìö What is SQL?\n",
    "\n",
    "SQL (Structured Query Language) is the standard language for managing and querying relational databases. It enables data retrieval, manipulation, and analysis through declarative statements.\n",
    "\n",
    "**Why SQL?**\n",
    "- ‚úÖ Industry-standard for data management (90%+ of databases)\n",
    "- ‚úÖ Declarative syntax (what you want, not how to get it)\n",
    "- ‚úÖ Powerful for complex data relationships and aggregations\n",
    "- ‚úÖ Essential for post-silicon data analysis (STDF, yield reports)\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Parametric Test Analysis**\n",
    "- Input: STDF files with test results (voltage, current, frequency)\n",
    "- Output: Pass/fail statistics, outlier detection, trend analysis\n",
    "- Value: Identify failing devices, reduce test time by 20-30%\n",
    "\n",
    "**Wafer Map Analytics**\n",
    "- Input: Die coordinates (x, y), bin categories, test parameters\n",
    "- Output: Spatial patterns, cluster analysis, yield by region\n",
    "- Value: Detect process issues early, save $2-5M per wafer batch\n",
    "\n",
    "**Equipment Performance Monitoring**\n",
    "- Input: ATE tester logs, maintenance records, calibration data\n",
    "- Output: Uptime metrics, failure prediction, cost analysis\n",
    "- Value: Reduce downtime by 15%, extend equipment life 2-3 years\n",
    "\n",
    "**Yield Reporting & Forecasting**\n",
    "- Input: Historical test results, product mix, lot information\n",
    "- Output: Yield trends, Cp/Cpk statistics, prediction models\n",
    "- Value: Improve yield 3-5%, enable proactive process adjustments\n",
    "\n",
    "---\n",
    "\n",
    "Let's master SQL fundamentals! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e38b1",
   "metadata": {},
   "source": [
    "# 003: SQL Fundamentals for Data Engineering\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** SQL syntax and relational database concepts\n",
    "- **Master** data querying with SELECT, JOIN, WHERE, GROUP BY\n",
    "- **Implement** database design and normalization principles\n",
    "- **Apply** SQL to post-silicon test data analysis\n",
    "- **Build** efficient queries for large-scale data retrieval\n",
    "\n",
    "## üìö What is SQL?\n",
    "\n",
    "**SQL (Structured Query Language)** is the standard language for managing and manipulating relational databases. It provides a declarative way to query, insert, update, and delete data stored in tables with defined relationships.\n",
    "\n",
    "SQL is essential for data engineering and analytics because:\n",
    "- Databases store structured data efficiently\n",
    "- Joins enable complex data relationships\n",
    "- Aggregations provide powerful analytics\n",
    "- Indexing ensures fast retrieval at scale\n",
    "\n",
    "**Why SQL?**\n",
    "- ‚úÖ Industry standard for data management (40+ years)\n",
    "- ‚úÖ Works with all major databases (MySQL, PostgreSQL, Oracle, SQL Server)\n",
    "- ‚úÖ Declarative syntax (say what you want, not how to get it)\n",
    "- ‚úÖ Optimized execution plans (database engine handles optimization)\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Test Data Warehouse Queries**\n",
    "- Input: STDF test results stored in relational tables\n",
    "- Output: Aggregated yield, parametric statistics by lot/wafer/die\n",
    "- Value: Enable fast analytics on billions of test records\n",
    "\n",
    "**Multi-Site Test Correlation**\n",
    "- Input: Test results from multiple ATE sites\n",
    "- Output: Correlation analysis identifying equipment drift\n",
    "- Value: Early detection of site-specific issues ($2-5M savings)\n",
    "\n",
    "**Parametric Trend Analysis**\n",
    "- Input: Time-series test parameter measurements\n",
    "- Output: Trending reports with statistical process control\n",
    "- Value: Proactive yield management and process optimization\n",
    "\n",
    "**Failure Mode Classification**\n",
    "- Input: Test failure patterns across wafers and lots\n",
    "- Output: Grouped failure modes for root cause analysis\n",
    "- Value: Faster debug cycles (40-60% reduction in FA time)\n",
    "\n",
    "## üîÑ SQL Query Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Raw Data Tables] --> B[SELECT with WHERE]\n",
    "    B --> C[JOIN Multiple Tables]\n",
    "    C --> D[GROUP BY & Aggregate]\n",
    "    D --> E[ORDER BY & LIMIT]\n",
    "    E --> F[Results]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style F fill:#e1ffe1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- 001: DSA Python Mastery (data structures)\n",
    "- 002: Python Advanced Concepts (iteration, comprehensions)\n",
    "\n",
    "**Next Steps:**\n",
    "- 004: Statistics Fundamentals (for data analysis)\n",
    "- 091: Apache Spark (distributed SQL at scale)\n",
    "\n",
    "---\n",
    "\n",
    "Let's master SQL for data engineering! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ce68ef",
   "metadata": {},
   "source": [
    "## üìê Part 1: Database Basics & Table Creation\n",
    "\n",
    "**Relational databases** organize data into tables (relations) with rows (records) and columns (attributes):\n",
    "- **Table**: Collection of related data (e.g., `test_results`, `devices`, `wafers`)\n",
    "- **Row**: Single record (e.g., one test result for one device)\n",
    "- **Column**: Attribute (e.g., `test_name`, `test_value`, `pass_fail`)\n",
    "- **Primary Key**: Unique identifier for each row (e.g., `device_id`)\n",
    "- **Foreign Key**: Reference to primary key in another table (e.g., `wafer_id` ‚Üí `wafers.wafer_id`)\n",
    "\n",
    "**SQLite basics:**\n",
    "```python\n",
    "import sqlite3\n",
    "\n",
    "# Create in-memory database\n",
    "conn = sqlite3.connect(':memory:')  # Or 'database.db' for file\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table\n",
    "cursor.execute('''\n",
    "    CREATE TABLE test_results (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        device_id VARCHAR(50),\n",
    "        test_name VARCHAR(100),\n",
    "        test_value REAL,\n",
    "        pass_fail VARCHAR(10),\n",
    "        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "    )\n",
    "''')\n",
    "conn.commit()\n",
    "```\n",
    "\n",
    "**Data types:**\n",
    "- `INTEGER`: Whole numbers (device counts, bin numbers)\n",
    "- `REAL`: Floating point (voltage, current, frequency)\n",
    "- `TEXT/VARCHAR`: Strings (test names, device IDs)\n",
    "- `DATETIME`: Timestamps (test execution time)\n",
    "- `BOOLEAN`: True/False (pass/fail as 0/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c880a",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Create STDF-like test database and populate with synthetic semiconductor test data\n",
    "\n",
    "**Key Points:**\n",
    "- **SQLite in-memory**: Fast testing, no file persistence (use file for production)\n",
    "- **Synthetic data**: 1000 devices √ó 10 tests = 10,000 test records\n",
    "- **Realistic parameters**: Vdd (voltage), Idd (current), Freq (frequency), Leakage, Power\n",
    "- **Pass/Fail logic**: Devices fail if any test outside spec limits\n",
    "- **Timestamp**: Simulate test execution chronology\n",
    "\n",
    "**Why This Matters:**\n",
    "- Real STDF files have millions of records (50M+ for AMD)\n",
    "- SQL handles large datasets 10√ó faster than pandas (indexed queries)\n",
    "- Enables complex analytics (JOINs, aggregations, window functions)\n",
    "\n",
    "**Post-silicon context:**\n",
    "- AMD: 50M test records in PostgreSQL, <100ms query time\n",
    "- NVIDIA: Wafer database with spatial indexes for die location queries\n",
    "- Qualcomm: Multi-site databases with site_id foreign keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92f3732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Create STDF Test Database\n",
    "\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create in-memory database\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables\n",
    "print(\"=\" * 60)\n",
    "print(\"Creating STDF Test Database Schema\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Table 1: Devices\n",
    "cursor.execute('''\n",
    "    CREATE TABLE devices (\n",
    "        device_id VARCHAR(50) PRIMARY KEY,\n",
    "        wafer_id VARCHAR(50),\n",
    "        die_x INTEGER,\n",
    "        die_y INTEGER,\n",
    "        test_date DATETIME,\n",
    "        final_bin INTEGER\n",
    "    )\n",
    "''')\n",
    "\n",
    "# Table 2: Test Results\n",
    "cursor.execute('''\n",
    "    CREATE TABLE test_results (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        device_id VARCHAR(50),\n",
    "        test_name VARCHAR(100),\n",
    "        test_value REAL,\n",
    "        lower_limit REAL,\n",
    "        upper_limit REAL,\n",
    "        pass_fail VARCHAR(10),\n",
    "        test_time_ms REAL,\n",
    "        FOREIGN KEY (device_id) REFERENCES devices(device_id)\n",
    "    )\n",
    "''')\n",
    "\n",
    "print(\"‚úÖ Tables created: devices, test_results\")\n",
    "\n",
    "# Generate synthetic data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Generating Synthetic STDF Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_devices = 1000\n",
    "n_tests = 10\n",
    "\n",
    "# Test specifications\n",
    "test_specs = {\n",
    "    'Vdd_1.8V': (1.71, 1.89, 1.8),\n",
    "    'Idd_Active': (80, 120, 100),\n",
    "    'Freq_Max': (1900, 2100, 2000),\n",
    "    'Leakage_Cold': (0, 50, 10),\n",
    "    'Leakage_Hot': (0, 100, 30),\n",
    "    'Power_Active': (150, 250, 200),\n",
    "    'Power_Sleep': (0, 5, 1),\n",
    "    'Setup_Time': (0.8, 1.2, 1.0),\n",
    "    'Hold_Time': (0.9, 1.1, 1.0),\n",
    "    'Rise_Time': (0.4, 0.6, 0.5)\n",
    "}\n",
    "\n",
    "# Insert devices\n",
    "base_date = datetime(2024, 1, 1)\n",
    "devices_data = []\n",
    "for i in range(n_devices):\n",
    "    device_id = f\"DEV{i:05d}\"\n",
    "    wafer_id = f\"W{(i//100):03d}\"\n",
    "    die_x = i % 20\n",
    "    die_y = (i // 20) % 10\n",
    "    test_date = base_date + timedelta(minutes=i*5)\n",
    "    devices_data.append((device_id, wafer_id, die_x, die_y, test_date.isoformat(), 0))\n",
    "\n",
    "cursor.executemany('INSERT INTO devices VALUES (?, ?, ?, ?, ?, ?)', devices_data)\n",
    "\n",
    "# Insert test results\n",
    "test_data = []\n",
    "for device_id, *_ in devices_data:\n",
    "    for test_name, (lower, upper, nominal) in test_specs.items():\n",
    "        # Generate test value (95% pass, 5% fail)\n",
    "        if np.random.rand() < 0.95:\n",
    "            test_value = np.random.normal(nominal, (upper-lower)/6)\n",
    "        else:\n",
    "            # Intentional failure\n",
    "            test_value = np.random.choice([lower - 0.1, upper + 0.1])\n",
    "        \n",
    "        pass_fail = 'PASS' if lower <= test_value <= upper else 'FAIL'\n",
    "        test_time_ms = np.random.uniform(5, 20)\n",
    "        \n",
    "        test_data.append((device_id, test_name, test_value, lower, upper, pass_fail, test_time_ms))\n",
    "\n",
    "cursor.executemany('''\n",
    "    INSERT INTO test_results (device_id, test_name, test_value, lower_limit, upper_limit, pass_fail, test_time_ms)\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "''', test_data)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "print(f\"‚úÖ Inserted {n_devices} devices\")\n",
    "print(f\"‚úÖ Inserted {len(test_data)} test results ({n_devices} √ó {n_tests} tests)\")\n",
    "\n",
    "# Verify data\n",
    "cursor.execute('SELECT COUNT(*) FROM devices')\n",
    "device_count = cursor.fetchone()[0]\n",
    "\n",
    "cursor.execute('SELECT COUNT(*) FROM test_results')\n",
    "result_count = cursor.fetchone()[0]\n",
    "\n",
    "cursor.execute(\"SELECT COUNT(*) FROM test_results WHERE pass_fail = 'FAIL'\")\n",
    "fail_count = cursor.fetchone()[0]\n",
    "\n",
    "print(f\"\\nüìä Database Summary:\")\n",
    "print(f\"   Devices: {device_count}\")\n",
    "print(f\"   Test Results: {result_count}\")\n",
    "print(f\"   Failures: {fail_count} ({fail_count/result_count*100:.1f}%)\")\n",
    "print(f\"   Overall Yield: {100 - fail_count/result_count*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9f17cb",
   "metadata": {},
   "source": [
    "## üìê Part 2: SELECT Queries - Data Retrieval\n",
    "\n",
    "SQL's `SELECT` statement retrieves data from tables. It's the foundation of all database queries.\n",
    "\n",
    "**Basic SELECT Syntax:**\n",
    "```sql\n",
    "SELECT column1, column2, ...\n",
    "FROM table_name\n",
    "WHERE condition\n",
    "ORDER BY column1 [ASC|DESC]\n",
    "LIMIT n;\n",
    "```\n",
    "\n",
    "**Key Clauses:**\n",
    "- **SELECT**: Specifies which columns to return (`*` = all columns)\n",
    "- **WHERE**: Filters rows based on conditions (e.g., `test_value > 100`)\n",
    "- **DISTINCT**: Returns only unique values\n",
    "- **ORDER BY**: Sorts results (ASC = ascending, DESC = descending)\n",
    "- **LIMIT**: Restricts number of rows returned\n",
    "\n",
    "**Post-Silicon Use Cases:**\n",
    "- **Qualcomm**: Query 5M test results for specific parameter violations in <500ms\n",
    "- **AMD**: Filter wafer W012 devices with Vdd failures ‚Üí 45 devices in 20ms\n",
    "- **NVIDIA**: Sort devices by frequency to identify top 10 performers\n",
    "- **Intel**: Find distinct test names across 200 wafer lots\n",
    "\n",
    "**SELECT vs Pandas:**\n",
    "- SQL: Optimized for filtering, indexing, and aggregation on large data\n",
    "- Pandas: Better for complex transformations and in-memory analysis\n",
    "- Rule: Use SQL for filtering, pandas for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b144ea",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Query test database with SELECT statements to filter, sort, and analyze STDF data.\n",
    "\n",
    "**Key Points:**\n",
    "- **WHERE clause**: Filters rows before returning results (e.g., find all voltage failures)\n",
    "- **ORDER BY**: Sorts results by column values (critical for identifying worst performers)\n",
    "- **LIMIT**: Returns only top N results (useful for \"top 10 failures\" queries)\n",
    "- **DISTINCT**: Removes duplicates (essential for finding unique test names or wafer IDs)\n",
    "\n",
    "**Why This Matters:**\n",
    "- AMD scenario: 50M test records ‚Üí filter Vdd failures ‚Üí 450 devices in 85ms (vs 8 sec pandas)\n",
    "- NVIDIA use case: Sort 1M devices by frequency ‚Üí identify top 100 performers ‚Üí 120ms query\n",
    "- Production value: Real-time test floor dashboards need <100ms query response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd1f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: SELECT Queries\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Part 2: SELECT Queries - Data Retrieval\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Query 1: Select all columns from test_results (limit 5)\n",
    "print(\"\\n1Ô∏è‚É£ SELECT * (All Columns) - First 5 Results:\")\n",
    "cursor.execute('SELECT * FROM test_results LIMIT 5')\n",
    "results = cursor.fetchall()\n",
    "for row in results:\n",
    "    print(f\"   {row}\")\n",
    "\n",
    "# Query 2: Select specific columns\n",
    "print(\"\\n2Ô∏è‚É£ SELECT Specific Columns (device_id, test_name, test_value):\")\n",
    "cursor.execute('SELECT device_id, test_name, test_value FROM test_results LIMIT 5')\n",
    "results = cursor.fetchall()\n",
    "for row in results:\n",
    "    print(f\"   Device: {row[0]}, Test: {row[1]}, Value: {row[2]:.2f}\")\n",
    "\n",
    "# Query 3: WHERE clause - Filter failures\n",
    "print(\"\\n3Ô∏è‚É£ WHERE Clause - Filter FAIL Results:\")\n",
    "cursor.execute(\"SELECT device_id, test_name, test_value FROM test_results WHERE pass_fail = 'FAIL' LIMIT 10\")\n",
    "results = cursor.fetchall()\n",
    "print(f\"   Found {len(results)} failures (showing first 10):\")\n",
    "for row in results:\n",
    "    print(f\"   ‚ùå Device: {row[0]}, Test: {row[1]}, Value: {row[2]:.2f}\")\n",
    "\n",
    "# Query 4: WHERE with numeric conditions\n",
    "print(\"\\n4Ô∏è‚É£ WHERE with Numeric Conditions - High Leakage:\")\n",
    "cursor.execute('''\n",
    "    SELECT device_id, test_name, test_value, upper_limit\n",
    "    FROM test_results\n",
    "    WHERE test_name LIKE '%Leakage%' AND test_value > 80\n",
    "    LIMIT 10\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "print(f\"   Devices with leakage > 80 ¬µA:\")\n",
    "for row in results:\n",
    "    print(f\"   ‚ö†Ô∏è  Device: {row[0]}, {row[1]}: {row[2]:.2f} ¬µA (limit: {row[3]})\")\n",
    "\n",
    "# Query 5: ORDER BY - Sort by test value\n",
    "print(\"\\n5Ô∏è‚É£ ORDER BY - Top 10 Highest Frequency Devices:\")\n",
    "cursor.execute('''\n",
    "    SELECT device_id, test_value\n",
    "    FROM test_results\n",
    "    WHERE test_name = 'Freq_Max'\n",
    "    ORDER BY test_value DESC\n",
    "    LIMIT 10\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "for i, row in enumerate(results, 1):\n",
    "    print(f\"   #{i} Device: {row[0]}, Frequency: {row[1]:.2f} MHz\")\n",
    "\n",
    "# Query 6: DISTINCT - Unique test names\n",
    "print(\"\\n6Ô∏è‚É£ DISTINCT - Unique Test Names:\")\n",
    "cursor.execute('SELECT DISTINCT test_name FROM test_results ORDER BY test_name')\n",
    "results = cursor.fetchall()\n",
    "for row in results:\n",
    "    print(f\"   ‚Ä¢ {row[0]}\")\n",
    "\n",
    "# Query 7: Multiple conditions with AND/OR\n",
    "print(\"\\n7Ô∏è‚É£ Complex WHERE - Vdd Failures on Wafer W005:\")\n",
    "cursor.execute('''\n",
    "    SELECT d.device_id, d.wafer_id, t.test_value, t.upper_limit\n",
    "    FROM test_results t\n",
    "    JOIN devices d ON t.device_id = d.device_id\n",
    "    WHERE d.wafer_id = 'W005' \n",
    "      AND t.test_name = 'Vdd_1.8V' \n",
    "      AND t.pass_fail = 'FAIL'\n",
    "    ORDER BY t.test_value DESC\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "print(f\"   Found {len(results)} Vdd failures on wafer W005:\")\n",
    "for row in results[:5]:  # Show first 5\n",
    "    print(f\"   ‚ùå Device: {row[0]}, Vdd: {row[2]:.3f}V (limit: {row[3]:.3f}V)\")\n",
    "\n",
    "print(\"\\n‚úÖ SELECT queries complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568faded",
   "metadata": {},
   "source": [
    "## üìê Part 3: Aggregations - Statistical Analysis\n",
    "\n",
    "SQL aggregation functions summarize data: COUNT, SUM, AVG, MIN, MAX. Combined with `GROUP BY`, they enable powerful analytics.\n",
    "\n",
    "**Aggregation Syntax:**\n",
    "```sql\n",
    "SELECT column1, AGG_FUNC(column2)\n",
    "FROM table_name\n",
    "WHERE condition\n",
    "GROUP BY column1\n",
    "HAVING AGG_FUNC(column2) > threshold\n",
    "ORDER BY AGG_FUNC(column2) DESC;\n",
    "```\n",
    "\n",
    "**Key Functions:**\n",
    "- **COUNT(*)**: Counts rows (e.g., \"How many test failures per device?\")\n",
    "- **AVG(column)**: Average value (e.g., \"Mean Vdd per wafer\")\n",
    "- **SUM(column)**: Total sum (e.g., \"Total test time per device\")\n",
    "- **MIN/MAX(column)**: Min/max value (e.g., \"Best/worst frequency\")\n",
    "- **GROUP BY**: Groups rows with same values (e.g., by device_id or wafer_id)\n",
    "- **HAVING**: Filters groups (e.g., \"wafers with yield < 90%\")\n",
    "\n",
    "**GROUP BY vs WHERE:**\n",
    "- **WHERE**: Filters individual rows BEFORE aggregation\n",
    "- **HAVING**: Filters groups AFTER aggregation\n",
    "- Example: `WHERE pass_fail = 'FAIL' GROUP BY device_id HAVING COUNT(*) > 3` ‚Üí devices with >3 failures\n",
    "\n",
    "**Post-Silicon Analytics:**\n",
    "- **Qualcomm**: Aggregate 50M tests ‚Üí yield by wafer ‚Üí identify 12 low-yield wafers in <200ms\n",
    "- **AMD**: AVG(test_time_ms) by test_name ‚Üí identify slow tests ‚Üí reduce test time 15%\n",
    "- **NVIDIA**: COUNT failures by die position ‚Üí spatial correlation ‚Üí flag edge dies\n",
    "- **Intel**: MIN/MAX Vdd per lot ‚Üí process drift detection ‚Üí 0.05V variance alert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e635876",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Use aggregation functions to compute yield statistics, failure counts, and test time analytics.\n",
    "\n",
    "**Key Points:**\n",
    "- **COUNT(*) with GROUP BY**: Counts occurrences per group (e.g., failures per wafer)\n",
    "- **AVG() with HAVING**: Filters groups based on average values (e.g., wafers with low yield)\n",
    "- **MIN/MAX**: Identifies outliers (e.g., devices with extreme voltage/frequency)\n",
    "- **Multiple aggregations**: Compute multiple metrics in one query (yield%, avg test time, failure count)\n",
    "\n",
    "**Why This Matters:**\n",
    "- AMD scenario: 10 wafers √ó 1000 devices ‚Üí yield by wafer ‚Üí identify 2 bad wafers ‚Üí scrape before packaging ($500K saved)\n",
    "- NVIDIA use case: Test time analytics ‚Üí slow tests consume 60% of time ‚Üí optimize 3 tests ‚Üí 25% faster test flow\n",
    "- Production impact: Real-time yield dashboards updated every 5 minutes using aggregation queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86b2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Aggregations\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Part 3: Aggregations - Statistical Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Query 1: COUNT - Total test results\n",
    "print(\"\\n1Ô∏è‚É£ COUNT(*) - Total Records:\")\n",
    "cursor.execute('SELECT COUNT(*) FROM test_results')\n",
    "total = cursor.fetchone()[0]\n",
    "print(f\"   Total test results: {total:,}\")\n",
    "\n",
    "# Query 2: COUNT with GROUP BY - Failures per test\n",
    "print(\"\\n2Ô∏è‚É£ COUNT with GROUP BY - Failures per Test:\")\n",
    "cursor.execute('''\n",
    "    SELECT test_name, COUNT(*) as failure_count\n",
    "    FROM test_results\n",
    "    WHERE pass_fail = 'FAIL'\n",
    "    GROUP BY test_name\n",
    "    ORDER BY failure_count DESC\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "for row in results:\n",
    "    print(f\"   {row[0]:<20} {row[1]:>4} failures\")\n",
    "\n",
    "# Query 3: AVG - Average test values by test\n",
    "print(\"\\n3Ô∏è‚É£ AVG - Average Test Values:\")\n",
    "cursor.execute('''\n",
    "    SELECT test_name, \n",
    "           AVG(test_value) as avg_value,\n",
    "           AVG(lower_limit) as lower,\n",
    "           AVG(upper_limit) as upper\n",
    "    FROM test_results\n",
    "    GROUP BY test_name\n",
    "    ORDER BY test_name\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "for row in results:\n",
    "    print(f\"   {row[0]:<20} Avg: {row[1]:>8.2f} (limits: {row[2]:.2f} - {row[3]:.2f})\")\n",
    "\n",
    "# Query 4: Yield by wafer (GROUP BY with calculation)\n",
    "print(\"\\n4Ô∏è‚É£ Yield Calculation - Per Wafer:\")\n",
    "cursor.execute('''\n",
    "    SELECT d.wafer_id,\n",
    "           COUNT(*) as total_tests,\n",
    "           SUM(CASE WHEN t.pass_fail = 'PASS' THEN 1 ELSE 0 END) as passes,\n",
    "           SUM(CASE WHEN t.pass_fail = 'FAIL' THEN 1 ELSE 0 END) as failures,\n",
    "           ROUND(100.0 * SUM(CASE WHEN t.pass_fail = 'PASS' THEN 1 ELSE 0 END) / COUNT(*), 2) as yield_pct\n",
    "    FROM test_results t\n",
    "    JOIN devices d ON t.device_id = d.device_id\n",
    "    GROUP BY d.wafer_id\n",
    "    ORDER BY yield_pct ASC\n",
    "    LIMIT 10\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "print(f\"   {'Wafer':<10} {'Total':<8} {'Pass':<8} {'Fail':<8} {'Yield%':<8}\")\n",
    "print(f\"   {'-'*50}\")\n",
    "for row in results:\n",
    "    print(f\"   {row[0]:<10} {row[1]:<8} {row[2]:<8} {row[3]:<8} {row[4]:<8}\")\n",
    "\n",
    "# Query 5: HAVING - Wafers with low yield\n",
    "print(\"\\n5Ô∏è‚É£ HAVING Clause - Low Yield Wafers (<94%):\")\n",
    "cursor.execute('''\n",
    "    SELECT d.wafer_id,\n",
    "           COUNT(*) as total_tests,\n",
    "           ROUND(100.0 * SUM(CASE WHEN t.pass_fail = 'PASS' THEN 1 ELSE 0 END) / COUNT(*), 2) as yield_pct\n",
    "    FROM test_results t\n",
    "    JOIN devices d ON t.device_id = d.device_id\n",
    "    GROUP BY d.wafer_id\n",
    "    HAVING yield_pct < 94\n",
    "    ORDER BY yield_pct ASC\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "print(f\"   Found {len(results)} low-yield wafers:\")\n",
    "for row in results:\n",
    "    print(f\"   ‚ö†Ô∏è  Wafer {row[0]}: {row[1]} tests, {row[2]}% yield\")\n",
    "\n",
    "# Query 6: MIN/MAX - Outlier detection\n",
    "print(\"\\n6Ô∏è‚É£ MIN/MAX - Frequency Outliers:\")\n",
    "cursor.execute('''\n",
    "    SELECT \n",
    "        test_name,\n",
    "        MIN(test_value) as min_freq,\n",
    "        AVG(test_value) as avg_freq,\n",
    "        MAX(test_value) as max_freq,\n",
    "        MAX(test_value) - MIN(test_value) as range_freq\n",
    "    FROM test_results\n",
    "    WHERE test_name = 'Freq_Max'\n",
    "    GROUP BY test_name\n",
    "''')\n",
    "row = cursor.fetchone()\n",
    "print(f\"   Test: {row[0]}\")\n",
    "print(f\"   Min:   {row[1]:.2f} MHz\")\n",
    "print(f\"   Avg:   {row[2]:.2f} MHz\")\n",
    "print(f\"   Max:   {row[3]:.2f} MHz\")\n",
    "print(f\"   Range: {row[4]:.2f} MHz\")\n",
    "\n",
    "# Query 7: Test time analytics\n",
    "print(\"\\n7Ô∏è‚É£ Test Time Analytics - Slowest Tests:\")\n",
    "cursor.execute('''\n",
    "    SELECT test_name,\n",
    "           COUNT(*) as num_tests,\n",
    "           AVG(test_time_ms) as avg_time_ms,\n",
    "           SUM(test_time_ms) as total_time_ms\n",
    "    FROM test_results\n",
    "    GROUP BY test_name\n",
    "    ORDER BY total_time_ms DESC\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "for row in results[:5]:\n",
    "    print(f\"   {row[0]:<20} Avg: {row[2]:>6.2f}ms, Total: {row[3]:>8,.0f}ms ({row[1]:>5} tests)\")\n",
    "\n",
    "print(\"\\n‚úÖ Aggregation queries complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38673a49",
   "metadata": {},
   "source": [
    "## üìê Part 4: JOINs - Combining Multiple Tables\n",
    "\n",
    "JOINs combine rows from multiple tables based on related columns. Essential for relational analytics.\n",
    "\n",
    "**JOIN Types:**\n",
    "\n",
    "```sql\n",
    "-- INNER JOIN: Only matching rows from both tables\n",
    "SELECT * FROM devices d\n",
    "INNER JOIN test_results t ON d.device_id = t.device_id;\n",
    "\n",
    "-- LEFT JOIN: All rows from left table + matching rows from right\n",
    "SELECT * FROM devices d\n",
    "LEFT JOIN test_results t ON d.device_id = t.device_id;\n",
    "\n",
    "-- RIGHT JOIN: All rows from right table + matching rows from left\n",
    "SELECT * FROM test_results t\n",
    "RIGHT JOIN devices d ON t.device_id = d.device_id;\n",
    "\n",
    "-- FULL OUTER JOIN: All rows from both tables\n",
    "SELECT * FROM devices d\n",
    "FULL OUTER JOIN test_results t ON d.device_id = t.device_id;\n",
    "\n",
    "-- CROSS JOIN: Cartesian product (all combinations)\n",
    "SELECT * FROM devices, test_results;  -- Use with caution!\n",
    "```\n",
    "\n",
    "**When to Use Each JOIN:**\n",
    "- **INNER JOIN**: Default choice (only devices with test results)\n",
    "- **LEFT JOIN**: Keep all devices even if no test results (data quality check)\n",
    "- **RIGHT JOIN**: Keep all test results even if device info missing (rare)\n",
    "- **FULL OUTER JOIN**: Find mismatches (devices without tests OR tests without devices)\n",
    "- **CROSS JOIN**: Generate combinations (e.g., all devices √ó all test types for planning)\n",
    "\n",
    "**Post-Silicon Multi-Table Analytics:**\n",
    "- **AMD**: JOIN devices + test_results + wafer_lots ‚Üí 3-table analysis ‚Üí yield by lot + spatial ‚Üí 150ms\n",
    "- **NVIDIA**: LEFT JOIN devices + test_results ‚Üí find devices with missing tests ‚Üí data quality audit\n",
    "- **Qualcomm**: JOIN test_results + test_limits + specifications ‚Üí dynamic limit checking\n",
    "- **Intel**: INNER JOIN devices + test_results + retest_history ‚Üí failure rate trending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f6daa7",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Demonstrate INNER JOIN and LEFT JOIN to combine devices and test_results for multi-table analytics.\n",
    "\n",
    "**Key Points:**\n",
    "- **INNER JOIN**: Returns only devices that have test results (most common use case)\n",
    "- **LEFT JOIN**: Returns all devices even if they have no test results (data quality checks)\n",
    "- **Aliasing (d, t)**: Shorthand for table names to simplify queries\n",
    "- **Multi-condition JOINs**: Can join on multiple columns (e.g., device_id AND test_date)\n",
    "\n",
    "**Why This Matters:**\n",
    "- AMD scenario: JOIN devices + test_results ‚Üí spatial analysis ‚Üí yield by die position ‚Üí edge vs center comparison\n",
    "- NVIDIA use case: LEFT JOIN devices + test_results ‚Üí find 45 devices with missing tests ‚Üí data quality issue\n",
    "- Production impact: JOINs enable multi-dimensional analytics (spatial + parametric + temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfef31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4: JOINs\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Part 4: JOINs - Combining Multiple Tables\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Query 1: INNER JOIN - Devices with Vdd failures\n",
    "print(\"\\n1Ô∏è‚É£ INNER JOIN - Devices with Vdd Failures (wafer info):\")\n",
    "cursor.execute('''\n",
    "    SELECT d.device_id, d.wafer_id, d.die_x, d.die_y, \n",
    "           t.test_value, t.lower_limit, t.upper_limit\n",
    "    FROM devices d\n",
    "    INNER JOIN test_results t ON d.device_id = t.device_id\n",
    "    WHERE t.test_name = 'Vdd_1.8V' AND t.pass_fail = 'FAIL'\n",
    "    ORDER BY d.wafer_id, d.die_x, d.die_y\n",
    "    LIMIT 10\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "print(f\"   {'Device':<10} {'Wafer':<8} {'Die(x,y)':<12} {'Vdd':<8} {'Limits':<15}\")\n",
    "print(f\"   {'-'*60}\")\n",
    "for row in results:\n",
    "    print(f\"   {row[0]:<10} {row[1]:<8} ({row[2]:>2},{row[3]:>2})      {row[4]:.3f}V  {row[5]:.3f}-{row[6]:.3f}V\")\n",
    "\n",
    "# Query 2: INNER JOIN with aggregation - Failure count per wafer\n",
    "print(\"\\n2Ô∏è‚É£ INNER JOIN + Aggregation - Failure Count per Wafer:\")\n",
    "cursor.execute('''\n",
    "    SELECT d.wafer_id,\n",
    "           COUNT(DISTINCT d.device_id) as total_devices,\n",
    "           COUNT(CASE WHEN t.pass_fail = 'FAIL' THEN 1 END) as total_failures,\n",
    "           ROUND(100.0 * COUNT(CASE WHEN t.pass_fail = 'FAIL' THEN 1 END) / COUNT(*), 2) as failure_rate\n",
    "    FROM devices d\n",
    "    INNER JOIN test_results t ON d.device_id = t.device_id\n",
    "    GROUP BY d.wafer_id\n",
    "    ORDER BY failure_rate DESC\n",
    "    LIMIT 10\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "for row in results:\n",
    "    print(f\"   Wafer {row[0]}: {row[1]} devices, {row[2]} failures ({row[3]}%)\")\n",
    "\n",
    "# Query 3: INNER JOIN - Spatial analysis (edge vs center dies)\n",
    "print(\"\\n3Ô∏è‚É£ Spatial Analysis - Edge vs Center Dies:\")\n",
    "cursor.execute('''\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN d.die_x IN (0, 19) OR d.die_y IN (0, 9) THEN 'Edge'\n",
    "            ELSE 'Center'\n",
    "        END as position,\n",
    "        COUNT(DISTINCT d.device_id) as num_devices,\n",
    "        ROUND(100.0 * SUM(CASE WHEN t.pass_fail = 'PASS' THEN 1 ELSE 0 END) / COUNT(*), 2) as yield_pct\n",
    "    FROM devices d\n",
    "    INNER JOIN test_results t ON d.device_id = t.device_id\n",
    "    GROUP BY position\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "for row in results:\n",
    "    print(f\"   {row[0]:<8} dies: {row[1]:>4} devices, {row[2]:>6}% yield\")\n",
    "\n",
    "# Query 4: Multi-table JOIN - Device with worst test time\n",
    "print(\"\\n4Ô∏è‚É£ Multi-Column Analysis - Devices with Longest Total Test Time:\")\n",
    "cursor.execute('''\n",
    "    SELECT d.device_id, d.wafer_id,\n",
    "           COUNT(*) as num_tests,\n",
    "           SUM(t.test_time_ms) as total_time_ms,\n",
    "           AVG(t.test_time_ms) as avg_time_ms\n",
    "    FROM devices d\n",
    "    INNER JOIN test_results t ON d.device_id = t.device_id\n",
    "    GROUP BY d.device_id, d.wafer_id\n",
    "    ORDER BY total_time_ms DESC\n",
    "    LIMIT 10\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "for row in results:\n",
    "    print(f\"   Device {row[0]} (Wafer {row[1]}): {row[2]} tests, {row[3]:.2f}ms total ({row[4]:.2f}ms avg)\")\n",
    "\n",
    "# Query 5: LEFT JOIN - Data quality check (devices with missing tests)\n",
    "print(\"\\n5Ô∏è‚É£ LEFT JOIN - Data Quality Check:\")\n",
    "cursor.execute('''\n",
    "    SELECT d.device_id, d.wafer_id,\n",
    "           COUNT(t.id) as test_count\n",
    "    FROM devices d\n",
    "    LEFT JOIN test_results t ON d.device_id = t.device_id\n",
    "    GROUP BY d.device_id, d.wafer_id\n",
    "    HAVING test_count < 10\n",
    "    ORDER BY test_count ASC\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "if len(results) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Found {len(results)} devices with incomplete tests:\")\n",
    "    for row in results[:5]:\n",
    "        print(f\"   Device {row[0]}: Only {row[2]} tests (expected 10)\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ All devices have complete test results (10 tests each)\")\n",
    "\n",
    "print(\"\\n‚úÖ JOIN queries complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016777f6",
   "metadata": {},
   "source": [
    "## üìê Part 5: Subqueries & CTEs - Advanced Queries\n",
    "\n",
    "**Subqueries** (nested queries) allow queries inside other queries. **CTEs** (Common Table Expressions) create named temporary result sets for readability.\n",
    "\n",
    "**Subquery Types:**\n",
    "\n",
    "```sql\n",
    "-- Scalar subquery (returns single value)\n",
    "SELECT device_id, test_value\n",
    "FROM test_results\n",
    "WHERE test_value > (SELECT AVG(test_value) FROM test_results WHERE test_name = 'Freq_Max');\n",
    "\n",
    "-- IN subquery (returns multiple values)\n",
    "SELECT * FROM devices\n",
    "WHERE device_id IN (SELECT device_id FROM test_results WHERE pass_fail = 'FAIL');\n",
    "\n",
    "-- Correlated subquery (references outer query)\n",
    "SELECT device_id, \n",
    "       (SELECT COUNT(*) FROM test_results t WHERE t.device_id = d.device_id AND pass_fail = 'FAIL') as failures\n",
    "FROM devices d;\n",
    "```\n",
    "\n",
    "**CTE (WITH Clause):**\n",
    "\n",
    "```sql\n",
    "WITH failing_devices AS (\n",
    "    SELECT device_id, COUNT(*) as fail_count\n",
    "    FROM test_results\n",
    "    WHERE pass_fail = 'FAIL'\n",
    "    GROUP BY device_id\n",
    ")\n",
    "SELECT * FROM failing_devices WHERE fail_count > 2;\n",
    "```\n",
    "\n",
    "**CTE Advantages:**\n",
    "- ‚úÖ **Readability**: Break complex queries into logical steps\n",
    "- ‚úÖ **Reusability**: Reference CTE multiple times in same query\n",
    "- ‚úÖ **Debugging**: Test each CTE independently\n",
    "- ‚úÖ **Performance**: Database can optimize CTE execution\n",
    "\n",
    "**Subquery vs CTE:**\n",
    "- **Subquery**: Inline, harder to read for complex logic\n",
    "- **CTE**: Named, easier to understand and maintain\n",
    "- **Rule**: Use CTE for queries >20 lines or multiple references\n",
    "\n",
    "**Post-Silicon Use Cases:**\n",
    "- **AMD**: Subquery to find devices with test values 3œÉ above mean ‚Üí outlier detection\n",
    "- **NVIDIA**: CTE to compute yield by wafer, then JOIN to wafer metadata ‚Üí 2-step analysis\n",
    "- **Qualcomm**: Recursive CTE to find retest cascades (device fails ‚Üí retest ‚Üí fails again ‚Üí retest)\n",
    "- **Intel**: Multiple CTEs to build parametric limits from historical data ‚Üí dynamic limit calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b1c9b",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Use subqueries and CTEs for advanced analytics like outlier detection and multi-step analysis.\n",
    "\n",
    "**Key Points:**\n",
    "- **Scalar subquery**: Returns single value (e.g., AVG) used in WHERE comparison\n",
    "- **IN subquery**: Filters rows based on results from another query\n",
    "- **CTE (WITH clause)**: Creates named temporary table for readability and reusability\n",
    "- **Multiple CTEs**: Chain multiple CTEs for complex multi-step analysis\n",
    "\n",
    "**Why This Matters:**\n",
    "- AMD scenario: CTE to compute device-level stats ‚Üí then filter for outliers ‚Üí identify 23 anomalous devices in 95ms\n",
    "- NVIDIA use case: Subquery to find top 10% performers ‚Üí JOIN to spatial data ‚Üí correlation analysis\n",
    "- Production impact: CTEs enable modular query design ‚Üí easier debugging and maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5: Subqueries & CTEs\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Part 5: Subqueries & CTEs - Advanced Queries\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Query 1: Scalar subquery - Devices with above-average frequency\n",
    "print(\"\\n1Ô∏è‚É£ Scalar Subquery - Above-Average Frequency Devices:\")\n",
    "cursor.execute('''\n",
    "    SELECT device_id, test_value as frequency\n",
    "    FROM test_results\n",
    "    WHERE test_name = 'Freq_Max' \n",
    "      AND test_value > (SELECT AVG(test_value) FROM test_results WHERE test_name = 'Freq_Max')\n",
    "    ORDER BY test_value DESC\n",
    "    LIMIT 10\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "cursor.execute(\"SELECT AVG(test_value) FROM test_results WHERE test_name = 'Freq_Max'\")\n",
    "avg_freq = cursor.fetchone()[0]\n",
    "print(f\"   Average frequency: {avg_freq:.2f} MHz\")\n",
    "print(f\"   Devices above average:\")\n",
    "for row in results:\n",
    "    print(f\"   ‚Ä¢ {row[0]}: {row[1]:.2f} MHz (+{row[1] - avg_freq:.2f} MHz)\")\n",
    "\n",
    "# Query 2: IN subquery - Devices with any failures\n",
    "print(\"\\n2Ô∏è‚É£ IN Subquery - Devices with Failures:\")\n",
    "cursor.execute('''\n",
    "    SELECT device_id, wafer_id, die_x, die_y\n",
    "    FROM devices\n",
    "    WHERE device_id IN (\n",
    "        SELECT DISTINCT device_id \n",
    "        FROM test_results \n",
    "        WHERE pass_fail = 'FAIL'\n",
    "    )\n",
    "    LIMIT 10\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "print(f\"   Devices with at least one test failure:\")\n",
    "for row in results:\n",
    "    print(f\"   ‚ùå Device {row[0]} (Wafer {row[1]}, Die {row[2]},{row[3]})\")\n",
    "\n",
    "# Query 3: CTE - Multi-step analysis\n",
    "print(\"\\n3Ô∏è‚É£ CTE - Device Failure Statistics:\")\n",
    "cursor.execute('''\n",
    "    WITH device_stats AS (\n",
    "        SELECT device_id,\n",
    "               COUNT(*) as total_tests,\n",
    "               SUM(CASE WHEN pass_fail = 'FAIL' THEN 1 ELSE 0 END) as failures,\n",
    "               ROUND(100.0 * SUM(CASE WHEN pass_fail = 'FAIL' THEN 1 ELSE 0 END) / COUNT(*), 2) as failure_rate\n",
    "        FROM test_results\n",
    "        GROUP BY device_id\n",
    "    )\n",
    "    SELECT device_id, total_tests, failures, failure_rate\n",
    "    FROM device_stats\n",
    "    WHERE failure_rate > 10\n",
    "    ORDER BY failure_rate DESC\n",
    "    LIMIT 10\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "print(f\"   High-failure devices (>10% failure rate):\")\n",
    "for row in results:\n",
    "    print(f\"   ‚ö†Ô∏è  {row[0]}: {row[2]} failures / {row[1]} tests ({row[3]}%)\")\n",
    "\n",
    "# Query 4: Multiple CTEs - Wafer-level analytics\n",
    "print(\"\\n4Ô∏è‚É£ Multiple CTEs - Wafer Yield Analysis:\")\n",
    "cursor.execute('''\n",
    "    WITH wafer_stats AS (\n",
    "        SELECT d.wafer_id,\n",
    "               COUNT(DISTINCT d.device_id) as num_devices,\n",
    "               COUNT(*) as total_tests,\n",
    "               SUM(CASE WHEN t.pass_fail = 'PASS' THEN 1 ELSE 0 END) as passes\n",
    "        FROM devices d\n",
    "        INNER JOIN test_results t ON d.device_id = t.device_id\n",
    "        GROUP BY d.wafer_id\n",
    "    ),\n",
    "    wafer_yield AS (\n",
    "        SELECT wafer_id, num_devices, total_tests, passes,\n",
    "               ROUND(100.0 * passes / total_tests, 2) as yield_pct\n",
    "        FROM wafer_stats\n",
    "    )\n",
    "    SELECT wafer_id, num_devices, yield_pct,\n",
    "           CASE \n",
    "               WHEN yield_pct >= 95 THEN '‚úÖ Excellent'\n",
    "               WHEN yield_pct >= 90 THEN '‚úîÔ∏è  Good'\n",
    "               WHEN yield_pct >= 85 THEN '‚ö†Ô∏è  Marginal'\n",
    "               ELSE '‚ùå Poor'\n",
    "           END as quality\n",
    "    FROM wafer_yield\n",
    "    ORDER BY yield_pct DESC\n",
    "    LIMIT 10\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "print(f\"   {'Wafer':<10} {'Devices':<10} {'Yield%':<10} {'Quality':<15}\")\n",
    "print(f\"   {'-'*50}\")\n",
    "for row in results:\n",
    "    print(f\"   {row[0]:<10} {row[1]:<10} {row[2]:<10} {row[3]:<15}\")\n",
    "\n",
    "# Query 5: CTE for outlier detection\n",
    "print(\"\\n5Ô∏è‚É£ CTE - Statistical Outlier Detection (3œÉ rule):\")\n",
    "cursor.execute('''\n",
    "    WITH test_stats AS (\n",
    "        SELECT test_name,\n",
    "               AVG(test_value) as mean_value,\n",
    "               AVG(test_value) + 3 * (AVG(test_value * test_value) - AVG(test_value) * AVG(test_value)) as upper_3sigma,\n",
    "               AVG(test_value) - 3 * (AVG(test_value * test_value) - AVG(test_value) * AVG(test_value)) as lower_3sigma\n",
    "        FROM test_results\n",
    "        GROUP BY test_name\n",
    "    )\n",
    "    SELECT t.device_id, t.test_name, t.test_value, s.mean_value\n",
    "    FROM test_results t\n",
    "    INNER JOIN test_stats s ON t.test_name = s.test_name\n",
    "    WHERE t.test_value > s.upper_3sigma OR t.test_value < s.lower_3sigma\n",
    "    ORDER BY t.test_name, t.test_value DESC\n",
    "    LIMIT 15\n",
    "''')\n",
    "results = cursor.fetchall()\n",
    "print(f\"   Statistical outliers (>3œÉ from mean):\")\n",
    "for row in results:\n",
    "    deviation = abs(row[2] - row[3])\n",
    "    print(f\"   üìä Device {row[0]}, {row[1]}: {row[2]:.2f} (mean: {row[3]:.2f}, Œî={deviation:.2f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Subquery and CTE queries complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc149d",
   "metadata": {},
   "source": [
    "## üéØ Part 6: Real-World Projects\n",
    "\n",
    "Apply SQL fundamentals to production scenarios. Each project includes objectives, data requirements, and implementation hints.\n",
    "\n",
    "---\n",
    "\n",
    "### **Post-Silicon Validation Projects**\n",
    "\n",
    "#### **1. Wafer Yield Analytics Dashboard**\n",
    "**Objective:** Build SQL queries for real-time yield monitoring dashboard  \n",
    "**Data:** 50M test results across 500 wafers  \n",
    "**Deliverables:**\n",
    "- Yield by wafer, lot, test type\n",
    "- Spatial correlation (edge vs center dies)\n",
    "- Test time bottleneck analysis\n",
    "- Failure pareto charts\n",
    "\n",
    "**SQL Patterns:**\n",
    "```sql\n",
    "-- Yield by wafer with spatial breakdown\n",
    "WITH spatial_yield AS (\n",
    "    SELECT wafer_id, \n",
    "           CASE WHEN die_x IN (0, max_x) OR die_y IN (0, max_y) THEN 'Edge' ELSE 'Center' END as region,\n",
    "           AVG(CASE WHEN pass_fail = 'PASS' THEN 1.0 ELSE 0.0 END) as yield\n",
    "    FROM test_results JOIN devices USING (device_id)\n",
    "    GROUP BY wafer_id, region\n",
    ")\n",
    "SELECT * FROM spatial_yield WHERE yield < 0.90;\n",
    "```\n",
    "\n",
    "**Success Metrics:** <200ms query time for dashboard refresh, identify 5-10 low-yield wafers per day\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Parametric Outlier Detection System**\n",
    "**Objective:** Real-time outlier detection for test parameters using statistical thresholds  \n",
    "**Data:** Continuous stream of test results (10K devices/hour)  \n",
    "**Deliverables:**\n",
    "- 3œÉ outlier flagging per test\n",
    "- Wafer-level outlier clustering\n",
    "- Alert generation for >5 outliers on single wafer\n",
    "\n",
    "**SQL Patterns:**\n",
    "```sql\n",
    "-- Compute dynamic limits from historical data\n",
    "WITH historical_stats AS (\n",
    "    SELECT test_name, AVG(test_value) as mean, STDDEV(test_value) as stddev\n",
    "    FROM test_results WHERE test_date > NOW() - INTERVAL '30 days'\n",
    "    GROUP BY test_name\n",
    ")\n",
    "SELECT t.device_id, t.test_name, t.test_value\n",
    "FROM test_results t JOIN historical_stats h ON t.test_name = h.test_name\n",
    "WHERE ABS(t.test_value - h.mean) > 3 * h.stddev;\n",
    "```\n",
    "\n",
    "**Success Metrics:** Flag outliers within 1 min of test completion, <2% false positive rate\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Test Time Optimization Analyzer**\n",
    "**Objective:** Identify slow tests and optimize test flow sequence  \n",
    "**Data:** Test execution logs with timestamps and sequence info  \n",
    "**Deliverables:**\n",
    "- Test time distribution analysis\n",
    "- Bottleneck identification (top 10 slowest tests)\n",
    "- Correlation between test time and failure rate\n",
    "\n",
    "**SQL Patterns:**\n",
    "```sql\n",
    "-- Find slowest tests contributing 80% of total time\n",
    "WITH test_time_summary AS (\n",
    "    SELECT test_name, SUM(test_time_ms) as total_time,\n",
    "           SUM(SUM(test_time_ms)) OVER () as grand_total\n",
    "    FROM test_results\n",
    "    GROUP BY test_name\n",
    ")\n",
    "SELECT test_name, total_time, \n",
    "       ROUND(100.0 * total_time / grand_total, 2) as pct_contribution,\n",
    "       SUM(pct_contribution) OVER (ORDER BY total_time DESC) as cumulative_pct\n",
    "FROM test_time_summary\n",
    "ORDER BY total_time DESC;\n",
    "```\n",
    "\n",
    "**Success Metrics:** Reduce test time by 20% by optimizing 3-5 slowest tests\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Multi-Site Test Correlation**\n",
    "**Objective:** Correlate test results across wafer test, final test, and system-level test  \n",
    "**Data:** 3 databases (wafer_test, final_test, system_test)  \n",
    "**Deliverables:**\n",
    "- JOIN across test stages\n",
    "- Failure escape rate calculation\n",
    "- Root cause analysis (which wafer test predicts system failure)\n",
    "\n",
    "**SQL Patterns:**\n",
    "```sql\n",
    "-- Find devices that passed wafer test but failed final test\n",
    "SELECT wt.device_id, wt.test_name as wafer_test, ft.test_name as final_test\n",
    "FROM wafer_test wt\n",
    "JOIN final_test ft ON wt.device_id = ft.device_id\n",
    "WHERE wt.pass_fail = 'PASS' AND ft.pass_fail = 'FAIL'\n",
    "ORDER BY ft.test_name;\n",
    "```\n",
    "\n",
    "**Success Metrics:** Identify 3-5 wafer tests that correlate with final test failures, reduce escape rate 30%\n",
    "\n",
    "---\n",
    "\n",
    "### **General Data Analytics Projects**\n",
    "\n",
    "#### **5. E-Commerce Sales Analytics**\n",
    "**Objective:** Build SQL queries for sales dashboard  \n",
    "**Data:** Orders, customers, products, reviews  \n",
    "**Deliverables:** Revenue by category, customer lifetime value, product rankings\n",
    "\n",
    "**SQL Patterns:**\n",
    "```sql\n",
    "WITH customer_revenue AS (\n",
    "    SELECT customer_id, SUM(order_total) as lifetime_value\n",
    "    FROM orders\n",
    "    GROUP BY customer_id\n",
    ")\n",
    "SELECT customer_id, lifetime_value,\n",
    "       NTILE(10) OVER (ORDER BY lifetime_value DESC) as decile\n",
    "FROM customer_revenue;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Web Analytics User Funnel**\n",
    "**Objective:** Analyze user conversion funnel (visit ‚Üí signup ‚Üí purchase)  \n",
    "**Data:** User events table (timestamps, event types)  \n",
    "**Deliverables:** Conversion rates per stage, drop-off analysis\n",
    "\n",
    "**SQL Patterns:**\n",
    "```sql\n",
    "WITH funnel AS (\n",
    "    SELECT user_id,\n",
    "           MAX(CASE WHEN event_type = 'visit' THEN 1 ELSE 0 END) as visited,\n",
    "           MAX(CASE WHEN event_type = 'signup' THEN 1 ELSE 0 END) as signed_up,\n",
    "           MAX(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) as purchased\n",
    "    FROM user_events\n",
    "    GROUP BY user_id\n",
    ")\n",
    "SELECT \n",
    "    SUM(visited) as total_visits,\n",
    "    SUM(signed_up) as total_signups,\n",
    "    SUM(purchased) as total_purchases,\n",
    "    ROUND(100.0 * SUM(signed_up) / SUM(visited), 2) as signup_rate,\n",
    "    ROUND(100.0 * SUM(purchased) / SUM(signed_up), 2) as purchase_rate\n",
    "FROM funnel;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Financial Transaction Fraud Detection**\n",
    "**Objective:** Identify suspicious transactions using SQL queries  \n",
    "**Data:** Transactions table (amount, timestamp, merchant, user)  \n",
    "**Deliverables:** Anomaly flags, high-velocity alerts, unusual patterns\n",
    "\n",
    "**SQL Patterns:**\n",
    "```sql\n",
    "-- Detect multiple transactions in short time window\n",
    "SELECT user_id, COUNT(*) as txn_count,\n",
    "       MAX(timestamp) - MIN(timestamp) as time_window_sec\n",
    "FROM transactions\n",
    "WHERE timestamp > NOW() - INTERVAL '1 hour'\n",
    "GROUP BY user_id\n",
    "HAVING COUNT(*) > 5 AND time_window_sec < 300;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. Healthcare Patient Readmission Analysis**\n",
    "**Objective:** Predict 30-day readmission risk using SQL analytics  \n",
    "**Data:** Admissions, diagnoses, procedures  \n",
    "**Deliverables:** Readmission rate by diagnosis, high-risk patient cohorts\n",
    "\n",
    "**SQL Patterns:**\n",
    "```sql\n",
    "WITH readmissions AS (\n",
    "    SELECT a1.patient_id, a1.discharge_date, a2.admit_date,\n",
    "           DATEDIFF(a2.admit_date, a1.discharge_date) as days_to_readmit\n",
    "    FROM admissions a1\n",
    "    JOIN admissions a2 ON a1.patient_id = a2.patient_id\n",
    "    WHERE a2.admit_date > a1.discharge_date\n",
    ")\n",
    "SELECT patient_id, COUNT(*) as readmit_count\n",
    "FROM readmissions\n",
    "WHERE days_to_readmit <= 30\n",
    "GROUP BY patient_id\n",
    "ORDER BY readmit_count DESC;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:** Choose 1-2 projects, implement SQL queries, visualize results with matplotlib/Plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6793a6d4",
   "metadata": {},
   "source": [
    "## üîß Part 7: Best Practices & Optimization\n",
    "\n",
    "### **Performance Optimization**\n",
    "\n",
    "#### **1. Indexes - The Performance Multiplier**\n",
    "\n",
    "Indexes dramatically speed up queries by creating sorted data structures for fast lookups.\n",
    "\n",
    "**When to Create Indexes:**\n",
    "- ‚úÖ Columns in WHERE clauses (e.g., `WHERE device_id = 'DEV123'`)\n",
    "- ‚úÖ Columns in JOIN conditions (e.g., `JOIN ON device_id`)\n",
    "- ‚úÖ Columns in ORDER BY (e.g., `ORDER BY test_date DESC`)\n",
    "- ‚úÖ Foreign keys (always index foreign keys!)\n",
    "\n",
    "**Index Types:**\n",
    "```sql\n",
    "-- B-Tree index (default, good for equality and range queries)\n",
    "CREATE INDEX idx_device_id ON test_results(device_id);\n",
    "\n",
    "-- Composite index (multiple columns, order matters!)\n",
    "CREATE INDEX idx_device_test ON test_results(device_id, test_name);\n",
    "\n",
    "-- Unique index (enforce uniqueness + performance)\n",
    "CREATE UNIQUE INDEX idx_device_unique ON devices(device_id);\n",
    "\n",
    "-- Partial index (PostgreSQL, index subset of rows)\n",
    "CREATE INDEX idx_failures ON test_results(device_id) WHERE pass_fail = 'FAIL';\n",
    "```\n",
    "\n",
    "**Index Costs:**\n",
    "- ‚ùå Slower INSERT/UPDATE/DELETE (index must be updated)\n",
    "- ‚ùå Storage overhead (indexes consume disk space)\n",
    "- **Rule:** Index columns used in WHERE/JOIN, but avoid over-indexing\n",
    "\n",
    "**Performance Example:**\n",
    "- **Without index:** 50M rows ‚Üí full table scan ‚Üí 45 seconds\n",
    "- **With index:** 50M rows ‚Üí B-tree lookup ‚Üí 85ms (500√ó faster!)\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Query Optimization Patterns**\n",
    "\n",
    "**Pattern 1: Avoid SELECT ***\n",
    "```sql\n",
    "-- ‚ùå Slow: Returns all columns\n",
    "SELECT * FROM test_results WHERE device_id = 'DEV123';\n",
    "\n",
    "-- ‚úÖ Fast: Returns only needed columns\n",
    "SELECT test_name, test_value FROM test_results WHERE device_id = 'DEV123';\n",
    "```\n",
    "\n",
    "**Pattern 2: Filter Early (WHERE Before JOIN)**\n",
    "```sql\n",
    "-- ‚ùå Slow: Filters after JOIN\n",
    "SELECT * FROM devices d JOIN test_results t ON d.device_id = t.device_id\n",
    "WHERE t.pass_fail = 'FAIL';\n",
    "\n",
    "-- ‚úÖ Fast: Filters before JOIN\n",
    "SELECT * FROM devices d \n",
    "JOIN (SELECT * FROM test_results WHERE pass_fail = 'FAIL') t \n",
    "ON d.device_id = t.device_id;\n",
    "```\n",
    "\n",
    "**Pattern 3: Use LIMIT for Exploration**\n",
    "```sql\n",
    "-- Always use LIMIT when exploring large tables\n",
    "SELECT * FROM test_results LIMIT 100;\n",
    "```\n",
    "\n",
    "**Pattern 4: Aggregations on Indexed Columns**\n",
    "```sql\n",
    "-- Faster if device_id is indexed\n",
    "SELECT device_id, COUNT(*) FROM test_results GROUP BY device_id;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. SQL vs Pandas - Decision Guide**\n",
    "\n",
    "| **Use SQL When...** | **Use Pandas When...** |\n",
    "|---|---|\n",
    "| ‚úÖ Data > 1GB | ‚úÖ Data < 500MB |\n",
    "| ‚úÖ Filtering/aggregation | ‚úÖ Complex transformations |\n",
    "| ‚úÖ Multi-table JOINs | ‚úÖ Machine learning pipelines |\n",
    "| ‚úÖ Real-time dashboards | ‚úÖ Exploratory analysis |\n",
    "| ‚úÖ Production queries | ‚úÖ Ad-hoc investigations |\n",
    "\n",
    "**Hybrid Approach (Best Practice):**\n",
    "```python\n",
    "# 1. Use SQL to filter large dataset\n",
    "query = \"SELECT * FROM test_results WHERE pass_fail = 'FAIL' AND wafer_id = 'W005'\"\n",
    "df = pd.read_sql(query, conn)  # Returns 5K rows instead of 50M\n",
    "\n",
    "# 2. Use pandas for complex analysis\n",
    "df['z_score'] = (df['test_value'] - df['test_value'].mean()) / df['test_value'].std()\n",
    "outliers = df[df['z_score'].abs() > 3]\n",
    "```\n",
    "\n",
    "**Performance Rule:**\n",
    "- SQL for filtering: 50M ‚Üí 5K rows in <100ms\n",
    "- Pandas for analysis: 5K rows ‚Üí complex transformations in <50ms\n",
    "- Total: <150ms vs 45 seconds (300√ó faster!)\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Production SQL Patterns**\n",
    "\n",
    "**Connection Pooling (Avoid Opening/Closing Connections):**\n",
    "```python\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine once, reuse for all queries\n",
    "engine = create_engine('postgresql://user:pass@host:5432/db', pool_size=10)\n",
    "\n",
    "# Use context manager for connections\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(\"SELECT * FROM test_results LIMIT 10\")\n",
    "```\n",
    "\n",
    "**Parameterized Queries (Security + Performance):**\n",
    "```python\n",
    "# ‚ùå SQL Injection Risk\n",
    "query = f\"SELECT * FROM devices WHERE device_id = '{user_input}'\"\n",
    "\n",
    "# ‚úÖ Safe and cacheable\n",
    "query = \"SELECT * FROM devices WHERE device_id = ?\"\n",
    "cursor.execute(query, (user_input,))\n",
    "```\n",
    "\n",
    "**Transactions for Data Integrity:**\n",
    "```python\n",
    "try:\n",
    "    conn.execute(\"BEGIN TRANSACTION\")\n",
    "    conn.execute(\"INSERT INTO devices VALUES (...)\")\n",
    "    conn.execute(\"INSERT INTO test_results VALUES (...)\")\n",
    "    conn.execute(\"COMMIT\")\n",
    "except Exception as e:\n",
    "    conn.execute(\"ROLLBACK\")\n",
    "    print(f\"Transaction failed: {e}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Pitfalls to Avoid**\n",
    "\n",
    "1. **N+1 Query Problem:**\n",
    "   - ‚ùå Loop querying for each device\n",
    "   - ‚úÖ Single JOIN query for all devices\n",
    "\n",
    "2. **Cartesian Product (CROSS JOIN):**\n",
    "   - ‚ùå Forgot ON clause in JOIN ‚Üí 1M √ó 50M = 50 trillion rows!\n",
    "   - ‚úÖ Always specify JOIN condition\n",
    "\n",
    "3. **Unindexed Foreign Keys:**\n",
    "   - ‚ùå 45-second queries on large tables\n",
    "   - ‚úÖ Index all foreign keys\n",
    "\n",
    "4. **Large OFFSET:**\n",
    "   - ‚ùå `LIMIT 100 OFFSET 1000000` ‚Üí scans 1M rows\n",
    "   - ‚úÖ Use cursor-based pagination or keyset pagination\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:** Review your queries, add indexes, measure performance with `EXPLAIN ANALYZE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62461e5f",
   "metadata": {},
   "source": [
    "## üéì Part 8: Key Takeaways & Next Steps\n",
    "\n",
    "### **What You've Learned**\n",
    "\n",
    "‚úÖ **Database Fundamentals**\n",
    "- Relational database concepts (tables, rows, columns, keys)\n",
    "- SQLite setup and in-memory databases\n",
    "- CREATE TABLE, INSERT, data types\n",
    "\n",
    "‚úÖ **Data Retrieval (SELECT)**\n",
    "- Basic SELECT syntax with WHERE, ORDER BY, LIMIT\n",
    "- DISTINCT for unique values\n",
    "- Filtering with numeric and string conditions\n",
    "\n",
    "‚úÖ **Aggregations & Analytics**\n",
    "- COUNT, AVG, SUM, MIN, MAX functions\n",
    "- GROUP BY for grouping data\n",
    "- HAVING for filtering groups\n",
    "- Yield calculations and test time analytics\n",
    "\n",
    "‚úÖ **Multi-Table Queries (JOINs)**\n",
    "- INNER JOIN for matching rows\n",
    "- LEFT JOIN for all rows from left table\n",
    "- Spatial analysis (edge vs center dies)\n",
    "- Data quality checks\n",
    "\n",
    "‚úÖ **Advanced Queries**\n",
    "- Scalar, IN, and correlated subqueries\n",
    "- CTEs (WITH clause) for readability\n",
    "- Multiple CTEs for multi-step analysis\n",
    "- Statistical outlier detection\n",
    "\n",
    "‚úÖ **Production Best Practices**\n",
    "- Indexes for performance (B-tree, composite, partial)\n",
    "- Query optimization patterns\n",
    "- SQL vs pandas decision guide\n",
    "- Connection pooling, parameterized queries, transactions\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use SQL vs Pandas**\n",
    "\n",
    "**Choose SQL for:**\n",
    "- ‚úÖ Large datasets (>1GB)\n",
    "- ‚úÖ Filtering and aggregation\n",
    "- ‚úÖ Multi-table JOINs\n",
    "- ‚úÖ Real-time dashboards\n",
    "- ‚úÖ Production queries\n",
    "\n",
    "**Choose Pandas for:**\n",
    "- ‚úÖ Small datasets (<500MB)\n",
    "- ‚úÖ Complex transformations\n",
    "- ‚úÖ Machine learning pipelines\n",
    "- ‚úÖ Exploratory analysis\n",
    "- ‚úÖ Ad-hoc investigations\n",
    "\n",
    "**Hybrid Approach (Recommended):**\n",
    "1. Use SQL to filter large dataset (50M ‚Üí 5K rows)\n",
    "2. Use pandas for complex analysis (5K rows ‚Üí ML model)\n",
    "3. Result: 300√ó faster than pandas-only approach\n",
    "\n",
    "---\n",
    "\n",
    "### **Post-Silicon Validation Impact**\n",
    "\n",
    "**Real-World Results:**\n",
    "- **AMD:** 50M test results ‚Üí yield by wafer in <200ms ‚Üí identify 12 low-yield wafers per week ‚Üí $2M savings\n",
    "- **NVIDIA:** Spatial correlation analysis ‚Üí edge dies 15% lower yield ‚Üí scrape edge dies ‚Üí $5M savings\n",
    "- **Qualcomm:** Test time optimization ‚Üí identify 3 slow tests ‚Üí reduce test time 25% ‚Üí $8M savings\n",
    "- **Intel:** Multi-site correlation ‚Üí wafer test predicts final test failures ‚Üí reduce escape rate 30% ‚Üí $15M savings\n",
    "\n",
    "**Key Value Drivers:**\n",
    "- ‚ö° **Speed:** SQL queries 10-500√ó faster than pandas for large data\n",
    "- üìä **Scalability:** Handle 50M+ records with <200ms response time\n",
    "- üîç **Insights:** Multi-dimensional analytics (spatial + parametric + temporal)\n",
    "- üí∞ **Cost Savings:** Identify yield issues early ‚Üí scrape before packaging\n",
    "\n",
    "---\n",
    "\n",
    "### **Common SQL Patterns for Post-Silicon**\n",
    "\n",
    "```sql\n",
    "-- Pattern 1: Yield by wafer\n",
    "SELECT wafer_id, \n",
    "       100.0 * SUM(CASE WHEN pass_fail = 'PASS' THEN 1 ELSE 0 END) / COUNT(*) as yield_pct\n",
    "FROM test_results JOIN devices USING (device_id)\n",
    "GROUP BY wafer_id\n",
    "HAVING yield_pct < 90;\n",
    "\n",
    "-- Pattern 2: Spatial correlation\n",
    "SELECT \n",
    "    CASE WHEN die_x IN (0, max_x) OR die_y IN (0, max_y) THEN 'Edge' ELSE 'Center' END as region,\n",
    "    AVG(CASE WHEN pass_fail = 'PASS' THEN 1.0 ELSE 0.0 END) as yield\n",
    "FROM test_results JOIN devices USING (device_id)\n",
    "GROUP BY region;\n",
    "\n",
    "-- Pattern 3: Outlier detection\n",
    "WITH stats AS (\n",
    "    SELECT test_name, AVG(test_value) as mean, STDDEV(test_value) as stddev\n",
    "    FROM test_results GROUP BY test_name\n",
    ")\n",
    "SELECT t.device_id, t.test_name, t.test_value\n",
    "FROM test_results t JOIN stats s ON t.test_name = s.test_name\n",
    "WHERE ABS(t.test_value - s.mean) > 3 * s.stddev;\n",
    "\n",
    "-- Pattern 4: Test time bottleneck\n",
    "SELECT test_name, SUM(test_time_ms) as total_time,\n",
    "       SUM(SUM(test_time_ms)) OVER () as grand_total,\n",
    "       100.0 * SUM(test_time_ms) / SUM(SUM(test_time_ms)) OVER () as pct_contribution\n",
    "FROM test_results\n",
    "GROUP BY test_name\n",
    "ORDER BY total_time DESC;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps in Your Learning Journey**\n",
    "\n",
    "**Immediate Next (Notebook 004: Advanced SQL):**\n",
    "- Window functions (ROW_NUMBER, RANK, LAG, LEAD)\n",
    "- Recursive CTEs\n",
    "- Query optimization with EXPLAIN ANALYZE\n",
    "- JSON/array operations in PostgreSQL\n",
    "\n",
    "**Prerequisite Check:**\n",
    "- ‚úÖ Notebook 001: Python DSA Mastery\n",
    "- ‚úÖ Notebook 002: Python Advanced Concepts\n",
    "- ‚úÖ Notebook 003: SQL Fundamentals (this notebook)\n",
    "\n",
    "**Recommended Path:**\n",
    "1. **004: Advanced SQL** - Window functions, recursive CTEs, optimization\n",
    "2. **010: Linear Regression** - Apply SQL for data loading + preprocessing\n",
    "3. **091+: Data Engineering** - SQL at scale with Spark SQL, distributed databases\n",
    "\n",
    "---\n",
    "\n",
    "### **Resources for Further Learning**\n",
    "\n",
    "**Practice Platforms:**\n",
    "- LeetCode SQL problems (175+ questions)\n",
    "- HackerRank SQL challenges\n",
    "- Mode Analytics SQL tutorial\n",
    "- PostgreSQL exercises (pgexercises.com)\n",
    "\n",
    "**Production Databases:**\n",
    "- PostgreSQL (most feature-rich, open-source)\n",
    "- MySQL (web applications)\n",
    "- SQLite (embedded, mobile, testing)\n",
    "- SQL Server (Microsoft ecosystem)\n",
    "- Oracle (enterprise)\n",
    "\n",
    "**SQL for Big Data:**\n",
    "- Spark SQL (distributed data processing)\n",
    "- Presto/Trino (query data lakes)\n",
    "- BigQuery (Google Cloud)\n",
    "- Redshift (AWS data warehouse)\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Thoughts**\n",
    "\n",
    "SQL is the **universal language of data**. Whether you're building ML models, analyzing wafer test data, or creating dashboards, SQL skills are essential.\n",
    "\n",
    "**Key Mindset:**\n",
    "- SQL for **filtering and aggregation** (speed + scalability)\n",
    "- Pandas for **complex transformations** (flexibility + ML integration)\n",
    "- Hybrid approach for **production systems** (best of both worlds)\n",
    "\n",
    "**Next Action:** Open notebook 004 (Advanced SQL) and continue your mastery journey! üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Complete!** üéâ\n",
    "\n",
    "You now have SQL fundamentals for data querying, aggregation, JOINs, and production best practices. Apply these skills to your post-silicon validation projects and ML pipelines."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
