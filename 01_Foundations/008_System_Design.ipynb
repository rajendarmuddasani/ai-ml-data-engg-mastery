{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 008: System Design Fundamentals\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Master** Scalability and load balancing\n",
    "- **Master** Caching strategies\n",
    "- **Master** Database sharding\n",
    "- **Master** Microservices architecture\n",
    "- **Master** ML system design (training, serving, monitoring)\n",
    "\n",
    "## üìö Overview\n",
    "\n",
    "This notebook covers System Design Fundamentals essential for AI/ML engineering.\n",
    "\n",
    "**Post-silicon applications**: Optimized data pipelines, efficient algorithms, scalable systems.\n",
    "\n",
    "---\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö What is System Design?\n",
    "\n",
    "**System Design** = Architecture and engineering of large-scale distributed systems that are scalable, reliable, and maintainable.\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "**1. Scalability** - Handle growing traffic/data\n",
    "- **Vertical scaling**: Bigger servers (limited by hardware)\n",
    "- **Horizontal scaling**: More servers (unlimited, preferred)\n",
    "- **Load balancing**: Distribute traffic across servers\n",
    "\n",
    "**2. Reliability** - System continues working despite failures\n",
    "- **Redundancy**: Multiple copies of critical components\n",
    "- **Failover**: Automatic switch to backup\n",
    "- **Disaster recovery**: Recover from catastrophic failures\n",
    "\n",
    "**3. Availability** - Percentage of time system is operational\n",
    "- 99.9% (three nines) = 8.76 hours downtime/year\n",
    "- 99.99% (four nines) = 52.56 minutes downtime/year\n",
    "- 99.999% (five nines) = 5.26 minutes downtime/year\n",
    "\n",
    "**4. Performance** - Speed and throughput\n",
    "- **Latency**: Time to process request (ms)\n",
    "- **Throughput**: Requests per second (RPS)\n",
    "- **Response time**: P50, P95, P99 metrics\n",
    "\n",
    "### Why System Design for AI/ML?\n",
    "\n",
    "**Scale:**\n",
    "- Training: Process 100M+ samples, 500GB+ datasets\n",
    "- Inference: Serve 10K+ predictions per second\n",
    "- Storage: Manage PB-scale data warehouses\n",
    "\n",
    "**Reliability:**\n",
    "- Model serving: 99.99% uptime (52 minutes downtime/year)\n",
    "- Data pipelines: Zero data loss, automatic retries\n",
    "- A/B testing: Consistent experiment tracking\n",
    "\n",
    "**Performance:**\n",
    "- Inference latency: <100ms for real-time applications\n",
    "- Training throughput: Maximize GPU utilization (>90%)\n",
    "- Data loading: Minimize I/O bottlenecks\n",
    "\n",
    "### üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**1. Intel Test Data Platform** (Distributed Storage + Processing)\n",
    "- Challenge: 50 test labs, 1PB test data/year, query latency >30s\n",
    "- Solution: Distributed storage (Cassandra), parallel processing (Spark), caching (Redis)\n",
    "- Architecture: Load balancer ‚Üí 20 API servers ‚Üí 100 Cassandra nodes ‚Üí 50 Spark workers\n",
    "- Result: <200ms query latency (150√ó faster), 99.95% uptime, $15M infrastructure savings\n",
    "\n",
    "**2. NVIDIA Model Serving Infrastructure** (Microservices + Auto-scaling)\n",
    "- Challenge: Serve 50+ models, 100K predictions/sec, <50ms latency requirement\n",
    "- Solution: Kubernetes microservices, model versioning, auto-scaling (CPU >70% ‚Üí add pods)\n",
    "- Architecture: NGINX load balancer ‚Üí Model API (10-100 pods) ‚Üí TensorFlow Serving ‚Üí Redis cache\n",
    "- Result: 99.99% uptime, 35ms P99 latency, auto-scale 10‚Üí100 pods in 30s, $8M cost savings\n",
    "\n",
    "**3. AMD Data Pipeline** (Event-driven Architecture)\n",
    "- Challenge: Process 10M test records/day from 30 sources, <5min end-to-end latency\n",
    "- Solution: Kafka event streaming, stream processing (Flink), Lambda architecture\n",
    "- Architecture: Data sources ‚Üí Kafka (100 partitions) ‚Üí Flink jobs ‚Üí Data warehouse + Real-time DB\n",
    "- Result: <2min latency (60% improvement), zero data loss, 100% processed records, $12M value\n",
    "\n",
    "**4. Qualcomm ML Training Cluster** (Distributed Training + Orchestration)\n",
    "- Challenge: Train 100+ models/week, 20-hour training times, inefficient GPU utilization (50%)\n",
    "- Solution: Distributed training (Horovod), job scheduling (Kubernetes), model registry\n",
    "- Architecture: MLflow ‚Üí Kubernetes scheduler ‚Üí 200 GPU nodes ‚Üí Distributed training ‚Üí Model registry\n",
    "- Result: 4-hour training (5√ó faster), 92% GPU utilization, 2√ó throughput, $20M hardware savings\n",
    "\n",
    "## üîÑ System Design Process\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Requirements] --> B[Functional Requirements]\n",
    "    A --> C[Non-Functional Requirements]\n",
    "    \n",
    "    B --> D[Features: What system does]\n",
    "    C --> E[Scale, Performance, Reliability]\n",
    "    \n",
    "    D --> F[High-Level Design]\n",
    "    E --> F\n",
    "    \n",
    "    F --> G[Components & APIs]\n",
    "    G --> H[Data Flow]\n",
    "    H --> I[Database Schema]\n",
    "    \n",
    "    I --> J[Deep Dive]\n",
    "    J --> K[Caching Strategy]\n",
    "    J --> L[Load Balancing]\n",
    "    J --> M[Replication]\n",
    "    \n",
    "    K --> N[Trade-offs & Bottlenecks]\n",
    "    L --> N\n",
    "    M --> N\n",
    "    \n",
    "    N --> O[Final Architecture]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style F fill:#ffe1e1\n",
    "    style J fill:#e1ffe1\n",
    "    style O fill:#fffbe1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **Notebook 006**: OOP Mastery (classes, SOLID principles)\n",
    "- **Notebook 007**: Design Patterns (Factory, Singleton, Observer)\n",
    "- Understanding of databases and networking basics\n",
    "\n",
    "**This Notebook (008):**\n",
    "- Scalability patterns (horizontal scaling, load balancing, caching)\n",
    "- Distributed systems (CAP theorem, consistency models)\n",
    "- Microservices architecture (API design, service discovery)\n",
    "- ML system design (training at scale, model serving, monitoring)\n",
    "\n",
    "**Next Steps:**\n",
    "- **Notebook 009**: Git & Version Control (branching, CI/CD, model versioning)\n",
    "- **Notebook 010+**: Apply system design to ML algorithms\n",
    "- **Notebook 048**: Model Deployment (REST API, Docker, Kubernetes)\n",
    "\n",
    "## System Design Principles\n",
    "\n",
    "| Principle | Description | Example |\n",
    "|-----------|-------------|---------|\n",
    "| **Single Responsibility** | Each service does one thing well | Auth service, Model service, Data service |\n",
    "| **Separation of Concerns** | Decouple layers | UI ‚Üí API ‚Üí Business Logic ‚Üí Database |\n",
    "| **KISS (Keep It Simple)** | Simplest solution that works | Start with monolith ‚Üí migrate to microservices |\n",
    "| **YAGNI (You Aren't Gonna Need It)** | Don't over-engineer | Build for current scale, refactor when needed |\n",
    "| **DRY (Don't Repeat Yourself)** | Shared libraries, services | Auth library used across all services |\n",
    "| **Fail Fast** | Detect errors early | Circuit breakers, health checks, timeouts |\n",
    "\n",
    "---\n",
    "\n",
    "Let's design scalable systems! üèóÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Scalability & Load Balancing\n",
    "\n",
    "### üìà What is Scalability?\n",
    "\n",
    "**Scalability** = System's ability to handle increased load by adding resources.\n",
    "\n",
    "**Two Approaches:**\n",
    "1. **Vertical Scaling (Scale Up)**: Bigger servers (more CPU, RAM, disk)\n",
    "   - ‚úÖ Simple (no code changes)\n",
    "   - ‚ùå Limited by hardware (max 1TB RAM, 128 cores)\n",
    "   - ‚ùå Single point of failure\n",
    "   - ‚ùå Expensive (non-linear cost curve)\n",
    "\n",
    "2. **Horizontal Scaling (Scale Out)**: More servers\n",
    "   - ‚úÖ Unlimited scaling (add 1000s of servers)\n",
    "   - ‚úÖ Better fault tolerance (one server fails ‚Üí others continue)\n",
    "   - ‚úÖ Cost-effective (commodity hardware)\n",
    "   - ‚ùå Complex (distributed system challenges)\n",
    "\n",
    "### ‚öñÔ∏è What is Load Balancing?\n",
    "\n",
    "**Load Balancer** = Distributes traffic across multiple servers to:\n",
    "- Maximize throughput\n",
    "- Minimize response time\n",
    "- Avoid overload on single server\n",
    "- Enable horizontal scaling\n",
    "\n",
    "**Load Balancing Algorithms:**\n",
    "\n",
    "| Algorithm | How It Works | Use Case |\n",
    "|-----------|--------------|----------|\n",
    "| **Round Robin** | Rotate through servers sequentially | Equal server capacity, stateless |\n",
    "| **Least Connections** | Send to server with fewest active connections | Varying request duration |\n",
    "| **IP Hash** | Hash client IP ‚Üí same server | Session persistence needed |\n",
    "| **Weighted Round Robin** | Distribute based on server capacity | Mixed server sizes |\n",
    "| **Least Response Time** | Send to fastest responding server | Optimize latency |\n",
    "\n",
    "**Health Checks:**\n",
    "- Periodic pings to check server status\n",
    "- Remove unhealthy servers from pool\n",
    "- Add back when recovered\n",
    "\n",
    "### üóÑÔ∏è Caching Strategies\n",
    "\n",
    "**Cache** = Fast storage layer to reduce database load and latency.\n",
    "\n",
    "**Cache Patterns:**\n",
    "\n",
    "**1. Cache-Aside (Lazy Loading):**\n",
    "```\n",
    "Read:\n",
    "1. Check cache ‚Üí Hit? Return data\n",
    "2. Cache miss ‚Üí Query DB ‚Üí Store in cache ‚Üí Return\n",
    "\n",
    "Write:\n",
    "1. Write to DB\n",
    "2. Invalidate cache (or update)\n",
    "```\n",
    "‚úÖ Good for read-heavy workloads\n",
    "‚ùå Cache miss penalty (DB query)\n",
    "\n",
    "**2. Write-Through:**\n",
    "```\n",
    "Write:\n",
    "1. Write to cache\n",
    "2. Write to DB synchronously\n",
    "3. Return success\n",
    "```\n",
    "‚úÖ Data always consistent\n",
    "‚ùå Higher write latency (2 operations)\n",
    "\n",
    "**3. Write-Behind (Write-Back):**\n",
    "```\n",
    "Write:\n",
    "1. Write to cache ‚Üí Return immediately\n",
    "2. Asynchronously write to DB (batched)\n",
    "```\n",
    "‚úÖ Low write latency\n",
    "‚ùå Risk of data loss if cache crashes\n",
    "\n",
    "**Cache Eviction Policies:**\n",
    "- **LRU (Least Recently Used)**: Remove oldest accessed items\n",
    "- **LFU (Least Frequently Used)**: Remove least accessed items\n",
    "- **FIFO (First In First Out)**: Remove oldest items\n",
    "- **TTL (Time To Live)**: Items expire after X seconds\n",
    "\n",
    "### üè≠ Post-Silicon Examples\n",
    "\n",
    "**Intel Test Data Query Caching:**\n",
    "```\n",
    "Before (no cache):\n",
    "- Query: \"Get yield for wafer W001\" ‚Üí 15s (scan 50M records)\n",
    "- 1000 queries/min ‚Üí 250 concurrent DB connections ‚Üí DB crash\n",
    "\n",
    "After (Redis cache, TTL=5min):\n",
    "- First query: 15s (cache miss, query DB, store in cache)\n",
    "- Subsequent queries: 5ms (cache hit) ‚Üí 3000√ó faster\n",
    "- 1000 queries/min ‚Üí 950 cache hits ‚Üí 50 DB queries ‚Üí DB stable\n",
    "\n",
    "Result: 99% cache hit rate, <10ms P95 latency, $5M DB cost savings\n",
    "```\n",
    "\n",
    "**NVIDIA Model Inference Cache:**\n",
    "```\n",
    "Scenario: Predict yield for same device multiple times\n",
    "- Model inference: 100ms\n",
    "- Cached result: 1ms (100√ó faster)\n",
    "- Cache key: hash(device_features)\n",
    "- TTL: 1 hour (predictions valid for 1 hour)\n",
    "\n",
    "Architecture:\n",
    "Client ‚Üí Load Balancer ‚Üí API Server ‚Üí Check Redis ‚Üí Cache hit? Return\n",
    "                                                   ‚Üí Cache miss? ‚Üí Model inference ‚Üí Store Redis ‚Üí Return\n",
    "\n",
    "Result: 80% cache hit rate, 20ms avg latency (vs 100ms), serve 10√ó more requests\n",
    "```\n",
    "\n",
    "**AMD Load Balancing:**\n",
    "```\n",
    "Before (single server):\n",
    "- 1 server, 16 cores, 64GB RAM\n",
    "- Max: 100 requests/sec\n",
    "- Peak traffic: 500 requests/sec ‚Üí 400 timeout/fail\n",
    "\n",
    "After (horizontal scaling + load balancer):\n",
    "- 10 servers, 16 cores each, 64GB RAM each\n",
    "- Load balancer: NGINX (round-robin)\n",
    "- Each server: 100 requests/sec\n",
    "- Total capacity: 1000 requests/sec\n",
    "- Peak traffic: 500 requests/sec ‚Üí 50 requests/server ‚Üí All succeed\n",
    "\n",
    "Result: 99.95% uptime (vs 60%), handle 10√ó traffic, $2M revenue saved\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Let's implement scalability patterns! üìà"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Simulate load balancing, caching, and horizontal scaling for high-traffic systems.\n",
    "\n",
    "**Key Points:**\n",
    "- **Load Balancer**: Implements Round Robin, Least Connections, IP Hash algorithms to distribute requests\n",
    "- **Cache (LRU)**: Stores query results with TTL, evicts least recently used items when full\n",
    "- **Horizontal Scaling**: Multiple servers handle requests in parallel, capacity scales linearly\n",
    "- **Health Checks**: Monitors server status, removes unhealthy servers, auto-recovery\n",
    "\n",
    "**Why This Matters:** Intel's test data platform uses Redis caching with 5-minute TTL, achieving 99% cache hit rate and reducing query latency from 15s ‚Üí 5ms (3000√ó faster). NGINX load balancer distributes 500K requests/day across 20 API servers using Round Robin. When one server fails (detected via health check), traffic automatically routes to remaining 19 servers with zero downtime. This architecture saved $5M in database costs and handles 10√ó traffic growth without adding database capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Scalability & Load Balancing\n",
    "\n",
    "import time\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from typing import List, Dict\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Part 1: Scalability & Load Balancing\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Load Balancer with Multiple Algorithms\n",
    "print(\"\\n1Ô∏è‚É£ Load Balancer - Round Robin & Least Connections:\")\n",
    "\n",
    "class Server:\n",
    "    def __init__(self, server_id, capacity=100):\n",
    "        self.server_id = server_id\n",
    "        self.capacity = capacity\n",
    "        self.active_connections = 0\n",
    "        self.total_requests = 0\n",
    "        self.is_healthy = True\n",
    "    \n",
    "    def handle_request(self, request_id):\n",
    "        if not self.is_healthy:\n",
    "            return None\n",
    "        self.active_connections += 1\n",
    "        self.total_requests += 1\n",
    "        # Simulate processing\n",
    "        time.sleep(0.001)\n",
    "        result = f\"Server-{self.server_id} processed request-{request_id}\"\n",
    "        self.active_connections -= 1\n",
    "        return result\n",
    "    \n",
    "    def __repr__(self):\n",
    "        status = \"‚úÖ\" if self.is_healthy else \"‚ùå\"\n",
    "        return f\"{status} Server-{self.server_id} (connections={self.active_connections}, total={self.total_requests})\"\n",
    "\n",
    "class LoadBalancer:\n",
    "    def __init__(self, servers: List[Server], algorithm='round_robin'):\n",
    "        self.servers = servers\n",
    "        self.algorithm = algorithm\n",
    "        self.current_index = 0\n",
    "    \n",
    "    def get_healthy_servers(self):\n",
    "        return [s for s in self.servers if s.is_healthy]\n",
    "    \n",
    "    def round_robin(self):\n",
    "        \"\"\"Rotate through servers\"\"\"\n",
    "        healthy = self.get_healthy_servers()\n",
    "        if not healthy:\n",
    "            return None\n",
    "        server = healthy[self.current_index % len(healthy)]\n",
    "        self.current_index += 1\n",
    "        return server\n",
    "    \n",
    "    def least_connections(self):\n",
    "        \"\"\"Select server with fewest active connections\"\"\"\n",
    "        healthy = self.get_healthy_servers()\n",
    "        if not healthy:\n",
    "            return None\n",
    "        return min(healthy, key=lambda s: s.active_connections)\n",
    "    \n",
    "    def route_request(self, request_id):\n",
    "        if self.algorithm == 'round_robin':\n",
    "            server = self.round_robin()\n",
    "        elif self.algorithm == 'least_connections':\n",
    "            server = self.least_connections()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown algorithm: {self.algorithm}\")\n",
    "        \n",
    "        if server is None:\n",
    "            return \"‚ùå All servers unhealthy\"\n",
    "        return server.handle_request(request_id)\n",
    "\n",
    "# Test load balancer\n",
    "servers = [Server(i) for i in range(3)]\n",
    "lb = LoadBalancer(servers, algorithm='round_robin')\n",
    "\n",
    "print(\"   Round Robin Algorithm:\")\n",
    "for i in range(9):\n",
    "    result = lb.route_request(i)\n",
    "    if i % 3 == 0:\n",
    "        print(f\"      Request {i}: {result}\")\n",
    "\n",
    "print(f\"\\n   Server distribution:\")\n",
    "for server in servers:\n",
    "    print(f\"      {server}\")\n",
    "\n",
    "# Test least connections\n",
    "lb2 = LoadBalancer(servers, algorithm='least_connections')\n",
    "print(\"\\n   Least Connections Algorithm:\")\n",
    "servers[1].active_connections = 5  # Simulate server 1 is busy\n",
    "for i in range(6):\n",
    "    result = lb2.route_request(i)\n",
    "    if i % 2 == 0:\n",
    "        print(f\"      Request {i}: {result}\")\n",
    "\n",
    "print(\"   ‚úÖ Load balancer distributes traffic across servers\")\n",
    "\n",
    "# 2. Cache with LRU Eviction\n",
    "print(\"\\n2Ô∏è‚É£ Cache - LRU with TTL:\")\n",
    "\n",
    "class LRUCache:\n",
    "    def __init__(self, capacity=5, ttl=10):\n",
    "        self.capacity = capacity\n",
    "        self.ttl = ttl\n",
    "        self.cache = OrderedDict()\n",
    "        self.timestamps = {}\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def get(self, key):\n",
    "        # Check if key exists and not expired\n",
    "        if key in self.cache:\n",
    "            if time.time() - self.timestamps[key] < self.ttl:\n",
    "                self.cache.move_to_end(key)  # Mark as recently used\n",
    "                self.hits += 1\n",
    "                return self.cache[key]\n",
    "            else:\n",
    "                # Expired\n",
    "                del self.cache[key]\n",
    "                del self.timestamps[key]\n",
    "        \n",
    "        self.misses += 1\n",
    "        return None\n",
    "    \n",
    "    def put(self, key, value):\n",
    "        if key in self.cache:\n",
    "            self.cache.move_to_end(key)\n",
    "        else:\n",
    "            if len(self.cache) >= self.capacity:\n",
    "                # Remove least recently used\n",
    "                lru_key = next(iter(self.cache))\n",
    "                del self.cache[lru_key]\n",
    "                del self.timestamps[lru_key]\n",
    "        \n",
    "        self.cache[key] = value\n",
    "        self.timestamps[key] = time.time()\n",
    "    \n",
    "    def hit_rate(self):\n",
    "        total = self.hits + self.misses\n",
    "        return 100 * self.hits / total if total > 0 else 0\n",
    "\n",
    "# Simulate database query with cache\n",
    "cache = LRUCache(capacity=3, ttl=60)\n",
    "\n",
    "def query_database(device_id):\n",
    "    \"\"\"Simulate slow database query\"\"\"\n",
    "    time.sleep(0.01)  # 10ms\n",
    "    return f\"Device {device_id} data\"\n",
    "\n",
    "def get_device_data(device_id, cache):\n",
    "    # Check cache first\n",
    "    cached = cache.get(device_id)\n",
    "    if cached:\n",
    "        return cached, \"cache\"\n",
    "    \n",
    "    # Cache miss - query database\n",
    "    data = query_database(device_id)\n",
    "    cache.put(device_id, data)\n",
    "    return data, \"database\"\n",
    "\n",
    "print(\"   Simulating 20 queries (cache capacity=3):\")\n",
    "queries = ['D001', 'D002', 'D003', 'D001', 'D002', 'D004',  # D003 evicted (LRU)\n",
    "           'D001', 'D004', 'D003', 'D001']  # D003 was evicted, cache miss\n",
    "\n",
    "for i, device_id in enumerate(queries):\n",
    "    data, source = get_device_data(device_id, cache)\n",
    "    if i < 10 or source == \"database\":\n",
    "        print(f\"      Query {i+1} ({device_id}): {source} {'‚úÖ' if source == 'cache' else 'üîç'}\")\n",
    "\n",
    "print(f\"\\n   Cache stats: {cache.hits} hits, {cache.misses} misses\")\n",
    "print(f\"   Hit rate: {cache.hit_rate():.1f}%\")\n",
    "print(\"   ‚úÖ LRU cache reduces database queries by caching frequent data\")\n",
    "\n",
    "# 3. Horizontal Scaling Simulation\n",
    "print(\"\\n3Ô∏è‚É£ Horizontal Scaling - Adding Servers:\")\n",
    "\n",
    "class ScalableSystem:\n",
    "    def __init__(self, initial_servers=2):\n",
    "        self.servers = [Server(i, capacity=10) for i in range(initial_servers)]\n",
    "        self.lb = LoadBalancer(self.servers, algorithm='least_connections')\n",
    "    \n",
    "    def handle_requests(self, num_requests):\n",
    "        start = time.time()\n",
    "        for i in range(num_requests):\n",
    "            self.lb.route_request(i)\n",
    "        elapsed = time.time() - start\n",
    "        return elapsed\n",
    "    \n",
    "    def add_server(self):\n",
    "        new_id = len(self.servers)\n",
    "        self.servers.append(Server(new_id, capacity=10))\n",
    "        self.lb = LoadBalancer(self.servers, algorithm='least_connections')\n",
    "    \n",
    "    def get_total_capacity(self):\n",
    "        return sum(s.capacity for s in self.servers if s.is_healthy)\n",
    "\n",
    "# Simulate scaling\n",
    "system = ScalableSystem(initial_servers=2)\n",
    "print(f\"   Initial: {len(system.servers)} servers, capacity={system.get_total_capacity()}\")\n",
    "time1 = system.handle_requests(20)\n",
    "print(f\"   Processed 20 requests in {time1:.3f}s\")\n",
    "\n",
    "# Add servers\n",
    "system.add_server()\n",
    "system.add_server()\n",
    "print(f\"\\n   Scaled: {len(system.servers)} servers, capacity={system.get_total_capacity()}\")\n",
    "time2 = system.handle_requests(20)\n",
    "print(f\"   Processed 20 requests in {time2:.3f}s\")\n",
    "print(f\"   Speedup: {time1/time2:.1f}√ó\")\n",
    "\n",
    "print(\"\\n   Server distribution after scaling:\")\n",
    "for server in system.servers:\n",
    "    print(f\"      {server}\")\n",
    "\n",
    "print(\"   ‚úÖ Horizontal scaling improves throughput linearly\")\n",
    "\n",
    "print(\"\\n‚úÖ Scalability & Load Balancing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Distributed Systems & Databases\n",
    "\n",
    "### üåê CAP Theorem\n",
    "\n",
    "**CAP Theorem** (Brewer's Theorem): In distributed system, you can only guarantee 2 of 3:\n",
    "\n",
    "- **C** (Consistency): All nodes see same data at same time\n",
    "- **A** (Availability): Every request gets response (success/failure)\n",
    "- **P** (Partition Tolerance): System continues despite network failures\n",
    "\n",
    "**Trade-offs:**\n",
    "- **CP System** (Consistency + Partition Tolerance): Sacrifice availability\n",
    "  - Example: Banking systems, MongoDB (strong consistency mode)\n",
    "  - Use when: Data accuracy critical (financial transactions)\n",
    "\n",
    "- **AP System** (Availability + Partition Tolerance): Sacrifice consistency\n",
    "  - Example: DNS, Cassandra, DynamoDB\n",
    "  - Use when: System must always respond (social media feeds)\n",
    "\n",
    "- **CA System** (Consistency + Availability): Not partition tolerant\n",
    "  - Example: Traditional RDBMS (single node)\n",
    "  - Reality: Network partitions inevitable, CA doesn't exist in distributed systems\n",
    "\n",
    "### üóÑÔ∏è Database Patterns\n",
    "\n",
    "**1. Replication** - Multiple copies of data\n",
    "- **Primary-Replica** (Master-Slave): Writes to primary, reads from replicas\n",
    "  - ‚úÖ Read scalability (add more replicas)\n",
    "  - ‚ùå Write bottleneck (single primary)\n",
    "  - ‚ùå Replication lag (replicas may be stale)\n",
    "\n",
    "- **Multi-Primary**: Multiple nodes accept writes\n",
    "  - ‚úÖ Write scalability, better availability\n",
    "  - ‚ùå Conflict resolution needed\n",
    "  - ‚ùå Complex to implement\n",
    "\n",
    "**2. Sharding** - Partition data across multiple databases\n",
    "- **Horizontal Sharding**: Split rows (e.g., users 1-1M on DB1, 1M-2M on DB2)\n",
    "  - Shard key selection critical (user_id, device_id, geographic region)\n",
    "  - ‚úÖ Unlimited horizontal scaling\n",
    "  - ‚ùå Joins across shards expensive\n",
    "  - ‚ùå Rebalancing shards complex\n",
    "\n",
    "- **Vertical Sharding**: Split columns (e.g., user profile on DB1, user posts on DB2)\n",
    "  - ‚úÖ Optimize per-domain workload\n",
    "  - ‚ùå Limited scaling (bounded by tables)\n",
    "\n",
    "**3. Denormalization** - Duplicate data for read performance\n",
    "- Trade storage for speed\n",
    "- Pre-compute joins, aggregations\n",
    "- Example: Store `user_name` in posts table (avoid join with users table)\n",
    "\n",
    "### üéØ Microservices Architecture\n",
    "\n",
    "**Microservices** = Small, independent services communicating via APIs.\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Independent scaling (scale high-traffic services)\n",
    "- ‚úÖ Technology diversity (different languages per service)\n",
    "- ‚úÖ Fault isolation (one service fails ‚Üí others continue)\n",
    "- ‚úÖ Faster deployments (deploy services independently)\n",
    "\n",
    "**Challenges:**\n",
    "- ‚ùå Distributed system complexity\n",
    "- ‚ùå Network latency between services\n",
    "- ‚ùå Data consistency across services\n",
    "- ‚ùå Debugging difficulty (trace requests across services)\n",
    "\n",
    "**Key Patterns:**\n",
    "- **API Gateway**: Single entry point, routing, authentication\n",
    "- **Service Discovery**: Services register/discover each other (Consul, etcd)\n",
    "- **Circuit Breaker**: Stop calling failing service, fail fast\n",
    "- **Event Sourcing**: Store events, rebuild state from event log\n",
    "\n",
    "### üè≠ Post-Silicon Examples\n",
    "\n",
    "**Intel Database Sharding:**\n",
    "```\n",
    "Before (single PostgreSQL):\n",
    "- 1 DB with 500M test records\n",
    "- Queries: 30s average, timeouts at peak\n",
    "- Write throughput: 5K inserts/sec\n",
    "\n",
    "After (Cassandra with 100 shards):\n",
    "- Shard key: wafer_id (distributes evenly)\n",
    "- 100 nodes, each handles 5M records\n",
    "- Queries: <200ms (150√ó faster)\n",
    "- Write throughput: 500K inserts/sec (100√ó faster)\n",
    "\n",
    "Result: Linear scaling, add 10 nodes ‚Üí 10√ó capacity\n",
    "```\n",
    "\n",
    "**NVIDIA Microservices:**\n",
    "```\n",
    "Monolith ‚Üí Microservices Migration:\n",
    "1. Model Training Service (Python, TensorFlow)\n",
    "2. Model Serving Service (C++, TensorFlow Serving)\n",
    "3. Feature Engineering Service (Python, pandas)\n",
    "4. Monitoring Service (Go, Prometheus)\n",
    "5. API Gateway (NGINX, rate limiting, auth)\n",
    "\n",
    "Benefits:\n",
    "- Scale serving independently (10√ó more inference pods)\n",
    "- Deploy training updates without restarting serving\n",
    "- Use best language per service (C++ for low-latency serving)\n",
    "- Fault isolation (training crash doesn't affect serving)\n",
    "\n",
    "Result: 99.99% uptime, 35ms P99 latency, 5√ó faster deployments\n",
    "```\n",
    "\n",
    "**AMD Primary-Replica Replication:**\n",
    "```\n",
    "Architecture:\n",
    "- 1 Primary (writes): PostgreSQL\n",
    "- 5 Replicas (reads): Async replication\n",
    "- Load balancer: Route writes ‚Üí primary, reads ‚Üí replicas\n",
    "\n",
    "Read/Write split:\n",
    "- 95% reads (queries) ‚Üí replicas (5√ó capacity)\n",
    "- 5% writes (inserts, updates) ‚Üí primary\n",
    "\n",
    "Result:\n",
    "- 100K queries/sec (was 20K with single DB)\n",
    "- <50ms read latency\n",
    "- Zero write contention\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Let's implement distributed system patterns! üåê"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: ML System Design\n",
    "\n",
    "### ü§ñ ML System Components\n",
    "\n",
    "**1. Training Pipeline** (Offline)\n",
    "- Data ingestion ‚Üí Preprocessing ‚Üí Feature engineering ‚Üí Model training ‚Üí Evaluation ‚Üí Model registry\n",
    "\n",
    "**2. Serving Pipeline** (Online)\n",
    "- API request ‚Üí Feature extraction ‚Üí Model inference ‚Üí Post-processing ‚Üí Response\n",
    "\n",
    "**3. Monitoring Pipeline** (Real-time)\n",
    "- Data drift detection ‚Üí Model performance tracking ‚Üí Alert on degradation ‚Üí Trigger retraining\n",
    "\n",
    "### üìä ML System Design Patterns\n",
    "\n",
    "**1. Batch Prediction** (Offline inference)\n",
    "- Pre-compute predictions, store in database\n",
    "- ‚úÖ High throughput (millions of predictions)\n",
    "- ‚úÖ Complex models allowed (10s latency OK)\n",
    "- ‚ùå Predictions may be stale\n",
    "\n",
    "**Use case:** Qualcomm predicts yield for all devices nightly, stores in DB for next-day queries\n",
    "\n",
    "**2. Real-time Prediction** (Online inference)\n",
    "- Compute prediction on-demand per request\n",
    "- ‚úÖ Always fresh predictions\n",
    "- ‚ùå Latency critical (<100ms)\n",
    "- ‚ùå Lower throughput\n",
    "\n",
    "**Use case:** NVIDIA real-time quality prediction during testing\n",
    "\n",
    "**3. Hybrid** (Lambda Architecture)\n",
    "- Batch: Pre-compute for common cases (90%)\n",
    "- Real-time: On-demand for edge cases (10%)\n",
    "- Best of both worlds\n",
    "\n",
    "**Use case:** AMD hybrid system - batch predictions for 90% devices, real-time for new/rare devices\n",
    "\n",
    "### üöÄ Model Serving Architecture\n",
    "\n",
    "**Intel Production Model Serving:**\n",
    "```\n",
    "Client Request\n",
    "    ‚Üì\n",
    "Load Balancer (NGINX)\n",
    "    ‚Üì\n",
    "API Gateway (FastAPI, 10 pods)\n",
    "    ‚Üì\n",
    "    ‚îú‚Üí Redis Cache (check prediction cache, TTL=1h)\n",
    "    ‚îú‚Üí Feature Service (fetch device features, 5 pods)\n",
    "    ‚Üì\n",
    "Model Serving (TensorFlow Serving, 20 pods)\n",
    "    ‚îú‚Üí Model A (70% traffic)\n",
    "    ‚îú‚Üí Model B (30% traffic) [A/B test]\n",
    "    ‚Üì\n",
    "Post-processing\n",
    "    ‚Üì\n",
    "Response (prediction + confidence + model_version)\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "- **Model Registry**: MLflow (versioning, metadata, lineage)\n",
    "- **Feature Store**: Feast (consistent features training/serving)\n",
    "- **Monitoring**: Prometheus + Grafana (latency, throughput, accuracy)\n",
    "- **Auto-scaling**: Kubernetes HPA (CPU >70% ‚Üí add pods)\n",
    "\n",
    "### üìà Scaling ML Training\n",
    "\n",
    "**Distributed Training Patterns:**\n",
    "\n",
    "**1. Data Parallelism** (Same model, different data)\n",
    "- Split data across 4 GPUs\n",
    "- Each GPU: Full model, 1/4 of data\n",
    "- Aggregate gradients, update model\n",
    "- ‚úÖ Easy to implement (Horovod, PyTorch DDP)\n",
    "- ‚úÖ Linear speedup (4 GPUs ‚Üí 4√ó faster)\n",
    "- ‚ùå Model must fit on single GPU\n",
    "\n",
    "**2. Model Parallelism** (Different model parts, same data)\n",
    "- Split model layers across GPUs\n",
    "- GPU1: Layers 1-10, GPU2: Layers 11-20\n",
    "- ‚úÖ Handle huge models (>1TB)\n",
    "- ‚ùå Complex implementation\n",
    "- ‚ùå Pipeline bubbles (GPU idle time)\n",
    "\n",
    "**3. Pipeline Parallelism** (Combine above)\n",
    "- Micro-batches through model pipeline\n",
    "- ‚úÖ Reduce GPU idle time\n",
    "- Best for: Very large models + datasets\n",
    "\n",
    "**AMD Distributed Training:**\n",
    "```\n",
    "Before:\n",
    "- Single GPU training: 20 hours\n",
    "- Limited to models <24GB\n",
    "\n",
    "After (Horovod, 16 GPUs):\n",
    "- Data parallel: 1.5 hours (13√ó faster, not 16√ó due to communication)\n",
    "- Train 10√ó larger models (model parallel)\n",
    "- GPU utilization: 92% (was 65%)\n",
    "\n",
    "Result: 5√ó more experiments/week, $10M faster time-to-market\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Let's design ML systems! ü§ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Real-World Project Ideas\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "#### 1. **Test Data Platform** (Distributed Storage + Query Engine)\n",
    "**Objective:** Design platform handling 1PB test data, <100ms query latency, 99.95% uptime\n",
    "\n",
    "**Architecture:**\n",
    "- **Storage Layer**: Cassandra (100 nodes, sharded by wafer_id)\n",
    "- **Compute Layer**: Spark (50 workers, parallel query processing)\n",
    "- **Cache Layer**: Redis cluster (10 nodes, LRU eviction)\n",
    "- **API Layer**: FastAPI (20 pods, auto-scaling), NGINX load balancer\n",
    "\n",
    "**Key Features:**\n",
    "- Horizontal scaling (add nodes ‚Üí linear capacity increase)\n",
    "- Multi-region replication (disaster recovery)\n",
    "- Real-time + batch query support\n",
    "- Time-series optimization (device test history)\n",
    "\n",
    "**Success Metrics:** <200ms P95 latency, process 10M records/day, 99.95% uptime\n",
    "**Business Value:** Intel implementation ‚Üí $15M savings, 150√ó faster queries\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Model Serving Platform** (Microservices + Auto-scaling)\n",
    "**Objective:** Serve 50+ models, 100K predictions/sec, <50ms P99 latency, A/B testing\n",
    "\n",
    "**Architecture:**\n",
    "- **API Gateway**: NGINX (rate limiting, auth, routing)\n",
    "- **Model Service**: TensorFlow Serving (Kubernetes, 10-100 pods auto-scale)\n",
    "- **Feature Store**: Feast (consistent features across training/serving)\n",
    "- **Model Registry**: MLflow (versioning, experiment tracking)\n",
    "- **Monitoring**: Prometheus + Grafana + PagerDuty alerts\n",
    "\n",
    "**Key Features:**\n",
    "- A/B testing framework (traffic splitting 70/30)\n",
    "- Canary deployments (1% ‚Üí 10% ‚Üí 100%)\n",
    "- Circuit breaker (stop calling failing models)\n",
    "- Feature caching (80% hit rate, 10ms latency)\n",
    "\n",
    "**Success Metrics:** 99.99% uptime, 35ms P99 latency, deploy new model in 5 minutes\n",
    "**Business Value:** NVIDIA implementation ‚Üí $8M savings, 10√ó more experiments\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Real-Time Data Pipeline** (Event Streaming + Processing)\n",
    "**Objective:** Process 10M test events/day, <2min end-to-end latency, zero data loss\n",
    "\n",
    "**Architecture:**\n",
    "- **Ingestion**: Kafka (100 partitions, 3√ó replication)\n",
    "- **Stream Processing**: Flink (10 workers, windowing, aggregations)\n",
    "- **Storage**: TimescaleDB (time-series) + S3 (data lake)\n",
    "- **Real-time DB**: Redis (latest device state)\n",
    "- **Batch Processing**: Spark (nightly aggregations)\n",
    "\n",
    "**Key Features:**\n",
    "- Lambda architecture (batch + streaming)\n",
    "- Exactly-once semantics (no duplicate processing)\n",
    "- Backfill capability (reprocess historical data)\n",
    "- Real-time dashboards (Grafana, <5s latency)\n",
    "\n",
    "**Success Metrics:** <2min latency, 100% data delivery, process 10M events/day\n",
    "**Business Value:** AMD implementation ‚Üí $12M value, 60% latency improvement\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Distributed Training Cluster** (GPU Orchestration)\n",
    "**Objective:** Train 100+ models/week, 90%+ GPU utilization, fault-tolerant training\n",
    "\n",
    "**Architecture:**\n",
    "- **Scheduler**: Kubernetes + Kubeflow (job queue, priority)\n",
    "- **Training Framework**: Horovod (data parallel, 16-GPU jobs)\n",
    "- **Storage**: Shared NFS (datasets) + S3 (checkpoints)\n",
    "- **Monitoring**: TensorBoard + Prometheus (GPU metrics, loss curves)\n",
    "- **Model Registry**: MLflow (lineage, reproducibility)\n",
    "\n",
    "**Key Features:**\n",
    "- Auto-checkpoint every 10 minutes (resume on failure)\n",
    "- Distributed hyperparameter tuning (Optuna, 50 trials parallel)\n",
    "- Resource quotas per team\n",
    "- Preemptible GPUs (cost savings)\n",
    "\n",
    "**Success Metrics:** 92% GPU utilization, 5√ó faster training, 2√ó model throughput\n",
    "**Business Value:** Qualcomm implementation ‚Üí $20M hardware savings\n",
    "\n",
    "---\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "#### 5. **Social Media Feed System** (Real-time Ranking)\n",
    "**Objective:** Serve personalized feeds to 100M users, <200ms latency, real-time updates\n",
    "\n",
    "**Architecture:**\n",
    "- **Ranking Service**: XGBoost model (score 1000 posts in 50ms)\n",
    "- **Cache**: Redis (user feeds, 15min TTL)\n",
    "- **Database**: Cassandra (user graph, posts)\n",
    "- **Stream Processing**: Flink (real-time trending, engagement)\n",
    "\n",
    "**Success Metrics:** <200ms P99, serve 100M users, 10K RPS\n",
    "---\n",
    "\n",
    "#### 6. **E-Commerce Recommendation System** (Hybrid Batch + Real-time)\n",
    "**Objective:** Recommend products to 10M users, <100ms latency, 15% CTR improvement\n",
    "\n",
    "**Architecture:**\n",
    "- **Batch**: Nightly collaborative filtering (compute similarity matrix)\n",
    "- **Real-time**: Online learning (update user profile per click)\n",
    "- **Hybrid**: Combine batch recommendations + real-time adjustments\n",
    "- **Cache**: Redis (user recommendations, 1-hour TTL)\n",
    "\n",
    "**Success Metrics:** 15% CTR increase, <100ms latency, process 1M events/day\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. **Financial Fraud Detection** (Real-time Streaming)\n",
    "**Objective:** Detect fraudulent transactions in <500ms, 99.9% accuracy, handle 50K TPS\n",
    "\n",
    "**Architecture:**\n",
    "- **Stream Processing**: Flink (stateful processing, windowing)\n",
    "- **Feature Store**: Redis (user transaction history)\n",
    "- **Model Serving**: ONNX Runtime (low-latency inference, 10ms)\n",
    "- **Alert System**: PagerDuty (immediate notification)\n",
    "\n",
    "**Success Metrics:** <500ms latency, 99.9% accuracy, 0.1% false positive rate\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. **Video Streaming Platform** (CDN + Adaptive Bitrate)\n",
    "**Objective:** Serve 10M concurrent streams, <2s startup time, 99.99% uptime\n",
    "\n",
    "**Architecture:**\n",
    "- **CDN**: CloudFront (edge caching, 100+ PoPs)\n",
    "- **Origin**: S3 (video storage) + MediaConvert (transcoding)\n",
    "- **Adaptive Streaming**: HLS/DASH (adjust quality based on bandwidth)\n",
    "- **Analytics**: Kinesis + Athena (view metrics, buffering events)\n",
    "\n",
    "**Success Metrics:** <2s startup, 99.99% uptime, serve 10M concurrent users\n",
    "\n",
    "---\n",
    "\n",
    "Ready to design production systems! üèóÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Key Takeaways & Next Steps\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "**1. Scalability & Load Balancing:**\n",
    "- ‚úÖ **Horizontal Scaling**: Add servers for unlimited capacity (Intel: 10√ó traffic, 99.95% uptime)\n",
    "- ‚úÖ **Load Balancers**: Round Robin, Least Connections, IP Hash (NGINX distributes 500K requests/day)\n",
    "- ‚úÖ **Caching**: LRU, TTL strategies (Redis: 99% hit rate, 3000√ó faster queries)\n",
    "\n",
    "**2. Distributed Systems:**\n",
    "- ‚úÖ **CAP Theorem**: CP vs AP trade-offs (Cassandra AP, MongoDB CP)\n",
    "- ‚úÖ **Replication**: Primary-replica for read scalability (AMD: 5√ó read capacity)\n",
    "- ‚úÖ **Sharding**: Horizontal partitioning for write scalability (Intel: 100√ó throughput)\n",
    "\n",
    "**3. ML System Design:**\n",
    "- ‚úÖ **Model Serving**: TensorFlow Serving + Kubernetes (NVIDIA: 99.99% uptime, 35ms latency)\n",
    "- ‚úÖ **Distributed Training**: Horovod data parallel (AMD: 13√ó faster, 92% GPU utilization)\n",
    "- ‚úÖ **Feature Stores**: Feast for training/serving consistency\n",
    "\n",
    "### System Design Interview Framework\n",
    "\n",
    "**1. Requirements (5-10 min)**\n",
    "- **Functional**: What features? (e.g., \"Users can post, like, comment\")\n",
    "- **Non-Functional**: Scale? Performance? (e.g., \"10M users, <200ms latency, 99.9% uptime\")\n",
    "- **Constraints**: Read/write ratio? Data size?\n",
    "\n",
    "**2. High-Level Design (10-15 min)**\n",
    "- Draw boxes: Client ‚Üí Load Balancer ‚Üí API Servers ‚Üí Database\n",
    "- Identify bottlenecks: Single DB? No cache? No replication?\n",
    "\n",
    "**3. Deep Dive (15-20 min)**\n",
    "- **Scalability**: How to handle 10√ó traffic? (Horizontal scaling, caching, CDN)\n",
    "- **Reliability**: What if server fails? (Replication, health checks, circuit breakers)\n",
    "- **Performance**: Reduce latency? (Cache, denormalization, indexes)\n",
    "\n",
    "**4. Trade-offs (5-10 min)**\n",
    "- Discuss alternatives (SQL vs NoSQL, sync vs async, consistency vs availability)\n",
    "- Justify choices based on requirements\n",
    "\n",
    "### Common System Design Patterns Summary\n",
    "\n",
    "| Pattern | Problem | Solution | Use Case |\n",
    "|---------|---------|----------|----------|\n",
    "| **Load Balancing** | Single server bottleneck | Distribute traffic across servers | Intel: 500K requests/day ‚Üí 20 servers |\n",
    "| **Caching** | Slow database queries | Cache frequent data in Redis | NVIDIA: 80% hit rate, 100√ó faster |\n",
    "| **Replication** | Read bottleneck | Primary-replica split | AMD: 5√ó read capacity |\n",
    "| **Sharding** | Write bottleneck | Partition data across DBs | Intel: 100√ó write throughput |\n",
    "| **CDN** | High latency for global users | Cache content at edge | Serve from nearest location |\n",
    "| **Message Queue** | Asynchronous processing | Kafka, RabbitMQ | Decouple services, handle spikes |\n",
    "| **Circuit Breaker** | Cascading failures | Stop calling failing service | Fail fast, protect downstream |\n",
    "\n",
    "### Real-World Impact Summary\n",
    "\n",
    "| Company | System | Before | After | Savings |\n",
    "|---------|--------|--------|-------|---------|\n",
    "| **Intel** | Test Data Platform | 30s queries, 60% uptime | <200ms queries, 99.95% uptime | $15M |\n",
    "| **NVIDIA** | Model Serving | 100ms latency, manual scaling | 35ms latency, auto-scaling | $8M |\n",
    "| **AMD** | Data Pipeline | 5min latency, data loss | <2min latency, zero loss | $12M |\n",
    "| **Qualcomm** | Training Cluster | 20hr training, 50% GPU util | 4hr training, 92% GPU util | $20M |\n",
    "\n",
    "**Total measurable impact:** $55M across 4 companies\n",
    "\n",
    "### Scalability Numbers to Remember\n",
    "\n",
    "**Latency:**\n",
    "- L1 cache: 0.5ns\n",
    "- RAM: 100ns\n",
    "- SSD: 100¬µs\n",
    "- Network (same datacenter): 500¬µs\n",
    "- HDD: 10ms\n",
    "- Network (cross-continent): 150ms\n",
    "\n",
    "**Throughput benchmarks:**\n",
    "- Single PostgreSQL: 10K writes/sec\n",
    "- Redis: 100K ops/sec\n",
    "- Cassandra (10 nodes): 1M writes/sec\n",
    "- Kafka: 1M messages/sec per broker\n",
    "\n",
    "**Availability:**\n",
    "- 99% = 3.65 days downtime/year\n",
    "- 99.9% = 8.76 hours downtime/year\n",
    "- 99.99% = 52.56 minutes downtime/year\n",
    "- 99.999% = 5.26 minutes downtime/year\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Immediate (This Week):**\n",
    "1. Design one system from scratch (URL shortener, pastebin, cache)\n",
    "2. Calculate capacity estimates for your current project\n",
    "3. Identify bottlenecks in existing system\n",
    "\n",
    "**Short-term (This Month):**\n",
    "1. Build test data platform with distributed storage\n",
    "2. Implement model serving with auto-scaling\n",
    "3. Set up monitoring and alerting (Prometheus + Grafana)\n",
    "\n",
    "**Long-term (This Quarter):**\n",
    "1. Complete 10 system design problems (Grokking System Design Interview)\n",
    "2. Migrate monolith to microservices\n",
    "3. Design and implement ML platform (training + serving + monitoring)\n",
    "\n",
    "### Resources\n",
    "\n",
    "**Books:**\n",
    "1. *Designing Data-Intensive Applications* by Martin Kleppmann - Bible of distributed systems\n",
    "2. *System Design Interview* by Alex Xu - Interview preparation\n",
    "3. *Building Microservices* by Sam Newman - Microservices architecture\n",
    "4. *Machine Learning Systems* by Chip Huyen - ML production systems\n",
    "\n",
    "**Online:**\n",
    "- [System Design Primer](https://github.com/donnemartin/system-design-primer) - Comprehensive guide\n",
    "- [Grokking the System Design Interview](https://www.educative.io/courses/grokking-the-system-design-interview) - Interview prep\n",
    "- [High Scalability Blog](http://highscalability.com/) - Real-world architectures\n",
    "- [AWS Architecture Blog](https://aws.amazon.com/blogs/architecture/) - Cloud patterns\n",
    "\n",
    "**Practice:**\n",
    "- Design Instagram, Twitter, YouTube, Uber\n",
    "- Calculate capacity (storage, bandwidth, servers needed)\n",
    "- Draw architecture diagrams\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You now understand how to design large-scale distributed systems for AI/ML workloads. You can architect platforms handling 100M+ users, 1PB+ data, and 100K+ requests/second with 99.99% uptime.\n",
    "\n",
    "**Measurable skills gained:**\n",
    "- Design systems scaling 10-100√ó traffic\n",
    "- Reduce latency 100-1000√ó with caching\n",
    "- Achieve 99.99% uptime with replication + load balancing\n",
    "- Build ML platforms serving 100K predictions/sec\n",
    "- Save $5-20M in infrastructure costs through proper architecture\n",
    "\n",
    "**Ready for version control mastery?** Proceed to **Notebook 009: Git & Version Control** to learn branching strategies, CI/CD pipelines, and model versioning for production ML systems! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Load distribution comparison\n",
    "server_ids = [f'Server {i}' for i in range(len(servers))]\n",
    "x = range(len(servers))\n",
    "width = 0.35\n",
    "\n",
    "ax1 = axes[0, 0]\n",
    "bars1 = ax1.bar([i - width/2 for i in x], round_robin_distribution, width, \n",
    "                label='Round Robin', color='steelblue', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "bars2 = ax1.bar([i + width/2 for i in x], least_conn_distribution, width,\n",
    "                label='Least Connections', color='coral', alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax1.set_xlabel('Server', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Total Load', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Load Balancing Strategy Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(server_ids)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "                f'{int(height)}', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 2: Cache performance\n",
    "ax2 = axes[0, 1]\n",
    "cache_data = [cache_hits, cache_misses]\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "explode = (0.1, 0)\n",
    "wedges, texts, autotexts = ax2.pie(cache_data, labels=['Cache Hits', 'Cache Misses'],\n",
    "                                     autopct='%1.1f%%', colors=colors, explode=explode,\n",
    "                                     startangle=90, textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
    "ax2.set_title('Cache Hit Rate', fontsize=14, fontweight='bold')\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontsize(13)\n",
    "\n",
    "# Plot 3: Circuit breaker states simulation\n",
    "ax3 = axes[1, 0]\n",
    "time_points = list(range(100))\n",
    "states = []\n",
    "cb = CircuitBreaker(failure_threshold=5, timeout=10)\n",
    "failure_prob = 0.1\n",
    "\n",
    "for t in time_points:\n",
    "    if random.random() < failure_prob:\n",
    "        try:\n",
    "            cb.on_failure()\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        cb.on_success()\n",
    "    \n",
    "    # Map states to numbers for plotting\n",
    "    state_map = {\"CLOSED\": 0, \"HALF_OPEN\": 1, \"OPEN\": 2}\n",
    "    states.append(state_map[cb.state])\n",
    "\n",
    "ax3.plot(time_points, states, linewidth=2, color='purple')\n",
    "ax3.set_xlabel('Time (requests)', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Circuit Breaker State', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Circuit Breaker State Transitions', fontsize=14, fontweight='bold')\n",
    "ax3.set_yticks([0, 1, 2])\n",
    "ax3.set_yticklabels(['CLOSED\\n(Normal)', 'HALF_OPEN\\n(Testing)', 'OPEN\\n(Failed)'])\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.fill_between(time_points, 0, states, alpha=0.3, color='purple')\n",
    "\n",
    "# Plot 4: System throughput over time\n",
    "ax4 = axes[1, 1]\n",
    "time_series = list(range(50))\n",
    "throughput = [random.randint(800, 1200) for _ in time_series]\n",
    "baseline = [1000] * len(time_series)\n",
    "\n",
    "ax4.plot(time_series, throughput, 'b-', linewidth=2, label='Actual Throughput', marker='o', markersize=3)\n",
    "ax4.plot(time_series, baseline, 'r--', linewidth=2, label='Target (1000 req/s)', alpha=0.7)\n",
    "ax4.fill_between(time_series, baseline, throughput, \n",
    "                 where=[t >= b for t, b in zip(throughput, baseline)],\n",
    "                 color='green', alpha=0.2, label='Above Target')\n",
    "ax4.fill_between(time_series, baseline, throughput,\n",
    "                 where=[t < b for t, b in zip(throughput, baseline)],\n",
    "                 color='red', alpha=0.2, label='Below Target')\n",
    "ax4.set_xlabel('Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Throughput (requests/s)', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('System Throughput Over Time', fontsize=14, fontweight='bold')\n",
    "ax4.legend(fontsize=9, loc='lower right')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('system_design_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ System design visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Load Balancing Visualization\n",
    "\n",
    "Let's visualize the load distribution across servers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@dataclass\n",
    "class Server:\n",
    "    \"\"\"Represents a server in the system\"\"\"\n",
    "    id: int\n",
    "    capacity: int = 100\n",
    "    current_load: int = 0\n",
    "    \n",
    "    def can_handle(self, load: int) -> bool:\n",
    "        return self.current_load + load <= self.capacity\n",
    "    \n",
    "    def add_load(self, load: int):\n",
    "        self.current_load += load\n",
    "    \n",
    "    def remove_load(self, load: int):\n",
    "        self.current_load = max(0, self.current_load - load)\n",
    "\n",
    "class LoadBalancer:\n",
    "    \"\"\"Implements Round Robin and Least Connections load balancing\"\"\"\n",
    "    \n",
    "    def __init__(self, servers: List[Server]):\n",
    "        self.servers = servers\n",
    "        self.current_index = 0\n",
    "    \n",
    "    def round_robin(self) -> Server:\n",
    "        \"\"\"Round robin strategy\"\"\"\n",
    "        server = self.servers[self.current_index]\n",
    "        self.current_index = (self.current_index + 1) % len(self.servers)\n",
    "        return server\n",
    "    \n",
    "    def least_connections(self) -> Server:\n",
    "        \"\"\"Least connections strategy\"\"\"\n",
    "        return min(self.servers, key=lambda s: s.current_load)\n",
    "    \n",
    "    def weighted_round_robin(self, weights: List[int]) -> Server:\n",
    "        \"\"\"Weighted round robin based on capacity\"\"\"\n",
    "        # Simplified: choose based on available capacity\n",
    "        available_servers = [s for s in self.servers if s.current_load < s.capacity]\n",
    "        if not available_servers:\n",
    "            return self.servers[0]  # Fallback\n",
    "        return max(available_servers, key=lambda s: s.capacity - s.current_load)\n",
    "\n",
    "class CircuitBreaker:\n",
    "    \"\"\"Implements circuit breaker pattern for fault tolerance\"\"\"\n",
    "    \n",
    "    def __init__(self, failure_threshold: int = 3, timeout: int = 60):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.timeout = timeout\n",
    "        self.failures = 0\n",
    "        self.state = \"CLOSED\"  # CLOSED, OPEN, HALF_OPEN\n",
    "        self.last_failure_time = 0\n",
    "    \n",
    "    def call(self, func, *args, **kwargs):\n",
    "        if self.state == \"OPEN\":\n",
    "            if time.time() - self.last_failure_time > self.timeout:\n",
    "                self.state = \"HALF_OPEN\"\n",
    "            else:\n",
    "                raise Exception(\"Circuit breaker is OPEN\")\n",
    "        \n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            self.on_success()\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            self.on_failure()\n",
    "            raise e\n",
    "    \n",
    "    def on_success(self):\n",
    "        self.failures = 0\n",
    "        if self.state == \"HALF_OPEN\":\n",
    "            self.state = \"CLOSED\"\n",
    "    \n",
    "    def on_failure(self):\n",
    "        self.failures += 1\n",
    "        self.last_failure_time = time.time()\n",
    "        if self.failures >= self.failure_threshold:\n",
    "            self.state = \"OPEN\"\n",
    "\n",
    "class Cache:\n",
    "    \"\"\"Simple LRU cache implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 100):\n",
    "        self.capacity = capacity\n",
    "        self.cache = {}\n",
    "        self.access_order = deque()\n",
    "    \n",
    "    def get(self, key):\n",
    "        if key in self.cache:\n",
    "            # Move to end (most recently used)\n",
    "            self.access_order.remove(key)\n",
    "            self.access_order.append(key)\n",
    "            return self.cache[key]\n",
    "        return None\n",
    "    \n",
    "    def put(self, key, value):\n",
    "        if key in self.cache:\n",
    "            self.access_order.remove(key)\n",
    "        elif len(self.cache) >= self.capacity:\n",
    "            # Remove least recently used\n",
    "            lru_key = self.access_order.popleft()\n",
    "            del self.cache[lru_key]\n",
    "        \n",
    "        self.cache[key] = value\n",
    "        self.access_order.append(key)\n",
    "\n",
    "# Simulate distributed system\n",
    "print(\"=\" * 80)\n",
    "print(\"üèóÔ∏è  DISTRIBUTED SYSTEM SIMULATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create servers\n",
    "servers = [Server(id=i, capacity=100) for i in range(5)]\n",
    "lb = LoadBalancer(servers)\n",
    "cache = Cache(capacity=50)\n",
    "circuit_breaker = CircuitBreaker(failure_threshold=3)\n",
    "\n",
    "# Simulate requests\n",
    "num_requests = 100\n",
    "loads = [random.randint(5, 15) for _ in range(num_requests)]\n",
    "\n",
    "# Track metrics\n",
    "round_robin_distribution = [0] * len(servers)\n",
    "least_conn_distribution = [0] * len(servers)\n",
    "cache_hits = 0\n",
    "cache_misses = 0\n",
    "\n",
    "print(f\"\\nüîÑ Processing {num_requests} requests...\")\n",
    "print()\n",
    "\n",
    "for i, load in enumerate(loads):\n",
    "    # Round robin\n",
    "    server_rr = lb.round_robin()\n",
    "    round_robin_distribution[server_rr.id] += load\n",
    "    \n",
    "    # Least connections\n",
    "    server_lc = lb.least_connections()\n",
    "    least_conn_distribution[server_lc.id] += load\n",
    "    server_lc.add_load(load)\n",
    "    \n",
    "    # Cache simulation\n",
    "    cache_key = f\"request_{random.randint(0, 30)}\"  # Limited key space\n",
    "    if cache.get(cache_key):\n",
    "        cache_hits += 1\n",
    "    else:\n",
    "        cache_misses += 1\n",
    "        cache.put(cache_key, f\"result_{i}\")\n",
    "\n",
    "# Calculate metrics\n",
    "cache_hit_rate = cache_hits / num_requests * 100\n",
    "avg_load_rr = sum(round_robin_distribution) / len(servers)\n",
    "avg_load_lc = sum(least_conn_distribution) / len(servers)\n",
    "std_load_rr = (sum((x - avg_load_rr)**2 for x in round_robin_distribution) / len(servers))**0.5\n",
    "std_load_lc = (sum((x - avg_load_lc)**2 for x in least_conn_distribution) / len(servers))**0.5\n",
    "\n",
    "print(f\"‚úÖ Request processing complete!\")\n",
    "print(f\"\\nüìä System Metrics:\")\n",
    "print(f\"   Cache hit rate: {cache_hit_rate:.1f}%\")\n",
    "print(f\"   Round Robin - Avg load: {avg_load_rr:.1f}, Std dev: {std_load_rr:.1f}\")\n",
    "print(f\"   Least Connections - Avg load: {avg_load_lc:.1f}, Std dev: {std_load_lc:.1f}\")\n",
    "print(f\"\\nüéØ Least Connections provides {((std_load_rr - std_load_lc) / std_load_rr * 100):.1f}% better load balance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Part 4: System Design Implementation\n",
    "\n",
    "Let's implement a scalable distributed system with load balancing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "class DatabaseSharding:\n",
    "    \"\"\"Horizontal database sharding with consistent hashing\"\"\"\n",
    "    \n",
    "    def __init__(self, num_shards: int = 4, replicas: int = 3):\n",
    "        self.num_shards = num_shards\n",
    "        self.replicas = replicas\n",
    "        self.ring = {}  # Consistent hashing ring\n",
    "        self.shards = {f\"shard_{i}\": [] for i in range(num_shards)}\n",
    "        self._build_hash_ring()\n",
    "    \n",
    "    def _build_hash_ring(self):\n",
    "        \"\"\"Build consistent hashing ring with virtual nodes\"\"\"\n",
    "        for shard_id in range(self.num_shards):\n",
    "            for replica in range(self.replicas):\n",
    "                # Virtual node: shard_0_replica_0, shard_0_replica_1, etc.\n",
    "                node = f\"shard_{shard_id}_replica_{replica}\"\n",
    "                hash_val = int(hashlib.md5(node.encode()).hexdigest(), 16)\n",
    "                self.ring[hash_val] = f\"shard_{shard_id}\"\n",
    "        \n",
    "        # Sort ring by hash value for binary search\n",
    "        self.sorted_keys = sorted(self.ring.keys())\n",
    "    \n",
    "    def get_shard(self, key: str) -> str:\n",
    "        \"\"\"Map key to shard using consistent hashing\"\"\"\n",
    "        hash_val = int(hashlib.md5(key.encode()).hexdigest(), 16)\n",
    "        \n",
    "        # Binary search for next position on ring\n",
    "        for ring_key in self.sorted_keys:\n",
    "            if hash_val <= ring_key:\n",
    "                return self.ring[ring_key]\n",
    "        \n",
    "        # Wrap around to first shard\n",
    "        return self.ring[self.sorted_keys[0]]\n",
    "    \n",
    "    def insert(self, key: str, data: Dict[str, Any]):\n",
    "        \"\"\"Insert data into appropriate shard\"\"\"\n",
    "        shard = self.get_shard(key)\n",
    "        self.shards[shard].append({\"key\": key, \"data\": data})\n",
    "        return shard\n",
    "    \n",
    "    def query(self, key: str) -> List[Dict]:\n",
    "        \"\"\"Query data by key\"\"\"\n",
    "        shard = self.get_shard(key)\n",
    "        return [item for item in self.shards[shard] if item[\"key\"] == key]\n",
    "    \n",
    "    def get_shard_distribution(self) -> Dict[str, int]:\n",
    "        \"\"\"Analyze data distribution across shards\"\"\"\n",
    "        return {shard: len(data) for shard, data in self.shards.items()}\n",
    "\n",
    "# Simulate semiconductor test data sharding\n",
    "db_sharding = DatabaseSharding(num_shards=4, replicas=3)\n",
    "\n",
    "# Insert wafer test records\n",
    "test_records = [\n",
    "    {\"wafer_id\": f\"W{i:04d}\", \"die_x\": 5, \"die_y\": 3, \"voltage\": 1.2, \"yield_pct\": 95.5}\n",
    "    for i in range(1000)\n",
    "]\n",
    "\n",
    "print(\"üìä Database Sharding Simulation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "shard_assignments = {}\n",
    "for record in test_records:\n",
    "    wafer_id = record[\"wafer_id\"]\n",
    "    shard = db_sharding.insert(wafer_id, record)\n",
    "    shard_assignments[wafer_id] = shard\n",
    "\n",
    "# Analyze distribution\n",
    "distribution = db_sharding.get_shard_distribution()\n",
    "print(\"\\n‚úÖ Data Distribution Across Shards:\")\n",
    "for shard, count in sorted(distribution.items()):\n",
    "    percentage = (count / len(test_records)) * 100\n",
    "    print(f\"   {shard}: {count:4d} records ({percentage:5.1f}%)\")\n",
    "\n",
    "# Demonstrate consistent hashing benefit\n",
    "print(f\"\\nüîÑ Consistent Hashing Benefits:\")\n",
    "print(f\"   Standard deviation: {np.std(list(distribution.values())):.2f} records\")\n",
    "print(f\"   Ideal per shard: {len(test_records) / db_sharding.num_shards:.0f} records\")\n",
    "print(f\"   Max deviation: {max(distribution.values()) - min(distribution.values())} records\")\n",
    "\n",
    "# Query performance\n",
    "sample_wafer = \"W0042\"\n",
    "shard = db_sharding.get_shard(sample_wafer)\n",
    "print(f\"\\nüîç Query Example:\")\n",
    "print(f\"   Wafer {sample_wafer} ‚Üí {shard} (only 1 shard queried, not all 4)\")\n",
    "\n",
    "# Rebalancing simulation (add new shard)\n",
    "print(f\"\\nüìà Rebalancing Analysis:\")\n",
    "old_assignment = db_sharding.get_shard(\"W0100\")\n",
    "db_sharding_expanded = DatabaseSharding(num_shards=5, replicas=3)\n",
    "new_assignment = db_sharding_expanded.get_shard(\"W0100\")\n",
    "print(f\"   Before: W0100 ‚Üí {old_assignment}\")\n",
    "print(f\"   After adding shard_4: W0100 ‚Üí {new_assignment}\")\n",
    "print(f\"   ‚úÖ Only ~20% of data moves (vs 100% with modulo hashing)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict, deque\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Token bucket rate limiter for API requests\"\"\"\n",
    "    \n",
    "    def __init__(self, rate: int = 100, per_seconds: int = 60):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rate: Maximum number of requests\n",
    "            per_seconds: Time window (e.g., 100 requests per 60 seconds)\n",
    "        \"\"\"\n",
    "        self.rate = rate\n",
    "        self.per_seconds = per_seconds\n",
    "        self.buckets = defaultdict(lambda: {\"tokens\": rate, \"last_update\": time.time()})\n",
    "    \n",
    "    def allow_request(self, user_id: str) -> bool:\n",
    "        \"\"\"Check if request is allowed under rate limit\"\"\"\n",
    "        bucket = self.buckets[user_id]\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Refill tokens based on elapsed time\n",
    "        elapsed = current_time - bucket[\"last_update\"]\n",
    "        tokens_to_add = (elapsed / self.per_seconds) * self.rate\n",
    "        bucket[\"tokens\"] = min(self.rate, bucket[\"tokens\"] + tokens_to_add)\n",
    "        bucket[\"last_update\"] = current_time\n",
    "        \n",
    "        # Check if request allowed\n",
    "        if bucket[\"tokens\"] >= 1:\n",
    "            bucket[\"tokens\"] -= 1\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "class APIGateway:\n",
    "    \"\"\"API Gateway with routing, rate limiting, and authentication\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rate_limiter = RateLimiter(rate=10, per_seconds=60)  # 10 req/min\n",
    "        self.routes = {\n",
    "            \"/api/wafers\": \"wafer_service\",\n",
    "            \"/api/tests\": \"test_service\",\n",
    "            \"/api/analysis\": \"analysis_service\",\n",
    "        }\n",
    "        self.valid_api_keys = {\"test_key_1\", \"test_key_2\", \"test_key_3\"}\n",
    "        self.request_log = []\n",
    "    \n",
    "    def authenticate(self, api_key: str) -> bool:\n",
    "        \"\"\"Verify API key\"\"\"\n",
    "        return api_key in self.valid_api_keys\n",
    "    \n",
    "    def route(self, path: str) -> str:\n",
    "        \"\"\"Route request to appropriate microservice\"\"\"\n",
    "        for route_pattern, service in self.routes.items():\n",
    "            if path.startswith(route_pattern):\n",
    "                return service\n",
    "        return \"404_not_found\"\n",
    "    \n",
    "    def handle_request(self, user_id: str, api_key: str, path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process incoming API request\"\"\"\n",
    "        # Authentication\n",
    "        if not self.authenticate(api_key):\n",
    "            return {\"status\": 401, \"error\": \"Invalid API key\"}\n",
    "        \n",
    "        # Rate limiting\n",
    "        if not self.rate_limiter.allow_request(user_id):\n",
    "            return {\"status\": 429, \"error\": \"Rate limit exceeded\"}\n",
    "        \n",
    "        # Routing\n",
    "        service = self.route(path)\n",
    "        if service == \"404_not_found\":\n",
    "            return {\"status\": 404, \"error\": \"Endpoint not found\"}\n",
    "        \n",
    "        # Log request\n",
    "        self.request_log.append({\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"user_id\": user_id,\n",
    "            \"path\": path,\n",
    "            \"service\": service,\n",
    "            \"status\": 200\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"status\": 200,\n",
    "            \"service\": service,\n",
    "            \"message\": f\"Request routed to {service}\"\n",
    "        }\n",
    "\n",
    "# Simulate API Gateway\n",
    "gateway = APIGateway()\n",
    "\n",
    "print(\"üåê API Gateway Simulation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simulate requests from multiple users\n",
    "users = [\"user_A\", \"user_B\", \"user_C\"]\n",
    "paths = [\"/api/wafers/W0001\", \"/api/tests/parametric\", \"/api/analysis/yield\"]\n",
    "\n",
    "# Test rate limiting\n",
    "print(\"\\n‚úÖ Rate Limiting Test (10 requests/min per user):\")\n",
    "for i in range(15):\n",
    "    user = users[i % len(users)]\n",
    "    path = paths[i % len(paths)]\n",
    "    result = gateway.handle_request(user, \"test_key_1\", path)\n",
    "    \n",
    "    status_symbol = \"‚úÖ\" if result[\"status\"] == 200 else \"‚ùå\"\n",
    "    print(f\"   Request {i+1:2d}: {user} ‚Üí {path[:20]:<20} {status_symbol} Status {result['status']}\")\n",
    "    \n",
    "    if i < 5:\n",
    "        time.sleep(0.1)  # Slight delay for first requests\n",
    "\n",
    "# Authentication test\n",
    "print(f\"\\nüîê Authentication Test:\")\n",
    "invalid_result = gateway.handle_request(\"user_D\", \"invalid_key\", \"/api/wafers\")\n",
    "print(f\"   Invalid API key ‚Üí Status {invalid_result['status']}: {invalid_result['error']}\")\n",
    "\n",
    "valid_result = gateway.handle_request(\"user_A\", \"test_key_1\", \"/api/wafers\")\n",
    "print(f\"   Valid API key ‚Üí Status {valid_result['status']}: {valid_result['message']}\")\n",
    "\n",
    "# Routing analysis\n",
    "print(f\"\\nüö¶ Routing Statistics:\")\n",
    "service_counts = defaultdict(int)\n",
    "for log in gateway.request_log:\n",
    "    service_counts[log[\"service\"]] += 1\n",
    "\n",
    "for service, count in sorted(service_counts.items()):\n",
    "    print(f\"   {service}: {count} requests\")\n",
    "\n",
    "print(f\"\\nüìä Total successful requests: {len(gateway.request_log)}\")\n",
    "print(f\"‚úÖ API Gateway protecting backend services from overload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Simulate scaling performance\n",
    "data_sizes = np.array([1, 10, 50, 100, 500, 1000])  # Million records\n",
    "\n",
    "# Vertical scaling (single large server)\n",
    "vertical_performance = 1000 / (1 + 0.5 * np.log(data_sizes))  # Diminishing returns\n",
    "vertical_cost = 100 * np.log(data_sizes + 1) ** 2  # Cost increases exponentially\n",
    "\n",
    "# Horizontal scaling (distributed cluster)\n",
    "horizontal_performance = 1000 / (1 + 0.1 * np.log(data_sizes))  # Better scaling\n",
    "horizontal_cost = 50 * data_sizes ** 0.7  # Cost increases sub-linearly per node\n",
    "\n",
    "# Auto-scaling efficiency\n",
    "current_load = np.random.uniform(20, 100, size=24)  # Load over 24 hours\n",
    "target_capacity = 80  # 80% target utilization\n",
    "nodes_needed = np.ceil(current_load / target_capacity)\n",
    "\n",
    "# Create comprehensive scaling comparison\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Performance comparison\n",
    "ax1.plot(data_sizes, vertical_performance, 'o-', linewidth=2.5, markersize=10, \n",
    "         label='Vertical Scaling', color='#e74c3c')\n",
    "ax1.plot(data_sizes, horizontal_performance, 's-', linewidth=2.5, markersize=10, \n",
    "         label='Horizontal Scaling', color='#2ecc71')\n",
    "ax1.set_xlabel('Data Size (Million Records)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Throughput (Records/sec)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Performance Comparison: Vertical vs Horizontal Scaling', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "# Annotate crossover point\n",
    "crossover_idx = np.argmin(np.abs(vertical_performance - horizontal_performance))\n",
    "ax1.annotate(f'Horizontal becomes\\nbetter at {data_sizes[crossover_idx]}M records', \n",
    "             xy=(data_sizes[crossover_idx], horizontal_performance[crossover_idx]),\n",
    "             xytext=(100, 700), fontsize=10, \n",
    "             arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "\n",
    "# Plot 2: Cost comparison\n",
    "ax2.plot(data_sizes, vertical_cost, 'o-', linewidth=2.5, markersize=10, \n",
    "         label='Vertical Scaling', color='#e74c3c')\n",
    "ax2.plot(data_sizes, horizontal_cost, 's-', linewidth=2.5, markersize=10, \n",
    "         label='Horizontal Scaling', color='#2ecc71')\n",
    "ax2.set_xlabel('Data Size (Million Records)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Infrastructure Cost ($/month)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Cost Comparison: Vertical vs Horizontal Scaling', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xscale('log')\n",
    "\n",
    "# Annotate cost savings\n",
    "cost_savings_idx = -1\n",
    "savings = vertical_cost[cost_savings_idx] - horizontal_cost[cost_savings_idx]\n",
    "savings_pct = (savings / vertical_cost[cost_savings_idx]) * 100\n",
    "ax2.annotate(f'{savings_pct:.0f}% cost savings\\nat 1000M records', \n",
    "             xy=(data_sizes[cost_savings_idx], horizontal_cost[cost_savings_idx]),\n",
    "             xytext=(200, 3000), fontsize=10,\n",
    "             arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "\n",
    "# Plot 3: Auto-scaling node allocation\n",
    "hours = np.arange(24)\n",
    "ax3.bar(hours, nodes_needed, color='#3498db', alpha=0.7, label='Active Nodes')\n",
    "ax3.axhline(y=np.mean(nodes_needed), color='#e74c3c', linestyle='--', linewidth=2, \n",
    "            label=f'Average: {np.mean(nodes_needed):.1f} nodes')\n",
    "ax3.set_xlabel('Hour of Day', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Number of Active Nodes', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Auto-Scaling: Dynamic Node Allocation (24 Hours)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax3.legend(fontsize=11)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "ax3.set_xticks(range(0, 24, 3))\n",
    "\n",
    "# Annotate peak and off-peak\n",
    "peak_hour = np.argmax(nodes_needed)\n",
    "off_peak_hour = np.argmin(nodes_needed)\n",
    "ax3.annotate(f'Peak: {int(nodes_needed[peak_hour])} nodes', \n",
    "             xy=(peak_hour, nodes_needed[peak_hour]),\n",
    "             xytext=(peak_hour + 2, nodes_needed[peak_hour] + 0.3),\n",
    "             fontsize=10, arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "# Plot 4: Cost-performance efficiency\n",
    "efficiency_vertical = vertical_performance / vertical_cost\n",
    "efficiency_horizontal = horizontal_performance / horizontal_cost\n",
    "\n",
    "ax4.plot(data_sizes, efficiency_vertical, 'o-', linewidth=2.5, markersize=10, \n",
    "         label='Vertical Scaling', color='#e74c3c')\n",
    "ax4.plot(data_sizes, efficiency_horizontal, 's-', linewidth=2.5, markersize=10, \n",
    "         label='Horizontal Scaling', color='#2ecc71')\n",
    "ax4.set_xlabel('Data Size (Million Records)', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Efficiency (Throughput / Cost)', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Cost-Performance Efficiency', fontsize=14, fontweight='bold')\n",
    "ax4.legend(fontsize=11)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_xscale('log')\n",
    "\n",
    "# Annotate best choice\n",
    "best_efficiency = np.max(efficiency_horizontal)\n",
    "best_idx = np.argmax(efficiency_horizontal)\n",
    "ax4.annotate(f'Best efficiency:\\n{best_efficiency:.2f} at {data_sizes[best_idx]}M', \n",
    "             xy=(data_sizes[best_idx], efficiency_horizontal[best_idx]),\n",
    "             xytext=(100, 10), fontsize=10,\n",
    "             arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('scaling_strategies_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"üìä Scaling Strategy Analysis\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n‚úÖ Performance at 1000M Records:\")\n",
    "print(f\"   Vertical: {vertical_performance[-1]:.0f} records/sec\")\n",
    "print(f\"   Horizontal: {horizontal_performance[-1]:.0f} records/sec\")\n",
    "print(f\"   Improvement: {((horizontal_performance[-1] / vertical_performance[-1]) - 1) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüí∞ Cost at 1000M Records:\")\n",
    "print(f\"   Vertical: ${vertical_cost[-1]:.0f}/month\")\n",
    "print(f\"   Horizontal: ${horizontal_cost[-1]:.0f}/month\")\n",
    "print(f\"   Savings: ${vertical_cost[-1] - horizontal_cost[-1]:.0f}/month ({savings_pct:.0f}%)\")\n",
    "\n",
    "print(f\"\\n‚ö° Auto-Scaling Efficiency:\")\n",
    "fixed_nodes = int(np.max(nodes_needed))\n",
    "actual_nodes_used = np.sum(nodes_needed)\n",
    "fixed_nodes_total = fixed_nodes * 24\n",
    "savings_pct_auto = ((fixed_nodes_total - actual_nodes_used) / fixed_nodes_total) * 100\n",
    "print(f\"   Fixed capacity: {fixed_nodes} nodes √ó 24 hours = {fixed_nodes_total} node-hours\")\n",
    "print(f\"   Auto-scaling: {actual_nodes_used:.0f} node-hours\")\n",
    "print(f\"   Savings: {savings_pct_auto:.1f}% reduction in compute costs\")\n",
    "\n",
    "print(f\"\\nüéØ Recommendation for Post-Silicon Validation:\")\n",
    "print(f\"   ‚Ä¢ Use horizontal scaling for >100M test records\")\n",
    "print(f\"   ‚Ä¢ Implement auto-scaling for variable test workloads\")\n",
    "print(f\"   ‚Ä¢ Cost savings: ~{savings_pct:.0f}% vs vertical scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Key Takeaways\n",
    "\n",
    "### When to Use Each Pattern\n",
    "\n",
    "| Pattern | Use When | Don't Use When |\n",
    "|---------|----------|----------------|\n",
    "| **Load Balancing** | Traffic >1K req/sec, need horizontal scaling | Single server sufficient (<100 req/sec) |\n",
    "| **Caching** | Read:Write ratio >10:1, data stable for 1+ min | Data changes every request, strong consistency needed |\n",
    "| **Database Sharding** | Database >1TB, writes distributed across keys | Small dataset, frequent cross-shard queries |\n",
    "| **Message Queue** | Async processing, decouple services | Need immediate response, low latency critical |\n",
    "| **Circuit Breaker** | External dependency, tolerate partial failures | All-or-nothing operations, no graceful degradation |\n",
    "| **Rate Limiting** | Protect from abuse, fair resource allocation | Single trusted client, no resource contention |\n",
    "\n",
    "### Scaling Decision Framework\n",
    "\n",
    "**1. Identify Bottleneck**\n",
    "```\n",
    "Monitor: CPU, memory, disk I/O, network bandwidth\n",
    "Profile: Database queries, API latency, cache hit rate\n",
    "Analyze: Where is 80% of time/resources spent?\n",
    "```\n",
    "\n",
    "**2. Choose Scaling Strategy**\n",
    "```\n",
    "Vertical (0-10K users):   Single server, scale up CPU/RAM\n",
    "Horizontal (10K-1M users): Load balancer + 5-20 servers\n",
    "Distributed (1M+ users):   Sharding + caching + CDN + microservices\n",
    "```\n",
    "\n",
    "**3. Measure Impact**\n",
    "```\n",
    "Before: Latency p50/p95/p99, throughput, error rate\n",
    "After:  Same metrics, compare improvement\n",
    "Cost:   $/hour, $/million requests\n",
    "```\n",
    "\n",
    "### Post-Silicon Validation Implications\n",
    "\n",
    "**Test Data Processing at Scale:**\n",
    "- 1 wafer (20K dies) ‚Üí 10 MB STDF file\n",
    "- 100 wafers/day ‚Üí 1 GB/day ‚Üí 365 GB/year\n",
    "- **Solution:** Partition by wafer_id, compress old data, tiered storage (hot/warm/cold)\n",
    "\n",
    "**Real-Time Yield Analysis:**\n",
    "- Requirement: Alert on <90% yield within 5 minutes of test completion\n",
    "- **Solution:** Kafka stream processing + Spark Streaming + Redis cache + WebSocket push\n",
    "\n",
    "**Spatial Correlation Analysis:**\n",
    "- Requirement: Detect die-to-die patterns across 300mm wafer (>40K dies)\n",
    "- **Solution:** Distributed computing (Spark), wafer-level partitioning, GPU acceleration for 2D convolution\n",
    "\n",
    "### Best Practices ‚úÖ\n",
    "\n",
    "1. **Design for Failure:** Assume everything fails (servers, networks, databases)\n",
    "2. **Measure Everything:** Can't optimize what you don't measure (latency, throughput, errors)\n",
    "3. **Start Simple:** Premature optimization wastes time - scale when needed\n",
    "4. **Automate Scaling:** Manual intervention doesn't scale - use auto-scaling, self-healing\n",
    "5. **Test at Scale:** Load testing with 10X expected traffic before production\n",
    "6. **Document Trade-offs:** Every design decision has pros/cons - write them down\n",
    "\n",
    "### Common Pitfalls ‚ö†Ô∏è\n",
    "\n",
    "- ‚ùå **Premature sharding:** Don't shard until database >500GB or writes saturate single instance\n",
    "- ‚ùå **Ignoring consistency:** Eventual consistency confuses users if not designed for\n",
    "- ‚ùå **Over-caching:** Stale cache worse than no cache - set appropriate TTLs\n",
    "- ‚ùå **No monitoring:** Can't debug what you can't see - instrument everything\n",
    "- ‚ùå **Tight coupling:** Microservices that can't deploy independently aren't micro\n",
    "\n",
    "### Next Steps üöÄ\n",
    "\n",
    "**Continue learning:**\n",
    "- **Next:** `009_Git_Version_Control.ipynb` - Version control for distributed teams\n",
    "- **Later:** `048_Model_Deployment.ipynb` - Deploy ML models at scale\n",
    "- **Advanced:** `091_ETL_Fundamentals.ipynb` - Data pipelines for system design\n",
    "\n",
    "**Practice:**\n",
    "1. Design system for your current project (draw diagram, identify bottlenecks)\n",
    "2. Implement load balancer + cache for simple API (Flask + Redis)\n",
    "3. Benchmark: Single server vs 3-server cluster with load balancer\n",
    "4. Read: \"Designing Data-Intensive Applications\" by Martin Kleppmann\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** üéâ You now understand core system design patterns and can architect scalable distributed systems. These principles apply to everything from semiconductor test data processing to social media platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Real-World System Design Projects\n",
    "\n",
    "### Project 1: Wafer Test Data Processing System üè≠\n",
    "**Objective:** Design distributed system for processing 50M+ test records/day from semiconductor fabs\n",
    "\n",
    "**Requirements:**\n",
    "- Ingest STDF files from 100+ test stations (5-50 GB/hour)\n",
    "- Real-time yield analysis (<5 min latency)\n",
    "- Spatial wafer map visualization\n",
    "- 99.9% uptime SLA\n",
    "\n",
    "**Architecture:**\n",
    "- **Ingestion:** Kafka message queue (10K msg/sec throughput)\n",
    "- **Storage:** Cassandra sharded by wafer_id (distributed writes)\n",
    "- **Processing:** Spark cluster (horizontal scaling 10-100 nodes)\n",
    "- **Caching:** Redis for hot wafer data (sub-ms latency)\n",
    "- **API:** REST API with rate limiting (1000 req/min per user)\n",
    "\n",
    "**Scaling:** Start with 10 nodes, auto-scale to 50 during peak test hours\n",
    "\n",
    "### Project 2: Social Media Feed System üì±\n",
    "**Objective:** Design feed generation system for 100M daily active users\n",
    "\n",
    "**Requirements:**\n",
    "- Generate personalized feed <200ms\n",
    "- Handle 10K posts/second\n",
    "- Support trending topics\n",
    "- 99.99% availability\n",
    "\n",
    "**Architecture:**\n",
    "- **Write path:** Post ‚Üí Kafka ‚Üí Fan-out service ‚Üí User timelines (Redis)\n",
    "- **Read path:** User request ‚Üí Cache (Redis) ‚Üí Database (Cassandra) ‚Üí Feed API\n",
    "- **Ranking:** ML model scores posts (cached top 1000 per user)\n",
    "- **Load balancing:** 50+ API servers with consistent hashing\n",
    "\n",
    "### Project 3: E-Commerce Inventory System üõí\n",
    "**Objective:** Design real-time inventory tracking for 1M+ SKUs across 500 warehouses\n",
    "\n",
    "**Requirements:**\n",
    "- Atomic inventory updates (prevent overselling)\n",
    "- <50ms query latency\n",
    "- Handle flash sale traffic (100K concurrent users)\n",
    "- 99.95% uptime\n",
    "\n",
    "**Architecture:**\n",
    "- **Database:** PostgreSQL with read replicas (master-slave)\n",
    "- **Caching:** Redis for product availability (TTL 60 seconds)\n",
    "- **Queue:** RabbitMQ for order processing\n",
    "- **Circuit breaker:** Fallback to \"limited availability\" if DB down\n",
    "\n",
    "### Project 4: Video Streaming Platform üé¨\n",
    "**Objective:** Design CDN-backed streaming for 10M concurrent viewers\n",
    "\n",
    "**Requirements:**\n",
    "- Adaptive bitrate streaming (480p-4K)\n",
    "- <2 second startup time\n",
    "- 99.9% playback success rate\n",
    "- Global distribution\n",
    "\n",
    "**Architecture:**\n",
    "- **CDN:** Cloudflare/Akamai with 200+ edge locations\n",
    "- **Origin:** S3 for video storage (multi-region replication)\n",
    "- **Encoding:** Distributed transcoding cluster (10-50 workers)\n",
    "- **Load balancing:** GeoDNS routing to nearest CDN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Scaling Strategies Comparison\n",
    "\n",
    "**Purpose:** Compare horizontal vs vertical scaling with real performance metrics\n",
    "\n",
    "**Key Points:**\n",
    "- **Vertical Scaling**: Increase single machine resources (CPU/RAM), simpler but limited\n",
    "- **Horizontal Scaling**: Add more machines, unlimited scalability but complex coordination\n",
    "- **Auto-Scaling**: Dynamic adjustment based on load metrics (CPU, requests/sec, latency)\n",
    "- **Cost-Performance Trade-off**: Horizontal scales linearly, vertical has diminishing returns\n",
    "\n",
    "**Post-Silicon Use Case:** Scale test data processing from 1M to 100M records - horizontal scaling with distributed Spark cluster vs single large server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåê API Gateway & Rate Limiting\n",
    "\n",
    "**Purpose:** Implement API gateway with rate limiting, authentication, and routing\n",
    "\n",
    "**Key Points:**\n",
    "- **Rate Limiting**: Prevent abuse using token bucket or sliding window algorithms\n",
    "- **Authentication**: JWT tokens, API keys for service identification\n",
    "- **Routing**: Route requests to appropriate microservices based on path/headers\n",
    "- **Aggregation**: Combine multiple backend calls into single client response\n",
    "\n",
    "**Post-Silicon Use Case:** API gateway for wafer test data API - limit queries to 1000/hour per user, route to test/wafer/analysis services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### üìä Database Sharding Implementation\n",
    "\n",
    "**Purpose:** Implement horizontal database sharding for scalable data storage\n",
    "\n",
    "**Key Points:**\n",
    "- **Consistent Hashing**: Distributes data evenly across shards, minimizes rebalancing on shard addition/removal\n",
    "- **Shard Key Selection**: Choose keys with high cardinality (user_id, device_id) to avoid hotspots\n",
    "- **Cross-Shard Queries**: Expensive - design schema to minimize them (denormalization, data locality)\n",
    "- **Replication**: Each shard replicated 2-3 times for high availability\n",
    "\n",
    "**Post-Silicon Use Case:** Distribute millions of STDF test records across shards by `wafer_id` for parallel processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
