{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# K-Nearest Neighbors (KNN): Instance-Based Learning\n",
        "\n",
        "## \ud83d\udcd8 What You'll Master\n",
        "\n",
        "**K-Nearest Neighbors (KNN)** is a **non-parametric, instance-based** learning algorithm that makes predictions based on the **similarity** to training examples. Unlike tree-based or linear models that learn a global function, KNN is a **\"lazy learner\"** \u2014 it stores all training data and makes predictions by looking at the K most similar instances.\n",
        "\n",
        "### \ud83c\udfaf Why KNN Matters\n",
        "\n",
        "1. **Simple yet powerful**: No training phase, works well for small-medium datasets\n",
        "2. **Non-parametric**: Makes no assumptions about data distribution (unlike linear regression)\n",
        "3. **Naturally handles multi-class**: Classification with 10+ classes without modification\n",
        "4. **Interpretable**: \"You're like these 5 other examples\"\n",
        "5. **Foundation for similarity-based systems**: Recommendation engines, case-based reasoning\n",
        "\n",
        "### \ud83d\udd2c Real-World Applications\n",
        "\n",
        "**Post-Silicon Validation:**\n",
        "- **Similar die detection**: Find dies with similar parametric profiles for root cause analysis\n",
        "- **Reference-based prediction**: \"This device looks like known failures\"\n",
        "- **Spatial similarity**: Identify neighboring dies with correlated failures\n",
        "- **Test correlation**: Find tests that behave similarly across devices\n",
        "\n",
        "**General AI/ML:**\n",
        "- **Recommendation systems**: \"Users like you also liked...\"\n",
        "- **Anomaly detection**: Identify samples far from all neighbors\n",
        "- **Medical diagnosis**: \"Patients with similar symptoms had condition X\"\n",
        "- **Content-based filtering**: Find similar images, documents, or products\n",
        "\n",
        "### \ud83d\udcca Learning Path Context\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "    A[Linear Models<br/>010-015] --> B[Tree Models<br/>016-018]\n",
        "    B --> C[Boosting<br/>019-021]\n",
        "    C --> D[Meta-Ensembles<br/>022]\n",
        "    D --> E[KNN<br/>023 YOU ARE HERE]\n",
        "    E --> F[SVM<br/>024]\n",
        "    F --> G[Naive Bayes<br/>025]\n",
        "    G --> H[Clustering<br/>026-030]\n",
        "    \n",
        "    style E fill:#ff6b6b,stroke:#c92a2a,stroke-width:3px,color:#fff\n",
        "```\n",
        "\n",
        "**What Makes KNN Different:**\n",
        "- **No training phase**: Just stores data (contrast with tree/boosting training)\n",
        "- **Prediction = search**: Find K nearest neighbors each time\n",
        "- **Distance-based**: Everything depends on how you measure \"similarity\"\n",
        "- **Memory-based**: Stores all training data (can be large)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd0d How KNN Works: The Similarity Principle\n",
        "\n",
        "### KNN Algorithm Flow\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[New Sample x] --> B[Calculate Distance<br/>to ALL Training Samples]\n",
        "    B --> C[Sort by Distance:<br/>Closest to Farthest]\n",
        "    C --> D[Select K Nearest<br/>Neighbors]\n",
        "    D --> E{Task Type?}\n",
        "    E -->|Classification| F[Majority Vote<br/>among K neighbors]\n",
        "    E -->|Regression| G[Average of<br/>K neighbor values]\n",
        "    F --> H[Predicted Class]\n",
        "    G --> I[Predicted Value]\n",
        "    \n",
        "    style A fill:#e3f2fd\n",
        "    style D fill:#fff3e0\n",
        "    style H fill:#c8e6c9\n",
        "    style I fill:#c8e6c9\n",
        "```\n",
        "\n",
        "### \ud83d\udcdd What's Happening in KNN?\n",
        "\n",
        "**1. Distance Calculation** - Measure similarity to all training samples\n",
        "\n",
        "**2. K Selection** - Choose number of neighbors (hyperparameter)\n",
        "\n",
        "**3. Voting/Averaging** - Aggregate predictions from K neighbors\n",
        "\n",
        "**4. Prediction** - Output most common class (classification) or average (regression)\n",
        "\n",
        "### \ud83c\udfaf Key Insight\n",
        "\n",
        "**\"Similar inputs should have similar outputs\"**\n",
        "\n",
        "If 8 out of 10 nearest neighbors are \"Pass\", the new sample is likely \"Pass\" too.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcd0 Mathematical Foundation\n",
        "\n",
        "### 1\ufe0f\u20e3 Distance Metrics: Measuring Similarity\n",
        "\n",
        "**Euclidean Distance** (most common, L2 norm):\n",
        "\n",
        "$$\n",
        "d(x, x') = \\sqrt{\\sum_{j=1}^p (x_j - x'_j)^2}\n",
        "$$\n",
        "\n",
        "- $x, x'$: Two samples with $p$ features\n",
        "- Geometric \"straight-line\" distance\n",
        "- **Sensitive to scale**: Feature with range [0, 1000] dominates feature with [0, 1]\n",
        "- **Requires normalization**: Always use StandardScaler or MinMaxScaler\n",
        "\n",
        "**Manhattan Distance** (L1 norm, city-block):\n",
        "\n",
        "$$\n",
        "d(x, x') = \\sum_{j=1}^p |x_j - x'_j|\n",
        "$$\n",
        "\n",
        "- Sum of absolute differences (like walking city blocks)\n",
        "- Less sensitive to outliers than Euclidean\n",
        "- Good for high-dimensional data\n",
        "\n",
        "**Minkowski Distance** (generalized):\n",
        "\n",
        "$$\n",
        "d(x, x') = \\left( \\sum_{j=1}^p |x_j - x'_j|^q \\right)^{1/q}\n",
        "$$\n",
        "\n",
        "- $q=1$: Manhattan, $q=2$: Euclidean\n",
        "- Larger $q$: More weight to large differences\n",
        "\n",
        "**Cosine Similarity** (angle-based, for text/high-dim):\n",
        "\n",
        "$$\n",
        "\\text{similarity}(x, x') = \\frac{x \\cdot x'}{\\|x\\| \\|x'\\|} = \\frac{\\sum_{j=1}^p x_j x'_j}{\\sqrt{\\sum_{j=1}^p x_j^2} \\sqrt{\\sum_{j=1}^p x'^2_j}}\n",
        "$$\n",
        "\n",
        "- Measures angle, not magnitude (invariant to scale)\n",
        "- Distance: $d = 1 - \\text{similarity}$\n",
        "- Common in text classification (TF-IDF vectors)\n",
        "\n",
        "---\n",
        "\n",
        "### 2\ufe0f\u20e3 Classification: Majority Vote\n",
        "\n",
        "**Prediction for sample $x$**:\n",
        "\n",
        "$$\n",
        "\\hat{y}(x) = \\arg\\max_c \\sum_{i \\in \\mathcal{N}_K(x)} \\mathbb{1}(y_i = c)\n",
        "$$\n",
        "\n",
        "- $\\mathcal{N}_K(x)$: Set of K nearest neighbors to $x$\n",
        "- $\\mathbb{1}(\\cdot)$: Indicator function (1 if true, 0 otherwise)\n",
        "- $\\arg\\max_c$: Class with most votes among K neighbors\n",
        "\n",
        "**Example:** K=5 neighbors have classes [Pass, Pass, Fail, Pass, Pass]\n",
        "- Pass: 4 votes, Fail: 1 vote \u2192 Predict **Pass**\n",
        "\n",
        "**Weighted Voting** (closer neighbors have more influence):\n",
        "\n",
        "$$\n",
        "\\hat{y}(x) = \\arg\\max_c \\sum_{i \\in \\mathcal{N}_K(x)} w_i \\cdot \\mathbb{1}(y_i = c), \\quad w_i = \\frac{1}{d(x, x_i) + \\epsilon}\n",
        "$$\n",
        "\n",
        "- Closer neighbors get higher weight ($w_i \\propto 1/d$)\n",
        "- $\\epsilon$: Small constant to avoid division by zero\n",
        "\n",
        "---\n",
        "\n",
        "### 3\ufe0f\u20e3 Regression: Averaging\n",
        "\n",
        "**Prediction for sample $x$**:\n",
        "\n",
        "$$\n",
        "\\hat{y}(x) = \\frac{1}{K} \\sum_{i \\in \\mathcal{N}_K(x)} y_i\n",
        "$$\n",
        "\n",
        "- Simple average of K nearest neighbor values\n",
        "\n",
        "**Weighted Regression**:\n",
        "\n",
        "$$\n",
        "\\hat{y}(x) = \\frac{\\sum_{i \\in \\mathcal{N}_K(x)} w_i \\cdot y_i}{\\sum_{i \\in \\mathcal{N}_K(x)} w_i}, \\quad w_i = \\frac{1}{d(x, x_i) + \\epsilon}\n",
        "$$\n",
        "\n",
        "- Weighted average (closer neighbors matter more)\n",
        "\n",
        "---\n",
        "\n",
        "### 4\ufe0f\u20e3 Choosing K: Bias-Variance Trade-off\n",
        "\n",
        "**Small K (e.g., K=1, K=3):**\n",
        "- \u2705 Low bias (flexible, captures local patterns)\n",
        "- \u274c High variance (sensitive to noise, overfitting)\n",
        "- Decision boundary: Jagged, complex\n",
        "\n",
        "**Large K (e.g., K=50, K=100):**\n",
        "- \u2705 Low variance (smooth, stable predictions)\n",
        "- \u274c High bias (misses local patterns, underfitting)\n",
        "- Decision boundary: Smooth, simple\n",
        "\n",
        "**Optimal K:**\n",
        "- Use cross-validation to select K\n",
        "- Typical range: $K = \\sqrt{N}$ as starting point (where $N$ = training size)\n",
        "- Try odd K for binary classification (avoids ties)\n",
        "\n",
        "---\n",
        "\n",
        "### 5\ufe0f\u20e3 Computational Complexity\n",
        "\n",
        "**Training:** $O(1)$ \u2014 Just stores data, no learning!\n",
        "\n",
        "**Prediction:** $O(N \\cdot p)$ \u2014 Calculate distance to all $N$ training samples with $p$ features\n",
        "\n",
        "**Problem:** Prediction is **SLOW** for large datasets (millions of samples)\n",
        "\n",
        "**Solutions:**\n",
        "- **KD-Tree**: $O(p \\log N)$ search (works for $p < 20$)\n",
        "- **Ball Tree**: $O(\\log N)$ search (better for high dimensions)\n",
        "- **Approximate NN**: FAISS, Annoy (sacrifice accuracy for 10-100x speedup)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Import libraries for KNN implementation and evaluation\n",
        "\n",
        "**Key Points:**\n",
        "- **KNeighborsClassifier/Regressor**: sklearn's KNN implementation with multiple distance metrics\n",
        "- **StandardScaler**: **CRITICAL** for KNN \u2014 features must have similar scales\n",
        "- **make_classification/regression**: Generate synthetic data for demonstrations\n",
        "- **Metrics**: Accuracy, confusion matrix, MAE for evaluation\n",
        "- **KD-Tree option**: `algorithm='kd_tree'` for faster search (small p)\n",
        "\n",
        "**Why This Matters:** KNN is extremely sensitive to feature scales \u2014 always normalize!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, confusion_matrix, classification_report,\n",
        "    mean_absolute_error, mean_squared_error, r2_score,\n",
        "    roc_auc_score, roc_curve\n",
        ")\n",
        "from sklearn.datasets import make_classification, make_regression, make_blobs\n",
        "import time\n",
        "\n",
        "# Visualization settings\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\u2705 Imports complete\")\n",
        "print(f\"   NumPy: {np.__version__}\")\n",
        "print(f\"   Pandas: {pd.__version__}\")\n",
        "print(f\"\\n\ud83c\udfaf Ready to explore K-Nearest Neighbors!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Implement KNN from scratch to understand distance calculation and voting\n",
        "\n",
        "**Key Points:**\n",
        "- **euclidean_distance**: NumPy vectorized distance calculation ($\\sqrt{\\sum(x_i - x_j)^2}$)\n",
        "- **predict**: For each test sample, calculate distances to all training samples\n",
        "- **np.argsort**: Sort distances and get indices of K nearest neighbors\n",
        "- **np.bincount**: Count votes for each class (efficient voting)\n",
        "- **No training**: Just stores X_train and y_train!\n",
        "\n",
        "**Why This Matters:** KNN is conceptually simple but computationally expensive \u2014 understand the distance calculation bottleneck.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KNNClassifierScratch:\n",
        "    \"\"\"K-Nearest Neighbors Classifier from scratch\"\"\"\n",
        "    \n",
        "    def __init__(self, k=5):\n",
        "        self.k = k\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Store training data (no actual training)\"\"\"\n",
        "        self.X_train = np.array(X)\n",
        "        self.y_train = np.array(y)\n",
        "        return self\n",
        "    \n",
        "    def euclidean_distance(self, x1, x2):\n",
        "        \"\"\"Calculate Euclidean distance between two vectors\"\"\"\n",
        "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict class for each sample in X\"\"\"\n",
        "        predictions = []\n",
        "        \n",
        "        for x_test in X:\n",
        "            # Calculate distances to all training samples\n",
        "            distances = [self.euclidean_distance(x_test, x_train) \n",
        "                        for x_train in self.X_train]\n",
        "            \n",
        "            # Get indices of K nearest neighbors (smallest distances)\n",
        "            k_indices = np.argsort(distances)[:self.k]\n",
        "            \n",
        "            # Get labels of K nearest neighbors\n",
        "            k_nearest_labels = self.y_train[k_indices]\n",
        "            \n",
        "            # Majority vote (most common class)\n",
        "            most_common = np.bincount(k_nearest_labels).argmax()\n",
        "            predictions.append(most_common)\n",
        "        \n",
        "        return np.array(predictions)\n",
        "\n",
        "# Demo on simple dataset\n",
        "print(\"\ud83e\uddea Testing KNN from Scratch\\n\")\n",
        "\n",
        "# Generate small dataset\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_informative=2, \n",
        "                          n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# CRITICAL: Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train and predict\n",
        "knn_scratch = KNNClassifierScratch(k=5)\n",
        "knn_scratch.fit(X_train_scaled, y_train)\n",
        "\n",
        "start_time = time.time()\n",
        "y_pred = knn_scratch.predict(X_test_scaled)\n",
        "pred_time = time.time() - start_time\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\u2705 From-Scratch KNN Results (K=5):\")\n",
        "print(f\"   Accuracy: {accuracy:.4f}\")\n",
        "print(f\"   Prediction time: {pred_time*1000:.2f}ms for {len(X_test)} samples\")\n",
        "print(f\"   Per-sample: {pred_time*1000/len(X_test):.2f}ms\")\n",
        "\n",
        "print(f\"\\n\ud83d\udd0d How it works:\")\n",
        "print(f\"   1. For each test sample, calculate distance to ALL {len(X_train)} training samples\")\n",
        "print(f\"   2. Sort distances and find K={knn_scratch.k} nearest neighbors\")\n",
        "print(f\"   3. Majority vote among neighbors (most common class wins)\")\n",
        "print(f\"   4. Repeat for all {len(X_test)} test samples\")\n",
        "print(f\"\\n\u26a0\ufe0f Note: No training phase \u2014 just stores data!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Use sklearn's optimized KNN with multiple distance metrics and algorithms\n",
        "\n",
        "**Key Points:**\n",
        "- **n_neighbors=5**: K value (typical starting point)\n",
        "- **weights='uniform'**: All neighbors vote equally (vs 'distance' for weighted voting)\n",
        "- **metric='euclidean'**: Distance function (alternatives: 'manhattan', 'minkowski', 'cosine')\n",
        "- **algorithm='auto'**: Chooses best search method (brute force, KD-tree, ball tree)\n",
        "- **predict_proba**: Get probability estimates (useful for thresholding)\n",
        "\n",
        "**Why This Matters:** sklearn's KNN is 10-100x faster than from-scratch due to optimized algorithms (C extensions, KD-trees).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83d\ude80 Production KNN with sklearn\\n\")\n",
        "\n",
        "# Same dataset as before\n",
        "knn_sklearn = KNeighborsClassifier(\n",
        "    n_neighbors=5,\n",
        "    weights='uniform',  # 'uniform' or 'distance'\n",
        "    metric='euclidean',  # Distance function\n",
        "    algorithm='auto',    # 'auto', 'ball_tree', 'kd_tree', 'brute'\n",
        "    n_jobs=-1            # Parallel processing\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "knn_sklearn.fit(X_train_scaled, y_train)\n",
        "fit_time = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "y_pred_sklearn = knn_sklearn.predict(X_test_scaled)\n",
        "y_proba_sklearn = knn_sklearn.predict_proba(X_test_scaled)[:, 1]\n",
        "pred_time_sklearn = time.time() - start_time\n",
        "\n",
        "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "auc_sklearn = roc_auc_score(y_test, y_proba_sklearn)\n",
        "\n",
        "print(f\"\u2705 sklearn KNN Results (K=5):\")\n",
        "print(f\"   Fit time: {fit_time*1000:.2f}ms (just stores data)\")\n",
        "print(f\"   Prediction time: {pred_time_sklearn*1000:.2f}ms for {len(X_test)} samples\")\n",
        "print(f\"   Per-sample: {pred_time_sklearn*1000/len(X_test):.2f}ms\")\n",
        "print(f\"   Accuracy: {accuracy_sklearn:.4f}\")\n",
        "print(f\"   AUC: {auc_sklearn:.4f}\")\n",
        "\n",
        "speedup = pred_time / pred_time_sklearn\n",
        "print(f\"\\n\u26a1 Speedup over from-scratch: {speedup:.1f}x\")\n",
        "\n",
        "# Comparison with from-scratch\n",
        "print(f\"\\n\ud83d\udcca Validation (same predictions?): {np.array_equal(y_pred, y_pred_sklearn)}\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_sklearn)\n",
        "print(f\"\\n\ud83d\udccb Confusion Matrix:\")\n",
        "print(f\"   True Neg:  {cm[0,0]:3d}  |  False Pos: {cm[0,1]:3d}\")\n",
        "print(f\"   False Neg: {cm[1,0]:3d}  |  True Pos:  {cm[1,1]:3d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Find optimal K value using cross-validation\n",
        "\n",
        "**Key Points:**\n",
        "- **K range**: Test K=1, 3, 5, 7, 9, 11, 15, 20, 30 (common values)\n",
        "- **cross_val_score**: 5-fold CV to get robust accuracy estimate\n",
        "- **Bias-variance trade-off**: Small K (high variance), large K (high bias)\n",
        "- **Elbow method**: Look for K where accuracy plateaus\n",
        "- **Odd K preferred**: Avoids tie-breaking in binary classification\n",
        "\n",
        "**Why This Matters:** K is the most important hyperparameter in KNN \u2014 wrong K leads to poor performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83d\udd0d Finding Optimal K via Cross-Validation\\n\")\n",
        "\n",
        "# Test different K values\n",
        "k_values = [1, 3, 5, 7, 9, 11, 15, 20, 30, 50]\n",
        "cv_scores_mean = []\n",
        "cv_scores_std = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n",
        "    scores = cross_val_score(knn, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "    cv_scores_mean.append(scores.mean())\n",
        "    cv_scores_std.append(scores.std())\n",
        "    print(f\"   K={k:2d}: Accuracy = {scores.mean():.4f} \u00b1 {scores.std():.4f}\")\n",
        "\n",
        "# Find best K\n",
        "best_k_idx = np.argmax(cv_scores_mean)\n",
        "best_k = k_values[best_k_idx]\n",
        "best_accuracy = cv_scores_mean[best_k_idx]\n",
        "\n",
        "print(f\"\\n\u2705 Best K: {best_k} (Accuracy: {best_accuracy:.4f})\")\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax.errorbar(k_values, cv_scores_mean, yerr=cv_scores_std, \n",
        "            marker='o', linestyle='-', linewidth=2, markersize=8,\n",
        "            capsize=5, capthick=2, label='CV Accuracy \u00b1 Std')\n",
        "ax.axvline(best_k, color='red', linestyle='--', linewidth=2, \n",
        "           label=f'Best K={best_k}')\n",
        "ax.set_xlabel('K (Number of Neighbors)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Cross-Validation Accuracy', fontsize=12, fontweight='bold')\n",
        "ax.set_title('KNN: Finding Optimal K via 5-Fold Cross-Validation', \n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Annotate best K\n",
        "ax.annotate(f'Best K={best_k}\\nAcc={best_accuracy:.4f}', \n",
        "            xy=(best_k, best_accuracy), xytext=(best_k+10, best_accuracy-0.02),\n",
        "            arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
        "            fontsize=11, fontweight='bold', color='red')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf Observations:\")\n",
        "print(f\"   \u2022 Small K (1-3): High variance, sensitive to noise\")\n",
        "print(f\"   \u2022 Medium K ({best_k}): Best balance (bias-variance trade-off)\")\n",
        "print(f\"   \u2022 Large K (30-50): High bias, misses local patterns\")\n",
        "print(f\"   \u2022 Rule of thumb: Start with K=\u221aN = {int(np.sqrt(len(X_train)))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Compare different distance metrics (Euclidean, Manhattan, Minkowski, Cosine)\n",
        "\n",
        "**Key Points:**\n",
        "- **Euclidean**: Geometric distance, most common ($L_2$ norm)\n",
        "- **Manhattan**: City-block distance, less sensitive to outliers ($L_1$ norm)\n",
        "- **Minkowski (p=3)**: Generalized distance ($L_p$ norm)\n",
        "- **Cosine**: Angle-based, good for high-dimensional data (text, embeddings)\n",
        "- **Dataset-dependent**: No single \"best\" metric \u2014 try multiple\n",
        "\n",
        "**Why This Matters:** Distance metric choice can change accuracy by 5-15% \u2014 always experiment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83d\udccf Comparing Distance Metrics\\n\")\n",
        "\n",
        "metrics = {\n",
        "    'Euclidean (L2)': 'euclidean',\n",
        "    'Manhattan (L1)': 'manhattan',\n",
        "    'Minkowski (p=3)': 'minkowski',  # Need to set p parameter\n",
        "    'Cosine': 'cosine'\n",
        "}\n",
        "\n",
        "metric_results = {}\n",
        "\n",
        "for name, metric in metrics.items():\n",
        "    if metric == 'minkowski':\n",
        "        knn = KNeighborsClassifier(n_neighbors=best_k, metric=metric, p=3, n_jobs=-1)\n",
        "    else:\n",
        "        knn = KNeighborsClassifier(n_neighbors=best_k, metric=metric, n_jobs=-1)\n",
        "    \n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    y_proba = knn.predict_proba(X_test_scaled)[:, 1] if metric != 'cosine' else None\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
        "    \n",
        "    metric_results[name] = {'accuracy': accuracy, 'auc': auc}\n",
        "    \n",
        "    auc_str = f\"{auc:.4f}\" if auc else \"N/A\"\n",
        "    print(f\"   {name:<20} Accuracy: {accuracy:.4f}, AUC: {auc_str}\")\n",
        "\n",
        "# Best metric\n",
        "best_metric = max(metric_results.items(), key=lambda x: x[1]['accuracy'])\n",
        "print(f\"\\n\u2705 Best metric: {best_metric[0]} (Accuracy: {best_metric[1]['accuracy']:.4f})\")\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf When to use each metric:\")\n",
        "print(f\"   \u2022 Euclidean: Default choice, continuous features, geometric data\")\n",
        "print(f\"   \u2022 Manhattan: Outlier-robust, grid-like data (images, city maps)\")\n",
        "print(f\"   \u2022 Minkowski: Tunable p parameter (balance between L1/L2)\")\n",
        "print(f\"   \u2022 Cosine: Text data (TF-IDF), high-dimensional embeddings, angle matters more than magnitude\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Compare uniform voting (equal weights) vs distance-weighted voting\n",
        "\n",
        "**Key Points:**\n",
        "- **weights='uniform'**: All K neighbors vote equally (default)\n",
        "- **weights='distance'**: Closer neighbors have more influence ($w_i = 1/d_i$)\n",
        "- **Distance weighting**: Helps when nearest neighbors are very close (high confidence)\n",
        "- **Trade-off**: Weighted can overfit to very close neighbors\n",
        "\n",
        "**Why This Matters:** Distance weighting typically improves accuracy by 1-3% when decision boundaries are complex.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\u2696\ufe0f Uniform vs Distance-Weighted Voting\\n\")\n",
        "\n",
        "# Uniform voting (all neighbors equal)\n",
        "knn_uniform = KNeighborsClassifier(n_neighbors=best_k, weights='uniform', n_jobs=-1)\n",
        "knn_uniform.fit(X_train_scaled, y_train)\n",
        "y_pred_uniform = knn_uniform.predict(X_test_scaled)\n",
        "acc_uniform = accuracy_score(y_test, y_pred_uniform)\n",
        "\n",
        "# Distance-weighted voting (closer neighbors matter more)\n",
        "knn_weighted = KNeighborsClassifier(n_neighbors=best_k, weights='distance', n_jobs=-1)\n",
        "knn_weighted.fit(X_train_scaled, y_train)\n",
        "y_pred_weighted = knn_weighted.predict(X_test_scaled)\n",
        "acc_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "\n",
        "print(f\"\u2705 Voting Comparison (K={best_k}):\")\n",
        "print(f\"   Uniform (equal weights):    Accuracy = {acc_uniform:.4f}\")\n",
        "print(f\"   Distance-weighted:          Accuracy = {acc_weighted:.4f}\")\n",
        "print(f\"   Improvement: {(acc_weighted - acc_uniform)*100:+.2f}%\")\n",
        "\n",
        "print(f\"\\n\ud83d\udd0d How distance weighting works:\")\n",
        "print(f\"   \u2022 Weight w_i = 1 / (distance_i + \u03b5)\")\n",
        "print(f\"   \u2022 Closer neighbors get higher weight\")\n",
        "print(f\"   \u2022 Example: distance=0.1 \u2192 weight=10, distance=1.0 \u2192 weight=1.0\")\n",
        "print(f\"   \u2022 Prediction = weighted average of K neighbor classes\")\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf When to use each:\")\n",
        "print(f\"   \u2022 Uniform: Simple, robust, good starting point\")\n",
        "print(f\"   \u2022 Distance-weighted: Complex boundaries, varying density, when nearest neighbor very close\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Demonstrate why feature scaling is CRITICAL for KNN\n",
        "\n",
        "**Key Points:**\n",
        "- **Without scaling**: Feature with large range (0-1000) dominates distance calculation\n",
        "- **With scaling**: All features contribute equally to distance\n",
        "- **StandardScaler**: $z = (x - \\mu) / \\sigma$ (mean=0, std=1)\n",
        "- **MinMaxScaler**: $x' = (x - x_{min}) / (x_{max} - x_{min})$ (range [0,1])\n",
        "- **Impact**: Scaling can change accuracy by 20-40%!\n",
        "\n",
        "**Why This Matters:** KNN without scaling is almost always wrong \u2014 ALWAYS scale features!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\u26a0\ufe0f Feature Scaling Sensitivity Demo\\n\")\n",
        "\n",
        "# Create dataset with different feature scales\n",
        "np.random.seed(42)\n",
        "n_samples = 500\n",
        "X_unscaled = np.column_stack([\n",
        "    np.random.randn(n_samples) * 1,      # Feature 1: range ~[-3, 3]\n",
        "    np.random.randn(n_samples) * 1000,   # Feature 2: range ~[-3000, 3000] (DOMINATES!)\n",
        "    np.random.randn(n_samples) * 0.1     # Feature 3: range ~[-0.3, 0.3]\n",
        "])\n",
        "y_demo = (X_unscaled[:, 0] + X_unscaled[:, 1]/1000 + X_unscaled[:, 2]*10 > 0).astype(int)\n",
        "\n",
        "X_train_demo, X_test_demo, y_train_demo, y_test_demo = train_test_split(\n",
        "    X_unscaled, y_demo, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\ud83d\udcca Feature Ranges (training data):\")\n",
        "for i in range(X_train_demo.shape[1]):\n",
        "    print(f\"   Feature {i+1}: [{X_train_demo[:, i].min():.2f}, {X_train_demo[:, i].max():.2f}] \"\n",
        "          f\"(std={X_train_demo[:, i].std():.2f})\")\n",
        "\n",
        "# Test WITHOUT scaling\n",
        "print(f\"\\n\u274c KNN WITHOUT Scaling:\")\n",
        "knn_no_scale = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
        "knn_no_scale.fit(X_train_demo, y_train_demo)\n",
        "y_pred_no_scale = knn_no_scale.predict(X_test_demo)\n",
        "acc_no_scale = accuracy_score(y_test_demo, y_pred_no_scale)\n",
        "print(f\"   Accuracy: {acc_no_scale:.4f}\")\n",
        "print(f\"   Problem: Feature 2 (large range) dominates distance calculation!\")\n",
        "print(f\"   Distance \u2248 \u221a((f1_diff)\u00b2 + (f2_diff)\u00b2 + (f3_diff)\u00b2)\")\n",
        "print(f\"            \u2248 \u221a(1\u00b2 + 1000\u00b2 + 0.1\u00b2) \u2248 1000 (f2 dominates!)\")\n",
        "\n",
        "# Test WITH StandardScaler\n",
        "print(f\"\\n\u2705 KNN WITH StandardScaler:\")\n",
        "scaler_std = StandardScaler()\n",
        "X_train_std = scaler_std.fit_transform(X_train_demo)\n",
        "X_test_std = scaler_std.transform(X_test_demo)\n",
        "\n",
        "knn_std = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
        "knn_std.fit(X_train_std, y_train_demo)\n",
        "y_pred_std = knn_std.predict(X_test_std)\n",
        "acc_std = accuracy_score(y_test_demo, y_pred_std)\n",
        "print(f\"   Accuracy: {acc_std:.4f}\")\n",
        "print(f\"   All features now have mean=0, std=1 \u2192 equal contribution\")\n",
        "\n",
        "# Test WITH MinMaxScaler\n",
        "print(f\"\\n\u2705 KNN WITH MinMaxScaler:\")\n",
        "scaler_mm = MinMaxScaler()\n",
        "X_train_mm = scaler_mm.fit_transform(X_train_demo)\n",
        "X_test_mm = scaler_mm.transform(X_test_demo)\n",
        "\n",
        "knn_mm = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
        "knn_mm.fit(X_train_mm, y_train_demo)\n",
        "y_pred_mm = knn_mm.predict(X_test_mm)\n",
        "acc_mm = accuracy_score(y_test_demo, y_pred_mm)\n",
        "print(f\"   Accuracy: {acc_mm:.4f}\")\n",
        "print(f\"   All features now in range [0, 1] \u2192 equal contribution\")\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf Results Summary:\")\n",
        "print(f\"   No scaling:      {acc_no_scale:.4f} (POOR - dominated by large-scale feature)\")\n",
        "print(f\"   StandardScaler:  {acc_std:.4f} (GOOD - mean=0, std=1)\")\n",
        "print(f\"   MinMaxScaler:    {acc_mm:.4f} (GOOD - range [0,1])\")\n",
        "print(f\"   Improvement:     {(acc_std - acc_no_scale)*100:+.2f}% (scaling is CRITICAL!)\")\n",
        "\n",
        "print(f\"\\n\u2705 Best Practice: ALWAYS scale features before KNN!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Use KNN for regression (predict continuous values, not classes)\n",
        "\n",
        "**Key Points:**\n",
        "- **KNeighborsRegressor**: Same distance calculation, but averages K neighbor values\n",
        "- **Prediction**: $\\hat{y} = \\frac{1}{K} \\sum_{i \\in \\mathcal{N}_K(x)} y_i$ (simple average)\n",
        "- **Weighted average**: Closer neighbors contribute more (weights='distance')\n",
        "- **Evaluation**: MAE, RMSE, R\u00b2 (regression metrics)\n",
        "- **Use case**: Local patterns, non-parametric regression, missing data interpolation\n",
        "\n",
        "**Why This Matters:** KNN regression captures local patterns that global models (linear regression) miss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "print(\"\ud83d\udcca KNN Regression Demo\\n\")\n",
        "\n",
        "# Generate regression dataset\n",
        "X_reg, y_reg = make_regression(n_samples=500, n_features=10, n_informative=8,\n",
        "                               noise=15, random_state=42)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features (CRITICAL for KNN)\n",
        "scaler_reg = StandardScaler()\n",
        "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
        "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
        "\n",
        "# Test different K values\n",
        "k_values_reg = [1, 3, 5, 10, 20, 30]\n",
        "results_reg = []\n",
        "\n",
        "for k in k_values_reg:\n",
        "    knn_reg = KNeighborsRegressor(n_neighbors=k, weights='distance', n_jobs=-1)\n",
        "    knn_reg.fit(X_train_reg_scaled, y_train_reg)\n",
        "    y_pred_reg = knn_reg.predict(X_test_reg_scaled)\n",
        "    \n",
        "    mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))\n",
        "    r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "    \n",
        "    results_reg.append({'k': k, 'mae': mae, 'rmse': rmse, 'r2': r2})\n",
        "    print(f\"   K={k:2d}: MAE={mae:8.2f}, RMSE={rmse:8.2f}, R\u00b2={r2:6.4f}\")\n",
        "\n",
        "# Best K (by R\u00b2)\n",
        "best_result = max(results_reg, key=lambda x: x['r2'])\n",
        "print(f\"\\n\u2705 Best K: {best_result['k']} (R\u00b2={best_result['r2']:.4f})\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "k_vals = [r['k'] for r in results_reg]\n",
        "mae_vals = [r['mae'] for r in results_reg]\n",
        "rmse_vals = [r['rmse'] for r in results_reg]\n",
        "r2_vals = [r['r2'] for r in results_reg]\n",
        "\n",
        "axes[0].plot(k_vals, mae_vals, marker='o', linewidth=2, markersize=8)\n",
        "axes[0].set_xlabel('K', fontweight='bold')\n",
        "axes[0].set_ylabel('MAE', fontweight='bold')\n",
        "axes[0].set_title('Mean Absolute Error vs K', fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(k_vals, rmse_vals, marker='o', linewidth=2, markersize=8, color='orange')\n",
        "axes[1].set_xlabel('K', fontweight='bold')\n",
        "axes[1].set_ylabel('RMSE', fontweight='bold')\n",
        "axes[1].set_title('Root Mean Squared Error vs K', fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].plot(k_vals, r2_vals, marker='o', linewidth=2, markersize=8, color='green')\n",
        "axes[2].set_xlabel('K', fontweight='bold')\n",
        "axes[2].set_ylabel('R\u00b2', fontweight='bold')\n",
        "axes[2].set_title('R\u00b2 Score vs K', fontweight='bold')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "axes[2].axhline(best_result['r2'], color='red', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf KNN Regression Insights:\")\n",
        "print(f\"   \u2022 K=1: Perfect fit to training (high variance, overfitting)\")\n",
        "print(f\"   \u2022 K=5-10: Good balance (captures local patterns)\")\n",
        "print(f\"   \u2022 K=30+: Smooth predictions (high bias, underfitting)\")\n",
        "print(f\"   \u2022 Use weighted averaging (weights='distance') for better results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd2c Post-Silicon Application: Similar Device Detection for Root Cause Analysis\n",
        "\n",
        "### Business Problem\n",
        "\n",
        "**Scenario:** 50,000 devices tested, 200 fail with similar symptoms. Engineers need to find **all devices with similar parametric profiles** for root cause analysis.\n",
        "\n",
        "**Traditional Approach (Manual):**\n",
        "- Engineer looks at 25 parametric tests for each failure\n",
        "- Manually searches for similar patterns across 50K devices\n",
        "- Time: 2-4 hours per failure analysis\n",
        "- Misses subtle similarities (human pattern recognition limits)\n",
        "\n",
        "**KNN Approach (Automated):**\n",
        "- Calculate distance from failed device to all 50K devices\n",
        "- Return K=20 most similar devices (nearest neighbors)\n",
        "- Visualize common patterns across similar failures\n",
        "- Time: <1 second per query\n",
        "\n",
        "### \ud83d\udcb0 Business Value\n",
        "\n",
        "- **Root cause speed**: 2 hours \u2192 5 minutes (95% reduction)\n",
        "- **Similar failure detection**: Find hidden patterns missed manually\n",
        "- **Proactive identification**: Predict which devices might fail based on similarity to known failures\n",
        "- **Cost impact**: $500K-2M per major failure investigation saved\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Generate realistic 50K device dataset with spatial and parametric features\n",
        "\n",
        "**Key Points:**\n",
        "- **50K devices**: Production-scale dataset\n",
        "- **25 parametric tests**: Voltage, current, frequency, power, leakage, temperature, timing\n",
        "- **Spatial features**: die_x, die_y (wafer position), equipment_id, lot_id\n",
        "- **Failure modes**: Inject 3 distinct failure patterns (high leakage, timing violations, power anomalies)\n",
        "- **Similarity groups**: Devices with similar test profiles cluster together\n",
        "\n",
        "**Why This Matters:** KNN excels at finding similar instances in high-dimensional parametric test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83c\udfed Generating 50K Device Post-Silicon Dataset\\n\")\n",
        "\n",
        "np.random.seed(42)\n",
        "n_devices = 50000\n",
        "\n",
        "# Spatial features\n",
        "die_x = np.random.randint(0, 50, n_devices)\n",
        "die_y = np.random.randint(0, 50, n_devices)\n",
        "equipment_id = np.random.randint(1, 21, n_devices)  # 20 testers\n",
        "lot_id = np.random.randint(1, 51, n_devices)  # 50 lots\n",
        "\n",
        "# 25 Parametric tests (realistic semiconductor test parameters)\n",
        "vdd_core = np.random.normal(1.0, 0.02, n_devices)  # Core voltage\n",
        "vdd_io = np.random.normal(1.8, 0.03, n_devices)    # I/O voltage\n",
        "idd_active = np.random.normal(50, 5, n_devices)    # Active current (mA)\n",
        "idd_standby = np.random.normal(0.5, 0.1, n_devices) # Standby current (mA)\n",
        "freq_max = np.random.normal(2000, 100, n_devices)  # Max frequency (MHz)\n",
        "freq_min = np.random.normal(500, 50, n_devices)    # Min frequency (MHz)\n",
        "power_active = np.random.normal(100, 10, n_devices) # Active power (mW)\n",
        "power_idle = np.random.normal(5, 1, n_devices)     # Idle power (mW)\n",
        "leakage_total = np.random.lognormal(1.0, 0.5, n_devices) # Total leakage (\u03bcA)\n",
        "leakage_gate = np.random.lognormal(0.5, 0.3, n_devices)  # Gate leakage (\u03bcA)\n",
        "temp_junction = np.random.normal(85, 5, n_devices) # Junction temp (\u00b0C)\n",
        "temp_ambient = np.random.normal(25, 2, n_devices)  # Ambient temp (\u00b0C)\n",
        "timing_setup = np.random.normal(1.5, 0.2, n_devices) # Setup time (ns)\n",
        "timing_hold = np.random.normal(0.8, 0.1, n_devices)  # Hold time (ns)\n",
        "timing_clk2q = np.random.normal(2.0, 0.3, n_devices) # Clock-to-Q (ns)\n",
        "delay_rising = np.random.normal(1.2, 0.15, n_devices) # Rising edge delay (ns)\n",
        "delay_falling = np.random.normal(1.1, 0.12, n_devices) # Falling edge delay (ns)\n",
        "capacitance_in = np.random.normal(5.0, 0.5, n_devices) # Input capacitance (pF)\n",
        "capacitance_out = np.random.normal(8.0, 0.8, n_devices) # Output capacitance (pF)\n",
        "resistance_on = np.random.normal(50, 5, n_devices)  # On-resistance (\u03a9)\n",
        "resistance_off = np.random.normal(1e6, 1e5, n_devices) # Off-resistance (\u03a9)\n",
        "noise_margin_high = np.random.normal(0.4, 0.05, n_devices) # High noise margin (V)\n",
        "noise_margin_low = np.random.normal(0.4, 0.05, n_devices)  # Low noise margin (V)\n",
        "propagation_delay = np.random.normal(3.5, 0.4, n_devices)  # Total propagation (ns)\n",
        "slew_rate = np.random.normal(2.0, 0.3, n_devices)   # Slew rate (V/ns)\n",
        "\n",
        "# Inject 3 distinct failure modes (for similarity detection demo)\n",
        "n_failures = 200\n",
        "failure_indices = np.random.choice(n_devices, n_failures, replace=False)\n",
        "\n",
        "# Failure Mode 1: High leakage (first 70 failures)\n",
        "fm1_indices = failure_indices[:70]\n",
        "leakage_total[fm1_indices] = np.random.lognormal(3.0, 0.3, 70)  # 10x higher\n",
        "leakage_gate[fm1_indices] = np.random.lognormal(2.5, 0.3, 70)   # 5x higher\n",
        "power_idle[fm1_indices] = np.random.normal(15, 2, 70)           # 3x higher\n",
        "\n",
        "# Failure Mode 2: Timing violations (next 80 failures)\n",
        "fm2_indices = failure_indices[70:150]\n",
        "timing_setup[fm2_indices] = np.random.normal(0.5, 0.1, 80)      # Too fast (violation)\n",
        "timing_hold[fm2_indices] = np.random.normal(0.3, 0.05, 80)      # Too fast\n",
        "propagation_delay[fm2_indices] = np.random.normal(5.5, 0.5, 80) # Slow\n",
        "freq_max[fm2_indices] = np.random.normal(1500, 100, 80)         # Can't reach max freq\n",
        "\n",
        "# Failure Mode 3: Power anomalies (last 50 failures)\n",
        "fm3_indices = failure_indices[150:]\n",
        "power_active[fm3_indices] = np.random.normal(200, 20, 50)       # 2x higher\n",
        "idd_active[fm3_indices] = np.random.normal(100, 10, 50)         # 2x higher\n",
        "temp_junction[fm3_indices] = np.random.normal(105, 5, 50)       # Overheating\n",
        "\n",
        "# Create DataFrame\n",
        "df_ps = pd.DataFrame({\n",
        "    'device_id': range(n_devices),\n",
        "    'die_x': die_x,\n",
        "    'die_y': die_y,\n",
        "    'equipment_id': equipment_id,\n",
        "    'lot_id': lot_id,\n",
        "    'vdd_core': vdd_core,\n",
        "    'vdd_io': vdd_io,\n",
        "    'idd_active': idd_active,\n",
        "    'idd_standby': idd_standby,\n",
        "    'freq_max': freq_max,\n",
        "    'freq_min': freq_min,\n",
        "    'power_active': power_active,\n",
        "    'power_idle': power_idle,\n",
        "    'leakage_total': leakage_total,\n",
        "    'leakage_gate': leakage_gate,\n",
        "    'temp_junction': temp_junction,\n",
        "    'temp_ambient': temp_ambient,\n",
        "    'timing_setup': timing_setup,\n",
        "    'timing_hold': timing_hold,\n",
        "    'timing_clk2q': timing_clk2q,\n",
        "    'delay_rising': delay_rising,\n",
        "    'delay_falling': delay_falling,\n",
        "    'capacitance_in': capacitance_in,\n",
        "    'capacitance_out': capacitance_out,\n",
        "    'resistance_on': resistance_on,\n",
        "    'resistance_off': resistance_off,\n",
        "    'noise_margin_high': noise_margin_high,\n",
        "    'noise_margin_low': noise_margin_low,\n",
        "    'propagation_delay': propagation_delay,\n",
        "    'slew_rate': slew_rate\n",
        "})\n",
        "\n",
        "# Label failures\n",
        "df_ps['is_failure'] = 0\n",
        "df_ps.loc[failure_indices, 'is_failure'] = 1\n",
        "\n",
        "# Failure mode labels\n",
        "df_ps['failure_mode'] = 'Pass'\n",
        "df_ps.loc[fm1_indices, 'failure_mode'] = 'High_Leakage'\n",
        "df_ps.loc[fm2_indices, 'failure_mode'] = 'Timing_Violation'\n",
        "df_ps.loc[fm3_indices, 'failure_mode'] = 'Power_Anomaly'\n",
        "\n",
        "print(f\"\u2705 Dataset Generated:\")\n",
        "print(f\"   Total devices: {len(df_ps):,}\")\n",
        "print(f\"   Parametric tests: 25 (voltage, current, frequency, timing, power, leakage, etc.)\")\n",
        "print(f\"   Failures: {df_ps['is_failure'].sum():,} ({df_ps['is_failure'].mean()*100:.2f}%)\")\n",
        "print(f\"\\n\ud83d\udcca Failure Mode Distribution:\")\n",
        "print(df_ps['failure_mode'].value_counts())\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf Use Case: Find similar devices to any failed device for root cause analysis\")\n",
        "print(f\"   Example: Device 12345 fails \u2192 Find K=20 nearest neighbors \u2192 Analyze common patterns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Use KNN to find similar devices for root cause analysis\n",
        "\n",
        "**Key Points:**\n",
        "- **Query device**: Select a failed device (e.g., high leakage failure)\n",
        "- **kneighbors()**: Return distances and indices of K nearest neighbors\n",
        "- **Feature space**: 25 parametric tests (scaled) define \"similarity\"\n",
        "- **Interpretation**: Neighbors likely share same root cause\n",
        "- **Validation**: Check if neighbors have same failure mode\n",
        "\n",
        "**Why This Matters:** Automated similarity search replaces hours of manual analysis with <1 second query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83d\udd0d Similar Device Detection with KNN\\n\")\n",
        "\n",
        "# Prepare features (25 parametric tests)\n",
        "feature_cols = [col for col in df_ps.columns if col not in \n",
        "                ['device_id', 'is_failure', 'failure_mode', 'die_x', 'die_y', 'equipment_id', 'lot_id']]\n",
        "\n",
        "X_ps = df_ps[feature_cols].values\n",
        "\n",
        "# Scale features (CRITICAL for KNN)\n",
        "scaler_ps = StandardScaler()\n",
        "X_ps_scaled = scaler_ps.fit_transform(X_ps)\n",
        "\n",
        "# Build KNN model (K=20 similar devices)\n",
        "knn_ps = KNeighborsClassifier(n_neighbors=20, metric='euclidean', n_jobs=-1)\n",
        "knn_ps.fit(X_ps_scaled, df_ps['is_failure'].values)\n",
        "\n",
        "# Example: Find similar devices to a high leakage failure\n",
        "query_idx = fm1_indices[0]  # First high leakage failure\n",
        "query_device = X_ps_scaled[query_idx:query_idx+1]\n",
        "\n",
        "print(f\"\ud83c\udfaf Query Device: {df_ps.loc[query_idx, 'device_id']}\")\n",
        "print(f\"   Failure Mode: {df_ps.loc[query_idx, 'failure_mode']}\")\n",
        "print(f\"   Key Parameters:\")\n",
        "print(f\"     Leakage Total: {df_ps.loc[query_idx, 'leakage_total']:.2f} \u03bcA (high!)\")\n",
        "print(f\"     Leakage Gate:  {df_ps.loc[query_idx, 'leakage_gate']:.2f} \u03bcA (high!)\")\n",
        "print(f\"     Power Idle:    {df_ps.loc[query_idx, 'power_idle']:.2f} mW (high!)\")\n",
        "\n",
        "# Find K=20 nearest neighbors\n",
        "distances, neighbor_indices = knn_ps.kneighbors(query_device, n_neighbors=21)  # +1 for self\n",
        "neighbor_indices = neighbor_indices[0][1:]  # Exclude self\n",
        "distances = distances[0][1:]\n",
        "\n",
        "print(f\"\\n\ud83d\udd0d 20 Most Similar Devices:\")\n",
        "print(f\"   {'Device':<10} {'Distance':<12} {'Failure Mode':<20} {'Leakage Total':<15}\")\n",
        "print(f\"   {'-'*60}\")\n",
        "\n",
        "for i, (neighbor_idx, dist) in enumerate(zip(neighbor_indices[:10], distances[:10])):\n",
        "    device_id = df_ps.loc[neighbor_idx, 'device_id']\n",
        "    failure_mode = df_ps.loc[neighbor_idx, 'failure_mode']\n",
        "    leakage = df_ps.loc[neighbor_idx, 'leakage_total']\n",
        "    print(f\"   {device_id:<10} {dist:<12.4f} {failure_mode:<20} {leakage:<15.2f}\")\n",
        "\n",
        "print(f\"   ... (showing 10 of 20)\")\n",
        "\n",
        "# Analyze neighbor failure modes\n",
        "neighbor_failure_modes = df_ps.loc[neighbor_indices, 'failure_mode'].value_counts()\n",
        "print(f\"\\n\ud83d\udcca Neighbor Failure Mode Distribution:\")\n",
        "for mode, count in neighbor_failure_modes.items():\n",
        "    print(f\"   {mode:<25} {count:3d} ({count/20*100:5.1f}%)\")\n",
        "\n",
        "# Success metric: How many neighbors have SAME failure mode?\n",
        "same_mode_count = neighbor_failure_modes.get(df_ps.loc[query_idx, 'failure_mode'], 0)\n",
        "print(f\"\\n\u2705 Similarity Validation:\")\n",
        "print(f\"   Neighbors with SAME failure mode: {same_mode_count}/20 ({same_mode_count/20*100:.1f}%)\")\n",
        "print(f\"   Expected by chance: ~0.4% (200 failures / 50K devices)\")\n",
        "print(f\"   Enrichment: {(same_mode_count/20) / 0.004:.1f}x\")\n",
        "\n",
        "print(f\"\\n\ud83d\udca1 Root Cause Insight:\")\n",
        "print(f\"   \u2022 {same_mode_count} neighbors share '{df_ps.loc[query_idx, 'failure_mode']}' failure mode\")\n",
        "print(f\"   \u2022 Common pattern: High leakage + high idle power\")\n",
        "print(f\"   \u2022 Likely root cause: Process defect (gate oxide quality)\")\n",
        "print(f\"   \u2022 Actionable: Check wafer position, lot, equipment correlation\")\n",
        "\n",
        "print(f\"\\n\u23f1\ufe0f Performance:\")\n",
        "start_time = time.time()\n",
        "_, _ = knn_ps.kneighbors(query_device, n_neighbors=21)\n",
        "query_time = time.time() - start_time\n",
        "print(f\"   Query time: {query_time*1000:.2f}ms for 50K device search\")\n",
        "print(f\"   Manual analysis: ~2 hours \u2192 Automated: <1 second (7200x faster!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Visualize spatial distribution of similar devices on wafer map\n",
        "\n",
        "**Key Points:**\n",
        "- **Wafer map**: Scatter plot of die_x vs die_y positions\n",
        "- **Query device**: Red star (failed device being analyzed)\n",
        "- **Similar devices**: Colored by failure mode (shows clustering)\n",
        "- **Spatial correlation**: If neighbors cluster spatially \u2192 process defect\n",
        "- **Random distribution**: If neighbors scattered \u2192 parametric drift\n",
        "\n",
        "**Why This Matters:** Spatial patterns reveal whether failures are location-dependent (process) or random (design).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83d\uddfa\ufe0f Wafer Map Visualization of Similar Devices\\n\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "# Left plot: All devices\n",
        "ax1 = axes[0]\n",
        "for mode in df_ps['failure_mode'].unique():\n",
        "    mask = df_ps['failure_mode'] == mode\n",
        "    color = 'lightgray' if mode == 'Pass' else None\n",
        "    alpha = 0.3 if mode == 'Pass' else 0.7\n",
        "    ax1.scatter(df_ps[mask]['die_x'], df_ps[mask]['die_y'], \n",
        "                c=color, label=mode, alpha=alpha, s=10)\n",
        "\n",
        "ax1.scatter(df_ps.loc[query_idx, 'die_x'], df_ps.loc[query_idx, 'die_y'],\n",
        "            marker='*', s=500, c='red', edgecolors='black', linewidths=2,\n",
        "            label='Query Device', zorder=10)\n",
        "\n",
        "ax1.set_xlabel('Die X Position', fontweight='bold', fontsize=11)\n",
        "ax1.set_ylabel('Die Y Position', fontweight='bold', fontsize=11)\n",
        "ax1.set_title('Wafer Map: All 50K Devices', fontweight='bold', fontsize=13)\n",
        "ax1.legend(loc='upper right', fontsize=9)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Right plot: Query device + 20 nearest neighbors\n",
        "ax2 = axes[1]\n",
        "\n",
        "# Plot all devices in gray background\n",
        "ax2.scatter(df_ps['die_x'], df_ps['die_y'], c='lightgray', alpha=0.2, s=10, label='All devices')\n",
        "\n",
        "# Plot 20 nearest neighbors (colored by failure mode)\n",
        "neighbor_data = df_ps.loc[neighbor_indices]\n",
        "for mode in neighbor_data['failure_mode'].unique():\n",
        "    mask = neighbor_data['failure_mode'] == mode\n",
        "    ax2.scatter(neighbor_data[mask]['die_x'], neighbor_data[mask]['die_y'],\n",
        "                label=f'{mode} (neighbor)', s=100, alpha=0.8, edgecolors='black', linewidths=1.5)\n",
        "\n",
        "# Query device\n",
        "ax2.scatter(df_ps.loc[query_idx, 'die_x'], df_ps.loc[query_idx, 'die_y'],\n",
        "            marker='*', s=500, c='red', edgecolors='black', linewidths=2,\n",
        "            label='Query Device', zorder=10)\n",
        "\n",
        "ax2.set_xlabel('Die X Position', fontweight='bold', fontsize=11)\n",
        "ax2.set_ylabel('Die Y Position', fontweight='bold', fontsize=11)\n",
        "ax2.set_title('Query Device + 20 Nearest Neighbors', fontweight='bold', fontsize=13)\n",
        "ax2.legend(loc='upper right', fontsize=9)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Spatial analysis\n",
        "query_x = df_ps.loc[query_idx, 'die_x']\n",
        "query_y = df_ps.loc[query_idx, 'die_y']\n",
        "neighbor_data = df_ps.loc[neighbor_indices]\n",
        "\n",
        "spatial_distances = np.sqrt((neighbor_data['die_x'] - query_x)**2 + \n",
        "                           (neighbor_data['die_y'] - query_y)**2)\n",
        "mean_spatial_dist = spatial_distances.mean()\n",
        "\n",
        "print(f\"\ud83d\udccd Spatial Analysis:\")\n",
        "print(f\"   Query device position: ({query_x}, {query_y})\")\n",
        "print(f\"   Mean spatial distance to neighbors: {mean_spatial_dist:.2f} die positions\")\n",
        "print(f\"   Wafer diagonal: {np.sqrt(50**2 + 50**2):.2f} die positions\")\n",
        "print(f\"   Spatial clustering: {'YES (likely process defect)' if mean_spatial_dist < 20 else 'NO (parametric similarity)'}\")\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf Interpretation:\")\n",
        "if mean_spatial_dist < 20:\n",
        "    print(f\"   \u2705 Neighbors are spatially CLUSTERED \u2192 Process defect (localized issue)\")\n",
        "    print(f\"   \u2192 Check: Wafer position, lithography, etch uniformity\")\n",
        "else:\n",
        "    print(f\"   \u2705 Neighbors are spatially SCATTERED \u2192 Parametric similarity (not location-dependent)\")\n",
        "    print(f\"   \u2192 Check: Design sensitivity, test conditions, measurement accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u26a0\ufe0f The Curse of Dimensionality\n",
        "\n",
        "### Problem: KNN Degrades in High Dimensions\n",
        "\n",
        "**Mathematical Insight:**\n",
        "\n",
        "As dimensionality $p$ increases, **distances between all points become similar**:\n",
        "\n",
        "$$\n",
        "\\frac{d_{max} - d_{min}}{d_{min}} \\to 0 \\text{ as } p \\to \\infty\n",
        "$$\n",
        "\n",
        "- In high dimensions, **\"nearest\" and \"farthest\" neighbors have similar distances**\n",
        "- Notion of \"neighborhood\" breaks down\n",
        "- KNN predictions become random\n",
        "\n",
        "**Volume Explosion:**\n",
        "\n",
        "To cover 10% of data in:\n",
        "- 1D: Need range of 0.1 (10%)\n",
        "- 2D: Need range of $\\sqrt{0.1} \\approx 0.32$ (32%)\n",
        "- 10D: Need range of $0.1^{1/10} \\approx 0.8$ (80%!)\n",
        "- 100D: Need range of $0.1^{1/100} \\approx 0.977$ (98%!)\n",
        "\n",
        "**Implication:** In 100D, to find 10% of neighbors, you must search 98% of space!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Demonstrate curse of dimensionality with distance distribution analysis\n",
        "\n",
        "**Key Points:**\n",
        "- **Test dimensions**: 2D, 10D, 50D, 100D, 500D\n",
        "- **Distance distribution**: Calculate pairwise distances for 1000 random samples\n",
        "- **Distance ratio**: $(d_{max} - d_{min}) / d_{min}$ measures discrimination\n",
        "- **Expectation**: Ratio \u2192 0 as dimensionality increases (all distances similar)\n",
        "- **Practical limit**: KNN unreliable above ~50-100 dimensions without dimensionality reduction\n",
        "\n",
        "**Why This Matters:** Understanding curse of dimensionality prevents misuse of KNN on high-dim data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "print(\"\ud83c\udf00 Curse of Dimensionality Demonstration\\n\")\n",
        "\n",
        "dimensions = [2, 10, 50, 100, 500]\n",
        "n_samples = 1000\n",
        "results_dim = []\n",
        "\n",
        "for p in dimensions:\n",
        "    # Generate random data in p dimensions\n",
        "    X_dim = np.random.randn(n_samples, p)\n",
        "    \n",
        "    # Calculate pairwise Euclidean distances\n",
        "    distances = pdist(X_dim, metric='euclidean')\n",
        "    \n",
        "    d_min = distances.min()\n",
        "    d_max = distances.max()\n",
        "    d_mean = distances.mean()\n",
        "    d_std = distances.std()\n",
        "    \n",
        "    # Distance ratio: measures discrimination\n",
        "    ratio = (d_max - d_min) / d_min\n",
        "    \n",
        "    results_dim.append({\n",
        "        'p': p,\n",
        "        'd_min': d_min,\n",
        "        'd_max': d_max,\n",
        "        'd_mean': d_mean,\n",
        "        'd_std': d_std,\n",
        "        'ratio': ratio\n",
        "    })\n",
        "    \n",
        "    print(f\"   p={p:3d}: d_min={d_min:6.2f}, d_max={d_max:6.2f}, \"\n",
        "          f\"d_mean={d_mean:6.2f}, ratio={(d_max-d_min)/d_min:6.4f}\")\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Left: Distance distributions for different dimensions\n",
        "ax1 = axes[0]\n",
        "for p in [2, 10, 50, 100]:\n",
        "    X_demo = np.random.randn(500, p)\n",
        "    distances_demo = pdist(X_demo, metric='euclidean')\n",
        "    ax1.hist(distances_demo, bins=50, alpha=0.6, label=f'p={p}')\n",
        "\n",
        "ax1.set_xlabel('Euclidean Distance', fontweight='bold', fontsize=11)\n",
        "ax1.set_ylabel('Frequency', fontweight='bold', fontsize=11)\n",
        "ax1.set_title('Distance Distributions: Low vs High Dimensions', fontweight='bold', fontsize=13)\n",
        "ax1.legend(fontsize=10)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Right: Distance ratio vs dimensionality\n",
        "ax2 = axes[1]\n",
        "p_vals = [r['p'] for r in results_dim]\n",
        "ratio_vals = [r['ratio'] for r in results_dim]\n",
        "\n",
        "ax2.plot(p_vals, ratio_vals, marker='o', linewidth=2, markersize=10, color='red')\n",
        "ax2.set_xlabel('Dimensionality (p)', fontweight='bold', fontsize=11)\n",
        "ax2.set_ylabel('(d_max - d_min) / d_min', fontweight='bold', fontsize=11)\n",
        "ax2.set_title('Distance Discrimination Degrades with Dimensionality', fontweight='bold', fontsize=13)\n",
        "ax2.set_xscale('log')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf Observations:\")\n",
        "print(f\"   \u2022 p=2:   Clear separation between nearest/farthest (ratio={results_dim[0]['ratio']:.2f})\")\n",
        "print(f\"   \u2022 p=10:  Moderate discrimination (ratio={results_dim[1]['ratio']:.2f})\")\n",
        "print(f\"   \u2022 p=100: Poor discrimination (ratio={results_dim[3]['ratio']:.2f})\")\n",
        "print(f\"   \u2022 p=500: Almost no discrimination (ratio={results_dim[4]['ratio']:.2f})\")\n",
        "\n",
        "print(f\"\\n\u26a0\ufe0f Practical Implications:\")\n",
        "print(f\"   \u2022 KNN works well: p < 20-30 (low/medium dimensions)\")\n",
        "print(f\"   \u2022 KNN struggles: p > 50 (high dimensions)\")\n",
        "print(f\"   \u2022 Solutions:\")\n",
        "print(f\"     1. Dimensionality reduction: PCA, t-SNE, UMAP (reduce to p=10-20)\")\n",
        "print(f\"     2. Feature selection: Keep only most important features\")\n",
        "print(f\"     3. Use different model: Random Forest, XGBoost (handle high-dim better)\")\n",
        "print(f\"     4. Distance metric: Try Manhattan (L1) instead of Euclidean (L2)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf Real-World KNN Projects\n",
        "\n",
        "### \ud83d\udd2c Post-Silicon Validation Projects (4)\n",
        "\n",
        "### **1. Similar Failure Detection for Root Cause Analysis**\n",
        "**Objective:** Automated similarity search across 100K+ devices for failure investigation\n",
        "\n",
        "**Features:**\n",
        "- 25 parametric tests (voltage, current, frequency, timing, power, leakage)\n",
        "- Spatial features: die_x, die_y, wafer_id\n",
        "- Categorical: equipment_id, lot_id, test_program_version\n",
        "- Query: Failed device \u2192 Find K=50 most similar devices\n",
        "- Distance metric: Euclidean on scaled features\n",
        "\n",
        "**Success Metrics:**\n",
        "- Neighbor purity: >80% share same failure mode\n",
        "- Root cause time: 2 hours \u2192 5 minutes (95% reduction)\n",
        "- False positives: <10% (irrelevant neighbors)\n",
        "- Query latency: <100ms for 100K device search\n",
        "\n",
        "**Business Value:** $500K-2M per major failure investigation saved\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Reference Die Matching for Adaptive Testing**\n",
        "**Objective:** Match new devices to reference database for test flow optimization\n",
        "\n",
        "**Features:**\n",
        "- First 5 parametric tests (early in test flow)\n",
        "- Historical database: 1M+ reference devices with known outcomes\n",
        "- KNN query: New device \u2192 Find K=100 similar historical devices\n",
        "- Prediction: Skip unnecessary tests if all neighbors passed same tests\n",
        "- Adaptive: Test flow customized based on similarity\n",
        "\n",
        "**Success Metrics:**\n",
        "- Test time reduction: 15-25% (skip redundant tests)\n",
        "- Test escape rate: <0.1% (safety maintained)\n",
        "- Throughput increase: 500K \u2192 650K devices/day\n",
        "- Database update: Real-time (new devices added to reference)\n",
        "\n",
        "**Business Value:** $2-5M annual savings per production line\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Wafer Map Spatial Similarity Clustering**\n",
        "**Objective:** Identify spatial failure patterns using KNN clustering\n",
        "\n",
        "**Features:**\n",
        "- Spatial: die_x, die_y (wafer coordinates)\n",
        "- Parametric: 10 key tests (voltage, leakage, frequency)\n",
        "- Combined distance: 50% spatial + 50% parametric (weighted metric)\n",
        "- KNN clustering: Find K=20 neighbors for each die\n",
        "- Pattern detection: Clusters with >80% failures \u2192 spatial defect\n",
        "\n",
        "**Success Metrics:**\n",
        "- Defect detection: Identify 95% of systematic patterns\n",
        "- False alarms: <5% (avoid spurious patterns)\n",
        "- Real-time: Analyze 40K die wafer in <5 seconds\n",
        "- Visualization: Automatic wafer map highlighting\n",
        "\n",
        "**Business Value:** Early detection \u2192 $10-30M saved per fab year\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Multi-Site Test Correlation via Nearest Neighbor Analysis**\n",
        "**Objective:** Find similar devices across 3 test sites for correlation analysis\n",
        "\n",
        "**Features:**\n",
        "- Site A: 15 parametric tests (wafer test)\n",
        "- Site B: 20 parametric tests (final test)\n",
        "- Site C: 10 parametric tests (system test)\n",
        "- KNN across sites: Device at Site B \u2192 Find similar at Site A and C\n",
        "- Correlation analysis: Predict Site C results from Site A+B similarity\n",
        "\n",
        "**Success Metrics:**\n",
        "- Cross-site prediction: R\u00b2 > 0.85\n",
        "- Test elimination: Skip 30% of redundant tests at Site C\n",
        "- Escapes detected: Catch 90% of discrepancies between sites\n",
        "- Latency: <50ms per device query across 3 databases\n",
        "\n",
        "**Business Value:** $5-15M annual test cost reduction\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83c\udf10 General AI/ML Projects (4)\n",
        "\n",
        "### **5. Content-Based Recommendation Engine**\n",
        "**Objective:** Recommend products based on item similarity (not collaborative filtering)\n",
        "\n",
        "**Features:**\n",
        "- Product attributes: price, category, brand, color, size (15 features)\n",
        "- User viewing history: Last 10 viewed items\n",
        "- KNN query: Current product \u2192 Find K=20 similar products\n",
        "- Recommendation: Show similar items based on content, not user behavior\n",
        "- Distance: Cosine similarity for categorical, Euclidean for numerical\n",
        "\n",
        "**Success Metrics:**\n",
        "- Click-through rate: 8-12% (vs 5% random)\n",
        "- Conversion rate: 15-20% increase\n",
        "- Latency: <20ms per recommendation\n",
        "- Cold start: Works for new users (no behavior history needed)\n",
        "\n",
        "**Business Value:** 10-15% revenue increase \u2192 $5-20M for mid-size e-commerce\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Medical Diagnosis via Case-Based Reasoning**\n",
        "**Objective:** Find similar patient cases for diagnostic support\n",
        "\n",
        "**Features:**\n",
        "- Patient attributes: age, gender, BMI, blood pressure, lab results (50 features)\n",
        "- Symptoms: 20 binary indicators (fever, cough, fatigue, etc.)\n",
        "- Medical history: 10 categorical (diabetes, hypertension, etc.)\n",
        "- KNN query: New patient \u2192 Find K=10 most similar past cases\n",
        "- Diagnosis: Majority vote among neighbor diagnoses\n",
        "\n",
        "**Success Metrics:**\n",
        "- Diagnostic accuracy: 80-85% (comparable to junior doctors)\n",
        "- Top-3 accuracy: 95% (true diagnosis in top 3 suggestions)\n",
        "- Confidence: High when neighbors agree (>80% same diagnosis)\n",
        "- Explanation: Show doctor the 10 similar cases for validation\n",
        "\n",
        "**Business Value:** Diagnostic support \u2192 Reduce misdiagnosis 20-30%\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Anomaly Detection via Isolation Distance**\n",
        "**Objective:** Detect anomalies by finding samples far from all neighbors\n",
        "\n",
        "**Features:**\n",
        "- Transaction data: amount, merchant, location, time, user behavior (30 features)\n",
        "- KNN query: Each transaction \u2192 Find K=20 nearest neighbors\n",
        "- Anomaly score: Average distance to K neighbors (high = anomaly)\n",
        "- Threshold: Flag top 1% as suspicious (tunable)\n",
        "- Real-time: Score new transactions in <10ms\n",
        "\n",
        "**Success Metrics:**\n",
        "- Fraud detection: 85-90% recall at 5% false positive rate\n",
        "- Novel fraud: Catch new patterns (distance-based, not rule-based)\n",
        "- Latency: <10ms (real-time authorization)\n",
        "- Explainability: Show why transaction is anomalous (far from all neighbors)\n",
        "\n",
        "**Business Value:** Block $10-50M fraud annually, reduce false declines 20%\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Image Search via Feature Similarity (CNN Embeddings + KNN)**\n",
        "**Objective:** Find visually similar images using deep learning embeddings\n",
        "\n",
        "**Features:**\n",
        "- CNN embeddings: ResNet50 last layer (2048-dim vectors)\n",
        "- Dimensionality reduction: PCA to 128-dim (curse of dimensionality mitigation)\n",
        "- KNN index: FAISS library for billion-scale search\n",
        "- Query: Input image \u2192 Extract embedding \u2192 Find K=50 nearest neighbors\n",
        "- Distance: Cosine similarity (angle-based)\n",
        "\n",
        "**Success Metrics:**\n",
        "- Precision@10: >80% (8 out of 10 results are relevant)\n",
        "- Query latency: <50ms for 10M image database\n",
        "- Scale: 1B+ images with approximate NN (FAISS)\n",
        "- User satisfaction: 4.2/5 stars for result quality\n",
        "\n",
        "**Business Value:** Power visual search for e-commerce, stock photo sites\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u2705 Key Takeaways: KNN Mastery\n",
        "\n",
        "### \ud83c\udfaf When to Use KNN\n",
        "\n",
        "**\u2705 KNN Excels When:**\n",
        "- Small-medium datasets (<100K samples)\n",
        "- Low-medium dimensions (p < 20-30 features)\n",
        "- Interpretability matters (\"You're like these 5 examples\")\n",
        "- Non-parametric (no assumptions about data distribution)\n",
        "- Similarity search is the goal (recommendation, case-based reasoning)\n",
        "- Local patterns matter (complex, non-linear decision boundaries)\n",
        "\n",
        "**\u274c Avoid KNN When:**\n",
        "- Large datasets (>1M samples) \u2014 prediction too slow\n",
        "- High dimensions (p > 50) \u2014 curse of dimensionality\n",
        "- Real-time inference (<1ms) \u2014 distance calculation bottleneck\n",
        "- Features have vastly different scales \u2014 MUST scale, or KNN fails\n",
        "- Memory constrained \u2014 stores ALL training data\n",
        "- Categorical features dominate \u2014 use CatBoost or one-hot + scale\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd11 Critical Success Factors\n",
        "\n",
        "1. **ALWAYS Scale Features**\n",
        "   - Use StandardScaler or MinMaxScaler\n",
        "   - Unscaled features \u2192 large-range features dominate distance\n",
        "   - Impact: 20-40% accuracy difference\n",
        "\n",
        "2. **Choose K via Cross-Validation**\n",
        "   - Small K (1-3): High variance, overfitting\n",
        "   - Large K (50+): High bias, underfitting\n",
        "   - Rule of thumb: Start with K = \u221aN\n",
        "   - Use odd K for binary classification (avoid ties)\n",
        "\n",
        "3. **Distance Metric Matters**\n",
        "   - Euclidean: Default, continuous features\n",
        "   - Manhattan: Outlier-robust, grid-like data\n",
        "   - Cosine: Text, embeddings, high-dimensional\n",
        "   - Experiment: 5-15% accuracy variation\n",
        "\n",
        "4. **Weighted Voting Usually Better**\n",
        "   - weights='distance': Closer neighbors more influential\n",
        "   - Improvement: 1-3% over uniform voting\n",
        "   - Use for complex boundaries\n",
        "\n",
        "5. **Mitigate Curse of Dimensionality**\n",
        "   - Dimensionality reduction: PCA, t-SNE, UMAP (p \u2192 10-20)\n",
        "   - Feature selection: Keep only important features\n",
        "   - Alternative: Switch to Random Forest, XGBoost (handle high-dim)\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udcca Performance Expectations\n",
        "\n",
        "| Dataset Size | Dimensionality | Training Time | Prediction Time | Accuracy vs RF/XGB |\n",
        "|--------------|----------------|---------------|-----------------|--------------------|\n",
        "| <10K         | p < 20         | Instant       | <10ms           | Comparable         |\n",
        "| 10K-100K     | p < 20         | Instant       | 10-100ms        | Comparable         |\n",
        "| >100K        | p < 20         | Instant       | >100ms          | Similar, but slow  |\n",
        "| Any          | p > 50         | Instant       | Any             | Poor (curse of dim)|\n",
        "\n",
        "*Training time always instant (just stores data), prediction is bottleneck*\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd27 Best Practices\n",
        "\n",
        "1. **Always use StandardScaler/MinMaxScaler before KNN**\n",
        "   ```python\n",
        "   scaler = StandardScaler()\n",
        "   X_train_scaled = scaler.fit_transform(X_train)\n",
        "   X_test_scaled = scaler.transform(X_test)\n",
        "   ```\n",
        "\n",
        "2. **Cross-validation for K selection**\n",
        "   ```python\n",
        "   k_values = [1, 3, 5, 7, 9, 11, 15, 20]\n",
        "   for k in k_values:\n",
        "       scores = cross_val_score(KNN(n_neighbors=k), X, y, cv=5)\n",
        "   best_k = k_values[np.argmax(mean_scores)]\n",
        "   ```\n",
        "\n",
        "3. **Use KD-tree for low dimensions (<20), brute force for high**\n",
        "   ```python\n",
        "   knn = KNeighborsClassifier(algorithm='auto')  # sklearn chooses best\n",
        "   ```\n",
        "\n",
        "4. **For large datasets, use approximate NN (FAISS, Annoy)**\n",
        "   ```python\n",
        "   import faiss\n",
        "   index = faiss.IndexFlatL2(d)  # d = dimensionality\n",
        "   index.add(X_train)  # Add training data\n",
        "   D, I = index.search(X_test, k=10)  # 10-100x faster\n",
        "   ```\n",
        "\n",
        "5. **Dimensionality reduction for p > 50**\n",
        "   ```python\n",
        "   from sklearn.decomposition import PCA\n",
        "   pca = PCA(n_components=20)\n",
        "   X_reduced = pca.fit_transform(X)\n",
        "   knn.fit(X_reduced, y)  # KNN on reduced features\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\ude80 Next Steps\n",
        "\n",
        "1. **024_Support_Vector_Machines.ipynb** - Kernel methods, margin optimization\n",
        "2. **025_Naive_Bayes.ipynb** - Probabilistic classification\n",
        "3. **026_K_Means_Clustering.ipynb** - Unsupervised learning with KNN-like distance\n",
        "4. **030_Dimensionality_Reduction.ipynb** - PCA, t-SNE, UMAP for curse mitigation\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83c\udf93 What You've Mastered\n",
        "\n",
        "\u2705 **Instance-based learning** - No training, similarity-based prediction  \n",
        "\u2705 **Distance metrics** - Euclidean, Manhattan, Minkowski, Cosine  \n",
        "\u2705 **K selection** - Bias-variance trade-off, cross-validation  \n",
        "\u2705 **Feature scaling** - CRITICAL for KNN success  \n",
        "\u2705 **Voting strategies** - Uniform vs distance-weighted  \n",
        "\u2705 **Curse of dimensionality** - Why KNN fails in high dimensions  \n",
        "\u2705 **KNN regression** - Local averaging for continuous predictions  \n",
        "\u2705 **Similarity search** - Root cause analysis, recommendation systems  \n",
        "\u2705 **Production optimization** - KD-tree, FAISS, approximate NN  \n",
        "\u2705 **Business applications** - $500K-50M impact across domains  \n",
        "\n",
        "You now understand when KNN shines and when to use alternatives! \ud83c\udf89\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcda References and Further Reading\n",
        "\n",
        "### Original Papers\n",
        "\n",
        "1. **Nearest Neighbor Pattern Classification** (1967)  \n",
        "   Cover & Hart, IEEE Transactions on Information Theory  \n",
        "   Classic paper establishing KNN foundations\n",
        "\n",
        "2. **The Curse of Dimensionality in Data Mining and Time Series Prediction** (2005)  \n",
        "   Verleysen & Fran\u00e7ois, IWANN 2005  \n",
        "   Comprehensive analysis of dimensionality problems\n",
        "\n",
        "3. **An Investigation of Practical Approximate Nearest Neighbor Algorithms** (2004)  \n",
        "   Liu et al., NeurIPS 2004  \n",
        "   Comparison of fast NN search algorithms\n",
        "\n",
        "### Official Documentation\n",
        "\n",
        "4. **sklearn Neighbors Module**  \n",
        "   https://scikit-learn.org/stable/modules/neighbors.html\n",
        "\n",
        "5. **KNeighborsClassifier API**  \n",
        "   https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
        "\n",
        "6. **KNeighborsRegressor API**  \n",
        "   https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\n",
        "\n",
        "### Fast Nearest Neighbor Libraries\n",
        "\n",
        "7. **FAISS (Facebook AI Similarity Search)**  \n",
        "   https://github.com/facebookresearch/faiss  \n",
        "   Billion-scale approximate NN, GPU support\n",
        "\n",
        "8. **Annoy (Approximate Nearest Neighbors Oh Yeah)**  \n",
        "   https://github.com/spotify/annoy  \n",
        "   Spotify's library for fast NN search\n",
        "\n",
        "9. **NMSLIB (Non-Metric Space Library)**  \n",
        "   https://github.com/nmslib/nmslib  \n",
        "   Fast NN for various distance metrics\n",
        "\n",
        "### Books\n",
        "\n",
        "10. **The Elements of Statistical Learning** (Hastie, Tibshirani, Friedman)  \n",
        "    Chapter 13: Prototype Methods (includes KNN)\n",
        "\n",
        "11. **Pattern Recognition and Machine Learning** (Bishop)  \n",
        "    Chapter 2.5: Nearest Neighbor Methods\n",
        "\n",
        "### Related Notebooks\n",
        "\n",
        "- **024_Support_Vector_Machines.ipynb** (next) - Another instance-based method\n",
        "- **026_K_Means_Clustering.ipynb** - Similar distance-based approach\n",
        "- **030_Dimensionality_Reduction.ipynb** - Mitigation for curse of dimensionality\n",
        "- **010_Linear_Regression.ipynb** - Contrast with parametric models\n",
        "- **016_Decision_Trees.ipynb** - Alternative for high-dim data\n",
        "\n",
        "---\n",
        "\n",
        "**Notebook Complete!** \u2705  \n",
        "**Next:** 024_Support_Vector_Machines.ipynb - Kernel methods and margin optimization\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}