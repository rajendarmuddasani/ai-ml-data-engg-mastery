# 04 - Distance-Based Models

**Purpose:** Master instance-based and margin-based learning algorithms

**Why Distance-Based Models?** These algorithms work fundamentally differently from tree-based models. KNN uses similarity between instances (lazy learning), SVM finds optimal decision boundaries (margin maximization), and Naive Bayes leverages probability theory. Each has unique strengths: KNN for irregular decision boundaries, SVM for high-dimensional data, Naive Bayes for real-time inference.

---

## üìö Notebooks (023-025)

### **Instance-Based Learning**

#### **023_K_Nearest_Neighbors.ipynb** ‚úÖ (35 cells)
**Lazy learning - classify based on similarity to training examples**

**Topics Covered:**
- KNN algorithm for classification and regression
- Distance metrics (Euclidean, Manhattan, Minkowski, Cosine, Hamming)
- K selection (cross-validation, elbow method)
- Weighted KNN (distance weighting)
- Curse of dimensionality and feature scaling
- Efficient search: KD-tree, Ball tree, Locality-Sensitive Hashing
- Handling imbalanced data

**Mathematical Foundation:**
- Euclidean distance: d(x,y) = ‚àö(Œ£(x·µ¢-y·µ¢)¬≤)
- Manhattan distance: d(x,y) = Œ£|x·µ¢-y·µ¢|
- Minkowski distance: d(x,y) = (Œ£|x·µ¢-y·µ¢|·µñ)^(1/p)
- Weighted voting: w(x) = 1/d(x,x·µ¢)¬≤

**Key Concepts:**
- **Lazy Learning**: No training phase - stores all data and computes at prediction time
- **K Selection**: Small K ‚Üí complex boundaries (overfitting), Large K ‚Üí smooth boundaries (underfitting)
- **Distance Weighting**: Closer neighbors have more influence
- **Curse of Dimensionality**: In high dimensions, all points become equidistant

**Real-World Applications:**
- **Post-Silicon**: Similar die failure pattern matching on wafer maps
- **Post-Silicon**: Device parameter clustering (find similar test results)
- **Post-Silicon**: Nearest-neighbor outlier detection (parametric anomalies)
- **General**: Recommendation systems (find similar users/items)
- **General**: Image recognition (find similar images)
- **General**: Anomaly detection (distance to normal behavior)

**Advantages:**
- Simple and intuitive
- No training phase (online learning)
- Naturally handles multi-class problems
- Non-parametric (makes no assumptions about data distribution)

**Disadvantages:**
- Slow prediction (O(N) for each prediction)
- Memory intensive (stores entire dataset)
- Sensitive to irrelevant features
- Requires feature scaling

**Learning Outcomes:**
- Implement KNN from scratch
- Choose appropriate distance metrics
- Optimize K using cross-validation
- Apply efficient search structures (KD-tree)
- Handle curse of dimensionality with feature selection
- Scale features properly before KNN

**Performance Tips:**
- Use KD-tree or Ball tree for large datasets (100√ó speedup)
- Apply PCA for dimensionality reduction
- Use feature selection to remove irrelevant features
- Scale features to [0,1] range

---

### **Margin-Based Learning**

#### **024_Support_Vector_Machines.ipynb** ‚úÖ (27 cells)
**Maximum margin classifiers - find optimal decision boundaries**

**Topics Covered:**
- SVM for classification (SVC) and regression (SVR)
- Linear SVM and maximum margin hyperplane
- Kernel trick (RBF, polynomial, sigmoid)
- Soft margin (C parameter) for non-separable data
- Hyperparameter tuning (C, gamma, kernel)
- Support vectors and decision function
- Multi-class strategies (one-vs-one, one-vs-rest)

**Mathematical Foundation:**
- Decision boundary: w¬∑x + b = 0
- Margin: 2/||w||
- Optimization: min(1/2)||w||¬≤ + C¬∑Œ£Œæ·µ¢
- Kernel functions:
  - Linear: K(x,y) = x¬∑y
  - RBF: K(x,y) = exp(-Œ≥||x-y||¬≤)
  - Polynomial: K(x,y) = (Œ≥x¬∑y + r)·µà

**Key Concepts:**
- **Maximum Margin**: Find hyperplane that maximizes distance to nearest points
- **Support Vectors**: Only points near boundary matter (sparsity)
- **Kernel Trick**: Map data to higher dimensions without explicit computation
- **C Parameter**: Controls trade-off between margin width and misclassifications
- **Gamma Parameter**: Controls influence of single training example (RBF kernel)

**Real-World Applications:**
- **Post-Silicon**: Binary pass/fail classification (high accuracy needed)
- **Post-Silicon**: Wafer bin classification (speed vs power categories)
- **Post-Silicon**: High-dimensional parametric space classification (100+ features)
- **General**: Text classification (high-dimensional sparse data)
- **General**: Image classification (kernel methods)
- **General**: Bioinformatics (gene expression classification)

**Hyperparameter Tuning Guide:**
```python
# Small C ‚Üí Wide margin (underfitting risk)
# Large C ‚Üí Narrow margin (overfitting risk)
C = [0.1, 1, 10, 100]

# Small gamma ‚Üí Far influence (smooth boundary)
# Large gamma ‚Üí Close influence (complex boundary)
gamma = [0.001, 0.01, 0.1, 1]

# Kernel selection
kernel = ['linear', 'rbf', 'poly']
```

**When to Use SVM:**
- ‚úÖ High-dimensional data (text, genomics)
- ‚úÖ Clear margin of separation exists
- ‚úÖ More features than samples
- ‚úÖ Need kernel methods for non-linearity
- ‚ùå Very large datasets (slow training O(N¬≤))
- ‚ùå Noisy data with overlapping classes
- ‚ùå Need probability estimates (use Platt scaling)

**Learning Outcomes:**
- Understand maximum margin concept
- Apply kernel trick for non-linear problems
- Tune C and gamma systematically
- Choose appropriate kernel for problem
- Interpret support vectors
- Scale SVM to large datasets

**Performance Comparison:**
- **Linear SVM**: Fast, good for linearly separable data
- **RBF SVM**: Most popular, handles non-linearity well
- **Polynomial SVM**: Good for image data, needs careful tuning
- **Training Time**: O(N¬≤) to O(N¬≥) - slow for large N

---

### **Probabilistic Learning**

#### **025_Naive_Bayes.ipynb** ‚úÖ (22 cells)
**Fast probabilistic classifiers based on Bayes' theorem**

**Topics Covered:**
- Bayes' theorem and conditional probability
- Naive independence assumption
- Gaussian Naive Bayes (continuous features)
- Multinomial Naive Bayes (discrete counts, text)
- Bernoulli Naive Bayes (binary features)
- Laplace smoothing (additive smoothing)
- Probability calibration
- Real-time inference (< 1ms)

**Mathematical Foundation:**
- Bayes' theorem: P(y|X) = P(X|y)¬∑P(y) / P(X)
- Naive assumption: P(X|y) = P(x‚ÇÅ|y)¬∑P(x‚ÇÇ|y)¬∑...¬∑P(x‚Çô|y)
- Gaussian NB: P(x·µ¢|y) = (1/‚àö(2œÄœÉ¬≤))¬∑exp(-(x·µ¢-Œº)¬≤/(2œÉ¬≤))
- Multinomial NB: P(x·µ¢|y) = (count(x·µ¢,y) + Œ±) / (count(y) + Œ±¬∑n_features)
- Laplace smoothing: Add Œ± (typically 1) to avoid zero probabilities

**Key Concepts:**
- **Naive Independence**: Assume features are conditionally independent (rarely true, but works well)
- **Generative Model**: Models P(X|y) and P(y), then uses Bayes' theorem
- **Extremely Fast**: Training and prediction both very fast
- **Works with Small Data**: Effective even with limited training samples

**Naive Bayes Variants:**

**1. Gaussian NB:**
- For continuous features with normal distribution
- Use case: Sensor data, measurements, parametric test values

**2. Multinomial NB:**
- For discrete count features
- Use case: Text classification (word counts), document categorization

**3. Bernoulli NB:**
- For binary features (present/absent)
- Use case: Text classification (word presence), feature presence detection

**Real-World Applications:**
- **Post-Silicon**: Real-time test failure classification (< 10ms inference)
- **Post-Silicon**: Lot-level yield prediction (fast screening)
- **Post-Silicon**: Email-style log classification (Multinomial NB)
- **General**: Spam filtering (classic application, 99%+ accuracy)
- **General**: Sentiment analysis (text classification)
- **General**: Medical diagnosis (symptom ‚Üí disease probability)
- **General**: Real-time fraud detection (millisecond latency)

**Advantages:**
- ‚ö° Extremely fast training and prediction
- üìä Works well with small datasets
- üìà Handles high-dimensional data naturally
- üî¢ Provides probability estimates
- üíæ Low memory footprint
- üöÄ Easy to implement and interpret

**Disadvantages:**
- Naive independence assumption often violated
- Cannot learn feature interactions
- Sensitive to irrelevant features
- Probability estimates not always well-calibrated

**When to Use Naive Bayes:**
- ‚úÖ Text classification (spam, sentiment, categorization)
- ‚úÖ Real-time inference required (< 10ms)
- ‚úÖ Limited training data
- ‚úÖ High-dimensional data (text, sparse features)
- ‚úÖ Baseline model (fast to try)
- ‚ùå Features are highly correlated
- ‚ùå Need to model feature interactions
- ‚ùå Need perfectly calibrated probabilities

**Learning Outcomes:**
- Understand Bayes' theorem intuitively
- Implement Naive Bayes from scratch
- Choose appropriate variant (Gaussian/Multinomial/Bernoulli)
- Apply Laplace smoothing correctly
- Build real-time classifiers
- Calibrate probability estimates

**Performance:**
- Training: O(N¬∑D) - very fast
- Prediction: O(C¬∑D) - very fast (C = # classes, D = # features)
- Typically 70-85% accuracy (baseline, but very fast)

---

## üéØ Learning Path

**Recommended Order:**
1. **023 - K-Nearest Neighbors** ‚≠ê **START HERE** - Simplest algorithm, builds intuition
2. **024 - Support Vector Machines** - More sophisticated boundary finding
3. **025 - Naive Bayes** - Probabilistic approach, fastest inference

**Time Estimate:** 1-2 weeks (intensive) | 2-3 weeks (moderate pace)

---

## üìä Section Statistics

| Metric | Value |
|--------|-------|
| **Total Notebooks** | 3 |
| **Complete Notebooks** | 3 (100% ‚úÖ) |
| **Total Cells** | 84+ |
| **Real-World Projects** | 18+ |
| **Algorithms Covered** | 3 |

---

## üîë Key Learning Outcomes

After completing this section, you will:

‚úÖ **Algorithm Understanding**
- Master instance-based learning (KNN)
- Understand margin-based learning (SVM)
- Apply probabilistic learning (Naive Bayes)

‚úÖ **Distance Metrics**
- Choose appropriate metrics for data type
- Handle curse of dimensionality
- Scale features properly

‚úÖ **Kernel Methods**
- Apply kernel trick for non-linearity
- Tune kernel hyperparameters
- Understand computational tradeoffs

‚úÖ **Production Skills**
- Build real-time classifiers (< 10ms)
- Handle high-dimensional data
- Choose algorithm based on requirements

‚úÖ **Domain Applications**
- Classify semiconductor test failures
- Match similar die failure patterns
- Real-time yield prediction

---

## üîó Prerequisites

**Before starting this section, complete:**
- ‚úÖ **[02_Regression_Models](../02_Regression_Models/)** - Classification basics
- ‚úÖ Distance metrics (Euclidean, Manhattan, Cosine)
- ‚úÖ Probability theory (Bayes' theorem, conditional probability)
- ‚úÖ Linear algebra (dot products, norms)

---

## ‚û°Ô∏è Next Steps

After mastering distance-based models, continue to:

1. **[05_Clustering](../05_Clustering/)** - Unsupervised learning, finding patterns without labels
2. **[06_ML_Engineering](../06_ML_Engineering/)** - Feature engineering, model evaluation
3. **[03_Tree_Based_Models](../03_Tree_Based_Models/)** - Compare with ensemble methods

---

## üí° Study Tips

1. **Visualize Decision Boundaries** - Plot 2D examples to understand how each algorithm works
2. **Feature Scaling is Critical** - Always scale features for KNN and SVM (not needed for Naive Bayes)
3. **Start Simple** - Try linear models before kernel methods
4. **Use Grid Search** - Systematically tune C, gamma, K
5. **Benchmark Speed** - Compare training and inference times
6. **Understand Tradeoffs** - KNN (slow prediction), SVM (slow training), NB (fast everything)

---

## üõ†Ô∏è Tools & Libraries

**Core Libraries:**
- Scikit-learn (all three algorithms)
- Faiss (fast similarity search for KNN)
- LIBSVM (underlying SVM implementation)

**Distance Metrics:**
- Scipy.spatial.distance (comprehensive metrics)
- Scikit-learn.metrics.pairwise (pairwise distances)

**Visualization:**
- Matplotlib (decision boundaries)
- Seaborn (confusion matrices)

---

## üìà Algorithm Comparison

| Algorithm | Training Speed | Prediction Speed | Accuracy | Interpretability | Best For |
|-----------|----------------|------------------|----------|------------------|----------|
| **KNN** | ‚ö° Instant (lazy) | üê¢ Slow O(N) | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Irregular boundaries, small data |
| **SVM** | üê¢ Slow O(N¬≤-N¬≥) | ‚ö° Fast O(SV) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | High-dim, clear margins |
| **Naive Bayes** | ‚ö°‚ö° Very fast | ‚ö°‚ö° Very fast | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Text, real-time, baseline |

**When to Use Each:**
- **KNN**: Small datasets (<10K), irregular decision boundaries, need explainability
- **SVM**: High-dimensional data, clear separation, accuracy critical
- **Naive Bayes**: Text classification, real-time inference, limited training data

---

## üìà Progress Tracking

Mark notebooks as complete as you master them:

- [ ] 023_K_Nearest_Neighbors ‚≠ê **START HERE**
- [ ] 024_Support_Vector_Machines
- [ ] 025_Naive_Bayes

---

## üåü Why This Section Matters

**Industry Relevance:**
- KNN: Used in recommendation systems, anomaly detection
- SVM: Gold standard for text classification, bioinformatics
- Naive Bayes: Deployed in spam filters (Gmail uses it), real-time systems

**Career Impact:**
- Core ML algorithms in technical interviews
- Understand when NOT to use deep learning
- Foundation for understanding kernel methods
- Critical for building fast inference systems

**Unique Strengths:**
- Fundamentally different from tree-based methods
- Teaches distance metrics and similarity
- Introduces kernel methods (foundation for deep learning)
- Shows probabilistic reasoning approach

---

**Last Updated:** December 2025  
**Status:** All 3 notebooks complete (100% ‚úÖ)  
**Maintainer:** [@rajendarmuddasani](https://github.com/rajendarmuddasani)
