{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 024: Support Vector Machines (SVM)",
    "",
    "## \ud83d\udcd8 What You'll Master",
    "",
    "**Support Vector Machines (SVM)** are **maximum margin classifiers** that find the optimal decision boundary by maximizing the distance (margin) to the nearest training examples. Unlike KNN (instance-based) or trees (rule-based), SVMs use **kernel tricks** to handle non-linear boundaries in high-dimensional spaces.",
    "",
    "### \ud83c\udfaf Why SVMs Matter",
    "",
    "1. **Optimal boundary**: Maximizes margin \u2192 better generalization than arbitrary boundaries",
    "2. **Kernel trick**: Handles non-linear problems without explicit feature engineering",
    "3. **Works in high dimensions**: Effective even when p >> n (features > samples)",
    "4. **Robust to outliers**: Only support vectors matter (not all training points)",
    "5. **Theoretical foundation**: Strong mathematical guarantees (VC theory)",
    "",
    "### \ud83d\udd2c Real-World Applications",
    "",
    "**Post-Silicon Validation:**",
    "- **Pass/fail classification**: Binary decision with optimal boundary (minimize false positives/negatives)",
    "- **Defect detection**: Non-linear boundaries for complex failure patterns",
    "- **Margin-based binning**: Confidence-based device categorization",
    "- **High-dimensional test data**: 100+ parametric tests, SVM handles well",
    "",
    "**General AI/ML:**",
    "- **Text classification**: Spam detection, sentiment analysis (high-dimensional TF-IDF)",
    "- **Image classification**: Face detection, object recognition (with kernel tricks)",
    "- **Bioinformatics**: Gene expression classification, protein structure prediction",
    "- **Anomaly detection**: One-class SVM for outlier detection",
    "",
    "### \ud83d\udcca Learning Path Context",
    "",
    "```mermaid",
    "graph LR",
    "    A[Linear Models<br/>010-015] --> B[Tree Models<br/>016-018]",
    "    B --> C[Boosting<br/>019-021]",
    "    C --> D[Meta-Ensembles<br/>022]",
    "    D --> E[KNN<br/>023]",
    "    E --> F[SVM<br/>024 YOU ARE HERE]",
    "    F --> G[Naive Bayes<br/>025]",
    "    G --> H[Clustering<br/>026-030]",
    "    ",
    "    style F fill:#ff6b6b,stroke:#c92a2a,stroke-width:3px,color:#fff",
    "```",
    "",
    "**What Makes SVM Different:**",
    "- **Geometric approach**: Maximize margin (distance) to decision boundary",
    "- **Support vectors**: Only a subset of training points define the model",
    "- **Kernel methods**: Implicit mapping to infinite-dimensional space",
    "- **Convex optimization**: Global optimum guaranteed (no local minima)",
    "",
    "**Contrast with Previous Methods:**",
    "- **vs Linear Regression**: SVM maximizes margin, not minimizes error",
    "- **vs Decision Trees**: SVM finds global optimal boundary, trees are greedy",
    "- **vs KNN**: SVM learns explicit boundary, KNN stores all data",
    "- **vs Ensembles**: SVM is single model, ensembles combine multiple",
    "",
    "---",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd0d How SVM Works: The Margin Maximization Principle\n",
    "\n",
    "### SVM Classification Flow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Training Data<br/>X, y] --> B{Linearly<br/>Separable?}\n",
    "    B -->|Yes| C[Linear SVM<br/>Hard Margin]\n",
    "    B -->|Almost| D[Linear SVM<br/>Soft Margin C parameter]\n",
    "    B -->|No| E[Non-Linear SVM<br/>Kernel Trick]\n",
    "    \n",
    "    C --> F[Find Hyperplane<br/>w\u00b7x + b = 0]\n",
    "    D --> F\n",
    "    E --> G[Map to Higher Dim<br/>\u03c6 x: RBF/Poly/Sigmoid]\n",
    "    \n",
    "    G --> H[Find Hyperplane<br/>in Transformed Space]\n",
    "    F --> I[Maximize Margin<br/>2/||w||]\n",
    "    H --> I\n",
    "    \n",
    "    I --> J[Identify<br/>Support Vectors]\n",
    "    J --> K[Decision Function<br/>sign w\u00b7x + b]\n",
    "    \n",
    "    style A fill:#e3f2fd\n",
    "    style E fill:#fff3e0\n",
    "    style K fill:#c8e6c9\n",
    "```\n",
    "\n",
    "### \ud83d\udcdd What's Happening in SVM?\n",
    "\n",
    "**1. Hyperplane**: Decision boundary that separates classes\n",
    "- Equation: $w \\cdot x + b = 0$ (w = normal vector, b = bias)\n",
    "- In 2D: Line, in 3D: Plane, in high-dim: Hyperplane\n",
    "\n",
    "**2. Margin**: Distance from hyperplane to nearest points\n",
    "- Margin = $\\frac{2}{\\|w\\|}$ (perpendicular distance)\n",
    "- Goal: **Maximize margin** \u2192 better generalization\n",
    "\n",
    "**3. Support Vectors**: Training points closest to boundary\n",
    "- Only these points define the hyperplane\n",
    "- Other points can be removed without changing model\n",
    "\n",
    "**4. Soft Margin (C parameter)**: Allow misclassification\n",
    "- C = large: Hard margin (fewer errors, narrow margin)\n",
    "- C = small: Soft margin (more errors, wide margin)\n",
    "\n",
    "**5. Kernel Trick**: Handle non-linear boundaries\n",
    "- Map data to higher dimension: $\\phi(x)$\n",
    "- Linear boundary in high-dim = non-linear in original space\n",
    "- **Never compute $\\phi(x)$ explicitly** \u2014 use kernel function $K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)$\n",
    "\n",
    "### \ud83c\udfaf Key Insight\n",
    "\n",
    "**\"Maximize the safety zone between classes\"**\n",
    "\n",
    "A wide margin means the model is confident and less likely to make errors on new data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcd0 Mathematical Foundation\n",
    "\n",
    "### 1\ufe0f\u20e3 Linear SVM: Hard Margin (Separable Case)\n",
    "\n",
    "**Goal:** Find hyperplane $w \\cdot x + b = 0$ that separates classes with **maximum margin**\n",
    "\n",
    "**Margin width:**\n",
    "\n",
    "$$\n",
    "\\text{Margin} = \\frac{2}{\\|w\\|}\n",
    "$$\n",
    "\n",
    "**Optimization problem:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w, b} \\quad & \\frac{1}{2} \\|w\\|^2 \\\\\n",
    "\\text{subject to} \\quad & y_i (w \\cdot x_i + b) \\geq 1, \\quad \\forall i = 1, \\ldots, n\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $\\min \\frac{1}{2} \\|w\\|^2$: Maximize margin (minimize $\\|w\\| \\Rightarrow$ maximize $2/\\|w\\|$)\n",
    "- $y_i (w \\cdot x_i + b) \\geq 1$: Correct classification with margin\n",
    "- $y_i \\in \\{-1, +1\\}$: Binary labels\n",
    "\n",
    "**Decision function:**\n",
    "\n",
    "$$\n",
    "f(x) = \\text{sign}(w \\cdot x + b)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2\ufe0f\u20e3 Soft Margin SVM (Non-Separable Case)\n",
    "\n",
    "**Problem:** Real data not perfectly separable \u2192 allow some misclassification\n",
    "\n",
    "**Slack variables** $\\xi_i \\geq 0$: Measure violation of margin constraint\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w, b, \\xi} \\quad & \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^n \\xi_i \\\\\n",
    "\\text{subject to} \\quad & y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0, \\quad \\forall i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $C$: **Regularization parameter** (trade-off between margin width and violations)\n",
    "  - **Large C**: Fewer violations, narrow margin (high variance, overfitting)\n",
    "  - **Small C**: More violations, wide margin (high bias, underfitting)\n",
    "- $\\xi_i = 0$: Correctly classified, outside margin\n",
    "- $0 < \\xi_i < 1$: Correctly classified, inside margin\n",
    "- $\\xi_i \\geq 1$: Misclassified\n",
    "\n",
    "**Interpretation:**\n",
    "- First term $\\frac{1}{2}\\|w\\|^2$: Maximize margin\n",
    "- Second term $C \\sum \\xi_i$: Penalize violations\n",
    "- C controls bias-variance trade-off\n",
    "\n",
    "---\n",
    "\n",
    "### 3\ufe0f\u20e3 Kernel Trick: Non-Linear SVM\n",
    "\n",
    "**Idea:** Map data to higher-dimensional space where it becomes linearly separable\n",
    "\n",
    "**Feature mapping:**\n",
    "\n",
    "$$\n",
    "\\phi: \\mathbb{R}^p \\to \\mathbb{R}^{p'} \\quad (p' \\gg p, \\text{ even } p' = \\infty)\n",
    "$$\n",
    "\n",
    "**Example (2D \u2192 3D):**\n",
    "\n",
    "$$\n",
    "\\phi(x_1, x_2) = (x_1^2, \\sqrt{2} x_1 x_2, x_2^2)\n",
    "$$\n",
    "\n",
    "**Key insight:** SVM solution only depends on **dot products** $x_i \\cdot x_j$\n",
    "\n",
    "**Kernel function** (avoids explicit mapping):\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)\n",
    "$$\n",
    "\n",
    "**Decision function with kernel:**\n",
    "\n",
    "$$\n",
    "f(x) = \\text{sign}\\left( \\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b \\right)\n",
    "$$\n",
    "\n",
    "- $\\alpha_i$: Lagrange multipliers (learned from training)\n",
    "- $\\alpha_i > 0$ only for **support vectors**\n",
    "- Most $\\alpha_i = 0$ (sparse solution)\n",
    "\n",
    "---\n",
    "\n",
    "### 4\ufe0f\u20e3 Common Kernel Functions\n",
    "\n",
    "**Linear Kernel** (no transformation):\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = x_i \\cdot x_j\n",
    "$$\n",
    "\n",
    "- Use when: Data linearly separable, high-dimensional (text)\n",
    "\n",
    "**Polynomial Kernel**:\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = (x_i \\cdot x_j + r)^d\n",
    "$$\n",
    "\n",
    "- $d$: Degree (2 = quadratic, 3 = cubic)\n",
    "- $r$: Coefficient (typically 0 or 1)\n",
    "- Use when: Polynomial decision boundary expected\n",
    "\n",
    "**RBF (Radial Basis Function) / Gaussian Kernel** (most common):\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = \\exp\\left( -\\gamma \\|x_i - x_j\\|^2 \\right)\n",
    "$$\n",
    "\n",
    "- $\\gamma = \\frac{1}{2\\sigma^2}$: Controls \"influence radius\"\n",
    "- **Large $\\gamma$**: Tight fit (high variance, overfitting)\n",
    "- **Small $\\gamma$**: Smooth boundary (high bias, underfitting)\n",
    "- Use when: Non-linear boundary, no domain knowledge\n",
    "- **Default choice** for most problems\n",
    "\n",
    "**Sigmoid Kernel**:\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = \\tanh(\\gamma x_i \\cdot x_j + r)\n",
    "$$\n",
    "\n",
    "- Similar to neural network activation\n",
    "- Use when: Mimicking neural network behavior\n",
    "\n",
    "---\n",
    "\n",
    "### 5\ufe0f\u20e3 Hyperparameters and Tuning\n",
    "\n",
    "**C (Regularization):**\n",
    "- Range: $10^{-3}$ to $10^3$ (log scale)\n",
    "- Default: 1.0\n",
    "- Tune via: Grid search or Bayesian optimization\n",
    "\n",
    "**$\\gamma$ (RBF kernel):**\n",
    "- Range: $10^{-4}$ to $10^1$ (log scale)\n",
    "- Default: $\\gamma = \\frac{1}{n_{features} \\cdot \\text{Var}(X)}$\n",
    "- Relationship: Small $\\gamma$ = large influence radius\n",
    "\n",
    "**Grid search pattern:**\n",
    "```python\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6\ufe0f\u20e3 Computational Complexity\n",
    "\n",
    "**Training:** $O(n^2 p)$ to $O(n^3 p)$ \u2014 quadratic/cubic in samples!\n",
    "- Becomes slow for $n > 100K$\n",
    "- Use LinearSVC (liblinear) for large $n$, linear kernel\n",
    "- Use SGDClassifier (linear SVM) for $n > 1M$\n",
    "\n",
    "**Prediction:** $O(n_{sv} \\cdot p)$ \u2014 depends on support vectors\n",
    "- $n_{sv}$ typically 10-50% of training samples\n",
    "- Fast if few support vectors\n",
    "\n",
    "**Memory:** $O(n^2)$ \u2014 stores kernel matrix (Gram matrix)\n",
    "- Problem for $n > 100K$\n",
    "- Solution: Use kernel approximation (Nystroem, RBFSampler)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Import libraries for SVM implementation and evaluation\n",
    "\n",
    "**Key Points:**\n",
    "- **SVC**: Support Vector Classification (for classification tasks)\n",
    "- **SVR**: Support Vector Regression (for regression tasks)\n",
    "- **LinearSVC**: Fast linear SVM for large datasets (uses liblinear)\n",
    "- **StandardScaler**: **CRITICAL** for SVM \u2014 features must have similar scales (like KNN)\n",
    "- **GridSearchCV**: Hyperparameter tuning (C, gamma, kernel)\n",
    "\n",
    "**Why This Matters:** SVM extremely sensitive to feature scales \u2014 always normalize!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC, SVR, LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.datasets import make_classification, make_regression, make_circles, make_moons\n",
    "import time\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"\u2705 Imports complete\")\n",
    "print(f\"   NumPy: {np.__version__}\")\n",
    "print(f\"   Pandas: {pd.__version__}\")\n",
    "print(f\"\\n\ud83c\udfaf Ready to explore Support Vector Machines!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Simplified linear SVM using gradient descent (educational, not production)\n",
    "\n",
    "**Key Points:**\n",
    "- **Hinge loss**: $\\max(0, 1 - y_i (w \\cdot x_i + b))$ \u2014 penalizes margin violations\n",
    "- **Objective**: $\\frac{1}{2}\\|w\\|^2 + C \\sum_i \\max(0, 1 - y_i (w \\cdot x_i + b))$\n",
    "- **Gradient descent**: Iteratively update w, b to minimize objective\n",
    "- **Not optimal**: Real SVM uses quadratic programming (QP solvers)\n",
    "- **Educational**: Shows margin maximization principle\n",
    "\n",
    "**Why This Matters:** Understanding hinge loss and margin concept before using production SVM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVMScratch:\n",
    "    \"\"\"Simplified Linear SVM using gradient descent (educational)\"\"\"\n",
    "    \n",
    "    def __init__(self, C=1.0, learning_rate=0.001, n_iterations=1000):\n",
    "        self.C = C\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iterations\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train using gradient descent on hinge loss\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Convert labels to {-1, +1}\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        \n",
    "        # Gradient descent\n",
    "        for iteration in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                # Check if sample violates margin\n",
    "                condition = y_[idx] * (np.dot(x_i, self.w) + self.b) >= 1\n",
    "                \n",
    "                if condition:\n",
    "                    # Correctly classified, outside margin\n",
    "                    # Only regularization term (margin maximization)\n",
    "                    self.w -= self.lr * (2 * self.w)\n",
    "                else:\n",
    "                    # Misclassified or inside margin\n",
    "                    # Hinge loss gradient + regularization\n",
    "                    self.w -= self.lr * (2 * self.w - self.C * y_[idx] * x_i)\n",
    "                    self.b -= self.lr * (-self.C * y_[idx])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        linear_output = np.dot(X, self.w) + self.b\n",
    "        return np.where(linear_output >= 0, 1, 0)\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Distance to hyperplane (signed)\"\"\"\n",
    "        return np.dot(X, self.w) + self.b\n",
    "\n",
    "# Demo on simple dataset\n",
    "print(\"\ud83e\uddea Testing Linear SVM from Scratch\\n\")\n",
    "\n",
    "# Generate linearly separable data\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_informative=2,\n",
    "                          n_redundant=0, n_clusters_per_class=1, \n",
    "                          class_sep=2.0, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# CRITICAL: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train from-scratch SVM\n",
    "svm_scratch = LinearSVMScratch(C=1.0, learning_rate=0.001, n_iterations=1000)\n",
    "start_time = time.time()\n",
    "svm_scratch.fit(X_train_scaled, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "y_pred = svm_scratch.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\u2705 From-Scratch Linear SVM Results:\")\n",
    "print(f\"   Training time: {train_time*1000:.2f}ms\")\n",
    "print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "print(f\"   Learned weights w: {svm_scratch.w}\")\n",
    "print(f\"   Learned bias b: {svm_scratch.b:.4f}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udd0d How it works:\")\n",
    "print(f\"   1. Hinge loss: max(0, 1 - y*(w\u00b7x + b)) \u2014 penalizes margin violations\")\n",
    "print(f\"   2. Objective: minimize 1/2||w||\u00b2 + C*\u03a3 hinge_loss \u2014 margin + violations\")\n",
    "print(f\"   3. Gradient descent: update w, b to minimize objective\")\n",
    "print(f\"   4. Decision: sign(w\u00b7x + b) \u2014 positive class if w\u00b7x + b > 0\")\n",
    "\n",
    "print(f\"\\n\u26a0\ufe0f Note: Real SVM uses quadratic programming (QP) solvers \u2014 more efficient and accurate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Use sklearn's optimized SVC with linear kernel\n",
    "\n",
    "**Key Points:**\n",
    "- **SVC**: Uses libsvm (efficient QP solver, not gradient descent)\n",
    "- **kernel='linear'**: No transformation, linear boundary\n",
    "- **C=1.0**: Regularization parameter (balance margin width vs violations)\n",
    "- **Support vectors**: Only subset of training points define model\n",
    "- **Comparison**: 10-100x faster than from-scratch, more accurate\n",
    "\n",
    "**Why This Matters:** sklearn SVM uses state-of-the-art optimization (libsvm, liblinear) for production use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\ude80 Production Linear SVM with sklearn\\n\")\n",
    "\n",
    "# Same dataset as before\n",
    "svm_sklearn = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "\n",
    "start_time = time.time()\n",
    "svm_sklearn.fit(X_train_scaled, y_train)\n",
    "train_time_sklearn = time.time() - start_time\n",
    "\n",
    "y_pred_sklearn = svm_sklearn.predict(X_test_scaled)\n",
    "y_decision = svm_sklearn.decision_function(X_test_scaled)\n",
    "\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print(f\"\u2705 sklearn Linear SVM Results:\")\n",
    "print(f\"   Training time: {train_time_sklearn*1000:.2f}ms\")\n",
    "print(f\"   Accuracy: {accuracy_sklearn:.4f}\")\n",
    "print(f\"   Number of support vectors: {svm_sklearn.n_support_}\")\n",
    "print(f\"   Total support vectors: {sum(svm_sklearn.n_support_)} / {len(X_train_scaled)} \"\n",
    "      f\"({sum(svm_sklearn.n_support_)/len(X_train_scaled)*100:.1f}%)\")\n",
    "\n",
    "# Compare with from-scratch\n",
    "speedup = train_time / train_time_sklearn\n",
    "print(f\"\\n\u26a1 Speedup over from-scratch: {speedup:.1f}x\")\n",
    "print(f\"   From-scratch: {train_time*1000:.2f}ms, Accuracy: {accuracy:.4f}\")\n",
    "print(f\"   sklearn:      {train_time_sklearn*1000:.2f}ms, Accuracy: {accuracy_sklearn:.4f}\")\n",
    "\n",
    "# Support vector analysis\n",
    "print(f\"\\n\ud83c\udfaf Support Vector Insights:\")\n",
    "print(f\"   \u2022 Only {sum(svm_sklearn.n_support_)} out of {len(X_train_scaled)} training points are support vectors\")\n",
    "print(f\"   \u2022 These define the entire decision boundary\")\n",
    "print(f\"   \u2022 Other points can be removed without changing model\")\n",
    "print(f\"   \u2022 Sparse solution \u2192 efficient prediction\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_sklearn)\n",
    "print(f\"\\n\ud83d\udccb Confusion Matrix:\")\n",
    "print(f\"   True Neg:  {cm[0,0]:3d}  |  False Pos: {cm[0,1]:3d}\")\n",
    "print(f\"   False Neg: {cm[1,0]:3d}  |  True Pos:  {cm[1,1]:3d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Visualize SVM decision boundary, margin, and support vectors\n",
    "\n",
    "**Key Points:**\n",
    "- **Decision boundary**: Hyperplane where $w \\cdot x + b = 0$\n",
    "- **Margin boundaries**: Parallel lines at $w \\cdot x + b = \\pm 1$\n",
    "- **Support vectors**: Points on margin boundaries (circled)\n",
    "- **Margin width**: Distance between the two margin boundaries\n",
    "- **Visualization**: Only possible for 2D data (2 features)\n",
    "\n",
    "**Why This Matters:** Visual understanding of margin maximization principle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83c\udfa8 Visualizing SVM Decision Boundary and Margin\\n\")\n",
    "\n",
    "# Create mesh for decision boundary visualization\n",
    "x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# Predict on mesh\n",
    "Z = svm_sklearn.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot decision boundary (Z=0) and margins (Z=\u00b11)\n",
    "ax.contour(xx, yy, Z, levels=[-1, 0, 1], linewidths=[2, 3, 2],\n",
    "           linestyles=['--', '-', '--'], colors=['orange', 'black', 'orange'])\n",
    "\n",
    "# Fill regions\n",
    "ax.contourf(xx, yy, Z, levels=[-10, 0, 10], alpha=0.1, colors=['blue', 'red'])\n",
    "\n",
    "# Plot training points\n",
    "scatter = ax.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], \n",
    "                    c=y_train, cmap='coolwarm', s=50, edgecolors='k', alpha=0.7)\n",
    "\n",
    "# Highlight support vectors\n",
    "ax.scatter(svm_sklearn.support_vectors_[:, 0], svm_sklearn.support_vectors_[:, 1],\n",
    "           s=200, linewidth=2, facecolors='none', edgecolors='green', \n",
    "           label=f'Support Vectors (n={len(svm_sklearn.support_vectors_)})')\n",
    "\n",
    "ax.set_xlabel('Feature 1 (scaled)', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Feature 2 (scaled)', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Linear SVM: Decision Boundary, Margin, and Support Vectors', \n",
    "             fontweight='bold', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "ax.text(0.02, 0.98, 'Solid line: Decision boundary (w\u00b7x + b = 0)', \n",
    "        transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "ax.text(0.02, 0.92, 'Dashed lines: Margin boundaries (w\u00b7x + b = \u00b11)', \n",
    "        transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "ax.text(0.02, 0.86, f'Margin width: 2/||w|| = {2/np.linalg.norm(svm_sklearn.coef_):.3f}', \n",
    "        transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\ud83c\udfaf Visualization Insights:\")\n",
    "print(f\"   \u2022 Black line: Decision boundary (separates classes)\")\n",
    "print(f\"   \u2022 Orange dashed: Margin boundaries (parallel to decision boundary)\")\n",
    "print(f\"   \u2022 Green circles: Support vectors (only these define the model!)\")\n",
    "print(f\"   \u2022 Margin width: {2/np.linalg.norm(svm_sklearn.coef_):.3f} units\")\n",
    "print(f\"   \u2022 SVM maximizes this margin \u2192 better generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Demonstrate kernel trick with non-linearly separable data\n",
    "\n",
    "**Key Points:**\n",
    "- **make_moons**: Classic non-linear dataset (two interleaving crescents)\n",
    "- **kernel='rbf'**: Radial Basis Function (Gaussian kernel)\n",
    "- **RBF formula**: $K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2)$\n",
    "- **gamma**: Controls influence radius (small = smooth, large = complex boundary)\n",
    "- **Linear vs RBF**: Linear fails (50% accuracy), RBF succeeds (>95%)\n",
    "\n",
    "**Why This Matters:** Kernel trick handles non-linear patterns without explicit feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83c\udf19 Non-Linear SVM with RBF Kernel\\n\")\n",
    "\n",
    "# Generate non-linear dataset (two interleaving moons)\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.15, random_state=42)\n",
    "X_train_moons, X_test_moons, y_train_moons, y_test_moons = train_test_split(\n",
    "    X_moons, y_moons, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_moons = StandardScaler()\n",
    "X_train_moons_scaled = scaler_moons.fit_transform(X_train_moons)\n",
    "X_test_moons_scaled = scaler_moons.transform(X_test_moons)\n",
    "\n",
    "# Test Linear SVM (will fail)\n",
    "print(\"1\ufe0f\u20e3 Linear SVM on non-linear data:\")\n",
    "svm_linear_moons = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm_linear_moons.fit(X_train_moons_scaled, y_train_moons)\n",
    "y_pred_linear = svm_linear_moons.predict(X_test_moons_scaled)\n",
    "acc_linear = accuracy_score(y_test_moons, y_pred_linear)\n",
    "print(f\"   Accuracy: {acc_linear:.4f} (POOR - linear boundary can't separate moons!)\")\n",
    "\n",
    "# Test RBF SVM (will succeed)\n",
    "print(f\"\\n2\ufe0f\u20e3 RBF SVM on non-linear data:\")\n",
    "svm_rbf_moons = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_rbf_moons.fit(X_train_moons_scaled, y_train_moons)\n",
    "y_pred_rbf = svm_rbf_moons.predict(X_test_moons_scaled)\n",
    "acc_rbf = accuracy_score(y_test_moons, y_pred_rbf)\n",
    "print(f\"   Accuracy: {acc_rbf:.4f} (EXCELLENT - RBF kernel handles non-linearity!)\")\n",
    "print(f\"   Gamma: {svm_rbf_moons.gamma:.6f} (auto-computed from data scale)\")\n",
    "print(f\"   Support vectors: {sum(svm_rbf_moons.n_support_)} / {len(X_train_moons_scaled)} \"\n",
    "      f\"({sum(svm_rbf_moons.n_support_)/len(X_train_moons_scaled)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n\u2705 Improvement: {(acc_rbf - acc_linear)/acc_linear*100:+.1f}% (RBF vs Linear)\")\n",
    "\n",
    "# Visualization comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "for idx, (svm_model, title, acc) in enumerate([\n",
    "    (svm_linear_moons, 'Linear SVM (Fails)', acc_linear),\n",
    "    (svm_rbf_moons, 'RBF SVM (Succeeds)', acc_rbf)\n",
    "]):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create mesh\n",
    "    x_min, x_max = X_train_moons_scaled[:, 0].min() - 0.5, X_train_moons_scaled[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_train_moons_scaled[:, 1].min() - 0.5, X_train_moons_scaled[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Predict on mesh\n",
    "    Z = svm_model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    ax.contourf(xx, yy, Z, levels=20, alpha=0.3, cmap='coolwarm')\n",
    "    ax.contour(xx, yy, Z, levels=[0], linewidths=3, colors='black')\n",
    "    \n",
    "    # Training points\n",
    "    ax.scatter(X_train_moons_scaled[:, 0], X_train_moons_scaled[:, 1],\n",
    "               c=y_train_moons, cmap='coolwarm', s=50, edgecolors='k', alpha=0.7)\n",
    "    \n",
    "    # Support vectors\n",
    "    ax.scatter(svm_model.support_vectors_[:, 0], svm_model.support_vectors_[:, 1],\n",
    "               s=200, linewidth=2, facecolors='none', edgecolors='green',\n",
    "               label=f'Support Vectors (n={len(svm_model.support_vectors_)})')\n",
    "    \n",
    "    ax.set_xlabel('Feature 1 (scaled)', fontweight='bold', fontsize=11)\n",
    "    ax.set_ylabel('Feature 2 (scaled)', fontweight='bold', fontsize=11)\n",
    "    ax.set_title(f'{title}\\nAccuracy: {acc:.4f}', fontweight='bold', fontsize=13)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Key Insight:\")\n",
    "print(f\"   \u2022 Linear kernel: Straight line boundary \u2192 fails on moons\")\n",
    "print(f\"   \u2022 RBF kernel: Curved boundary \u2192 perfectly separates moons\")\n",
    "print(f\"   \u2022 Kernel trick: Maps to high-dim space without computing \u03c6(x) explicitly\")\n",
    "print(f\"   \u2022 RBF is default choice when you don't know data structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Compare different kernel functions on same dataset\n",
    "\n",
    "**Key Points:**\n",
    "- **Linear**: $K(x_i, x_j) = x_i \\cdot x_j$ (no transformation)\n",
    "- **Polynomial (degree=3)**: $K(x_i, x_j) = (x_i \\cdot x_j + 1)^3$\n",
    "- **RBF**: $K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2)$\n",
    "- **Sigmoid**: $K(x_i, x_j) = \\tanh(\\gamma x_i \\cdot x_j + 0)$\n",
    "- **Performance**: RBF typically best for unknown data structure\n",
    "\n",
    "**Why This Matters:** Kernel choice can change accuracy by 10-30% \u2014 always experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udcca Comparing Different Kernel Functions\\n\")\n",
    "\n",
    "kernels = {\n",
    "    'Linear': 'linear',\n",
    "    'Polynomial (d=2)': ('poly', 2),\n",
    "    'Polynomial (d=3)': ('poly', 3),\n",
    "    'RBF': 'rbf',\n",
    "    'Sigmoid': 'sigmoid'\n",
    "}\n",
    "\n",
    "kernel_results = {}\n",
    "\n",
    "for name, kernel_config in kernels.items():\n",
    "    if isinstance(kernel_config, tuple):\n",
    "        kernel, degree = kernel_config\n",
    "        svm = SVC(kernel=kernel, degree=degree, C=1.0, gamma='scale', random_state=42)\n",
    "    else:\n",
    "        svm = SVC(kernel=kernel_config, C=1.0, gamma='scale', random_state=42)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    svm.fit(X_train_moons_scaled, y_train_moons)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    y_pred = svm.predict(X_test_moons_scaled)\n",
    "    accuracy = accuracy_score(y_test_moons, y_pred)\n",
    "    n_sv = sum(svm.n_support_)\n",
    "    \n",
    "    kernel_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'train_time': train_time,\n",
    "        'n_support_vectors': n_sv\n",
    "    }\n",
    "    \n",
    "    print(f\"   {name:<20} Accuracy: {accuracy:.4f}, Train: {train_time*1000:6.2f}ms, SV: {n_sv:3d}\")\n",
    "\n",
    "# Best kernel\n",
    "best_kernel = max(kernel_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "print(f\"\\n\u2705 Best kernel: {best_kernel[0]} (Accuracy: {best_kernel[1]['accuracy']:.4f})\")\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "kernel_names = list(kernel_results.keys())\n",
    "accuracies = [kernel_results[k]['accuracy'] for k in kernel_names]\n",
    "train_times = [kernel_results[k]['train_time']*1000 for k in kernel_names]\n",
    "\n",
    "x_pos = np.arange(len(kernel_names))\n",
    "bars = ax.bar(x_pos, accuracies, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "\n",
    "# Highlight best\n",
    "best_idx = kernel_names.index(best_kernel[0])\n",
    "bars[best_idx].set_color('green')\n",
    "bars[best_idx].set_alpha(0.9)\n",
    "\n",
    "ax.set_xlabel('Kernel Type', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontweight='bold', fontsize=12)\n",
    "ax.set_title('SVM Kernel Comparison (Moons Dataset)', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(kernel_names, rotation=15, ha='right')\n",
    "ax.set_ylim([0.4, 1.0])\n",
    "ax.axhline(y=0.5, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Random guess')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Annotate values\n",
    "for i, (acc, t) in enumerate(zip(accuracies, train_times)):\n",
    "    ax.text(i, acc + 0.02, f'{acc:.3f}\\n{t:.1f}ms', \n",
    "            ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Kernel Selection Guidelines:\")\n",
    "print(f\"   \u2022 Linear: Fast, interpretable, works when p >> n (text data)\")\n",
    "print(f\"   \u2022 Polynomial: Specific degree expected, can overfit (try d=2,3)\")\n",
    "print(f\"   \u2022 RBF: Default choice, handles non-linearity, robust\")\n",
    "print(f\"   \u2022 Sigmoid: Mimics neural network, rarely better than RBF\")\n",
    "print(f\"\\n   \u2192 Start with RBF, try Linear if high-dimensional (p > 10K)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Tune C and gamma hyperparameters using GridSearchCV\n",
    "\n",
    "**Key Points:**\n",
    "- **C**: Regularization (large = hard margin, small = soft margin)\n",
    "- **gamma**: RBF kernel width (large = tight fit, small = smooth)\n",
    "- **GridSearchCV**: Try all combinations, select best via cross-validation\n",
    "- **Log scale**: Search [0.001, 0.01, 0.1, 1, 10, 100] for C and gamma\n",
    "- **Best combination**: Often C~1-10, gamma~0.01-0.1 (dataset-dependent)\n",
    "\n",
    "**Why This Matters:** Proper tuning improves accuracy by 5-15% compared to defaults.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udd27 Hyperparameter Tuning: C and Gamma\\n\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "print(f\"\ud83d\udd0d Grid Search: {len(param_grid['C'])} C values \u00d7 {len(param_grid['gamma'])} gamma values \"\n",
    "      f\"= {len(param_grid['C']) * len(param_grid['gamma'])} combinations\")\n",
    "print(f\"   with 5-fold CV = {len(param_grid['C']) * len(param_grid['gamma']) * 5} total fits\\n\")\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(kernel='rbf', random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train_moons_scaled, y_train_moons)\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "print(f\"\u2705 Grid Search Complete ({grid_time:.2f}s)\\n\")\n",
    "print(f\"   Best parameters: C={grid_search.best_params_['C']}, gamma={grid_search.best_params_['gamma']}\")\n",
    "print(f\"   Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Test on hold-out set\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred_best = best_svm.predict(X_test_moons_scaled)\n",
    "acc_best = accuracy_score(y_test_moons, y_pred_best)\n",
    "print(f\"   Test accuracy: {acc_best:.4f}\")\n",
    "\n",
    "# Compare with default\n",
    "svm_default = SVC(kernel='rbf', random_state=42)  # C=1.0, gamma='scale'\n",
    "svm_default.fit(X_train_moons_scaled, y_train_moons)\n",
    "y_pred_default = svm_default.predict(X_test_moons_scaled)\n",
    "acc_default = accuracy_score(y_test_moons, y_pred_default)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Comparison:\")\n",
    "print(f\"   Default (C=1.0, gamma='scale'):  {acc_default:.4f}\")\n",
    "print(f\"   Tuned (C={grid_search.best_params_['C']}, gamma={grid_search.best_params_['gamma']}):    {acc_best:.4f}\")\n",
    "print(f\"   Improvement: {(acc_best - acc_default)*100:+.2f}%\")\n",
    "\n",
    "# Heatmap of grid search results\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "scores = results_df['mean_test_score'].values.reshape(len(param_grid['C']), len(param_grid['gamma']))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "im = ax.imshow(scores, cmap='viridis', aspect='auto')\n",
    "ax.set_xticks(np.arange(len(param_grid['gamma'])))\n",
    "ax.set_yticks(np.arange(len(param_grid['C'])))\n",
    "ax.set_xticklabels(param_grid['gamma'])\n",
    "ax.set_yticklabels(param_grid['C'])\n",
    "ax.set_xlabel('Gamma (RBF kernel width)', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('C (Regularization)', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Grid Search: CV Accuracy Heatmap', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Annotate cells\n",
    "for i in range(len(param_grid['C'])):\n",
    "    for j in range(len(param_grid['gamma'])):\n",
    "        text = ax.text(j, i, f'{scores[i, j]:.3f}',\n",
    "                      ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Mark best\n",
    "best_idx = np.unravel_index(np.argmax(scores), scores.shape)\n",
    "ax.add_patch(plt.Rectangle((best_idx[1]-0.5, best_idx[0]-0.5), 1, 1, \n",
    "                           fill=False, edgecolor='red', linewidth=3))\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='CV Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Hyperparameter Insights:\")\n",
    "print(f\"   \u2022 Small C + Small gamma: Underfitting (smooth, wide margin)\")\n",
    "print(f\"   \u2022 Large C + Large gamma: Overfitting (complex boundary, tight fit)\")\n",
    "print(f\"   \u2022 Best balance: C={grid_search.best_params_['C']}, gamma={grid_search.best_params_['gamma']} \"\n",
    "      f\"(depends on data scale and complexity)\")\n",
    "print(f\"   \u2022 Always use GridSearchCV or RandomizedSearchCV for tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udd2c Post-Silicon Validation Application: Device Pass/Fail Classification\n",
    "\n",
    "### Business Context\n",
    "\n",
    "**Challenge:** Semiconductor manufacturing generates 50,000+ test devices with 100+ parametric measurements. Engineers need to predict device pass/fail before expensive final testing.\n",
    "\n",
    "**SVM Solution:**\n",
    "- **Non-linear boundaries**: Device failures follow complex patterns (voltage \u00d7 current \u00d7 temperature interactions)\n",
    "- **High-dimensional data**: 100+ parametric tests \u2192 RBF kernel handles naturally\n",
    "- **Margin-based confidence**: Distance to decision boundary indicates reliability\n",
    "- **Sparse solution**: Only support vectors needed (10-30% of data) \u2192 fast deployment\n",
    "\n",
    "**Business Impact:**\n",
    "- **Test Cost Reduction**: $2-5M per product (skip unnecessary final tests)\n",
    "- **Yield Improvement**: 2-5% (early identification of failure modes)\n",
    "- **Time to Market**: 3-6 weeks faster (parallel analysis vs sequential testing)\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "**50,000 devices** with measurements:\n",
    "- **Electrical**: Vdd (25 levels), Idd (10 states), Frequency (20 corners)\n",
    "- **Power**: Leakage current, active power, standby power\n",
    "- **Timing**: Setup time, hold time, propagation delay (25 paths)\n",
    "- **Spatial**: wafer_id, die_x, die_y (30\u00d730 wafer maps)\n",
    "- **Environmental**: Temperature (3 corners: -40\u00b0C, 25\u00b0C, 125\u00b0C)\n",
    "\n",
    "**Outcome:** Pass (87%) / Fail (13%) \u2014 imbalanced classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Generate realistic 50K device dataset with parametric test results\n",
    "\n",
    "**Key Points:**\n",
    "- **100 features**: Voltage, current, frequency, power, timing, temperature\n",
    "- **Non-linear failures**: Interactions between voltage, current, temperature\n",
    "- **13% failure rate**: Realistic manufacturing yield (87% pass)\n",
    "- **Spatial correlation**: Wafer location affects failure probability\n",
    "- **Imbalanced classes**: More passing devices than failing (class_weight needed)\n",
    "\n",
    "**Why This Matters:** Real semiconductor data has complex, non-linear failure patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udd2c Generating Post-Silicon Validation Dataset (50K devices)\\n\")\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_devices = 50000\n",
    "n_features = 100\n",
    "\n",
    "# Generate base parametric test data\n",
    "X_devices = np.random.randn(n_devices, n_features)\n",
    "\n",
    "# Realistic feature naming\n",
    "feature_names = []\n",
    "feature_names += [f'Vdd_{i}' for i in range(25)]  # 25 voltage domains\n",
    "feature_names += [f'Idd_{i}' for i in range(10)]  # 10 current measurements\n",
    "feature_names += [f'Freq_{i}' for i in range(20)]  # 20 frequency corners\n",
    "feature_names += [f'Power_{i}' for i in range(15)]  # Power measurements\n",
    "feature_names += [f'Timing_{i}' for i in range(25)]  # 25 timing paths\n",
    "feature_names += ['Leakage', 'Active_Power', 'Standby_Power', 'Temp', 'Wafer_X']  # 5 additional\n",
    "\n",
    "# Create DataFrame\n",
    "df_devices = pd.DataFrame(X_devices, columns=feature_names)\n",
    "\n",
    "# Add spatial information (wafer coordinates)\n",
    "df_devices['wafer_id'] = np.random.randint(1, 11, size=n_devices)  # 10 wafers\n",
    "df_devices['die_x'] = np.random.randint(0, 30, size=n_devices)  # 30\u00d730 wafer\n",
    "df_devices['die_y'] = np.random.randint(0, 30, size=n_devices)\n",
    "\n",
    "# Complex failure mechanism (non-linear interactions)\n",
    "# Failure = f(Vdd, Idd, Temp, spatial correlation)\n",
    "failure_score = (\n",
    "    0.3 * df_devices['Vdd_0'] * df_devices['Idd_0'] +  # Voltage \u00d7 Current interaction\n",
    "    0.2 * df_devices['Temp'] ** 2 +  # Quadratic temperature effect\n",
    "    0.15 * df_devices['Leakage'] * df_devices['Temp'] +  # Leakage increases with temp\n",
    "    0.1 * (df_devices['die_x'] - 15) ** 2 / 100 +  # Spatial: edge of wafer\n",
    "    0.1 * (df_devices['die_y'] - 15) ** 2 / 100 +\n",
    "    0.15 * np.random.randn(n_devices)  # Random variation\n",
    ")\n",
    "\n",
    "# Convert to binary (13% failure rate)\n",
    "failure_threshold = np.percentile(failure_score, 87)  # 87th percentile = pass\n",
    "y_devices = (failure_score > failure_threshold).astype(int)  # 0=Pass, 1=Fail\n",
    "\n",
    "df_devices['pass_fail'] = y_devices\n",
    "\n",
    "print(f\"\u2705 Dataset Generated:\")\n",
    "print(f\"   Total devices: {n_devices:,}\")\n",
    "print(f\"   Features: {n_features}\")\n",
    "print(f\"   Pass: {(y_devices == 0).sum():,} ({(y_devices == 0).sum()/n_devices*100:.1f}%)\")\n",
    "print(f\"   Fail: {(y_devices == 1).sum():,} ({(y_devices == 1).sum()/n_devices*100:.1f}%)\")\n",
    "print(f\"\\n\ud83d\udcca Sample Data:\")\n",
    "print(df_devices[['Vdd_0', 'Idd_0', 'Temp', 'Leakage', 'die_x', 'die_y', 'pass_fail']].head(10))\n",
    "\n",
    "# Train/test split\n",
    "X_train_devices, X_test_devices, y_train_devices, y_test_devices = train_test_split(\n",
    "    df_devices[feature_names], y_devices, test_size=0.2, random_state=42, stratify=y_devices\n",
    ")\n",
    "\n",
    "print(f\"\\n\ud83d\udd00 Train/Test Split:\")\n",
    "print(f\"   Training: {len(X_train_devices):,} devices\")\n",
    "print(f\"   Testing: {len(X_test_devices):,} devices\")\n",
    "\n",
    "# Feature scaling (CRITICAL for SVM!)\n",
    "scaler_devices = StandardScaler()\n",
    "X_train_devices_scaled = scaler_devices.fit_transform(X_train_devices)\n",
    "X_test_devices_scaled = scaler_devices.transform(X_test_devices)\n",
    "\n",
    "print(f\"\u2705 Features scaled (StandardScaler applied)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Train RBF SVM with class weighting for imbalanced data\n",
    "\n",
    "**Key Points:**\n",
    "- **class_weight='balanced'**: Penalizes minority class (Fail) errors more\n",
    "- **RBF kernel**: Handles non-linear voltage \u00d7 current \u00d7 temperature interactions\n",
    "- **C and gamma tuning**: Use GridSearchCV for optimal hyperparameters\n",
    "- **Training time**: ~10-60 seconds for 40K devices (quadratic complexity)\n",
    "- **Support vectors**: Typically 20-40% of training data\n",
    "\n",
    "**Why This Matters:** Class weighting prevents model from predicting all \"Pass\" (naive 87% accuracy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udd27 Training SVM on 50K Device Dataset\\n\")\n",
    "\n",
    "# Baseline: Default RBF SVM\n",
    "print(\"1\ufe0f\u20e3 Baseline: Default RBF SVM (no tuning)\")\n",
    "svm_baseline = SVC(kernel='rbf', class_weight='balanced', random_state=42)\n",
    "\n",
    "start_time = time.time()\n",
    "svm_baseline.fit(X_train_devices_scaled, y_train_devices)\n",
    "train_time_baseline = time.time() - start_time\n",
    "\n",
    "y_pred_baseline = svm_baseline.predict(X_test_devices_scaled)\n",
    "acc_baseline = accuracy_score(y_test_devices, y_pred_baseline)\n",
    "f1_baseline = f1_score(y_test_devices, y_pred_baseline)\n",
    "\n",
    "print(f\"   Training time: {train_time_baseline:.2f}s\")\n",
    "print(f\"   Accuracy: {acc_baseline:.4f}\")\n",
    "print(f\"   F1-score: {f1_baseline:.4f}\")\n",
    "print(f\"   Support vectors: {sum(svm_baseline.n_support_):,} / {len(X_train_devices_scaled):,} \"\n",
    "      f\"({sum(svm_baseline.n_support_)/len(X_train_devices_scaled)*100:.1f}%)\")\n",
    "\n",
    "# Hyperparameter tuning (reduced grid for speed)\n",
    "print(f\"\\n2\ufe0f\u20e3 Hyperparameter Tuning (GridSearchCV)\")\n",
    "param_grid_devices = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "print(f\"   Grid: {len(param_grid_devices['C'])} C \u00d7 {len(param_grid_devices['gamma'])} gamma \"\n",
    "      f\"= {len(param_grid_devices['C']) * len(param_grid_devices['gamma'])} combinations (3-fold CV)\")\n",
    "\n",
    "grid_devices = GridSearchCV(\n",
    "    SVC(kernel='rbf', class_weight='balanced', random_state=42),\n",
    "    param_grid_devices,\n",
    "    cv=3,  # 3-fold for speed\n",
    "    scoring='f1',  # Optimize F1 (better for imbalanced)\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "grid_devices.fit(X_train_devices_scaled, y_train_devices)\n",
    "tune_time = time.time() - start_time\n",
    "\n",
    "print(f\"   Tuning time: {tune_time:.2f}s\")\n",
    "print(f\"   Best params: C={grid_devices.best_params_['C']}, gamma={grid_devices.best_params_['gamma']}\")\n",
    "print(f\"   Best CV F1-score: {grid_devices.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "svm_tuned = grid_devices.best_estimator_\n",
    "y_pred_tuned = svm_tuned.predict(X_test_devices_scaled)\n",
    "acc_tuned = accuracy_score(y_test_devices, y_pred_tuned)\n",
    "f1_tuned = f1_score(y_test_devices, y_pred_tuned)\n",
    "\n",
    "print(f\"\\n3\ufe0f\u20e3 Test Set Results (Tuned Model):\")\n",
    "print(f\"   Accuracy: {acc_tuned:.4f} (improvement: {(acc_tuned-acc_baseline)*100:+.2f}%)\")\n",
    "print(f\"   F1-score: {f1_tuned:.4f} (improvement: {(f1_tuned-f1_baseline)*100:+.2f}%)\")\n",
    "print(f\"   Support vectors: {sum(svm_tuned.n_support_):,} / {len(X_train_devices_scaled):,} \"\n",
    "      f\"({sum(svm_tuned.n_support_)/len(X_train_devices_scaled)*100:.1f}%)\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\n\ud83d\udccb Classification Report:\")\n",
    "print(classification_report(y_test_devices, y_pred_tuned, \n",
    "                          target_names=['Pass (0)', 'Fail (1)'], digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_devices, y_pred_tuned)\n",
    "print(f\"\\n\ud83c\udfaf Confusion Matrix:\")\n",
    "print(f\"   True Pass:  {cm[0,0]:5d}  |  False Fail: {cm[0,1]:4d}\")\n",
    "print(f\"   False Pass: {cm[1,0]:5d}  |  True Fail:  {cm[1,1]:4d}\")\n",
    "\n",
    "# Business metrics\n",
    "false_pass = cm[1, 0]  # Failed device predicted as Pass (COSTLY!)\n",
    "false_fail = cm[0, 1]  # Passed device predicted as Fail (wasted opportunity)\n",
    "\n",
    "cost_false_pass = false_pass * 500  # $500 per escaping failure\n",
    "cost_false_fail = false_fail * 50   # $50 per unnecessary retest\n",
    "total_cost = cost_false_pass + cost_false_fail\n",
    "\n",
    "print(f\"\\n\ud83d\udcb0 Business Impact:\")\n",
    "print(f\"   False Pass (escaping failures): {false_pass:,} devices \u00d7 $500 = ${cost_false_pass:,}\")\n",
    "print(f\"   False Fail (unnecessary retest): {false_fail:,} devices \u00d7 $50 = ${cost_false_fail:,}\")\n",
    "print(f\"   Total cost: ${total_cost:,}\")\n",
    "print(f\"\\n   \ud83c\udfaf Recall (Fail detection): {cm[1,1]/(cm[1,0]+cm[1,1])*100:.1f}% \"\n",
    "      f\"(catching {cm[1,1]:,} / {cm[1,0]+cm[1,1]:,} failures)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Use SVM decision function for margin-based confidence scores\n",
    "\n",
    "**Key Points:**\n",
    "- **decision_function()**: Returns signed distance to hyperplane\n",
    "- **Positive distance**: Predicted Pass (class 0)\n",
    "- **Negative distance**: Predicted Fail (class 1)\n",
    "- **Large |distance|**: High confidence (far from boundary)\n",
    "- **Small |distance|**: Low confidence (near boundary, manual review)\n",
    "\n",
    "**Why This Matters:** Prioritize manual inspection of low-confidence predictions (near decision boundary).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83c\udfaf Margin-Based Confidence Scoring\\n\")\n",
    "\n",
    "# Get decision function values (signed distance to hyperplane)\n",
    "decision_values = svm_tuned.decision_function(X_test_devices_scaled)\n",
    "\n",
    "# Analyze confidence distribution\n",
    "print(f\"\ud83d\udcca Decision Function Statistics:\")\n",
    "print(f\"   Min: {decision_values.min():.4f}\")\n",
    "print(f\"   Max: {decision_values.max():.4f}\")\n",
    "print(f\"   Mean: {decision_values.mean():.4f}\")\n",
    "print(f\"   Std: {decision_values.std():.4f}\")\n",
    "\n",
    "# Identify low-confidence predictions (|distance| < threshold)\n",
    "confidence_threshold = 0.5\n",
    "low_confidence = np.abs(decision_values) < confidence_threshold\n",
    "n_low_confidence = low_confidence.sum()\n",
    "\n",
    "print(f\"\\n\u26a0\ufe0f Low Confidence Predictions (|distance| < {confidence_threshold}):\")\n",
    "print(f\"   Count: {n_low_confidence:,} / {len(decision_values):,} \"\n",
    "      f\"({n_low_confidence/len(decision_values)*100:.1f}%)\")\n",
    "print(f\"   Recommendation: Manual review for these devices\")\n",
    "\n",
    "# Accuracy by confidence level\n",
    "high_confidence = ~low_confidence\n",
    "acc_high_conf = accuracy_score(y_test_devices[high_confidence], \n",
    "                               y_pred_tuned[high_confidence])\n",
    "acc_low_conf = accuracy_score(y_test_devices[low_confidence], \n",
    "                              y_pred_tuned[low_confidence])\n",
    "\n",
    "print(f\"\\n\u2705 Accuracy by Confidence:\")\n",
    "print(f\"   High confidence (|d| \u2265 {confidence_threshold}): {acc_high_conf:.4f} \"\n",
    "      f\"(n={high_confidence.sum():,})\")\n",
    "print(f\"   Low confidence (|d| < {confidence_threshold}): {acc_low_conf:.4f} \"\n",
    "      f\"(n={low_confidence.sum():,})\")\n",
    "\n",
    "# Visualization: Decision function histogram\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Histogram by true label\n",
    "ax = axes[0]\n",
    "ax.hist(decision_values[y_test_devices == 0], bins=50, alpha=0.6, \n",
    "        label='True Pass (0)', color='blue', edgecolor='black')\n",
    "ax.hist(decision_values[y_test_devices == 1], bins=50, alpha=0.6, \n",
    "        label='True Fail (1)', color='red', edgecolor='black')\n",
    "ax.axvline(x=0, color='black', linestyle='--', linewidth=2, label='Decision boundary')\n",
    "ax.axvline(x=confidence_threshold, color='orange', linestyle='--', linewidth=2)\n",
    "ax.axvline(x=-confidence_threshold, color='orange', linestyle='--', linewidth=2, \n",
    "          label=f'Confidence threshold (\u00b1{confidence_threshold})')\n",
    "ax.set_xlabel('Decision Function (distance to hyperplane)', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Decision Function Distribution by True Label', fontweight='bold', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Confidence vs Accuracy\n",
    "ax = axes[1]\n",
    "confidence_bins = np.linspace(0, 3, 10)\n",
    "bin_accuracies = []\n",
    "bin_counts = []\n",
    "\n",
    "for i in range(len(confidence_bins) - 1):\n",
    "    mask = (np.abs(decision_values) >= confidence_bins[i]) & \\\n",
    "           (np.abs(decision_values) < confidence_bins[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        bin_acc = accuracy_score(y_test_devices[mask], y_pred_tuned[mask])\n",
    "        bin_accuracies.append(bin_acc)\n",
    "        bin_counts.append(mask.sum())\n",
    "    else:\n",
    "        bin_accuracies.append(0)\n",
    "        bin_counts.append(0)\n",
    "\n",
    "bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2\n",
    "ax.plot(bin_centers, bin_accuracies, marker='o', linewidth=2, markersize=8, color='green')\n",
    "ax.set_xlabel('Confidence (|distance to hyperplane|)', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Accuracy vs Confidence Level', fontweight='bold', fontsize=13)\n",
    "ax.set_ylim([0.5, 1.0])\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Annotate bins\n",
    "for i, (x, y, count) in enumerate(zip(bin_centers, bin_accuracies, bin_counts)):\n",
    "    if count > 0:\n",
    "        ax.text(x, y + 0.02, f'n={count}', ha='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Confidence-Based Strategy:\")\n",
    "print(f\"   1. High confidence (|d| > 1.0): Auto-accept prediction ({(np.abs(decision_values) > 1.0).sum():,} devices)\")\n",
    "print(f\"   2. Medium confidence (0.5 < |d| < 1.0): Standard review ({((np.abs(decision_values) > 0.5) & (np.abs(decision_values) < 1.0)).sum():,} devices)\")\n",
    "print(f\"   3. Low confidence (|d| < 0.5): Manual inspection ({(np.abs(decision_values) < 0.5).sum():,} devices)\")\n",
    "print(f\"\\n   \ud83d\udca1 This prioritization reduces manual review workload by {(1 - n_low_confidence/len(decision_values))*100:.1f}%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\ude80 8 Real-World Project Ideas\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "#### **Project 1: Multi-Class Defect Root Cause Classifier**\n",
    "- **Objective:** Classify device failures into root causes (leakage, timing, power, stuck-at-fault)\n",
    "- **Data:** 100K devices, 150 parametric tests, 5 failure modes\n",
    "- **SVM Approach:** One-vs-Rest SVC with class_weight, RBF kernel\n",
    "- **Features:** Voltage sweep curves, frequency-power curves, temperature response\n",
    "- **Success Metric:** 90%+ multi-class accuracy, <2 hour debug time per failure\n",
    "- **Business Value:** $500K-2M per product (reduce debug cycles from weeks to days)\n",
    "\n",
    "#### **Project 2: Wafer-Level Spatial Defect Detection**\n",
    "- **Objective:** Detect spatial patterns (process defects vs random failures)\n",
    "- **Data:** 300\u00d7300 die wafer maps, 50 parametric tests per die\n",
    "- **SVM Approach:** SVC with spatial features (die_x, die_y, neighbors), polynomial kernel (degree=2)\n",
    "- **Features:** Raw parametric + spatial correlation (8-neighbor average)\n",
    "- **Success Metric:** Separate systematic (95%+) from random defects (<5%)\n",
    "- **Business Value:** $2-5M per fab (identify process excursions early)\n",
    "\n",
    "#### **Project 3: Margin-Based Reliability Predictor**\n",
    "- **Objective:** Predict long-term reliability (10-year lifetime) from initial tests\n",
    "- **Data:** 50K devices, accelerated aging data, parametric drift over time\n",
    "- **SVM Approach:** SVR for time-to-failure regression, margin = reliability confidence\n",
    "- **Features:** Initial parametric + stress test response + temperature cycling\n",
    "- **Success Metric:** Predict 10-year failures within \u00b115% (vs \u00b130% current)\n",
    "- **Business Value:** $10-30M (reduce warranty returns by 40%)\n",
    "\n",
    "#### **Project 4: Multi-Site Test Correlation Engine**\n",
    "- **Objective:** Identify which wafer-level tests predict final-test failures\n",
    "- **Data:** 200K devices, 80 wafer tests \u2192 120 final tests\n",
    "- **SVM Approach:** Feature selection via recursive feature elimination (RFE), linear SVC\n",
    "- **Features:** All wafer parametric tests (high-dimensional p=80)\n",
    "- **Success Metric:** Reduce final test time 30% by skipping redundant tests\n",
    "- **Business Value:** $5-15M per product line (test time = $0.50 per device)\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "#### **Project 5: Medical Diagnosis Support System**\n",
    "- **Objective:** Classify diseases from patient symptoms and lab results\n",
    "- **Data:** 100K patients, 200 features (symptoms, vitals, lab tests, imaging scores)\n",
    "- **SVM Approach:** Multi-class SVC with class_weight, RBF kernel, SHAP for interpretability\n",
    "- **Features:** Demographic + symptoms + lab values + medical history\n",
    "- **Success Metric:** 92%+ accuracy, match specialist diagnosis, <5% false negatives\n",
    "- **Business Value:** $20-50M healthcare system (earlier intervention, reduce misdiagnosis)\n",
    "\n",
    "#### **Project 6: Fraud Detection for Financial Transactions**\n",
    "- **Objective:** Real-time credit card fraud detection\n",
    "- **Data:** 10M transactions, 50 features (amount, location, merchant, time, user behavior)\n",
    "- **SVM Approach:** LinearSVC for speed (sub-millisecond), class_weight for imbalance\n",
    "- **Features:** Transaction amount, velocity (transactions/hour), location anomaly, merchant category\n",
    "- **Success Metric:** 98%+ precision (minimize false positives), <10ms latency\n",
    "- **Business Value:** $50-200M (prevent $1B+ fraud, reduce false declines)\n",
    "\n",
    "#### **Project 7: Text Classification (Sentiment Analysis)**\n",
    "- **Objective:** Classify customer reviews (positive/negative/neutral)\n",
    "- **Data:** 500K reviews, TF-IDF vectorization (10K vocabulary)\n",
    "- **SVM Approach:** LinearSVC (fast for high-dim text), TF-IDF features\n",
    "- **Features:** TF-IDF vectors, n-grams (1-3), word embeddings (Word2Vec)\n",
    "- **Success Metric:** 88%+ accuracy, <100ms inference per review\n",
    "- **Business Value:** $10-30M (automated customer feedback analysis, trend detection)\n",
    "\n",
    "#### **Project 8: Image Classification (Handwritten Digit Recognition)**\n",
    "- **Objective:** Classify MNIST digits (0-9)\n",
    "- **Data:** 70K images (28\u00d728 pixels = 784 features)\n",
    "- **SVM Approach:** RBF SVC, pixel intensity features, data augmentation\n",
    "- **Features:** Raw pixels (normalized 0-1), optional PCA for dimensionality reduction\n",
    "- **Success Metric:** 98%+ accuracy (competitive with shallow networks)\n",
    "- **Business Value:** $5-15M (automated document processing, check reading)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udcda SVM Best Practices & When to Use\n",
    "\n",
    "### \u2705 When to Use SVM\n",
    "\n",
    "1. **Binary or Multi-Class Classification**\n",
    "   - Clear margin between classes\n",
    "   - Need probabilistic confidence (via `probability=True`)\n",
    "\n",
    "2. **High-Dimensional Data (p > n)**\n",
    "   - Text classification (TF-IDF: p = 10K-100K)\n",
    "   - Genomics data (p = 20K genes, n = 100 samples)\n",
    "   - SVM works well when features > samples\n",
    "\n",
    "3. **Non-Linear Decision Boundaries**\n",
    "   - Complex patterns (RBF kernel)\n",
    "   - Interactions between features (polynomial kernel)\n",
    "   - Kernel trick handles without explicit feature engineering\n",
    "\n",
    "4. **Small to Medium Datasets (n < 100K)**\n",
    "   - Training time $O(n^2 p)$ to $O(n^3 p)$ \u2192 slow for large n\n",
    "   - Use LinearSVC or SGDClassifier for n > 100K\n",
    "\n",
    "5. **Margin-Based Confidence Needed**\n",
    "   - Distance to hyperplane = reliability score\n",
    "   - Prioritize manual review for low-confidence predictions\n",
    "\n",
    "### \u274c When NOT to Use SVM\n",
    "\n",
    "1. **Large Datasets (n > 100K)**\n",
    "   - Training time becomes prohibitive\n",
    "   - Alternative: LinearSVC (liblinear), SGDClassifier (linear SVM via SGD)\n",
    "   - For non-linear: kernel approximation (Nystroem, RBFSampler) + LinearSVC\n",
    "\n",
    "2. **Need Probabilistic Predictions**\n",
    "   - SVM decision function is not true probability\n",
    "   - `probability=True` uses Platt scaling (adds overhead)\n",
    "   - Alternative: Logistic Regression (native probabilities)\n",
    "\n",
    "3. **Multi-Output Regression**\n",
    "   - SVR only handles single output\n",
    "   - Alternative: Random Forest Regressor (multi-output)\n",
    "\n",
    "4. **Need Feature Importance**\n",
    "   - SVM with RBF kernel: no interpretable coefficients\n",
    "   - Linear SVM: `coef_` available but not as clear as tree-based\n",
    "   - Alternative: Random Forest, XGBoost (feature_importances_)\n",
    "\n",
    "5. **Real-Time Requirements (< 1ms)**\n",
    "   - Prediction $O(n_{sv} \\cdot p)$ can be slow\n",
    "   - Alternative: Logistic Regression, Naive Bayes (faster inference)\n",
    "\n",
    "### \ud83d\udd27 Hyperparameter Tuning Checklist\n",
    "\n",
    "```python\n",
    "# 1. Always scale features first!\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. Start with RBF kernel (default choice)\n",
    "svm = SVC(kernel='rbf', class_weight='balanced', random_state=42)\n",
    "\n",
    "# 3. Tune C and gamma via GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. If high-dimensional (p > 1000), try Linear first\n",
    "from sklearn.svm import LinearSVC\n",
    "svm_linear = LinearSVC(C=1.0, class_weight='balanced', max_iter=1000, random_state=42)\n",
    "svm_linear.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 5. For large datasets (n > 100K), use SGDClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "svm_sgd = SGDClassifier(loss='hinge', alpha=0.0001, max_iter=1000, random_state=42)\n",
    "svm_sgd.fit(X_train_scaled, y_train)\n",
    "```\n",
    "\n",
    "### \ud83c\udfaf SVM vs Other Algorithms\n",
    "\n",
    "| **Criterion** | **SVM** | **Logistic Regression** | **Random Forest** | **KNN** |\n",
    "|---------------|---------|------------------------|-------------------|----------|\n",
    "| **Training Speed** | Slow ($O(n^2 p)$) | Fast ($O(np)$) | Medium ($O(n \\log n \\cdot p)$) | None (lazy) |\n",
    "| **Prediction Speed** | Fast ($O(n_{sv} \\cdot p)$) | Very Fast ($O(p)$) | Fast ($O(\\text{trees} \\cdot \\log n)$) | Slow ($O(np)$) |\n",
    "| **Non-Linear Boundaries** | \u2705 (kernel trick) | \u274c (linear only) | \u2705 (tree splits) | \u2705 (local) |\n",
    "| **High Dimensional (p >> n)** | \u2705 | \u2705 | \u26a0\ufe0f (needs tuning) | \u274c (curse of dim) |\n",
    "| **Interpretability** | \u274c (kernel), \u26a0\ufe0f (linear) | \u2705 (coefficients) | \u26a0\ufe0f (feature importance) | \u2705 (neighbors) |\n",
    "| **Probabilistic Output** | \u26a0\ufe0f (Platt scaling) | \u2705 (native) | \u2705 (vote proportion) | \u2705 (proportion) |\n",
    "| **Imbalanced Data** | \u2705 (class_weight) | \u2705 (class_weight) | \u2705 (class_weight) | \u26a0\ufe0f (weighted KNN) |\n",
    "| **Hyperparameter Sensitivity** | High (C, gamma) | Low | Medium (trees, depth) | High (K, metric) |\n",
    "\n",
    "### \ud83d\ude80 Production Deployment Tips\n",
    "\n",
    "1. **Save scaler with model** \u2014 scaling parameters must match training\n",
    "   ```python\n",
    "   import joblib\n",
    "   joblib.dump((scaler, svm_model), 'svm_production.pkl')\n",
    "   scaler, model = joblib.load('svm_production.pkl')\n",
    "   ```\n",
    "\n",
    "2. **Use `probability=False` if possible** \u2014 2-5x faster inference\n",
    "\n",
    "3. **For large n_support_vectors, consider kernel approximation**:\n",
    "   ```python\n",
    "   from sklearn.kernel_approximation import Nystroem\n",
    "   nystroem = Nystroem(kernel='rbf', gamma=0.1, n_components=100)\n",
    "   X_approx = nystroem.fit_transform(X_train)\n",
    "   svm_linear = LinearSVC().fit(X_approx, y_train)\n",
    "   ```\n",
    "\n",
    "4. **Monitor decision function distribution in production**:\n",
    "   - Shift in distribution \u2192 model drift\n",
    "   - Increasing low-confidence predictions \u2192 retrain needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udf93 Key Takeaways\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "1. **Maximum Margin Principle**\n",
    "   - SVM finds hyperplane with largest margin (2/||w||)\n",
    "   - Better generalization than arbitrary boundary\n",
    "   - Only support vectors (points on margin) define model\n",
    "\n",
    "2. **Kernel Trick Magic**\n",
    "   - Maps data to high-dimensional space without explicit computation\n",
    "   - $K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)$ (never compute $\\phi$ directly)\n",
    "   - RBF kernel can map to **infinite-dimensional space**!\n",
    "\n",
    "3. **Soft Margin & C Parameter**\n",
    "   - Hard margin: No violations, only works if perfectly separable\n",
    "   - Soft margin: Allow violations with penalty (slack variables $\\xi_i$)\n",
    "   - C controls trade-off: Large C = hard margin, small C = soft margin\n",
    "\n",
    "4. **Sparse Solution**\n",
    "   - Only 10-40% of training points become support vectors\n",
    "   - Prediction only depends on support vectors (efficient)\n",
    "   - Contrast: KNN needs all n training points for prediction\n",
    "\n",
    "5. **Margin-Based Confidence**\n",
    "   - decision_function() = signed distance to hyperplane\n",
    "   - Large |distance| = high confidence (far from boundary)\n",
    "   - Small |distance| = low confidence (near boundary, manual review)\n",
    "\n",
    "### Mathematical Insights\n",
    "\n",
    "- **Primal formulation**: $\\min \\frac{1}{2}\\|w\\|^2 + C \\sum \\xi_i$ (minimize margin + violations)\n",
    "- **Dual formulation**: $\\max \\sum \\alpha_i - \\frac{1}{2} \\sum \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)$ (Lagrange multipliers)\n",
    "- **Decision function**: $f(x) = \\text{sign}(\\sum \\alpha_i y_i K(x_i, x) + b)$ (only $\\alpha_i > 0$ for support vectors)\n",
    "- **RBF kernel**: $K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2)$ where $\\gamma = 1/(2\\sigma^2)$\n",
    "\n",
    "### Practical Wisdom\n",
    "\n",
    "1. **ALWAYS scale features** \u2014 SVM is distance-based (like KNN)\n",
    "2. **Start with RBF kernel** \u2014 Default choice for unknown data structure\n",
    "3. **Use GridSearchCV** \u2014 C and gamma dramatically affect performance\n",
    "4. **Handle imbalance** \u2014 `class_weight='balanced'` for imbalanced classes\n",
    "5. **Check training time** \u2014 If n > 100K, use LinearSVC or SGDClassifier\n",
    "6. **Kernel selection**:\n",
    "   - Linear: High-dimensional (p > 10K), interpretability needed\n",
    "   - RBF: Default, non-linear patterns, robust\n",
    "   - Polynomial: Known degree of interaction (d=2 quadratic, d=3 cubic)\n",
    "7. **Support vector analysis** \u2014 Many support vectors (>50%) \u2192 consider simpler model\n",
    "\n",
    "### When SVM Shines\n",
    "\n",
    "\u2705 High-dimensional data (text, genomics: p >> n)\n",
    "\u2705 Clear margin between classes\n",
    "\u2705 Non-linear boundaries (RBF kernel)\n",
    "\u2705 Small to medium datasets (n < 100K)\n",
    "\u2705 Need margin-based confidence scores\n",
    "\n",
    "### When to Choose Alternatives\n",
    "\n",
    "\u274c Large datasets (n > 100K) \u2192 LinearSVC, SGDClassifier, or XGBoost\n",
    "\u274c Need probabilistic predictions \u2192 Logistic Regression\n",
    "\u274c Need feature importance \u2192 Random Forest, XGBoost\n",
    "\u274c Multi-output regression \u2192 Random Forest Regressor\n",
    "\u274c Real-time (< 1ms) \u2192 Logistic Regression, Naive Bayes\n",
    "\n",
    "### SVM in ML Pipeline\n",
    "\n",
    "```\n",
    "Data \u2192 Feature Engineering \u2192 StandardScaler \u2192 SVM \u2192 Evaluation\n",
    "                                                \u2193\n",
    "                                         GridSearchCV (C, gamma)\n",
    "                                                \u2193\n",
    "                                    decision_function (confidence)\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **025 Naive Bayes**: Probabilistic classification (contrast with margin-based SVM)\n",
    "- **026 K-Means Clustering**: Unsupervised learning (no labels)\n",
    "- **027 PCA**: Dimensionality reduction (improve SVM performance)\n",
    "\n",
    "### References & Further Reading\n",
    "\n",
    "1. **Books**:\n",
    "   - *The Elements of Statistical Learning* (Hastie, Tibshirani, Friedman) \u2014 Chapter 12: Support Vector Machines\n",
    "   - *Pattern Recognition and Machine Learning* (Bishop) \u2014 Chapter 7: Sparse Kernel Machines\n",
    "\n",
    "2. **Papers**:\n",
    "   - Vapnik (1995): *The Nature of Statistical Learning Theory* (original SVM theory)\n",
    "   - Cortes & Vapnik (1995): *Support-Vector Networks* (soft margin SVM)\n",
    "   - Sch\u00f6lkopf et al. (1998): *Nonlinear Component Analysis as a Kernel Eigenvalue Problem*\n",
    "\n",
    "3. **sklearn Documentation**:\n",
    "   - [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "   - [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "   - [User Guide: Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html)\n",
    "\n",
    "4. **Tutorials**:\n",
    "   - [StatQuest: Support Vector Machines (YouTube)](https://www.youtube.com/watch?v=efR1C6CvhmE)\n",
    "   - [Andrew Ng: CS229 Lecture Notes on SVM](http://cs229.stanford.edu/notes/cs229-notes3.pdf)\n",
    "\n",
    "---\n",
    "\n",
    "**\ud83c\udf89 Congratulations! You've mastered Support Vector Machines!**\n",
    "\n",
    "You now understand:\n",
    "- \u2705 Maximum margin classification principle\n",
    "- \u2705 Kernel trick for non-linear boundaries\n",
    "- \u2705 Hyperparameter tuning (C, gamma)\n",
    "- \u2705 From-scratch implementation (hinge loss)\n",
    "- \u2705 Production sklearn usage (SVC, SVR)\n",
    "- \u2705 Post-silicon application (50K device classification)\n",
    "- \u2705 Margin-based confidence scoring\n",
    "- \u2705 When to use SVM vs alternatives\n",
    "\n",
    "**Ready for Naive Bayes? Let's continue the journey! \ud83d\ude80**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}