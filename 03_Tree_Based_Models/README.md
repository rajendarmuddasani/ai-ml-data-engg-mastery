# 03 - Tree-Based Models

**Purpose:** Master decision trees, random forests, and gradient boosting (most powerful ML algorithms)

## Notebooks

- **016_Decision_Trees.ipynb** - CART, RSS/Gini splitting, pruning, feature importance
- **017_Random_Forest.ipynb** - Bagging, OOB error, variance reduction, parallel training
- **018_Gradient_Boosting.ipynb** - Sequential learning, pseudo-residuals, learning rate, early stopping
- **019_XGBoost.ipynb** - L1/L2 regularization, 2nd-order gradients, sparsity handling, GPU acceleration
- **020_LightGBM.ipynb** - Histogram-based, leaf-wise growth, GOSS, EFB (10-100Ã— speedup)
- **021_CatBoost.ipynb** - Ordered boosting, ordered target statistics, high-cardinality categoricals
- **022_Voting_Stacking_Ensembles.ipynb** - Model combination, meta-learning, production robustness

## Key Learning Outcomes

- Understand bagging vs boosting vs stacking
- Master XGBoost/LightGBM/CatBoost for Kaggle-level performance
- Apply to semiconductor yield prediction (90%+ accuracy)
- Build production-grade ensembles with 99.95% uptime
- Optimize hyperparameters systematically

## Prerequisites

- **02_Regression_Models** (supervised learning basics)
- Understanding of overfitting and cross-validation

## Next Steps

Proceed to **04_Distance_Based_Models** for instance-based learning.
