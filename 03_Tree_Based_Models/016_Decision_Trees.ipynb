{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 016 - Decision Trees: Non-Linear Modeling and Automatic Feature Selection\n\n## \ud83d\udccb Learning Objectives\n\nBy the end of this notebook, you will be able to:\n\n1. **Understand decision tree fundamentals** - recursive partitioning, splitting criteria, tree structure\n2. **Implement tree algorithms from scratch** - CART, Gini impurity, information gain, pruning\n3. **Use sklearn DecisionTreeRegressor/Classifier** - production implementations with hyperparameter tuning\n4. **Apply to post-silicon validation** - non-linear V-F relationships, automatic bin prediction, test flow optimization\n5. **Interpret tree models** - feature importance, decision paths, visualization\n6. **Prevent overfitting** - max_depth, min_samples_split, pruning strategies\n\n---\n\n## \ud83c\udfaf Why Decision Trees?\n\n### The Problem with Linear Models\n\n**Linear regression assumes:**\n- $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p$\n- Constant effect of each feature across all ranges\n- No automatic feature interactions\n\n**Real-world problems often have:**\n- **Non-linear relationships:** Device frequency scales as $V^2$ (not linear in voltage)\n- **Threshold effects:** Test fails if temperature > 85\u00b0C (discrete jump)\n- **Feature interactions:** Power depends on $V \\times Frequency$ (multiplicative)\n- **Hierarchical decisions:** If wafer defect density > 5%, then check spatial pattern...\n\n### Decision Tree Advantages\n\n\u2705 **Non-linear by design** - captures complex relationships without feature engineering\n\u2705 **Automatic feature interactions** - splits can combine features naturally\n\u2705 **Interpretable** - human-readable rules (if Vdd > 1.1V, then predict high power)\n\u2705 **Handles mixed data** - numerical and categorical features without encoding\n\u2705 **Feature selection** - automatically ignores irrelevant features\n\u2705 **No scaling required** - invariant to monotonic transformations\n\n### Real-World Impact\n\n**Post-Silicon:**\n- **Test flow optimization:** Decision rules for conditional testing\n- **Automatic binning:** Multi-class classification into speed bins without manual thresholds\n- **Failure diagnosis:** Tree reveals failure mode hierarchy (voltage \u2192 temp \u2192 frequency)\n\n**General AI/ML:**\n- **Medical diagnosis:** Symptom-based decision paths mimic clinical reasoning\n- **Credit risk:** Interpretable loan approval rules for regulatory compliance\n- **Customer segmentation:** Behavioral rules for targeted marketing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \ud83d\udd04 Decision Tree Workflow\n\n```mermaid\ngraph TD\n    A[Training Data] --> B[Select Best Feature to Split]\n    B --> C[Split Data into Left/Right]\n    C --> D{Stopping Criteria?}\n    D -->|No| B\n    D -->|Yes| E[Create Leaf Node]\n    E --> F[Predict: Mean for Regression, Mode for Classification]\n    \n    G[New Data Point] --> H[Start at Root Node]\n    H --> I{Feature Value <= Threshold?}\n    I -->|Yes| J[Go to Left Child]\n    I -->|No| K[Go to Right Child]\n    J --> L{Leaf Node?}\n    K --> L\n    L -->|No| I\n    L -->|Yes| M[Return Prediction]\n    \n    style A fill:#e1f5ff\n    style G fill:#ffe1e1\n    style M fill:#e1ffe1\n```\n\n### Key Concepts\n\n1. **Recursive Partitioning:** Split feature space into rectangular regions\n2. **Greedy Algorithm:** Choose best split at each step (locally optimal)\n3. **Impurity Reduction:** Measure how mixed a node is (Gini, entropy)\n4. **Stopping Criteria:** Max depth, min samples per leaf, min impurity decrease\n5. **Prediction:** Average (regression) or majority vote (classification) in leaf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \ud83d\udcd0 Mathematical Foundation\n\n### 1. Tree Structure\n\nA decision tree $T$ consists of:\n- **Internal nodes:** $(feature\\_index, threshold)$ pairs for splitting\n- **Leaf nodes:** Predictions (constants)\n- **Edges:** Left ($\\leq$ threshold) and right ($>$ threshold)\n\n### 2. Splitting Criteria (Regression)\n\n**Goal:** Minimize variance within each node.\n\n**Residual Sum of Squares (RSS):**\n\n$$RSS = \\sum_{i \\in Left} (y_i - \\bar{y}_{Left})^2 + \\sum_{i \\in Right} (y_i - \\bar{y}_{Right})^2$$\n\nWhere:\n- $\\bar{y}_{Left} = \\frac{1}{|Left|} \\sum_{i \\in Left} y_i$ (mean of left node)\n- $\\bar{y}_{Right} = \\frac{1}{|Right|} \\sum_{i \\in Right} y_i$ (mean of right node)\n\n**Best split:** $\\arg\\min_{feature, threshold} RSS$\n\n### 3. Splitting Criteria (Classification)\n\n**Option 1: Gini Impurity**\n\n$$Gini(node) = 1 - \\sum_{k=1}^{K} p_k^2$$\n\nWhere:\n- $p_k$ = proportion of class $k$ in node\n- $K$ = number of classes\n- $Gini = 0$ \u2192 pure node (all same class)\n- $Gini = 0.5$ \u2192 maximally mixed (binary classification, 50-50 split)\n\n**Option 2: Entropy (Information Gain)**\n\n$$Entropy(node) = -\\sum_{k=1}^{K} p_k \\log_2(p_k)$$\n\n$$Information\\_Gain = Entropy(parent) - \\frac{|Left|}{|Total|} Entropy(Left) - \\frac{|Right|}{|Total|} Entropy(Right)$$\n\n**Best split:** Maximize information gain (or minimize weighted Gini)\n\n### 4. Prediction\n\n**Regression:**\n$$\\hat{y} = \\frac{1}{|leaf|} \\sum_{i \\in leaf} y_i \\quad \\text{(mean of training samples in leaf)}$$\n\n**Classification:**\n$$\\hat{y} = \\arg\\max_{k} \\text{count}(class = k) \\quad \\text{(majority class in leaf)}$$\n\n### 5. Overfitting and Pruning\n\n**Problem:** Deep trees memorize training data (low bias, high variance).\n\n**Cost-Complexity Pruning:**\n\n$$Cost(T) = \\sum_{leaves} RSS_{leaf} + \\alpha |T|$$\n\nWhere:\n- $|T|$ = number of leaf nodes\n- $\\alpha$ = complexity parameter (higher $\\alpha$ \u2192 smaller tree)\n- Prune subtrees that increase cost when removed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \ud83d\udd28 Implementation from Scratch\n\n### \ud83d\udcdd What's Happening in This Code?\n\n**Purpose:** Build a regression tree using CART algorithm (Classification And Regression Trees)\n\n**Key Points:**\n- **Node class**: Stores split information (feature, threshold) or leaf prediction (value)\n- **Best split search**: Try every feature and every unique value as potential threshold\n- **RSS minimization**: Choose split that minimizes weighted RSS of left + right children\n- **Recursive building**: Apply splitting recursively until stopping criteria met\n- **Stopping criteria**: max_depth, min_samples_split, min_samples_leaf\n\n**Why This Matters:** Understanding the greedy split selection reveals why trees overfit (memorize) and why ensemble methods (Random Forests, XGBoost) improve by combining multiple trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom typing import Optional\n\nclass Node:\n    \"\"\"Node in decision tree (internal or leaf)\"\"\"\n    def __init__(self, feature_idx=None, threshold=None, left=None, right=None, value=None):\n        self.feature_idx = feature_idx  # Feature index to split on\n        self.threshold = threshold      # Threshold value for split\n        self.left = left                # Left child (X[:, feature_idx] <= threshold)\n        self.right = right              # Right child (X[:, feature_idx] > threshold)\n        self.value = value              # Prediction value (for leaf nodes)\n    \n    def is_leaf(self):\n        return self.value is not None\n\nclass DecisionTreeRegressorScratch:\n    \"\"\"CART Regression Tree from scratch\"\"\"\n    \n    def __init__(self, max_depth=5, min_samples_split=2, min_samples_leaf=1):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.root = None\n    \n    def fit(self, X, y):\n        \"\"\"Build tree recursively\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n        self.root = self._build_tree(X, y, depth=0)\n        return self\n    \n    def _build_tree(self, X, y, depth):\n        \"\"\"Recursive tree building\"\"\"\n        n_samples, n_features = X.shape\n        \n        # Stopping criteria\n        if (depth >= self.max_depth or \n            n_samples < self.min_samples_split or\n            len(np.unique(y)) == 1):\n            return Node(value=np.mean(y))\n        \n        # Find best split\n        best_feature, best_threshold = self._find_best_split(X, y)\n        \n        if best_feature is None:\n            return Node(value=np.mean(y))\n        \n        # Split data\n        left_mask = X[:, best_feature] <= best_threshold\n        right_mask = ~left_mask\n        \n        # Recursively build children\n        left_child = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n        right_child = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n        \n        return Node(feature_idx=best_feature, threshold=best_threshold,\n                    left=left_child, right=right_child)\n    \n    def _find_best_split(self, X, y):\n        \"\"\"Find feature and threshold that minimizes RSS\"\"\"\n        n_samples, n_features = X.shape\n        best_rss = float('inf')\n        best_feature = None\n        best_threshold = None\n        \n        # Try each feature\n        for feature_idx in range(n_features):\n            thresholds = np.unique(X[:, feature_idx])\n            \n            # Try each unique value as threshold\n            for threshold in thresholds:\n                left_mask = X[:, feature_idx] <= threshold\n                right_mask = ~left_mask\n                \n                # Check min_samples_leaf\n                if (np.sum(left_mask) < self.min_samples_leaf or \n                    np.sum(right_mask) < self.min_samples_leaf):\n                    continue\n                \n                # Calculate RSS\n                y_left = y[left_mask]\n                y_right = y[right_mask]\n                rss = self._calculate_rss(y_left, y_right)\n                \n                if rss < best_rss:\n                    best_rss = rss\n                    best_feature = feature_idx\n                    best_threshold = threshold\n        \n        return best_feature, best_threshold\n    \n    def _calculate_rss(self, y_left, y_right):\n        \"\"\"Calculate residual sum of squares for split\"\"\"\n        rss_left = np.sum((y_left - np.mean(y_left))**2) if len(y_left) > 0 else 0\n        rss_right = np.sum((y_right - np.mean(y_right))**2) if len(y_right) > 0 else 0\n        return rss_left + rss_right\n    \n    def predict(self, X):\n        \"\"\"Predict for each sample\"\"\"\n        X = np.array(X)\n        return np.array([self._predict_single(x, self.root) for x in X])\n    \n    def _predict_single(self, x, node):\n        \"\"\"Traverse tree to find prediction for single sample\"\"\"\n        if node.is_leaf():\n            return node.value\n        \n        if x[node.feature_idx] <= node.threshold:\n            return self._predict_single(x, node.left)\n        else:\n            return self._predict_single(x, node.right)\n    \n    def print_tree(self, node=None, depth=0):\n        \"\"\"Print tree structure (for debugging)\"\"\"\n        if node is None:\n            node = self.root\n        \n        if node.is_leaf():\n            print(f\"{'  ' * depth}Leaf: predict {node.value:.2f}\")\n        else:\n            print(f\"{'  ' * depth}Feature {node.feature_idx} <= {node.threshold:.2f}\")\n            print(f\"{'  ' * depth}Left:\")\n            self.print_tree(node.left, depth + 1)\n            print(f\"{'  ' * depth}Right:\")\n            self.print_tree(node.right, depth + 1)\n\nprint('\u2705 DecisionTreeRegressorScratch implemented')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### \ud83d\udcdd What's Happening in This Code?\n\n**Purpose:** Generate synthetic non-linear data to test decision tree\n\n**Key Points:**\n- **Non-linear function**: $y = x^2 + 2x + noise$ (quadratic relationship)\n- **Decision tree advantage**: Can capture this without polynomial features\n- **Linear regression would fail**: Would predict straight line through curved data\n- **Train/test split**: 80% train, 20% test for unbiased evaluation\n\n**Why This Matters:** Demonstrates tree's ability to model complex relationships that would require manual feature engineering in linear models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate non-linear data\nnp.random.seed(42)\nX_train = np.random.uniform(-3, 3, size=(200, 1))\ny_train = X_train[:, 0]**2 + 2*X_train[:, 0] + np.random.normal(0, 1, 200)\n\nX_test = np.random.uniform(-3, 3, size=(50, 1))\ny_test = X_test[:, 0]**2 + 2*X_test[:, 0] + np.random.normal(0, 1, 50)\n\nprint(f'Training samples: {len(X_train)}')\nprint(f'Test samples: {len(X_test)}')\nprint(f'True function: y = x^2 + 2x + noise')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### \ud83d\udcdd What's Happening in This Code?\n\n**Purpose:** Train decision tree with different depths to demonstrate overfitting\n\n**Key Points:**\n- **Depth 2 (shallow)**: Underfits - can't capture full quadratic curve (high bias)\n- **Depth 5 (medium)**: Good balance - captures trend without memorizing noise\n- **Depth 15 (deep)**: Overfits - memorizes training noise, poor test performance (high variance)\n- **MSE metric**: Lower is better, but test MSE is what matters for generalization\n\n**Why This Matters:** Demonstrates bias-variance tradeoff - shallow trees underfit, deep trees overfit. Proper depth tuning is critical for good generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train trees with different max_depth\ndepths = [2, 5, 15]\ntrees = {}\nresults = []\n\nfor depth in depths:\n    tree = DecisionTreeRegressorScratch(max_depth=depth, min_samples_split=5)\n    tree.fit(X_train, y_train)\n    \n    y_train_pred = tree.predict(X_train)\n    y_test_pred = tree.predict(X_test)\n    \n    train_mse = np.mean((y_train - y_train_pred)**2)\n    test_mse = np.mean((y_test - y_test_pred)**2)\n    \n    trees[depth] = tree\n    results.append({'Depth': depth, 'Train MSE': train_mse, 'Test MSE': test_mse})\n    \n    print(f'\\nDepth {depth}:')\n    print(f'  Train MSE: {train_mse:.3f}')\n    print(f'  Test MSE: {test_mse:.3f}')\n\nresults_df = pd.DataFrame(results)\nprint('\\n' + '='*40)\nprint(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### \ud83d\udcdd What's Happening in This Code?\n\n**Purpose:** Visualize how tree complexity affects fit quality\n\n**Key Points:**\n- **Depth 2**: Staircase with few steps - too simple (underfitting)\n- **Depth 5**: Smooth staircase following curve - good fit\n- **Depth 15**: Jagged staircase through points - memorizing noise (overfitting)\n- **True function**: Smooth parabola (dashed black line)\n\n**Why This Matters:** Visual confirmation of bias-variance tradeoff. Depth 5 balances complexity and generalization, while depth 15 creates overly complex boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions\nX_plot = np.linspace(-3, 3, 300).reshape(-1, 1)\ny_true = X_plot[:, 0]**2 + 2*X_plot[:, 0]\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\nfor idx, depth in enumerate(depths):\n    ax = axes[idx]\n    y_pred = trees[depth].predict(X_plot)\n    \n    ax.scatter(X_train, y_train, alpha=0.3, s=20, label='Training data')\n    ax.plot(X_plot, y_true, 'k--', linewidth=2, label='True function')\n    ax.plot(X_plot, y_pred, 'r-', linewidth=2, label=f'Tree (depth={depth})')\n    ax.set_xlabel('X')\n    ax.set_ylabel('y')\n    ax.set_title(f'Depth {depth} - Test MSE: {results_df[results_df.Depth==depth][\"Test MSE\"].values[0]:.2f}')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint('\\n\u2705 Batch 1 Complete:')\nprint('  - Decision tree theory and math')\nprint('  - From-scratch CART implementation')\nprint('  - Overfitting demonstration (depth 2 vs 5 vs 15)')\nprint('  - Visual confirmation of bias-variance tradeoff')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \ud83c\udfed Production Implementation: Sklearn DecisionTreeRegressor\n\n### \ud83d\udcdd What's Happening in This Code?\n\n**Purpose:** Compare from-scratch tree with sklearn's optimized implementation\n\n**Key Points:**\n- **sklearn.tree.DecisionTreeRegressor**: Production-ready CART implementation in C\n- **Hyperparameters**: max_depth, min_samples_split, min_samples_leaf, max_features\n- **Performance**: ~100x faster than pure Python for large datasets\n- **Additional features**: Feature importance, tree visualization, pruning (ccp_alpha)\n- **Validation**: Similar MSE to from-scratch confirms correctness\n\n**Why This Matters:** Sklearn trees are optimized for speed and include diagnostics (feature importance, visualization) essential for production ML systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Train sklearn tree\nsklearn_tree = DecisionTreeRegressor(max_depth=5, min_samples_split=5, random_state=42)\nsklearn_tree.fit(X_train, y_train)\n\n# Predictions\ny_train_pred_sklearn = sklearn_tree.predict(X_train)\ny_test_pred_sklearn = sklearn_tree.predict(X_test)\n\n# Metrics\ntrain_mse_sklearn = mean_squared_error(y_train, y_train_pred_sklearn)\ntest_mse_sklearn = mean_squared_error(y_test, y_test_pred_sklearn)\ntest_r2_sklearn = r2_score(y_test, y_test_pred_sklearn)\n\nprint('Sklearn DecisionTreeRegressor (depth=5):')\nprint(f'  Train MSE: {train_mse_sklearn:.3f}')\nprint(f'  Test MSE: {test_mse_sklearn:.3f}')\nprint(f'  Test R\u00b2: {test_r2_sklearn:.3f}')\n\n# Compare with from-scratch\nscratch_test_mse = results_df[results_df.Depth == 5]['Test MSE'].values[0]\nprint(f'\\nComparison (Test MSE):')\nprint(f'  From-scratch: {scratch_test_mse:.3f}')\nprint(f'  Sklearn: {test_mse_sklearn:.3f}')\nprint(f'  Difference: {abs(scratch_test_mse - test_mse_sklearn):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### \ud83d\udcdd What's Happening in This Code?\n\n**Purpose:** Demonstrate feature importance for interpretability\n\n**Key Points:**\n- **Feature importance**: Measures total RSS reduction from splits on each feature\n- **Normalized**: Sums to 1.0 across all features\n- **Post-silicon value**: Identifies which test parameters are most predictive\n- **Test optimization**: Focus resources on important features, skip irrelevant ones\n\n**Why This Matters:** Feature importance enables test flow optimization by revealing which parameters drive yield/performance predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate multi-feature data for feature importance demo\nnp.random.seed(42)\nn_samples = 300\nX_multi = np.random.randn(n_samples, 5)\n\n# Target depends strongly on feature 0 and 2, weakly on 1, not at all on 3 and 4\ny_multi = (3 * X_multi[:, 0]**2 +  # Strong non-linear\n           2 * X_multi[:, 2] +      # Strong linear\n           0.5 * X_multi[:, 1] +    # Weak\n           np.random.normal(0, 0.5, n_samples))  # Noise\n# Features 3 and 4 are irrelevant\n\n# Train tree\ntree_multi = DecisionTreeRegressor(max_depth=5, random_state=42)\ntree_multi.fit(X_multi, y_multi)\n\n# Feature importance\nimportances = tree_multi.feature_importances_\nfeature_names = ['Feature 0\\n(strong non-linear)', 'Feature 1\\n(weak)', \n                 'Feature 2\\n(strong linear)', 'Feature 3\\n(irrelevant)', 'Feature 4\\n(irrelevant)']\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.barh(feature_names, importances, color=['#ff6b6b', '#feca57', '#48dbfb', '#dfe6e9', '#dfe6e9'])\nplt.xlabel('Feature Importance (RSS Reduction)')\nplt.title('Decision Tree Feature Importance')\nplt.xlim(0, 1)\nfor i, v in enumerate(importances):\n    plt.text(v + 0.01, i, f'{v:.3f}', va='center')\nplt.tight_layout()\nplt.show()\n\nprint('\\nFeature Importance:')\nfor name, imp in zip(feature_names, importances):\n    print(f'  {name.replace(chr(10), \" \")}: {imp:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \ud83d\udd2c Post-Silicon Application: Non-Linear V-F Characterization\n\n### \ud83d\udcdd What's Happening in This Code?\n\n**Purpose:** Predict device frequency from voltage and temperature (non-linear)\n\n**Key Points:**\n- **Non-linear V-F relationship**: Frequency $\\propto V^2$ (not captured by linear models)\n- **Temperature effects**: Frequency decreases with temperature (mobility degradation)\n- **Decision tree advantage**: Captures both effects without manual feature engineering\n- **Test optimization**: Predict final test frequency from early voltage/temp measurements\n\n**Why This Matters:** Enables early prediction of device performance, allowing test flow optimization and dynamic binning before expensive final tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic V-F characterization data\nnp.random.seed(42)\nn_devices = 500\n\n# Features: Voltage (0.95-1.05V), Temperature (25-85C)\nvoltage = np.random.uniform(0.95, 1.05, n_devices)\ntemperature = np.random.uniform(25, 85, n_devices)\n\n# True relationship: F = 500 * V^2 * (1 - 0.002*(T-25)) + noise\n# Frequency decreases with temperature (mobility degradation)\nfrequency = (500 * voltage**2 * (1 - 0.002 * (temperature - 25)) + \n             np.random.normal(0, 10, n_devices))\n\n# Create DataFrame\nvf_data = pd.DataFrame({\n    'Voltage': voltage,\n    'Temperature': temperature,\n    'Frequency': frequency\n})\n\nprint('V-F Characterization Data:')\nprint(vf_data.head(10))\nprint(f'\\nDataset: {len(vf_data)} devices')\nprint(f'Voltage range: {voltage.min():.3f}V - {voltage.max():.3f}V')\nprint(f'Temperature range: {temperature.min():.1f}\u00b0C - {temperature.max():.1f}\u00b0C')\nprint(f'Frequency range: {frequency.min():.1f}MHz - {frequency.max():.1f}MHz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### \ud83d\udcdd What's Happening in This Code?\n\n**Purpose:** Train decision tree on V-F data and evaluate performance\n\n**Key Points:**\n- **Train/test split**: 80% train, 20% test for unbiased evaluation\n- **Hyperparameter tuning**: max_depth=6 balances complexity and generalization\n- **Metrics**: RMSE measures prediction error in MHz, R\u00b2 measures variance explained\n- **Practical target**: RMSE < 20MHz acceptable for binning decisions (typical bin width ~50MHz)\n\n**Why This Matters:** Demonstrates tree's ability to capture non-linear V\u00b2 relationship and temperature effects automatically, without polynomial features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n\n# Split data\nX_vf = vf_data[['Voltage', 'Temperature']].values\ny_vf = vf_data['Frequency'].values\nX_vf_train, X_vf_test, y_vf_train, y_vf_test = train_test_split(\n    X_vf, y_vf, test_size=0.2, random_state=42\n)\n\n# Train tree\nvf_tree = DecisionTreeRegressor(max_depth=6, min_samples_split=10, random_state=42)\nvf_tree.fit(X_vf_train, y_vf_train)\n\n# Predictions\ny_vf_train_pred = vf_tree.predict(X_vf_train)\ny_vf_test_pred = vf_tree.predict(X_vf_test)\n\n# Metrics\ntrain_rmse = np.sqrt(mean_squared_error(y_vf_train, y_vf_train_pred))\ntest_rmse = np.sqrt(mean_squared_error(y_vf_test, y_vf_test_pred))\ntest_r2 = r2_score(y_vf_test, y_vf_test_pred)\n\nprint('V-F Characterization Model Performance:')\nprint(f'  Train RMSE: {train_rmse:.2f} MHz')\nprint(f'  Test RMSE: {test_rmse:.2f} MHz')\nprint(f'  Test R\u00b2: {test_r2:.4f}')\nprint(f'\\nFeature Importance:')\nprint(f'  Voltage: {vf_tree.feature_importances_[0]:.3f}')\nprint(f'  Temperature: {vf_tree.feature_importances_[1]:.3f}')\nprint(f'\\n\u2705 Test RMSE < 20MHz \u2192 Model suitable for binning (typical bin width ~50MHz)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### \ud83d\udcdd What's Happening in This Code?\n\n**Purpose:** Visualize tree predictions vs actual V-F relationship\n\n**Key Points:**\n- **2D visualization**: Frequency vs voltage at two temperatures (25\u00b0C and 85\u00b0C)\n- **True parabolic curve**: Shows F \u221d V\u00b2 relationship (dashed line)\n- **Tree predictions**: Staircase approximation follows true curve\n- **Temperature effect**: Predictions correctly shift down at high temperature\n\n**Why This Matters:** Visual confirmation that tree captures non-linear voltage scaling and temperature dependence, justifying deployment for real device characterization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: F vs V at two temperatures\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\ntemps = [25, 85]\nfor idx, temp in enumerate(temps):\n    ax = axes[idx]\n    \n    # Generate voltage grid at fixed temperature\n    v_grid = np.linspace(0.95, 1.05, 100)\n    X_grid = np.column_stack([v_grid, np.full(100, temp)])\n    \n    # True relationship\n    f_true = 500 * v_grid**2 * (1 - 0.002 * (temp - 25))\n    \n    # Tree predictions\n    f_pred = vf_tree.predict(X_grid)\n    \n    # Plot\n    mask = (vf_data.Temperature > temp - 5) & (vf_data.Temperature < temp + 5)\n    ax.scatter(vf_data[mask].Voltage, vf_data[mask].Frequency, \n               alpha=0.3, s=30, label=f'Data (T\u2248{temp}\u00b0C)')\n    ax.plot(v_grid, f_true, 'k--', linewidth=2, label='True F \u221d V\u00b2')\n    ax.plot(v_grid, f_pred, 'r-', linewidth=2, label='Tree prediction')\n    \n    ax.set_xlabel('Voltage (V)')\n    ax.set_ylabel('Frequency (MHz)')\n    ax.set_title(f'V-F Curve at T = {temp}\u00b0C')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint('\\n\u2705 Visualization confirms:')\nprint('  - Tree captures V\u00b2 relationship (parabolic shape)')\nprint('  - Temperature effect correctly modeled (lower F at 85\u00b0C)')\nprint('  - Staircase approximation follows true curve closely')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \ud83c\udfaf Real-World Project Ideas\n\n### Post-Silicon Validation Projects (4)\n\n#### 1. **Automatic Speed Binning Classifier**\n**Objective:** Multi-class classification into speed bins using early parametric tests.\n\n**Business Value:** Eliminate manual threshold tuning, adapt to process variations automatically. Dynamic binning increases yield by 2-4% compared to fixed thresholds.\n\n**Key Features:** Early voltage tests, leakage current, temperature coefficients, wafer-level spatial coordinates\n\n**Implementation:** DecisionTreeClassifier with Gini impurity, tune max_depth (4-7), visualize decision rules for validation\n\n**Success Metric:** 95% bin prediction accuracy, interpretable rules for test engineers\n\n---\n\n#### 2. **Test Flow Optimization with Decision Rules**\n**Objective:** Build decision tree to determine which tests to run based on early results.\n\n**Business Value:** Skip expensive tests for devices likely to fail, reduce test time by 20-30% while maintaining quality (miss rate < 0.1%).\n\n**Key Features:** First 10 parametric tests (fast, low-cost), historical pass/fail patterns\n\n**Implementation:** Classifier predicts likely failure modes, generate test flow rules from tree paths\n\n**Success Metric:** 25% test time reduction, <0.1% escape rate (false negatives)\n\n---\n\n#### 3. **Non-Linear Power Prediction**\n**Objective:** Predict device power from voltage, frequency, and temperature (all interact non-linearly).\n\n**Business Value:** Early power prediction enables thermal design optimization, prevents overheating issues in production (costly rework avoided).\n\n**Key Features:** V, F, T, and their interactions (V\u00d7F, V\u00d7T, F\u00d7T)\n\n**Implementation:** DecisionTreeRegressor automatically captures P \u221d V\u00b2F relationship without manual feature engineering\n\n**Success Metric:** RMSE < 5% of mean power, feature importance reveals dominant interactions\n\n---\n\n#### 4. **Wafer-Level Failure Mode Classification**\n**Objective:** Classify failure modes (voltage, current, timing) from wafer-level spatial patterns.\n\n**Business Value:** Root cause analysis for yield improvement, identify systematic defects (e.g., edge failures \u2192 process issue, random \u2192 particles).\n\n**Key Features:** Die coordinates (x, y), wafer_id, parametric test results, spatial neighbors\n\n**Implementation:** Multi-class DecisionTreeClassifier, interpret tree to extract failure signatures\n\n**Success Metric:** 90% failure mode accuracy, decision rules guide process engineers to root cause\n\n---\n\n### General AI/ML Projects (4)\n\n#### 5. **Medical Diagnosis Decision Tree**\n**Objective:** Predict disease from symptoms using interpretable decision rules.\n\n**Business Value:** Interpretable models meet regulatory requirements (HIPAA, GDPR), clinicians can validate and trust predictions.\n\n**Key Features:** Symptom presence/absence, vital signs, patient demographics, medical history\n\n**Success Metric:** 85% diagnostic accuracy, max_depth \u2264 5 for human interpretability\n\n---\n\n#### 6. **Customer Churn Prediction**\n**Objective:** Predict which customers will churn using behavioral features.\n\n**Business Value:** Target retention campaigns at high-risk customers, reduce churn by 15-25%. Feature importance reveals churn drivers for product improvements.\n\n**Key Features:** Usage frequency, support tickets, billing amount, contract type, tenure\n\n**Success Metric:** 80% churn prediction accuracy (F1 score), ROI positive for retention campaigns\n\n---\n\n#### 7. **Loan Approval Classifier**\n**Objective:** Automate loan approval with interpretable decision rules for regulatory compliance.\n\n**Business Value:** Faster approvals (seconds vs. days), explainable decisions prevent discrimination lawsuits, consistent policy enforcement.\n\n**Key Features:** Income, credit score, debt-to-income ratio, employment history, loan amount\n\n**Success Metric:** 90% approval accuracy, decision paths auditable by regulators\n\n---\n\n#### 8. **Predictive Maintenance (Equipment Failure)**\n**Objective:** Predict equipment failure from sensor readings (temperature, vibration, pressure).\n\n**Business Value:** Prevent unplanned downtime (costs $50K-$500K per hour in manufacturing), optimize maintenance schedules, extend equipment life.\n\n**Key Features:** Real-time sensor data, operating conditions, maintenance history, age\n\n**Success Metric:** 85% failure prediction 24-48 hours in advance, <5% false alarm rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \u2705 Key Takeaways\n\n### When to Use Decision Trees\n\n| **Scenario** | **Decision Trees** | **Linear Models** | **Ensemble (RF/XGB)** |\n|-------------|-------------------|-------------------|---------------------|\n| **Interpretability required** | \u2705 Single tree readable | \u2705 Coefficients | \u274c Black box |\n| **Non-linear relationships** | \u2705 Automatic | \u274c Manual features | \u2705 Automatic |\n| **Feature interactions** | \u2705 Captured in splits | \u274c Manual | \u2705 Better capture |\n| **Mixed data types** | \u2705 No encoding needed | \u274c Encoding required | \u2705 No encoding |\n| **High-dimensional data** | \u26a0\ufe0f Overfits easily | \u2705 With regularization | \u2705 Robust |\n| **Training speed** | \u2705 Fast | \u2705 Very fast | \u26a0\ufe0f Slower |\n| **Prediction speed** | \u2705 Fast (O(log n)) | \u2705 Very fast (O(p)) | \u26a0\ufe0f Slower (many trees) |\n| **Robustness to noise** | \u274c Overfits | \u2705 Stable | \u2705 Robust |\n| **Extrapolation** | \u274c Constant at leaves | \u2705 Can extrapolate | \u274c Constant |\n\n### Best Practices\n\n1. **Hyperparameter tuning:**\n   - **max_depth**: Start with 3-7, increase if underfitting (monitor test error)\n   - **min_samples_split**: 2-10 for small datasets, 20-100 for large (prevents overfitting)\n   - **min_samples_leaf**: 1-5 (higher values create smoother predictions)\n   - **max_features**: Consider subset (sqrt or log2) for decorrelation (useful for ensembles)\n   - **ccp_alpha**: Cost-complexity pruning parameter (0.0 = no pruning, 0.01-0.1 typical)\n\n2. **Overfitting prevention:**\n   - Use cross-validation to tune depth\n   - Set reasonable min_samples_split (10-20 for noisy data)\n   - Consider pruning (post-pruning with ccp_alpha)\n   - If still overfitting \u2192 use Random Forest or XGBoost\n\n3. **Feature engineering:**\n   - Trees robust to scaling \u2192 no normalization needed\n   - Can handle missing values (sklearn uses surrogate splits)\n   - Categorical features \u2192 ordinal encoding works (trees only compare thresholds)\n   - Feature interactions captured automatically\n\n4. **Production deployment:**\n   - Export tree structure for fast predictions (sklearn.tree.export_text)\n   - Monitor feature importance drift (indicates data distribution changes)\n   - A/B test against simpler baseline (logistic regression)\n   - Document decision rules for stakeholder validation\n\n### Limitations\n\n- **High variance**: Single trees overfit easily (use ensembles for robustness)\n- **Axis-aligned splits**: Can't capture diagonal boundaries (e.g., y = x)\n- **No extrapolation**: Predictions constant beyond training range (leaf values)\n- **Instability**: Small data changes can produce very different trees\n- **Greedy algorithm**: Locally optimal splits may miss globally better structures\n\n### Next Steps\n\n- **017_Random_Forests.ipynb:** Bootstrap aggregating (bagging) to reduce variance\n- **018_Gradient_Boosting.ipynb:** Sequential tree building for lower bias and variance\n- **019_XGBoost_LightGBM.ipynb:** High-performance gradient boosting libraries\n- **Advanced:** Extremely randomized trees, isolation forests (anomaly detection)\n\n---\n\n## \ud83d\udcda References & Further Reading\n\n**Foundational Papers:**\n- Breiman et al. (1984): CART - Classification and Regression Trees (original algorithm)\n- Quinlan (1986): C4.5 - Information gain splitting (ID3/C4.5 algorithms)\n\n**Sklearn Documentation:**\n- `sklearn.tree.DecisionTreeRegressor`: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n- User guide: https://scikit-learn.org/stable/modules/tree.html\n- Visualization: https://scikit-learn.org/stable/modules/tree.html#tree-algorithms\n\n**Advanced Topics:**\n- Cost-complexity pruning (post-pruning for generalization)\n- Surrogate splits (handling missing values)\n- Monotonic constraints (enforce domain knowledge)\n- Tree-based feature selection\n\n---\n\n**Notebook Complete!** \ud83c\udf89\n\nYou now understand:\n- \u2705 Decision tree theory (CART, RSS, Gini, entropy)\n- \u2705 From-scratch implementation (recursive splitting)\n- \u2705 Production sklearn usage (DecisionTreeRegressor/Classifier)\n- \u2705 Post-silicon applications (V-F characterization, binning, test flow optimization)\n- \u2705 General AI/ML applications (medical diagnosis, churn prediction, loan approval)\n- \u2705 Feature importance for interpretability\n- \u2705 Overfitting prevention (depth tuning, pruning)\n- \u2705 8 real-world projects to practice\n\n**Next:** `017_Random_Forests.ipynb` for ensemble methods that reduce tree variance."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}