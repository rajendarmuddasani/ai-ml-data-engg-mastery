{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 017 - Random Forests: Bootstrap Aggregating for Robust Predictions\n\n## \ud83d\udccb Learning Objectives\n\nBy the end of this notebook, you will be able to:\n\n1. **Understand Random Forest fundamentals** - bagging, bootstrap sampling, random feature selection\n2. **Implement Random Forest from scratch** - build ensemble of decorrelated trees\n3. **Use sklearn RandomForestRegressor/Classifier** - production implementations with tuning\n4. **Apply to post-silicon validation** - robust yield prediction, multi-failure detection, feature ranking\n5. **Interpret ensemble models** - feature importance aggregation, OOB error estimation\n6. **Tune for optimal performance** - n_estimators, max_features, max_depth, min_samples_split\n\n---\n\n## \ud83c\udfaf Why Random Forests?\n\n### The Problem with Single Decision Trees\n\n**Decision trees suffer from:**\n- **High variance**: Small data changes \u2192 completely different trees\n- **Overfitting**: Deep trees memorize training noise\n- **Instability**: Not robust to outliers or sampling variation\n- **Greedy splitting**: Locally optimal decisions may miss global patterns\n\n**Example:** Train 5 trees on same data with slight variations \u2192 5 very different structures\n\n### Random Forest Solution: Wisdom of the Crowd\n\n**Key insight:** Average of many diverse models is more stable than any single model.\n\n\u2705 **Variance reduction**: Averaging reduces prediction variance (bias stays same)\n\u2705 **Robustness**: Outliers affect only some trees, not the entire forest\n\u2705 **No overfitting**: Individual trees can be deep (high variance), averaging stabilizes\n\u2705 **Decorrelation**: Random feature selection ensures trees are different\n\u2705 **OOB validation**: Built-in cross-validation (out-of-bag samples)\n\n### Real-World Impact\n\n**Post-Silicon:**\n- **Robust yield prediction**: Stable predictions despite noisy test data\n- **Multi-failure detection**: Each tree specializes in different failure modes\n- **Feature importance**: Aggregate rankings reveal truly important parameters\n- **Missing data handling**: Trees naturally handle missing values via surrogate splits\n\n**General AI/ML:**\n- **Kaggle competitions**: Random Forests consistently rank in top 3 methods\n- **Credit scoring**: Robust to data quality issues (missing values, outliers)\n- **Medical diagnosis**: Ensemble consensus reduces misdiagnosis risk\n- **Production ML**: Minimal tuning, handles mixed data types, fast training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \ud83d\udd04 Random Forest Algorithm\n\n```mermaid\ngraph TD\n    A[Training Data N samples] --> B[Bootstrap Sample 1]\n    A --> C[Bootstrap Sample 2]\n    A --> D[Bootstrap Sample K]\n    \n    B --> E[Tree 1: Random m features at each split]\n    C --> F[Tree 2: Random m features at each split]\n    D --> G[Tree K: Random m features at each split]\n    \n    E --> H[Predictions from Tree 1]\n    F --> I[Predictions from Tree 2]\n    G --> J[Predictions from Tree K]\n    \n    H --> K[Average for Regression]\n    I --> K\n    J --> K\n    \n    H --> L[Majority Vote for Classification]\n    I --> L\n    J --> L\n    \n    K --> M[Final Prediction]\n    L --> M\n    \n    style A fill:#e1f5ff\n    style M fill:#e1ffe1\n    style B fill:#fff3cd\n    style C fill:#fff3cd\n    style D fill:#fff3cd\n```\n\n### Key Concepts\n\n1. **Bootstrap Aggregating (Bagging):** Sample N observations with replacement \u2192 K different datasets\n2. **Random Feature Selection:** At each split, consider only random subset of m features (m \u2248 \u221ap)\n3. **Decorrelation:** Bootstrap + random features \u2192 trees make different mistakes\n4. **Out-of-Bag (OOB) Error:** ~37% of samples left out of each bootstrap \u2192 free validation set\n5. **Averaging/Voting:** Regression = mean prediction, Classification = majority class\n\n### Mathematical Intuition\n\n**Why averaging reduces variance:**\n\nIf we have K independent models with variance $\\sigma^2$, the ensemble variance is:\n\n$$Var(\\bar{f}) = \\frac{\\sigma^2}{K}$$\n\n**With correlation \u03c1 between models:**\n\n$$Var(\\bar{f}) = \\rho \\sigma^2 + \\frac{1-\\rho}{K} \\sigma^2$$\n\nWhere:\n- First term: Irreducible variance from correlation\n- Second term: Reducible variance (decreases with K)\n- **Goal:** Minimize \u03c1 via random feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \ud83d\udcd0 Mathematical Foundation\n\n### 1. Bootstrap Sampling\n\n**Procedure:** Sample N observations from training set **with replacement**.\n\n**Result:** Each bootstrap sample has:\n- ~63.2% unique observations (in-bag)\n- ~36.8% duplicates or missing observations (out-of-bag)\n\n**Proof:** Probability observation i is NOT selected in one draw:\n\n$$P(\\text{not selected}) = \\left(1 - \\frac{1}{N}\\right)^N \\approx e^{-1} \\approx 0.368$$\n\nTherefore, ~63.2% of observations appear at least once.\n\n### 2. Random Feature Selection\n\nAt each split, consider random subset of m features:\n\n- **Classification:** $m = \\sqrt{p}$ (default)\n- **Regression:** $m = p/3$ (default)\n- **Max decorrelation:** $m = 1$ (extremely randomized trees)\n\n**Why?** Prevents strong features from dominating all trees \u2192 more diverse ensemble.\n\n### 3. Prediction\n\n**Regression:**\n$$\\hat{y}_{RF} = \\frac{1}{K} \\sum_{k=1}^{K} \\hat{y}_k(x)$$\n\n**Classification:**\n$$\\hat{y}_{RF} = \\arg\\max_c \\sum_{k=1}^{K} \\mathbb{1}(\\hat{y}_k(x) = c)$$\n\n### 4. Out-of-Bag (OOB) Error\n\nFor each observation i:\n1. Find all trees where i was NOT in bootstrap sample (~37% of trees)\n2. Predict i using only those trees\n3. Compare to true label\n\n$$OOB\\_Error = \\frac{1}{N} \\sum_{i=1}^{N} L(y_i, \\hat{y}_{OOB,i})$$\n\n**Advantage:** OOB error \u2248 cross-validation error, no need for separate validation set.\n\n### 5. Feature Importance\n\n**Method 1: Mean Decrease in Impurity (MDI)**\n\n$$Importance(feature\\_j) = \\frac{1}{K} \\sum_{k=1}^{K} \\sum_{t \\in Tree_k} \\Delta Impurity_t \\cdot \\mathbb{1}(split\\_feature = j)$$\n\n**Method 2: Permutation Importance**\n1. Compute OOB error\n2. Shuffle feature j in OOB samples\n3. Compute new OOB error\n4. Importance = increase in error\n\n**Advantage:** Permutation importance handles correlated features better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \ud83d\udd28 Implementation from Scratch\n\n### \ud83d\udcdd What's Happening in This Code?\n\n**Purpose:** Build Random Forest using our DecisionTreeRegressorScratch from notebook 016\n\n**Key Points:**\n- **Bootstrap sampling**: Use `np.random.choice` with `replace=True` for each tree\n- **Random features**: At each split, consider only random subset (max_features)\n- **Parallel trees**: Each tree trained independently (can parallelize)\n- **Prediction**: Average all tree predictions for final output\n- **OOB tracking**: Store which samples were out-of-bag for each tree\n\n**Why This Matters:** Understanding bootstrap + random features reveals how Random Forests achieve decorrelation and variance reduction. This is the foundation for all ensemble methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom copy import deepcopy\n\n# Reuse DecisionTreeRegressorScratch from 016\nclass Node:\n    def __init__(self, feature_idx=None, threshold=None, left=None, right=None, value=None):\n        self.feature_idx = feature_idx\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.value = value\n    \n    def is_leaf(self):\n        return self.value is not None\n\nclass DecisionTreeRegressorScratch:\n    def __init__(self, max_depth=5, min_samples_split=2, min_samples_leaf=1, max_features=None):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.max_features = max_features\n        self.root = None\n    \n    def fit(self, X, y):\n        X = np.array(X)\n        y = np.array(y)\n        self.n_features = X.shape[1]\n        if self.max_features is None:\n            self.max_features = self.n_features\n        self.root = self._build_tree(X, y, depth=0)\n        return self\n    \n    def _build_tree(self, X, y, depth):\n        n_samples, n_features = X.shape\n        \n        if (depth >= self.max_depth or n_samples < self.min_samples_split or len(np.unique(y)) == 1):\n            return Node(value=np.mean(y))\n        \n        best_feature, best_threshold = self._find_best_split(X, y)\n        \n        if best_feature is None:\n            return Node(value=np.mean(y))\n        \n        left_mask = X[:, best_feature] <= best_threshold\n        right_mask = ~left_mask\n        \n        left_child = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n        right_child = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n        \n        return Node(feature_idx=best_feature, threshold=best_threshold,\n                    left=left_child, right=right_child)\n    \n    def _find_best_split(self, X, y):\n        n_samples, n_features = X.shape\n        \n        # Random feature selection for Random Forest\n        feature_indices = np.random.choice(n_features, self.max_features, replace=False)\n        \n        best_rss = float('inf')\n        best_feature = None\n        best_threshold = None\n        \n        for feature_idx in feature_indices:\n            thresholds = np.unique(X[:, feature_idx])\n            \n            for threshold in thresholds:\n                left_mask = X[:, feature_idx] <= threshold\n                right_mask = ~left_mask\n                \n                if (np.sum(left_mask) < self.min_samples_leaf or \n                    np.sum(right_mask) < self.min_samples_leaf):\n                    continue\n                \n                y_left = y[left_mask]\n                y_right = y[right_mask]\n                rss = self._calculate_rss(y_left, y_right)\n                \n                if rss < best_rss:\n                    best_rss = rss\n                    best_feature = feature_idx\n                    best_threshold = threshold\n        \n        return best_feature, best_threshold\n    \n    def _calculate_rss(self, y_left, y_right):\n        rss_left = np.sum((y_left - np.mean(y_left))**2) if len(y_left) > 0 else 0\n        rss_right = np.sum((y_right - np.mean(y_right))**2) if len(y_right) > 0 else 0\n        return rss_left + rss_right\n    \n    def predict(self, X):\n        X = np.array(X)\n        return np.array([self._predict_single(x, self.root) for x in X])\n    \n    def _predict_single(self, x, node):\n        if node.is_leaf():\n            return node.value\n        \n        if x[node.feature_idx] <= node.threshold:\n            return self._predict_single(x, node.left)\n        else:\n            return self._predict_single(x, node.right)\n\nprint('\u2705 DecisionTreeRegressorScratch with max_features implemented')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomForestRegressorScratch:\n    \"\"\"Random Forest from scratch using bootstrap + random features\"\"\"\n    \n    def __init__(self, n_estimators=100, max_depth=10, min_samples_split=2, \n                 max_features='sqrt', random_state=None):\n        self.n_estimators = n_estimators\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.max_features = max_features\n        self.random_state = random_state\n        self.trees = []\n        self.oob_indices = []  # Track OOB samples for each tree\n    \n    def fit(self, X, y):\n        \"\"\"Train forest of trees with bootstrap sampling\"\"\"\n        if self.random_state is not None:\n            np.random.seed(self.random_state)\n        \n        X = np.array(X)\n        y = np.array(y)\n        n_samples, n_features = X.shape\n        \n        # Determine max_features\n        if self.max_features == 'sqrt':\n            max_features = int(np.sqrt(n_features))\n        elif self.max_features == 'log2':\n            max_features = int(np.log2(n_features))\n        elif isinstance(self.max_features, int):\n            max_features = self.max_features\n        else:  # Default to p/3 for regression\n            max_features = max(1, n_features // 3)\n        \n        # Build each tree\n        for i in range(self.n_estimators):\n            # Bootstrap sample\n            bootstrap_indices = np.random.choice(n_samples, n_samples, replace=True)\n            X_bootstrap = X[bootstrap_indices]\n            y_bootstrap = y[bootstrap_indices]\n            \n            # Track OOB indices\n            oob_mask = np.ones(n_samples, dtype=bool)\n            oob_mask[bootstrap_indices] = False\n            self.oob_indices.append(np.where(oob_mask)[0])\n            \n            # Train tree\n            tree = DecisionTreeRegressorScratch(\n                max_depth=self.max_depth,\n                min_samples_split=self.min_samples_split,\n                max_features=max_features\n            )\n            tree.fit(X_bootstrap, y_bootstrap)\n            self.trees.append(tree)\n        \n        return self\n    \n    def predict(self, X):\n        \"\"\"Average predictions from all trees\"\"\"\n        X = np.array(X)\n        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n        return np.mean(tree_predictions, axis=0)\n    \n    def compute_oob_score(self, X, y):\n        \"\"\"Compute out-of-bag R\u00b2 score\"\"\"\n        X = np.array(X)\n        y = np.array(y)\n        n_samples = len(y)\n        oob_predictions = np.zeros(n_samples)\n        oob_counts = np.zeros(n_samples)\n        \n        # For each tree, predict its OOB samples\n        for tree, oob_idx in zip(self.trees, self.oob_indices):\n            if len(oob_idx) > 0:\n                oob_predictions[oob_idx] += tree.predict(X[oob_idx])\n                oob_counts[oob_idx] += 1\n        \n        # Average OOB predictions\n        mask = oob_counts > 0\n        oob_predictions[mask] /= oob_counts[mask]\n        \n        # Compute R\u00b2\n        ss_res = np.sum((y[mask] - oob_predictions[mask])**2)\n        ss_tot = np.sum((y[mask] - np.mean(y[mask]))**2)\n        oob_r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n        \n        return oob_r2\n\nprint('\u2705 RandomForestRegressorScratch implemented')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### \ud83d\udcdd What's Happening in This Code?\n\n**Purpose:** Generate noisy non-linear data to demonstrate Random Forest variance reduction\n\n**Key Points:**\n- **Non-linear function**: $y = sin(2\\pi x) + noise$ (challenging for linear models)\n- **High noise**: Substantial random noise tests ensemble robustness\n- **Single tree weakness**: Individual tree will overfit noise\n- **Random Forest strength**: Averaging smooths out overfitting\n\n**Why This Matters:** Sine function with noise is perfect test case - single tree will create jagged staircase, but Random Forest averages to smooth curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate non-linear data with noise\nnp.random.seed(42)\nX_train = np.random.uniform(0, 1, size=(200, 1))\ny_train = np.sin(2 * np.pi * X_train[:, 0]) + np.random.normal(0, 0.3, 200)\n\nX_test = np.random.uniform(0, 1, size=(50, 1))\ny_test = np.sin(2 * np.pi * X_test[:, 0]) + np.random.normal(0, 0.3, 50)\n\nprint('Training data generated:')\nprint(f'  Train samples: {len(X_train)}')\nprint(f'  Test samples: {len(X_test)}')\nprint(f'  True function: y = sin(2\u03c0x) + noise')\nprint(f'  Noise std: 0.3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### \ud83d\udcdd What's Happening in This Code?\n\n**Purpose:** Train single tree vs Random Forest to demonstrate variance reduction\n\n**Key Points:**\n- **Single tree**: max_depth=10 (intentionally deep to overfit)\n- **Random Forest**: 50 trees with max_depth=10 (each overfits, but average is smooth)\n- **max_features**: sqrt(1) = 1 (only 1 feature, so all trees use same feature but different splits)\n- **OOB score**: Built-in validation without separate test set\n\n**Why This Matters:** Demonstrates core Random Forest principle - averaging high-variance models produces low-variance ensemble (wisdom of the crowd)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train single deep tree (high variance)\nsingle_tree = DecisionTreeRegressorScratch(max_depth=10, min_samples_split=2)\nsingle_tree.fit(X_train, y_train)\n\n# Train Random Forest (variance reduction via averaging)\nrf_scratch = RandomForestRegressorScratch(\n    n_estimators=50,\n    max_depth=10,\n    min_samples_split=2,\n    max_features='sqrt',\n    random_state=42\n)\nrf_scratch.fit(X_train, y_train)\n\n# Predictions\ny_train_pred_tree = single_tree.predict(X_train)\ny_test_pred_tree = single_tree.predict(X_test)\n\ny_train_pred_rf = rf_scratch.predict(X_train)\ny_test_pred_rf = rf_scratch.predict(X_test)\n\n# Metrics\ntrain_mse_tree = np.mean((y_train - y_train_pred_tree)**2)\ntest_mse_tree = np.mean((y_test - y_test_pred_tree)**2)\n\ntrain_mse_rf = np.mean((y_train - y_train_pred_rf)**2)\ntest_mse_rf = np.mean((y_test - y_test_pred_rf)**2)\n\n# OOB score\noob_r2 = rf_scratch.compute_oob_score(X_train, y_train)\n\nprint('Performance Comparison:')\nprint('\\nSingle Tree (depth=10):')\nprint(f'  Train MSE: {train_mse_tree:.4f}')\nprint(f'  Test MSE: {test_mse_tree:.4f}')\nprint(f'  Overfit ratio: {test_mse_tree / train_mse_tree:.2f}x')\n\nprint('\\nRandom Forest (50 trees, depth=10):')\nprint(f'  Train MSE: {train_mse_rf:.4f}')\nprint(f'  Test MSE: {test_mse_rf:.4f}')\nprint(f'  OOB R\u00b2: {oob_r2:.4f}')\nprint(f'  Overfit ratio: {test_mse_rf / train_mse_rf:.2f}x')\n\nprint(f'\\n\u2705 Random Forest reduces test MSE by {(1 - test_mse_rf/test_mse_tree)*100:.1f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### \ud83d\udcdd What's Happening in This Code?\n\n**Purpose:** Visualize how Random Forest smooths jagged single-tree predictions\n\n**Key Points:**\n- **True function**: Smooth sine wave (dashed black line)\n- **Single tree**: Jagged staircase overfits training noise (red)\n- **Random Forest**: Smooth curve follows true function (blue)\n- **Variance reduction**: Forest averages out individual tree mistakes\n\n**Why This Matters:** Visual proof that ensemble averaging produces smoother, more generalizable predictions than any single model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization\nX_plot = np.linspace(0, 1, 300).reshape(-1, 1)\ny_true = np.sin(2 * np.pi * X_plot[:, 0])\ny_pred_tree = single_tree.predict(X_plot)\ny_pred_rf = rf_scratch.predict(X_plot)\n\nplt.figure(figsize=(12, 5))\n\n# Single tree\nplt.subplot(1, 2, 1)\nplt.scatter(X_train, y_train, alpha=0.3, s=30, label='Training data')\nplt.plot(X_plot, y_true, 'k--', linewidth=2, label='True function')\nplt.plot(X_plot, y_pred_tree, 'r-', linewidth=2, label='Single tree')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(f'Single Tree (Test MSE: {test_mse_tree:.3f})')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Random Forest\nplt.subplot(1, 2, 2)\nplt.scatter(X_train, y_train, alpha=0.3, s=30, label='Training data')\nplt.plot(X_plot, y_true, 'k--', linewidth=2, label='True function')\nplt.plot(X_plot, y_pred_rf, 'b-', linewidth=2, label='Random Forest (50 trees)')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title(f'Random Forest (Test MSE: {test_mse_rf:.3f})')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint('\\n\u2705 Batch 1 Complete:')\nprint('  - Random Forest theory (bagging, bootstrap, random features)')\nprint('  - From-scratch implementation with OOB scoring')\nprint('  - Variance reduction demonstration (single tree vs forest)')\nprint('  - Visual confirmation: smooth forest vs jagged tree')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \ud83c\udfed Production Implementation: Sklearn RandomForestRegressor\n\n### \ud83d\udcdd What's Happening in This Code?\n\n**Purpose:** Use sklearn's optimized Random Forest with advanced features\n\n**Key Points:**\n- **sklearn.ensemble.RandomForestRegressor**: C-optimized implementation (100x faster)\n- **Parallel training**: n_jobs=-1 uses all CPU cores\n- **OOB built-in**: oob_score=True automatically computes OOB R\u00b2\n- **Feature importance**: Aggregated from all trees (mean decrease in impurity)\n- **Warm start**: Can add more trees incrementally without retraining\n\n**Why This Matters:** Production Random Forests handle massive datasets with parallel processing and provide extensive diagnostics for model interpretation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Train sklearn Random Forest\nrf_sklearn = RandomForestRegressor(\n    n_estimators=100,\n    max_depth=10,\n    min_samples_split=2,\n    max_features='sqrt',\n    oob_score=True,\n    n_jobs=-1,  # Use all CPU cores\n    random_state=42\n)\nrf_sklearn.fit(X_train, y_train)\n\n# Predictions\ny_train_pred_sklearn = rf_sklearn.predict(X_train)\ny_test_pred_sklearn = rf_sklearn.predict(X_test)\n\n# Metrics\ntrain_mse_sklearn = mean_squared_error(y_train, y_train_pred_sklearn)\ntest_mse_sklearn = mean_squared_error(y_test, y_test_pred_sklearn)\ntest_r2_sklearn = r2_score(y_test, y_test_pred_sklearn)\noob_score_sklearn = rf_sklearn.oob_score_\n\nprint('Sklearn RandomForestRegressor (100 trees):')\nprint(f'  Train MSE: {train_mse_sklearn:.4f}')\nprint(f'  Test MSE: {test_mse_sklearn:.4f}')\nprint(f'  Test R\u00b2: {test_r2_sklearn:.4f}')\nprint(f'  OOB Score (R\u00b2): {oob_score_sklearn:.4f}')\n\nprint(f'\\nComparison with from-scratch:')\nprint(f'  From-scratch (50 trees): MSE = {test_mse_rf:.4f}')\nprint(f'  Sklearn (100 trees): MSE = {test_mse_sklearn:.4f}')\nprint(f'  Improvement: {(test_mse_rf - test_mse_sklearn)/test_mse_rf*100:.1f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### \ud83d\udcdd What's Happening in This Code?\n\n**Purpose:** Demonstrate impact of n_estimators on performance\n\n**Key Points:**\n- **n_estimators**: More trees \u2192 lower variance, but diminishing returns\n- **OOB tracking**: Monitor OOB score to detect overfitting (or lack thereof)\n- **Train vs OOB vs Test**: OOB closely tracks test performance\n- **Convergence**: Typically plateau around 100-500 trees\n\n**Why This Matters:** Shows Random Forests rarely overfit - more trees almost always help or plateau (unlike single trees which overfit with more depth)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Study impact of n_estimators\nn_trees_range = [1, 5, 10, 25, 50, 100, 200]\nresults = []\n\nfor n_trees in n_trees_range:\n    rf = RandomForestRegressor(\n        n_estimators=n_trees,\n        max_depth=10,\n        max_features='sqrt',\n        oob_score=True,\n        random_state=42\n    )\n    rf.fit(X_train, y_train)\n    \n    train_mse = mean_squared_error(y_train, rf.predict(X_train))\n    test_mse = mean_squared_error(y_test, rf.predict(X_test))\n    oob_r2 = rf.oob_score_\n    \n    results.append({\n        'n_trees': n_trees,\n        'train_mse': train_mse,\n        'test_mse': test_mse,\n        'oob_r2': oob_r2\n    })\n\nresults_df = pd.DataFrame(results)\nprint(results_df.to_string(index=False))\n\n# Plot convergence\nplt.figure(figsize=(10, 5))\nplt.plot(results_df.n_trees, results_df.train_mse, 'b-o', label='Train MSE')\nplt.plot(results_df.n_trees, results_df.test_mse, 'r-s', label='Test MSE')\nplt.axhline(test_mse_tree, color='gray', linestyle='--', label='Single tree baseline')\nplt.xlabel('Number of Trees')\nplt.ylabel('MSE')\nplt.title('Random Forest Convergence')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xscale('log')\nplt.tight_layout()\nplt.show()\n\nprint(f'\\n\u2705 Performance plateaus around {results_df.loc[results_df.test_mse.idxmin(), \"n_trees\"]:.0f} trees')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \ud83d\udd2c Post-Silicon Application: Robust Multi-Parameter Yield Prediction\n\n### \ud83d\udcdd What's Happening in This Code?\n\n**Purpose:** Predict device yield from noisy multi-parameter test data\n\n**Key Points:**\n- **10 test parameters**: Voltage, current, frequency, temperature, power, leakage, etc.\n- **Complex interactions**: Yield depends on V\u00d7F, V\u00d7Temp, and other non-linear combinations\n- **Noisy data**: Real test data has measurement errors and outliers\n- **Random Forest advantage**: Robust to noise, captures interactions automatically\n\n**Why This Matters:** Production test data is messy - Random Forests handle noise and missing values better than single trees or linear models, making them ideal for real-world semiconductor yield prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic multi-parameter yield data\nnp.random.seed(42)\nn_devices = 1000\n\n# 10 test parameters\nvoltage = np.random.uniform(0.95, 1.05, n_devices)\ncurrent = np.random.uniform(0.8, 1.2, n_devices)\nfrequency = np.random.uniform(450, 550, n_devices)\ntemperature = np.random.uniform(25, 85, n_devices)\npower = voltage * current * frequency / 100 + np.random.normal(0, 0.5, n_devices)\nleakage = np.random.exponential(scale=10, size=n_devices)\ndelay = np.random.uniform(5, 15, n_devices)\nnoise_margin = np.random.uniform(0.1, 0.3, n_devices)\njitter = np.random.uniform(10, 50, n_devices)\nskew = np.random.uniform(-5, 5, n_devices)\n\n# Yield depends on complex interactions\n# Good devices: low power, high frequency, low leakage, good margins\nyield_score = (\n    100 +\n    10 * (frequency - 500) / 50 +  # Higher freq \u2192 higher yield\n    -15 * (power - 5) / 2 +        # Higher power \u2192 lower yield\n    -8 * (leakage - 10) / 10 +     # Higher leakage \u2192 lower yield\n    5 * (noise_margin - 0.2) / 0.1 +  # Better margin \u2192 higher yield\n    -3 * (temperature - 55) / 30 +  # Higher temp \u2192 lower yield\n    np.random.normal(0, 5, n_devices)  # Measurement noise\n)\n\n# Convert to binary yield (pass/fail threshold at 95%)\nyield_binary = (yield_score > 95).astype(int)\n\n# Create DataFrame\nyield_data = pd.DataFrame({\n    'Voltage': voltage,\n    'Current': current,\n    'Frequency': frequency,\n    'Temperature': temperature,\n    'Power': power,\n    'Leakage': leakage,\n    'Delay': delay,\n    'Noise_Margin': noise_margin,\n    'Jitter': jitter,\n    'Skew': skew,\n    'Yield': yield_binary\n})\n\nprint('Multi-Parameter Yield Data:')\nprint(yield_data.head(10))\nprint(f'\\nDataset: {len(yield_data)} devices')\nprint(f'Yield rate: {yield_binary.mean()*100:.1f}%')\nprint(f'Features: {yield_data.shape[1]-1} test parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n### \ud83d\udcdd What's Happening in This Code?\n\n**Purpose:** Train Random Forest classifier for binary yield prediction\n\n**Key Points:**\n- **RandomForestClassifier**: For binary classification (pass/fail)\n- **Class imbalance**: Yield rate ~60-80% typical (handle via class_weight if needed)\n- **Feature importance**: Reveals which parameters drive yield\n- **Accuracy + AUC**: Both metrics important for production deployment\n\n**Why This Matters:** Feature importance guides test optimization - focus on parameters that strongly predict yield, reduce testing of irrelevant parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n\n# Split data\nX_yield = yield_data.drop('Yield', axis=1).values\ny_yield = yield_data['Yield'].values\nX_yield_train, X_yield_test, y_yield_train, y_yield_test = train_test_split(\n    X_yield, y_yield, test_size=0.2, random_state=42, stratify=y_yield\n)\n\n# Train Random Forest classifier\nrf_yield = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=15,\n    min_samples_split=10,\n    max_features='sqrt',\n    oob_score=True,\n    n_jobs=-1,\n    random_state=42\n)\nrf_yield.fit(X_yield_train, y_yield_train)\n\n# Predictions\ny_yield_pred = rf_yield.predict(X_yield_test)\ny_yield_proba = rf_yield.predict_proba(X_yield_test)[:, 1]\n\n# Metrics\naccuracy = accuracy_score(y_yield_test, y_yield_pred)\nauc = roc_auc_score(y_yield_test, y_yield_proba)\noob_score = rf_yield.oob_score_\n\nprint('Yield Prediction Model Performance:')\nprint(f'  Accuracy: {accuracy:.3f}')\nprint(f'  AUC-ROC: {auc:.3f}')\nprint(f'  OOB Score: {oob_score:.3f}')\nprint('\\nClassification Report:')\nprint(classification_report(y_yield_test, y_yield_pred, target_names=['Fail', 'Pass']))\n\n# Feature importance\nfeature_names = yield_data.columns[:-1]\nimportances = rf_yield.feature_importances_\nindices = np.argsort(importances)[::-1]\n\nprint('\\nFeature Importance (Top 5):')\nfor i in range(5):\n    print(f'  {feature_names[indices[i]]}: {importances[indices[i]]:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize feature importance\nplt.figure(figsize=(10, 6))\nplt.barh(range(len(importances)), importances[indices], color='skyblue')\nplt.yticks(range(len(importances)), [feature_names[i] for i in indices])\nplt.xlabel('Feature Importance')\nplt.title('Random Forest Feature Importance for Yield Prediction')\nplt.tight_layout()\nplt.show()\n\nprint('\\n\u2705 Feature importance reveals:')\nprint('  - Which parameters most strongly predict yield')\nprint('  - Candidates for test time reduction (skip low-importance features)')\nprint('  - Process optimization targets (improve high-importance parameters)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \ud83c\udfaf Real-World Project Ideas\n\n### Post-Silicon Validation Projects (4)\n\n#### 1. **Robust Multi-Failure Mode Detection**\n**Objective:** Multi-class classification of failure modes with noisy test data.\n\n**Business Value:** Identify root causes faster (voltage fail vs current fail vs timing fail). Reduces debug time by 40-60% compared to manual analysis.\n\n**Key Features:** All parametric tests, wafer coordinates, lot info, temperature, voltage corners\n\n**Implementation:** RandomForestClassifier with 200-500 trees, tune max_depth (10-20), use permutation importance to identify diagnostic tests\n\n**Success Metric:** 90% failure mode classification accuracy, <5% test time increase\n\n---\n\n#### 2. **Test Importance Ranking for Flow Optimization**\n**Objective:** Rank tests by importance, eliminate redundant/low-value tests.\n\n**Business Value:** Reduce test time 20-35% while maintaining yield coverage. $500K-$2M annual savings per product.\n\n**Key Features:** Historical test results (all parameters), final yield outcomes\n\n**Implementation:** Train RF on full test suite, use feature importance + permutation importance to rank tests, validate removal of bottom 20% on holdout data\n\n**Success Metric:** 25% test time reduction, <0.5% yield escape rate increase\n\n---\n\n#### 3. **Wafer-Level Spatial Pattern Classification**\n**Objective:** Classify wafer defects (edge fail, center fail, random, systematic).\n\n**Business Value:** Automated root cause analysis for yield improvement. Edge fails \u2192 etch issue, center fails \u2192 deposition uniformity, random \u2192 particles.\n\n**Key Features:** Die coordinates (x, y), parametric values, neighbors' results, radial distance\n\n**Implementation:** Random Forest with spatial features, visualize decision boundaries on wafer maps\n\n**Success Metric:** 85% pattern classification accuracy, actionable insights for process engineers\n\n---\n\n#### 4. **Robust Yield Prediction with Missing Data**\n**Objective:** Predict final test yield from incomplete wafer test data.\n\n**Business Value:** Early yield prediction enables production planning. Random Forests handle missing values naturally (surrogate splits).\n\n**Key Features:** Wafer test parameters (10-30% missing randomly), lot characteristics\n\n**Implementation:** RF trained on complete data, evaluate on data with artificial missingness\n\n**Success Metric:** <5% RMSE degradation with 30% missing data vs complete data\n\n---\n\n### General AI/ML Projects (4)\n\n#### 5. **Credit Scoring with Noisy Data**\n**Objective:** Predict loan default from applicant data with missing values and outliers.\n\n**Business Value:** Random Forests robust to data quality issues, no imputation needed. Reduces default rate by 15-20% vs simpler models.\n\n**Key Features:** Income, credit history, employment, debt-to-income, age, location\n\n**Success Metric:** AUC > 0.80, handles 20% missing data without imputation\n\n---\n\n#### 6. **Medical Diagnosis Ensemble**\n**Objective:** Diagnose disease from symptoms and test results using ensemble voting.\n\n**Business Value:** Ensemble reduces misdiagnosis risk. Feature importance reveals most diagnostic symptoms for triage.\n\n**Key Features:** Symptoms (binary), vital signs, lab tests, demographics, medical history\n\n**Success Metric:** 90% diagnostic accuracy, interpretable feature importance for clinicians\n\n---\n\n#### 7. **Customer Churn Prediction**\n**Objective:** Predict which customers will churn next quarter.\n\n**Business Value:** Target retention campaigns at high-risk customers. RF handles mixed data types (numerical usage + categorical plan type) without encoding.\n\n**Key Features:** Usage patterns, support tickets, billing amount, contract type, tenure\n\n**Success Metric:** 75% churn prediction accuracy, 20% reduction in churn via targeted campaigns\n\n---\n\n#### 8. **Kaggle Competition Baseline**\n**Objective:** Random Forest as strong baseline for any tabular data competition.\n\n**Business Value:** Minimal tuning, fast training, often top 10% performance out-of-box. Establishes performance floor for more complex models.\n\n**Key Features:** Any tabular data (mixed types, missing values, outliers)\n\n**Success Metric:** Achieve top 25% leaderboard with default hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n## \u2705 Key Takeaways\n\n### When to Use Random Forests\n\n| **Scenario** | **Random Forests** | **Single Decision Tree** | **XGBoost** |\n|-------------|-------------------|-------------------------|-------------|\n| **Robustness to noise** | \u2705 Excellent | \u274c Overfits | \u2705 Very good |\n| **Missing data handling** | \u2705 Built-in (surrogates) | \u2705 Yes | \u26a0\ufe0f Needs imputation |\n| **Training speed** | \u2705 Fast (parallel) | \u2705 Very fast | \u26a0\ufe0f Slower |\n| **Tuning difficulty** | \u2705 Minimal | \u26a0\ufe0f Easy to overfit | \u26a0\ufe0f Many hyperparameters |\n| **Interpretability** | \u26a0\ufe0f Feature importance only | \u2705 Full tree readable | \u26a0\ufe0f Feature importance only |\n| **Performance ceiling** | \u2705 High | \u274c Low (high variance) | \u2705 Highest |\n| **Overfitting risk** | \u2705 Low (averaging) | \u274c High | \u26a0\ufe0f Moderate |\n| **Production deployment** | \u2705 Stable, reliable | \u274c Unstable | \u2705 Best performance |\n\n### Best Practices\n\n1. **Hyperparameter tuning priorities:**\n   - **n_estimators**: 100-500 (more is almost always better, diminishing returns after 500)\n   - **max_features**: sqrt(p) for classification, p/3 for regression (default is good)\n   - **max_depth**: 10-30 (deeper than single tree, averaging prevents overfitting)\n   - **min_samples_split**: 2-20 (higher for very noisy data)\n   - **min_samples_leaf**: 1-10 (higher creates smoother predictions)\n\n2. **Feature engineering:**\n   - No scaling needed (tree-based)\n   - Handles categorical features with ordinal encoding\n   - Missing values handled automatically (surrogate splits)\n   - Interaction features helpful but not required (trees capture some interactions)\n\n3. **Overfitting prevention:**\n   - Use OOB score for validation (no separate validation set needed)\n   - More trees \u2192 lower variance (unlike single tree)\n   - Increase min_samples_split if overfitting persists\n   - Consider max_features < p for decorrelation\n\n4. **Production deployment:**\n   - Use n_jobs=-1 for parallel training (utilize all CPUs)\n   - Monitor feature importance drift (indicates data distribution changes)\n   - Warm start allows incremental tree addition without retraining\n   - Serialize with joblib.dump() for fast loading\n\n### Limitations\n\n- **Memory usage**: Stores all trees (can be large for 1000+ trees with deep depth)\n- **Prediction speed**: Slower than single tree (must query all trees)\n- **Extrapolation**: Cannot predict beyond training range (piecewise constant)\n- **Interpretability**: Less interpretable than single tree (black box ensemble)\n- **Imbalanced data**: May need class_weight='balanced' for minority class\n\n### Next Steps\n\n- **018_Gradient_Boosting.ipynb:** Sequential tree building (boosting vs bagging)\n- **019_XGBoost.ipynb:** High-performance gradient boosting with regularization\n- **020_LightGBM.ipynb:** Fast histogram-based boosting for large datasets\n- **Advanced:** Extremely randomized trees, isolation forests, Mondrian forests\n\n---\n\n## \ud83d\udcda References & Further Reading\n\n**Foundational Papers:**\n- Breiman (2001): Random Forests - Original paper introducing bagging + random features\n- Breiman (1996): Bagging Predictors - Bootstrap aggregating foundation\n\n**Sklearn Documentation:**\n- RandomForestRegressor: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n- RandomForestClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n- User guide: https://scikit-learn.org/stable/modules/ensemble.html#forest\n\n**Advanced Topics:**\n- Out-of-bag error estimation (built-in cross-validation)\n- Permutation importance (better for correlated features)\n- Partial dependence plots (visualize feature effects)\n- Proximity matrices (measure sample similarity)\n\n---\n\n**Notebook Complete!** \ud83c\udf89\n\nYou now understand:\n- \u2705 Random Forest theory (bagging, bootstrap, random features, variance reduction)\n- \u2705 From-scratch implementation with OOB scoring\n- \u2705 Production sklearn usage (parallel training, feature importance)\n- \u2705 Post-silicon applications (yield prediction, failure detection, test optimization)\n- \u2705 General AI/ML applications (credit scoring, medical diagnosis, churn prediction)\n- \u2705 Feature importance for interpretability\n- \u2705 Hyperparameter tuning (n_estimators, max_features, max_depth)\n- \u2705 8 real-world projects to practice\n\n**Next:** `018_Gradient_Boosting.ipynb` for sequential boosting (different from bagging)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}