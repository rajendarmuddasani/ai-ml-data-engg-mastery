{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 019: XGBoost (Extreme Gradient Boosting)\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "- **Understand** XGBoost's regularized objective and system optimizations\n",
        "- **Implement** XGBoost for classification and regression tasks\n",
        "- **Master** advanced features (early stopping, custom objectives, GPU acceleration)\n",
        "- **Apply** XGBoost to large-scale semiconductor data analytics\n",
        "- **Build** production-grade models with 95%+ accuracy and fast inference\n",
        "\n",
        "## üìö What is XGBoost?\n",
        "\n",
        "XGBoost is an optimized gradient boosting implementation with regularization, parallel processing, and advanced features. It's the go-to algorithm for winning Kaggle competitions and industry applications.\n",
        "\n",
        "**Why XGBoost?**\n",
        "- ‚úÖ Best-in-class accuracy (95-98% for many tasks)\n",
        "- ‚úÖ 10x faster than standard GBM (parallel tree construction)\n",
        "- ‚úÖ Built-in regularization (prevents overfitting)\n",
        "- ‚úÖ Handles missing values and sparse data efficiently\n",
        "\n",
        "## üè≠ Post-Silicon Validation Use Cases\n",
        "\n",
        "**High-Throughput Yield Classification**\n",
        "- Input: 1M+ test records, 200 features, streaming data\n",
        "- Output: Real-time pass/fail predictions (100K devices/hour)\n",
        "- Value: Enable adaptive testing, reduce costs $15M/year\n",
        "\n",
        "**Feature Importance Analysis**\n",
        "- Input: 500 parametric tests from wafer probe + final test\n",
        "- Output: XGBoost SHAP values identifying top 15 critical tests\n",
        "- Value: Eliminate 485 redundant tests, save 70% test time\n",
        "\n",
        "**Anomaly Detection at Scale**\n",
        "- Input: STDF files from 20 ATE testers, 24/7 operation\n",
        "- Output: XGBoost classifier flagging 0.1% anomalies (99.9% precision)\n",
        "- Value: Early detection of equipment drift, prevent 1-2% yield loss\n",
        "\n",
        "**Multi-Objective Optimization**\n",
        "- Input: Test cost, yield impact, coverage for 300 tests\n",
        "- Output: XGBoost ranking + genetic algorithm for optimal test suite\n",
        "- Value: 40% cost reduction, maintain 98% defect coverage\n",
        "\n",
        "---\n",
        "\n",
        "Let's master XGBoost! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 019: XGBoost - Extreme Gradient Boosting\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "- **Understand** XGBoost's regularized objective function with L1/L2 penalties\n",
        "- **Master** second-order gradient approximation (Newton's method)\n",
        "- **Implement** XGBoost API (native + sklearn wrapper) for production\n",
        "- **Apply** GPU acceleration and parallel tree building for scale\n",
        "- **Build** real-time adaptive test systems for semiconductor manufacturing\n",
        "\n",
        "## üìö What is XGBoost?\n",
        "\n",
        "**XGBoost** (Extreme Gradient Boosting) is an optimized implementation of gradient boosting with key innovations:\n",
        "\n",
        "**Regularized Objective:**\n",
        "$$\\mathcal{L} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\Omega(f_k)$$\n",
        "\n",
        "Where $\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\|\\mathbf{w}\\|^2$ penalizes tree complexity (T = leaves, w = leaf weights)\n",
        "\n",
        "**Why XGBoost?**\n",
        "- ‚úÖ Regularization prevents overfitting (L1/L2 on leaves)\n",
        "- ‚úÖ 10-100√ó faster than traditional GBM (parallelization)\n",
        "- ‚úÖ Handles missing values automatically (learn best split direction)\n",
        "- ‚úÖ Built-in CV and early stopping (production-ready)\n",
        "\n",
        "## üè≠ Post-Silicon Validation Use Cases\n",
        "\n",
        "**Real-Time Adaptive Test System**\n",
        "- Input: Streaming test results (1000+ devices/hour)\n",
        "- Output: Dynamic test sequence optimization (<50ms decisions)\n",
        "- Value: 30-40% test time reduction = $10-20M ATE savings\n",
        "\n",
        "**Multi-Site Equipment Correlation**\n",
        "- Input: Parametric data from 10+ test sites\n",
        "- Output: XGBoost model identifying site-specific drift patterns\n",
        "- Value: Early equipment failure detection ($5-15M prevention)\n",
        "\n",
        "**Parametric Outlier Detection at Scale**\n",
        "- Input: 500-1000 test parameters per device\n",
        "- Output: Real-time anomaly scores for marginal devices\n",
        "- Value: Field failure prevention (10√ó ROI on quality cost)\n",
        "\n",
        "**Wafer Map Pattern Classification**\n",
        "- Input: Spatial yield maps with complex defect signatures\n",
        "- Output: Multi-class pattern recognition (scratch, hotspot, edge)\n",
        "- Value: 3-5 day faster root cause identification\n",
        "\n",
        "## üîÑ XGBoost Workflow\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "    A[Data + DMatrix] --> B[Define Objective]\n",
        "    B --> C[Set Regularization]\n",
        "    C --> D[Parallel Tree Building]\n",
        "    D --> E[Early Stopping]\n",
        "    E --> F{Validation Score}\n",
        "    F -->|Improving| D\n",
        "    F -->|No Improvement| G[Best Model]\n",
        "    \n",
        "    style A fill:#e1f5ff\n",
        "    style G fill:#e1ffe1\n",
        "```\n",
        "\n",
        "## üìä Learning Path Context\n",
        "\n",
        "**Prerequisites:**\n",
        "- 018: Gradient Boosting (core algorithm)\n",
        "- 017: Random Forest (ensemble fundamentals)\n",
        "\n",
        "**Next Steps:**\n",
        "- 020: LightGBM (even faster gradient boosting)\n",
        "- 021: CatBoost (categorical feature handling)\n",
        "\n",
        "---\n",
        "\n",
        "Let's master XGBoost for production AI! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 019 - XGBoost (Extreme Gradient Boosting)\n",
        "\n",
        "## üéØ What You'll Learn\n",
        "\n",
        "**XGBoost** is an optimized and regularized implementation of gradient boosting that has dominated machine learning competitions (Kaggle) and production systems since 2014. It extends standard gradient boosting with advanced regularization, parallel tree construction, and efficient memory management.\n",
        "\n",
        "**Why XGBoost After Gradient Boosting?**\n",
        "- **Standard GBM**: Sequential tree building, no regularization, slow on large datasets\n",
        "- **XGBoost**: Parallel tree construction, L1/L2 regularization, sparsity-aware, GPU support\n",
        "- **Key innovation**: Second-order Taylor approximation for better accuracy + built-in regularization\n",
        "\n",
        "**Real-World Dominance:**\n",
        "- **Kaggle**: Winner of 17 out of 29 competitions in 2015 used XGBoost\n",
        "- **Post-Silicon**: 10-100x faster training on million-device STDF datasets\n",
        "- **Production**: Industry standard for structured data ML (credit scoring, fraud detection, ranking)\n",
        "- **Business**: Better accuracy than GBM with less overfitting, faster training\n",
        "\n",
        "**Learning Path:**\n",
        "1. Understand XGBoost innovations vs standard GBM\n",
        "2. Learn regularized objective with L1/L2 penalties\n",
        "3. Master XGBoost API and hyperparameter tuning\n",
        "4. Apply to post-silicon high-throughput analysis\n",
        "5. Deploy production models with GPU acceleration\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä XGBoost Workflow with Regularization\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[Training Data X, y] --> B[Initialize F0 = 0]\n",
        "    B --> C[Iteration m = 1 to M]\n",
        "    C --> D[Compute 1st & 2nd order gradients: g, h]\n",
        "    D --> E[Build tree with regularized objective]\n",
        "    E --> F[Add L1/L2 penalties on weights]\n",
        "    F --> G[Parallel leaf scoring across features]\n",
        "    G --> H[Update: F_m = F_m-1 + Œ∑¬∑tree_m]\n",
        "    H --> I{m < M OR early_stop?}\n",
        "    I -->|Continue| C\n",
        "    I -->|Stop| J[Final Model: F_M]\n",
        "    J --> K[Predict: ≈∑ = F_M X]\n",
        "    \n",
        "    style A fill:#e1f5ff\n",
        "    style J fill:#fff4e1\n",
        "    style K fill:#f0f0f0\n",
        "    style F fill:#ffe1e1\n",
        "```\n",
        "\n",
        "**Key Differences from GBM:**\n",
        "- **2nd order gradients** (Hessian) ‚Üí better convergence\n",
        "- **Regularization** (L1/L2) ‚Üí prevents overfitting\n",
        "- **Parallel tree construction** ‚Üí 10x faster\n",
        "- **Sparsity-aware** ‚Üí handles missing values natively\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßÆ Mathematical Foundation\n",
        "\n",
        "### Regularized Objective Function\n",
        "\n",
        "**Standard GBM objective:**\n",
        "$$\\mathcal{L} = \\sum_{i=1}^{n} L(y_i, \\hat{y}_i)$$\n",
        "\n",
        "**XGBoost regularized objective:**\n",
        "$$\\mathcal{L}_{XGB} = \\sum_{i=1}^{n} L(y_i, \\hat{y}_i) + \\sum_{m=1}^{M} \\Omega(f_m)$$\n",
        "\n",
        "Where the **regularization term** is:\n",
        "$$\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2 + \\alpha \\sum_{j=1}^{T} |w_j|$$\n",
        "\n",
        "- $T$: Number of leaves in tree\n",
        "- $w_j$: Weight (prediction value) in leaf $j$\n",
        "- $\\gamma$: Minimum loss reduction to create split (controls tree growth)\n",
        "- $\\lambda$: L2 regularization on leaf weights (ridge penalty)\n",
        "- $\\alpha$: L1 regularization on leaf weights (lasso penalty)\n",
        "\n",
        "### Second-Order Taylor Approximation\n",
        "\n",
        "At iteration $t$, approximate loss using Taylor expansion:\n",
        "\n",
        "$$\\mathcal{L}^{(t)} \\approx \\sum_{i=1}^{n} \\left[ L(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i) \\right] + \\Omega(f_t)$$\n",
        "\n",
        "Where:\n",
        "- **First-order gradient (g)**: $g_i = \\frac{\\partial L(y_i, \\hat{y}^{(t-1)})}{\\partial \\hat{y}^{(t-1)}}$\n",
        "- **Second-order gradient (h)**: $h_i = \\frac{\\partial^2 L(y_i, \\hat{y}^{(t-1)})}{\\partial (\\hat{y}^{(t-1)})^2}$\n",
        "\n",
        "For squared loss: $g_i = \\hat{y}_i - y_i$, $h_i = 1$\n",
        "\n",
        "### Optimal Leaf Weight and Gain\n",
        "\n",
        "For a leaf containing instances $I$, the **optimal weight** is:\n",
        "\n",
        "$$w^*_j = -\\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}$$\n",
        "\n",
        "The **gain** from splitting leaf $I$ into left $I_L$ and right $I_R$:\n",
        "\n",
        "$$\\text{Gain} = \\frac{1}{2} \\left[ \\frac{(\\sum_{i \\in I_L} g_i)^2}{\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{(\\sum_{i \\in I_R} g_i)^2}{\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{(\\sum_{i \\in I} g_i)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right] - \\gamma$$\n",
        "\n",
        "**Key insight:** $\\lambda$ in denominator shrinks weights ‚Üí less overfitting. $\\gamma$ prevents splits with small gain.\n",
        "\n",
        "---\n",
        "\n",
        "### XGBoost vs Standard GBM\n",
        "\n",
        "| Feature | Standard GBM | XGBoost |\n",
        "|---------|--------------|----------|\n",
        "| **Gradients** | 1st order only | 1st + 2nd order (Hessian) |\n",
        "| **Regularization** | None (only depth/samples) | L1 + L2 + gamma |\n",
        "| **Tree building** | Sequential | Parallel (level-wise) |\n",
        "| **Missing values** | Requires imputation | Native handling (learns best direction) |\n",
        "| **Sparsity** | Dense computation | Sparse-aware algorithms |\n",
        "| **Hardware** | CPU only | CPU + GPU support |\n",
        "| **Speed (1M samples)** | ~10 minutes | ~1 minute (10x faster) |\n",
        "| **Overfitting control** | Learning rate + early stop | + L1/L2/gamma regularization |\n",
        "\n",
        "---\n",
        "\n",
        "### Key Hyperparameters\n",
        "\n",
        "**Tree Structure (control overfitting):**\n",
        "- `max_depth` (3-10): Maximum tree depth\n",
        "- `min_child_weight` (1-10): Minimum sum of Hessian in leaf (higher = conservative)\n",
        "- `gamma` (0-5): Minimum loss reduction for split (higher = fewer splits)\n",
        "\n",
        "**Regularization:**\n",
        "- `lambda` (1-10): L2 regularization (ridge penalty on weights)\n",
        "- `alpha` (0-1): L1 regularization (lasso penalty, promotes sparsity)\n",
        "\n",
        "**Learning:**\n",
        "- `learning_rate` / `eta` (0.01-0.3): Step size (lower = more robust)\n",
        "- `n_estimators` (100-5000): Number of trees\n",
        "- `subsample` (0.5-1.0): Fraction of samples per tree\n",
        "- `colsample_bytree` (0.5-1.0): Fraction of features per tree\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Installation and Setup\n",
        "\n",
        "### üìù What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Install XGBoost library and verify installation.\n",
        "\n",
        "**Key Points:**\n",
        "- **XGBoost**: Separate library (not in sklearn by default)\n",
        "- **Installation**: `pip install xgboost` or `conda install xgboost`\n",
        "- **GPU support**: Requires CUDA toolkit for GPU acceleration (`pip install xgboost[gpu]`)\n",
        "- **Verification**: Import and check version\n",
        "\n",
        "**Why This Matters:** XGBoost is a standalone library with C++ backend, significantly faster than pure Python implementations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install XGBoost (uncomment if needed)\n",
        "# !pip install xgboost\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, roc_auc_score\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "\n",
        "print(f\"‚úÖ XGBoost version: {xgb.__version__}\")\n",
        "print(f\"   NumPy version: {np.__version__}\")\n",
        "print(f\"   Pandas version: {pd.__version__}\")\n",
        "print(f\"\\nüìä XGBoost ready for high-performance gradient boosting!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Basic XGBoost Regression Example\n",
        "\n",
        "### üìù What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Compare standard GBM vs XGBoost on a simple regression task.\n",
        "\n",
        "**Key Points:**\n",
        "- **XGBRegressor**: Drop-in replacement for sklearn's GradientBoostingRegressor\n",
        "- **Default params**: Already well-tuned (lambda=1, gamma=0, max_depth=6)\n",
        "- **Training speed**: Notice XGBoost is faster even with same n_estimators\n",
        "- **Accuracy**: Often better due to regularization preventing overfitting\n",
        "\n",
        "**Why This Matters:** XGBoost provides better accuracy with less hyperparameter tuning effort. Default parameters work well for most tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import time\n",
        "\n",
        "# Generate synthetic non-linear regression data\n",
        "np.random.seed(42)\n",
        "X, y = make_regression(n_samples=1000, n_features=10, n_informative=8, \n",
        "                       noise=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train sklearn GradientBoostingRegressor\n",
        "start_time = time.time()\n",
        "gbm_sklearn = GradientBoostingRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=4,\n",
        "    random_state=42\n",
        ")\n",
        "gbm_sklearn.fit(X_train, y_train)\n",
        "gbm_time = time.time() - start_time\n",
        "gbm_pred = gbm_sklearn.predict(X_test)\n",
        "gbm_mse = mean_squared_error(y_test, gbm_pred)\n",
        "gbm_r2 = r2_score(y_test, gbm_pred)\n",
        "\n",
        "# Train XGBoost\n",
        "start_time = time.time()\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=4,\n",
        "    random_state=42\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_time = time.time() - start_time\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "xgb_mse = mean_squared_error(y_test, xgb_pred)\n",
        "xgb_r2 = r2_score(y_test, xgb_pred)\n",
        "\n",
        "# Compare results\n",
        "print(\"üîç Comparison: Standard GBM vs XGBoost\\n\")\n",
        "print(\"Sklearn GradientBoostingRegressor:\")\n",
        "print(f\"  Training time: {gbm_time:.3f}s\")\n",
        "print(f\"  Test MSE: {gbm_mse:.2f}\")\n",
        "print(f\"  Test R¬≤:  {gbm_r2:.4f}\")\n",
        "\n",
        "print(\"\\nXGBoost (XGBRegressor):\")\n",
        "print(f\"  Training time: {xgb_time:.3f}s\")\n",
        "print(f\"  Test MSE: {xgb_mse:.2f}\")\n",
        "print(f\"  Test R¬≤:  {xgb_r2:.4f}\")\n",
        "\n",
        "print(f\"\\nüìä XGBoost Advantages:\")\n",
        "print(f\"   Speed improvement: {gbm_time / xgb_time:.1f}x faster\")\n",
        "print(f\"   Accuracy improvement: {((gbm_mse - xgb_mse) / gbm_mse * 100):.1f}% lower MSE\")\n",
        "print(f\"   (Due to: L2 regularization, 2nd order gradients, optimized tree construction)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìù What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Demonstrate the impact of XGBoost's L1/L2 regularization on preventing overfitting.\n",
        "\n",
        "**Key Points:**\n",
        "- **No regularization** (lambda=0, alpha=0): Overfits training data\n",
        "- **L2 regularization** (lambda=1-10): Shrinks leaf weights smoothly\n",
        "- **L1 regularization** (alpha=0.1-1): Promotes sparse solutions (some weights ‚Üí 0)\n",
        "- **Combined** (lambda + alpha): Best generalization for most tasks\n",
        "\n",
        "**Why This Matters:** Regularization is XGBoost's secret weapon. It allows using deeper trees (max_depth=6-10) without overfitting, unlike standard GBM which needs shallow trees (max_depth=3-5).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate data with noise to show overfitting tendency\n",
        "X, y = make_regression(n_samples=500, n_features=20, n_informative=10,\n",
        "                       noise=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Test different regularization settings\n",
        "models = {\n",
        "    'No regularization': {'reg_lambda': 0, 'reg_alpha': 0},\n",
        "    'L2 only (Œª=5)': {'reg_lambda': 5, 'reg_alpha': 0},\n",
        "    'L1 only (Œ±=0.5)': {'reg_lambda': 0, 'reg_alpha': 0.5},\n",
        "    'L1 + L2 (default)': {'reg_lambda': 1, 'reg_alpha': 0.1}\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, params in models.items():\n",
        "    model = XGBRegressor(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=6,  # Deeper tree to show overfitting risk\n",
        "        **params,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    train_pred = model.predict(X_train)\n",
        "    test_pred = model.predict(X_test)\n",
        "    \n",
        "    train_mse = mean_squared_error(y_train, train_pred)\n",
        "    test_mse = mean_squared_error(y_test, test_pred)\n",
        "    \n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Train MSE': train_mse,\n",
        "        'Test MSE': test_mse,\n",
        "        'Overfitting Gap': test_mse - train_mse\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"üî¨ Impact of Regularization on Overfitting:\\n\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(\"\\nüìà Key Observations:\")\n",
        "best_model = results_df.loc[results_df['Test MSE'].idxmin(), 'Model']\n",
        "print(f\"   ‚Ä¢ No regularization ‚Üí lowest train MSE, but worst test MSE (overfitting)\")\n",
        "print(f\"   ‚Ä¢ L2 regularization ‚Üí smooth weight shrinkage, balanced performance\")\n",
        "print(f\"   ‚Ä¢ L1 regularization ‚Üí sparse solutions, feature selection\")\n",
        "print(f\"   ‚Ä¢ Best: '{best_model}' (lowest test MSE)\")\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "x_pos = np.arange(len(results_df))\n",
        "width = 0.35\n",
        "ax.bar(x_pos - width/2, results_df['Train MSE'], width, label='Train MSE', alpha=0.8)\n",
        "ax.bar(x_pos + width/2, results_df['Test MSE'], width, label='Test MSE', alpha=0.8)\n",
        "ax.set_xlabel('Model Configuration', fontsize=12)\n",
        "ax.set_ylabel('MSE', fontsize=12)\n",
        "ax.set_title('Regularization Impact on Train vs Test Performance', fontsize=14)\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(results_df['Model'], rotation=15, ha='right')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Batch 1 Complete: XGBoost Foundations\n",
        "\n",
        "**What We've Built:**\n",
        "1. ‚úÖ **Conceptual understanding**: XGBoost = GBM + regularization + 2nd order gradients + parallel construction\n",
        "2. ‚úÖ **Mathematical foundation**: Regularized objective (L1/L2), second-order Taylor approximation, optimal leaf weights\n",
        "3. ‚úÖ **Installation and setup**: XGBoost library installation and verification\n",
        "4. ‚úÖ **Basic usage**: XGBRegressor API, comparison with sklearn GBM (speed + accuracy)\n",
        "5. ‚úÖ **Regularization demo**: Impact of lambda/alpha on overfitting prevention\n",
        "\n",
        "**Key Insights:**\n",
        "- **2nd order gradients** (Hessian) provide better approximation ‚Üí faster convergence\n",
        "- **L2 regularization** (lambda) shrinks weights smoothly ‚Üí less overfitting\n",
        "- **L1 regularization** (alpha) promotes sparsity ‚Üí automatic feature selection\n",
        "- **Parallel tree building** ‚Üí 10x faster than sequential GBM\n",
        "- **Default parameters** work well (lambda=1, alpha=0, max_depth=6)\n",
        "\n",
        "**Next (Batch 2):**\n",
        "- Advanced hyperparameter tuning (grid search, early stopping)\n",
        "- DMatrix format for maximum performance\n",
        "- Post-silicon application: High-throughput wafer-level analysis (1M+ devices)\n",
        "- Native API vs sklearn API (when to use each)\n",
        "- 8 real-world project templates\n",
        "- GPU acceleration for massive datasets\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° XGBoost Native API with DMatrix\n",
        "\n",
        "### üìù What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Use XGBoost's native DMatrix format for maximum performance on large datasets.\n",
        "\n",
        "**Key Points:**\n",
        "- **DMatrix**: XGBoost's internal data structure, optimized for speed and memory\n",
        "- **Native API** (`xgb.train`): Lower-level, more control, faster than sklearn wrapper\n",
        "- **Sklearn API** (`XGBRegressor`): Higher-level, easier, compatible with sklearn pipelines\n",
        "- **When to use DMatrix**: Datasets > 100K samples, need maximum speed, custom evaluation\n",
        "\n",
        "**Why This Matters:** DMatrix can be 2-5x faster than numpy arrays for large datasets. Essential for production systems processing millions of samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate larger dataset to show DMatrix benefits\n",
        "X, y = make_regression(n_samples=10000, n_features=50, n_informative=40,\n",
        "                       noise=15, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Method 1: Sklearn API (easier, slower)\n",
        "start_time = time.time()\n",
        "xgb_sklearn = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6)\n",
        "xgb_sklearn.fit(X_train, y_train)\n",
        "sklearn_time = time.time() - start_time\n",
        "sklearn_pred = xgb_sklearn.predict(X_test)\n",
        "sklearn_mse = mean_squared_error(y_test, sklearn_pred)\n",
        "\n",
        "# Method 2: Native API with DMatrix (faster, more control)\n",
        "start_time = time.time()\n",
        "\n",
        "# Create DMatrix objects\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters (dictionary format)\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 6,\n",
        "    'eta': 0.1,              # learning_rate\n",
        "    'lambda': 1,             # L2 regularization\n",
        "    'alpha': 0,              # L1 regularization\n",
        "    'subsample': 1.0,\n",
        "    'colsample_bytree': 1.0,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train with evaluation monitoring\n",
        "evallist = [(dtrain, 'train'), (dtest, 'test')]\n",
        "evals_result = {}\n",
        "bst = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    num_boost_round=100,\n",
        "    evals=evallist,\n",
        "    evals_result=evals_result,\n",
        "    verbose_eval=False\n",
        ")\n",
        "\n",
        "native_time = time.time() - start_time\n",
        "native_pred = bst.predict(dtest)\n",
        "native_mse = mean_squared_error(y_test, native_pred)\n",
        "\n",
        "# Compare\n",
        "print(\"‚ö° Native API vs Sklearn API Comparison:\\n\")\n",
        "print(f\"Sklearn API (XGBRegressor):\")\n",
        "print(f\"  Training time: {sklearn_time:.3f}s\")\n",
        "print(f\"  Test MSE: {sklearn_mse:.2f}\")\n",
        "\n",
        "print(f\"\\nNative API (xgb.train + DMatrix):\")\n",
        "print(f\"  Training time: {native_time:.3f}s\")\n",
        "print(f\"  Test MSE: {native_mse:.2f}\")\n",
        "\n",
        "print(f\"\\nüìä Native API Benefits:\")\n",
        "print(f\"   Speed improvement: {sklearn_time / native_time:.2f}x faster\")\n",
        "print(f\"   Memory efficient: DMatrix uses internal compression\")\n",
        "print(f\"   More control: Access to all parameters, custom objectives\")\n",
        "print(f\"\\nüí° Use sklearn API for: pipelines, grid search, familiarity\")\n",
        "print(f\"   Use native API for: production, large data, custom evaluation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìù What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Use early stopping to automatically find optimal n_estimators and prevent overfitting.\n",
        "\n",
        "**Key Points:**\n",
        "- **Early stopping**: Monitor validation metric, stop when no improvement for N rounds\n",
        "- **early_stopping_rounds**: Patience parameter (typical: 10-50)\n",
        "- **eval_metric**: What to optimize (rmse, mae, logloss, auc, etc.)\n",
        "- **Best iteration**: Model automatically uses the best iteration, not the last\n",
        "\n",
        "**Why This Matters:** Early stopping is the most effective way to prevent overfitting in XGBoost. Set n_estimators high (1000-10000), let early stopping find optimal point.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 1: Early stopping with sklearn API\n",
        "xgb_early = XGBRegressor(\n",
        "    n_estimators=1000,           # Set high, early stopping will find optimal\n",
        "    learning_rate=0.05,\n",
        "    max_depth=5,\n",
        "    early_stopping_rounds=20,    # Stop if no improvement for 20 rounds\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_early.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"üõë Early Stopping Results (Sklearn API):\\n\")\n",
        "print(f\"  n_estimators set: 1000\")\n",
        "print(f\"  Best iteration: {xgb_early.best_iteration}\")\n",
        "print(f\"  Trees used: {xgb_early.best_iteration + 1}\")\n",
        "print(f\"  Stopped early: {1000 - (xgb_early.best_iteration + 1)} rounds saved\")\n",
        "print(f\"  Test MSE: {mean_squared_error(y_test, xgb_early.predict(X_test)):.2f}\")\n",
        "\n",
        "# Method 2: Early stopping with native API (more flexible)\n",
        "bst_early = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    num_boost_round=1000,\n",
        "    evals=[(dtrain, 'train'), (dtest, 'test')],\n",
        "    early_stopping_rounds=20,\n",
        "    verbose_eval=False\n",
        ")\n",
        "\n",
        "print(f\"\\nüõë Early Stopping Results (Native API):\\n\")\n",
        "print(f\"  n_estimators set: 1000\")\n",
        "print(f\"  Best iteration: {bst_early.best_iteration}\")\n",
        "print(f\"  Trees used: {bst_early.best_iteration + 1}\")\n",
        "print(f\"  Best score: {bst_early.best_score:.4f}\")\n",
        "\n",
        "print(f\"\\nüí° Early Stopping Best Practices:\")\n",
        "print(f\"   ‚Ä¢ Set n_estimators = 1000-10000 (let early stopping decide)\")\n",
        "print(f\"   ‚Ä¢ Use early_stopping_rounds = 20-50 (patience)\")\n",
        "print(f\"   ‚Ä¢ Lower learning_rate requires more patience (50-100 rounds)\")\n",
        "print(f\"   ‚Ä¢ Always use separate validation set (not training set)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéõÔ∏è Hyperparameter Tuning Strategy\n",
        "\n",
        "### üìù What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Systematic hyperparameter tuning using grid search and cross-validation.\n",
        "\n",
        "**Key Points:**\n",
        "- **Stage 1**: Tune tree structure (max_depth, min_child_weight)\n",
        "- **Stage 2**: Tune regularization (lambda, alpha, gamma)\n",
        "- **Stage 3**: Tune sampling (subsample, colsample_bytree)\n",
        "- **Stage 4**: Fine-tune learning rate (lower + more trees)\n",
        "\n",
        "**Why This Matters:** XGBoost has many hyperparameters. Systematic tuning in stages prevents combinatorial explosion and finds good configurations efficiently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Stage 1: Tree structure\n",
        "print(\"üîß Stage 1: Tuning tree structure (max_depth, min_child_weight)\\n\")\n",
        "\n",
        "xgb_base = XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "param_grid_stage1 = {\n",
        "    'max_depth': [3, 4, 5, 6, 7],\n",
        "    'min_child_weight': [1, 3, 5]\n",
        "}\n",
        "\n",
        "grid_search_stage1 = GridSearchCV(\n",
        "    xgb_base,\n",
        "    param_grid_stage1,\n",
        "    cv=3,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "grid_search_stage1.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters: {grid_search_stage1.best_params_}\")\n",
        "print(f\"Best CV score: {-grid_search_stage1.best_score_:.2f}\")\n",
        "\n",
        "# Stage 2: Regularization (using best params from stage 1)\n",
        "print(f\"\\nüîß Stage 2: Tuning regularization (lambda, alpha, gamma)\\n\")\n",
        "\n",
        "xgb_stage2 = XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=grid_search_stage1.best_params_['max_depth'],\n",
        "    min_child_weight=grid_search_stage1.best_params_['min_child_weight'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "param_grid_stage2 = {\n",
        "    'reg_lambda': [0.1, 1, 5, 10],\n",
        "    'reg_alpha': [0, 0.1, 0.5, 1],\n",
        "    'gamma': [0, 0.1, 0.5, 1]\n",
        "}\n",
        "\n",
        "grid_search_stage2 = GridSearchCV(\n",
        "    xgb_stage2,\n",
        "    param_grid_stage2,\n",
        "    cv=3,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "grid_search_stage2.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters: {grid_search_stage2.best_params_}\")\n",
        "print(f\"Best CV score: {-grid_search_stage2.best_score_:.2f}\")\n",
        "\n",
        "# Final model with best hyperparameters\n",
        "xgb_tuned = XGBRegressor(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=grid_search_stage1.best_params_['max_depth'],\n",
        "    min_child_weight=grid_search_stage1.best_params_['min_child_weight'],\n",
        "    reg_lambda=grid_search_stage2.best_params_['reg_lambda'],\n",
        "    reg_alpha=grid_search_stage2.best_params_['reg_alpha'],\n",
        "    gamma=grid_search_stage2.best_params_['gamma'],\n",
        "    early_stopping_rounds=30,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_tuned.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "tuned_pred = xgb_tuned.predict(X_test)\n",
        "tuned_mse = mean_squared_error(y_test, tuned_pred)\n",
        "tuned_r2 = r2_score(y_test, tuned_pred)\n",
        "\n",
        "print(f\"\\nüéØ Final Tuned Model Performance:\")\n",
        "print(f\"  Test MSE: {tuned_mse:.2f}\")\n",
        "print(f\"  Test R¬≤:  {tuned_r2:.4f}\")\n",
        "print(f\"  Trees used: {xgb_tuned.best_iteration + 1} (stopped early from 500)\")\n",
        "\n",
        "print(f\"\\nüìä Tuning Process Summary:\")\n",
        "print(f\"   Stage 1 (tree structure): 15 combinations tested\")\n",
        "print(f\"   Stage 2 (regularization): 64 combinations tested\")\n",
        "print(f\"   Total: 79 models evaluated via cross-validation\")\n",
        "print(f\"   Result: {tuned_mse:.2f} MSE (improved from default)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¨ Post-Silicon Application: High-Throughput Wafer Analysis\n",
        "\n",
        "### üìù What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Predict device yield across 1 million devices using XGBoost for production-scale analysis.\n",
        "\n",
        "**Key Points:**\n",
        "- **Scale**: 1M devices √ó 30 parametric tests = 30M data points\n",
        "- **Business problem**: Identify low-yield devices early in production ‚Üí scrap before expensive packaging\n",
        "- **XGBoost advantage**: Handles 1M samples in ~2 minutes (vs 20+ minutes for sklearn GBM)\n",
        "- **Features**: Parametric tests + spatial data (wafer_id, die_x, die_y) + process metadata\n",
        "- **Target**: Binary yield (pass/fail) at final test\n",
        "\n",
        "**Business Value:**\n",
        "- **Cost savings**: $0.50/device packaging cost √ó 100K failing devices caught early = $50K per lot\n",
        "- **Throughput**: Real-time yield prediction enables dynamic test flow adjustment\n",
        "- **Quality**: 95%+ prediction accuracy ‚Üí reliable early screening\n",
        "\n",
        "**Why This Matters:** This is a real production use case. Semiconductor manufacturers use XGBoost for yield prediction, test time optimization, and adaptive testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate realistic large-scale semiconductor dataset\n",
        "print(\"üè≠ Generating high-throughput wafer dataset...\\n\")\n",
        "\n",
        "np.random.seed(42)\n",
        "n_devices = 100000  # 100K devices (scaled down from 1M for demo speed)\n",
        "\n",
        "# Parametric test results (30 features)\n",
        "voltage_vdd = np.random.normal(1.8, 0.04, n_devices)\n",
        "current_idd = np.random.normal(150, 15, n_devices)\n",
        "frequency_max = np.random.normal(2000, 80, n_devices)\n",
        "temperature = np.random.uniform(25, 85, n_devices)\n",
        "power = voltage_vdd * current_idd\n",
        "leakage = np.random.exponential(8, n_devices)\n",
        "delay_prop = np.random.normal(500, 40, n_devices)\n",
        "jitter_clk = np.random.exponential(18, n_devices)\n",
        "noise_margin = np.random.normal(0.3, 0.05, n_devices)\n",
        "skew = np.random.normal(0, 15, n_devices)\n",
        "\n",
        "# Add 20 more test parameters for realism\n",
        "for i in range(20):\n",
        "    globals()[f'test_{i+11}'] = np.random.normal(100, 10, n_devices)\n",
        "\n",
        "# Spatial features (wafer map position)\n",
        "wafer_id = np.random.randint(1, 26, n_devices)  # 25 wafers\n",
        "die_x = np.random.randint(0, 50, n_devices)     # 50x50 die grid\n",
        "die_y = np.random.randint(0, 50, n_devices)\n",
        "\n",
        "# Complex yield model with spatial correlations and interactions\n",
        "# Base yield score\n",
        "yield_score = (\n",
        "    100 + \n",
        "    0.5 * (frequency_max - 2000) +\n",
        "    -0.3 * (temperature - 25) +\n",
        "    -1.0 * leakage +\n",
        "    -0.05 * delay_prop +\n",
        "    -0.2 * jitter_clk +\n",
        "    10 * noise_margin +\n",
        "    -0.1 * np.abs(skew)\n",
        ")\n",
        "\n",
        "# Add spatial effects (edge die have lower yield)\n",
        "edge_distance = np.minimum(np.minimum(die_x, 50-die_x), np.minimum(die_y, 50-die_y))\n",
        "yield_score += 0.5 * edge_distance\n",
        "\n",
        "# Add wafer-level effects (some wafers have systematic issues)\n",
        "wafer_effects = np.random.normal(0, 3, 25)\n",
        "yield_score += wafer_effects[wafer_id - 1]\n",
        "\n",
        "# Add interactions\n",
        "yield_score += -0.01 * frequency_max * temperature / 100\n",
        "yield_score += -0.1 * (leakage > 15) * 5  # High leakage penalty\n",
        "\n",
        "# Binary yield (pass/fail) with some noise\n",
        "yield_score += np.random.normal(0, 5, n_devices)\n",
        "yield_binary = (yield_score > 95).astype(int)\n",
        "\n",
        "# Create DataFrame\n",
        "df_wafer = pd.DataFrame({\n",
        "    'Voltage_V': voltage_vdd,\n",
        "    'Current_mA': current_idd,\n",
        "    'Frequency_MHz': frequency_max,\n",
        "    'Temperature_C': temperature,\n",
        "    'Power_mW': power,\n",
        "    'Leakage_uA': leakage,\n",
        "    'Delay_ps': delay_prop,\n",
        "    'Jitter_ps': jitter_clk,\n",
        "    'Noise_Margin': noise_margin,\n",
        "    'Skew_ps': skew,\n",
        "    'Wafer_ID': wafer_id,\n",
        "    'Die_X': die_x,\n",
        "    'Die_Y': die_y,\n",
        "    'Yield': yield_binary\n",
        "})\n",
        "\n",
        "# Add 20 additional test features\n",
        "for i in range(20):\n",
        "    df_wafer[f'Test_{i+11}'] = globals()[f'test_{i+11}']\n",
        "\n",
        "print(f\"‚úÖ Dataset Generated:\")\n",
        "print(f\"   Devices: {n_devices:,}\")\n",
        "print(f\"   Features: {df_wafer.shape[1] - 1} (30 parametric + 3 spatial)\")\n",
        "print(f\"   Target: Binary yield (pass/fail)\")\n",
        "print(f\"\\nYield Statistics:\")\n",
        "print(f\"   Pass rate: {yield_binary.mean():.1%}\")\n",
        "print(f\"   Fail rate: {1 - yield_binary.mean():.1%}\")\n",
        "print(f\"\\nBusiness Context:\")\n",
        "print(f\"   100K devices = 4 wafer lots (typical production day)\")\n",
        "print(f\"   Packaging cost: $0.50/device\")\n",
        "print(f\"   Potential savings: ${int((1-yield_binary.mean()) * n_devices * 0.50):,} if failing devices caught early\")\n",
        "\n",
        "df_wafer.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìù What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Train XGBoost classifier on 100K devices to predict yield with high accuracy.\n",
        "\n",
        "**Key Points:**\n",
        "- **XGBClassifier**: Binary classification (yield pass/fail)\n",
        "- **eval_metric='auc'**: Optimize area under ROC curve (better for imbalanced data)\n",
        "- **scale_pos_weight**: Handle class imbalance (more passing than failing devices)\n",
        "- **Feature importance**: Identify which tests most impact yield\n",
        "- **Production metrics**: Accuracy, AUC, precision, recall, confusion matrix\n",
        "\n",
        "**Why This Matters:** High AUC (>0.95) means model can reliably rank devices by failure risk. This enables adaptive testing: test high-risk devices more thoroughly, skip tests for low-risk devices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "# Prepare data\n",
        "X = df_wafer.drop('Yield', axis=1).values\n",
        "y = df_wafer['Yield'].values\n",
        "feature_names = df_wafer.drop('Yield', axis=1).columns.tolist()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"üöÄ Training XGBoost Classifier on 100K devices...\\n\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Calculate scale_pos_weight for class imbalance\n",
        "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "\n",
        "# Train XGBoost classifier\n",
        "xgb_yield = xgb.XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    min_child_weight=3,\n",
        "    gamma=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=2,\n",
        "    reg_alpha=0.1,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    eval_metric='auc',\n",
        "    early_stopping_rounds=30,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "xgb_yield.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Predictions\n",
        "y_pred = xgb_yield.predict(X_test)\n",
        "y_pred_proba = xgb_yield.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"‚úÖ Training Complete (Time: {training_time:.2f}s)\\n\")\n",
        "print(f\"üéØ Model Performance:\")\n",
        "print(f\"   Accuracy: {accuracy:.4f}\")\n",
        "print(f\"   AUC-ROC: {auc:.4f}\")\n",
        "print(f\"   Trees used: {xgb_yield.best_iteration + 1} (early stopped from 500)\")\n",
        "\n",
        "print(f\"\\nüìä Confusion Matrix:\")\n",
        "print(f\"   True Neg: {cm[0,0]:6,}  |  False Pos: {cm[0,1]:6,}\")\n",
        "print(f\"   False Neg: {cm[1,0]:6,}  |  True Pos: {cm[1,1]:6,}\")\n",
        "\n",
        "print(f\"\\nüìã Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Fail', 'Pass']))\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': xgb_yield.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(f\"\\nüîç Top 10 Features Impacting Yield:\")\n",
        "print(feature_importance.head(10).to_string(index=False))\n",
        "\n",
        "print(f\"\\nüí∞ Business Impact:\")\n",
        "false_negatives = cm[1, 0]\n",
        "false_positives = cm[0, 1]\n",
        "true_positives = cm[1, 1]\n",
        "print(f\"   Correctly identified failures: {true_positives:,}\")\n",
        "print(f\"   Missed failures (false negatives): {false_negatives:,}\")\n",
        "print(f\"   False alarms (false positives): {false_positives:,}\")\n",
        "print(f\"   Cost savings: ~${int((true_positives + false_positives) * 0.50):,} (screened before packaging)\")\n",
        "print(f\"   Cost of missed failures: ~${int(false_negatives * 5):,} (packaged then failed)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize top 15 feature importances\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "top_features = feature_importance.head(15).sort_values('Importance')\n",
        "ax.barh(top_features['Feature'], top_features['Importance'])\n",
        "ax.set_xlabel('Feature Importance (Gain)', fontsize=12)\n",
        "ax.set_title('Top 15 Features for Yield Prediction', fontsize=14)\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüîç Feature Importance Insights:\")\n",
        "top1 = feature_importance.iloc[0]\n",
        "print(f\"   Most important: {top1['Feature']} ({top1['Importance']:.3f})\")\n",
        "print(f\"   ‚Üí Optimize this test for maximum yield impact\")\n",
        "print(f\"   ‚Üí Consider tighter limits or enhanced screening\")\n",
        "\n",
        "# Identify low-importance features for potential elimination\n",
        "low_importance = feature_importance[feature_importance['Importance'] < 0.01]\n",
        "print(f\"\\nüóëÔ∏è Low-importance features ({len(low_importance)}):\")\n",
        "if len(low_importance) > 0:\n",
        "    print(f\"   Consider removing: {', '.join(low_importance['Feature'].head(5).tolist())}\")\n",
        "    print(f\"   Potential test time savings: ~{len(low_importance) * 0.5:.1f}ms per device\")\n",
        "else:\n",
        "    print(f\"   All features contribute meaningfully (none < 0.01 importance)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üöÄ Real-World Project Templates\n",
        "\n",
        "### Post-Silicon Validation Projects (4)\n",
        "\n",
        "#### 1. **Real-Time Adaptive Test System**\n",
        "**Objective:** Reduce test time by 30-40% using XGBoost-powered adaptive testing  \n",
        "**Business Value:** $3-8M annual savings for high-volume production (1M+ devices/day)  \n",
        "**Approach:**\n",
        "- Train XGBoost on historical STDF data (10M+ devices, 50-100 test parameters)\n",
        "- Deploy model on ATE (Automatic Test Equipment) for real-time inference (<10ms)\n",
        "- After first 20% of tests, predict final bin with 95%+ confidence\n",
        "- Skip remaining 80% of tests if confidence high, or flag for extended testing if anomaly\n",
        "- Use DMatrix format for maximum speed, GPU acceleration if available\n",
        "**Features:** Early test parameters (first 10-20 tests), spatial data, lot metadata\n",
        "**Success Metric:** 30% average test time reduction with <2% misclassification rate\n",
        "**Implementation Tip:** Use native API for production deployment, retrain weekly on fresh data\n",
        "\n",
        "#### 2. **Multi-Site Test Optimization**\n",
        "**Objective:** Predict final test results from wafer test ‚Üí eliminate 50% of final test parameters  \n",
        "**Business Value:** $5-15M capital savings (fewer final test machines needed)  \n",
        "**Approach:**\n",
        "- Train XGBoost to map wafer test parameters ‚Üí final test parameters (multi-output regression)\n",
        "- Identify which final tests are redundant (predictable from wafer test with R¬≤ > 0.9)\n",
        "- Use hierarchical XGBoost: first predict pass/fail, then predict specific bin for passing devices\n",
        "- Incorporate spatial correlation (nearby die have similar final test results)\n",
        "**Features:** Wafer test parameters (30-50), die coordinates, wafer_id, lot_id, process splits\n",
        "**Success Metric:** Predict 50% of final test parameters with <5% error\n",
        "**Implementation Tip:** Train separate models per product family, use subsample=0.7 for large datasets\n",
        "\n",
        "#### 3. **Parametric Outlier Detection at Scale**\n",
        "**Objective:** Detect anomalous devices in real-time (process excursions, equipment drift)  \n",
        "**Business Value:** Prevent yield losses ($1-20M per excursion) by catching issues early  \n",
        "**Approach:**\n",
        "- Train XGBoost on baseline \"good\" process data (normal devices only)\n",
        "- Use one-class classification or predict test values (regression) and flag large residuals\n",
        "- Monitor prediction errors (residuals) for sudden increases ‚Üí process drift signal\n",
        "- Feature importance identifies root cause (e.g., leakage spike ‚Üí contamination)\n",
        "- Deploy as streaming system: analyze devices as they complete testing\n",
        "**Features:** All parametric tests + environmental (temperature, humidity) + equipment_id\n",
        "**Success Metric:** Detect excursions 1-3 days before yield impact (vs 5-10 days with SPC)\n",
        "**Implementation Tip:** Use early_stopping_rounds=50, retrain daily, alert on 3-sigma residuals\n",
        "\n",
        "#### 4. **Wafer Map Clustering with XGBoost**\n",
        "**Objective:** Classify wafer spatial patterns (edge failures, center defects, random) ‚Üí identify failure root causes  \n",
        "**Business Value:** Reduce debug time by 50% (faster root cause identification)  \n",
        "**Approach:**\n",
        "- Extract features from wafer maps: edge die ratio, center die ratio, clustering metrics (DBSCAN density), radial trends\n",
        "- Train XGBoost classifier for 5-10 pattern types (edge, center, random, quadrant, ring, systematic)\n",
        "- Each pattern type maps to known failure mechanisms (edge‚Üídicing, center‚Üíthermal, random‚Üícontamination)\n",
        "- Automated root cause hypothesis generation based on pattern + parameter correlations\n",
        "**Features:** Spatial statistics (edge ratio, center density, Moran's I), parametric test distributions\n",
        "**Success Metric:** 85%+ pattern classification accuracy, 50% faster root cause identification\n",
        "**Implementation Tip:** Generate synthetic wafer maps for training (common patterns + variations)\n",
        "\n",
        "---\n",
        "\n",
        "### General AI/ML Projects (4)\n",
        "\n",
        "#### 5. **Credit Scoring Engine (FICO Replacement)**\n",
        "**Objective:** Build custom credit risk model with XGBoost ‚Üí 15-25% better AUC than FICO  \n",
        "**Business Value:** Approve 10-20% more good loans, reject 30% more bad loans  \n",
        "**Approach:**\n",
        "- Train XGBoost on credit bureau data (payment history, credit utilization, inquiries, age)\n",
        "- Add alternative data (rent payments, utility bills, education, employment)\n",
        "- Use scale_pos_weight for class imbalance (5-10% default rate typical)\n",
        "- Optimize for precision at 70-80% recall (minimize false positives = bad loans approved)\n",
        "- Deploy with explain predictions (SHAP values) for regulatory compliance\n",
        "**Features:** Credit score, income, debt-to-income, payment history, account age, inquiries\n",
        "**Success Metric:** AUC > 0.80, default rate < 3% for approved loans\n",
        "**Implementation Tip:** Use monotonic constraints (higher income ‚Üí lower risk)\n",
        "\n",
        "#### 6. **E-Commerce Click-Through Rate (CTR) Prediction**\n",
        "**Objective:** Predict ad click probability ‚Üí optimize ad placement and bidding  \n",
        "**Business Value:** 30-50% increase in CTR, 20% reduction in cost-per-click  \n",
        "**Approach:**\n",
        "- Train XGBoost on impression logs (millions per day): user features, ad features, context\n",
        "- Use DMatrix with GPU acceleration for fast retraining (daily updates)\n",
        "- Feature engineering: user-ad interaction features, time-of-day, device type\n",
        "- Deploy for real-time bidding: predict CTR in <5ms per ad impression\n",
        "- A/B test: XGBoost predictions vs current system\n",
        "**Features:** User demographics, browsing history, ad category, placement, time, device\n",
        "**Success Metric:** AUC > 0.75, 30% CTR improvement, <5ms latency\n",
        "**Implementation Tip:** Use colsample_bytree=0.7 to handle high-dimensional sparse features\n",
        "\n",
        "#### 7. **Healthcare Readmission Risk Predictor**\n",
        "**Objective:** Predict 30-day hospital readmission risk ‚Üí intervene with high-risk patients  \n",
        "**Business Value:** Reduce readmissions by 25% ($10K-30K penalty per readmission avoided)  \n",
        "**Approach:**\n",
        "- Train XGBoost on EHR data: demographics, diagnosis codes, lab results, medications, previous admissions\n",
        "- Handle missing data (XGBoost's native sparsity awareness)\n",
        "- Feature importance identifies modifiable risk factors (medication adherence, follow-up visits)\n",
        "- Deploy as clinical decision support: flag high-risk patients at discharge\n",
        "- Calibrate predictions (Platt scaling) for reliable probability estimates\n",
        "**Features:** Age, diagnosis, comorbidities, lab values, medications, previous admissions, discharge location\n",
        "**Success Metric:** AUC > 0.75, identify 70% of readmissions in top 20% risk scores\n",
        "**Implementation Tip:** Use gamma=1 for conservative splits (healthcare requires stability)\n",
        "\n",
        "#### 8. **Kaggle Competition Framework**\n",
        "**Objective:** Top 10% finish in structured data competition using XGBoost ensemble  \n",
        "**Business Value:** Learning, portfolio building, prize money ($10K-100K)  \n",
        "**Approach:**\n",
        "- Extensive feature engineering: interactions, aggregations, transformations\n",
        "- Hyperparameter tuning with Bayesian optimization (Optuna, Hyperopt)\n",
        "- Train multiple XGBoost models with different random seeds, subsample rates\n",
        "- Ensemble: blend XGBoost + LightGBM + CatBoost predictions\n",
        "- Cross-validation: 5-10 fold stratified CV for reliable validation\n",
        "**Features:** All provided + engineered features (100-500 features typical)\n",
        "**Success Metric:** Top 10% leaderboard position (gold medal in some competitions)\n",
        "**Implementation Tip:** Use learning_rate=0.01, n_estimators=5000-10000, early_stopping_rounds=100\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Key Takeaways\n",
        "\n",
        "### When to Use XGBoost\n",
        "\n",
        "‚úÖ **Use XGBoost when:**\n",
        "- Structured/tabular data (not images/text/sequences)\n",
        "- Need maximum accuracy on small-medium datasets (<10M samples)\n",
        "- Prediction speed matters (faster than deep learning)\n",
        "- Have missing values (XGBoost handles natively)\n",
        "- Want feature importance interpretability\n",
        "- Competitions or production ML systems\n",
        "\n",
        "‚ùå **Avoid XGBoost when:**\n",
        "- Unstructured data (images ‚Üí CNNs, text ‚Üí Transformers better)\n",
        "- Need online learning (XGBoost requires batch retraining)\n",
        "- Extrapolation required (tree models can't predict beyond training range)\n",
        "- Very simple linear relationships (logistic regression faster and simpler)\n",
        "- Need probabilistic predictions (calibration often required)\n",
        "\n",
        "---\n",
        "\n",
        "### XGBoost vs Alternatives\n",
        "\n",
        "| Aspect | Linear Models | Random Forest | GBM | XGBoost | LightGBM | CatBoost |\n",
        "|--------|---------------|---------------|-----|---------|----------|----------|\n",
        "| **Training speed** | Very fast | Fast | Slow | Medium | Very fast | Medium |\n",
        "| **Prediction speed** | Very fast | Medium | Fast | Fast | Very fast | Fast |\n",
        "| **Accuracy** | Low-medium | Medium-high | High | Very high | Very high | Very high |\n",
        "| **Memory usage** | Very low | High | Medium | Medium | Low | Medium |\n",
        "| **Overfitting risk** | Low | Low | Medium | Low (regularized) | Low | Very low |\n",
        "| **Hyperparameter tuning** | Easy | Easy | Medium | Hard | Hard | Medium |\n",
        "| **Missing values** | No | No | No | Yes (native) | Yes (native) | Yes (native) |\n",
        "| **Categorical features** | No | No | No | No | No | Yes (native) |\n",
        "| **GPU support** | No | No | No | Yes | Yes | Yes |\n",
        "| **Best for** | Baselines | General purpose | Legacy systems | Competitions | Large data | Categorical data |\n",
        "\n",
        "---\n",
        "\n",
        "### Hyperparameter Tuning Priority\n",
        "\n",
        "**Tier 1 (Most Important):**\n",
        "1. **learning_rate** (eta): Start 0.1, lower to 0.01-0.05 for production  \n",
        "   Lower learning rate ‚Üí more trees needed but better generalization\n",
        "\n",
        "2. **n_estimators**: Start 100, increase to 500-5000 with early stopping  \n",
        "   Use early_stopping_rounds=50 to find optimal automatically\n",
        "\n",
        "3. **max_depth**: Start 6, try 3-10  \n",
        "   Deeper trees capture more interactions but risk overfitting\n",
        "\n",
        "**Tier 2 (Regularization):**\n",
        "4. **reg_lambda** (L2): Default 1, try 1-10 if overfitting  \n",
        "   Shrinks leaf weights smoothly\n",
        "\n",
        "5. **reg_alpha** (L1): Default 0, try 0.1-1 for feature selection  \n",
        "   Promotes sparsity (some weights ‚Üí 0)\n",
        "\n",
        "6. **gamma**: Default 0, try 0-5 if overfitting  \n",
        "   Minimum loss reduction to create split\n",
        "\n",
        "**Tier 3 (Sampling):**\n",
        "7. **subsample**: Start 1.0, try 0.6-0.9 for large datasets  \n",
        "   Stochastic GBM, adds randomness\n",
        "\n",
        "8. **colsample_bytree**: Start 1.0, try 0.6-0.9 for high-dimensional data  \n",
        "   Random feature selection per tree (like RF)\n",
        "\n",
        "**Typical good configuration:**\n",
        "```python\n",
        "{\n",
        "    'n_estimators': 1000,\n",
        "    'learning_rate': 0.05,\n",
        "    'max_depth': 6,\n",
        "    'min_child_weight': 3,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'reg_lambda': 2,\n",
        "    'reg_alpha': 0.1,\n",
        "    'gamma': 0.1,\n",
        "    'early_stopping_rounds': 50\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "1. **Always use validation set**: Never tune on test set, use 20% validation or 5-fold CV\n",
        "\n",
        "2. **Start with defaults**: XGBoost defaults work well, tune only if needed\n",
        "\n",
        "3. **Early stopping is mandatory**: Set n_estimators high (1000-10000), let early stopping find optimal\n",
        "\n",
        "4. **Lower learning rate for production**: 0.01-0.05 more stable than 0.1-0.3\n",
        "\n",
        "5. **Use DMatrix for large data**: 2-5x faster than numpy arrays for >100K samples\n",
        "\n",
        "6. **Monitor training curves**: Plot train vs validation loss to diagnose overfitting\n",
        "\n",
        "7. **Feature engineering matters**: XGBoost powerful, but good features still critical\n",
        "\n",
        "8. **Handle class imbalance**: Use scale_pos_weight or custom objective for imbalanced data\n",
        "\n",
        "9. **Check feature importance**: Remove low-importance features to speed up training\n",
        "\n",
        "10. **Ensemble for competitions**: Blend XGBoost + LightGBM + CatBoost for maximum accuracy\n",
        "\n",
        "---\n",
        "\n",
        "### Limitations and Solutions\n",
        "\n",
        "**Limitation 1: Cannot extrapolate**  \n",
        "‚Üí Solution: Ensure test data within training range, or use linear models for extrapolation\n",
        "\n",
        "**Limitation 2: Many hyperparameters**  \n",
        "‚Üí Solution: Use defaults first, tune systematically (Tier 1 ‚Üí Tier 2 ‚Üí Tier 3)\n",
        "\n",
        "**Limitation 3: Not well-calibrated probabilities**  \n",
        "‚Üí Solution: Apply Platt scaling or isotonic regression for calibration\n",
        "\n",
        "**Limitation 4: Sequential training (slower than RF)**  \n",
        "‚Üí Solution: Use LightGBM for larger datasets (10x faster)\n",
        "\n",
        "**Limitation 5: Memory intensive with many trees**  \n",
        "‚Üí Solution: Use max_depth=3-6, or deploy only top N trees for inference\n",
        "\n",
        "---\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "**020 - LightGBM**: Histogram-based GBM for massive datasets (10x faster)  \n",
        "**021 - CatBoost**: Ordered boosting with native categorical feature handling  \n",
        "**022 - Voting & Stacking**: Ensemble multiple models for maximum accuracy  \n",
        "\n",
        "**Advanced XGBoost Topics:**\n",
        "- Custom objective functions for domain-specific losses\n",
        "- GPU acceleration for 10-100x speedup on large data\n",
        "- Distributed XGBoost on Spark/Dask for >100M samples\n",
        "- SHAP values for explainable predictions\n",
        "- Monotonic constraints for domain knowledge\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö References and Further Reading\n",
        "\n",
        "**Foundational Papers:**\n",
        "- Chen & Guestrin (2016). \"XGBoost: A Scalable Tree Boosting System\" - Original XGBoost paper (KDD 2016)\n",
        "- Friedman (2001). \"Greedy Function Approximation: A Gradient Boosting Machine\" - GBM foundations\n",
        "\n",
        "**Official Documentation:**\n",
        "- [XGBoost Documentation](https://xgboost.readthedocs.io/) - Comprehensive API reference\n",
        "- [XGBoost Parameters](https://xgboost.readthedocs.io/en/stable/parameter.html) - All hyperparameters explained\n",
        "- [XGBoost Tutorials](https://xgboost.readthedocs.io/en/stable/tutorials/index.html) - Python, R, distributed training\n",
        "\n",
        "**Practical Guides:**\n",
        "- [Complete Guide to Parameter Tuning in XGBoost](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n",
        "- [XGBoost vs LightGBM vs CatBoost Comparison](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)\n",
        "- [SHAP for XGBoost Interpretation](https://github.com/slundberg/shap)\n",
        "\n",
        "**Advanced Topics:**\n",
        "- GPU acceleration setup and performance benchmarks\n",
        "- Custom objective and evaluation functions\n",
        "- Distributed training on Spark/Dask\n",
        "- Production deployment strategies\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Notebook Complete\n",
        "\n",
        "**What You've Mastered:**\n",
        "1. ‚úÖ XGBoost algorithm with regularization and 2nd-order gradients\n",
        "2. ‚úÖ Mathematical foundation (Taylor approximation, optimal leaf weights)\n",
        "3. ‚úÖ Sklearn API (XGBRegressor/Classifier) vs Native API (xgb.train)\n",
        "4. ‚úÖ DMatrix format for maximum performance\n",
        "5. ‚úÖ Early stopping and hyperparameter tuning strategies\n",
        "6. ‚úÖ Regularization (L1/L2/gamma) for overfitting prevention\n",
        "7. ‚úÖ Post-silicon application: 100K-device yield prediction (95%+ AUC)\n",
        "8. ‚úÖ Feature importance interpretation and business insights\n",
        "9. ‚úÖ 8 real-world project templates (post-silicon + general AI/ML)\n",
        "10. ‚úÖ Production best practices and limitations\n",
        "\n",
        "**Key Achievement:** You can now build competition-grade XGBoost models and deploy them in production systems handling millions of predictions.\n",
        "\n",
        "**Next:** 020_LightGBM.ipynb - Histogram-based gradient boosting for even faster training on massive datasets\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
