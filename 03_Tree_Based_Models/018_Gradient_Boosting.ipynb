{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 018: Gradient Boosting Machines (GBM)\n",
    "\n",
    "## \ud83c\udfaf What You'll Learn\n",
    "\n",
    "**Gradient Boosting** is a sequential ensemble technique that builds models iteratively, where each new model corrects the errors of the previous ones. Unlike Random Forest (which builds trees in parallel and averages), Gradient Boosting builds trees sequentially, with each tree learning from the residuals (errors) of the combined ensemble so far.\n",
    "\n",
    "**Why Gradient Boosting After Random Forest?**\n",
    "- **Random Forest** (bagging): Reduces variance by averaging independent trees \u2192 robust, parallel\n",
    "- **Gradient Boosting** (boosting): Reduces bias by sequentially correcting errors \u2192 powerful, sequential\n",
    "- **Key difference**: RF trains trees independently, GBM trains trees dependently (each learns from previous mistakes)\n",
    "\n",
    "**Real-World Power:**\n",
    "- **Post-Silicon**: Iteratively improve yield prediction by focusing on hard-to-predict devices\n",
    "- **General AI/ML**: Kaggle competition winner (XGBoost/LightGBM built on GBM principles)\n",
    "- **Business**: Better accuracy than RF on structured data, especially with careful tuning\n",
    "\n",
    "**Learning Path:**\n",
    "1. Understand boosting vs bagging conceptually\n",
    "2. Learn gradient descent in function space\n",
    "3. Implement from scratch (forward stagewise additive modeling)\n",
    "4. Use sklearn's GradientBoostingRegressor\n",
    "5. Apply to post-silicon parametric test optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Gradient Boosting Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Training Data X, y] --> B[Initialize F0 = mean of y]\n",
    "    B --> C[Iteration m = 1 to M]\n",
    "    C --> D[Compute Residuals: r = y - F_m-1]\n",
    "    D --> E[Train Weak Learner h_m on X, r]\n",
    "    E --> F[Update Model: F_m = F_m-1 + \u03b7\u00b7h_m]\n",
    "    F --> G{m < M?}\n",
    "    G -->|Yes| C\n",
    "    G -->|No| H[Final Model F_M = \u03a3 \u03b7\u00b7h_m]\n",
    "    H --> I[Predict: \u0177 = F_M X]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style H fill:#fff4e1\n",
    "    style I fill:#f0f0f0\n",
    "```\n",
    "\n",
    "**Key Insight:** Each tree `h_m` learns to predict the residuals (errors) of the current ensemble `F_{m-1}`. The learning rate `\u03b7` (eta, typically 0.01-0.3) controls how much each tree contributes, preventing overfitting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\uddee Mathematical Foundation\n",
    "\n",
    "### Boosting as Gradient Descent in Function Space\n",
    "\n",
    "**Objective:** Minimize loss function $L(y, F(x))$ by iteratively adding weak learners.\n",
    "\n",
    "**Algorithm (Friedman 2001):**\n",
    "\n",
    "1. **Initialize** with constant prediction:  \n",
    "   $$F_0(x) = \\arg\\min_\\gamma \\sum_{i=1}^{n} L(y_i, \\gamma)$$\n",
    "   For squared error: $F_0(x) = \\bar{y}$ (mean of targets)\n",
    "\n",
    "2. **For iteration** $m = 1$ to $M$:\n",
    "\n",
    "   a. **Compute pseudo-residuals** (negative gradient of loss):  \n",
    "   $$r_{im} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F=F_{m-1}}$$\n",
    "   For squared error: $r_{im} = y_i - F_{m-1}(x_i)$ (simple residuals)\n",
    "\n",
    "   b. **Fit weak learner** $h_m(x)$ to pseudo-residuals:  \n",
    "   $$h_m = \\arg\\min_h \\sum_{i=1}^{n} (r_{im} - h(x_i))^2$$\n",
    "\n",
    "   c. **Update ensemble** with learning rate $\\eta$:  \n",
    "   $$F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)$$\n",
    "\n",
    "3. **Final model:**  \n",
    "   $$F_M(x) = F_0(x) + \\eta \\sum_{m=1}^{M} h_m(x)$$\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "- **$M$ (n_estimators)**: Number of boosting iterations (trees). More = better fit, but risk overfitting. Typical: 100-1000\n",
    "- **$\\eta$ (learning_rate)**: Step size for each tree's contribution. Lower = more robust, needs more trees. Typical: 0.01-0.3\n",
    "- **max_depth**: Depth of each weak learner. Shallow trees (2-6) work best for boosting. Typical: 3-8\n",
    "- **subsample**: Fraction of data for each tree (stochastic gradient boosting). Adds randomness. Typical: 0.5-1.0\n",
    "\n",
    "### Boosting vs Bagging (Random Forest)\n",
    "\n",
    "| Aspect | Random Forest (Bagging) | Gradient Boosting |\n",
    "|--------|-------------------------|-------------------|\n",
    "| **Training** | Parallel (independent trees) | Sequential (dependent trees) |\n",
    "| **Tree depth** | Deep trees (10-30) | Shallow trees (3-8) |\n",
    "| **Focus** | Reduce variance | Reduce bias |\n",
    "| **Overfitting** | Less prone (averaging) | More prone (sequential fitting) |\n",
    "| **Speed** | Fast (parallelizable) | Slower (sequential) |\n",
    "| **Accuracy** | Good | Often better with tuning |\n",
    "| **Hyperparameter sensitivity** | Low | High (learning_rate, n_estimators) |\n",
    "\n",
    "**Intuition:** RF asks \"What would many experts say independently?\", GBM asks \"How can I fix what the previous expert got wrong?\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 From-Scratch Implementation\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement Gradient Boosting from first principles to understand the sequential learning process.\n",
    "\n",
    "**Key Points:**\n",
    "- **DecisionTreeRegressorSimple**: Minimal tree (just splits data once) as weak learner for boosting\n",
    "- **GradientBoostingRegressorScratch**: Sequential ensemble builder\n",
    "  - Initialize with mean (F_0)\n",
    "  - Loop: compute residuals \u2192 fit tree \u2192 update ensemble\n",
    "  - Predict by summing all tree predictions\n",
    "- **Learning rate**: Controls contribution of each tree (prevents overfitting from aggressive fitting)\n",
    "\n",
    "**Why This Matters:** Understanding the sequential correction process is key to tuning GBM models effectively. Each tree is a \"correction\" not a \"prediction\".\n",
    "\n",
    "**Implementation Note:** For simplicity, we use decision stumps (1-split trees). Production GBM uses full trees with max_depth 3-8.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Simple Decision Stump (1-split tree) for weak learner\n",
    "class DecisionStump:\n",
    "    \"\"\"Minimal decision tree with single split (weak learner for boosting).\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.feature_idx = None\n",
    "        self.threshold = None\n",
    "        self.left_value = None\n",
    "        self.right_value = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Find best single split to minimize RSS.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        best_rss = float('inf')\n",
    "        \n",
    "        # Try all features and thresholds\n",
    "        for feature_idx in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                # Split data\n",
    "                left_mask = X[:, feature_idx] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                if left_mask.sum() == 0 or right_mask.sum() == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Compute RSS for this split\n",
    "                left_value = y[left_mask].mean()\n",
    "                right_value = y[right_mask].mean()\n",
    "                \n",
    "                left_rss = ((y[left_mask] - left_value) ** 2).sum()\n",
    "                right_rss = ((y[right_mask] - right_value) ** 2).sum()\n",
    "                total_rss = left_rss + right_rss\n",
    "                \n",
    "                if total_rss < best_rss:\n",
    "                    best_rss = total_rss\n",
    "                    self.feature_idx = feature_idx\n",
    "                    self.threshold = threshold\n",
    "                    self.left_value = left_value\n",
    "                    self.right_value = right_value\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using the learned split.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        predictions = np.zeros(n_samples)\n",
    "        \n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Code Continuation (2/2)\n",
    "\n",
    "Continuing implementation...\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "        left_mask = X[:, self.feature_idx] <= self.threshold\n",
    "        predictions[left_mask] = self.left_value\n",
    "        predictions[~left_mask] = self.right_value\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Gradient Boosting Regressor from scratch\n",
    "class GradientBoostingRegressorScratch:\n",
    "    \"\"\"Gradient Boosting implementation using decision stumps as weak learners.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.initial_prediction = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit gradient boosting model using sequential residual correction.\"\"\"\n",
    "        # Initialize with mean (F_0)\n",
    "        self.initial_prediction = y.mean()\n",
    "        current_predictions = np.full(len(y), self.initial_prediction)\n",
    "        \n",
    "        # Sequential boosting iterations\n",
    "        for m in range(self.n_estimators):\n",
    "            # Compute residuals (negative gradient for squared loss)\n",
    "            residuals = y - current_predictions\n",
    "            \n",
    "            # Fit weak learner to residuals\n",
    "            tree = DecisionStump(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Update predictions with learning rate\n",
    "            tree_predictions = tree.predict(X)\n",
    "            current_predictions += self.learning_rate * tree_predictions\n",
    "            \n",
    "            # Store tree\n",
    "            self.trees.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict by summing contributions from all trees.\"\"\"\n",
    "        predictions = np.full(X.shape[0], self.initial_prediction)\n",
    "        \n",
    "        for tree in self.trees:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "print(\"\u2705 From-scratch Gradient Boosting implementation ready\")\n",
    "print(f\"   - DecisionStump: Single-split weak learner\")\n",
    "print(f\"   - GradientBoostingRegressorScratch: Sequential ensemble builder\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Test from-scratch gradient boosting on non-linear data to see sequential error correction in action.\n",
    "\n",
    "**Key Points:**\n",
    "- **Test data**: $y = x^2 + 2x + noise$ (quadratic relationship)\n",
    "- **Progressive fitting**: Watch how adding more trees improves fit (10 \u2192 50 \u2192 100 trees)\n",
    "- **Learning rate impact**: Lower learning rate (0.1) requires more trees but is more stable\n",
    "- **Visualization**: See how ensemble prediction evolves as we add trees\n",
    "\n",
    "**Why This Matters:** Demonstrates that boosting can approximate complex functions by combining many simple models (stumps). Each stump corrects the previous ensemble's mistakes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-linear test data\n",
    "np.random.seed(42)\n",
    "X_train = np.random.uniform(-3, 3, 200).reshape(-1, 1)\n",
    "y_train = X_train.ravel()**2 + 2*X_train.ravel() + np.random.normal(0, 0.5, 200)\n",
    "\n",
    "X_test = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y_test_true = X_test.ravel()**2 + 2*X_test.ravel()\n",
    "\n",
    "# Train models with different numbers of estimators\n",
    "models = {\n",
    "    '10 trees': GradientBoostingRegressorScratch(n_estimators=10, learning_rate=0.1),\n",
    "    '50 trees': GradientBoostingRegressorScratch(n_estimators=50, learning_rate=0.1),\n",
    "    '100 trees': GradientBoostingRegressorScratch(n_estimators=100, learning_rate=0.1)\n",
    "}\n",
    "\n",
    "# Fit and evaluate\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test_true, y_pred)\n",
    "    results[name] = {'model': model, 'y_pred': y_pred, 'mse': mse}\n",
    "    print(f\"{name:12} - Test MSE: {mse:.4f}\")\n",
    "\n",
    "# Visualize progressive improvement\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (name, result) in enumerate(results.items()):\n",
    "    ax = axes[idx]\n",
    "    ax.scatter(X_train, y_train, alpha=0.3, label='Training data')\n",
    "    ax.plot(X_test, y_test_true, 'g--', linewidth=2, label='True function')\n",
    "    ax.plot(X_test, result['y_pred'], 'r-', linewidth=2, label=f'GBM {name}')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f\"{name}\\nMSE: {result['mse']:.4f}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca Observation: More trees \u2192 better approximation of quadratic function\")\n",
    "print(\"   Each tree corrects residual errors from previous ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Demonstrate the critical tradeoff between learning rate and number of estimators.\n",
    "\n",
    "**Key Points:**\n",
    "- **High learning rate (0.5)**: Aggressive updates, faster convergence, risk of overfitting\n",
    "- **Medium learning rate (0.1)**: Balanced, good default choice\n",
    "- **Low learning rate (0.01)**: Slow, smooth convergence, needs many trees (500+)\n",
    "- **Best practice**: Lower learning rate + more trees = better generalization (but slower training)\n",
    "\n",
    "**Why This Matters:** In production, you typically use learning_rate=0.01-0.05 with n_estimators=1000-5000 for best performance. This is the single most important hyperparameter pair in gradient boosting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates (fixed 100 trees)\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "lr_results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = GradientBoostingRegressorScratch(n_estimators=100, learning_rate=lr)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test_true, y_pred)\n",
    "    lr_results[lr] = {'y_pred': y_pred, 'mse': mse}\n",
    "    print(f\"Learning rate {lr:4.2f} - Test MSE: {mse:.4f}\")\n",
    "\n",
    "# Visualize learning rate impact\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.scatter(X_train, y_train, alpha=0.3, label='Training data', s=20)\n",
    "ax.plot(X_test, y_test_true, 'g--', linewidth=3, label='True function')\n",
    "\n",
    "for lr, result in lr_results.items():\n",
    "    ax.plot(X_test, result['y_pred'], linewidth=2, \n",
    "            label=f'LR={lr} (MSE={result[\"mse\"]:.3f})')\n",
    "\n",
    "ax.set_xlabel('X', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Impact of Learning Rate (100 trees)', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2696\ufe0f Learning Rate Tradeoff:\")\n",
    "print(\"   \u2022 High LR (0.5): Fast but jagged, may overfit\")\n",
    "print(\"   \u2022 Medium LR (0.1): Good balance for most tasks\")\n",
    "print(\"   \u2022 Low LR (0.01): Smooth, needs 500+ trees for convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \u2705 Batch 1 Complete: Gradient Boosting Foundations\n",
    "\n",
    "**What We've Built:**\n",
    "1. \u2705 **Conceptual understanding**: Boosting = sequential error correction (vs RF's parallel averaging)\n",
    "2. \u2705 **Mathematical foundation**: Gradient descent in function space, pseudo-residuals, learning rate\n",
    "3. \u2705 **From-scratch implementation**: GradientBoostingRegressorScratch with DecisionStump weak learners\n",
    "4. \u2705 **Learning rate analysis**: Demonstrated critical tradeoff between LR and n_estimators\n",
    "\n",
    "**Key Insights:**\n",
    "- Each tree predicts **residuals** (errors), not original targets\n",
    "- Learning rate controls how aggressively we correct errors (lower = more robust)\n",
    "- Shallow trees (depth 1-3) work better than deep trees for boosting\n",
    "- More trees generally improve accuracy until convergence/overfitting\n",
    "\n",
    "**Next (Batch 2):**\n",
    "- sklearn's GradientBoostingRegressor (production-ready with regularization)\n",
    "- Early stopping and validation curves\n",
    "- Post-silicon application: Test time prediction with iterative improvement\n",
    "- 8 real-world project templates\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\ude80 Production Implementation: Sklearn GradientBoostingRegressor\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Use sklearn's production-ready gradient boosting with full trees and advanced features.\n",
    "\n",
    "**Key Points:**\n",
    "- **GradientBoostingRegressor**: Full CART trees (not stumps), optimized C implementation\n",
    "- **max_depth=4**: Shallow trees typical for boosting (vs RF's depth 10-30)\n",
    "- **subsample=0.8**: Stochastic GBM - trains each tree on 80% random sample (adds regularization)\n",
    "- **Early stopping**: Monitor validation loss, stop when no improvement (prevents overfitting)\n",
    "- **n_iter_no_change**: Stop if validation score doesn't improve for N iterations\n",
    "\n",
    "**Why This Matters:** Production GBM is highly tuned - C implementation, regularization, early stopping. Much faster and more robust than from-scratch version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data for validation\n",
    "X_train_full, X_val, y_train_full, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train sklearn GradientBoostingRegressor with early stopping\n",
    "gbm_sklearn = GradientBoostingRegressor(\n",
    "    n_estimators=500,           # Max iterations (will stop early)\n",
    "    learning_rate=0.1,          # Step size\n",
    "    max_depth=4,                # Shallow trees for boosting\n",
    "    subsample=0.8,              # Stochastic GBM (80% samples per tree)\n",
    "    validation_fraction=0.2,    # Use 20% for early stopping\n",
    "    n_iter_no_change=20,        # Stop if no improvement for 20 iterations\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gbm_sklearn.fit(X_train_full, y_train_full)\n",
    "\n",
    "# Predictions\n",
    "y_pred_sklearn = gbm_sklearn.predict(X_test)\n",
    "mse_sklearn = mean_squared_error(y_test_true, y_pred_sklearn)\n",
    "r2_sklearn = r2_score(y_test_true, y_pred_sklearn)\n",
    "\n",
    "print(f\"Sklearn GradientBoostingRegressor:\")\n",
    "print(f\"  Test MSE: {mse_sklearn:.4f}\")\n",
    "print(f\"  Test R\u00b2:  {r2_sklearn:.4f}\")\n",
    "print(f\"  Trees used: {gbm_sklearn.n_estimators_} (early stopped from max 500)\")\n",
    "\n",
    "# Compare with from-scratch\n",
    "gbm_scratch = GradientBoostingRegressorScratch(n_estimators=100, learning_rate=0.1)\n",
    "gbm_scratch.fit(X_train, y_train)\n",
    "y_pred_scratch = gbm_scratch.predict(X_test)\n",
    "mse_scratch = mean_squared_error(y_test_true, y_pred_scratch)\n",
    "\n",
    "print(f\"\\nFrom-scratch GBM (100 trees):\")\n",
    "print(f\"  Test MSE: {mse_scratch:.4f}\")\n",
    "print(f\"\\n\ud83d\udcca Sklearn improvement: {((mse_scratch - mse_sklearn) / mse_scratch * 100):.1f}% lower MSE\")\n",
    "print(\"   (Due to: deeper trees, stochastic sampling, optimized splits)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Visualize training vs validation loss to diagnose overfitting and find optimal n_estimators.\n",
    "\n",
    "**Key Points:**\n",
    "- **Training loss**: Always decreases (model fits training data better)\n",
    "- **Validation loss**: Decreases then plateaus/increases (overfitting signal)\n",
    "- **Optimal point**: Where validation loss is minimized\n",
    "- **Early stopping**: Automatically stops at this optimal point\n",
    "\n",
    "**Why This Matters:** This plot is the most important diagnostic for tuning n_estimators and learning_rate. In production, you monitor validation loss and stop training when it stops improving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training vs validation loss\n",
    "train_scores = gbm_sklearn.train_score_\n",
    "val_scores = np.zeros(len(train_scores))\n",
    "\n",
    "# Compute validation scores at each iteration (staged_predict)\n",
    "for i, y_pred_staged in enumerate(gbm_sklearn.staged_predict(X_val)):\n",
    "    val_scores[i] = mean_squared_error(y_val, y_pred_staged)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "ax.plot(range(1, len(train_scores)+1), -train_scores, label='Training Loss', linewidth=2)\n",
    "ax.plot(range(1, len(val_scores)+1), val_scores, label='Validation Loss', linewidth=2)\n",
    "ax.axvline(x=np.argmin(val_scores)+1, color='r', linestyle='--', \n",
    "           label=f'Optimal: {np.argmin(val_scores)+1} trees')\n",
    "ax.set_xlabel('Number of Trees', fontsize=12)\n",
    "ax.set_ylabel('MSE', fontsize=12)\n",
    "ax.set_title('Training vs Validation Loss (Gradient Boosting)', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Optimal n_estimators: {np.argmin(val_scores)+1}\")\n",
    "print(f\"   Validation loss minimized at this point\")\n",
    "print(f\"   Early stopping prevented overfitting beyond this\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd2c Post-Silicon Application: Test Time Prediction\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Predict semiconductor test time from parametric test results - critical for production throughput optimization.\n",
    "\n",
    "**Key Points:**\n",
    "- **Business problem**: Test time varies 5-50ms per device (1M devices/day \u2192 14-139 hours difference)\n",
    "- **Features**: 8 parametric tests (voltage, current, frequency, power, leakage, delay, noise, jitter)\n",
    "- **Target**: Test time in milliseconds (complex interactions: outliers trigger retests, frequency sweeps)\n",
    "- **Why GBM**: Captures non-linear interactions (e.g., high leakage + high temp \u2192 extended test)\n",
    "- **Business value**: Predict slow devices \u2192 prioritize them \u2192 optimize test flow \u2192 reduce overall test time\n",
    "\n",
    "**Why This Matters:** Test time optimization is a multi-million dollar opportunity. Reducing average test time by 10% = 10% throughput increase with no hardware investment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic semiconductor test time data\n",
    "np.random.seed(42)\n",
    "n_devices = 1000\n",
    "\n",
    "# Parametric test results (8 features)\n",
    "voltage = np.random.normal(1.8, 0.05, n_devices)      # Supply voltage (V)\n",
    "current = np.random.normal(150, 20, n_devices)        # Current draw (mA)\n",
    "frequency = np.random.normal(2000, 100, n_devices)    # Max frequency (MHz)\n",
    "temperature = np.random.uniform(25, 85, n_devices)    # Test temperature (\u00b0C)\n",
    "power = voltage * current                              # Power consumption (mW)\n",
    "leakage = np.random.exponential(10, n_devices)        # Leakage current (\u00b5A)\n",
    "delay = np.random.normal(500, 50, n_devices)          # Propagation delay (ps)\n",
    "jitter = np.random.exponential(20, n_devices)         # Clock jitter (ps)\n",
    "\n",
    "# Complex test time model with interactions\n",
    "base_time = 20  # Base test time (ms)\n",
    "test_time = base_time + \\\n",
    "            0.01 * (frequency - 2000) + \\\n",
    "            0.1 * (temperature - 25) + \\\n",
    "            0.5 * leakage + \\\n",
    "            0.02 * delay + \\\n",
    "            0.3 * jitter + \\\n",
    "            0.001 * (frequency * temperature / 100) + \\\n",
    "            0.01 * (leakage > 20) * 10 + \\\n",
    "            np.random.normal(0, 2, n_devices)  # Measurement noise\n",
    "\n",
    "# Create DataFrame\n",
    "df_test = pd.DataFrame({\n",
    "    'Voltage_V': voltage,\n",
    "    'Current_mA': current,\n",
    "    'Frequency_MHz': frequency,\n",
    "    'Temperature_C': temperature,\n",
    "    'Power_mW': power,\n",
    "    'Leakage_uA': leakage,\n",
    "    'Delay_ps': delay,\n",
    "    'Jitter_ps': jitter,\n",
    "    'TestTime_ms': test_time\n",
    "})\n",
    "\n",
    "print(\"\ud83d\udd2c Post-Silicon Test Time Dataset Generated:\")\n",
    "print(f\"   Devices: {n_devices}\")\n",
    "print(f\"   Features: 8 parametric tests\")\n",
    "print(f\"   Target: Test time (ms)\")\n",
    "print(f\"\\nTest time statistics:\")\n",
    "print(df_test['TestTime_ms'].describe())\n",
    "print(f\"\\nBusiness context:\")\n",
    "print(f\"   Mean test time: {df_test['TestTime_ms'].mean():.2f} ms\")\n",
    "print(f\"   For 1M devices/day: {df_test['TestTime_ms'].mean() * 1e6 / 3600000:.1f} hours total\")\n",
    "print(f\"   10% reduction \u2192 {df_test['TestTime_ms'].mean() * 1e6 * 0.1 / 3600000:.1f} hours saved/day\")\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Train gradient boosting model to predict test time and extract actionable insights.\n",
    "\n",
    "**Key Points:**\n",
    "- **Train-test split**: 80-20 split for validation\n",
    "- **Model**: 300 trees, learning_rate=0.05 (lower for stability), max_depth=5\n",
    "- **Metrics**: MSE and R\u00b2 for regression quality, feature importance for insights\n",
    "- **Feature importance**: Identifies which tests most influence test time (prioritize optimization here)\n",
    "- **Production use**: Deploy model to predict test time \u2192 schedule slow devices during off-peak\n",
    "\n",
    "**Why This Matters:** Feature importance reveals optimization opportunities. If leakage is most important, invest in faster leakage test equipment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df_test.drop('TestTime_ms', axis=1).values\n",
    "y = df_test['TestTime_ms'].values\n",
    "feature_names = df_test.drop('TestTime_ms', axis=1).columns.tolist()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Gradient Boosting model\n",
    "gbm_test_time = GradientBoostingRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gbm_test_time.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_train_pred = gbm_test_time.predict(X_train)\n",
    "y_test_pred = gbm_test_time.predict(X_test)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\ud83c\udfaf Gradient Boosting - Test Time Prediction Results:\")\n",
    "print(f\"\\nTraining Performance:\")\n",
    "print(f\"  MSE: {train_mse:.4f} ms\u00b2\")\n",
    "print(f\"  R\u00b2:  {train_r2:.4f}\")\n",
    "print(f\"  RMSE: {np.sqrt(train_mse):.4f} ms\")\n",
    "\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"  MSE: {test_mse:.4f} ms\u00b2\")\n",
    "print(f\"  R\u00b2:  {test_r2:.4f}\")\n",
    "print(f\"  RMSE: {np.sqrt(test_mse):.4f} ms\")\n",
    "\n",
    "print(f\"\\n\ud83d\udca1 Business Impact:\")\n",
    "print(f\"   Prediction accuracy: \u00b1{np.sqrt(test_mse):.2f} ms (vs mean {y.mean():.2f} ms)\")\n",
    "print(f\"   Can identify slow devices (>30ms) with {test_r2:.1%} confidence\")\n",
    "print(f\"   Enables proactive test scheduling optimization\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': gbm_test_time.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Top 5 Features Impacting Test Time:\")\n",
    "print(feature_importance.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "feature_importance_sorted = feature_importance.sort_values('Importance')\n",
    "ax.barh(feature_importance_sorted['Feature'], feature_importance_sorted['Importance'])\n",
    "ax.set_xlabel('Feature Importance', fontsize=12)\n",
    "ax.set_title('Feature Importance: Test Time Prediction', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udd0d Interpretation:\")\n",
    "top_feature = feature_importance.iloc[0]['Feature']\n",
    "top_importance = feature_importance.iloc[0]['Importance']\n",
    "print(f\"   '{top_feature}' has {top_importance:.1%} importance\")\n",
    "print(f\"   \u2192 Focus optimization efforts on {top_feature} measurement\")\n",
    "print(f\"   \u2192 Consider faster test equipment or parallel testing for this parameter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\ude80 Real-World Project Templates\n",
    "\n",
    "### Post-Silicon Validation Projects (4)\n",
    "\n",
    "#### 1. **Predictive Test Time Optimizer**\n",
    "**Objective:** Reduce total test time by 15% through intelligent scheduling  \n",
    "**Business Value:** $2-5M annual savings for high-volume production  \n",
    "**Approach:**\n",
    "- Train GBM on 1M+ historical test records (features: parametric results, bin predictions)\n",
    "- Predict test time before running full test suite\n",
    "- Schedule slow devices (>40ms) during off-peak hours, fast devices during peak\n",
    "- Use partial test results to refine predictions in real-time\n",
    "**Features:** Initial parametric tests (voltage, current, frequency), device metadata (lot, wafer_id)\n",
    "**Success Metric:** 15% reduction in average test time (20ms \u2192 17ms)\n",
    "**Implementation Tip:** Use learning_rate=0.01, n_estimators=1000-2000 for production stability\n",
    "\n",
    "#### 2. **Adaptive Binning Engine**\n",
    "**Objective:** Predict final device bin from early test results (skip unnecessary tests)  \n",
    "**Business Value:** 25-40% test time reduction, increased throughput  \n",
    "**Approach:**\n",
    "- Multi-class GBM (GradientBoostingClassifier) for 5-10 bins (speed grades, power classes)\n",
    "- Train on first 30% of test parameters \u2192 predict final bin\n",
    "- Adaptive testing: skip remaining tests if confidence > 95%\n",
    "- Hyperparameter tuning: max_depth=6-8 for complex decision boundaries\n",
    "**Features:** Early-stage parametric tests (first 10 of 50 tests)\n",
    "**Success Metric:** 90%+ bin prediction accuracy after 30% of tests\n",
    "**Implementation Tip:** Use staged_predict to get confidence scores, only skip tests when confident\n",
    "\n",
    "#### 3. **Multi-Site Test Correlation**\n",
    "**Objective:** Predict final test results from wafer test results (reduce final test time)  \n",
    "**Business Value:** Eliminate redundant final tests \u2192 30% cost reduction  \n",
    "**Approach:**\n",
    "- Train GBM to map wafer test parameters \u2192 final test parameters\n",
    "- Capture spatial dependencies (die_x, die_y) and process variations (wafer_id, lot_id)\n",
    "- Predict which devices will fail final test \u2192 skip them or adjust test limits\n",
    "- Use subsample=0.7 to handle dataset imbalance (more passing than failing devices)\n",
    "**Features:** Wafer test parametrics + spatial coordinates + process metadata\n",
    "**Success Metric:** 85%+ prediction accuracy for final test pass/fail\n",
    "**Implementation Tip:** Use HistGradientBoostingRegressor for large datasets (10M+ rows)\n",
    "\n",
    "#### 4. **Yield Drift Detection System**\n",
    "**Objective:** Detect process drift early (before yield drops) using GBM residuals  \n",
    "**Business Value:** Prevent yield excursions ($1-10M loss per event)  \n",
    "**Approach:**\n",
    "- Train GBM on baseline period (healthy process)\n",
    "- Monitor prediction residuals (y_actual - y_pred) over time\n",
    "- Alert when residuals exceed 2-3 standard deviations (process drift signal)\n",
    "- Use feature importance to identify root cause (e.g., leakage increased \u2192 contamination)\n",
    "**Features:** All parametric tests + environmental data (temperature, humidity)\n",
    "**Success Metric:** Detect drift 2-5 days before yield drop (vs 7-14 days with control charts)\n",
    "**Implementation Tip:** Retrain model weekly to adapt to gradual process changes\n",
    "\n",
    "---\n",
    "\n",
    "### General AI/ML Projects (4)\n",
    "\n",
    "#### 5. **Customer Lifetime Value (CLV) Predictor**\n",
    "**Objective:** Predict 12-month customer value for personalized marketing  \n",
    "**Business Value:** 20-30% increase in marketing ROI  \n",
    "**Approach:**\n",
    "- Train GBM on customer features (demographics, purchase history, engagement)\n",
    "- Predict continuous CLV value (regression) or CLV tiers (classification)\n",
    "- Use predictions to segment customers: high-value (premium offers), low-value (retention campaigns)\n",
    "- Hyperparameter tuning: learning_rate=0.05, n_estimators=500-1000, max_depth=5-7\n",
    "**Features:** Purchase frequency, average order value, recency, engagement metrics, demographics\n",
    "**Success Metric:** R\u00b2 > 0.7 for CLV prediction, 25% increase in high-value customer retention\n",
    "\n",
    "#### 6. **Credit Risk Scoring Engine**\n",
    "**Objective:** Predict loan default probability for approval decisions  \n",
    "**Business Value:** Reduce default rate by 15-25% while maintaining approval rate  \n",
    "**Approach:**\n",
    "- Binary classification GBM (default vs non-default)\n",
    "- Train on credit history, income, employment, debt-to-income ratio\n",
    "- Use staged_predict_proba for confidence scores \u2192 adjust interest rates based on risk\n",
    "- Handle class imbalance with scale_pos_weight or sample weighting\n",
    "**Features:** Credit score, income, employment length, debt ratio, payment history\n",
    "**Success Metric:** AUC > 0.85, default rate < 5% while maintaining 70%+ approval rate\n",
    "\n",
    "#### 7. **Demand Forecasting System**\n",
    "**Objective:** Predict product demand 4 weeks ahead for inventory optimization  \n",
    "**Business Value:** Reduce inventory costs by 20%, prevent stockouts  \n",
    "**Approach:**\n",
    "- Time series regression with GBM (features: lagged demand, trends, seasonality, promotions)\n",
    "- Train separate models for different product categories\n",
    "- Use learning_rate=0.01 for stable forecasts, n_estimators=1000-2000\n",
    "- Incorporate external features (weather, holidays, competitor pricing)\n",
    "**Features:** 12-week demand history, seasonality indicators, promotion flags, external events\n",
    "**Success Metric:** MAPE < 15%, stockout rate < 2%\n",
    "\n",
    "#### 8. **Fraud Detection Pipeline**\n",
    "**Objective:** Real-time fraud detection with <100ms latency  \n",
    "**Business Value:** Reduce fraud losses by 40-60%  \n",
    "**Approach:**\n",
    "- Binary GBM classifier (fraud vs legitimate transactions)\n",
    "- Train on transaction features (amount, location, time, merchant, user behavior)\n",
    "- Deploy model with early stopping (n_estimators=200, max_depth=4 for speed)\n",
    "- Use staged_predict with threshold optimization (balance false positives vs false negatives)\n",
    "**Features:** Transaction amount, velocity (transactions/hour), location deviation, merchant risk score\n",
    "**Success Metric:** 95%+ fraud detection rate, <5% false positive rate, <100ms prediction latency\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf93 Key Takeaways\n",
    "\n",
    "### When to Use Gradient Boosting\n",
    "\n",
    "\u2705 **Use GBM when:**\n",
    "- Structured/tabular data with complex interactions\n",
    "- Prediction accuracy is critical (competitions, production)\n",
    "- You have time for hyperparameter tuning\n",
    "- Features are mixed types (continuous + categorical)\n",
    "- Need feature importance for interpretation\n",
    "\n",
    "\u274c **Avoid GBM when:**\n",
    "- Need ultra-fast training (use Random Forest instead)\n",
    "- Very high-dimensional sparse data (linear models better)\n",
    "- Noisy labels (RF more robust)\n",
    "- Need perfect parallelization (RF trains in parallel, GBM is sequential)\n",
    "- Extrapolation required (GBM can't predict beyond training range)\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient Boosting vs Random Forest vs Linear Models\n",
    "\n",
    "| Aspect | Linear Regression | Random Forest | Gradient Boosting |\n",
    "|--------|-------------------|---------------|-------------------|\n",
    "| **Complexity** | Simple (linear) | Medium (non-linear) | High (non-linear) |\n",
    "| **Training speed** | Very fast | Fast (parallel) | Slow (sequential) |\n",
    "| **Prediction speed** | Very fast | Medium | Fast |\n",
    "| **Accuracy** | Low-medium | Medium-high | High |\n",
    "| **Interpretability** | High | Medium | Medium |\n",
    "| **Overfitting risk** | Low | Low | High (needs tuning) |\n",
    "| **Hyperparameter sensitivity** | Low | Low | High |\n",
    "| **Feature engineering** | Critical | Less critical | Least critical |\n",
    "| **Best for** | Linear relationships | Robust classification | Competitions, max accuracy |\n",
    "\n",
    "---\n",
    "\n",
    "### Hyperparameter Tuning Strategy\n",
    "\n",
    "**Priority order for tuning:**\n",
    "\n",
    "1. **learning_rate + n_estimators** (most important, tune together):\n",
    "   - Start: learning_rate=0.1, n_estimators=100\n",
    "   - Lower learning_rate \u2192 increase n_estimators proportionally\n",
    "   - Production: learning_rate=0.01-0.05, n_estimators=500-2000\n",
    "\n",
    "2. **max_depth** (second priority):\n",
    "   - Start: 3-5 for boosting (shallower than RF)\n",
    "   - Increase if underfitting: 6-8\n",
    "   - Deep trees (10+) usually overfit in GBM\n",
    "\n",
    "3. **subsample** (regularization):\n",
    "   - Start: 0.8-1.0\n",
    "   - Lower (0.5-0.7) if overfitting\n",
    "   - Adds stochastic element (like RF)\n",
    "\n",
    "4. **min_samples_split / min_samples_leaf** (fine-tuning):\n",
    "   - Increase (5-10) if overfitting\n",
    "   - Prevents tiny splits in trees\n",
    "\n",
    "**Grid search example:**\n",
    "```python\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [500, 1000, 2000],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'subsample': [0.7, 0.8, 1.0]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always use validation set / cross-validation**: Monitor validation loss to prevent overfitting\n",
    "2. **Start with low learning_rate**: 0.01-0.05 more robust than 0.1-0.3\n",
    "3. **Use early stopping**: Set n_iter_no_change=20-50 to stop when validation loss plateaus\n",
    "4. **Scale features**: Not strictly required, but can improve convergence speed\n",
    "5. **Handle missing values**: sklearn GBM handles them, but consider imputation for better performance\n",
    "6. **Check feature importance**: Remove low-importance features to speed up training\n",
    "7. **Use HistGradientBoostingRegressor**: For large datasets (>10K samples), much faster\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations and Solutions\n",
    "\n",
    "**Limitation 1: Sensitive to outliers**  \n",
    "\u2192 Solution: Use robust loss functions (Huber, quantile) or remove outliers\n",
    "\n",
    "**Limitation 2: Sequential training (slow)**  \n",
    "\u2192 Solution: Use XGBoost/LightGBM (next notebooks) for parallel tree building\n",
    "\n",
    "**Limitation 3: No extrapolation**  \n",
    "\u2192 Solution: Ensure test data is within training range, or use linear models for extrapolation\n",
    "\n",
    "**Limitation 4: Overfitting with default params**  \n",
    "\u2192 Solution: Always tune hyperparameters, use validation curves\n",
    "\n",
    "**Limitation 5: Memory intensive for large n_estimators**  \n",
    "\u2192 Solution: Use max_depth=3-5 (smaller trees), or LightGBM (more efficient)\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**019 - XGBoost**: Extreme Gradient Boosting with regularization and parallel processing  \n",
    "**020 - LightGBM**: Histogram-based GBM for massive datasets  \n",
    "**021 - CatBoost**: Ordered boosting with categorical feature handling  \n",
    "\n",
    "**Advanced Topics:**\n",
    "- DART (Dropout Additive Regression Trees) for better generalization\n",
    "- Multi-output GBM for simultaneous prediction of multiple targets\n",
    "- Monotonic constraints for domain knowledge integration\n",
    "- Interaction constraints to limit tree complexity\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda References and Further Reading\n",
    "\n",
    "**Foundational Papers:**\n",
    "- Friedman, J. H. (2001). \"Greedy Function Approximation: A Gradient Boosting Machine\" - Original GBM paper\n",
    "- Friedman, J. H. (2002). \"Stochastic Gradient Boosting\" - Introduced subsample parameter\n",
    "\n",
    "**sklearn Documentation:**\n",
    "- [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n",
    "- [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
    "- [HistGradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html) - For large datasets\n",
    "\n",
    "**Advanced Topics:**\n",
    "- Comparison: XGBoost vs LightGBM vs CatBoost (next notebooks)\n",
    "- Hyperparameter tuning strategies for GBM\n",
    "- Production deployment considerations\n",
    "\n",
    "---\n",
    "\n",
    "## \u2705 Notebook Complete\n",
    "\n",
    "**What You've Mastered:**\n",
    "1. \u2705 Gradient boosting algorithm and mathematics\n",
    "2. \u2705 From-scratch implementation with decision stumps\n",
    "3. \u2705 Sklearn GradientBoostingRegressor with early stopping\n",
    "4. \u2705 Hyperparameter tuning (learning_rate, n_estimators, max_depth)\n",
    "5. \u2705 Post-silicon test time prediction application\n",
    "6. \u2705 Feature importance interpretation\n",
    "7. \u2705 8 real-world project templates\n",
    "8. \u2705 Best practices and limitations\n",
    "\n",
    "**Next:** 019_XGBoost.ipynb - Extreme Gradient Boosting with regularization and GPU acceleration\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}