{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 021 - CatBoost: Ordered Boosting & Categorical Feature Mastery\n",
        "\n",
        "## \ud83d\udcd8 Introduction\n",
        "\n",
        "**CatBoost** (Categorical Boosting) is Yandex's gradient boosting framework designed to excel at handling categorical features. Released in 2017, it introduces **ordered boosting** to reduce prediction shift and **ordered target statistics** for robust categorical encoding.\n",
        "\n",
        "### Key Innovations\n",
        "\n",
        "1. **Ordered Boosting** - Eliminates prediction shift from target leakage\n",
        "2. **Ordered Target Statistics** - Smart categorical encoding without overfitting\n",
        "3. **Oblivious Trees** - Symmetric trees with same split at each level (fast prediction)\n",
        "4. **GPU Acceleration** - Optimized for both training and inference\n",
        "5. **Robust to Overfitting** - Performs well with default parameters\n",
        "\n",
        "### When to Use CatBoost\n",
        "\n",
        "\u2705 **High-cardinality categoricals** (equipment_id: 1000s, user_id: millions)  \n",
        "\u2705 **Small-medium datasets** (<1M samples) where accuracy matters most  \n",
        "\u2705 **Need robust defaults** (minimal hyperparameter tuning)  \n",
        "\u2705 **Interpretability required** (symmetric trees easier to visualize)  \n",
        "\u2705 **Limited time for feature engineering** (handles categoricals automatically)  \n",
        "\n",
        "### Comparison with XGBoost and LightGBM\n",
        "\n",
        "| Aspect | XGBoost | LightGBM | CatBoost |\n",
        "|--------|---------|----------|----------|\n",
        "| **Categorical handling** | Manual encoding | Native (basic) | **Ordered target stats** |\n",
        "| **Speed (large data)** | Medium | **Fastest** | Slower |\n",
        "| **Accuracy (small data)** | High | High | **Highest** |\n",
        "| **Overfitting resistance** | Good (with tuning) | Good (with tuning) | **Excellent (defaults)** |\n",
        "| **Tree structure** | Level-wise | Leaf-wise | **Oblivious (symmetric)** |\n",
        "| **Default performance** | Needs tuning | Needs tuning | **Strong defaults** |\n",
        "| **GPU support** | Yes | Yes | **Yes (optimized)** |\n",
        "| **Best for** | General purpose | Large datasets | Categoricals + small data |\n",
        "\n",
        "### Learning Path Context\n",
        "\n",
        "- **018_Gradient_Boosting** - Sequential boosting foundations\n",
        "- **019_XGBoost** - Regularized gradient boosting\n",
        "- **020_LightGBM** - Histogram-based speedup\n",
        "- **021_CatBoost (this)** - Ordered boosting + categorical mastery\n",
        "- **022_Voting_Stacking** (next) - Ensemble meta-learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd04 CatBoost Workflow\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    A[Input Data] --> B{Detect Categorical Features}\n",
        "    B --> C[Ordered Target Statistics]\n",
        "    C --> D[Random Permutations]\n",
        "    D --> E[Calculate Target Stats]\n",
        "    E --> F[Ordered Boosting]\n",
        "    F --> G[Build Oblivious Tree]\n",
        "    G --> H[Same Split at Each Level]\n",
        "    H --> I{More Trees?}\n",
        "    I -->|Yes| F\n",
        "    I -->|No| J[Final Ensemble]\n",
        "    J --> K[Predictions]\n",
        "    \n",
        "    style C fill:#e1f5ff\n",
        "    style F fill:#fff4e1\n",
        "    style G fill:#f0e1ff\n",
        "    style J fill:#e1ffe1\n",
        "```\n",
        "\n",
        "**Key Stages:**\n",
        "1. **Ordered Target Statistics** - Encode categoricals using historical targets\n",
        "2. **Random Permutations** - Multiple orderings prevent overfitting\n",
        "3. **Ordered Boosting** - Use different subsets for residual calculation\n",
        "4. **Oblivious Trees** - Symmetric structure (2^depth leaves)\n",
        "5. **Ensemble** - Combine trees with shrinkage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcd0 Mathematical Foundation\n",
        "\n",
        "### 1. Ordered Target Statistics (Categorical Encoding)\n",
        "\n",
        "Traditional target encoding causes **target leakage** when the same data point's target influences its encoding.\n",
        "\n",
        "**Problem with naive target encoding:**\n",
        "$$\\hat{x}_i^{cat} = \\frac{\\sum_{j: x_j^{cat} = x_i^{cat}} y_j}{\\sum_{j: x_j^{cat} = x_i^{cat}} 1}$$\n",
        "\n",
        "This includes $y_i$ in its own encoding \u2192 overfitting!\n",
        "\n",
        "**CatBoost's Ordered Target Statistics:**\n",
        "$$\\hat{x}_i^{cat} = \\frac{\\sum_{j: j < i, x_j^{cat} = x_i^{cat}} y_j + a \\cdot P}{\\sum_{j: j < i, x_j^{cat} = x_i^{cat}} 1 + a}$$\n",
        "\n",
        "Where:\n",
        "- $j < i$: Only use **previous examples** in the random permutation\n",
        "- $P$: Prior (global target mean)\n",
        "- $a$: Weight of prior (regularization, default 1)\n",
        "\n",
        "**Key insight:** Each sample's encoding depends only on samples that came **before** it in a random ordering.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Ordered Boosting\n",
        "\n",
        "Traditional gradient boosting suffers from **prediction shift** - the model used to compute gradients sees the same data it was trained on.\n",
        "\n",
        "**Standard GBM (prediction shift):**\n",
        "$$g_i = \\frac{\\partial L(y_i, F_{t-1}(x_i))}{\\partial F_{t-1}(x_i)}$$\n",
        "Where $F_{t-1}$ was trained on data including $x_i$ \u2192 biased gradients!\n",
        "\n",
        "**CatBoost's Ordered Boosting:**\n",
        "$$g_i = \\frac{\\partial L(y_i, M_i(x_i))}{\\partial M_i(x_i)}$$\n",
        "Where $M_i$ is trained on $\\{(x_j, y_j): j < i\\}$ only\n",
        "\n",
        "**Implementation:** Use multiple random permutations, for each sample $i$, compute gradient using a model trained only on samples before $i$ in that permutation.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Oblivious Decision Trees\n",
        "\n",
        "CatBoost uses **oblivious (symmetric) trees** where all nodes at the same level use the **same splitting criterion**.\n",
        "\n",
        "**Standard tree:** Each node has independent split \u2192 2^depth - 1 splits  \n",
        "**Oblivious tree:** Same split at each level \u2192 depth splits only\n",
        "\n",
        "**Structure:**\n",
        "```\n",
        "Depth 0:        [Split: feature_1 < 5]\n",
        "               /                      \\\\\n",
        "Depth 1:  [Split: feature_2 < 10]  [Split: feature_2 < 10]\n",
        "          /        \\               /        \\\\\n",
        "Leaves:  L1       L2             L3        L4\n",
        "```\n",
        "\n",
        "**Advantages:**\n",
        "- **Fast prediction**: $O(\\text{depth})$ vs $O(\\text{depth} \\times \\text{features})$\n",
        "- **Less overfitting**: Fewer parameters (depth splits vs 2^depth - 1 splits)\n",
        "- **Interpretable**: Easier to visualize and understand\n",
        "\n",
        "**Prediction formula:**\n",
        "$$F(x) = \\sum_{t=1}^T \\alpha_t \\cdot h_t(x)$$\n",
        "Where each $h_t$ is an oblivious tree with $2^{\\text{depth}}$ leaves.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Loss Function (same as GBM/XGBoost)\n",
        "\n",
        "**Regression (MSE):**\n",
        "$$L = \\frac{1}{N} \\sum_{i=1}^N (y_i - F(x_i))^2$$\n",
        "\n",
        "**Classification (Logloss):**\n",
        "$$L = -\\frac{1}{N} \\sum_{i=1}^N [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)]$$\n",
        "\n",
        "Where $p_i = \\sigma(F(x_i))$ and $\\sigma(z) = 1/(1 + e^{-z})$\n",
        "\n",
        "---\n",
        "\n",
        "### Key Hyperparameters\n",
        "\n",
        "| Parameter | Description | Default | Typical Range |\n",
        "|-----------|-------------|---------|---------------|\n",
        "| `iterations` | Number of trees | 1000 | 100-5000 |\n",
        "| `learning_rate` | Shrinkage | 0.03 | 0.01-0.3 |\n",
        "| `depth` | Tree depth | 6 | 4-10 |\n",
        "| `l2_leaf_reg` | L2 regularization | 3.0 | 1-10 |\n",
        "| `border_count` | Splits per feature | 254 | 32-255 |\n",
        "| `random_strength` | Randomness in splits | 1.0 | 0-10 |\n",
        "| `bagging_temperature` | Bayesian bootstrap | 1.0 | 0-10 |\n",
        "\n",
        "**CatBoost's strength:** Works well with defaults, minimal tuning needed!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd27 Installation\n",
        "\n",
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Install CatBoost library for gradient boosting with categorical features.\n",
        "\n",
        "**Key Points:**\n",
        "- **CatBoost**: Yandex's gradient boosting with ordered boosting\n",
        "- **GPU support**: Automatically enabled if CUDA available\n",
        "- **Compatible**: Works with sklearn API and native CatBoost API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install CatBoost\n",
        "# !pip install catboost\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
        "\n",
        "# CatBoost\n",
        "import catboost as cb\n",
        "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
        "\n",
        "# For comparison\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "print(f\"\u2705 Libraries loaded successfully\")\n",
        "print(f\"   CatBoost version: {cb.__version__}\")\n",
        "print(f\"   GPU available: {cb.get_gpu_device_count() > 0}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udff7\ufe0f Categorical Encoding: Ordered Target Statistics vs One-Hot\n",
        "\n",
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Demonstrate why CatBoost's ordered target statistics outperforms traditional encoding methods.\n",
        "\n",
        "**Key Points:**\n",
        "- **One-hot encoding**: 100 categories \u2192 100 columns (dimension explosion)\n",
        "- **Target encoding (naive)**: Uses target values \u2192 causes overfitting\n",
        "- **Ordered target statistics**: Uses only **previous** samples \u2192 no leakage\n",
        "- **Regularization**: Prior weight prevents overfitting on rare categories\n",
        "\n",
        "**Why This Matters:** For post-silicon data with equipment_id (1000s), lot_id (10,000s), traditional encoding is infeasible. CatBoost handles this automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate dataset with high-cardinality categorical\n",
        "np.random.seed(42)\n",
        "n_samples = 5000\n",
        "\n",
        "# High-cardinality categorical (100 equipment IDs)\n",
        "equipment_ids = [f'EQ_{i:03d}' for i in range(100)]\n",
        "equipment_id = np.random.choice(equipment_ids, n_samples)\n",
        "\n",
        "# Equipment has systematic effect on yield\n",
        "equipment_quality = {eq: np.random.normal(0, 10) for eq in equipment_ids}\n",
        "\n",
        "# Numerical features\n",
        "voltage = np.random.normal(1.8, 0.05, n_samples)\n",
        "temperature = np.random.uniform(25, 85, n_samples)\n",
        "\n",
        "# Target depends on equipment + numerical features\n",
        "y = (\n",
        "    100 +\n",
        "    np.array([equipment_quality[eq] for eq in equipment_id]) +\n",
        "    5 * (voltage - 1.8) / 0.05 +\n",
        "    -0.2 * (temperature - 25) +\n",
        "    np.random.normal(0, 3, n_samples)\n",
        ")\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'equipment_id': equipment_id,\n",
        "    'voltage': voltage,\n",
        "    'temperature': temperature,\n",
        "    'yield_score': y\n",
        "})\n",
        "\n",
        "print(\"\ud83d\udcca High-Cardinality Categorical Dataset:\")\n",
        "print(df.head(10))\n",
        "print(f\"\\nShape: {df.shape}\")\n",
        "print(f\"Unique equipment IDs: {df['equipment_id'].nunique()}\")\n",
        "print(f\"\\nEquipment ID distribution (top 10):\")\n",
        "print(df['equipment_id'].value_counts().head(10))\n",
        "\n",
        "# Split data\n",
        "X = df.drop('yield_score', axis=1)\n",
        "y = df['yield_score'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\n\u2705 Train: {len(X_train)}, Test: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Compare CatBoost (ordered target stats) vs XGBoost (one-hot) vs LightGBM (native categorical).\n",
        "\n",
        "**Key Points:**\n",
        "- **CatBoost**: Specify `cat_features` \u2192 automatic ordered target statistics\n",
        "- **XGBoost**: Requires manual one-hot encoding \u2192 100 columns from 1 feature\n",
        "- **LightGBM**: Native categorical but simpler encoding (not ordered)\n",
        "- **Metrics**: Training time, accuracy, memory usage\n",
        "\n",
        "**Why This Matters:** On high-cardinality categoricals, CatBoost typically achieves 2-5% better accuracy with less effort.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83d\ude80 Comparing categorical encoding methods...\\n\")\n",
        "\n",
        "# 1. CatBoost with ordered target statistics\n",
        "print(\"1\ufe0f\u20e3 CatBoost (Ordered Target Statistics)\")\n",
        "start_time = time.time()\n",
        "\n",
        "cat_model = CatBoostRegressor(\n",
        "    iterations=200,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "cat_model.fit(\n",
        "    X_train, y_train,\n",
        "    cat_features=['equipment_id'],  # Specify categorical features\n",
        "    eval_set=(X_test, y_test),\n",
        "    early_stopping_rounds=20,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "cat_time = time.time() - start_time\n",
        "cat_pred = cat_model.predict(X_test)\n",
        "cat_mse = mean_squared_error(y_test, cat_pred)\n",
        "cat_r2 = r2_score(y_test, cat_pred)\n",
        "\n",
        "print(f\"   Time: {cat_time:.2f}s\")\n",
        "print(f\"   Test MSE: {cat_mse:.2f}\")\n",
        "print(f\"   Test R\u00b2:  {cat_r2:.4f}\")\n",
        "print(f\"   Trees: {cat_model.tree_count_}\")\n",
        "\n",
        "# 2. XGBoost with one-hot encoding\n",
        "print(f\"\\n2\ufe0f\u20e3 XGBoost (One-Hot Encoding)\")\n",
        "X_train_ohe = pd.get_dummies(X_train, columns=['equipment_id'])\n",
        "X_test_ohe = pd.get_dummies(X_test, columns=['equipment_id'])\n",
        "X_test_ohe = X_test_ohe.reindex(columns=X_train_ohe.columns, fill_value=0)\n",
        "\n",
        "start_time = time.time()\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "xgb_model.fit(\n",
        "    X_train_ohe, y_train,\n",
        "    eval_set=[(X_test_ohe, y_test)],\n",
        "    early_stopping_rounds=20,\n",
        "    verbose=False\n",
        ")\n",
        "xgb_time = time.time() - start_time\n",
        "\n",
        "xgb_pred = xgb_model.predict(X_test_ohe)\n",
        "xgb_mse = mean_squared_error(y_test, xgb_pred)\n",
        "xgb_r2 = r2_score(y_test, xgb_pred)\n",
        "\n",
        "print(f\"   Time: {xgb_time:.2f}s\")\n",
        "print(f\"   Features after encoding: {X_train_ohe.shape[1]} (from 3 original)\")\n",
        "print(f\"   Test MSE: {xgb_mse:.2f}\")\n",
        "print(f\"   Test R\u00b2:  {xgb_r2:.4f}\")\n",
        "\n",
        "# 3. LightGBM with native categorical\n",
        "print(f\"\\n3\ufe0f\u20e3 LightGBM (Native Categorical)\")\n",
        "start_time = time.time()\n",
        "\n",
        "lgb_model = lgb.LGBMRegressor(\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.1,\n",
        "    num_leaves=31,\n",
        "    random_state=42,\n",
        "    verbose=-1\n",
        ")\n",
        "lgb_model.fit(\n",
        "    X_train, y_train,\n",
        "    categorical_feature=['equipment_id'],\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)]\n",
        ")\n",
        "lgb_time = time.time() - start_time\n",
        "\n",
        "lgb_pred = lgb_model.predict(X_test)\n",
        "lgb_mse = mean_squared_error(y_test, lgb_pred)\n",
        "lgb_r2 = r2_score(y_test, lgb_pred)\n",
        "\n",
        "print(f\"   Time: {lgb_time:.2f}s\")\n",
        "print(f\"   Test MSE: {lgb_mse:.2f}\")\n",
        "print(f\"   Test R\u00b2:  {lgb_r2:.4f}\")\n",
        "\n",
        "# Comparison\n",
        "print(f\"\\n\ud83d\udcca Summary:\")\n",
        "print(f\"   {'Method':<25} {'Time (s)':<12} {'MSE':<12} {'R\u00b2':<10}\")\n",
        "print(f\"   {'-'*60}\")\n",
        "print(f\"   {'CatBoost (Ordered)':<25} {cat_time:<12.2f} {cat_mse:<12.2f} {cat_r2:<10.4f}\")\n",
        "print(f\"   {'XGBoost (One-Hot)':<25} {xgb_time:<12.2f} {xgb_mse:<12.2f} {xgb_r2:<10.4f}\")\n",
        "print(f\"   {'LightGBM (Native)':<25} {lgb_time:<12.2f} {lgb_mse:<12.2f} {lgb_r2:<10.4f}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udca1 Key Insights:\")\n",
        "print(f\"   \u2022 CatBoost accuracy advantage: {(xgb_mse - cat_mse) / xgb_mse * 100:.1f}% lower MSE than XGBoost\")\n",
        "print(f\"   \u2022 No feature engineering required: 3 features vs {X_train_ohe.shape[1]} after one-hot\")\n",
        "print(f\"   \u2022 Ordered target statistics prevents overfitting on rare categories\")\n",
        "print(f\"   \u2022 Best for: equipment_id (1000s), lot_id (10,000s), user_id (millions)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udccb Batch 1 Summary: Foundations Complete\n",
        "\n",
        "### \u2705 What We've Covered\n",
        "\n",
        "1. **Ordered Target Statistics** - Categorical encoding without target leakage\n",
        "2. **Ordered Boosting** - Eliminates prediction shift\n",
        "3. **Oblivious Trees** - Symmetric structure for fast prediction\n",
        "4. **Categorical Encoding Comparison** - CatBoost vs XGBoost vs LightGBM\n",
        "\n",
        "### \ud83c\udfaf Key Insights\n",
        "\n",
        "- **CatBoost excels at high-cardinality categoricals**: equipment_id (1000s), user_id (millions)\n",
        "- **Ordered target statistics**: Only uses previous samples \u2192 no target leakage\n",
        "- **No feature engineering**: Automatic encoding beats manual one-hot\n",
        "- **Strong defaults**: Works well without extensive hyperparameter tuning\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\ude80 Coming in Batch 2\n",
        "\n",
        "- **Oblivious tree visualization** - Understand symmetric tree structure\n",
        "- **Post-silicon application** - 100K device classification with equipment/lot/wafer IDs\n",
        "- **Feature importance** - Identify critical categorical vs numerical features\n",
        "- **Hyperparameter tuning** - Depth, learning rate, regularization\n",
        "- **8 Real-world projects** - Post-silicon (4) + General AI/ML (4)\n",
        "- **Comparison guide** - When to use CatBoost vs XGBoost vs LightGBM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd2c Post-Silicon Application: Equipment-Lot-Wafer Yield Prediction\n",
        "\n",
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Build production-scale yield classifier with high-cardinality categoricals (equipment, lot, wafer IDs).\n",
        "\n",
        "**Key Points:**\n",
        "- **Scale**: 100K devices with realistic categorical distributions\n",
        "- **Categoricals**: 200 equipment IDs + 500 lot IDs + 1000 wafer IDs = 1700 unique categories\n",
        "- **Business scenario**: Predict yield before packaging to save $0.50-2.00 per device\n",
        "- **CatBoost advantage**: Handles 1700 categories natively (XGBoost would create 1700 columns!)\n",
        "\n",
        "**Business Value:**\n",
        "- **Early screening**: Identify failing devices before expensive packaging\n",
        "- **Equipment correlation**: Discover which equipment IDs predict failures\n",
        "- **Lot tracking**: Detect problematic lots early in production\n",
        "- **Cost savings**: $50K-200K per day from improved screening\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate realistic 100K device dataset\n",
        "print(\"\ud83c\udfed Generating 100K device production dataset...\\n\")\n",
        "\n",
        "np.random.seed(42)\n",
        "n_devices = 100000\n",
        "\n",
        "# High-cardinality categorical features\n",
        "equipment_ids = [f'EQ_{i:04d}' for i in range(200)]  # 200 test machines\n",
        "lot_ids = [f'LOT_{i:05d}' for i in range(500)]       # 500 lots\n",
        "wafer_ids = [f'WFR_{i:05d}' for i in range(1000)]    # 1000 wafers\n",
        "\n",
        "equipment_id = np.random.choice(equipment_ids, n_devices)\n",
        "lot_id = np.random.choice(lot_ids, n_devices)\n",
        "wafer_id = np.random.choice(wafer_ids, n_devices)\n",
        "\n",
        "# Equipment/lot/wafer quality effects (systematic variations)\n",
        "equipment_effects = {eq: np.random.normal(0, 3) for eq in equipment_ids}\n",
        "lot_effects = {lot: np.random.normal(0, 4) for lot in lot_ids}\n",
        "wafer_effects = {wfr: np.random.normal(0, 2) for wfr in wafer_ids}\n",
        "\n",
        "# Parametric test results (15 features)\n",
        "voltage = np.random.normal(1.8, 0.05, n_devices)\n",
        "current = np.random.normal(150, 20, n_devices)\n",
        "frequency = np.random.normal(2000, 100, n_devices)\n",
        "temperature = np.random.uniform(25, 85, n_devices)\n",
        "power = voltage * current\n",
        "leakage = np.random.exponential(10, n_devices)\n",
        "delay = np.random.normal(500, 50, n_devices)\n",
        "jitter = np.random.exponential(20, n_devices)\n",
        "noise_margin = np.random.normal(0.3, 0.06, n_devices)\n",
        "skew = np.random.normal(0, 18, n_devices)\n",
        "\n",
        "# Additional parametric tests\n",
        "test_11 = np.random.normal(100, 12, n_devices)\n",
        "test_12 = np.random.normal(50, 8, n_devices)\n",
        "test_13 = np.random.exponential(15, n_devices)\n",
        "test_14 = np.random.normal(200, 25, n_devices)\n",
        "test_15 = np.random.uniform(10, 90, n_devices)\n",
        "\n",
        "# Spatial features\n",
        "die_x = np.random.randint(0, 50, n_devices)\n",
        "die_y = np.random.randint(0, 50, n_devices)\n",
        "\n",
        "# Complex yield model\n",
        "yield_score = (\n",
        "    100 +\n",
        "    0.6 * (frequency - 2000) / 100 +\n",
        "    -0.4 * (temperature - 25) / 10 +\n",
        "    -1.2 * leakage / 10 +\n",
        "    -0.08 * delay / 50 +\n",
        "    -0.3 * jitter / 20 +\n",
        "    12 * noise_margin +\n",
        "    np.array([equipment_effects[eq] for eq in equipment_id]) +\n",
        "    np.array([lot_effects[lot] for lot in lot_id]) +\n",
        "    np.array([wafer_effects[wfr] for wfr in wafer_id]) +\n",
        "    np.random.normal(0, 6, n_devices)\n",
        ")\n",
        "\n",
        "# Binary yield (threshold at 95)\n",
        "yield_binary = (yield_score > 95).astype(int)\n",
        "\n",
        "# Create DataFrame\n",
        "df_ps = pd.DataFrame({\n",
        "    'Equipment_ID': equipment_id,\n",
        "    'Lot_ID': lot_id,\n",
        "    'Wafer_ID': wafer_id,\n",
        "    'Voltage_V': voltage,\n",
        "    'Current_mA': current,\n",
        "    'Frequency_MHz': frequency,\n",
        "    'Temperature_C': temperature,\n",
        "    'Power_mW': power,\n",
        "    'Leakage_uA': leakage,\n",
        "    'Delay_ps': delay,\n",
        "    'Jitter_ps': jitter,\n",
        "    'Noise_Margin': noise_margin,\n",
        "    'Skew_ps': skew,\n",
        "    'Test_11': test_11,\n",
        "    'Test_12': test_12,\n",
        "    'Test_13': test_13,\n",
        "    'Test_14': test_14,\n",
        "    'Test_15': test_15,\n",
        "    'Die_X': die_x,\n",
        "    'Die_Y': die_y,\n",
        "    'Yield': yield_binary\n",
        "})\n",
        "\n",
        "print(f\"\u2705 Production Dataset Generated:\")\n",
        "print(f\"   Devices: {n_devices:,}\")\n",
        "print(f\"   Features: {df_ps.shape[1] - 1}\")\n",
        "print(f\"   - Categorical: 3 (Equipment_ID: 200, Lot_ID: 500, Wafer_ID: 1000)\")\n",
        "print(f\"   - Numerical: 15 parametric tests\")\n",
        "print(f\"   - Spatial: 2 (Die_X, Die_Y)\")\n",
        "\n",
        "print(f\"\\nYield Statistics:\")\n",
        "print(f\"   Pass rate: {yield_binary.mean():.1%}\")\n",
        "print(f\"   Fail rate: {1 - yield_binary.mean():.1%}\")\n",
        "\n",
        "print(f\"\\nCategorical Feature Cardinality:\")\n",
        "print(f\"   Equipment_ID: {df_ps['Equipment_ID'].nunique()} unique values\")\n",
        "print(f\"   Lot_ID: {df_ps['Lot_ID'].nunique()} unique values\")\n",
        "print(f\"   Wafer_ID: {df_ps['Wafer_ID'].nunique()} unique values\")\n",
        "print(f\"   Total unique categories: {df_ps['Equipment_ID'].nunique() + df_ps['Lot_ID'].nunique() + df_ps['Wafer_ID'].nunique()}\")\n",
        "\n",
        "print(f\"\\nBusiness Context:\")\n",
        "print(f\"   100K devices = ~5 wafer lots (full production day)\")\n",
        "print(f\"   Packaging cost: $1.00/device average\")\n",
        "print(f\"   Potential daily savings: ${int((1-yield_binary.mean()) * n_devices * 1.00):,}\")\n",
        "\n",
        "print(f\"\\n{df_ps.head(10)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### \ud83d\udcdd What's Happening in This Code?\n",
        "\n",
        "**Purpose:** Train CatBoost classifier with native categorical feature handling and compare with XGBoost.\n",
        "\n",
        "**Key Points:**\n",
        "- **cat_features parameter**: Automatically applies ordered target statistics\n",
        "- **Pool object**: CatBoost's internal data structure (like DMatrix/Dataset)\n",
        "- **Early stopping**: Monitor AUC on validation set\n",
        "- **XGBoost comparison**: Must one-hot encode \u2192 1700 columns vs 20 original features\n",
        "\n",
        "**Why This Matters:** One-hot encoding 1700 categories creates sparse, high-dimensional data. CatBoost handles this elegantly with ordered target statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "# Prepare data\n",
        "X = df_ps.drop('Yield', axis=1)\n",
        "y = df_ps['Yield'].values\n",
        "feature_names = X.columns.tolist()\n",
        "cat_feature_names = ['Equipment_ID', 'Lot_ID', 'Wafer_ID']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"\ud83d\ude80 Training CatBoost on 100K devices...\\n\")\n",
        "\n",
        "# 1. CatBoost with native categorical handling\n",
        "print(\"1\ufe0f\u20e3 CatBoost (Ordered Target Statistics)\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Create Pool objects (CatBoost's native data structure)\n",
        "train_pool = Pool(X_train, y_train, cat_features=cat_feature_names)\n",
        "test_pool = Pool(X_test, y_test, cat_features=cat_feature_names)\n",
        "\n",
        "cat_clf = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    learning_rate=0.05,\n",
        "    depth=6,\n",
        "    l2_leaf_reg=3,\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "cat_clf.fit(\n",
        "    train_pool,\n",
        "    eval_set=test_pool,\n",
        "    early_stopping_rounds=30,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "cat_time = time.time() - start_time\n",
        "\n",
        "# Predictions\n",
        "y_pred_cat = cat_clf.predict(X_test)\n",
        "y_pred_proba_cat = cat_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Metrics\n",
        "cat_accuracy = accuracy_score(y_test, y_pred_cat)\n",
        "cat_auc = roc_auc_score(y_test, y_pred_proba_cat)\n",
        "cat_cm = confusion_matrix(y_test, y_pred_cat)\n",
        "\n",
        "print(f\"   Training time: {cat_time:.2f}s\")\n",
        "print(f\"   Trees used: {cat_clf.tree_count_} (early stopped from 500)\")\n",
        "print(f\"   Accuracy: {cat_accuracy:.4f}\")\n",
        "print(f\"   AUC-ROC: {cat_auc:.4f}\")\n",
        "\n",
        "# 2. XGBoost with one-hot encoding (for comparison)\n",
        "print(f\"\\n2\ufe0f\u20e3 XGBoost (One-Hot Encoding)\")\n",
        "print(f\"   Encoding 1700 categories...\")\n",
        "\n",
        "X_train_ohe = pd.get_dummies(X_train, columns=cat_feature_names)\n",
        "X_test_ohe = pd.get_dummies(X_test, columns=cat_feature_names)\n",
        "X_test_ohe = X_test_ohe.reindex(columns=X_train_ohe.columns, fill_value=0)\n",
        "\n",
        "print(f\"   Features after encoding: {X_train_ohe.shape[1]} (from 20 original)\")\n",
        "\n",
        "start_time = time.time()\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "xgb_clf.fit(\n",
        "    X_train_ohe, y_train,\n",
        "    eval_set=[(X_test_ohe, y_test)],\n",
        "    early_stopping_rounds=30,\n",
        "    verbose=False\n",
        ")\n",
        "xgb_time = time.time() - start_time\n",
        "\n",
        "y_pred_xgb = xgb_clf.predict(X_test_ohe)\n",
        "y_pred_proba_xgb = xgb_clf.predict_proba(X_test_ohe)[:, 1]\n",
        "\n",
        "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
        "xgb_auc = roc_auc_score(y_test, y_pred_proba_xgb)\n",
        "\n",
        "print(f\"   Training time: {xgb_time:.2f}s\")\n",
        "print(f\"   Accuracy: {xgb_accuracy:.4f}\")\n",
        "print(f\"   AUC-ROC: {xgb_auc:.4f}\")\n",
        "\n",
        "# Comparison\n",
        "print(f\"\\n\ud83d\udcca Comparison:\")\n",
        "print(f\"   {'Metric':<25} {'CatBoost':<15} {'XGBoost':<15}\")\n",
        "print(f\"   {'-'*55}\")\n",
        "print(f\"   {'Training time (s)':<25} {cat_time:<15.2f} {xgb_time:<15.2f}\")\n",
        "print(f\"   {'Feature count':<25} {X_train.shape[1]:<15} {X_train_ohe.shape[1]:<15}\")\n",
        "print(f\"   {'Accuracy':<25} {cat_accuracy:<15.4f} {xgb_accuracy:<15.4f}\")\n",
        "print(f\"   {'AUC-ROC':<25} {cat_auc:<15.4f} {xgb_auc:<15.4f}\")\n",
        "\n",
        "print(f\"\\n\u26a1 CatBoost Advantages:\")\n",
        "print(f\"   \u2022 {xgb_time / cat_time:.1f}x faster training\")\n",
        "print(f\"   \u2022 {X_train_ohe.shape[1] / X_train.shape[1]:.0f}x fewer features (20 vs {X_train_ohe.shape[1]})\")\n",
        "print(f\"   \u2022 {((xgb_auc - cat_auc) / xgb_auc * -100):.2f}% better AUC\")\n",
        "print(f\"   \u2022 No feature engineering required\")\n",
        "print(f\"   \u2022 Handles 1700 unique categories natively\")\n",
        "\n",
        "# Detailed metrics for CatBoost\n",
        "print(f\"\\n\ud83d\udccb CatBoost Confusion Matrix:\")\n",
        "print(f\"   True Neg:  {cat_cm[0,0]:7,}  |  False Pos: {cat_cm[0,1]:6,}\")\n",
        "print(f\"   False Neg: {cat_cm[1,0]:7,}  |  True Pos:  {cat_cm[1,1]:6,}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udccb Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_cat, target_names=['Fail', 'Pass']))\n",
        "\n",
        "# Business impact\n",
        "false_negatives = cat_cm[1, 0]\n",
        "false_positives = cat_cm[0, 1]\n",
        "true_negatives = cat_cm[0, 0]\n",
        "\n",
        "print(f\"\ud83d\udcb0 Business Impact (100K device analysis):\")\n",
        "print(f\"   Correctly caught failures: {true_negatives:,} devices\")\n",
        "print(f\"   Cost avoided: ${true_negatives * 1.0:,.0f} (packaging saved)\")\n",
        "print(f\"   Missed failures: {false_negatives:,} devices\")\n",
        "print(f\"   Cost of misses: ${false_negatives * 5.0:,.0f} (packaged then failed)\")\n",
        "print(f\"   False alarms: {false_positives:,} devices\")\n",
        "print(f\"   Opportunity cost: ${false_positives * 3.0:,.0f} (good devices scrapped)\")\n",
        "print(f\"\\n   Net benefit: ${(true_negatives * 1.0 - false_positives * 3.0 - false_negatives * 5.0):,.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance analysis\n",
        "feature_importance = cat_clf.get_feature_importance()\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importance\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\ud83d\udcca Top 15 Most Important Features:\\n\")\n",
        "print(feature_importance_df.head(15).to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_15 = feature_importance_df.head(15)\n",
        "plt.barh(range(len(top_15)), top_15['Importance'])\n",
        "plt.yticks(range(len(top_15)), top_15['Feature'])\n",
        "plt.xlabel('Importance', fontsize=12)\n",
        "plt.title('Top 15 Feature Importances - CatBoost Yield Predictor', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze categorical vs numerical\n",
        "cat_importance = feature_importance_df[feature_importance_df['Feature'].isin(cat_feature_names)]['Importance'].sum()\n",
        "num_importance = feature_importance_df[~feature_importance_df['Feature'].isin(cat_feature_names)]['Importance'].sum()\n",
        "total_importance = feature_importance_df['Importance'].sum()\n",
        "\n",
        "print(f\"\\n\ud83d\udca1 Categorical vs Numerical Importance:\")\n",
        "print(f\"   Categorical features: {cat_importance / total_importance * 100:.1f}% of total importance\")\n",
        "print(f\"   Numerical features: {num_importance / total_importance * 100:.1f}% of total importance\")\n",
        "print(f\"\\n   Top categorical: {feature_importance_df[feature_importance_df['Feature'].isin(cat_feature_names)].iloc[0]['Feature']}\")\n",
        "print(f\"   Top numerical: {feature_importance_df[~feature_importance_df['Feature'].isin(cat_feature_names)].iloc[0]['Feature']}\")\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf Insights for Test Optimization:\")\n",
        "print(f\"   \u2022 Equipment/Lot/Wafer effects account for {cat_importance / total_importance * 100:.1f}% of prediction\")\n",
        "print(f\"   \u2022 Ordered target statistics captures systematic quality variations\")\n",
        "print(f\"   \u2022 Top 5 features: {', '.join(top_15['Feature'].head(5).tolist())}\")\n",
        "print(f\"   \u2022 Consider equipment calibration for high-importance equipment IDs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf Real-World CatBoost Projects\n",
        "\n",
        "Below are 8 comprehensive project ideas demonstrating CatBoost's categorical feature mastery:\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd2c Post-Silicon Validation Projects (4)\n",
        "\n",
        "### **1. Equipment-Specific Yield Predictor**\n",
        "**Objective:** Build per-equipment yield models to identify problematic test machines\n",
        "\n",
        "**Features:**\n",
        "- Equipment_ID (500+ testers), operator_id, shift, maintenance_date\n",
        "- Parametric tests: voltage, current, frequency (15+ tests)\n",
        "- Temporal: days_since_calibration, cumulative_test_count\n",
        "- CatBoost ordered target statistics for equipment quality encoding\n",
        "\n",
        "**Success Metrics:**\n",
        "- Per-equipment AUC >0.90 (identify bad machines)\n",
        "- Detect equipment drift within 1000 devices\n",
        "- False positive rate <5% (avoid unnecessary downtime)\n",
        "- Recommend calibration 2-3 days before quality degrades\n",
        "\n",
        "**Business Value:** Prevent bad equipment from processing 50K+ devices \u2192 $50K-200K savings per incident\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Lot-Level Quality Prediction**\n",
        "**Objective:** Predict lot yield from first 1000 devices to enable early interventions\n",
        "\n",
        "**Features:**\n",
        "- Lot_ID (1000s), fab_site, process_node, wafer_supplier\n",
        "- Early parametric data: first 1000 devices from 25-wafer lot\n",
        "- Environmental: temperature, humidity, cleanroom class\n",
        "- Process: etch_time, implant_dose, anneal_temperature\n",
        "\n",
        "**Success Metrics:**\n",
        "- Predict final lot yield within \u00b13% after 1000 devices\n",
        "- 80%+ accuracy on lot pass/fail (>95% yield)\n",
        "- Early warning: 2-3 days before lot completion\n",
        "- Actionable: Identify correctable process issues\n",
        "\n",
        "**Business Value:** Stop bad lots early \u2192 save $500K-2M per lot (avoid processing remaining wafers)\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Multi-Site Categorical Correlation Engine**\n",
        "**Objective:** Unified model across 5 global fabs with site-specific categorical features\n",
        "\n",
        "**Features:**\n",
        "- Site_ID (5 fabs), equipment_id per site (100+ each), lot_id (10,000s)\n",
        "- Site-specific: local_vendor_id, operator_language, shift_pattern\n",
        "- Shared parametrics: voltage, frequency, power (standardized tests)\n",
        "- CatBoost handles 500+ unique equipment IDs across sites\n",
        "\n",
        "**Success Metrics:**\n",
        "- Unified model AUC >0.92 (all sites combined)\n",
        "- Per-site AUC variance <2%\n",
        "- Identify 15+ cross-site systematic patterns\n",
        "- Transfer learning: New site reaches 90% accuracy with 10K samples\n",
        "\n",
        "**Business Value:** Standardize quality predictions \u2192 $10-30M annual savings from shared learnings\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Supplier Quality Scoring with Categorical Tracing**\n",
        "**Objective:** Track wafer/material supplier quality using categorical IDs\n",
        "\n",
        "**Features:**\n",
        "- Wafer_supplier_id, material_lot_id, chemical_batch_id (high cardinality)\n",
        "- Supply chain: ship_date, storage_duration, handling_count\n",
        "- Quality metrics: defect_density, thickness_uniformity\n",
        "- Parametric results: Post-processing test yields\n",
        "\n",
        "**Success Metrics:**\n",
        "- Supplier ranking accuracy >85% (compare with manual audits)\n",
        "- Detect bad material batch within 5000 devices\n",
        "- Predict supplier yield impact within \u00b12%\n",
        "- Automated alerts for sub-spec supplier quality\n",
        "\n",
        "**Business Value:** Negotiate with suppliers using data \u2192 5-10% cost reduction on materials ($5-15M annually)\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83c\udf10 General AI/ML Projects (4)\n",
        "\n",
        "### **5. E-Commerce Recommendation with User/Product IDs**\n",
        "**Objective:** Predict purchase probability using high-cardinality user and product IDs\n",
        "\n",
        "**Features:**\n",
        "- user_id (millions), product_id (100,000s), category_id (1000s)\n",
        "- Behavioral: clicks, cart_adds, time_on_page\n",
        "- Contextual: device_type, location, time_of_day\n",
        "- CatBoost ordered target statistics prevents user_id overfitting\n",
        "\n",
        "**Success Metrics:**\n",
        "- AUC >0.80 (industry benchmark)\n",
        "- Precision >40% (top 10% predictions)\n",
        "- Inference latency <20ms\n",
        "- Daily retraining on 10M events\n",
        "\n",
        "**Business Value:** 15-25% conversion rate improvement \u2192 $5-15M additional revenue per quarter\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Credit Risk Scoring with Merchant/Location IDs**\n",
        "**Objective:** Predict default risk using categorical merchant and location data\n",
        "\n",
        "**Features:**\n",
        "- merchant_id (millions), zip_code (40,000+), occupation_code (1000s)\n",
        "- Financial: income, debt_ratio, payment_history\n",
        "- Behavioral: transaction_velocity, merchant_category\n",
        "- Temporal: days_since_last_payment, account_age\n",
        "\n",
        "**Success Metrics:**\n",
        "- AUC >0.75 (regulatory minimum)\n",
        "- Precision >60% (minimize false approvals)\n",
        "- Fair lending: No bias on protected categories\n",
        "- Explainable: Feature importance for audit compliance\n",
        "\n",
        "**Business Value:** Reduce default rate 20% \u2192 $50-200M annual savings for large lender\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Healthcare Patient Risk with ICD Diagnosis Codes**\n",
        "**Objective:** Predict readmission using high-cardinality ICD-10 codes\n",
        "\n",
        "**Features:**\n",
        "- primary_diagnosis_code (70,000 ICD-10 codes), hospital_id, insurance_type\n",
        "- Clinical: age, length_of_stay, num_procedures, comorbidity_score\n",
        "- Medication: drug_count, high_risk_meds (categoricals)\n",
        "- CatBoost handles 70K ICD codes without dimension explosion\n",
        "\n",
        "**Success Metrics:**\n",
        "- AUC >0.75 (published benchmark)\n",
        "- Precision >70% for high-risk patients\n",
        "- Recall >80% (catch most readmissions)\n",
        "- Interpretable: Top diagnosis codes for each prediction\n",
        "\n",
        "**Business Value:** Reduce readmissions 15-20% \u2192 $3-8M annual savings per hospital\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Marketing Attribution with Campaign/Channel IDs**\n",
        "**Objective:** Multi-touch attribution using categorical campaign and channel data\n",
        "\n",
        "**Features:**\n",
        "- campaign_id (10,000s), channel_id (100s), creative_id (1000s)\n",
        "- Touchpoint sequence: first_touch, last_touch, touch_count\n",
        "- User: demographic_segment, purchase_history_category\n",
        "- Temporal: days_since_first_touch, hour_of_conversion\n",
        "\n",
        "**Success Metrics:**\n",
        "- Attribution accuracy >80% (vs ground truth experiments)\n",
        "- ROAS prediction within \u00b115%\n",
        "- Campaign ranking correlation >0.90 with incrementality tests\n",
        "- Handles 10K+ campaigns with CatBoost ordered encoding\n",
        "\n",
        "**Business Value:** Optimize $10-100M ad budget \u2192 20-30% efficiency gain ($2-30M savings)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u2705 Key Takeaways: CatBoost Mastery\n",
        "\n",
        "### \ud83c\udfaf Use CatBoost When:\n",
        "\n",
        "1. **High-cardinality categorical features**\n",
        "   - Equipment_ID (1000s), user_id (millions), ICD codes (70,000+)\n",
        "   - Ordered target statistics prevents overfitting\n",
        "   - No one-hot encoding required\n",
        "\n",
        "2. **Small-medium datasets (<1M samples)**\n",
        "   - Better accuracy than XGBoost/LightGBM on small data\n",
        "   - Strong default parameters\n",
        "   - Less hyperparameter tuning required\n",
        "\n",
        "3. **Need robust defaults**\n",
        "   - Works well out-of-the-box\n",
        "   - Less prone to overfitting than other GBM libraries\n",
        "   - Oblivious trees reduce parameter count\n",
        "\n",
        "4. **Interpretability required**\n",
        "   - Symmetric trees easier to visualize\n",
        "   - Feature importance clearly separates categorical vs numerical\n",
        "   - Ordered encoding is explainable (historical target average)\n",
        "\n",
        "5. **Limited feature engineering time**\n",
        "   - Handles categoricals automatically\n",
        "   - No need for target encoding or hashing\n",
        "   - Fast experimentation\n",
        "\n",
        "---\n",
        "\n",
        "### \u26a0\ufe0f Use XGBoost/LightGBM Instead When:\n",
        "\n",
        "1. **Large datasets (>1M samples)**\n",
        "   - LightGBM 5-20x faster on massive data\n",
        "   - XGBoost more mature ecosystem\n",
        "   - CatBoost slower on large datasets\n",
        "\n",
        "2. **Few or no categorical features**\n",
        "   - CatBoost's main advantage unused\n",
        "   - XGBoost/LightGBM may be faster\n",
        "   - No benefit from ordered target statistics\n",
        "\n",
        "3. **Need maximum speed**\n",
        "   - LightGBM histogram-based is fastest\n",
        "   - XGBoost level-wise faster than CatBoost oblivious\n",
        "   - Real-time constraints (<100ms)\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udcca CatBoost vs XGBoost vs LightGBM\n",
        "\n",
        "| Aspect | XGBoost | LightGBM | CatBoost |\n",
        "|--------|---------|----------|----------|\n",
        "| **Categorical handling** | Manual encoding | Native (basic) | **Ordered target stats** |\n",
        "| **Speed (large data)** | Medium | **Fastest** | Slower |\n",
        "| **Accuracy (small data)** | High | High | **Highest** |\n",
        "| **Default performance** | Needs tuning | Needs tuning | **Strong defaults** |\n",
        "| **Overfitting resistance** | Good | Good | **Excellent** |\n",
        "| **Tree structure** | Level-wise | Leaf-wise | **Oblivious** |\n",
        "| **Best for** | General purpose | Large datasets | **Categoricals + small data** |\n",
        "| **Feature engineering** | Required | Some required | **Minimal** |\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd27 Best Practices\n",
        "\n",
        "1. **Always specify categorical features**\n",
        "   ```python\n",
        "   model.fit(X_train, y_train, cat_features=['col1', 'col2'])\n",
        "   ```\n",
        "\n",
        "2. **Use Pool objects for efficiency**\n",
        "   ```python\n",
        "   train_pool = Pool(X_train, y_train, cat_features=['col1'])\n",
        "   model.fit(train_pool)\n",
        "   ```\n",
        "\n",
        "3. **Start with defaults**\n",
        "   - `iterations=1000`, `learning_rate=0.03`, `depth=6`\n",
        "   - Tune only if accuracy insufficient\n",
        "   - Most important: `depth`, `l2_leaf_reg`, `learning_rate`\n",
        "\n",
        "4. **Enable early stopping**\n",
        "   ```python\n",
        "   model.fit(train, eval_set=test, early_stopping_rounds=30)\n",
        "   ```\n",
        "\n",
        "5. **Use GPU for medium-large datasets**\n",
        "   ```python\n",
        "   CatBoostClassifier(task_type='GPU', devices='0')\n",
        "   ```\n",
        "\n",
        "6. **Leverage built-in cross-validation**\n",
        "   ```python\n",
        "   cv_results = model.cv(params, pool, fold_count=5)\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\ude80 Next Steps\n",
        "\n",
        "1. **022_Voting_Stacking_Ensembles.ipynb** - Meta-learning and model combination\n",
        "2. **023_Hyperparameter_Optimization.ipynb** - Optuna, Hyperopt, Bayesian optimization\n",
        "3. **024_Model_Interpretation.ipynb** - SHAP, LIME, feature interactions\n",
        "4. **025_Imbalanced_Learning.ipynb** - Class weights, SMOTE, custom loss functions\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83c\udf93 What You've Mastered\n",
        "\n",
        "\u2705 **Ordered target statistics** - Categorical encoding without target leakage  \n",
        "\u2705 **Ordered boosting** - Eliminates prediction shift  \n",
        "\u2705 **Oblivious trees** - Symmetric structure for fast prediction  \n",
        "\u2705 **High-cardinality categoricals** - Handle 1000s-millions of unique values  \n",
        "\u2705 **Production deployment** - Equipment/Lot/Wafer quality prediction  \n",
        "\u2705 **Feature importance** - Categorical vs numerical contribution analysis  \n",
        "\u2705 **Robust defaults** - Minimal tuning for strong performance  \n",
        "\u2705 **Business applications** - $50K-30M impact across 8 domains  \n",
        "\n",
        "You now understand when CatBoost's ordered boosting and categorical mastery outperforms other gradient boosting frameworks! \ud83c\udf89\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcda References and Further Reading\n",
        "\n",
        "### Original Papers\n",
        "\n",
        "1. **CatBoost: unbiased boosting with categorical features** (2018)  \n",
        "   Prokhorenkova et al., NeurIPS 2018  \n",
        "   https://arxiv.org/abs/1706.09516\n",
        "\n",
        "2. **Ordered Boosting**  \n",
        "   Section 4 of CatBoost paper - eliminates prediction shift\n",
        "\n",
        "3. **Ordered Target Statistics**  \n",
        "   Section 3 of CatBoost paper - categorical feature encoding\n",
        "\n",
        "4. **Oblivious Decision Trees**  \n",
        "   Section 2.2 - symmetric tree structure\n",
        "\n",
        "### Official Documentation\n",
        "\n",
        "5. **CatBoost Official Docs**  \n",
        "   https://catboost.ai/docs/\n",
        "\n",
        "6. **Categorical Features Guide**  \n",
        "   https://catboost.ai/docs/concepts/categorical-features.html\n",
        "\n",
        "7. **Training Parameters**  \n",
        "   https://catboost.ai/docs/concepts/parameter-tuning.html\n",
        "\n",
        "8. **GPU Training**  \n",
        "   https://catboost.ai/docs/features/training-on-gpu.html\n",
        "\n",
        "### Comparison Studies\n",
        "\n",
        "9. **Benchmarking CatBoost vs XGBoost vs LightGBM** (2019)  \n",
        "   Performance comparison on categorical-heavy datasets\n",
        "\n",
        "10. **Kaggle Winning Solutions with CatBoost**  \n",
        "    https://www.kaggle.com - Search \"CatBoost\" for competition examples\n",
        "\n",
        "### Related Notebooks in This Series\n",
        "\n",
        "- **018_Gradient_Boosting.ipynb** - Sequential boosting foundations\n",
        "- **019_XGBoost.ipynb** - Regularized gradient boosting\n",
        "- **020_LightGBM.ipynb** - Histogram-based speedup\n",
        "- **022_Voting_Stacking_Ensembles.ipynb** (next) - Meta-learning\n",
        "- **016_Decision_Trees.ipynb** - Tree-based models fundamentals\n",
        "- **017_Random_Forest.ipynb** - Parallel ensemble methods\n",
        "\n",
        "---\n",
        "\n",
        "**Notebook Complete!** \u2705  \n",
        "**Next:** 022_Voting_Stacking_Ensembles.ipynb - Combining multiple models for superior performance\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}