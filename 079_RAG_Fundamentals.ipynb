{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28704176",
   "metadata": {},
   "source": [
    "## 1. What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** combines:\n",
    "1. **Retrieval**: Finding relevant information from a knowledge base\n",
    "2. **Augmentation**: Adding that information to the prompt\n",
    "3. **Generation**: Using an LLM to generate responses with retrieved context\n",
    "\n",
    "### Why RAG?\n",
    "\n",
    "**Without RAG:**\n",
    "```\n",
    "User: \"What is the STDF PTR record format?\"\n",
    "LLM: \"I don't have specific information about STDF formats...\"\n",
    "```\n",
    "\n",
    "**With RAG:**\n",
    "```\n",
    "User: \"What is the STDF PTR record format?\"\n",
    "System retrieves: [STDF spec documentation about PTR records]\n",
    "LLM: \"The STDF PTR (Parametric Test Record) contains...\"\n",
    "[Accurate, sourced response]\n",
    "```\n",
    "\n",
    "### RAG vs Fine-Tuning vs Prompt Engineering\n",
    "\n",
    "| Approach | Knowledge Update | Cost | Use Case |\n",
    "|----------|-----------------|------|----------|\n",
    "| **RAG** | Easy (add documents) | Low | Dynamic knowledge, documents |\n",
    "| **Fine-Tuning** | Retrain model | High | Behavioral changes, style |\n",
    "| **Prompt Engineering** | Change prompt | Very Low | Simple tasks, known context |\n",
    "\n",
    "### When to Use RAG?\n",
    "\n",
    "‚úÖ **Use RAG when:**\n",
    "- Need to query large document collections\n",
    "- Knowledge changes frequently\n",
    "- Want to cite sources\n",
    "- Working with proprietary/internal data\n",
    "- Cost-effective solution needed\n",
    "\n",
    "‚ùå **Don't use RAG when:**\n",
    "- Simple Q&A with fixed knowledge\n",
    "- Real-time performance critical (<100ms)\n",
    "- No document/knowledge base available\n",
    "- Task doesn't need external knowledge\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d3c75",
   "metadata": {},
   "source": [
    "## 2. RAG Architecture\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   User      ‚îÇ\n",
    "‚îÇ   Query     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Query Embedding    ‚îÇ  ‚Üê Encode query to vector\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Vector Database    ‚îÇ  ‚Üê Find similar documents\n",
    "‚îÇ  (Similarity Search)‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Retrieved Documents ‚îÇ  ‚Üê Top-k most relevant chunks\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Prompt Construction ‚îÇ  ‚Üê Context + Query\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   LLM Generation    ‚îÇ  ‚Üê Generate answer\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Final Response     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2db211f",
   "metadata": {},
   "source": [
    "## 3. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install langchain chromadb sentence-transformers openai tiktoken pypdf python-docx\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# For vector storage\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# For text processing\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(\"\\nüì¶ Key Components:\")\n",
    "print(\"  - sentence-transformers: Generate embeddings\")\n",
    "print(\"  - chromadb: Vector database\")\n",
    "print(\"  - LangChain: RAG orchestration (optional)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2048f8d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Document Preparation & Chunking\n",
    "\n",
    "Chunking is critical for RAG performance. Good chunks:\n",
    "- Contain complete thoughts\n",
    "- Are not too large (context window) or too small (missing context)\n",
    "- Have overlap to maintain continuity\n",
    "\n",
    "### Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a4415",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentChunker:\n",
    "    \"\"\"Various strategies for chunking documents\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fixed_size_chunking(text: str, chunk_size: int = 512, overlap: int = 50) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into fixed-size chunks with overlap\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            chunk_size: Characters per chunk\n",
    "            overlap: Overlapping characters between chunks\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        text_length = len(text)\n",
    "        \n",
    "        while start < text_length:\n",
    "            end = start + chunk_size\n",
    "            chunk = text[start:end]\n",
    "            chunks.append(chunk)\n",
    "            start += (chunk_size - overlap)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def sentence_chunking(text: str, max_sentences: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text by sentences, grouping max_sentences together\n",
    "        \"\"\"\n",
    "        # Simple sentence splitting (can use spacy/nltk for better results)\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        chunks = []\n",
    "        for i in range(0, len(sentences), max_sentences):\n",
    "            chunk = '. '.join(sentences[i:i+max_sentences]) + '.'\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def paragraph_chunking(text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text by paragraphs (double newlines)\n",
    "        \"\"\"\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        chunks = [p.strip() for p in paragraphs if p.strip()]\n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def semantic_chunking(text: str, embedding_model, similarity_threshold: float = 0.7) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split based on semantic similarity (advanced)\n",
    "        Group sentences with similar meanings\n",
    "        \"\"\"\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        if len(sentences) <= 1:\n",
    "            return sentences\n",
    "        \n",
    "        # Generate embeddings for each sentence\n",
    "        embeddings = embedding_model.encode(sentences)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = [sentences[0]]\n",
    "        \n",
    "        for i in range(1, len(sentences)):\n",
    "            # Calculate similarity with previous sentence\n",
    "            similarity = np.dot(embeddings[i-1], embeddings[i]) / (\n",
    "                np.linalg.norm(embeddings[i-1]) * np.linalg.norm(embeddings[i])\n",
    "            )\n",
    "            \n",
    "            if similarity >= similarity_threshold:\n",
    "                current_chunk.append(sentences[i])\n",
    "            else:\n",
    "                chunks.append('. '.join(current_chunk) + '.')\n",
    "                current_chunk = [sentences[i]]\n",
    "        \n",
    "        # Add last chunk\n",
    "        if current_chunk:\n",
    "            chunks.append('. '.join(current_chunk) + '.')\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Example semiconductor documentation\n",
    "sample_stdf_doc = \"\"\"\n",
    "STDF (Standard Test Data Format) is the industry standard for semiconductor test data.\n",
    "It was developed by Teradyne in the 1980s and has become widely adopted.\n",
    "\n",
    "The PTR (Parametric Test Record) is one of the most important record types in STDF.\n",
    "It contains the results of parametric tests performed on devices.\n",
    "Each PTR includes test number, test result, and pass/fail status.\n",
    "\n",
    "The FTR (Functional Test Record) stores functional test results.\n",
    "Unlike parametric tests, functional tests verify digital logic operations.\n",
    "FTR records include test number and binary pass/fail results.\n",
    "\n",
    "STDF files are binary files that improve storage efficiency.\n",
    "They use a specific byte ordering and data type encoding.\n",
    "Reading STDF files requires specialized parsers.\n",
    "\n",
    "Common uses of STDF data include yield analysis and failure analysis.\n",
    "Engineers use STDF data to identify manufacturing defects.\n",
    "Statistical analysis of STDF data helps improve production processes.\n",
    "\"\"\"\n",
    "\n",
    "# Test different chunking strategies\n",
    "chunker = DocumentChunker()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHUNKING STRATEGY COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. Fixed Size Chunking (chunk_size=200, overlap=50):\")\n",
    "fixed_chunks = chunker.fixed_size_chunking(sample_stdf_doc, chunk_size=200, overlap=50)\n",
    "for i, chunk in enumerate(fixed_chunks, 1):\n",
    "    print(f\"\\n  Chunk {i} ({len(chunk)} chars):\")\n",
    "    print(f\"  {chunk[:100]}...\")\n",
    "\n",
    "print(\"\\n2. Sentence Chunking (max_sentences=3):\")\n",
    "sentence_chunks = chunker.sentence_chunking(sample_stdf_doc, max_sentences=3)\n",
    "for i, chunk in enumerate(sentence_chunks, 1):\n",
    "    print(f\"\\n  Chunk {i}:\")\n",
    "    print(f\"  {chunk}\")\n",
    "\n",
    "print(\"\\n3. Paragraph Chunking:\")\n",
    "paragraph_chunks = chunker.paragraph_chunking(sample_stdf_doc)\n",
    "for i, chunk in enumerate(paragraph_chunks, 1):\n",
    "    print(f\"\\n  Chunk {i}:\")\n",
    "    print(f\"  {chunk}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Fixed Size: {len(fixed_chunks)} chunks\")\n",
    "print(f\"Sentence-based: {len(sentence_chunks)} chunks\")\n",
    "print(f\"Paragraph-based: {len(paragraph_chunks)} chunks\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac9bcb",
   "metadata": {},
   "source": [
    "### Choosing the Right Chunking Strategy\n",
    "\n",
    "| Strategy | Pros | Cons | Best For |\n",
    "|----------|------|------|----------|\n",
    "| **Fixed Size** | Simple, consistent size | May split mid-sentence | General purpose |\n",
    "| **Sentence** | Complete thoughts | Variable size | Q&A systems |\n",
    "| **Paragraph** | Natural boundaries | Can be too large | Long-form content |\n",
    "| **Semantic** | Meaningful groups | Computationally expensive | High-quality retrieval |\n",
    "\n",
    "**Recommendation for STDF docs**: Paragraph or Sentence chunking (technical documentation has clear structure)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69dfef",
   "metadata": {},
   "source": [
    "## 5. Embeddings - Converting Text to Vectors\n",
    "\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678628e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"‚úÖ Model loaded: {embedding_model}\")\n",
    "print(f\"   Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Generate embeddings for sample chunks\n",
    "chunks_to_embed = paragraph_chunks[:3]  # Use first 3 paragraphs\n",
    "\n",
    "embeddings = embedding_model.encode(chunks_to_embed)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EMBEDDING GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, (chunk, embedding) in enumerate(zip(chunks_to_embed, embeddings), 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  Text (first 80 chars): {chunk[:80]}...\")\n",
    "    print(f\"  Embedding shape: {embedding.shape}\")\n",
    "    print(f\"  First 10 dimensions: {embedding[:10]}\")\n",
    "    print(f\"  Embedding norm: {np.linalg.norm(embedding):.4f}\")\n",
    "\n",
    "# Demonstrate semantic similarity\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SEMANTIC SIMILARITY DEMO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_queries = [\n",
    "    \"What is STDF?\",\n",
    "    \"Tell me about PTR records\",\n",
    "    \"How to read STDF files\",\n",
    "    \"What is the weather like?\"  # Irrelevant query\n",
    "]\n",
    "\n",
    "query_embeddings = embedding_model.encode(test_queries)\n",
    "\n",
    "# Calculate similarity with each chunk\n",
    "for query, query_emb in zip(test_queries, query_embeddings):\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"  Similarities with chunks:\")\n",
    "    \n",
    "    for i, chunk_emb in enumerate(embeddings, 1):\n",
    "        # Cosine similarity\n",
    "        similarity = np.dot(query_emb, chunk_emb) / (\n",
    "            np.linalg.norm(query_emb) * np.linalg.norm(chunk_emb)\n",
    "        )\n",
    "        print(f\"    Chunk {i}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0811dd2",
   "metadata": {},
   "source": [
    "### Popular Embedding Models\n",
    "\n",
    "| Model | Dim | Speed | Quality | Use Case |\n",
    "|-------|-----|-------|---------|----------|\n",
    "| **all-MiniLM-L6-v2** | 384 | Fast | Good | General purpose |\n",
    "| **all-mpnet-base-v2** | 768 | Medium | Better | Higher quality |\n",
    "| **text-embedding-ada-002** (OpenAI) | 1536 | API | Best | Production |\n",
    "| **instructor-large** | 768 | Medium | Domain-specific | Specialized |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd38d9f",
   "metadata": {},
   "source": [
    "## 6. Vector Database - ChromaDB\n",
    "\n",
    "Store and retrieve embeddings efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa388f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB\n",
    "chroma_client = chromadb.Client(Settings(\n",
    "    anonymized_telemetry=False,\n",
    "    is_persistent=False  # In-memory for demo\n",
    "))\n",
    "\n",
    "# Create a collection\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"stdf_documentation\",\n",
    "    metadata={\"description\": \"STDF specification and test data documentation\"}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ChromaDB collection created\")\n",
    "\n",
    "# Prepare documents for insertion\n",
    "documents = paragraph_chunks\n",
    "doc_embeddings = embedding_model.encode(documents)\n",
    "\n",
    "# Add documents to collection\n",
    "collection.add(\n",
    "    embeddings=doc_embeddings.tolist(),\n",
    "    documents=documents,\n",
    "    ids=[f\"doc_{i}\" for i in range(len(documents))],\n",
    "    metadatas=[{\"source\": \"STDF_spec\", \"chunk_id\": i} for i in range(len(documents))]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Added {len(documents)} documents to vector database\")\n",
    "print(f\"   Collection size: {collection.count()} documents\")\n",
    "\n",
    "# Query the collection\n",
    "def query_rag_system(query: str, n_results: int = 3):\n",
    "    \"\"\"Query the RAG system and return relevant chunks\"\"\"\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    \n",
    "    # Search vector database\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test queries\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RAG RETRIEVAL DEMO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_queries = [\n",
    "    \"What is PTR record in STDF?\",\n",
    "    \"How is STDF data analyzed?\",\n",
    "    \"What are functional test records?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print('='*70)\n",
    "    \n",
    "    results = query_rag_system(query, n_results=2)\n",
    "    \n",
    "    for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0]), 1):\n",
    "        print(f\"\\n  Result {i} (distance: {distance:.4f}):\")\n",
    "        print(f\"  {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e03d1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Complete RAG Pipeline\n",
    "\n",
    "Now let's build a complete RAG system that generates answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969ec42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAGSystem:\n",
    "    \"\"\"\n",
    "    A simple RAG system without external LLM API\n",
    "    (For demonstration - in production, use GPT-4, Claude, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model, vector_db_collection):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.collection = vector_db_collection\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve relevant documents\"\"\"\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_model.encode([query])[0]\n",
    "        \n",
    "        # Search vector database\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=top_k\n",
    "        )\n",
    "        \n",
    "        # Format results\n",
    "        retrieved_docs = []\n",
    "        for i in range(len(results['documents'][0])):\n",
    "            retrieved_docs.append({\n",
    "                'content': results['documents'][0][i],\n",
    "                'distance': results['distances'][0][i],\n",
    "                'metadata': results['metadatas'][0][i]\n",
    "            })\n",
    "        \n",
    "        return retrieved_docs\n",
    "    \n",
    "    def generate_response(self, query: str, retrieved_docs: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        Generate response based on retrieved documents\n",
    "        (Simplified - in production, use actual LLM)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Build context from retrieved documents\n",
    "        context = \"\\n\\n\".join([f\"Context {i+1}:\\n{doc['content']}\" \n",
    "                               for i, doc in enumerate(retrieved_docs)])\n",
    "        \n",
    "        # In production, you would do:\n",
    "        # prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "        # response = llm.generate(prompt)\n",
    "        \n",
    "        # For demo, we'll return a formatted response\n",
    "        response = f\"\"\"\n",
    "Based on the retrieved documentation:\n",
    "\n",
    "{context}\n",
    "\n",
    "[In production, an LLM would generate a natural language answer here based on the context above]\n",
    "\n",
    "Query: {query}\n",
    "Number of sources: {len(retrieved_docs)}\n",
    "Average relevance: {np.mean([doc['distance'] for doc in retrieved_docs]):.4f}\n",
    "        \"\"\"\n",
    "        \n",
    "        return response.strip()\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 3, verbose: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Complete RAG query pipeline\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with query, retrieved docs, and generated response\n",
    "        \"\"\"\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"üîç Processing query: {question}\")\n",
    "            print(f\"üìö Retrieving top {top_k} documents...\")\n",
    "        \n",
    "        # Step 1: Retrieve relevant documents\n",
    "        retrieved_docs = self.retrieve(question, top_k=top_k)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"‚úÖ Retrieved {len(retrieved_docs)} documents\")\n",
    "            for i, doc in enumerate(retrieved_docs, 1):\n",
    "                print(f\"   {i}. Distance: {doc['distance']:.4f}\")\n",
    "        \n",
    "        # Step 2: Generate response\n",
    "        if verbose:\n",
    "            print(f\"üí≠ Generating response...\")\n",
    "        \n",
    "        response = self.generate_response(question, retrieved_docs)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"‚úÖ Response generated\")\n",
    "        \n",
    "        return {\n",
    "            'query': question,\n",
    "            'retrieved_docs': retrieved_docs,\n",
    "            'response': response,\n",
    "            'num_sources': len(retrieved_docs)\n",
    "        }\n",
    "\n",
    "# Initialize RAG system\n",
    "rag_system = SimpleRAGSystem(embedding_model, collection)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPLETE RAG SYSTEM DEMO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test the RAG system\n",
    "questions = [\n",
    "    \"What is STDF and why is it used?\",\n",
    "    \"Explain PTR and FTR records\",\n",
    "    \"How do engineers use STDF data?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    result = rag_system.query(question, top_k=2)\n",
    "    print(f\"\\n{result['response']}\")\n",
    "    print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf44101a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Advanced RAG Techniques\n",
    "\n",
    "### 8.1 Hybrid Search (Keyword + Semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class HybridRAGSystem(SimpleRAGSystem):\n",
    "    \"\"\"RAG with hybrid search (semantic + keyword)\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model, vector_db_collection, documents):\n",
    "        super().__init__(embedding_model, vector_db_collection)\n",
    "        \n",
    "        # Build TF-IDF index for keyword search\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(documents)\n",
    "        self.documents = documents\n",
    "    \n",
    "    def keyword_search(self, query: str, top_k: int = 3) -> List[int]:\n",
    "        \"\"\"Perform keyword-based search using TF-IDF\"\"\"\n",
    "        query_vec = self.tfidf_vectorizer.transform([query])\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarities = (self.tfidf_matrix @ query_vec.T).toarray().flatten()\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        return top_indices.tolist()\n",
    "    \n",
    "    def hybrid_retrieve(self, query: str, top_k: int = 3, alpha: float = 0.5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Hybrid retrieval combining semantic and keyword search\n",
    "        \n",
    "        Args:\n",
    "            alpha: Weight for semantic search (1-alpha for keyword)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Semantic search\n",
    "        semantic_results = self.retrieve(query, top_k=top_k*2)\n",
    "        \n",
    "        # Keyword search\n",
    "        keyword_indices = self.keyword_search(query, top_k=top_k*2)\n",
    "        \n",
    "        # Combine scores (simplified - in production, use reciprocal rank fusion)\n",
    "        combined_scores = {}\n",
    "        \n",
    "        for i, doc in enumerate(semantic_results):\n",
    "            doc_id = doc['metadata']['chunk_id']\n",
    "            # Lower distance = higher relevance, so invert\n",
    "            combined_scores[doc_id] = alpha * (1 - doc['distance'])\n",
    "        \n",
    "        for rank, idx in enumerate(keyword_indices):\n",
    "            score = (1 - alpha) * (1 - rank / len(keyword_indices))\n",
    "            if idx in combined_scores:\n",
    "                combined_scores[idx] += score\n",
    "            else:\n",
    "                combined_scores[idx] = score\n",
    "        \n",
    "        # Sort by combined score\n",
    "        sorted_indices = sorted(combined_scores.items(), \n",
    "                               key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        \n",
    "        # Retrieve full documents\n",
    "        results = []\n",
    "        for doc_id, score in sorted_indices:\n",
    "            results.append({\n",
    "                'content': self.documents[doc_id],\n",
    "                'score': score,\n",
    "                'metadata': {'chunk_id': doc_id, 'source': 'hybrid'}\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test hybrid search\n",
    "hybrid_rag = HybridRAGSystem(embedding_model, collection, documents)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HYBRID SEARCH COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "query = \"parametric test PTR\"\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "\n",
    "print(\"\\n1. Semantic Search Only:\")\n",
    "semantic_results = rag_system.retrieve(query, top_k=3)\n",
    "for i, doc in enumerate(semantic_results, 1):\n",
    "    print(f\"   {i}. {doc['content'][:100]}... (distance: {doc['distance']:.4f})\")\n",
    "\n",
    "print(\"\\n2. Hybrid Search (Semantic + Keyword):\")\n",
    "hybrid_results = hybrid_rag.hybrid_retrieve(query, top_k=3, alpha=0.7)\n",
    "for i, doc in enumerate(hybrid_results, 1):\n",
    "    print(f\"   {i}. {doc['content'][:100]}... (score: {doc['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0b475",
   "metadata": {},
   "source": [
    "### 8.2 Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db33b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(original_query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Expand query with synonyms and related terms\n",
    "    (In production, use LLM for sophisticated expansion)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simple synonym/related terms dictionary\n",
    "    expansions = {\n",
    "        'PTR': ['Parametric Test Record', 'parametric test'],\n",
    "        'FTR': ['Functional Test Record', 'functional test'],\n",
    "        'STDF': ['Standard Test Data Format', 'test data format'],\n",
    "        'test': ['testing', 'measurement', 'evaluation'],\n",
    "        'record': ['data', 'entry', 'information'],\n",
    "    }\n",
    "    \n",
    "    # Expand query\n",
    "    expanded_queries = [original_query]\n",
    "    \n",
    "    for term, synonyms in expansions.items():\n",
    "        if term.lower() in original_query.lower():\n",
    "            for synonym in synonyms:\n",
    "                expanded = original_query.replace(term, synonym)\n",
    "                if expanded != original_query:\n",
    "                    expanded_queries.append(expanded)\n",
    "    \n",
    "    return expanded_queries\n",
    "\n",
    "# Test query expansion\n",
    "original = \"What is PTR record format?\"\n",
    "expanded = expand_query(original)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"QUERY EXPANSION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Original: {original}\")\n",
    "print(f\"\\nExpanded queries:\")\n",
    "for i, eq in enumerate(expanded, 1):\n",
    "    print(f\"  {i}. {eq}\")\n",
    "\n",
    "# Retrieve with expanded queries\n",
    "all_results = []\n",
    "for eq in expanded:\n",
    "    results = rag_system.retrieve(eq, top_k=2)\n",
    "    all_results.extend(results)\n",
    "\n",
    "# Deduplicate by content\n",
    "seen_content = set()\n",
    "unique_results = []\n",
    "for doc in all_results:\n",
    "    if doc['content'] not in seen_content:\n",
    "        seen_content.add(doc['content'])\n",
    "        unique_results.append(doc)\n",
    "\n",
    "print(f\"\\nüìö Retrieved {len(unique_results)} unique documents using query expansion\")\n",
    "for i, doc in enumerate(unique_results[:3], 1):\n",
    "    print(f\"\\n  {i}. {doc['content'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b27ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Real-World Project: STDF Documentation RAG\n",
    "\n",
    "Let's build a comprehensive RAG system for semiconductor test documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada0bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended STDF documentation\n",
    "comprehensive_stdf_docs = [\n",
    "    \"\"\"\n",
    "    STDF Overview: The Standard Test Data Format (STDF) is a binary format \n",
    "    for semiconductor test data. It was developed by Teradyne in 1988 and \n",
    "    is now the industry standard. STDF files store test results from \n",
    "    Automatic Test Equipment (ATE) used in semiconductor manufacturing.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    File Structure: STDF files consist of a series of records. Each record \n",
    "    has a header containing record type and length, followed by the data fields. \n",
    "    Records include FAR (File Attributes Record), MIR (Master Information Record), \n",
    "    and various test result records.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    PTR - Parametric Test Record: Contains results from parametric tests that \n",
    "    measure electrical characteristics. Fields include TEST_NUM (test number), \n",
    "    RESULT (measured value), LO_LIMIT and HI_LIMIT (specification limits), \n",
    "    UNITS (measurement units), and TEST_FLG (pass/fail status). Used for voltage, \n",
    "    current, frequency measurements.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    FTR - Functional Test Record: Stores results from functional tests that verify \n",
    "    digital logic operations. Contains TEST_NUM, CYCL_CNT (number of cycles executed), \n",
    "    REL_VADR (failing vector address if applicable), and pass/fail status. Used for \n",
    "    testing logical functionality of devices.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    MPR - Multiple Result Parametric Record: Similar to PTR but can store multiple \n",
    "    test results in a single record. Useful for tests that produce arrays of values, \n",
    "    such as pin-to-pin measurements or sampled waveforms. Contains arrays of results \n",
    "    rather than single values.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Data Analysis: STDF data is analyzed for yield improvement and failure analysis. \n",
    "    Common analyses include: (1) Yield calculation and trending, (2) Pareto analysis \n",
    "    of failure modes, (3) Correlation between test parameters, (4) Wafer mapping, \n",
    "    (5) Outlier detection, and (6) Statistical process control.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Parsing STDF: Reading STDF requires handling binary data with specific byte ordering \n",
    "    (little-endian). Each field has a defined data type (U1, U2, U4, I1, I2, I4, R4, R8, \n",
    "    Cn, Bn, etc.). Python libraries like pystdf can parse STDF files. Must handle \n",
    "    variable-length records and arrays.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Test Flow: A typical test flow in STDF includes: (1) File setup records (FAR, MIR), \n",
    "    (2) Wafer and device information (WIR, WRR, PIR, PRR), (3) Test definitions (TSR), \n",
    "    (4) Test results (PTR, FTR, MPR), (5) Bin summary (SBR, HBR), and (6) File close (MRR).\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Best Practices: When working with STDF data: (1) Always validate record structure, \n",
    "    (2) Handle missing or corrupt data gracefully, (3) Use appropriate tools for analysis, \n",
    "    (4) Consider data volume (files can be gigabytes), (5) Implement caching for large datasets, \n",
    "    (6) Document test specifications clearly.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    STDF V4: The current standard is STDF V4, which supports modern test equipment features. \n",
    "    It includes records for wafer test, final test, and package test. Supports multiple sites \n",
    "    (parallel testing), complex test structures, and extended device information. Backward \n",
    "    compatible with STDF V3.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Create production-ready RAG system\n",
    "class ProductionRAGSystem:\n",
    "    \"\"\"\n",
    "    Production-ready RAG system for STDF documentation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model_name='all-MiniLM-L6-v2'):\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        self.chroma_client = chromadb.Client(Settings(\n",
    "            anonymized_telemetry=False,\n",
    "            is_persistent=False\n",
    "        ))\n",
    "        self.collection = None\n",
    "        self.documents = []\n",
    "        \n",
    "    def index_documents(self, documents: List[str], collection_name: str = \"stdf_kb\"):\n",
    "        \"\"\"Index documents into vector database\"\"\"\n",
    "        \n",
    "        # Clean and chunk documents\n",
    "        processed_docs = []\n",
    "        for doc in documents:\n",
    "            # Clean whitespace\n",
    "            doc = ' '.join(doc.split())\n",
    "            processed_docs.append(doc)\n",
    "        \n",
    "        self.documents = processed_docs\n",
    "        \n",
    "        # Generate embeddings\n",
    "        print(f\"üîÑ Generating embeddings for {len(processed_docs)} documents...\")\n",
    "        embeddings = self.embedding_model.encode(processed_docs, show_progress_bar=True)\n",
    "        \n",
    "        # Create collection\n",
    "        if self.collection:\n",
    "            self.chroma_client.delete_collection(collection_name)\n",
    "        \n",
    "        self.collection = self.chroma_client.create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"description\": \"STDF knowledge base\"}\n",
    "        )\n",
    "        \n",
    "        # Add to vector database\n",
    "        self.collection.add(\n",
    "            embeddings=embeddings.tolist(),\n",
    "            documents=processed_docs,\n",
    "            ids=[f\"stdf_doc_{i}\" for i in range(len(processed_docs))],\n",
    "            metadatas=[{\"doc_id\": i, \"category\": \"stdf\"} for i in range(len(processed_docs))]\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Indexed {len(processed_docs)} documents\")\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Search knowledge base\"\"\"\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_model.encode([query])[0]\n",
    "        \n",
    "        # Search\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=top_k\n",
    "        )\n",
    "        \n",
    "        # Format results\n",
    "        search_results = {\n",
    "            'query': query,\n",
    "            'results': []\n",
    "        }\n",
    "        \n",
    "        for i in range(len(results['documents'][0])):\n",
    "            search_results['results'].append({\n",
    "                'content': results['documents'][0][i],\n",
    "                'distance': results['distances'][0][i],\n",
    "                'metadata': results['metadatas'][0][i]\n",
    "            })\n",
    "        \n",
    "        return search_results\n",
    "    \n",
    "    def answer_question(self, question: str, top_k: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Answer question using RAG\n",
    "        (In production, integrate with OpenAI/Anthropic API)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Retrieve relevant docs\n",
    "        search_results = self.search(question, top_k=top_k)\n",
    "        \n",
    "        # Build context\n",
    "        context_parts = []\n",
    "        for i, result in enumerate(search_results['results'], 1):\n",
    "            context_parts.append(f\"[Source {i}]\\n{result['content']}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # In production:\n",
    "        # prompt = f\"\"\"Based on the following context, answer the question.\n",
    "        #\n",
    "        # Context:\n",
    "        # {context}\n",
    "        #\n",
    "        # Question: {question}\n",
    "        #\n",
    "        # Answer:\"\"\"\n",
    "        #\n",
    "        # answer = openai.ChatCompletion.create(\n",
    "        #     model=\"gpt-4\",\n",
    "        #     messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        # )\n",
    "        \n",
    "        # For demo, return formatted context\n",
    "        answer = f\"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Relevant Information Found:\n",
    "{context}\n",
    "\n",
    "[An LLM would generate a natural language answer here synthesizing the above sources]\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        return answer\n",
    "\n",
    "# Initialize and index\n",
    "print(\"=\"*70)\n",
    "print(\"PRODUCTION STDF RAG SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "stdf_rag = ProductionRAGSystem()\n",
    "stdf_rag.index_documents(comprehensive_stdf_docs)\n",
    "\n",
    "# Test with various questions\n",
    "test_questions = [\n",
    "    \"What is the difference between PTR and FTR records?\",\n",
    "    \"How do I parse STDF files in Python?\",\n",
    "    \"What analysis can be done with STDF data?\",\n",
    "    \"Explain the structure of STDF V4\",\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print('='*70)\n",
    "    answer = stdf_rag.answer_question(question, top_k=2)\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87da0bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. RAG Evaluation Metrics\n",
    "\n",
    "How do we know if our RAG system is good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f832cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    \"\"\"Evaluate RAG system performance\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def retrieval_metrics(relevant_docs: List[str], retrieved_docs: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate retrieval metrics\n",
    "        \n",
    "        Args:\n",
    "            relevant_docs: List of actually relevant document IDs\n",
    "            retrieved_docs: List of retrieved document IDs\n",
    "        \"\"\"\n",
    "        relevant_set = set(relevant_docs)\n",
    "        retrieved_set = set(retrieved_docs)\n",
    "        \n",
    "        # True positives\n",
    "        tp = len(relevant_set & retrieved_set)\n",
    "        \n",
    "        # Precision: What fraction of retrieved docs are relevant?\n",
    "        precision = tp / len(retrieved_set) if retrieved_set else 0\n",
    "        \n",
    "        # Recall: What fraction of relevant docs were retrieved?\n",
    "        recall = tp / len(relevant_set) if relevant_set else 0\n",
    "        \n",
    "        # F1 Score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'retrieved_count': len(retrieved_set),\n",
    "            'relevant_count': len(relevant_set)\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def mrr(relevant_docs: List[str], retrieved_docs_ranked: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Mean Reciprocal Rank\n",
    "        Score based on position of first relevant document\n",
    "        \"\"\"\n",
    "        for i, doc in enumerate(retrieved_docs_ranked, 1):\n",
    "            if doc in relevant_docs:\n",
    "                return 1.0 / i\n",
    "        return 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def ndcg(relevant_docs: List[str], retrieved_docs_ranked: List[str], k: int = None) -> float:\n",
    "        \"\"\"\n",
    "        Normalized Discounted Cumulative Gain\n",
    "        Accounts for ranking quality\n",
    "        \"\"\"\n",
    "        if k:\n",
    "            retrieved_docs_ranked = retrieved_docs_ranked[:k]\n",
    "        \n",
    "        # Calculate DCG\n",
    "        dcg = 0.0\n",
    "        for i, doc in enumerate(retrieved_docs_ranked, 1):\n",
    "            if doc in relevant_docs:\n",
    "                dcg += 1.0 / np.log2(i + 1)\n",
    "        \n",
    "        # Calculate ideal DCG\n",
    "        idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant_docs), len(retrieved_docs_ranked))))\n",
    "        \n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "# Example evaluation\n",
    "print(\"=\"*70)\n",
    "print(\"RAG EVALUATION EXAMPLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simulate ground truth\n",
    "relevant_docs = ['stdf_doc_2', 'stdf_doc_3', 'stdf_doc_6']  # PTR, FTR, parsing\n",
    "retrieved_docs = ['stdf_doc_2', 'stdf_doc_3', 'stdf_doc_5', 'stdf_doc_1']\n",
    "\n",
    "evaluator = RAGEvaluator()\n",
    "\n",
    "metrics = evaluator.retrieval_metrics(relevant_docs, retrieved_docs)\n",
    "print(\"\\nRetrieval Metrics:\")\n",
    "print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {metrics['f1_score']:.4f}\")\n",
    "\n",
    "mrr_score = evaluator.mrr(relevant_docs, retrieved_docs)\n",
    "print(f\"\\nMean Reciprocal Rank: {mrr_score:.4f}\")\n",
    "\n",
    "ndcg_score = evaluator.ndcg(relevant_docs, retrieved_docs, k=3)\n",
    "print(f\"NDCG@3: {ndcg_score:.4f}\")\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(f\"  - Retrieved 2 out of 3 relevant documents\")\n",
    "print(f\"  - First relevant doc at position 1 (excellent)\")\n",
    "print(f\"  - Overall ranking quality: {ndcg_score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d392a16a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Advanced RAG Patterns\n",
    "\n",
    "### Common RAG Challenges & Solutions\n",
    "\n",
    "| Challenge | Solution | Implementation |\n",
    "|-----------|----------|----------------|\n",
    "| **Chunk size too large** | Reduce chunk size, increase overlap | Notebook 080 |\n",
    "| **Retrieval misses context** | Use parent-child chunking | Advanced RAG |\n",
    "| **Hallucination** | Add source citations, confidence scores | Prompt engineering |\n",
    "| **Slow retrieval** | Use approximate nearest neighbors, cache | Vector DB optimization |\n",
    "| **Multi-hop questions** | Iterative retrieval, chain of thought | Agentic RAG |\n",
    "| **Domain mismatch** | Fine-tune embeddings, use domain LLMs | Fine-tuning |\n",
    "\n",
    "### Next-Level RAG Techniques (Covered in 080):\n",
    "- üîπ **Re-ranking**: Two-stage retrieval for better precision\n",
    "- üîπ **Query transformation**: Rewrite queries for better matching\n",
    "- üîπ **Contextual compression**: Remove irrelevant parts of chunks\n",
    "- üîπ **Self-RAG**: Model checks if retrieved content is relevant\n",
    "- üîπ **Corrective RAG**: Falls back to web search if local docs insufficient\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56ae92f",
   "metadata": {},
   "source": [
    "## 12. Key Takeaways\n",
    "\n",
    "### ‚úÖ RAG is Essential When:\n",
    "- Building Q&A systems over documents\n",
    "- Need up-to-date information beyond training data\n",
    "- Want to cite sources and provide transparency\n",
    "- Working with proprietary/private data\n",
    "- Cost-effective alternative to fine-tuning\n",
    "\n",
    "### üéØ Best Practices:\n",
    "1. **Chunking**: Experiment with strategies, use overlap\n",
    "2. **Embeddings**: Choose model based on domain and speed requirements\n",
    "3. **Retrieval**: Start with top-k=3-5, tune based on performance\n",
    "4. **Evaluation**: Always measure precision, recall, and user satisfaction\n",
    "5. **Hybrid Search**: Combine semantic and keyword for robustness\n",
    "6. **Metadata**: Store source, date, section for filtering\n",
    "7. **Monitoring**: Track retrieval quality and user feedback\n",
    "\n",
    "### ‚ö†Ô∏è Common Pitfalls:\n",
    "- Chunks too large or too small\n",
    "- Not handling edge cases (empty results)\n",
    "- Ignoring retrieval quality metrics\n",
    "- Over-relying on retrieval without LLM verification\n",
    "- Not updating knowledge base regularly\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16bc242",
   "metadata": {},
   "source": [
    "## 13. Practice Exercises\n",
    "\n",
    "### Exercise 1: Custom Chunking\n",
    "Implement a chunking strategy optimized for technical specifications with:\n",
    "- Section headers preserved\n",
    "- Tables kept together\n",
    "- Code blocks not split\n",
    "\n",
    "### Exercise 2: Multi-Document RAG\n",
    "Extend the RAG system to handle multiple document types:\n",
    "- PDF files (specs)\n",
    "- Code files (examples)\n",
    "- Test logs (results)\n",
    "\n",
    "### Exercise 3: RAG with Citations\n",
    "Modify the system to return citations with each answer:\n",
    "- Document name\n",
    "- Page/section number\n",
    "- Relevance score\n",
    "\n",
    "### Exercise 4: Real STDF RAG\n",
    "Build a RAG system for your actual STDF documentation:\n",
    "- Index your company's test specs\n",
    "- Add test procedure documents\n",
    "- Include troubleshooting guides\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e48b301",
   "metadata": {},
   "source": [
    "## 14. Integration with LLMs\n",
    "\n",
    "### Using OpenAI API (Production Example)\n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "def rag_with_gpt4(question: str, retrieved_docs: List[str]) -> str:\n",
    "    \"\"\"RAG with GPT-4\"\"\"\n",
    "    \n",
    "    context = \"\\n\\n\".join([f\"Source {i+1}:\\n{doc}\" \n",
    "                          for i, doc in enumerate(retrieved_docs)])\n",
    "    \n",
    "    prompt = f\"\"\"You are a semiconductor test engineer assistant. \n",
    "Answer the question based on the provided STDF documentation context.\n",
    "If the answer is not in the context, say \"I don't find that information in the provided documentation.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful semiconductor test engineer assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3  # Lower temperature for factual responses\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "```\n",
    "\n",
    "### Using LangChain (Simplified)\n",
    "\n",
    "```python\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Create retrieval chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Query\n",
    "result = qa_chain({\"query\": \"What is PTR record?\"})\n",
    "print(result['result'])\n",
    "print(result['source_documents'])\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9dc0ed",
   "metadata": {},
   "source": [
    "## 15. Next Steps\n",
    "\n",
    "**Congratulations! üéâ** You've mastered RAG fundamentals!\n",
    "\n",
    "### Continue Your Learning:\n",
    "- ‚Üí **080_Advanced_RAG_Techniques.ipynb** - Re-ranking, query transformation, self-RAG\n",
    "- ‚Üí **081_Knowledge_Graphs_Basics.ipynb** - Structured knowledge representation\n",
    "- ‚Üí **083_KG_Enhanced_RAG.ipynb** - Combine KG with RAG for better results\n",
    "- ‚Üí **084_Semantic_Search_Advanced.ipynb** - Advanced retrieval techniques\n",
    "- ‚Üí **086_LangChain_Framework.ipynb** - Production RAG with LangChain\n",
    "- ‚Üí **090_Production_Agent_Systems.ipynb** - Deploy RAG in production\n",
    "\n",
    "### Project Ideas:\n",
    "1. **STDF Documentation Assistant**: Complete Q&A system for test documentation\n",
    "2. **Failure Analysis RAG**: Retrieve similar historical failures\n",
    "3. **Test Procedure Guide**: Interactive guide with RAG\n",
    "4. **Multi-Modal RAG**: Combine text, images, and wafer maps\n",
    "\n",
    "---\n",
    "\n",
    "**You're now ready to build production RAG systems! üöÄ**\n",
    "\n",
    "**Next: Advanced RAG Techniques (080) ‚Üí**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
