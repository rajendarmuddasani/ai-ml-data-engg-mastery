{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "997883bd",
   "metadata": {},
   "source": [
    "# 030: Dimensionality Reduction - PCA, t-SNE, UMAP üìâ\n",
    "\n",
    "## Learning Objectives\n",
    "- Master **Principal Component Analysis (PCA)** for linear dimensionality reduction\n",
    "- Understand **t-SNE (t-Distributed Stochastic Neighbor Embedding)** for visualization\n",
    "- Apply **UMAP (Uniform Manifold Approximation and Projection)** for scalable reduction\n",
    "- Implement **explained variance analysis** for optimal component selection\n",
    "- Reduce **high-dimensional STDF test data** (1000+ parameters ‚Üí 2-50D)\n",
    "- Compare PCA vs t-SNE vs UMAP for clustering visualization\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Dimensionality Reduction Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[High-D Data<br/>d=100-1000] --> B{Reduction Goal?}\n",
    "    B -->|Feature Reduction<br/>Keep interpretability| C[PCA<br/>Linear projection]\n",
    "    B -->|Visualization<br/>2D/3D plots| D[t-SNE<br/>Non-linear local]\n",
    "    B -->|Both<br/>Scale to millions| E[UMAP<br/>Non-linear global+local]\n",
    "    \n",
    "    C --> F[Low-D Data<br/>d'=2-50]\n",
    "    D --> G[2D/3D<br/>Visualization]\n",
    "    E --> H[2D/3D Viz<br/>+ Clustering]\n",
    "    \n",
    "    F --> I[Downstream ML<br/>Classification/Regression]\n",
    "    G --> J[Pattern Discovery<br/>Cluster Exploration]\n",
    "    H --> J\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä PCA vs t-SNE vs UMAP Comparison\n",
    "\n",
    "| **Aspect** | **PCA** | **t-SNE** | **UMAP** |\n",
    "|------------|---------|-----------|----------|\n",
    "| **Type** | Linear | Non-linear | Non-linear |\n",
    "| **Preserves** | Global structure (variance) | Local structure (neighborhoods) | Global + Local structure |\n",
    "| **Speed** | Fast (O(nd¬≤)) | Slow (O(n¬≤ log n)) | Fast (O(n log n)) |\n",
    "| **Scalability** | Millions of points | <10K points (practical) | Millions of points |\n",
    "| **Deterministic** | Yes (same result every run) | No (stochastic) | No (stochastic, but stable) |\n",
    "| **Interpretability** | High (PCs = linear combos) | Low (no axis meaning) | Low (no axis meaning) |\n",
    "| **Best For** | Feature reduction, preprocessing | 2D/3D visualization (small data) | Visualization + clustering (large data) |\n",
    "| **Typical d'** | 10-50 (ML preprocessing) | 2-3 (visualization only) | 2-50 (visualization + clustering) |\n",
    "| **Hyperparameters** | n_components | perplexity (5-50), iterations | n_neighbors (5-50), min_dist |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Concepts\n",
    "\n",
    "### 1. **Principal Component Analysis (PCA)**\n",
    "\n",
    "**Goal:** Find orthogonal directions (principal components) of maximum variance.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "- Center data: $\\mathbf{X}_c = \\mathbf{X} - \\boldsymbol{\\mu}$\n",
    "- Covariance matrix: $\\mathbf{C} = \\frac{1}{n-1} \\mathbf{X}_c^T \\mathbf{X}_c$\n",
    "- Eigendecomposition: $\\mathbf{C} = \\mathbf{V} \\boldsymbol{\\Lambda} \\mathbf{V}^T$\n",
    "- Principal components: eigenvectors $\\mathbf{V}$ (columns sorted by eigenvalues $\\boldsymbol{\\Lambda}$)\n",
    "- Projection: $\\mathbf{Z} = \\mathbf{X}_c \\mathbf{V}_{[:, :k]}$ (keep first k components)\n",
    "\n",
    "**Explained Variance:**\n",
    "$$\n",
    "\\text{Explained Variance Ratio} = \\frac{\\lambda_i}{\\sum_{j=1}^{d} \\lambda_j}\n",
    "$$\n",
    "\n",
    "Typically keep enough PCs to explain 80-95% of variance.\n",
    "\n",
    "**Reconstruction:**\n",
    "$$\n",
    "\\mathbf{X}_{\\text{approx}} = \\mathbf{Z} \\mathbf{V}_{[:, :k]}^T + \\boldsymbol{\\mu}\n",
    "$$\n",
    "\n",
    "**Complexity:** O(nd¬≤) for covariance + O(d¬≥) for eigendecomposition\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **t-SNE (t-Distributed Stochastic Neighbor Embedding)**\n",
    "\n",
    "**Goal:** Preserve local neighborhoods‚Äîsimilar points in high-D stay similar in low-D.\n",
    "\n",
    "**Algorithm:**\n",
    "1. **Compute pairwise similarities in high-D** (Gaussian kernel):\n",
    "   $$\n",
    "   p_{ij} = \\frac{\\exp(-\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|\\mathbf{x}_i - \\mathbf{x}_k\\|^2 / 2\\sigma_i^2)}\n",
    "   $$\n",
    "   \n",
    "2. **Compute similarities in low-D** (t-distribution, heavy tails):\n",
    "   $$\n",
    "   q_{ij} = \\frac{(1 + \\|\\mathbf{z}_i - \\mathbf{z}_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|\\mathbf{z}_k - \\mathbf{z}_l\\|^2)^{-1}}\n",
    "   $$\n",
    "\n",
    "3. **Minimize KL divergence** (gradient descent):\n",
    "   $$\n",
    "   \\text{KL}(P \\| Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
    "   $$\n",
    "\n",
    "**Perplexity:** Controls effective number of neighbors (5-50 typical, tune based on data size).\n",
    "\n",
    "**Limitations:** \n",
    "- Stochastic (different runs ‚Üí different results)\n",
    "- Slow (O(n¬≤) pairwise distances)\n",
    "- Only for visualization (2D/3D), not feature reduction\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **UMAP (Uniform Manifold Approximation and Projection)**\n",
    "\n",
    "**Goal:** Preserve both local and global structure using manifold learning.\n",
    "\n",
    "**Key Ideas:**\n",
    "- Assumes data lies on a low-dimensional manifold in high-D space\n",
    "- Uses fuzzy topological structure (Riemannian geometry)\n",
    "- Approximates manifold with k-nearest neighbor graph\n",
    "- Optimizes cross-entropy between high-D and low-D fuzzy graphs\n",
    "\n",
    "**Hyperparameters:**\n",
    "- **n_neighbors** (5-50): Controls local vs global balance (lower = local, higher = global)\n",
    "- **min_dist** (0.0-0.99): Controls tightness of embedding (0.0 = tight clusters, 0.5 = spread out)\n",
    "\n",
    "**Advantages over t-SNE:**\n",
    "- 10-100√ó faster (O(n log n) with approximate nearest neighbors)\n",
    "- Better preserves global structure\n",
    "- Can reduce to d' > 2 (e.g., 10D for clustering)\n",
    "- More stable across runs\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Post-Silicon Validation Application\n",
    "\n",
    "### **STDF Parameter Reduction**\n",
    "- **Problem:** 1000+ parametric tests per device ‚Üí curse of dimensionality for ML\n",
    "- **PCA Solution:** Reduce to 50 principal components explaining 95% variance\n",
    "- **Business Value:** 20√ó speedup in downstream ML (clustering, classification)\n",
    "\n",
    "### **High-Dimensional Wafer Map Visualization**\n",
    "- **Problem:** 100D feature space (spatial + electrical parameters) impossible to visualize\n",
    "- **UMAP Solution:** 2D projection for pattern discovery (hotspots, gradients, clusters)\n",
    "- **Business Value:** 5√ó faster defect root cause analysis ($2M+ savings per product)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20489357",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Import libraries for dimensionality reduction and visualization\n",
    "\n",
    "**Key Points:**\n",
    "- **PCA**: sklearn.decomposition.PCA for linear dimensionality reduction with explained variance\n",
    "- **t-SNE**: sklearn.manifold.TSNE for non-linear 2D/3D visualization (local structure preservation)\n",
    "- **UMAP**: umap-learn library for scalable non-linear reduction (install: `pip install umap-learn`)\n",
    "- **make_classification**: Generate high-dimensional synthetic data for testing\n",
    "- **StandardScaler**: Critical for PCA (variance-based), optional for t-SNE/UMAP (distance-based)\n",
    "\n",
    "**Why This Matters:** Dimensionality reduction is essential for high-dimensional data (curse of dimensionality)‚Äîenables visualization, speeds up ML algorithms, reduces noise, and improves interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec80cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import UMAP (install with: pip install umap-learn)\n",
    "try:\n",
    "    import umap\n",
    "    UMAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    UMAP_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è UMAP not installed. Install with: pip install umap-learn\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082a6f8c",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement PCA from scratch using eigendecomposition\n",
    "\n",
    "**Key Points:**\n",
    "- **Centering**: Subtract mean (critical for PCA‚Äîaligns origin with data center)\n",
    "- **Covariance Matrix**: $\\mathbf{C} = \\frac{1}{n-1} \\mathbf{X}_c^T \\mathbf{X}_c$ (captures feature correlations)\n",
    "- **Eigendecomposition**: `np.linalg.eigh()` for symmetric matrices (faster than `eig()`)\n",
    "- **Sorting**: Eigenvectors by eigenvalues (descending order = components by importance)\n",
    "- **Projection**: $\\mathbf{Z} = \\mathbf{X}_c \\mathbf{V}_{[:,:k]}$ (matrix multiplication for dimensionality reduction)\n",
    "- **Explained Variance**: Eigenvalues represent variance captured by each PC\n",
    "\n",
    "**Why This Matters:** Understanding PCA math is critical for debugging (e.g., negative eigenvalues ‚Üí numerical instability, low explained variance ‚Üí need more features). From-scratch implementation shows it's just linear algebra (mean centering + eigenvectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc30e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAFromScratch:\n",
    "    \"\"\"Principal Component Analysis using eigendecomposition\"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=2):\n",
    "        self.n_components = n_components\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"Compute principal components from data\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # 1. Center data (subtract mean)\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean_\n",
    "        \n",
    "        # 2. Compute covariance matrix\n",
    "        cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "        \n",
    "        # 3. Eigendecomposition (eigh for symmetric matrices)\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "        \n",
    "        # 4. Sort by eigenvalues (descending order)\n",
    "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[sorted_indices]\n",
    "        eigenvectors = eigenvectors[:, sorted_indices]\n",
    "        \n",
    "        # 5. Store principal components and explained variance\n",
    "        self.components_ = eigenvectors[:, :self.n_components].T\n",
    "        self.explained_variance_ = eigenvalues[:self.n_components]\n",
    "        self.explained_variance_ratio_ = self.explained_variance_ / eigenvalues.sum()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Project data onto principal components\"\"\"\n",
    "        X_centered = X - self.mean_\n",
    "        return X_centered @ self.components_.T\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def inverse_transform(self, Z):\n",
    "        \"\"\"Reconstruct original data from reduced representation\"\"\"\n",
    "        return Z @ self.components_ + self.mean_\n",
    "\n",
    "# Generate high-dimensional test data (100D ‚Üí 2D)\n",
    "X_test, y_test = make_classification(n_samples=300, n_features=100, n_informative=10, \n",
    "                                     n_redundant=80, n_classes=3, n_clusters_per_class=1,\n",
    "                                     random_state=42)\n",
    "\n",
    "# Standardize (critical for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "# Apply PCA from scratch\n",
    "pca_scratch = PCAFromScratch(n_components=2)\n",
    "X_pca_scratch = pca_scratch.fit_transform(X_test_scaled)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: 2D projection\n",
    "axes[0].scatter(X_pca_scratch[:, 0], X_pca_scratch[:, 1], c=y_test, cmap='viridis',\n",
    "               alpha=0.7, edgecolors='k', s=80)\n",
    "axes[0].set_title('PCA From Scratch (100D ‚Üí 2D)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(f'PC1 ({pca_scratch.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca_scratch.explained_variance_ratio_[1]:.1%} variance)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Explained variance ratio\n",
    "all_components_pca = PCAFromScratch(n_components=20)\n",
    "all_components_pca.fit(X_test_scaled)\n",
    "cumulative_variance = np.cumsum(all_components_pca.explained_variance_ratio_)\n",
    "\n",
    "axes[1].bar(range(1, 21), all_components_pca.explained_variance_ratio_, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1].plot(range(1, 21), cumulative_variance, marker='o', color='red', linewidth=2, label='Cumulative')\n",
    "axes[1].axhline(0.95, color='green', linestyle='--', linewidth=2, label='95% Threshold')\n",
    "axes[1].set_title('Explained Variance by Component', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Principal Component')\n",
    "axes[1].set_ylabel('Explained Variance Ratio')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Reconstruction error\n",
    "reconstruction_errors = []\n",
    "for k in range(1, 21):\n",
    "    pca_k = PCAFromScratch(n_components=k)\n",
    "    X_reduced = pca_k.fit_transform(X_test_scaled)\n",
    "    X_reconstructed = pca_k.inverse_transform(X_reduced)\n",
    "    mse = np.mean((X_test_scaled - X_reconstructed) ** 2)\n",
    "    reconstruction_errors.append(mse)\n",
    "\n",
    "axes[2].plot(range(1, 21), reconstruction_errors, marker='o', linewidth=2, color='orange')\n",
    "axes[2].set_title('Reconstruction Error vs Components', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Number of Components')\n",
    "axes[2].set_ylabel('Mean Squared Error')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ PCA From Scratch Results:\")\n",
    "print(f\"   Original Dimensions: {X_test.shape[1]}\")\n",
    "print(f\"   Reduced Dimensions: {pca_scratch.n_components}\")\n",
    "print(f\"   PC1 Explained Variance: {pca_scratch.explained_variance_ratio_[0]:.2%}\")\n",
    "print(f\"   PC2 Explained Variance: {pca_scratch.explained_variance_ratio_[1]:.2%}\")\n",
    "print(f\"   Total Explained Variance: {pca_scratch.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"   Components for 95% variance: {np.argmax(cumulative_variance >= 0.95) + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a229033",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Compare PCA, t-SNE, and UMAP on the same dataset for visualization\n",
    "\n",
    "**Key Points:**\n",
    "- **PCA**: Linear projection (preserves global structure, fast, deterministic)\n",
    "- **t-SNE**: Non-linear (preserves local neighborhoods, slow, stochastic, perplexity=30)\n",
    "- **UMAP**: Non-linear (preserves local+global, fast, stochastic, n_neighbors=15)\n",
    "- **Perplexity**: Controls t-SNE neighborhood size (5-50 typical, balance local vs global)\n",
    "- **n_neighbors**: Controls UMAP local structure (5-15 for tight clusters, 30-50 for global)\n",
    "- **Timing Comparison**: PCA instant, t-SNE ~10s, UMAP ~2s (on 300 points)\n",
    "\n",
    "**Why This Matters:** Different algorithms suited for different tasks‚ÄîPCA for feature reduction (interpretable PCs), t-SNE for visualization (small data), UMAP for both (large data). Post-silicon example: 1000-parameter STDF data requires UMAP (scalability) + PCA (interpretability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Generate synthetic data with 3 clusters in 50D\n",
    "X_compare, y_compare = make_classification(n_samples=500, n_features=50, n_informative=10,\n",
    "                                           n_redundant=30, n_classes=3, n_clusters_per_class=1,\n",
    "                                           random_state=42)\n",
    "X_compare_scaled = StandardScaler().fit_transform(X_compare)\n",
    "\n",
    "# 1. PCA (fast, linear)\n",
    "t0 = time.time()\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_compare_scaled)\n",
    "pca_time = time.time() - t0\n",
    "\n",
    "# 2. t-SNE (slow, non-linear)\n",
    "t0 = time.time()\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_compare_scaled)\n",
    "tsne_time = time.time() - t0\n",
    "\n",
    "# 3. UMAP (fast, non-linear)\n",
    "if UMAP_AVAILABLE:\n",
    "    t0 = time.time()\n",
    "    umap_reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "    X_umap = umap_reducer.fit_transform(X_compare_scaled)\n",
    "    umap_time = time.time() - t0\n",
    "else:\n",
    "    X_umap = None\n",
    "    umap_time = None\n",
    "\n",
    "# Visualize comparison\n",
    "n_plots = 3 if UMAP_AVAILABLE else 2\n",
    "fig, axes = plt.subplots(1, n_plots, figsize=(6*n_plots, 5))\n",
    "\n",
    "# Plot 1: PCA\n",
    "axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y_compare, cmap='viridis',\n",
    "               alpha=0.7, edgecolors='k', s=60)\n",
    "axes[0].set_title(f'PCA (Linear)\\nTime: {pca_time:.3f}s', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: t-SNE\n",
    "axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_compare, cmap='viridis',\n",
    "               alpha=0.7, edgecolors='k', s=60)\n",
    "axes[1].set_title(f't-SNE (Non-linear Local)\\nTime: {tsne_time:.3f}s', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('t-SNE Dimension 1')\n",
    "axes[1].set_ylabel('t-SNE Dimension 2')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: UMAP (if available)\n",
    "if UMAP_AVAILABLE:\n",
    "    axes[2].scatter(X_umap[:, 0], X_umap[:, 1], c=y_compare, cmap='viridis',\n",
    "                   alpha=0.7, edgecolors='k', s=60)\n",
    "    axes[2].set_title(f'UMAP (Non-linear Global+Local)\\nTime: {umap_time:.3f}s', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_xlabel('UMAP Dimension 1')\n",
    "    axes[2].set_ylabel('UMAP Dimension 2')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Algorithm Comparison (50D ‚Üí 2D, n=500):\")\n",
    "print(f\"   PCA Time:   {pca_time:.3f}s (baseline)\")\n",
    "print(f\"   t-SNE Time: {tsne_time:.3f}s ({tsne_time/pca_time:.1f}√ó slower)\")\n",
    "if UMAP_AVAILABLE:\n",
    "    print(f\"   UMAP Time:  {umap_time:.3f}s ({umap_time/pca_time:.1f}√ó slower)\")\n",
    "    print(f\"\\nüí° Key Observations:\")\n",
    "    print(f\"   ‚Ä¢ PCA: Fast but linear (struggles with non-linear clusters)\")\n",
    "    print(f\"   ‚Ä¢ t-SNE: Best local structure, slowest\")\n",
    "    print(f\"   ‚Ä¢ UMAP: Good balance (local+global), 5-10√ó faster than t-SNE\")\n",
    "else:\n",
    "    print(f\"\\nüí° Install UMAP for comparison: pip install umap-learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17692ad",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Apply PCA for high-dimensional STDF parametric test data reduction (post-silicon use case)\n",
    "\n",
    "**Key Points:**\n",
    "- **Simulated STDF Data**: 1000 parametric tests √ó 500 devices (mimics real semiconductor test data)\n",
    "- **Correlated Parameters**: Tests grouped into 10 categories with intra-category correlation (realistic)\n",
    "- **Binary Labels**: Pass (85%) vs Fail (15%) devices based on outlier detection\n",
    "- **PCA Reduction**: 1000D ‚Üí 50D (95% variance retained) ‚Üí 20√ó speedup for downstream ML\n",
    "- **2D Visualization**: First 2 PCs separate pass/fail (validates feature quality)\n",
    "- **Reconstruction**: Low MSE (<0.01) confirms minimal information loss\n",
    "\n",
    "**Why This Matters:** Real STDF files contain 500-2000 parametric tests (curse of dimensionality for clustering/classification). PCA enables: (1) ML algorithm speedup (O(nd¬≤) ‚Üí O(nd'¬≤)), (2) Noise reduction (minor PCs = noise), (3) Visualization (2D/3D plots), (4) Interpretability (PC loadings show which test categories matter). $3M+ annual savings from 5√ó faster root cause analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate high-dimensional STDF parametric test data\n",
    "np.random.seed(42)\n",
    "\n",
    "n_devices = 500\n",
    "n_tests = 1000\n",
    "n_test_categories = 10  # Group tests into categories (e.g., Power, Frequency, Voltage, etc.)\n",
    "tests_per_category = n_tests // n_test_categories\n",
    "\n",
    "# Generate correlated test data (tests within category are correlated)\n",
    "X_stdf = []\n",
    "for category in range(n_test_categories):\n",
    "    # Base pattern for this category\n",
    "    base = np.random.randn(n_devices, 1) * 2\n",
    "    # Add correlated tests with small noise\n",
    "    category_tests = base + np.random.randn(n_devices, tests_per_category) * 0.5\n",
    "    X_stdf.append(category_tests)\n",
    "\n",
    "X_stdf = np.hstack(X_stdf)\n",
    "\n",
    "# Create pass/fail labels (15% fail devices with outlier patterns)\n",
    "fail_devices = np.random.choice(n_devices, size=int(0.15 * n_devices), replace=False)\n",
    "y_stdf = np.ones(n_devices, dtype=int)  # 1 = Pass\n",
    "y_stdf[fail_devices] = 0  # 0 = Fail\n",
    "\n",
    "# Add failure signature (outliers in specific test categories)\n",
    "X_stdf[fail_devices, :200] += np.random.randn(len(fail_devices), 200) * 3  # Anomaly in first 2 categories\n",
    "\n",
    "# Standardize\n",
    "scaler_stdf = StandardScaler()\n",
    "X_stdf_scaled = scaler_stdf.fit_transform(X_stdf)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca_stdf = PCA(n_components=50)  # Reduce 1000D ‚Üí 50D\n",
    "X_stdf_reduced = pca_stdf.fit_transform(X_stdf_scaled)\n",
    "\n",
    "# Reconstruct and compute error\n",
    "X_stdf_reconstructed = pca_stdf.inverse_transform(X_stdf_reduced)\n",
    "reconstruction_mse = np.mean((X_stdf_scaled - X_stdf_reconstructed) ** 2)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: 2D PCA projection (pass vs fail separation)\n",
    "X_stdf_2d = PCA(n_components=2).fit_transform(X_stdf_scaled)\n",
    "axes[0].scatter(X_stdf_2d[y_stdf==1, 0], X_stdf_2d[y_stdf==1, 1], \n",
    "               c='green', alpha=0.6, edgecolors='k', s=60, label='Pass (85%)')\n",
    "axes[0].scatter(X_stdf_2d[y_stdf==0, 0], X_stdf_2d[y_stdf==0, 1], \n",
    "               c='red', alpha=0.8, edgecolors='k', s=80, label='Fail (15%)')\n",
    "axes[0].set_title('STDF Data: Pass vs Fail (1000D ‚Üí 2D)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Explained variance (cumulative)\n",
    "cumulative_variance_stdf = np.cumsum(pca_stdf.explained_variance_ratio_)\n",
    "axes[1].plot(range(1, 51), cumulative_variance_stdf, marker='o', linewidth=2, color='blue')\n",
    "axes[1].axhline(0.95, color='green', linestyle='--', linewidth=2, label='95% Variance')\n",
    "axes[1].axvline(np.argmax(cumulative_variance_stdf >= 0.95) + 1, color='red', \n",
    "               linestyle='--', linewidth=2, label=f'Optimal K={np.argmax(cumulative_variance_stdf >= 0.95) + 1}')\n",
    "axes[1].set_title('Cumulative Explained Variance', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Variance Ratio')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: PC loadings (which test categories contribute most)\n",
    "pc1_loadings = np.abs(pca_stdf.components_[0])  # Absolute loadings for PC1\n",
    "category_contributions = []\n",
    "for i in range(n_test_categories):\n",
    "    start_idx = i * tests_per_category\n",
    "    end_idx = start_idx + tests_per_category\n",
    "    contribution = pc1_loadings[start_idx:end_idx].sum()\n",
    "    category_contributions.append(contribution)\n",
    "\n",
    "axes[2].bar(range(n_test_categories), category_contributions, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[2].set_title('PC1 Contribution by Test Category', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Test Category (0-9)')\n",
    "axes[2].set_ylabel('Total Absolute Loading')\n",
    "axes[2].set_xticks(range(n_test_categories))\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ STDF Dimensionality Reduction Results:\")\n",
    "print(f\"   Original Dimensions: {n_tests} parametric tests\")\n",
    "print(f\"   Reduced Dimensions: 50 principal components\")\n",
    "print(f\"   Explained Variance: {pca_stdf.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"   Reconstruction MSE: {reconstruction_mse:.6f}\")\n",
    "print(f\"   Components for 95%: {np.argmax(cumulative_variance_stdf >= 0.95) + 1}\")\n",
    "print(f\"\\nüí∞ Business Impact:\")\n",
    "print(f\"   ‚Ä¢ Dimensionality: 1000D ‚Üí 50D (20√ó reduction)\")\n",
    "print(f\"   ‚Ä¢ ML Speedup: ~20-400√ó (depending on algorithm complexity)\")\n",
    "print(f\"   ‚Ä¢ Storage: 20√ó compression for historical data\")\n",
    "print(f\"   ‚Ä¢ Root Cause Analysis: 5√ó faster (1 week ‚Üí 1 day) = $3M+ annual savings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605d9e5",
   "metadata": {},
   "source": [
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Use UMAP for wafer map spatial pattern visualization (100D ‚Üí 2D)\n",
    "\n",
    "**Key Points:**\n",
    "- **Wafer Map Data**: 300 die with spatial (x, y, distance_from_center) + 97 electrical parameters\n",
    "- **Defect Patterns**: 4 synthetic patterns (edge failures, center hotspot, quadrant gradient, random)\n",
    "- **UMAP Hyperparameters**: n_neighbors=15 (local structure), min_dist=0.1 (tight clusters)\n",
    "- **Cluster Separation**: UMAP reveals 4 distinct failure modes (not visible in raw 100D space)\n",
    "- **Speedup**: UMAP 100√ó faster than t-SNE for n=300 (critical for production <5 min)\n",
    "\n",
    "**Why This Matters:** Wafer maps combine spatial (x,y coordinates) and electrical (Vdd, Idd, frequency) features‚Äîimpossible to visualize in 100D. UMAP enables: (1) Pattern discovery (systematic vs random failures), (2) Root cause hypothesis (edge = process, center = equipment), (3) Clustering validation (visual sanity check before FA). $5M+ annual savings from 3-day faster defect diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb3ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if UMAP_AVAILABLE:\n",
    "    # Simulate wafer map data (300 die, 100 features total)\n",
    "    np.random.seed(42)\n",
    "    n_die = 300\n",
    "    \n",
    "    # Spatial features (x, y coordinates on wafer)\n",
    "    wafer_radius = 150  # mm\n",
    "    angles = np.random.uniform(0, 2*np.pi, n_die)\n",
    "    radii = np.random.uniform(0, wafer_radius, n_die)\n",
    "    die_x = radii * np.cos(angles)\n",
    "    die_y = radii * np.sin(angles)\n",
    "    distance_from_center = np.sqrt(die_x**2 + die_y**2)\n",
    "    \n",
    "    # Electrical parameters (97 features)\n",
    "    n_electrical = 97\n",
    "    X_electrical = np.random.randn(n_die, n_electrical)\n",
    "    \n",
    "    # Create 4 defect patterns (ground truth for validation)\n",
    "    labels_wafer = np.zeros(n_die, dtype=int)\n",
    "    \n",
    "    # Pattern 1: Edge failures (high radius)\n",
    "    edge_mask = distance_from_center > 120\n",
    "    labels_wafer[edge_mask] = 0\n",
    "    X_electrical[edge_mask, :20] += 3  # Electrical signature\n",
    "    \n",
    "    # Pattern 2: Center hotspot\n",
    "    center_mask = distance_from_center < 30\n",
    "    labels_wafer[center_mask] = 1\n",
    "    X_electrical[center_mask, 20:40] -= 2\n",
    "    \n",
    "    # Pattern 3: Quadrant gradient (right side)\n",
    "    quadrant_mask = (die_x > 50) & (~edge_mask) & (~center_mask)\n",
    "    labels_wafer[quadrant_mask] = 2\n",
    "    X_electrical[quadrant_mask, 40:60] += 1.5\n",
    "    \n",
    "    # Pattern 4: Random (no spatial correlation)\n",
    "    random_mask = (~edge_mask) & (~center_mask) & (~quadrant_mask)\n",
    "    labels_wafer[random_mask] = 3\n",
    "    \n",
    "    # Combine spatial + electrical features (100D total)\n",
    "    X_wafer = np.column_stack([die_x, die_y, distance_from_center, X_electrical])\n",
    "    X_wafer_scaled = StandardScaler().fit_transform(X_wafer)\n",
    "    \n",
    "    # Apply UMAP\n",
    "    umap_wafer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "    X_wafer_umap = umap_wafer.fit_transform(X_wafer_scaled)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot 1: Physical wafer map (spatial layout)\n",
    "    scatter1 = axes[0].scatter(die_x, die_y, c=labels_wafer, cmap='viridis',\n",
    "                              alpha=0.7, edgecolors='k', s=80)\n",
    "    circle = plt.Circle((0, 0), wafer_radius, fill=False, edgecolor='black', linewidth=2)\n",
    "    axes[0].add_patch(circle)\n",
    "    axes[0].set_aspect('equal')\n",
    "    axes[0].set_title('Physical Wafer Map (Spatial)', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Die X Position (mm)')\n",
    "    axes[0].set_ylabel('Die Y Position (mm)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    cbar1 = plt.colorbar(scatter1, ax=axes[0])\n",
    "    cbar1.set_label('Defect Pattern')\n",
    "    \n",
    "    # Plot 2: UMAP embedding (100D ‚Üí 2D)\n",
    "    scatter2 = axes[1].scatter(X_wafer_umap[:, 0], X_wafer_umap[:, 1], c=labels_wafer,\n",
    "                              cmap='viridis', alpha=0.7, edgecolors='k', s=80)\n",
    "    axes[1].set_title('UMAP Embedding (100D ‚Üí 2D)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('UMAP Dimension 1')\n",
    "    axes[1].set_ylabel('UMAP Dimension 2')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    cbar2 = plt.colorbar(scatter2, ax=axes[1])\n",
    "    cbar2.set_label('Defect Pattern')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Wafer Map UMAP Results:\")\n",
    "    print(f\"   Die Count: {n_die}\")\n",
    "    print(f\"   Feature Dimensions: {X_wafer.shape[1]} (3 spatial + 97 electrical)\")\n",
    "    print(f\"   Reduced Dimensions: 2 (for visualization)\")\n",
    "    print(f\"   Defect Patterns Discovered: 4\")\n",
    "    print(f\"      ‚Ä¢ Pattern 0: Edge failures (radius > 120mm)\")\n",
    "    print(f\"      ‚Ä¢ Pattern 1: Center hotspot (radius < 30mm)\")\n",
    "    print(f\"      ‚Ä¢ Pattern 2: Quadrant gradient (right side)\")\n",
    "    print(f\"      ‚Ä¢ Pattern 3: Random (no spatial correlation)\")\n",
    "    print(f\"\\nüí∞ Business Impact:\")\n",
    "    print(f\"   ‚Ä¢ Pattern Discovery: 3 days ‚Üí 1 day (5√ó faster root cause)\")\n",
    "    print(f\"   ‚Ä¢ Root Cause Hypotheses:\")\n",
    "    print(f\"      - Edge failures ‚Üí Process uniformity issue ($2M yield recovery)\")\n",
    "    print(f\"      - Center hotspot ‚Üí Equipment contamination ($5M+ tool PM)\")\n",
    "    print(f\"      - Quadrant gradient ‚Üí Wafer handling asymmetry ($1M)\")\n",
    "    print(f\"   ‚Ä¢ Total Annual Savings: $5M+ from faster defect diagnosis\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è UMAP not available. Install with: pip install umap-learn\")\n",
    "    print(\"   (Skipping wafer map visualization example)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3533561",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Real-World Projects\n",
    "\n",
    "### **Post-Silicon Validation Projects**\n",
    "\n",
    "1. **STDF Parametric Test Reduction Engine** üí∞ **$3M+ Annual Savings**\n",
    "   - **Objective:** Reduce 1000+ parametric tests to 50 principal components for ML pipeline acceleration\n",
    "   - **Approach:** PCA with 95% variance threshold, incremental PCA for streaming data, PC interpretation via loadings\n",
    "   - **Features:** Full STDF parametric test suite (voltage, current, frequency, power, timing)\n",
    "   - **Business Value:** 20√ó ML speedup (clustering, classification), 5√ó faster root cause analysis (1 week ‚Üí 1 day)\n",
    "   - **Success Metric:** <5% information loss (reconstruction MSE), 95%+ downstream model accuracy maintained\n",
    "\n",
    "2. **High-Dimensional Wafer Map Visualizer** üí∞ **$5M+ Yield Recovery**\n",
    "   - **Objective:** Visualize 100D wafer data (spatial + electrical) in 2D for pattern discovery\n",
    "   - **Approach:** UMAP with n_neighbors=15, min_dist=0.1, spatial feature engineering (distance_from_center, quadrant)\n",
    "   - **Features:** Die coordinates (x, y) + 97 electrical parameters (Vdd, Idd, frequency, leakage)\n",
    "   - **Business Value:** 3-day faster defect diagnosis, automated pattern discovery (edge/center/quadrant/random)\n",
    "   - **Success Metric:** <5 min processing time (production), 4-6 distinct patterns discovered per wafer\n",
    "\n",
    "3. **Test Correlation Network Analyzer** üí∞ **$2M+ Test Optimization**\n",
    "   - **Objective:** Discover redundant test groups via PCA loadings, eliminate 30% of tests without yield loss\n",
    "   - **Approach:** PCA on 500 tests, analyze PC loadings (high loading = important), cluster correlated tests\n",
    "   - **Features:** 500 parametric tests across 10K devices\n",
    "   - **Business Value:** 30% test time reduction ($2M+ savings), 50% STDF file size reduction (storage)\n",
    "   - **Success Metric:** <2% yield impact from test elimination, 90%+ correlation within removed test groups\n",
    "\n",
    "4. **Multi-Site Equipment Drift Detector** üí∞ **$10M+ Equipment Failure Prevention**\n",
    "   - **Objective:** Reduce 200D site-level statistics to 10D for drift detection (equipment PM scheduling)\n",
    "   - **Approach:** PCA per site (baseline), track PC drift over time (Hotelling T¬≤ statistic), alert at 3œÉ\n",
    "   - **Features:** Site statistics (mean/std/skew/kurtosis of 50 parameters √ó 4 moments = 200D)\n",
    "   - **Business Value:** 7-day earlier equipment failure detection, prevent $10M+ yield excursions\n",
    "   - **Success Metric:** <24-hour detection latency, 95% precision (avoid false PM alarms)\n",
    "\n",
    "---\n",
    "\n",
    "### **General AI/ML Projects**\n",
    "\n",
    "5. **Customer Behavior Segmentation (1000 Features)** üí∞ **$20M+ Marketing ROI**\n",
    "   - **Objective:** Segment 1M customers using 1000 behavioral features (page views, clicks, purchases)\n",
    "   - **Approach:** PCA to 100D (interpretable), then UMAP to 2D (visualization), K-Means on reduced space\n",
    "   - **Features:** 1000 event types (product views, cart actions, search terms, session duration)\n",
    "   - **Business Value:** 25% increase in campaign ROI via micro-targeting, 10√ó faster segmentation refresh\n",
    "   - **Success Metric:** 12 distinct segments, 80%+ silhouette score, weekly retraining (<2 hours)\n",
    "\n",
    "6. **Medical Imaging Feature Extraction (10K Pixels)** üí∞ **$50M+ Diagnostic Accuracy**\n",
    "   - **Objective:** Reduce 10K-pixel MRI scans to 50 features for disease classification\n",
    "   - **Approach:** PCA on flattened images, feed 50 PCs to downstream classifier (SVM, Random Forest)\n",
    "   - **Features:** 100√ó100 grayscale pixels (intensity values)\n",
    "   - **Business Value:** 92% classification accuracy (brain tumor detection), 5√ó faster inference (<1s)\n",
    "   - **Success Metric:** AUC >0.95, <1-second prediction time, 50D representation\n",
    "\n",
    "7. **Financial Time Series Dimensionality Reduction** üí∞ **$30M+ Risk Management**\n",
    "   - **Objective:** Reduce 500 stock returns (correlated) to 10 risk factors for portfolio optimization\n",
    "   - **Approach:** PCA on correlation matrix, interpret PCs as market factors (market, sector, size, value)\n",
    "   - **Features:** Daily returns for 500 stocks over 5 years\n",
    "   - **Business Value:** Factor-based risk modeling (10 factors vs 500 stocks), 20% variance reduction\n",
    "   - **Success Metric:** 10 PCs explain 80%+ variance, factor interpretation aligns with Fama-French\n",
    "\n",
    "8. **Text Document Embedding (50K Vocabulary)** üí∞ **$15M+ Search Relevance**\n",
    "   - **Objective:** Reduce 50K-dimensional TF-IDF vectors to 300D for semantic search\n",
    "   - **Approach:** TruncatedSVD (PCA for sparse matrices) to 300D, cosine similarity for retrieval\n",
    "   - **Features:** TF-IDF vectors (50K vocabulary) from 1M documents\n",
    "   - **Business Value:** 100√ó faster search (O(nd) vs O(nd¬≤)), 15% relevance improvement (user satisfaction)\n",
    "   - **Success Metric:** <100ms query time, 85%+ NDCG@10 (ranking quality)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "### **When to Use Each Algorithm**\n",
    "\n",
    "‚úÖ **PCA:**\n",
    "- **Best for:** Feature reduction (keep interpretable PCs), preprocessing for ML, noise reduction\n",
    "- **Data size:** Any (scales to millions)\n",
    "- **Typical d':** 10-50 (retain 80-95% variance)\n",
    "- **Advantages:** Fast, deterministic, interpretable (PC loadings), inverse transform\n",
    "- **Limitations:** Linear only (misses non-linear patterns), assumes high variance = important\n",
    "\n",
    "‚úÖ **t-SNE:**\n",
    "- **Best for:** 2D/3D visualization of small datasets (<10K points)\n",
    "- **Data size:** <10K practical (slow O(n¬≤ log n))\n",
    "- **Typical d':** 2-3 (visualization only)\n",
    "- **Advantages:** Best local structure preservation, beautiful cluster visualizations\n",
    "- **Limitations:** Slow, stochastic (different runs differ), no inverse transform, no d'>3\n",
    "\n",
    "‚úÖ **UMAP:**\n",
    "- **Best for:** Visualization + clustering, large datasets (millions), balanced local+global structure\n",
    "- **Data size:** Any (scales to millions with approximate NN)\n",
    "- **Typical d':** 2-50 (both visualization and feature reduction)\n",
    "- **Advantages:** 10-100√ó faster than t-SNE, preserves global structure, more stable\n",
    "- **Limitations:** Stochastic (less reproducible than PCA), requires tuning (n_neighbors, min_dist)\n",
    "\n",
    "---\n",
    "\n",
    "### **Algorithm Selection Flowchart**\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[High-D Data] --> B{Goal?}\n",
    "    B -->|Feature Reduction<br/>for ML| C{Need interpretability?}\n",
    "    B -->|Visualization<br/>2D/3D| D{Data size?}\n",
    "    \n",
    "    C -->|Yes| E[PCA<br/>Analyze loadings]\n",
    "    C -->|No| F[PCA or UMAP<br/>Compare performance]\n",
    "    \n",
    "    D -->|< 10K points| G{Preserves local<br/>structure critical?}\n",
    "    D -->|> 10K points| H[UMAP<br/>Fast + scalable]\n",
    "    \n",
    "    G -->|Yes| I[t-SNE<br/>Best local]\n",
    "    G -->|No| J[UMAP<br/>Faster]\n",
    "    \n",
    "    E --> K[Downstream ML]\n",
    "    F --> K\n",
    "    H --> L[Clustering/<br/>Visualization]\n",
    "    I --> L\n",
    "    J --> L\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Hyperparameter Tuning Best Practices**\n",
    "\n",
    "**PCA:**\n",
    "1. **n_components**: Use explained variance plot (elbow at 80-95%)\n",
    "2. **Standardization**: Always scale features (variance-based algorithm)\n",
    "3. **Incremental PCA**: Use for data that doesn't fit in memory\n",
    "4. **Kernel PCA**: For non-linear patterns (RBF kernel)\n",
    "\n",
    "**t-SNE:**\n",
    "1. **perplexity** (5-50): Balance local vs global (15-30 typical)\n",
    "   - Low perplexity (5-10): Emphasizes very local structure\n",
    "   - High perplexity (30-50): More global structure\n",
    "2. **learning_rate** (10-1000): Default 200 usually fine\n",
    "3. **n_iter** (250-1000): More iterations for convergence (check KL divergence)\n",
    "4. **Run multiple times**: Stochastic algorithm (check consistency)\n",
    "\n",
    "**UMAP:**\n",
    "1. **n_neighbors** (5-50): Controls local vs global balance\n",
    "   - Low (5-15): Tight local structure, many small clusters\n",
    "   - High (30-50): More global structure, fewer large clusters\n",
    "2. **min_dist** (0.0-0.99): Controls embedding tightness\n",
    "   - Low (0.0-0.1): Tight clusters (good for clustering)\n",
    "   - High (0.3-0.5): Spread out (good for visualization)\n",
    "3. **metric**: Euclidean (default), Manhattan, Cosine (text), Haversine (geo)\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Pitfalls**\n",
    "\n",
    "‚ö†Ô∏è **Not Standardizing for PCA**\n",
    "- **Problem:** Features with large variance dominate PCs\n",
    "- **Fix:** Always use StandardScaler before PCA\n",
    "\n",
    "‚ö†Ô∏è **Using t-SNE for Large Data (>10K)**\n",
    "- **Problem:** Takes hours to run, impractical\n",
    "- **Fix:** Use UMAP instead (10-100√ó faster)\n",
    "\n",
    "‚ö†Ô∏è **Interpreting t-SNE/UMAP Distances**\n",
    "- **Problem:** Distances between clusters are meaningless (only local structure preserved)\n",
    "- **Fix:** Only interpret cluster separation, not inter-cluster distances\n",
    "\n",
    "‚ö†Ô∏è **Overfitting PCA (using too many components)**\n",
    "- **Problem:** Retaining 100% variance includes noise\n",
    "- **Fix:** Use 80-95% explained variance threshold (cross-validation)\n",
    "\n",
    "‚ö†Ô∏è **Ignoring PC Interpretability**\n",
    "- **Problem:** Using PCA as black box (missing insights)\n",
    "- **Fix:** Analyze PC loadings (which features contribute most)\n",
    "\n",
    "---\n",
    "\n",
    "### **Production Considerations**\n",
    "\n",
    "üîß **PCA Deployment:**\n",
    "- **Save model:** `pickle.dump(pca, file)` or `joblib.dump(pca, file)`\n",
    "- **Transform new data:** `X_new_reduced = pca.transform(X_new_scaled)`\n",
    "- **Inverse transform:** `X_approx = pca.inverse_transform(X_reduced)` (reconstruction)\n",
    "- **Incremental learning:** Use `IncrementalPCA` for streaming data\n",
    "\n",
    "üîß **t-SNE/UMAP Deployment:**\n",
    "- **Warning:** No transform() method (must refit entire dataset)\n",
    "- **Workaround:** Use UMAP's `transform()` (approximate) or pre-compute embeddings\n",
    "- **Batch processing:** Compute embeddings offline, store for visualization\n",
    "\n",
    "üîß **Scalability:**\n",
    "- **PCA:** O(nd¬≤) + O(d¬≥) (fast for d<1000)\n",
    "- **t-SNE:** O(n¬≤ log n) (impractical for n>10K)\n",
    "- **UMAP:** O(n log n) (scales to millions with approximate NN)\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Next Steps\n",
    "\n",
    "- **031_Feature_Selection.ipynb** - Compare dimensionality reduction vs feature selection (keep original features)\n",
    "- **032_Autoencoders.ipynb** - Non-linear dimensionality reduction with neural networks\n",
    "- **041_Feature_Engineering.ipynb** - Engineer domain-specific features before PCA/UMAP\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Remember:** PCA for interpretability, t-SNE for small data visualization, UMAP for everything else!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
