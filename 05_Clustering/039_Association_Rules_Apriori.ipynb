{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a9d024c",
   "metadata": {},
   "source": [
    "## üßÆ Mathematical Foundation\n",
    "\n",
    "### 1. Terminology\n",
    "\n",
    "**Itemset**: A collection of items, e.g., {Bread, Milk}\n",
    "\n",
    "**Transaction**: A set of items in a single transaction/record\n",
    "\n",
    "**Database**: D = {T1, T2, ..., Tn} where each Ti is a transaction\n",
    "\n",
    "**k-itemset**: Itemset with k items\n",
    "\n",
    "### 2. Support\n",
    "\n",
    "**Support** of itemset X: Proportion of transactions containing X\n",
    "\n",
    "$$\\text{support}(X) = \\frac{|\\{T \\in D : X \\subseteq T\\}|}{|D|}$$\n",
    "\n",
    "Example: If {Bread, Milk} appears in 300 of 1000 transactions, support = 0.3 (30%)\n",
    "\n",
    "**Minimum support threshold**: User-defined, e.g., 0.01 (1%) to filter rare itemsets\n",
    "\n",
    "### 3. Association Rule\n",
    "\n",
    "**Rule**: X ‚Üí Y where X, Y are itemsets, X ‚à© Y = ‚àÖ\n",
    "\n",
    "Example: {Bread} ‚Üí {Milk}\n",
    "\n",
    "### 4. Confidence\n",
    "\n",
    "**Confidence** of rule X ‚Üí Y: Conditional probability P(Y|X)\n",
    "\n",
    "$$\\text{confidence}(X \\to Y) = \\frac{\\text{support}(X \\cup Y)}{\\text{support}(X)}$$\n",
    "\n",
    "Example: If {Bread} appears 500 times, {Bread, Milk} appears 300 times:  \n",
    "confidence({Bread} ‚Üí {Milk}) = 300/500 = 0.6 (60%)\n",
    "\n",
    "**Interpretation**: \"60% of transactions with Bread also have Milk\"\n",
    "\n",
    "### 5. Lift\n",
    "\n",
    "**Lift** measures how much more likely Y is given X, compared to Y alone:\n",
    "\n",
    "$$\\text{lift}(X \\to Y) = \\frac{\\text{confidence}(X \\to Y)}{\\text{support}(Y)} = \\frac{\\text{support}(X \\cup Y)}{\\text{support}(X) \\times \\text{support}(Y)}$$\n",
    "\n",
    "**Interpretation**:\n",
    "- Lift > 1: X and Y occur together more than expected (positive correlation)\n",
    "- Lift = 1: X and Y independent\n",
    "- Lift < 1: X and Y negatively correlated (substitutes)\n",
    "\n",
    "Example: If support({Milk}) = 0.4, confidence = 0.6:  \n",
    "lift = 0.6 / 0.4 = 1.5 (50% more likely with Bread)\n",
    "\n",
    "### 6. Apriori Principle\n",
    "\n",
    "**Key insight**: If an itemset is frequent, all its subsets must be frequent\n",
    "\n",
    "**Contrapositive** (used for pruning): If an itemset is infrequent, all its supersets are infrequent\n",
    "\n",
    "**Example**: If {Bread, Milk} is infrequent, no need to check {Bread, Milk, Eggs}\n",
    "\n",
    "**Efficiency**: Drastically reduces candidate itemsets to check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65150be2",
   "metadata": {},
   "source": [
    "## üíª Implementation from Scratch\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Build Apriori algorithm from scratch with efficient pruning\n",
    "\n",
    "**Key Points:**\n",
    "- **get_frequent_1_itemsets()**: Count single items, filter by min_support\n",
    "- **generate_candidates()**: Join Lk-1 with itself to create k-itemsets (self-join)\n",
    "- **prune()**: Remove candidates with infrequent subsets (Apriori principle)\n",
    "- **get_support()**: Count itemset occurrences in transactions\n",
    "- **generate_rules()**: For each frequent itemset, generate all possible rules\n",
    "- **Metrics**: Compute confidence and lift for each rule\n",
    "\n",
    "**Why This Matters:** \n",
    "- Understanding self-join + pruning clarifies exponential search space reduction\n",
    "- Implementation shows computational complexity (multiple database scans)\n",
    "- Enables custom modifications (weighted support, hierarchical itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d21e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations, chain\n",
    "from collections import defaultdict\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "class AprioriMiner:\n",
    "    \"\"\"Apriori algorithm for association rule mining.\"\"\"\n",
    "    \n",
    "    def __init__(self, min_support=0.01, min_confidence=0.5, min_lift=1.0):\n",
    "        self.min_support = min_support\n",
    "        self.min_confidence = min_confidence\n",
    "        self.min_lift = min_lift\n",
    "        self.frequent_itemsets = []\n",
    "        self.rules = []\n",
    "        \n",
    "    def fit(self, transactions):\n",
    "        \"\"\"Mine frequent itemsets and generate association rules.\"\"\"\n",
    "        self.transactions = [set(t) for t in transactions]\n",
    "        self.n_transactions = len(transactions)\n",
    "        \n",
    "        # Generate frequent itemsets\n",
    "        self.frequent_itemsets = self._apriori()\n",
    "        \n",
    "        # Generate rules\n",
    "        self.rules = self._generate_rules()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _get_support(self, itemset):\n",
    "        \"\"\"Calculate support for an itemset.\"\"\"\n",
    "        count = sum(1 for t in self.transactions if itemset.issubset(t))\n",
    "        return count / self.n_transactions\n",
    "    \n",
    "    def _get_frequent_1_itemsets(self):\n",
    "        \"\"\"Get frequent 1-itemsets.\"\"\"\n",
    "        item_counts = defaultdict(int)\n",
    "        for transaction in self.transactions:\n",
    "            for item in transaction:\n",
    "                item_counts[frozenset([item])] += 1\n",
    "        \n",
    "        # Filter by min_support\n",
    "        min_count = self.min_support * self.n_transactions\n",
    "        frequent = {itemset: count/self.n_transactions \n",
    "                   for itemset, count in item_counts.items() \n",
    "                   if count >= min_count}\n",
    "        return frequent\n",
    "    \n",
    "    def _generate_candidates(self, Lk_prev, k):\n",
    "        \"\"\"Generate k-itemset candidates from (k-1)-itemsets.\"\"\"\n",
    "        candidates = set()\n",
    "        items_list = list(Lk_prev.keys())\n",
    "        \n",
    "        # Self-join: combine itemsets that differ by only 1 item\n",
    "        for i in range(len(items_list)):\n",
    "            for j in range(i+1, len(items_list)):\n",
    "                union = items_list[i] | items_list[j]\n",
    "                if len(union) == k:\n",
    "                    candidates.add(union)\n",
    "        \n",
    "        return candidates\n",
    "    \n",
    "    def _prune(self, candidates, Lk_prev):\n",
    "        \"\"\"Prune candidates with infrequent subsets.\"\"\"\n",
    "        pruned = set()\n",
    "        for candidate in candidates:\n",
    "            # Check all (k-1) subsets\n",
    "            subsets = [frozenset(s) for s in combinations(candidate, len(candidate)-1)]\n",
    "            if all(subset in Lk_prev for subset in subsets):\n",
    "                pruned.add(candidate)\n",
    "        return pruned\n",
    "    \n",
    "    def _apriori(self):\n",
    "        \"\"\"Main Apriori algorithm.\"\"\"\n",
    "        # Start with 1-itemsets\n",
    "        L1 = self._get_frequent_1_itemsets()\n",
    "        all_frequent = [L1]\n",
    "        \n",
    "        k = 2\n",
    "        Lk_prev = L1\n",
    "        \n",
    "        while Lk_prev:\n",
    "            # Generate candidates\n",
    "            candidates = self._generate_candidates(Lk_prev, k)\n",
    "            \n",
    "            # Prune\n",
    "            candidates = self._prune(candidates, Lk_prev)\n",
    "            \n",
    "            # Calculate support and filter\n",
    "            Lk = {}\n",
    "            for itemset in candidates:\n",
    "                support = self._get_support(itemset)\n",
    "                if support >= self.min_support:\n",
    "                    Lk[itemset] = support\n",
    "            \n",
    "            if Lk:\n",
    "                all_frequent.append(Lk)\n",
    "                Lk_prev = Lk\n",
    "                k += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Flatten all frequent itemsets\n",
    "        frequent = {}\n",
    "        for Lk in all_frequent:\n",
    "            frequent.update(Lk)\n",
    "        \n",
    "        return frequent\n",
    "    \n",
    "    def _generate_rules(self):\n",
    "        \"\"\"Generate association rules from frequent itemsets.\"\"\"\n",
    "        rules = []\n",
    "        \n",
    "        for itemset, support_xy in self.frequent_itemsets.items():\n",
    "            if len(itemset) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Generate all possible rules X -> Y\n",
    "            for i in range(1, len(itemset)):\n",
    "                for antecedent in combinations(itemset, i):\n",
    "                    antecedent = frozenset(antecedent)\n",
    "                    consequent = itemset - antecedent\n",
    "                    \n",
    "                    if antecedent in self.frequent_itemsets:\n",
    "                        support_x = self.frequent_itemsets[antecedent]\n",
    "                        confidence = support_xy / support_x\n",
    "                        \n",
    "                        if confidence >= self.min_confidence:\n",
    "                            support_y = self._get_support(consequent)\n",
    "                            lift = confidence / support_y if support_y > 0 else 0\n",
    "                            \n",
    "                            if lift >= self.min_lift:\n",
    "                                rules.append({\n",
    "                                    'antecedent': antecedent,\n",
    "                                    'consequent': consequent,\n",
    "                                    'support': support_xy,\n",
    "                                    'confidence': confidence,\n",
    "                                    'lift': lift\n",
    "                                })\n",
    "        \n",
    "        return sorted(rules, key=lambda x: x['lift'], reverse=True)\n",
    "\n",
    "print(\"‚úÖ Apriori implementation complete!\")\n",
    "print(\"   - Support: Frequency of itemset\")\n",
    "print(\"   - Confidence: P(Y|X) for rule X‚ÜíY\")\n",
    "print(\"   - Lift: Correlation strength (>1 = positive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a03179",
   "metadata": {},
   "source": [
    "## üß™ Test on Market Basket Data\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Validate implementation on classic market basket example\n",
    "\n",
    "**Key Points:**\n",
    "- **Synthetic transactions**: 1000 shopping baskets with product correlations\n",
    "- **Product relationships**: Bread-Milk (strong), Chips-Soda (medium), random noise\n",
    "- **min_support=0.02**: Itemset must appear in 2%+ of transactions\n",
    "- **min_confidence=0.3**: Rule must be correct 30%+ of the time\n",
    "- **Visualization**: Top rules by lift (strongest correlations)\n",
    "\n",
    "**Why This Matters:** \n",
    "- Classic use case validates algorithm correctness\n",
    "- Shows interpretable results (common shopping patterns)\n",
    "- Demonstrates parameter sensitivity (support/confidence trade-off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf13f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic market basket data\n",
    "products = ['Bread', 'Milk', 'Eggs', 'Cheese', 'Butter', \n",
    "            'Chips', 'Soda', 'Beer', 'Wine', 'Coffee']\n",
    "\n",
    "transactions = []\n",
    "for _ in range(1000):\n",
    "    transaction = []\n",
    "    \n",
    "    # Common patterns\n",
    "    if np.random.rand() < 0.3:  # Breakfast combo\n",
    "        transaction.extend(['Bread', 'Milk', 'Eggs'])\n",
    "    if np.random.rand() < 0.2:  # Dairy combo\n",
    "        transaction.extend(['Milk', 'Cheese', 'Butter'])\n",
    "    if np.random.rand() < 0.15:  # Snack combo\n",
    "        transaction.extend(['Chips', 'Soda'])\n",
    "    if np.random.rand() < 0.1:  # Party combo\n",
    "        transaction.extend(['Beer', 'Wine', 'Chips'])\n",
    "    \n",
    "    # Random items\n",
    "    n_random = np.random.randint(0, 3)\n",
    "    transaction.extend(np.random.choice(products, n_random, replace=False))\n",
    "    \n",
    "    transactions.append(list(set(transaction)))  # Remove duplicates\n",
    "\n",
    "print(f\"Generated {len(transactions)} transactions\")\n",
    "print(f\"Sample transactions:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Transaction {i+1}: {transactions[i]}\")\n",
    "\n",
    "# Mine association rules\n",
    "miner = AprioriMiner(min_support=0.02, min_confidence=0.3, min_lift=1.0)\n",
    "miner.fit(transactions)\n",
    "\n",
    "print(f\"\\nüìä Mining Results:\")\n",
    "print(f\"   Frequent itemsets: {len(miner.frequent_itemsets)}\")\n",
    "print(f\"   Association rules: {len(miner.rules)}\")\n",
    "\n",
    "# Display top rules\n",
    "print(\"\\nüîù Top 10 Rules by Lift:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Antecedent':<20} {'Consequent':<20} {'Support':>10} {'Confidence':>12} {'Lift':>8}\")\n",
    "print(\"-\" * 80)\n",
    "for rule in miner.rules[:10]:\n",
    "    ant_str = ', '.join(sorted(rule['antecedent']))\n",
    "    cons_str = ', '.join(sorted(rule['consequent']))\n",
    "    print(f\"{ant_str:<20} {cons_str:<20} {rule['support']:>10.3f} \"\n",
    "          f\"{rule['confidence']:>12.3f} {rule['lift']:>8.3f}\")\n",
    "\n",
    "# Visualize top rules\n",
    "if len(miner.rules) > 0:\n",
    "    top_rules = miner.rules[:15]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Confidence vs Support colored by Lift\n",
    "    ax = axes[0]\n",
    "    supports = [r['support'] for r in top_rules]\n",
    "    confidences = [r['confidence'] for r in top_rules]\n",
    "    lifts = [r['lift'] for r in top_rules]\n",
    "    \n",
    "    scatter = ax.scatter(supports, confidences, c=lifts, s=100, \n",
    "                        cmap='RdYlGn', edgecolors='black', linewidths=1)\n",
    "    plt.colorbar(scatter, ax=ax, label='Lift')\n",
    "    ax.set_xlabel('Support')\n",
    "    ax.set_ylabel('Confidence')\n",
    "    ax.set_title('Top 15 Rules: Support vs Confidence\\n(Color = Lift)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Lift bar chart\n",
    "    ax = axes[1]\n",
    "    rule_labels = [f\"{', '.join(sorted(r['antecedent']))} ‚Üí {', '.join(sorted(r['consequent']))}\" \n",
    "                   for r in top_rules[:10]]\n",
    "    lifts_top10 = [r['lift'] for r in top_rules[:10]]\n",
    "    \n",
    "    y_pos = np.arange(len(rule_labels))\n",
    "    ax.barh(y_pos, lifts_top10, color='coral')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(rule_labels, fontsize=8)\n",
    "    ax.set_xlabel('Lift')\n",
    "    ax.set_title('Top 10 Rules by Lift')\n",
    "    ax.axvline(1.0, color='red', linestyle='--', linewidth=2, label='Lift=1 (independent)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9ad82d",
   "metadata": {},
   "source": [
    "## üè≠ Post-Silicon Application: Test Correlation Analysis\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Discover which semiconductor tests fail together (common root causes)\n",
    "\n",
    "**Key Points:**\n",
    "- **Transactions = devices**: Each device is a transaction\n",
    "- **Items = failed tests**: If test fails, it's in the transaction\n",
    "- **Example**: {VDD_HIGH, FREQ_LOW} ‚Üí {LEAKAGE_HIGH} means voltage/frequency failures predict leakage\n",
    "- **Business value**: Correlated failures suggest common defect (process, equipment)\n",
    "- **Actionable**: High-lift rules guide debug prioritization\n",
    "- **Test optimization**: If A‚ÜíB with 100% confidence, test B is redundant\n",
    "\n",
    "**Why This Matters:** \n",
    "- Reduces debug time (focus on correlated failures)\n",
    "- Identifies systematic defects (vs random)\n",
    "- Enables test flow optimization (remove redundant tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10421fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate semiconductor test failure data\n",
    "np.random.seed(42)\n",
    "tests = ['VDD_LOW', 'VDD_HIGH', 'IDD_HIGH', 'FREQ_LOW', 'FREQ_HIGH',\n",
    "         'LEAKAGE_HIGH', 'TIMING_FAIL', 'POWER_HIGH', 'TEMP_HIGH', 'NOISE_HIGH']\n",
    "\n",
    "devices = []\n",
    "for device_id in range(5000):\n",
    "    failed_tests = []\n",
    "    \n",
    "    # Failure mode 1: Process defect (voltage + leakage + timing)\n",
    "    if np.random.rand() < 0.10:\n",
    "        failed_tests.extend(['VDD_HIGH', 'LEAKAGE_HIGH', 'TIMING_FAIL'])\n",
    "    \n",
    "    # Failure mode 2: Frequency issue (freq + power)\n",
    "    if np.random.rand() < 0.08:\n",
    "        failed_tests.extend(['FREQ_LOW', 'POWER_HIGH'])\n",
    "    \n",
    "    # Failure mode 3: Thermal issue (temp + noise + idd)\n",
    "    if np.random.rand() < 0.05:\n",
    "        failed_tests.extend(['TEMP_HIGH', 'NOISE_HIGH', 'IDD_HIGH'])\n",
    "    \n",
    "    # Failure mode 4: Voltage droop (vdd_low + freq_low)\n",
    "    if np.random.rand() < 0.07:\n",
    "        failed_tests.extend(['VDD_LOW', 'FREQ_LOW'])\n",
    "    \n",
    "    # Random failures (noise)\n",
    "    n_random = np.random.poisson(0.3)  # Low rate\n",
    "    if n_random > 0:\n",
    "        failed_tests.extend(np.random.choice(tests, min(n_random, 3), replace=False))\n",
    "    \n",
    "    devices.append(list(set(failed_tests)))  # Remove duplicates\n",
    "\n",
    "# Filter devices with at least 1 failure\n",
    "failed_devices = [d for d in devices if len(d) > 0]\n",
    "\n",
    "print(f\"üìä Test Failure Data:\")\n",
    "print(f\"   Total devices: {len(devices)}\")\n",
    "print(f\"   Devices with failures: {len(failed_devices)}\")\n",
    "print(f\"   Failure rate: {len(failed_devices)/len(devices)*100:.1f}%\")\n",
    "print(f\"\\n   Sample failed devices:\")\n",
    "for i in range(3):\n",
    "    print(f\"     Device {i+1}: {failed_devices[i]}\")\n",
    "\n",
    "# Mine test correlations\n",
    "miner_psv = AprioriMiner(min_support=0.02, min_confidence=0.5, min_lift=1.5)\n",
    "miner_psv.fit(failed_devices)\n",
    "\n",
    "print(f\"\\nüîç Test Correlation Mining:\")\n",
    "print(f\"   Frequent failure patterns: {len(miner_psv.frequent_itemsets)}\")\n",
    "print(f\"   Strong correlation rules: {len(miner_psv.rules)}\")\n",
    "\n",
    "# Display top correlations\n",
    "print(\"\\n‚ö†Ô∏è  Top 10 Test Correlations (High Lift):\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'If These Tests Fail':<30} {'Then This Fails':<20} {'Support':>10} {'Confidence':>12} {'Lift':>8}\")\n",
    "print(\"-\" * 90)\n",
    "for rule in miner_psv.rules[:10]:\n",
    "    ant_str = ', '.join(sorted(rule['antecedent']))\n",
    "    cons_str = ', '.join(sorted(rule['consequent']))\n",
    "    print(f\"{ant_str:<30} {cons_str:<20} {rule['support']:>10.3f} \"\n",
    "          f\"{rule['confidence']:>12.3f} {rule['lift']:>8.3f}\")\n",
    "\n",
    "# Visualize\n",
    "if len(miner_psv.rules) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Network graph of top correlations\n",
    "    ax = axes[0]\n",
    "    top_rules = miner_psv.rules[:8]\n",
    "    \n",
    "    # Create test frequency dict\n",
    "    test_freq = defaultdict(int)\n",
    "    for rule in top_rules:\n",
    "        for test in rule['antecedent']:\n",
    "            test_freq[test] += 1\n",
    "        for test in rule['consequent']:\n",
    "            test_freq[test] += 1\n",
    "    \n",
    "    # Simple visualization (could use networkx for better layout)\n",
    "    ax.text(0.5, 0.9, 'Top Test Correlations Network', \n",
    "            ha='center', va='top', fontsize=14, fontweight='bold')\n",
    "    y = 0.8\n",
    "    for i, rule in enumerate(top_rules):\n",
    "        ant = ', '.join(sorted(rule['antecedent']))\n",
    "        cons = ', '.join(sorted(rule['consequent']))\n",
    "        ax.text(0.1, y, ant, ha='left', va='center', fontsize=9, \n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "        ax.annotate('', xy=(0.7, y), xytext=(0.35, y),\n",
    "                   arrowprops=dict(arrowstyle='->', lw=2*rule['lift'], color='coral'))\n",
    "        ax.text(0.9, y, cons, ha='right', va='center', fontsize=9,\n",
    "               bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "        ax.text(0.5, y-0.02, f\"lift={rule['lift']:.2f}\", \n",
    "               ha='center', va='top', fontsize=7, style='italic')\n",
    "        y -= 0.1\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Confidence vs Lift scatter\n",
    "    ax = axes[1]\n",
    "    confidences = [r['confidence'] for r in miner_psv.rules[:20]]\n",
    "    lifts = [r['lift'] for r in miner_psv.rules[:20]]\n",
    "    supports = [r['support']*1000 for r in miner_psv.rules[:20]]  # Size by support\n",
    "    \n",
    "    scatter = ax.scatter(confidences, lifts, s=supports, alpha=0.6, \n",
    "                        c=lifts, cmap='YlOrRd', edgecolors='black', linewidths=1)\n",
    "    plt.colorbar(scatter, ax=ax, label='Lift')\n",
    "    ax.set_xlabel('Confidence')\n",
    "    ax.set_ylabel('Lift')\n",
    "    ax.set_title('Test Correlation Rules\\n(Size = Support, Color = Lift)')\n",
    "    ax.axhline(1.0, color='blue', linestyle='--', linewidth=1, alpha=0.5, label='Lift=1')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nüí° Actionable Insights:\")\n",
    "print(\"   - High lift rules indicate systematic defects (common root cause)\")\n",
    "print(\"   - High confidence rules enable predictive test skipping\")\n",
    "print(\"   - Cluster correlated tests for efficient debug workflows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e4bc1",
   "metadata": {},
   "source": [
    "## üîß Production Implementation with mlxtend\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Use mlxtend library for optimized, production-ready association mining\n",
    "\n",
    "**Key Points:**\n",
    "- **TransactionEncoder**: Converts list of transactions to one-hot DataFrame\n",
    "- **apriori()**: Optimized C-based implementation (10-100x faster)\n",
    "- **association_rules()**: Generates rules with all metrics (support, confidence, lift, leverage, conviction)\n",
    "- **Additional metrics**:\n",
    "  - **Leverage**: support(X‚à™Y) - support(X)√ósupport(Y) (absolute correlation)\n",
    "  - **Conviction**: [1 - support(Y)] / [1 - confidence(X‚ÜíY)] (rule strength)\n",
    "- **Comparison**: Validate from-scratch matches production library\n",
    "\n",
    "**Why This Matters:** \n",
    "- Production needs speed (process millions of transactions)\n",
    "- mlxtend integrates with pandas workflows\n",
    "- Additional metrics provide deeper insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94144d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Convert transactions to one-hot encoded DataFrame\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(failed_devices).transform(failed_devices)\n",
    "df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "\n",
    "print(\"üìä One-Hot Encoded Data:\")\n",
    "print(df.head())\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "\n",
    "# Mine frequent itemsets\n",
    "frequent_itemsets = apriori(df, min_support=0.02, use_colnames=True)\n",
    "print(f\"\\nüîç Frequent Itemsets: {len(frequent_itemsets)}\")\n",
    "print(frequent_itemsets.head(10))\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.5)\n",
    "rules = rules[rules['lift'] >= 1.5]  # Filter by lift\n",
    "rules = rules.sort_values('lift', ascending=False)\n",
    "\n",
    "print(f\"\\n‚ö° Association Rules: {len(rules)}\")\n",
    "print(\"\\nTop 10 Rules:\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift', \n",
    "             'leverage', 'conviction']].head(10).to_string())\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confidence vs Lift (mlxtend)\n",
    "ax = axes[0]\n",
    "scatter = ax.scatter(rules['confidence'], rules['lift'], \n",
    "                     s=rules['support']*1000, alpha=0.6,\n",
    "                     c=rules['lift'], cmap='RdYlGn', \n",
    "                     edgecolors='black', linewidths=1)\n",
    "plt.colorbar(scatter, ax=ax, label='Lift')\n",
    "ax.set_xlabel('Confidence')\n",
    "ax.set_ylabel('Lift')\n",
    "ax.set_title('mlxtend: Confidence vs Lift\\n(Size = Support)')\n",
    "ax.axhline(1.0, color='blue', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Leverage vs Conviction\n",
    "ax = axes[1]\n",
    "scatter = ax.scatter(rules['leverage'], rules['conviction'], \n",
    "                     s=rules['support']*1000, alpha=0.6,\n",
    "                     c=rules['lift'], cmap='RdYlGn',\n",
    "                     edgecolors='black', linewidths=1)\n",
    "plt.colorbar(scatter, ax=ax, label='Lift')\n",
    "ax.set_xlabel('Leverage')\n",
    "ax.set_ylabel('Conviction')\n",
    "ax.set_title('Advanced Metrics: Leverage vs Conviction\\n(Higher = Stronger Rule)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ mlxtend Results:\")\n",
    "print(f\"   - Frequent itemsets: {len(frequent_itemsets)}\")\n",
    "print(f\"   - Association rules: {len(rules)}\")\n",
    "print(f\"   - Implementation: Optimized C-based (10-100x faster)\")\n",
    "print(f\"   - Additional metrics: leverage, conviction available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe73c6f2",
   "metadata": {},
   "source": [
    "## üéØ Real-World Project Ideas\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "1. **Automated Test Flow Optimizer** üí∞ $10M+ Test Cost Reduction\n",
    "   - **Objective**: Mine 500+ tests for redundancy, remove 20-30% without yield impact\n",
    "   - **Features**: Test results (pass/fail), device characteristics, test sequence\n",
    "   - **Success Metric**: 25% test time reduction, <0.1% yield loss\n",
    "   - **Implementation**: Apriori on test failures, remove tests with confidence>0.95 prediction\n",
    "   - **Business Value**: Reduce ATE time, increase throughput\n",
    "\n",
    "2. **Failure Mode Taxonomy Builder** üí∞ $15M+ Debug Efficiency\n",
    "   - **Objective**: Automatically discover and categorize defect signatures\n",
    "   - **Features**: 50+ parametric tests, spatial wafer location, lot/wafer ID, equipment\n",
    "   - **Success Metric**: Identify 10+ systematic failure modes, 40% faster FA\n",
    "   - **Implementation**: Hierarchical Apriori, cluster high-lift rules\n",
    "   - **Business Value**: Systematic defect detection, root cause prioritization\n",
    "\n",
    "3. **Multi-Site Correlation Engine** üí∞ $8M+ Quality Control\n",
    "   - **Objective**: Discover equipment-specific failure patterns across 10+ sites\n",
    "   - **Features**: Test results + site_id + tester_id + handler_id + timestamp\n",
    "   - **Success Metric**: Detect 5+ site-specific issues, 7 days earlier\n",
    "   - **Implementation**: Per-site Apriori, compare rule sets, flag unique patterns\n",
    "   - **Business Value**: Equipment health monitoring, process variation detection\n",
    "\n",
    "4. **Supplier Quality Fingerprint Analyzer** üí∞ $20M+ Supply Chain\n",
    "   - **Objective**: Identify supplier-specific defect signatures for incoming inspection\n",
    "   - **Features**: Parametric tests + supplier_id + lot_id + date_code\n",
    "   - **Success Metric**: 95% supplier classification accuracy, <2% false reject\n",
    "   - **Implementation**: Association rules per supplier, anomaly = missing expected rules\n",
    "   - **Business Value**: Counterfeit detection, supplier qualification\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "5. **E-Commerce Recommendation Engine** üí∞ $50M+ Revenue Increase\n",
    "   - **Objective**: Product recommendations via market basket analysis\n",
    "   - **Features**: Order history, product categories, user segments\n",
    "   - **Success Metric**: 15% conversion rate increase, 20% average order value\n",
    "   - **Implementation**: Apriori on purchase transactions, real-time rule lookup\n",
    "   - **Business Value**: Cross-sell, upsell, personalized bundles\n",
    "\n",
    "6. **Medical Diagnosis Support System** üí∞ $100M+ Healthcare Savings\n",
    "   - **Objective**: Discover symptom combinations predicting rare diseases\n",
    "   - **Features**: Patient symptoms, lab results, demographics, medical history\n",
    "   - **Success Metric**: 90% rare disease detection, 30% earlier diagnosis\n",
    "   - **Implementation**: Apriori on diagnosis records, flag novel symptom patterns\n",
    "   - **Business Value**: Early intervention, reduced treatment costs\n",
    "\n",
    "7. **Fraud Detection Pattern Miner** üí∞ $200M+ Fraud Prevention\n",
    "   - **Objective**: Discover multi-transaction fraud patterns\n",
    "   - **Features**: Transaction sequences, merchant categories, amounts, locations\n",
    "   - **Success Metric**: 85% fraud ring detection, <0.5% false positive\n",
    "   - **Implementation**: Sequential Apriori (time-aware), detect abnormal sequences\n",
    "   - **Business Value**: Organized fraud detection, network analysis\n",
    "\n",
    "8. **Network Security Intrusion Signatures** üí∞ $80M+ Breach Prevention\n",
    "   - **Objective**: Mine multi-step attack patterns from network logs\n",
    "   - **Features**: IP addresses, ports, protocols, timestamps, packet signatures\n",
    "   - **Success Metric**: 95% zero-day detection, <100ms rule evaluation\n",
    "   - **Implementation**: Streaming Apriori on sliding windows, alert on novel patterns\n",
    "   - **Business Value**: Early attack detection, automated response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82fa046",
   "metadata": {},
   "source": [
    "## üîç Key Takeaways\n",
    "\n",
    "### ‚úÖ When to Use Association Rule Mining\n",
    "- **Transactional data**: Sets of items/events per entity (purchases, tests, symptoms)\n",
    "- **Pattern discovery**: Unsupervised, no labels needed\n",
    "- **Correlation analysis**: Find items that co-occur more than expected\n",
    "- **Large datasets**: Millions of transactions (Apriori scales with pruning)\n",
    "- **Interpretable insights**: Business users understand \"if-then\" rules\n",
    "\n",
    "### ‚ùå Limitations\n",
    "- **Binary data**: Works with presence/absence, not quantities (use FP-Growth for weighted)\n",
    "- **Many false positives**: Low support/confidence can yield spurious rules\n",
    "- **Computationally expensive**: Exponential itemset space (2^n)\n",
    "- **No causality**: Correlation ‚â† causation (lift just measures association)\n",
    "- **Parameter sensitivity**: min_support/confidence require domain expertise\n",
    "\n",
    "### üîß Best Practices\n",
    "1. **Start with high min_support** (0.05-0.1): Avoid rare itemsets initially\n",
    "2. **Use lift > 1.0**: Filter independent/negatively correlated rules\n",
    "3. **Domain validation**: Verify rules make business sense (not just statistical)\n",
    "4. **Hierarchical items**: Group similar items (e.g., all dairy products)\n",
    "5. **Temporal analysis**: Consider sequence (A before B, not just A with B)\n",
    "6. **Stratify**: Run separate analyses per segment (high/low spenders, different sites)\n",
    "\n",
    "### üìä Metrics Guide\n",
    "\n",
    "| Metric | Formula | Interpretation | Typical Threshold |\n",
    "|--------|---------|----------------|-------------------|\n",
    "| **Support** | freq(X‚à™Y) / N | How often itemset appears | 0.01-0.1 (1-10%) |\n",
    "| **Confidence** | support(X‚à™Y) / support(X) | P(Y|X), rule accuracy | 0.5-0.9 (50-90%) |\n",
    "| **Lift** | confidence / support(Y) | Correlation strength | >1.0 (positive) |\n",
    "| **Leverage** | support(X‚à™Y) - support(X)√ósupport(Y) | Absolute correlation | >0.01 |\n",
    "| **Conviction** | [1-support(Y)] / [1-confidence] | Rule strength (‚àû if perfect) | >1.1 |\n",
    "\n",
    "### üöÄ Next Steps\n",
    "- **FP-Growth**: Faster algorithm (no candidate generation)\n",
    "- **Sequential patterns**: Time-ordered itemsets (e.g., customer journey)\n",
    "- **Multi-level association**: Hierarchical itemsets (product ‚Üí category)\n",
    "- **Quantitative rules**: Handle numeric attributes (age, price)\n",
    "\n",
    "### üî¨ Algorithm Variants\n",
    "- **Apriori**: Classic, simple, works for small-medium data\n",
    "- **FP-Growth**: Faster, tree-based, no candidate generation (use for >100K transactions)\n",
    "- **Eclat**: Vertical data format, depth-first search\n",
    "- **OPUS**: Direct mining without support threshold"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
