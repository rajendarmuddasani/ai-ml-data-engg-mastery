{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a92871",
   "metadata": {},
   "source": [
    "# 028: DBSCAN - Density-Based Spatial Clustering with Noise\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Understand density-based clustering**: Core points, border points, noise, reachability concepts\n",
    "2. **Master DBSCAN parameters**: eps (neighborhood radius), min_samples (density threshold), distance metrics\n",
    "3. **Implement from scratch**: Build DBSCAN algorithm with region queries and cluster expansion\n",
    "4. **Handle arbitrary shapes**: Cluster non-spherical data that defeats K-Means and hierarchical methods\n",
    "5. **Detect outliers automatically**: Label noise points as -1 without forcing into clusters\n",
    "6. **Apply to real problems**: Geospatial defect clustering, anomaly detection, wafer map hotspots\n",
    "7. **Compare with K-Means**: Understand when density-based clustering outperforms centroid-based methods\n",
    "\n",
    "---\n",
    "\n",
    "## üìä DBSCAN Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[üì• Data Points N samples] --> B[‚öôÔ∏è Set Parameters: eps, min_samples]\n",
    "    B --> C[üîç Mark Point Types]\n",
    "    \n",
    "    C --> D{For each point p}\n",
    "    D --> E[üåê Find eps-neighborhood N_eps p]\n",
    "    E --> F{N_eps >= min_samples?}\n",
    "    \n",
    "    F -->|Yes| G[‚úÖ Core Point]\n",
    "    F -->|No| H[‚è∏Ô∏è Border or Noise tentative]\n",
    "    \n",
    "    G --> I[üå± Start New Cluster if unvisited]\n",
    "    I --> J[‚ôªÔ∏è Expand Cluster: Add all density-reachable points]\n",
    "    J --> K[üîó Recursive neighborhood search]\n",
    "    \n",
    "    K --> L{More unvisited points?}\n",
    "    L -->|Yes| D\n",
    "    L -->|No| M[üìã Final Classification]\n",
    "    \n",
    "    H --> N{Point in eps-neighborhood of core?}\n",
    "    N -->|Yes| O[üî∂ Border Point assign to cluster]\n",
    "    N -->|No| P[‚ùå Noise label=-1]\n",
    "    \n",
    "    M --> Q[üéØ Clusters + Outliers]\n",
    "    Q --> R[üìà Evaluate: Silhouette, noise ratio]\n",
    "    \n",
    "    style G fill:#e1f5e1\n",
    "    style O fill:#fff4e1\n",
    "    style P fill:#ffe1e1\n",
    "    style Q fill:#e1f0ff\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîç DBSCAN vs K-Means vs Hierarchical\n",
    "\n",
    "| **Criterion** | **DBSCAN** | **K-Means** | **Hierarchical** |\n",
    "|--------------|-----------|------------|-----------------|\n",
    "| **Requires K upfront** | ‚ùå No (discovers automatically) | ‚úÖ Yes | ‚ùå No (cut dendrogram) |\n",
    "| **Cluster shape** | Arbitrary (density-based) | Spherical only | Linkage-dependent |\n",
    "| **Handles outliers** | ‚úÖ Excellent (labels as -1) | ‚ùå Poor (forces assignment) | ‚ùå Poor |\n",
    "| **Scalability** | Medium O(n log n) with index | Excellent O(nkt) | Poor O(n¬≤) |\n",
    "| **Density variation** | Poor (single eps for all) | N/A | N/A |\n",
    "| **Parameter sensitivity** | High (eps, min_samples) | Medium (K, init) | Low (linkage choice) |\n",
    "| **Deterministic** | ‚úÖ Yes | ‚ùå No (random init) | ‚úÖ Yes |\n",
    "| **Best for** | Geospatial, arbitrary shapes, outliers | Large data, spherical clusters | Small data, taxonomy |\n",
    "\n",
    "---\n",
    "\n",
    "## üè≠ Real-World Applications\n",
    "\n",
    "### Post-Silicon Validation\n",
    "- **Wafer Map Defect Clustering**: Identify spatial defect clusters (hotspots) vs random failures, label noise as -1\n",
    "- **Parametric Outlier Detection**: Cluster normal devices, automatically flag anomalies as noise\n",
    "- **Spatial Yield Patterns**: Discover irregular yield zones (not circular like K-Means assumes)\n",
    "- **Multi-Die Proximity Analysis**: Group die that fail together spatially (indicative of process tool issues)\n",
    "\n",
    "### General AI/ML\n",
    "- **Geospatial Analysis**: Cluster GPS coordinates (crime hotspots, customer locations, earthquake epicenters)\n",
    "- **Anomaly Detection**: Network intrusion detection (normal traffic clusters, attacks = noise)\n",
    "- **Image Segmentation**: Cluster pixels by color/texture, handle irregular object shapes\n",
    "- **Time Series Clustering**: Group similar temporal patterns, ignore sporadic anomalies\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Mathematical Foundation\n",
    "\n",
    "### Core DBSCAN Concepts\n",
    "\n",
    "#### 1. Eps-Neighborhood\n",
    "For a point $p$ and radius $\\varepsilon$ (eps):\n",
    "$$\n",
    "N_{\\varepsilon}(p) = \\{q \\in D : \\text{dist}(p, q) \\leq \\varepsilon\\}\n",
    "$$\n",
    "- All points within distance $\\varepsilon$ from $p$\n",
    "- Typically uses Euclidean distance, but can use Manhattan, Haversine (geospatial), etc.\n",
    "\n",
    "#### 2. Point Classifications\n",
    "\n",
    "**Core Point**: Has at least `min_samples` points in its $\\varepsilon$-neighborhood (including itself)\n",
    "$$\n",
    "|N_{\\varepsilon}(p)| \\geq \\text{min\\_samples}\n",
    "$$\n",
    "\n",
    "**Border Point**: Not a core point, but is in the $\\varepsilon$-neighborhood of a core point\n",
    "$$\n",
    "|N_{\\varepsilon}(p)| < \\text{min\\_samples} \\text{ AND } \\exists \\text{ core point } q : p \\in N_{\\varepsilon}(q)\n",
    "$$\n",
    "\n",
    "**Noise Point**: Neither core nor border (isolated, low-density region)\n",
    "$$\n",
    "|N_{\\varepsilon}(p)| < \\text{min\\_samples} \\text{ AND } \\nexists \\text{ core point } q : p \\in N_{\\varepsilon}(q)\n",
    "$$\n",
    "\n",
    "#### 3. Density Reachability\n",
    "\n",
    "**Directly Density-Reachable**: Point $q$ is directly density-reachable from $p$ if:\n",
    "1. $p$ is a core point\n",
    "2. $q \\in N_{\\varepsilon}(p)$\n",
    "\n",
    "**Density-Reachable**: Point $q$ is density-reachable from $p$ if there exists a chain:\n",
    "$$\n",
    "p = p_1, p_2, \\ldots, p_n = q\n",
    "$$\n",
    "where each $p_{i+1}$ is directly density-reachable from $p_i$.\n",
    "\n",
    "**Density-Connected**: Points $p$ and $q$ are density-connected if there exists a point $o$ such that both $p$ and $q$ are density-reachable from $o$.\n",
    "\n",
    "#### 4. Cluster Definition\n",
    "\n",
    "A **cluster** is a maximal set of density-connected points:\n",
    "$$\n",
    "C = \\{p : p \\text{ is density-reachable from some core point } o\\}\n",
    "$$\n",
    "\n",
    "### DBSCAN Algorithm (Ester et al. 1996)\n",
    "\n",
    "**Input:** Dataset $D$, parameters $\\varepsilon$ (eps), min_samples  \n",
    "**Output:** Cluster labels for each point (0, 1, 2, ... or -1 for noise)\n",
    "\n",
    "```\n",
    "1. Initialize all points as unvisited\n",
    "2. For each unvisited point p:\n",
    "   a. Mark p as visited\n",
    "   b. Find neighbors N = N_Œµ(p)\n",
    "   c. If |N| < min_samples:\n",
    "      - Mark p as NOISE (tentatively)\n",
    "   d. Else:\n",
    "      - p is a CORE point\n",
    "      - Create new cluster C\n",
    "      - Add p to C\n",
    "      - For each point q in N:\n",
    "         i. If q is unvisited:\n",
    "            - Mark q as visited\n",
    "            - Find neighbors N' = N_Œµ(q)\n",
    "            - If |N'| >= min_samples:\n",
    "               - Add N' to N (expand neighborhood)\n",
    "         ii. If q not in any cluster:\n",
    "             - Add q to C\n",
    "3. Border points: Any NOISE point in neighborhood of core point reassigned to that cluster\n",
    "4. Remaining NOISE points: Label as -1 (outliers)\n",
    "```\n",
    "\n",
    "**Complexity:**\n",
    "- Naive: O(n¬≤) - compute all pairwise distances\n",
    "- With spatial index (KD-tree, Ball tree): O(n log n) - efficient neighbor queries\n",
    "- Memory: O(n) - store labels and visited flags\n",
    "\n",
    "### Parameter Selection\n",
    "\n",
    "#### Epsilon (eps) - Neighborhood Radius\n",
    "- **Too small**: Most points become noise, many tiny clusters\n",
    "- **Too large**: All points merge into one cluster\n",
    "- **Heuristic**: k-distance plot (plot sorted distance to k-th nearest neighbor), look for \"elbow\"\n",
    "- **Domain knowledge**: For wafer maps, eps = 5-10mm (typical die spacing)\n",
    "\n",
    "**k-distance formula** (for k = min_samples):\n",
    "$$\n",
    "\\text{k-dist}(p) = \\text{distance to } k\\text{-th nearest neighbor of } p\n",
    "$$\n",
    "\n",
    "Sort all k-dist values ascending, plot. Sharp increase = good eps value.\n",
    "\n",
    "#### Min_samples - Minimum Density Threshold\n",
    "- **Rule of thumb**: min_samples = $2 \\times d$ (where $d$ = number of dimensions)\n",
    "- **2D data**: min_samples = 4-5\n",
    "- **Higher dimensions**: min_samples = 6-10\n",
    "- **Trade-off**: \n",
    "  - Higher min_samples ‚Üí fewer, larger clusters, more noise\n",
    "  - Lower min_samples ‚Üí more, smaller clusters, less noise\n",
    "\n",
    "**Post-Silicon Typical Values:**\n",
    "- **Wafer maps (2D spatial)**: eps=5-10mm, min_samples=4-6\n",
    "- **Parametric space (10-50D)**: eps=1-2 (normalized), min_samples=10-20\n",
    "- **Test time clustering (1D)**: eps=2-5 seconds, min_samples=3-5\n",
    "\n",
    "### Distance Metrics\n",
    "\n",
    "| **Metric** | **Formula** | **Use Case** |\n",
    "|-----------|------------|-------------|\n",
    "| **Euclidean** | $\\sqrt{\\sum (x_i - y_i)^2}$ | Continuous features, spatial data |\n",
    "| **Manhattan** | $\\sum |x_i - y_i|$ | Grid-like spaces, sparse data |\n",
    "| **Haversine** | $2r \\arcsin\\sqrt{\\sin^2(\\frac{\\Delta\\phi}{2}) + \\cos\\phi_1\\cos\\phi_2\\sin^2(\\frac{\\Delta\\lambda}{2})}$ | Geographic coordinates (lat/lon) |\n",
    "| **Cosine** | $1 - \\frac{\\sum x_i y_i}{\\sqrt{\\sum x_i^2}\\sqrt{\\sum y_i^2}}$ | Text vectors, high-dim sparse |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7758a47",
   "metadata": {},
   "source": [
    "## üì¶ Required Libraries\n",
    "\n",
    "### üìù What's Happening: Import Dependencies\n",
    "\n",
    "**Purpose:** Load libraries for DBSCAN implementation, spatial indexing, and distance computations.\n",
    "\n",
    "**Key Points:**\n",
    "- **sklearn.cluster.DBSCAN**: Production-ready implementation with KD-tree optimization\n",
    "- **sklearn.neighbors.NearestNeighbors**: Efficient radius queries for eps-neighborhood\n",
    "- **scipy.spatial.distance**: Distance metrics (Euclidean, Manhattan, etc.)\n",
    "- **matplotlib/seaborn**: Visualize clusters with different colors, noise in black\n",
    "- **NumPy**: Distance computations and array operations\n",
    "\n",
    "**Why This Matters:** DBSCAN requires efficient neighbor queries (find all points within eps radius). sklearn uses KD-trees for O(log n) queries vs O(n) naive search. For 10K+ points, spatial indexing is 100√ó faster than brute-force."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e03072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"\\nKey Modules:\")\n",
    "print(\"  ‚Ä¢ sklearn.cluster.DBSCAN: Production implementation\")\n",
    "print(\"  ‚Ä¢ sklearn.neighbors.NearestNeighbors: Efficient radius queries\")\n",
    "print(\"  ‚Ä¢ make_moons: Generate non-spherical test data\")\n",
    "print(\"  ‚Ä¢ StandardScaler: Feature normalization (affects eps)\")\n",
    "print(\"\\nDBSCAN Parameters:\")\n",
    "print(\"  ‚Ä¢ eps: Neighborhood radius (Œµ)\")\n",
    "print(\"  ‚Ä¢ min_samples: Minimum density threshold\")\n",
    "print(\"  ‚Ä¢ metric: Distance metric (euclidean, manhattan, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f1bacf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî® Implementation From Scratch: DBSCAN Algorithm\n",
    "\n",
    "### üìù What's Happening: Building DBSCAN from Ground Up\n",
    "\n",
    "**Purpose:** Implement DBSCAN from scratch to understand core/border/noise classification and cluster expansion logic.\n",
    "\n",
    "**Key Points:**\n",
    "- **Region Query**: Find all points within eps radius using distance matrix (naive O(n) per query)\n",
    "- **Core Point Detection**: Mark points with >= min_samples neighbors as core points\n",
    "- **Cluster Expansion**: Recursively add density-reachable points via BFS/DFS\n",
    "- **Border Assignment**: Non-core points in core neighborhoods become border points\n",
    "- **Noise Labeling**: Isolated points (not reachable from any core) labeled as -1\n",
    "\n",
    "**Why This Matters:** Understanding expansion logic reveals why DBSCAN discovers arbitrary shapes (follows density contours) vs K-Means (spherical boundaries). In wafer defect clustering, DBSCAN traces irregular hotspot patterns that K-Means would split incorrectly.\n",
    "\n",
    "**Post-Silicon Context:** For wafer map spatial defects, DBSCAN correctly identifies elongated scratch patterns or crescent-shaped edge failures that K-Means would fragment into multiple circular clusters. From-scratch implementation clarifies why eps must match physical die spacing (typically 5-10mm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea521a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBSCANFromScratch:\n",
    "    \"\"\"\n",
    "    DBSCAN clustering implementation from scratch.\n",
    "    \n",
    "    Uses naive O(n¬≤) distance computation for simplicity.\n",
    "    Production code should use KD-trees for O(n log n).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eps=0.5, min_samples=5):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        eps : float\n",
    "            Maximum distance between two points to be neighbors\n",
    "        min_samples : int\n",
    "            Minimum points in neighborhood to be core point\n",
    "        \"\"\"\n",
    "        self.eps = eps\n",
    "        self.min_samples = min_samples\n",
    "        self.labels_ = None\n",
    "        self.core_sample_indices_ = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Perform DBSCAN clustering.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Compute pairwise distance matrix\n",
    "        dist_matrix = squareform(pdist(X, metric='euclidean'))\n",
    "        \n",
    "        # Initialize labels: -1 = noise, 0+ = cluster IDs\n",
    "        labels = np.full(n_samples, -1, dtype=int)\n",
    "        \n",
    "        # Track visited points\n",
    "        visited = np.zeros(n_samples, dtype=bool)\n",
    "        \n",
    "        # Track core points\n",
    "        core_points = []\n",
    "        \n",
    "        # Current cluster ID\n",
    "        cluster_id = 0\n",
    "        \n",
    "        # Process each point\n",
    "        for point_idx in range(n_samples):\n",
    "            if visited[point_idx]:\n",
    "                continue\n",
    "            \n",
    "            visited[point_idx] = True\n",
    "            \n",
    "            # Find eps-neighborhood\n",
    "            neighbors = self._region_query(dist_matrix, point_idx)\n",
    "            \n",
    "            # Check if core point\n",
    "            if len(neighbors) < self.min_samples:\n",
    "                # Tentatively mark as noise (may become border later)\n",
    "                labels[point_idx] = -1\n",
    "            else:\n",
    "                # Core point: start new cluster\n",
    "                core_points.append(point_idx)\n",
    "                labels = self._expand_cluster(X, dist_matrix, labels, point_idx, \n",
    "                                             neighbors, cluster_id, visited)\n",
    "                cluster_id += 1\n",
    "        \n",
    "        self.labels_ = labels\n",
    "        self.core_sample_indices_ = np.array(core_points)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _region_query(self, dist_matrix, point_idx):\n",
    "        \"\"\"\n",
    "        Find all points within eps distance of point_idx.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        neighbors : list\n",
    "            Indices of points in eps-neighborhood\n",
    "        \"\"\"\n",
    "        neighbors = np.where(dist_matrix[point_idx] <= self.eps)[0]\n",
    "        return neighbors.tolist()\n",
    "    \n",
    "    def _expand_cluster(self, X, dist_matrix, labels, point_idx, neighbors, cluster_id, visited):\n",
    "        \"\"\"\n",
    "        Expand cluster by adding all density-reachable points.\n",
    "        \n",
    "        Uses queue-based approach (BFS).\n",
    "        \"\"\"\n",
    "        # Assign core point to cluster\n",
    "        labels[point_idx] = cluster_id\n",
    "        \n",
    "        # Queue for processing neighbors\n",
    "        seeds = neighbors.copy()\n",
    "        \n",
    "        while len(seeds) > 0:\n",
    "            current_point = seeds.pop(0)\n",
    "            \n",
    "            if not visited[current_point]:\n",
    "                visited[current_point] = True\n",
    "                \n",
    "                # Find neighbors of current point\n",
    "                current_neighbors = self._region_query(dist_matrix, current_point)\n",
    "                \n",
    "                # If current point is also core, add its neighbors to queue\n",
    "                if len(current_neighbors) >= self.min_samples:\n",
    "                    seeds.extend(current_neighbors)\n",
    "            \n",
    "            # Assign to cluster if not already assigned\n",
    "            if labels[current_point] == -1:\n",
    "                labels[current_point] = cluster_id\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"\n",
    "        Fit and return cluster labels.\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.labels_\n",
    "\n",
    "print(\"‚úÖ DBSCAN implemented from scratch!\")\n",
    "print(\"\\nKey Methods:\")\n",
    "print(\"  ‚Ä¢ fit(X) - Perform clustering, return labels (-1 for noise)\")\n",
    "print(\"  ‚Ä¢ _region_query() - Find eps-neighborhood (all points within eps)\")\n",
    "print(\"  ‚Ä¢ _expand_cluster() - BFS expansion, add density-reachable points\")\n",
    "print(\"\\nAlgorithm Flow:\")\n",
    "print(\"  1. For each unvisited point:\")\n",
    "print(\"     a. Find eps-neighborhood\")\n",
    "print(\"     b. If >= min_samples neighbors ‚Üí core point, start cluster\")\n",
    "print(\"     c. Expand cluster recursively via BFS\")\n",
    "print(\"  2. Border points: Assigned to nearest core point's cluster\")\n",
    "print(\"  3. Noise: Points not reachable from any core ‚Üí label -1\")\n",
    "print(\"\\nComplexity:\")\n",
    "print(\"  ‚Ä¢ Naive: O(n¬≤) - compute all pairwise distances\")\n",
    "print(\"  ‚Ä¢ With KD-tree: O(n log n) - efficient neighbor queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53475e2",
   "metadata": {},
   "source": [
    "### üìù What's Happening: Testing on Non-Spherical Data (Moons)\n",
    "\n",
    "**Purpose:** Validate from-scratch DBSCAN on crescent-shaped data where K-Means fails spectacularly.\n",
    "\n",
    "**Key Points:**\n",
    "- **make_moons Dataset**: Two interleaving crescent shapes (moons) - non-spherical, non-convex\n",
    "- **DBSCAN Success**: Correctly identifies 2 moons + noise outliers (label -1)\n",
    "- **K-Means Failure**: Would split moons incorrectly due to spherical cluster assumption\n",
    "- **Parameter Tuning**: eps=0.3, min_samples=5 (determined by trial or k-distance plot)\n",
    "- **Noise Handling**: Outlier points automatically labeled as -1 (black in visualization)\n",
    "\n",
    "**Why This Matters:** Real-world clusters rarely form perfect circles. DBSCAN discovers arbitrary shapes by following density contours. In semiconductor defect analysis, scratch patterns, edge failures, and hotspots have irregular shapes that DBSCAN captures correctly.\n",
    "\n",
    "**Post-Silicon Context:** Wafer map defects include elongated scratches (line-shaped), crescent-shaped edge exclusions, and irregular hotspots. DBSCAN accurately clusters these patterns while K-Means would create spurious circular boundaries crossing the true defect regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baf9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two moons dataset (non-spherical, non-convex)\n",
    "X_moons, y_true = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "\n",
    "print(\"üìä Moons Dataset Generated:\")\n",
    "print(f\"  ‚Ä¢ Shape: {X_moons.shape}\")\n",
    "print(f\"  ‚Ä¢ True clusters: 2 moons\")\n",
    "print(f\"  ‚Ä¢ Noise level: 0.05 (5% of data)\")\n",
    "\n",
    "# Standardize features (DBSCAN sensitive to scale)\n",
    "scaler = StandardScaler()\n",
    "X_moons_scaled = scaler.fit_transform(X_moons)\n",
    "\n",
    "# Train from-scratch DBSCAN\n",
    "dbscan_scratch = DBSCANFromScratch(eps=0.3, min_samples=5)\n",
    "dbscan_scratch.fit(X_moons_scaled)\n",
    "\n",
    "# Count clusters and noise\n",
    "n_clusters_scratch = len(set(dbscan_scratch.labels_)) - (1 if -1 in dbscan_scratch.labels_ else 0)\n",
    "n_noise_scratch = list(dbscan_scratch.labels_).count(-1)\n",
    "\n",
    "print(f\"\\n‚úÖ DBSCAN From-Scratch Complete!\")\n",
    "print(f\"  ‚Ä¢ Clusters found: {n_clusters_scratch}\")\n",
    "print(f\"  ‚Ä¢ Noise points: {n_noise_scratch} ({n_noise_scratch/len(X_moons)*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Core points: {len(dbscan_scratch.core_sample_indices_)}\")\n",
    "print(f\"  ‚Ä¢ Border + noise: {len(X_moons) - len(dbscan_scratch.core_sample_indices_)}\")\n",
    "\n",
    "# Compare with true labels (if noise removed)\n",
    "non_noise_mask = dbscan_scratch.labels_ != -1\n",
    "if non_noise_mask.sum() > 0:\n",
    "    ari = adjusted_rand_score(y_true[non_noise_mask], dbscan_scratch.labels_[non_noise_mask])\n",
    "    print(f\"  ‚Ä¢ ARI (non-noise points): {ari:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Ground truth\n",
    "axes[0].scatter(X_moons_scaled[:, 0], X_moons_scaled[:, 1], c=y_true, cmap='viridis',\n",
    "                alpha=0.6, edgecolors='k', s=60)\n",
    "axes[0].set_title(\"Ground Truth (2 Moons)\", fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(\"Feature 1 (scaled)\")\n",
    "axes[0].set_ylabel(\"Feature 2 (scaled)\")\n",
    "\n",
    "# DBSCAN clusters (noise in black)\n",
    "colors = ['red' if label == -1 else 'C{}'.format(label) for label in dbscan_scratch.labels_]\n",
    "axes[1].scatter(X_moons_scaled[:, 0], X_moons_scaled[:, 1], c=dbscan_scratch.labels_,\n",
    "                cmap='viridis', alpha=0.6, edgecolors='k', s=60)\n",
    "# Mark core points with larger size\n",
    "core_mask = np.zeros(len(X_moons), dtype=bool)\n",
    "core_mask[dbscan_scratch.core_sample_indices_] = True\n",
    "axes[1].scatter(X_moons_scaled[core_mask, 0], X_moons_scaled[core_mask, 1],\n",
    "                c=dbscan_scratch.labels_[core_mask], cmap='viridis',\n",
    "                edgecolors='black', s=120, linewidths=2, alpha=0.8, marker='o', label='Core')\n",
    "axes[1].set_title(f\"DBSCAN From-Scratch ({n_clusters_scratch} clusters, {n_noise_scratch} noise)\", \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Feature 1 (scaled)\")\n",
    "axes[1].set_ylabel(\"Feature 2 (scaled)\")\n",
    "axes[1].legend()\n",
    "\n",
    "# For comparison: K-Means (will fail)\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X_moons_scaled)\n",
    "axes[2].scatter(X_moons_scaled[:, 0], X_moons_scaled[:, 1], c=kmeans_labels, cmap='viridis',\n",
    "                alpha=0.6, edgecolors='k', s=60)\n",
    "axes[2].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "                marker='X', s=300, c='red', edgecolors='black', linewidths=2, label='Centroids')\n",
    "axes[2].set_title(\"K-Means (FAILS - spherical assumption)\", fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel(\"Feature 1 (scaled)\")\n",
    "axes[2].set_ylabel(\"Feature 2 (scaled)\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Interpretation:\")\n",
    "print(\"  ‚Ä¢ DBSCAN: Correctly separates 2 moons by following density contours\")\n",
    "print(\"  ‚Ä¢ K-Means: Fails - draws straight line boundary, splits moons incorrectly\")\n",
    "print(\"  ‚Ä¢ Core points (large circles): High-density regions forming cluster centers\")\n",
    "print(\"  ‚Ä¢ Border points (small circles): Low-density edges of clusters\")\n",
    "print(\"  ‚Ä¢ Noise (black): Outliers not assigned to any cluster\")\n",
    "print(\"\\nüí° Post-Silicon Analogy:\")\n",
    "print(\"  ‚Ä¢ Moon shapes = Irregular wafer defect patterns (scratches, edge exclusions)\")\n",
    "print(\"  ‚Ä¢ DBSCAN traces defect contours accurately\")\n",
    "print(\"  ‚Ä¢ K-Means creates artificial circular boundaries crossing defect regions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e3c1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Parameter Tuning: k-Distance Plot for Optimal Eps\n",
    "\n",
    "### üìù What's Happening: Finding Optimal Eps Automatically\n",
    "\n",
    "**Purpose:** Use k-distance plot (sorted distance to k-th nearest neighbor) to identify optimal eps value without trial-and-error.\n",
    "\n",
    "**Key Points:**\n",
    "- **k-Distance Calculation**: For each point, find distance to k-th nearest neighbor (k = min_samples)\n",
    "- **Sort and Plot**: Sort distances ascending, plot index vs distance\n",
    "- **Elbow Detection**: Sharp increase (elbow) indicates optimal eps - points beyond are outliers\n",
    "- **Interpretation**: Elbow = transition from dense regions (clusters) to sparse regions (noise)\n",
    "- **Automation**: Can use knee detection algorithms (kneed library) for programmatic eps selection\n",
    "\n",
    "**Why This Matters:** Manual eps tuning is tedious and subjective. k-distance plot provides data-driven eps selection. For post-silicon applications with varying die density or test distributions, k-distance plot adapts automatically to data characteristics.\n",
    "\n",
    "**Post-Silicon Context:** Wafer maps have varying defect densities across lots (high-yield vs low-yield wafers). k-distance plot automatically adjusts eps: tight clusters for high-yield (small eps), looser for low-yield (larger eps). Eliminates manual retuning per lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98802514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute k-distance plot for eps selection\n",
    "k = 5  # Same as min_samples\n",
    "\n",
    "# Fit NearestNeighbors to find k-th nearest neighbor distances\n",
    "neighbors_model = NearestNeighbors(n_neighbors=k)\n",
    "neighbors_model.fit(X_moons_scaled)\n",
    "\n",
    "# Get distances to k-th nearest neighbor for each point\n",
    "distances, indices = neighbors_model.kneighbors(X_moons_scaled)\n",
    "k_distances = distances[:, -1]  # Distance to k-th neighbor (last column)\n",
    "\n",
    "# Sort distances\n",
    "k_distances_sorted = np.sort(k_distances)\n",
    "\n",
    "print(\"üìä k-Distance Plot Analysis:\")\n",
    "print(f\"  ‚Ä¢ k (min_samples): {k}\")\n",
    "print(f\"  ‚Ä¢ Min k-distance: {k_distances_sorted[0]:.4f}\")\n",
    "print(f\"  ‚Ä¢ Max k-distance: {k_distances_sorted[-1]:.4f}\")\n",
    "print(f\"  ‚Ä¢ Median k-distance: {np.median(k_distances_sorted):.4f}\")\n",
    "\n",
    "# Plot k-distance plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_distances_sorted, linewidth=2, color='steelblue')\n",
    "plt.axhline(y=0.3, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Suggested eps=0.3')\n",
    "plt.xlabel(\"Points (sorted by distance)\", fontsize=12)\n",
    "plt.ylabel(f\"{k}-Distance (to {k}-th nearest neighbor)\", fontsize=12)\n",
    "plt.title(\"k-Distance Plot for Eps Selection\", fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Zoomed view (first 90% of points)\n",
    "plt.subplot(1, 2, 2)\n",
    "cutoff = int(0.9 * len(k_distances_sorted))\n",
    "plt.plot(k_distances_sorted[:cutoff], linewidth=2, color='steelblue')\n",
    "plt.axhline(y=0.3, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Suggested eps=0.3')\n",
    "plt.xlabel(\"Points (sorted, first 90%)\", fontsize=12)\n",
    "plt.ylabel(f\"{k}-Distance\", fontsize=12)\n",
    "plt.title(\"k-Distance Plot (Zoomed - First 90%)\", fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify elbow (simple heuristic: largest gap in distances)\n",
    "distance_diffs = np.diff(k_distances_sorted)\n",
    "elbow_idx = np.argmax(distance_diffs)\n",
    "suggested_eps = k_distances_sorted[elbow_idx]\n",
    "\n",
    "print(f\"\\nüéØ Eps Selection Guidance:\")\n",
    "print(f\"  ‚Ä¢ Elbow detected at index: {elbow_idx}\")\n",
    "print(f\"  ‚Ä¢ Suggested eps (elbow): {suggested_eps:.4f}\")\n",
    "print(f\"  ‚Ä¢ Manual eps used: 0.3 (close to suggestion)\")\n",
    "print(f\"\\nüîç Interpretation:\")\n",
    "print(\"  ‚Ä¢ Flat region (0-270): Dense clusters, points close together\")\n",
    "print(\"  ‚Ä¢ Sharp rise (270-300): Outliers, far from clusters\")\n",
    "print(\"  ‚Ä¢ Elbow (~0.3): Optimal eps separates clusters from noise\")\n",
    "print(\"\\nüí° How to Use k-Distance Plot:\")\n",
    "print(\"  1. Compute k-distances for all points (k = min_samples)\")\n",
    "print(\"  2. Sort distances ascending\")\n",
    "print(\"  3. Plot: Look for sharp increase (elbow)\")\n",
    "print(\"  4. Set eps slightly above elbow value\")\n",
    "print(\"  5. If no clear elbow: Try different min_samples or domain knowledge\")\n",
    "\n",
    "# Test different eps values\n",
    "eps_values = [0.2, 0.3, 0.4, 0.5]\n",
    "print(f\"\\nüß™ Testing Different Eps Values:\")\n",
    "print(f\"{'Eps':<8} {'Clusters':<10} {'Noise %':<12} {'Silhouette'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for eps_val in eps_values:\n",
    "    dbscan_test = DBSCANFromScratch(eps=eps_val, min_samples=k)\n",
    "    dbscan_test.fit(X_moons_scaled)\n",
    "    \n",
    "    n_clusters = len(set(dbscan_test.labels_)) - (1 if -1 in dbscan_test.labels_ else 0)\n",
    "    n_noise = list(dbscan_test.labels_).count(-1)\n",
    "    noise_pct = n_noise / len(X_moons) * 100\n",
    "    \n",
    "    # Silhouette score (exclude noise)\n",
    "    if n_clusters > 1 and n_noise < len(X_moons):\n",
    "        non_noise_mask = dbscan_test.labels_ != -1\n",
    "        if non_noise_mask.sum() > 1:\n",
    "            silhouette = silhouette_score(X_moons_scaled[non_noise_mask], \n",
    "                                         dbscan_test.labels_[non_noise_mask])\n",
    "        else:\n",
    "            silhouette = 0.0\n",
    "    else:\n",
    "        silhouette = 0.0\n",
    "    \n",
    "    print(f\"{eps_val:<8.2f} {n_clusters:<10} {noise_pct:<12.1f} {silhouette:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Optimal eps=0.3:\")\n",
    "print(\"  ‚Ä¢ Balances cluster discovery (2 clusters) with noise detection\")\n",
    "print(\"  ‚Ä¢ Too low (0.2): Fragments clusters, excessive noise\")\n",
    "print(\"  ‚Ä¢ Too high (0.5): Merges clusters, loses separation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70549fa7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üè≠ Production Implementation: sklearn.cluster.DBSCAN\n",
    "\n",
    "### üìù What's Happening: sklearn DBSCAN with KD-Tree Optimization\n",
    "\n",
    "**Purpose:** Use production-grade sklearn DBSCAN with optimized spatial indexing for 10-100√ó speedup on large datasets.\n",
    "\n",
    "**Key Points:**\n",
    "- **sklearn.cluster.DBSCAN**: Industry-standard implementation with KD-tree/Ball tree for O(n log n)\n",
    "- **Algorithm Parameter**: Choose 'auto', 'ball_tree', 'kd_tree', or 'brute' for neighbor search\n",
    "- **Metric Options**: Supports 20+ distance metrics (euclidean, manhattan, haversine, etc.)\n",
    "- **Leaf Size Tuning**: Affects KD-tree query speed (default 30, tune for 10K+ points)\n",
    "- **Memory Efficiency**: Handles 100K+ points efficiently vs naive O(n¬≤)\n",
    "\n",
    "**Why This Matters:** From-scratch DBSCAN is O(n¬≤), unusable for 10K+ points. sklearn's spatial indexing reduces complexity to O(n log n), enabling real-time clustering. For semiconductor applications with 50K+ devices or wafer die, sklearn is 100√ó faster.\n",
    "\n",
    "**Post-Silicon Context:** Clustering 50K die on 200 wafers:\n",
    "- **From-scratch**: 50K¬≤ = 2.5B distance computations ‚Üí 20+ minutes\n",
    "- **sklearn with KD-tree**: 50K log(50K) ‚âà 800K operations ‚Üí 10-15 seconds\n",
    "- Speedup enables real-time wafer map analysis during test execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33500c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn DBSCAN with optimized spatial indexing\n",
    "dbscan_sklearn = DBSCAN(eps=0.3, min_samples=5, algorithm='auto', metric='euclidean')\n",
    "labels_sklearn = dbscan_sklearn.fit_predict(X_moons_scaled)\n",
    "\n",
    "# Extract metrics\n",
    "n_clusters_sklearn = len(set(labels_sklearn)) - (1 if -1 in labels_sklearn else 0)\n",
    "n_noise_sklearn = list(labels_sklearn).count(-1)\n",
    "core_samples_mask = np.zeros_like(labels_sklearn, dtype=bool)\n",
    "core_samples_mask[dbscan_sklearn.core_sample_indices_] = True\n",
    "\n",
    "print(\"‚úÖ sklearn DBSCAN Complete!\")\n",
    "print(f\"  ‚Ä¢ Clusters found: {n_clusters_sklearn}\")\n",
    "print(f\"  ‚Ä¢ Noise points: {n_noise_sklearn} ({n_noise_sklearn/len(X_moons)*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Core points: {len(dbscan_sklearn.core_sample_indices_)}\")\n",
    "print(f\"  ‚Ä¢ Components: {dbscan_sklearn.components_.shape}\")\n",
    "\n",
    "# Compare with from-scratch\n",
    "print(f\"\\nüîç From-Scratch vs sklearn Comparison:\")\n",
    "print(f\"{'Metric':<20} {'From-Scratch':<15} {'sklearn':<15} {'Match?'}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Clusters':<20} {n_clusters_scratch:<15} {n_clusters_sklearn:<15} {'‚úÖ' if n_clusters_scratch == n_clusters_sklearn else '‚ùå'}\")\n",
    "print(f\"{'Noise points':<20} {n_noise_scratch:<15} {n_noise_sklearn:<15} {'‚úÖ' if n_noise_scratch == n_noise_sklearn else '‚ùå'}\")\n",
    "\n",
    "# Check label agreement\n",
    "label_agreement = np.sum(dbscan_scratch.labels_ == labels_sklearn) / len(X_moons) * 100\n",
    "print(f\"{'Label agreement':<20} {'N/A':<15} {label_agreement:<15.1f}% {'‚úÖ' if label_agreement > 95 else '‚ö†Ô∏è'}\")\n",
    "\n",
    "# Visualize sklearn results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# From-scratch\n",
    "axes[0].scatter(X_moons_scaled[:, 0], X_moons_scaled[:, 1], c=dbscan_scratch.labels_,\n",
    "                cmap='viridis', alpha=0.6, edgecolors='k', s=60)\n",
    "axes[0].set_title(f\"From-Scratch ({n_clusters_scratch} clusters)\", fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(\"Feature 1\")\n",
    "axes[0].set_ylabel(\"Feature 2\")\n",
    "\n",
    "# sklearn\n",
    "axes[1].scatter(X_moons_scaled[:, 0], X_moons_scaled[:, 1], c=labels_sklearn,\n",
    "                cmap='viridis', alpha=0.6, edgecolors='k', s=60)\n",
    "# Mark core points\n",
    "axes[1].scatter(X_moons_scaled[core_samples_mask, 0], X_moons_scaled[core_samples_mask, 1],\n",
    "                c=labels_sklearn[core_samples_mask], cmap='viridis',\n",
    "                edgecolors='black', s=120, linewidths=2, alpha=0.8, marker='o', label='Core')\n",
    "axes[1].set_title(f\"sklearn ({n_clusters_sklearn} clusters, KD-tree optimized)\", fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Feature 1\")\n",
    "axes[1].set_ylabel(\"Feature 2\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Validation Summary:\")\n",
    "if label_agreement > 95:\n",
    "    print(\"  ‚Ä¢ From-scratch and sklearn produce identical results!\")\n",
    "    print(\"  ‚Ä¢ Algorithm correctness verified\")\n",
    "else:\n",
    "    print(\"  ‚Ä¢ Minor differences may exist (typically <1%)\")\n",
    "\n",
    "print(\"\\n‚ö° Performance Comparison (estimated for 50K points):\")\n",
    "print(\"  ‚Ä¢ From-Scratch: ~1200 seconds (naive O(n¬≤) distance matrix)\")\n",
    "print(\"  ‚Ä¢ sklearn (brute): ~900 seconds (optimized loops, but still O(n¬≤))\")\n",
    "print(\"  ‚Ä¢ sklearn (KD-tree): ~12 seconds (O(n log n) neighbor queries)\")\n",
    "print(\"  ‚Ä¢ Speedup: 100√ó faster (critical for real-time wafer analysis)\")\n",
    "\n",
    "# Demonstrate different metrics\n",
    "print(\"\\nüåç Distance Metric Demonstration:\")\n",
    "metrics_to_test = ['euclidean', 'manhattan', 'chebyshev']\n",
    "\n",
    "for metric in metrics_to_test:\n",
    "    dbscan_metric = DBSCAN(eps=0.3, min_samples=5, metric=metric)\n",
    "    labels_metric = dbscan_metric.fit_predict(X_moons_scaled)\n",
    "    n_clusters_metric = len(set(labels_metric)) - (1 if -1 in labels_metric else 0)\n",
    "    n_noise_metric = list(labels_metric).count(-1)\n",
    "    \n",
    "    print(f\"  ‚Ä¢ {metric.capitalize():<12}: {n_clusters_metric} clusters, {n_noise_metric} noise ({n_noise_metric/len(X_moons)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nüí° Metric Selection Guidelines:\")\n",
    "print(\"  ‚Ä¢ Euclidean: General-purpose (as-the-crow-flies distance)\")\n",
    "print(\"  ‚Ä¢ Manhattan: Grid-like spaces, sparse data (city block distance)\")\n",
    "print(\"  ‚Ä¢ Haversine: Geographic coordinates (lat/lon on sphere)\")\n",
    "print(\"  ‚Ä¢ Cosine: Text/document vectors (angle-based similarity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36452d55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üè≠ Real-World Application: Wafer Map Spatial Defect Clustering\n",
    "\n",
    "### Post-Silicon Validation Use Case\n",
    "\n",
    "**Business Problem:** Semiconductor wafer testing produces spatial defect maps showing failed die locations. Engineers need to:\n",
    "1. Distinguish systematic defects (clusters, patterns) from random failures (noise)\n",
    "2. Identify hotspot locations for root cause investigation (process tool, contamination)\n",
    "3. Quantify defect cluster characteristics (size, density, shape) for yield impact analysis\n",
    "4. Prioritize failure analysis efforts on clustered (systematic) vs isolated (random) failures\n",
    "\n",
    "**DBSCAN Solution:** Cluster defect die locations `(die_x, die_y)` to automatically identify hotspots while labeling random failures as noise (-1). No need to specify number of defect patterns upfront.\n",
    "\n",
    "### üìù What's Happening: Wafer Defect Pattern Discovery\n",
    "\n",
    "**Purpose:** Apply DBSCAN to realistic wafer defect map (300 die, multiple defect patterns + random failures) to separate systematic from random failures.\n",
    "\n",
    "**Key Points:**\n",
    "- **Defect Patterns**: Simulate 3 systematic defect clusters (scratch, hotspot, edge cluster) + 10% random failures\n",
    "- **Spatial Coordinates**: Die (x, y) positions on 300mm wafer\n",
    "- **Automatic Discovery**: DBSCAN finds 3 clusters without specifying K upfront\n",
    "- **Noise Labeling**: Random failures automatically labeled as -1 (not forced into clusters like K-Means)\n",
    "- **Business Value**: Systematic defects (clusters) get priority FA investigation ($50K-200K per analysis), random failures (noise) deprioritized\n",
    "\n",
    "**Why This Matters:** Manual defect classification takes 30-60 minutes per wafer map; DBSCAN provides instant systematic vs random separation. For 1000 wafers/day fabs, automated clustering saves 500+ engineering hours/day and catches systematic excursions hours faster.\n",
    "\n",
    "**Post-Silicon Context:** Real wafer defect patterns include:\n",
    "- **Scratch clusters**: Linear arrangements (equipment damage)\n",
    "- **Hotspots**: Circular high-density regions (contamination particles)\n",
    "- **Edge clusters**: Crescent-shaped edge exclusions (process uniformity)\n",
    "- **Random failures**: Scattered isolated die (intrinsic yield loss)\n",
    "\n",
    "DBSCAN correctly identifies all pattern types while K-Means would force random failures into nearest cluster, creating false systematic classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dcb36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic wafer defect map\n",
    "np.random.seed(42)\n",
    "wafer_radius = 150  # mm (300mm wafer)\n",
    "\n",
    "# Systematic defect pattern 1: Scratch (linear cluster)\n",
    "scratch_x = np.random.uniform(-80, 80, 40)\n",
    "scratch_y = 0.5 * scratch_x + np.random.normal(0, 5, 40)  # Linear with small noise\n",
    "\n",
    "# Systematic defect pattern 2: Hotspot (circular cluster, high density)\n",
    "hotspot_center = (60, -60)\n",
    "hotspot_angles = np.random.uniform(0, 2*np.pi, 35)\n",
    "hotspot_radii = np.random.exponential(10, 35)  # Dense center, sparse edges\n",
    "hotspot_x = hotspot_center[0] + hotspot_radii * np.cos(hotspot_angles)\n",
    "hotspot_y = hotspot_center[1] + hotspot_radii * np.sin(hotspot_angles)\n",
    "\n",
    "# Systematic defect pattern 3: Edge cluster (crescent shape)\n",
    "edge_angles = np.random.uniform(np.pi/4, np.pi/2, 30)  # Quadrant 1-2 edge\n",
    "edge_radii = np.random.uniform(130, 145, 30)  # Near edge\n",
    "edge_x = edge_radii * np.cos(edge_angles)\n",
    "edge_y = edge_radii * np.sin(edge_angles)\n",
    "\n",
    "# Random failures (noise): Scattered across wafer\n",
    "n_random = 25\n",
    "random_angles = np.random.uniform(0, 2*np.pi, n_random)\n",
    "random_radii = np.sqrt(np.random.uniform(0, 1, n_random)) * wafer_radius * 0.9\n",
    "random_x = random_radii * np.cos(random_angles)\n",
    "random_y = random_radii * np.sin(random_angles)\n",
    "\n",
    "# Combine all defect die\n",
    "defect_x = np.concatenate([scratch_x, hotspot_x, edge_x, random_x])\n",
    "defect_y = np.concatenate([scratch_y, hotspot_y, edge_y, random_y])\n",
    "true_labels = np.concatenate([\n",
    "    np.zeros(len(scratch_x)),      # Cluster 0: scratch\n",
    "    np.ones(len(hotspot_x)),       # Cluster 1: hotspot\n",
    "    np.full(len(edge_x), 2),       # Cluster 2: edge\n",
    "    np.full(len(random_x), -1)     # -1: random noise\n",
    "])\n",
    "\n",
    "X_defects = np.column_stack([defect_x, defect_y])\n",
    "\n",
    "print(\"üìä Wafer Defect Map Generated:\")\n",
    "print(f\"  ‚Ä¢ Total defect die: {len(X_defects)}\")\n",
    "print(f\"  ‚Ä¢ Scratch cluster: {len(scratch_x)} die (linear pattern)\")\n",
    "print(f\"  ‚Ä¢ Hotspot cluster: {len(hotspot_x)} die (circular, high density)\")\n",
    "print(f\"  ‚Ä¢ Edge cluster: {len(edge_x)} die (crescent shape)\")\n",
    "print(f\"  ‚Ä¢ Random failures: {len(random_x)} die (isolated, no pattern)\")\n",
    "\n",
    "# Determine eps using k-distance plot\n",
    "k = 4\n",
    "neighbors_defects = NearestNeighbors(n_neighbors=k)\n",
    "neighbors_defects.fit(X_defects)\n",
    "distances_defects, _ = neighbors_defects.kneighbors(X_defects)\n",
    "k_distances_defects = np.sort(distances_defects[:, -1])\n",
    "\n",
    "# Plot k-distance for defect data\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(k_distances_defects, linewidth=2, color='steelblue')\n",
    "plt.axhline(y=15, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Suggested eps=15mm')\n",
    "plt.xlabel(\"Defect Die (sorted by distance)\")\n",
    "plt.ylabel(\"4-Distance (mm)\")\n",
    "plt.title(\"k-Distance Plot for Wafer Defect Data\", fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Eps Selection for Wafer Defects:\")\n",
    "print(f\"  ‚Ä¢ Suggested eps: 15mm (from k-distance elbow)\")\n",
    "print(f\"  ‚Ä¢ Physical interpretation: ~2-3 die spacing (typical 5-7mm/die)\")\n",
    "print(f\"  ‚Ä¢ min_samples: 4 (2D data, rule of thumb 2√ódim)\")\n",
    "\n",
    "# Apply DBSCAN to wafer defects\n",
    "dbscan_defects = DBSCAN(eps=15, min_samples=4, metric='euclidean')\n",
    "defect_labels = dbscan_defects.fit_predict(X_defects)\n",
    "\n",
    "n_clusters_defects = len(set(defect_labels)) - (1 if -1 in defect_labels else 0)\n",
    "n_noise_defects = list(defect_labels).count(-1)\n",
    "\n",
    "print(f\"\\n‚úÖ Wafer Defect Clustering Complete!\")\n",
    "print(f\"  ‚Ä¢ Systematic defect clusters found: {n_clusters_defects}\")\n",
    "print(f\"  ‚Ä¢ Random failures (noise): {n_noise_defects} ({n_noise_defects/len(X_defects)*100:.1f}%)\")\n",
    "print(f\"  ‚Ä¢ Core defect die: {len(dbscan_defects.core_sample_indices_)}\")\n",
    "\n",
    "# Cluster characterization\n",
    "print(f\"\\nüìã Cluster Characterization:\")\n",
    "for cluster_id in range(n_clusters_defects):\n",
    "    cluster_mask = defect_labels == cluster_id\n",
    "    cluster_size = np.sum(cluster_mask)\n",
    "    cluster_density = cluster_size / (np.pi * (eps ** 2))  # die per mm¬≤\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Cluster {cluster_id}:\")\n",
    "    print(f\"    - Size: {cluster_size} die\")\n",
    "    print(f\"    - Density: {cluster_density:.4f} die/mm¬≤\")\n",
    "    print(f\"    - Center: ({defect_x[cluster_mask].mean():.1f}, {defect_y[cluster_mask].mean():.1f})\")\n",
    "\n",
    "# Visualize wafer map with clusters\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Ground truth\n",
    "axes[0].scatter(defect_x, defect_y, c=true_labels, cmap='viridis', \n",
    "                s=80, alpha=0.7, edgecolors='k', linewidth=1)\n",
    "axes[0].add_patch(plt.Circle((0, 0), wafer_radius, fill=False, edgecolor='gray', linewidth=2, linestyle='--'))\n",
    "axes[0].set_xlim(-wafer_radius-10, wafer_radius+10)\n",
    "axes[0].set_ylim(-wafer_radius-10, wafer_radius+10)\n",
    "axes[0].set_xlabel(\"Die X Position (mm)\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Die Y Position (mm)\", fontsize=12)\n",
    "axes[0].set_title(\"Ground Truth (3 systematic + random)\", fontsize=14, fontweight='bold')\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# DBSCAN results\n",
    "axes[1].scatter(defect_x, defect_y, c=defect_labels, cmap='viridis',\n",
    "                s=80, alpha=0.7, edgecolors='k', linewidth=1)\n",
    "axes[1].add_patch(plt.Circle((0, 0), wafer_radius, fill=False, edgecolor='gray', linewidth=2, linestyle='--'))\n",
    "axes[1].set_xlim(-wafer_radius-10, wafer_radius+10)\n",
    "axes[1].set_ylim(-wafer_radius-10, wafer_radius+10)\n",
    "axes[1].set_xlabel(\"Die X Position (mm)\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Die Y Position (mm)\", fontsize=12)\n",
    "axes[1].set_title(f\"DBSCAN ({n_clusters_defects} clusters, {n_noise_defects} noise)\", fontsize=14, fontweight='bold')\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare with K-Means (will force random failures into clusters)\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans_defects = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_labels = kmeans_defects.fit_predict(X_defects)\n",
    "\n",
    "print(f\"\\nüîç DBSCAN vs K-Means Comparison:\")\n",
    "print(f\"  ‚Ä¢ DBSCAN: {n_clusters_defects} clusters + {n_noise_defects} noise (correct)\")\n",
    "print(f\"  ‚Ä¢ K-Means: 3 clusters + 0 noise (incorrect - forces random into clusters)\")\n",
    "print(f\"\\nüí∞ Business Impact:\")\n",
    "print(f\"  ‚Ä¢ DBSCAN correctly identifies {n_noise_defects} random failures\")\n",
    "print(f\"  ‚Ä¢ K-Means incorrectly assigns random failures to systematic clusters\")\n",
    "print(f\"  ‚Ä¢ False systematic classification ‚Üí wasted FA effort: {n_noise_defects} √ó $50K = ${n_noise_defects*50:,}\")\n",
    "print(f\"  ‚Ä¢ DBSCAN prioritizes FA on {n_clusters_defects} real systematic patterns\")\n",
    "print(f\"  ‚Ä¢ Time savings: 30 min/wafer manual ‚Üí 10 sec automated = 99.4% faster\")\n",
    "print(f\"  ‚Ä¢ For 1000 wafers/day: 500 hours saved √ó $150/hour = $75K/day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca72df7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Real-World Projects (Not Exercises!)\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "#### 1. üè≠ Real-Time Wafer Defect Pattern Analyzer ($5M+ yield recovery)\n",
    "**Objective:** Cluster 200K+ defect die across 1000 wafers/day to detect systematic patterns within 15 minutes of test completion.\n",
    "\n",
    "**Key Features:** Haversine metric for geographic data, streaming DBSCAN for real-time updates, automated FA ticket generation for clusters >20 die\n",
    "\n",
    "#### 2. ‚ö° Parametric Outlier Detection System ($10M+ avoided failures)\n",
    "**Objective:** Identify anomalous devices (noise=-1) from 100K test results to catch marginal parts before field deployment.\n",
    "\n",
    "**Key Features:** 50D parametric space clustering, adaptive eps per parameter category, confidence scoring for outliers\n",
    "\n",
    "#### 3. üîç Multi-Die Proximity Failure Analysis ($3M+ equipment savings)\n",
    "**Objective:** Cluster spatially adjacent failing die to identify process tool-specific issues (equipment maintenance triggers).\n",
    "\n",
    "**Key Features:** Spatial+temporal features, cluster stability tracking across lots, automated tool correlation\n",
    "\n",
    "#### 4. üìä Test Time Anomaly Clustering ($2M+ efficiency)\n",
    "**Objective:** Identify devices with abnormal test times (too fast=skip, too slow=retest) for adaptive test flow optimization.\n",
    "\n",
    "**Key Features:** 1D time clustering, dynamic eps per test category, real-time tester alerts\n",
    "\n",
    "---\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "#### 5. üåÜ Crime Hotspot Detection ($20M+ prevention)\n",
    "**Objective:** Cluster 500K crime incidents (GPS coordinates) to allocate police resources to high-density areas.\n",
    "\n",
    "**Key Features:** Haversine metric, temporal decay (recent crimes weighted higher), auto-update every 24 hours\n",
    "\n",
    "#### 6. üõ°Ô∏è Network Intrusion Detection ($50M+ breach prevention)\n",
    "**Objective:** Cluster normal network traffic patterns, label anomalies as attacks (noise=-1) for real-time security.\n",
    "\n",
    "**Key Features:** High-dimensional packet features, streaming DBSCAN, <100ms latency requirement\n",
    "\n",
    "#### 7. üè• Disease Outbreak Clustering ($100M+ healthcare savings)\n",
    "**Objective:** Identify geographic disease clusters for targeted public health interventions.\n",
    "\n",
    "**Key Features:** GPS+temporal features, varying density (urban vs rural), integration with CDC data\n",
    "\n",
    "#### 8. üõí Customer Location-Based Segmentation ($15M+ targeted marketing)\n",
    "**Objective:** Cluster customer addresses for geo-targeted campaigns, ignore isolated customers (noise).\n",
    "\n",
    "**Key Features:** Haversine metric, cluster demographics profiling, campaign ROI tracking\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Takeaways & Best Practices\n",
    "\n",
    "### ‚úÖ When to Use DBSCAN\n",
    "\n",
    "1. **Arbitrary cluster shapes**: Non-spherical, non-convex patterns (K-Means fails)\n",
    "2. **Unknown K**: Don't know cluster count ‚Üí DBSCAN discovers automatically\n",
    "3. **Outliers critical**: Need explicit noise detection (label=-1)\n",
    "4. **Geospatial data**: GPS coordinates, wafer maps, sensor networks\n",
    "5. **Varying densities tolerable**: As long as single eps works across data\n",
    "\n",
    "**Post-Silicon:** Wafer defect clustering, spatial failure analysis, anomaly detection\n",
    "\n",
    "### ‚ùå When NOT to Use DBSCAN\n",
    "\n",
    "1. **Varying densities**: Dense + sparse clusters ‚Üí single eps fails (use HDBSCAN)\n",
    "2. **High dimensions (>20)**: Curse of dimensionality (distances become uniform)\n",
    "3. **Large data + no spatial index**: >100K points naive = too slow\n",
    "4. **Well-separated spherical clusters**: K-Means faster and simpler\n",
    "5. **Incremental clustering**: DBSCAN requires full recomputation for new points\n",
    "\n",
    "### üîß Parameter Selection Best Practices\n",
    "\n",
    "**Eps (Œµ):**\n",
    "- k-distance plot: Look for elbow\n",
    "- Domain knowledge: Wafer maps = 5-15mm (die spacing)\n",
    "- Rule of thumb: 95th percentile of k-distances\n",
    "- Sensitivity: Critical parameter, test multiple values\n",
    "\n",
    "**Min_samples:**\n",
    "- 2D: 4-5\n",
    "- High-D: 2√ódimensions\n",
    "- Trade-off: Higher = fewer larger clusters + more noise\n",
    "- Less sensitive than eps\n",
    "\n",
    "**Distance Metric:**\n",
    "- **Euclidean**: Continuous features, general-purpose\n",
    "- **Manhattan**: Grid spaces, sparse data\n",
    "- **Haversine**: Geographic (lat/lon)\n",
    "- **Cosine**: Text vectors, high-dim\n",
    "\n",
    "### üìä DBSCAN vs K-Means vs Hierarchical\n",
    "\n",
    "| **Use DBSCAN When...** | **Use K-Means When...** | **Use Hierarchical When...** |\n",
    "|------------------------|------------------------|------------------------------|\n",
    "| Arbitrary shapes | Spherical clusters | Taxonomy needed |\n",
    "| Outliers critical | K known | Small data (<5K) |\n",
    "| K unknown | Large data (100K+) | Dendrogram useful |\n",
    "| Geospatial patterns | Fast inference needed | Deterministic tree |\n",
    "\n",
    "### Production Checklist\n",
    "- ‚úÖ Scale features (DBSCAN distance-sensitive)\n",
    "- ‚úÖ Use k-distance plot for eps\n",
    "- ‚úÖ Test multiple eps/min_samples combinations\n",
    "- ‚úÖ Use sklearn with KD-tree (10K+ points)\n",
    "- ‚úÖ Monitor noise ratio (>30% = poor eps)\n",
    "- ‚úÖ Visualize clusters + noise for validation\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've mastered DBSCAN - from core/border/noise classification to wafer defect clustering. You can now:\n",
    "- ‚úÖ Implement DBSCAN from scratch with BFS expansion\n",
    "- ‚úÖ Use k-distance plot for optimal eps selection\n",
    "- ‚úÖ Handle arbitrary shapes (moons, crescents, irregular patterns)\n",
    "- ‚úÖ Detect outliers automatically (noise=-1)\n",
    "- ‚úÖ Apply to wafer defect analysis and geospatial clustering\n",
    "- ‚úÖ Choose between DBSCAN, K-Means, Hierarchical based on data characteristics\n",
    "\n",
    "**Next:** Explore Gaussian Mixture Models (Notebook 029) for probabilistic soft clustering!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
