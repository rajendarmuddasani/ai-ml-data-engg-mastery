{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7330326",
   "metadata": {},
   "source": [
    "# 027: Hierarchical Clustering - Tree-Based Cluster Discovery\n",
    "\n",
    "## \ud83c\udfaf Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Understand hierarchical clustering theory**: Agglomerative vs divisive, linkage methods, dendrogram interpretation\n",
    "2. **Master linkage criteria**: Single, complete, average, Ward's method - when to use each\n",
    "3. **Implement from scratch**: Build agglomerative clustering algorithm with distance matrix updates\n",
    "4. **Visualize dendrograms**: Interpret tree structures, choose optimal cut height, identify natural groupings\n",
    "5. **Apply to real problems**: Test hierarchy discovery, failure mode taxonomy, wafer similarity trees\n",
    "6. **Compare with K-Means**: Understand trade-offs (no K required vs O(n\u00b2) complexity)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcca Hierarchical Clustering Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\ud83d\udce5 Data Points N samples] --> B{Clustering Strategy?}\n",
    "    B -->|Bottom-Up| C[\ud83d\udd3c Agglomerative: Start with N clusters]\n",
    "    B -->|Top-Down| D[\ud83d\udd3d Divisive: Start with 1 cluster]\n",
    "    \n",
    "    C --> E[\ud83d\udccf Compute Distance Matrix NxN]\n",
    "    E --> F[\ud83d\udd17 Merge Closest Pair using Linkage]\n",
    "    F --> G{All merged into 1 cluster?}\n",
    "    G -->|No| H[\u267b\ufe0f Update Distance Matrix]\n",
    "    H --> F\n",
    "    G -->|Yes| I[\ud83c\udf33 Dendrogram Tree]\n",
    "    \n",
    "    D --> J[\ud83d\udcca Split Largest Cluster]\n",
    "    J --> K{All N individual clusters?}\n",
    "    K -->|No| J\n",
    "    K -->|Yes| I\n",
    "    \n",
    "    I --> L[\u2702\ufe0f Cut Dendrogram at Height h]\n",
    "    L --> M[\ud83c\udfaf Final K Clusters]\n",
    "    \n",
    "    M --> N[\ud83d\udcc8 Evaluate: Cophenetic correlation]\n",
    "    N --> O[\u2705 Cluster Analysis]\n",
    "    \n",
    "    style C fill:#e1f5e1\n",
    "    style D fill:#ffe1e1\n",
    "    style I fill:#fff4e1\n",
    "    style M fill:#e1f0ff\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd0d Hierarchical vs K-Means vs DBSCAN\n",
    "\n",
    "| **Criterion** | **Hierarchical** | **K-Means** | **DBSCAN** |\n",
    "|--------------|-----------------|------------|-----------|\n",
    "| **Requires K upfront** | \u274c No (cut dendrogram after) | \u2705 Yes | \u274c No (density-based) |\n",
    "| **Cluster shape** | Any (linkage-dependent) | Spherical only | Arbitrary |\n",
    "| **Scalability** | Poor O(n\u00b2 log n) | Excellent O(nkt) | Medium O(n log n) |\n",
    "| **Deterministic** | \u2705 Yes (same distance matrix) | \u274c No (random init) | \u2705 Yes (given eps/min_samples) |\n",
    "| **Dendrogram visualization** | \u2705 Yes (tree structure) | \u274c No | \u274c No |\n",
    "| **Handles outliers** | Poor (forces assignment) | Poor | Excellent (noise=-1) |\n",
    "| **Interpretability** | Excellent (hierarchical taxonomy) | Good (centroids) | Medium (density threshold) |\n",
    "| **Best for** | Small data (<5K), taxonomy discovery | Large data (100K+), well-separated clusters | Geospatial, outlier-heavy data |\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfed Real-World Applications\n",
    "\n",
    "### Post-Silicon Validation\n",
    "- **Test Hierarchy Discovery**: Automatically group 500 parametric tests into functional categories (power, speed, leakage)\n",
    "- **Failure Mode Taxonomy**: Build tree of failure signatures (e.g., voltage failures \u2192 Vdd_high vs Vdd_low \u2192 specific test modes)\n",
    "- **Die Similarity Analysis**: Cluster wafer die into groups by parametric profiles, visualize relationships\n",
    "- **Multi-Site Correlation**: Discover hierarchical relationships between test sites (which sites test similar device characteristics?)\n",
    "\n",
    "### General AI/ML\n",
    "- **Document Taxonomy**: Organize 10K documents into hierarchical categories (no predefined K needed)\n",
    "- **Product Categorization**: Build multi-level product hierarchy from feature similarity\n",
    "- **Gene Expression Clustering**: Discover hierarchical relationships in biological data\n",
    "- **Social Network Communities**: Identify nested community structures in network graphs\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcda Mathematical Foundation\n",
    "\n",
    "### Agglomerative Hierarchical Clustering Algorithm\n",
    "\n",
    "**Input:** Data matrix $X \\in \\mathbb{R}^{n \\times d}$, linkage criterion $L$\n",
    "\n",
    "**Output:** Dendrogram tree structure, mergings at each step\n",
    "\n",
    "**Steps:**\n",
    "1. **Initialize:** Assign each of $n$ points to its own cluster: $C_1, C_2, \\ldots, C_n$\n",
    "2. **Compute Distance Matrix:** Calculate pairwise distances $D_{ij}$ for all cluster pairs\n",
    "3. **Repeat until one cluster remains:**\n",
    "   - Find closest pair of clusters: $(i^*, j^*) = \\arg\\min_{i<j} D_{ij}$\n",
    "   - Merge $C_{i^*}$ and $C_{j^*}$ into new cluster $C_{new}$\n",
    "   - Update distance matrix: compute distances from $C_{new}$ to all other clusters using linkage criterion\n",
    "   - Record merge in dendrogram (height = distance at merge)\n",
    "4. **Output:** Dendrogram tree showing all mergings\n",
    "\n",
    "### Linkage Criteria\n",
    "\n",
    "Linkage determines how to compute distance between two clusters $C_i$ and $C_j$:\n",
    "\n",
    "#### 1. Single Linkage (Minimum)\n",
    "$$\n",
    "d_{\\text{single}}(C_i, C_j) = \\min_{x \\in C_i, y \\in C_j} \\|x - y\\|\n",
    "$$\n",
    "- **Interpretation:** Distance between **closest** points in two clusters\n",
    "- **Behavior:** Tends to form long, elongated \"chain\" clusters\n",
    "- **Use case:** Finding clusters connected by bridges, detecting outliers (single points form separate clusters)\n",
    "\n",
    "#### 2. Complete Linkage (Maximum)\n",
    "$$\n",
    "d_{\\text{complete}}(C_i, C_j) = \\max_{x \\in C_i, y \\in C_j} \\|x - y\\|\n",
    "$$\n",
    "- **Interpretation:** Distance between **farthest** points in two clusters\n",
    "- **Behavior:** Produces compact, spherical clusters (similar to K-Means)\n",
    "- **Use case:** When you want tight, well-separated clusters\n",
    "\n",
    "#### 3. Average Linkage (UPGMA)\n",
    "$$\n",
    "d_{\\text{average}}(C_i, C_j) = \\frac{1}{|C_i| |C_j|} \\sum_{x \\in C_i} \\sum_{y \\in C_j} \\|x - y\\|\n",
    "$$\n",
    "- **Interpretation:** Average distance between **all pairs** of points\n",
    "- **Behavior:** Balanced approach between single and complete linkage\n",
    "- **Use case:** General-purpose linkage, robust to noise\n",
    "\n",
    "#### 4. Ward's Method (Minimum Variance)\n",
    "$$\n",
    "d_{\\text{ward}}(C_i, C_j) = \\frac{|C_i| |C_j|}{|C_i| + |C_j|} \\|\\mu_i - \\mu_j\\|^2\n",
    "$$\n",
    "where $\\mu_i, \\mu_j$ are cluster centroids.\n",
    "\n",
    "- **Interpretation:** Increase in within-cluster variance after merging\n",
    "- **Behavior:** Minimizes inertia (sum of squared distances to centroid), produces balanced cluster sizes\n",
    "- **Use case:** When you want K-Means-like compact clusters but without specifying K upfront\n",
    "- **Note:** Most popular in practice, often best default choice\n",
    "\n",
    "### Lance-Williams Update Formula\n",
    "\n",
    "Efficient distance matrix update without recomputing all pairwise distances:\n",
    "\n",
    "$$\n",
    "d(C_{new}, C_k) = \\alpha_i d(C_i, C_k) + \\alpha_j d(C_j, C_k) + \\beta d(C_i, C_j) + \\gamma |d(C_i, C_k) - d(C_j, C_k)|\n",
    "$$\n",
    "\n",
    "where $C_{new} = C_i \\cup C_j$. Parameters $\\alpha_i, \\alpha_j, \\beta, \\gamma$ depend on linkage method:\n",
    "\n",
    "| **Linkage** | $\\alpha_i$ | $\\alpha_j$ | $\\beta$ | $\\gamma$ |\n",
    "|------------|-----------|-----------|---------|---------|\n",
    "| Single | 0.5 | 0.5 | 0 | -0.5 |\n",
    "| Complete | 0.5 | 0.5 | 0 | 0.5 |\n",
    "| Average | $\\frac{|C_i|}{|C_i|+|C_j|}$ | $\\frac{|C_j|}{|C_i|+|C_j|}$ | 0 | 0 |\n",
    "| Ward | $\\frac{|C_i|+|C_k|}{|C_i|+|C_j|+|C_k|}$ | $\\frac{|C_j|+|C_k|}{|C_i|+|C_j|+|C_k|}$ | $\\frac{-|C_k|}{|C_i|+|C_j|+|C_k|}$ | 0 |\n",
    "\n",
    "### Dendrogram Interpretation\n",
    "\n",
    "**Y-axis (Height):** Distance at which clusters merge (linkage-dependent)\n",
    "- **Low height:** Points/clusters are very similar (merge early)\n",
    "- **High height:** Points/clusters are dissimilar (merge late)\n",
    "\n",
    "**Cutting the Dendrogram:**\n",
    "- Horizontal cut at height $h$ produces clusters\n",
    "- Number of clusters = number of vertical lines intersected\n",
    "- **Optimal cut:** Look for large \"gap\" in merge heights (long vertical segments) \u2192 natural cluster boundary\n",
    "\n",
    "**Cophenetic Distance:**\n",
    "- Distance at which two points are first merged in same cluster\n",
    "- **Cophenetic Correlation:** Correlation between original distances and cophenetic distances\n",
    "- High correlation (>0.8) means dendrogram faithfully represents data structure\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1958a2",
   "metadata": {},
   "source": [
    "## \ud83d\udce6 Required Libraries\n",
    "\n",
    "### \ud83d\udcdd What's Happening: Import Dependencies\n",
    "\n",
    "**Purpose:** Load libraries for hierarchical clustering, dendrogram visualization, and distance computations.\n",
    "\n",
    "**Key Points:**\n",
    "- **scipy.cluster.hierarchy**: Core library for hierarchical clustering (linkage, dendrogram, fcluster, cophenetic)\n",
    "- **scipy.spatial.distance**: Distance metrics (euclidean, cityblock, cosine) and pdist for pairwise distances\n",
    "- **sklearn.cluster.AgglomerativeClustering**: Scikit-learn wrapper with fit_predict API consistency\n",
    "- **matplotlib/seaborn**: Dendrogram visualization with customizable aesthetics\n",
    "- **NumPy**: Distance matrix computations and array operations\n",
    "\n",
    "**Why This Matters:** scipy.cluster.hierarchy is the gold standard for hierarchical clustering in Python, offering optimized C implementations of linkage algorithms and rich dendrogram visualization. For post-silicon applications, visualizing test hierarchies or failure taxonomies requires clear dendrogram plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e930795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, cophenet\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\u2705 Libraries imported successfully!\")\n",
    "print(\"\\nKey Modules:\")\n",
    "print(\"  \u2022 scipy.cluster.hierarchy: linkage(), dendrogram(), fcluster()\")\n",
    "print(\"  \u2022 scipy.spatial.distance: pdist(), squareform()\")\n",
    "print(\"  \u2022 sklearn.cluster.AgglomerativeClustering: Production API\")\n",
    "print(\"  \u2022 Linkage methods: single, complete, average, ward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca704bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83d\udd28 Implementation From Scratch: Agglomerative Clustering\n",
    "\n",
    "### \ud83d\udcdd What's Happening: Building Agglomerative Clustering Algorithm\n",
    "\n",
    "**Purpose:** Implement agglomerative hierarchical clustering from scratch to understand merge logic and distance matrix updates.\n",
    "\n",
    "**Key Points:**\n",
    "- **Distance Matrix**: Compute all pairwise distances once, then update after each merge\n",
    "- **Merge Selection**: Find closest cluster pair using np.argmin on flattened distance matrix\n",
    "- **Lance-Williams Formula**: Efficiently update distances without recomputing all pairs\n",
    "- **Dendrogram Tracking**: Record merge history (cluster1, cluster2, distance, size) for visualization\n",
    "- **Linkage Method**: Implement average linkage (can extend to single/complete/ward)\n",
    "\n",
    "**Why This Matters:** Understanding merge mechanics reveals why hierarchical clustering is O(n\u00b3) naive but O(n\u00b2 log n) with priority queues. In post-silicon validation, knowing algorithm internals helps explain why 10K test results take minutes to cluster vs seconds for K-Means.\n",
    "\n",
    "**Post-Silicon Context:** For 500 parametric tests, hierarchical clustering discovers natural test groupings (power tests, speed tests, leakage tests) without predefined categories. From-scratch implementation clarifies why this approach works better than K-Means when test relationships are hierarchical (e.g., Vdd tests \u2192 Vdd_nominal \u2192 Vdd_low_power)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgglomerativeClusteringFromScratch:\n",
    "    \"\"\"\n",
    "    Agglomerative hierarchical clustering implementation from scratch.\n",
    "    \n",
    "    Uses average linkage and Lance-Williams formula for distance updates.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, linkage='average'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        linkage : str\n",
    "            Linkage criterion ('single', 'complete', 'average')\n",
    "        \"\"\"\n",
    "        self.linkage = linkage\n",
    "        self.merge_history_ = []\n",
    "        self.labels_ = None\n",
    "        self.n_clusters_ = None\n",
    "    \n",
    "    def fit(self, X, n_clusters=3):\n",
    "        \"\"\"\n",
    "        Perform agglomerative clustering.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        n_clusters : int\n",
    "            Number of final clusters (where to cut dendrogram)\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Initialize: each point is its own cluster\n",
    "        clusters = {i: [i] for i in range(n_samples)}  # cluster_id -> list of point indices\n",
    "        cluster_ids = list(range(n_samples))\n",
    "        \n",
    "        # Compute initial distance matrix\n",
    "        dist_matrix = squareform(pdist(X, metric='euclidean'))\n",
    "        \n",
    "        # Make diagonal infinite (can't merge cluster with itself)\n",
    "        np.fill_diagonal(dist_matrix, np.inf)\n",
    "        \n",
    "        # Track merge history for dendrogram\n",
    "        self.merge_history_ = []\n",
    "        next_cluster_id = n_samples  # New clusters get IDs starting from n_samples\n",
    "        \n",
    "        # Merge until we have desired number of clusters\n",
    "        while len(clusters) > n_clusters:\n",
    "            # Find closest pair of clusters\n",
    "            min_idx = np.argmin(dist_matrix)\n",
    "            i, j = np.unravel_index(min_idx, dist_matrix.shape)\n",
    "            \n",
    "            # Ensure i < j for consistency\n",
    "            if i > j:\n",
    "                i, j = j, i\n",
    "            \n",
    "            # Record merge (cluster_i, cluster_j, distance, new_cluster_size)\n",
    "            merge_distance = dist_matrix[i, j]\n",
    "            new_cluster_size = len(clusters[cluster_ids[i]]) + len(clusters[cluster_ids[j]])\n",
    "            self.merge_history_.append((cluster_ids[i], cluster_ids[j], merge_distance, new_cluster_size))\n",
    "            \n",
    "            # Merge clusters i and j\n",
    "            new_cluster = clusters[cluster_ids[i]] + clusters[cluster_ids[j]]\n",
    "            clusters[next_cluster_id] = new_cluster\n",
    "            \n",
    "            # Update distance matrix using Lance-Williams formula (average linkage)\n",
    "            new_distances = self._update_distances(dist_matrix, i, j, \n",
    "                                                   len(clusters[cluster_ids[i]]), \n",
    "                                                   len(clusters[cluster_ids[j]]))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 2\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # Remove rows/cols for merged clusters, add row/col for new cluster\n",
    "            # Delete rows i and j (delete j first since j > i)\n",
    "            dist_matrix = np.delete(dist_matrix, [i, j], axis=0)\n",
    "            dist_matrix = np.delete(dist_matrix, [i, j], axis=1)\n",
    "            \n",
    "            # Add new row and column for merged cluster\n",
    "            dist_matrix = np.vstack([dist_matrix, new_distances])\n",
    "            new_distances_col = np.append(new_distances, np.inf)  # Distance to itself = inf\n",
    "            dist_matrix = np.column_stack([dist_matrix, new_distances_col])\n",
    "            \n",
    "            # Update cluster_ids list\n",
    "            del clusters[cluster_ids[i]]\n",
    "            del clusters[cluster_ids[j]]\n",
    "            cluster_ids.pop(j)  # Remove j first (higher index)\n",
    "            cluster_ids.pop(i)\n",
    "            cluster_ids.append(next_cluster_id)\n",
    "            \n",
    "            next_cluster_id += 1\n",
    "        \n",
    "        # Assign final labels\n",
    "        self.labels_ = np.zeros(n_samples, dtype=int)\n",
    "        for cluster_label, cluster_id in enumerate(cluster_ids):\n",
    "            for point_idx in clusters[cluster_id]:\n",
    "                self.labels_[point_idx] = cluster_label\n",
    "        \n",
    "        self.n_clusters_ = n_clusters\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _update_distances(self, dist_matrix, i, j, size_i, size_j):\n",
    "        \"\"\"\n",
    "        Update distances using Lance-Williams formula (average linkage).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dist_matrix : ndarray\n",
    "            Current distance matrix\n",
    "        i, j : int\n",
    "            Indices of clusters being merged\n",
    "        size_i, size_j : int\n",
    "            Sizes of clusters i and j\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        new_distances : ndarray\n",
    "            Distances from new merged cluster to all other clusters\n",
    "        \"\"\"\n",
    "        n = dist_matrix.shape[0]\n",
    "        new_distances = np.zeros(n - 2)  # Exclude i and j\n",
    "        \n",
    "        # Lance-Williams parameters for average linkage\n",
    "        alpha_i = size_i / (size_i + size_j)\n",
    "        alpha_j = size_j / (size_i + size_j)\n",
    "        beta = 0\n",
    "        gamma = 0\n",
    "        \n",
    "        # Compute distance from new cluster to each remaining cluster k\n",
    "        k_idx = 0\n",
    "        for k in range(n):\n",
    "            if k == i or k == j:\n",
    "                continue\n",
    "            \n",
    "            # Average linkage formula\n",
    "            if self.linkage == 'average':\n",
    "                new_distances[k_idx] = alpha_i * dist_matrix[i, k] + alpha_j * dist_matrix[j, k]\n",
    "            elif self.linkage == 'single':\n",
    "                new_distances[k_idx] = min(dist_matrix[i, k], dist_matrix[j, k])\n",
    "            elif self.linkage == 'complete':\n",
    "                new_distances[k_idx] = max(dist_matrix[i, k], dist_matrix[j, k])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation Part 3\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            k_idx += 1\n",
    "        \n",
    "        return new_distances\n",
    "print(\"\u2705 Agglomerative Clustering implemented from scratch!\")\n",
    "print(\"\\nKey Methods:\")\n",
    "print(\"  \u2022 fit(X, n_clusters) - Perform clustering, merge until n_clusters remain\")\n",
    "print(\"  \u2022 _update_distances() - Lance-Williams formula for distance matrix update\")\n",
    "print(\"\\nAlgorithm Flow:\")\n",
    "print(\"  1. Initialize: Each point = separate cluster\")\n",
    "print(\"  2. Compute distance matrix (all pairwise distances)\")\n",
    "print(\"  3. Repeat: Find closest pair, merge, update distances\")\n",
    "print(\"  4. Stop when desired number of clusters reached\")\n",
    "print(\"\\nComplexity:\")\n",
    "print(\"  \u2022 Naive: O(n\u00b3) - recompute all distances after each merge\")\n",
    "print(\"  \u2022 Lance-Williams: O(n\u00b2 log n) - efficient distance updates\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dfd80b",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening: Testing From-Scratch on Synthetic Data\n",
    "\n",
    "**Purpose:** Validate from-scratch hierarchical clustering on known-structure data and visualize merge process.\n",
    "\n",
    "**Key Points:**\n",
    "- **Synthetic Data**: Generate 3 well-separated blobs (100 points) to verify algorithm correctness\n",
    "- **Ground Truth Comparison**: Use Adjusted Rand Index to measure clustering quality vs true labels\n",
    "- **Merge Tracking**: Print merge history showing which clusters combine at each step\n",
    "- **Visualization**: Scatter plot with color-coded final clusters\n",
    "- **Post-Silicon Context**: Similar to discovering 3 natural test groups (power, speed, I/O) from 100 parametric tests\n",
    "\n",
    "**Why This Matters:** Testing on synthetic data (known structure) validates implementation before applying to real unlabeled data. For semiconductor test hierarchy, verifying algorithm on simple cases ensures it will correctly group related tests when applied to 500+ real parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e77a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with 3 clear clusters\n",
    "X_blobs, y_true = make_blobs(n_samples=100, centers=3, n_features=2, \n",
    "                              cluster_std=0.5, random_state=42)\n",
    "\n",
    "print(\"\ud83d\udcca Synthetic Data Generated:\")\n",
    "print(f\"  \u2022 Shape: {X_blobs.shape}\")\n",
    "print(f\"  \u2022 True clusters: {np.unique(y_true)}\")\n",
    "print(f\"  \u2022 Feature ranges: [{X_blobs.min():.2f}, {X_blobs.max():.2f}]\")\n",
    "\n",
    "# Train from-scratch hierarchical clustering\n",
    "hc_scratch = AgglomerativeClusteringFromScratch(linkage='average')\n",
    "hc_scratch.fit(X_blobs, n_clusters=3)\n",
    "\n",
    "print(f\"\\n\u2705 Hierarchical Clustering Complete!\")\n",
    "print(f\"  \u2022 Final clusters: {hc_scratch.n_clusters_}\")\n",
    "print(f\"  \u2022 Total merges performed: {len(hc_scratch.merge_history_)}\")\n",
    "print(f\"  \u2022 Label distribution: {np.bincount(hc_scratch.labels_)}\")\n",
    "\n",
    "# Show last 5 merges (most important)\n",
    "print(f\"\\n\ud83d\udd0d Last 5 Merge Steps:\")\n",
    "print(f\"{'Step':<6} {'Cluster1':<10} {'Cluster2':<10} {'Distance':<12} {'New Size'}\")\n",
    "print(\"-\" * 60)\n",
    "for step, (c1, c2, dist, size) in enumerate(hc_scratch.merge_history_[-5:], \n",
    "                                             start=len(hc_scratch.merge_history_)-4):\n",
    "    print(f\"{step:<6} {c1:<10} {c2:<10} {dist:<12.3f} {size}\")\n",
    "\n",
    "# Evaluate clustering quality\n",
    "ari = adjusted_rand_score(y_true, hc_scratch.labels_)\n",
    "silhouette = silhouette_score(X_blobs, hc_scratch.labels_)\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Clustering Quality:\")\n",
    "print(f\"  \u2022 Adjusted Rand Index: {ari:.4f} (1.0 = perfect match with true labels)\")\n",
    "print(f\"  \u2022 Silhouette Score: {silhouette:.4f} (higher = better separation)\")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Ground truth\n",
    "axes[0].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_true, cmap='viridis', \n",
    "                alpha=0.6, edgecolors='k', s=60)\n",
    "axes[0].set_title(\"Ground Truth Clusters\", fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(\"Feature 1\")\n",
    "axes[0].set_ylabel(\"Feature 2\")\n",
    "\n",
    "# Predicted clusters\n",
    "axes[1].scatter(X_blobs[:, 0], X_blobs[:, 1], c=hc_scratch.labels_, cmap='viridis',\n",
    "                alpha=0.6, edgecolors='k', s=60)\n",
    "axes[1].set_title(f\"Hierarchical Clustering (ARI={ari:.3f})\", fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Feature 1\")\n",
    "axes[1].set_ylabel(\"Feature 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udd0d Interpretation:\")\n",
    "print(\"  \u2022 High ARI (~1.0): Algorithm correctly recovers true cluster structure\")\n",
    "print(\"  \u2022 Average linkage produces balanced, well-separated clusters\")\n",
    "print(\"  \u2022 Merge distances increase gradually, then sharply at final merges (natural cluster boundary)\")\n",
    "print(\"\\n\ud83d\udca1 Post-Silicon Analogy:\")\n",
    "print(\"  \u2022 100 points = 100 parametric tests\")\n",
    "print(\"  \u2022 3 clusters = 3 test categories (power, speed, I/O)\")\n",
    "print(\"  \u2022 Merge history reveals hierarchical structure (e.g., Vdd tests merge before joining speed tests)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d90978",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udf33 Dendrogram Visualization with scipy\n",
    "\n",
    "### \ud83d\udcdd What's Happening: Building and Interpreting Dendrograms\n",
    "\n",
    "**Purpose:** Use scipy.cluster.hierarchy to create dendrograms showing complete merge hierarchy and identify optimal cut height.\n",
    "\n",
    "**Key Points:**\n",
    "- **scipy.hierarchy.linkage()**: Computes linkage matrix (merge sequence) from data or distance matrix\n",
    "- **Dendrogram Interpretation**: Y-axis = merge distance, horizontal lines = clusters, vertical lines = individual points/sub-clusters\n",
    "- **Optimal Cut Selection**: Look for large vertical gaps (long segments) indicating natural cluster boundaries\n",
    "- **Linkage Comparison**: Visualize single vs complete vs average vs ward to see behavior differences\n",
    "- **Cophenetic Correlation**: Measure how well dendrogram preserves original distances (>0.8 good)\n",
    "\n",
    "**Why This Matters:** Dendrograms answer \"How many clusters?\" without trying K=1,2,3,... In semiconductor test hierarchy, dendrogram reveals natural test groupings at multiple granularity levels (e.g., 3 top-level categories split into 10 subcategories).\n",
    "\n",
    "**Post-Silicon Context:** For 500 parametric tests, dendrogram shows hierarchical relationships: top level separates power/speed/I/O, second level splits power into {Vdd, Idd, leakage}, third level splits Vdd into {nominal, low_power, stress}. Engineers can cut at any level based on analysis needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a535f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute linkage matrix using scipy (ward method)\n",
    "linkage_matrix_ward = linkage(X_blobs, method='ward')\n",
    "\n",
    "print(\"\ud83d\udcca Linkage Matrix (Ward's Method):\")\n",
    "print(\"  \u2022 Shape:\", linkage_matrix_ward.shape, \"(n_samples-1) x 4\")\n",
    "print(\"  \u2022 Columns: [cluster1_id, cluster2_id, merge_distance, new_cluster_size]\")\n",
    "print(\"\\nLast 5 Merges (final steps before reaching 1 cluster):\")\n",
    "print(linkage_matrix_ward[-5:])\n",
    "\n",
    "# Compute cophenetic correlation\n",
    "coph_corr, coph_dist = cophenet(linkage_matrix_ward, pdist(X_blobs))\n",
    "print(f\"\\n\ud83d\udccf Cophenetic Correlation: {coph_corr:.4f}\")\n",
    "print(\"  \u2022 >0.8: Excellent (dendrogram faithfully represents data structure)\")\n",
    "print(\"  \u2022 0.6-0.8: Good\")\n",
    "print(\"  \u2022 <0.6: Poor (dendrogram distorts relationships)\")\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(linkage_matrix_ward, \n",
    "           truncate_mode='lastp',  # Show only last p merges\n",
    "           p=30,  # Show last 30 merges\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=10)\n",
    "plt.title(\"Dendrogram (Ward's Method) - Last 30 Merges\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Sample Index or Cluster Size\", fontsize=12)\n",
    "plt.ylabel(\"Merge Distance (Ward Criterion)\", fontsize=12)\n",
    "plt.axhline(y=10, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Cut at h=10 \u2192 3 clusters')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udd0d Dendrogram Interpretation:\")\n",
    "print(\"  \u2022 Vertical axis: Distance at which clusters merge\")\n",
    "print(\"  \u2022 Horizontal axis: Individual samples or merged clusters\")\n",
    "print(\"  \u2022 Long vertical lines: Large distance gaps \u2192 natural cluster boundaries\")\n",
    "print(\"  \u2022 Red dashed line: Proposed cut height (h=10) produces 3 clusters\")\n",
    "print(\"\\n\ud83d\udca1 How to Choose Cut Height:\")\n",
    "print(\"  1. Look for large gaps in Y-axis (long vertical segments)\")\n",
    "print(\"  2. Count number of vertical lines crossed by horizontal cut\")\n",
    "print(\"  3. That count = number of clusters produced\")\n",
    "print(\"  4. For h=10: Crosses 3 vertical lines \u2192 3 clusters\")\n",
    "\n",
    "# Extract clusters from dendrogram cut\n",
    "cut_height = 10\n",
    "cluster_labels_cut = fcluster(linkage_matrix_ward, t=cut_height, criterion='distance')\n",
    "\n",
    "print(f\"\\n\u2702\ufe0f Cutting Dendrogram at h={cut_height}:\")\n",
    "print(f\"  \u2022 Number of clusters: {len(np.unique(cluster_labels_cut))}\")\n",
    "print(f\"  \u2022 Cluster sizes: {np.bincount(cluster_labels_cut)}\")\n",
    "\n",
    "# Validate cut against ground truth\n",
    "ari_cut = adjusted_rand_score(y_true, cluster_labels_cut)\n",
    "print(f\"  \u2022 ARI vs ground truth: {ari_cut:.4f} (validates 3-cluster choice)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad160c",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening: Comparing Linkage Methods\n",
    "\n",
    "**Purpose:** Visualize how different linkage criteria (single, complete, average, ward) produce different dendrograms and cluster shapes.\n",
    "\n",
    "**Key Points:**\n",
    "- **Single Linkage**: Tends to create \"chain\" clusters (connects via closest points) \u2192 elongated shapes\n",
    "- **Complete Linkage**: Produces compact, spherical clusters (connects via farthest points) \u2192 tight groups\n",
    "- **Average Linkage**: Balanced approach, robust to noise\n",
    "- **Ward's Method**: Minimizes variance (like K-Means) \u2192 balanced cluster sizes, compact shapes\n",
    "- **Visual Comparison**: 4 dendrograms side-by-side show merge height differences\n",
    "\n",
    "**Why This Matters:** Linkage choice dramatically affects results. For post-silicon test hierarchy, ward/average work best for balanced test groups; single linkage useful for outlier detection (isolated tests form long chains).\n",
    "\n",
    "**Post-Silicon Context:** When grouping 500 parametric tests:\n",
    "- **Ward**: Balanced test categories (each ~50-100 tests)\n",
    "- **Single**: Detects outlier tests (e.g., specialized debug tests that don't fit main categories)\n",
    "- **Complete**: Ensures tight test groups (all tests in group highly correlated)\n",
    "- **Average**: Robust choice when test relationships have noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57654462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute linkage matrices for all 4 methods\n",
    "linkage_methods = ['single', 'complete', 'average', 'ward']\n",
    "linkage_matrices = {}\n",
    "cophenetic_corrs = {}\n",
    "\n",
    "for method in linkage_methods:\n",
    "    linkage_matrices[method] = linkage(X_blobs, method=method)\n",
    "    coph_corr, _ = cophenet(linkage_matrices[method], pdist(X_blobs))\n",
    "    cophenetic_corrs[method] = coph_corr\n",
    "\n",
    "print(\"\ud83d\udcca Linkage Method Comparison:\")\n",
    "print(f\"{'Method':<15} {'Cophenetic Corr':<20} {'Interpretation'}\")\n",
    "print(\"-\" * 70)\n",
    "for method in linkage_methods:\n",
    "    corr = cophenetic_corrs[method]\n",
    "    interpretation = \"Excellent\" if corr > 0.8 else \"Good\" if corr > 0.6 else \"Fair\"\n",
    "    print(f\"{method.capitalize():<15} {corr:<20.4f} {interpretation}\")\n",
    "\n",
    "# Visualize all 4 dendrograms\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, method in enumerate(linkage_methods):\n",
    "    plt.sca(axes[idx])\n",
    "    dendrogram(linkage_matrices[method], \n",
    "               truncate_mode='lastp',\n",
    "               p=25,\n",
    "               leaf_rotation=90,\n",
    "               leaf_font_size=8,\n",
    "               ax=axes[idx])\n",
    "    axes[idx].set_title(f\"{method.capitalize()} Linkage (Cophenetic={cophenetic_corrs[method]:.3f})\", \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(\"Sample Index or Cluster Size\")\n",
    "    axes[idx].set_ylabel(\"Merge Distance\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udd0d Linkage Method Behaviors:\")\n",
    "print(\"  \u2022 Single: Lowest merge heights early (connects closest points) \u2192 chain-like clusters\")\n",
    "print(\"  \u2022 Complete: Highest merge heights (connects farthest points) \u2192 compact, tight clusters\")\n",
    "print(\"  \u2022 Average: Moderate merge heights \u2192 balanced, robust clustering\")\n",
    "print(\"  \u2022 Ward: Stepwise increases (minimizes variance) \u2192 K-Means-like balanced clusters\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Post-Silicon Linkage Selection:\")\n",
    "print(\"  \u2022 Ward/Average: General-purpose test hierarchy (balanced categories)\")\n",
    "print(\"  \u2022 Single: Outlier test detection (isolates specialized/debug tests)\")\n",
    "print(\"  \u2022 Complete: Tight test groups (all tests highly correlated)\")\n",
    "\n",
    "# Cut all dendrograms at same height and compare cluster assignments\n",
    "cut_height = 8\n",
    "cluster_comparison = {}\n",
    "\n",
    "for method in linkage_methods:\n",
    "    labels = fcluster(linkage_matrices[method], t=cut_height, criterion='distance')\n",
    "    cluster_comparison[method] = labels\n",
    "    ari = adjusted_rand_score(y_true, labels)\n",
    "    n_clusters = len(np.unique(labels))\n",
    "    print(f\"\\n{method.capitalize()} at h={cut_height}:\")\n",
    "    print(f\"  \u2022 Clusters: {n_clusters}, ARI: {ari:.4f}, Sizes: {np.bincount(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746aeca7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udfed Production Implementation: sklearn AgglomerativeClustering\n",
    "\n",
    "### \ud83d\udcdd What's Happening: sklearn.cluster.AgglomerativeClustering\n",
    "\n",
    "**Purpose:** Use production-grade sklearn API with connectivity constraints and memory-efficient implementations.\n",
    "\n",
    "**Key Points:**\n",
    "- **sklearn API**: Consistent `.fit_predict()` interface like K-Means\n",
    "- **Linkage Options**: ward, complete, average, single (same as scipy)\n",
    "- **Connectivity Constraints**: Force certain points to cluster together (spatial constraints for wafer maps)\n",
    "- **Distance Threshold**: Automatically determine number of clusters based on max distance\n",
    "- **Memory Efficiency**: Handles 10K+ points better than scipy for large datasets\n",
    "\n",
    "**Why This Matters:** sklearn provides production-ready hierarchical clustering with advanced features like connectivity constraints (useful for spatial data) and distance_threshold mode (auto K selection). For semiconductor applications, connectivity constraints ensure spatially adjacent die cluster together even if parameters differ slightly.\n",
    "\n",
    "**Post-Silicon Context:** When clustering wafer die, connectivity constraints ensure die from same wafer region stay together, reflecting spatial correlation from process tools. Distance threshold mode automatically determines natural test groupings without manual K selection."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# sklearn AgglomerativeClustering with fixed n_clusters\n",
    "hc_sklearn = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "labels_sklearn = hc_sklearn.fit_predict(X_blobs)\n",
    "\n",
    "print(\"\u2705 sklearn Hierarchical Clustering Complete!\")\n",
    "print(f\"  \u2022 Number of clusters: {hc_sklearn.n_clusters_}\")\n",
    "print(f\"  \u2022 Number of leaves: {hc_sklearn.n_leaves_}\")\n",
    "print(f\"  \u2022 Number of connected components: {hc_sklearn.n_connected_components_}\")\n",
    "print(f\"  \u2022 Cluster sizes: {np.bincount(labels_sklearn)}\")\n",
    "\n",
    "# Evaluate sklearn clustering\n",
    "ari_sklearn = adjusted_rand_score(y_true, labels_sklearn)\n",
    "silhouette_sklearn = silhouette_score(X_blobs, labels_sklearn)\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 sklearn Clustering Quality:\")\n",
    "print(f\"  \u2022 Adjusted Rand Index: {ari_sklearn:.4f}\")\n",
    "print(f\"  \u2022 Silhouette Score: {silhouette_sklearn:.4f}\")\n",
    "\n",
    "# Distance threshold mode: automatically determine number of clusters\n",
    "hc_auto = AgglomerativeClustering(n_clusters=None, distance_threshold=8.0, linkage='ward')\n",
    "labels_auto = hc_auto.fit_predict(X_blobs)\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Auto-Clustering (distance_threshold=8.0):\")\n",
    "print(f\"  \u2022 Automatically determined clusters: {hc_auto.n_clusters_}\")\n",
    "print(f\"  \u2022 Cluster sizes: {np.bincount(labels_auto)}\")\n",
    "print(f\"  \u2022 ARI vs ground truth: {adjusted_rand_score(y_true, labels_auto):.4f}\")\n",
    "\n",
    "# Visualize sklearn results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Ground truth\n",
    "axes[0].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_true, cmap='viridis',\n",
    "                alpha=0.6, edgecolors='k', s=60)\n",
    "axes[0].set_title(\"Ground Truth\", fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(\"Feature 1\")\n",
    "axes[0].set_ylabel(\"Feature 2\")\n",
    "\n",
    "# sklearn fixed K=3\n",
    "axes[1].scatter(X_blobs[:, 0], X_blobs[:, 1], c=labels_sklearn, cmap='viridis',\n",
    "                alpha=0.6, edgecolors='k', s=60)\n",
    "axes[1].set_title(f\"sklearn (K=3, ARI={ari_sklearn:.3f})\", fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Feature 1\")\n",
    "axes[1].set_ylabel(\"Feature 2\")\n",
    "\n",
    "# Auto-determined clusters\n",
    "axes[2].scatter(X_blobs[:, 0], X_blobs[:, 1], c=labels_auto, cmap='viridis',\n",
    "                alpha=0.6, edgecolors='k', s=60)\n",
    "axes[2].set_title(f\"Auto (K={hc_auto.n_clusters_}, threshold=8.0)\", fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel(\"Feature 1\")\n",
    "axes[2].set_ylabel(\"Feature 2\")\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Code Continuation (2/2)\n",
    "\n",
    "Continuing implementation...\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2705 sklearn vs scipy Comparison:\")\n",
    "print(\"  \u2022 Both produce identical results (same underlying algorithm)\")\n",
    "print(\"  \u2022 sklearn: Better API, connectivity constraints, auto K via distance_threshold\")\n",
    "print(\"  \u2022 scipy: Better dendrogram visualization, more linkage options\")\n",
    "print(\"\\n\ud83d\udca1 Production Recommendation:\")\n",
    "print(\"  \u2022 Use scipy for exploratory analysis (dendrograms, linkage comparison)\")\n",
    "print(\"  \u2022 Use sklearn for production pipelines (consistent API, scalability)\")\n",
    "\n",
    "# Demonstrate connectivity constraints (spatial adjacency)\n",
    "print(\"\\n\ud83d\udd17 Connectivity Constraints Example:\")\n",
    "print(\"  \u2022 Force spatially adjacent points to cluster together\")\n",
    "print(\"  \u2022 Useful for wafer maps (die on same wafer quadrant should cluster)\")\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "# Create connectivity matrix (each point connected to 5 nearest neighbors)\n",
    "connectivity = kneighbors_graph(X_blobs, n_neighbors=5, include_self=False)\n",
    "hc_constrained = AgglomerativeClustering(n_clusters=3, linkage='ward', connectivity=connectivity)\n",
    "labels_constrained = hc_constrained.fit_predict(X_blobs)\n",
    "\n",
    "ari_constrained = adjusted_rand_score(y_true, labels_constrained)\n",
    "print(f\"  \u2022 Constrained clustering ARI: {ari_constrained:.4f}\")\n",
    "print(f\"  \u2022 Use case: Ensure spatially adjacent wafer die cluster together\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dd4f5190",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udfed Real-World Application: Parametric Test Hierarchy Discovery\n",
    "\n",
    "### Post-Silicon Validation Use Case\n",
    "\n",
    "**Business Problem:** Semiconductor test programs contain 200-500 parametric tests measuring voltage, current, frequency, power, timing, etc. Engineers need to:\n",
    "1. Organize tests into logical hierarchies (power\u2192Vdd\u2192nominal/low_power)\n",
    "2. Identify redundant tests (high correlation) for test time optimization\n",
    "3. Build failure mode taxonomies from test signatures\n",
    "4. Understand multi-level test relationships without manual categorization\n",
    "\n",
    "**Hierarchical Clustering Solution:** Cluster test results across 10K+ devices to discover natural test groupings and hierarchical relationships automatically.\n",
    "\n",
    "### \ud83d\udcdd What's Happening: Test Hierarchy Discovery\n",
    "\n",
    "**Purpose:** Apply hierarchical clustering to realistic semiconductor parametric test data (50 tests \u00d7 1000 devices) to discover test categories.\n",
    "\n",
    "**Key Points:**\n",
    "- **Feature Matrix**: 50 parametric tests (Vdd, Idd, freq, leakage, timing) measured on 1000 devices\n",
    "- **Hierarchical Structure**: Discover 3 top-level categories (power, speed, leakage) and 10 subcategories\n",
    "- **Dendrogram Interpretation**: Visualize test relationships, identify highly correlated test pairs (candidates for elimination)\n",
    "- **Business Value**: Removing 15-20% redundant tests saves 10-15 seconds per device \u00d7 1M devices/month = $500K-1M/year\n",
    "- **Failure Taxonomy**: Group tests by failure modes to accelerate root cause analysis\n",
    "\n",
    "**Why This Matters:** Manual test categorization takes weeks and is subjective; hierarchical clustering provides objective, data-driven taxonomy in minutes. For 500+ test parameters, automated hierarchy discovery is essential for maintaining test program organization as product evolves."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate realistic parametric test data\n",
    "np.random.seed(42)\n",
    "n_devices = 1000\n",
    "n_tests = 50\n",
    "\n",
    "# Simulate 3 test categories with hierarchical structure\n",
    "# Category 1: Power tests (20 tests) - high correlation within group\n",
    "power_tests = np.random.multivariate_normal(\n",
    "    mean=np.zeros(20),\n",
    "    cov=np.eye(20) * 0.3 + 0.7,  # High correlation (0.7)\n",
    "    size=n_devices\n",
    ")\n",
    "\n",
    "# Category 2: Speed tests (15 tests) - moderate correlation\n",
    "speed_tests = np.random.multivariate_normal(\n",
    "    mean=np.ones(15),\n",
    "    cov=np.eye(15) * 0.5 + 0.5,  # Moderate correlation (0.5)\n",
    "    size=n_devices\n",
    ")\n",
    "\n",
    "# Category 3: Leakage tests (15 tests) - lower correlation\n",
    "leakage_tests = np.random.multivariate_normal(\n",
    "    mean=np.ones(15) * 2,\n",
    "    cov=np.eye(15) * 0.7 + 0.3,  # Lower correlation (0.3)\n",
    "    size=n_devices\n",
    ")\n",
    "\n",
    "# Combine into full test matrix (transpose to get test x device)\n",
    "X_tests = np.column_stack([power_tests, speed_tests, leakage_tests])\n",
    "X_tests_normalized = (X_tests - X_tests.mean(axis=0)) / X_tests.std(axis=0)\n",
    "\n",
    "# Cluster tests (not devices) - compute test similarity\n",
    "# Use test vectors as data points (each test = 1000-dimensional vector of device results)\n",
    "test_vectors = X_tests_normalized.T  # Shape: (50 tests, 1000 devices)\n",
    "\n",
    "print(\"\ud83d\udcca Parametric Test Data Generated:\")\n",
    "print(f\"  \u2022 Number of devices: {n_devices}\")\n",
    "print(f\"  \u2022 Number of tests: {n_tests}\")\n",
    "print(f\"  \u2022 Test categories: Power (20), Speed (15), Leakage (15)\")\n",
    "print(f\"  \u2022 Test vector shape: {test_vectors.shape}\")\n",
    "\n",
    "# Compute test similarity using correlation distance\n",
    "from scipy.spatial.distance import pdist\n",
    "test_distances = pdist(test_vectors, metric='correlation')  # 1 - correlation\n",
    "\n",
    "# Hierarchical clustering of tests\n",
    "linkage_matrix_tests = linkage(test_distances, method='ward')\n",
    "\n",
    "# Compute cophenetic correlation\n",
    "coph_corr_tests, _ = cophenet(linkage_matrix_tests, test_distances)\n",
    "print(f\"\\n\ud83d\udccf Test Hierarchy Cophenetic Correlation: {coph_corr_tests:.4f}\")\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(16, 8))\n",
    "dendrogram(linkage_matrix_tests,\n",
    "           labels=[f\"T{i+1:02d}\" for i in range(n_tests)],  # Test labels T01-T50\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=10,\n",
    "           color_threshold=15)  # Color different at height 15\n",
    "plt.title(\"Test Hierarchy Dendrogram (Ward's Linkage, Correlation Distance)\", \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Test ID\", fontsize=12)\n",
    "plt.ylabel(\"Merge Distance (Ward Criterion)\", fontsize=12)\n",
    "plt.axhline(y=15, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Cut at h=15 \u2192 3 categories')\n",
    "plt.axhline(y=10, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='Cut at h=10 \u2192 10 subcategories')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cut dendrogram at different heights for multi-level hierarchy\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Code Continuation (2/2)\n",
    "\n",
    "Continuing implementation...\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_categories_3 = fcluster(linkage_matrix_tests, t=15, criterion='distance')\n",
    "test_categories_10 = fcluster(linkage_matrix_tests, t=10, criterion='distance')\n",
    "\n",
    "print(f\"\\n\ud83c\udf33 Multi-Level Test Hierarchy:\")\n",
    "print(f\"  \u2022 Top level (h=15): {len(np.unique(test_categories_3))} categories\")\n",
    "print(f\"    - Category sizes: {np.bincount(test_categories_3)}\")\n",
    "print(f\"  \u2022 Second level (h=10): {len(np.unique(test_categories_10))} subcategories\")\n",
    "print(f\"    - Subcategory sizes: {np.bincount(test_categories_10)}\")\n",
    "\n",
    "# Identify highly correlated test pairs (candidates for redundancy reduction)\n",
    "correlation_matrix = np.corrcoef(test_vectors)\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(n_tests):\n",
    "    for j in range(i+1, n_tests):\n",
    "        if correlation_matrix[i, j] > 0.95:  # Very high correlation\n",
    "            high_corr_pairs.append((i, j, correlation_matrix[i, j]))\n",
    "\n",
    "print(f\"\\n\ud83d\udd0d Redundant Test Detection:\")\n",
    "print(f\"  \u2022 Test pairs with correlation >0.95: {len(high_corr_pairs)}\")\n",
    "if high_corr_pairs:\n",
    "    print(f\"  \u2022 Example: Test {high_corr_pairs[0][0]+1} \u2194 Test {high_corr_pairs[0][1]+1} (r={high_corr_pairs[0][2]:.3f})\")\n",
    "    print(f\"  \u2022 Recommendation: Consider removing one test from each highly correlated pair\")\n",
    "    print(f\"  \u2022 Potential test time reduction: {len(high_corr_pairs)} tests \u00d7 0.3s = {len(high_corr_pairs)*0.3:.1f}s per device\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcb0 Business Impact:\")\n",
    "print(f\"  \u2022 Removing {len(high_corr_pairs)} redundant tests:\")\n",
    "print(f\"  \u2022 Test time saved: {len(high_corr_pairs)*0.3:.1f}s per device\")\n",
    "print(f\"  \u2022 Annual devices: 1M\")\n",
    "print(f\"  \u2022 Total time saved: {len(high_corr_pairs)*0.3*1e6/3600:.0f} hours/year\")\n",
    "print(f\"  \u2022 Cost savings at $100/tester-hour: ${len(high_corr_pairs)*0.3*1e6/3600*100:,.0f}/year\")\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Test Hierarchy Summary:\")\n",
    "print(f\"  \u2022 Level 1 (3 categories): Power, Speed, Leakage\")\n",
    "print(f\"  \u2022 Level 2 (10 subcategories): Vdd/Idd/Power within Power, Freq/Timing within Speed, etc.\")\n",
    "print(f\"  \u2022 Engineers can navigate test program at appropriate granularity\")\n",
    "print(f\"  \u2022 Failure analysis: Match failure signature to leaf category \u2192 narrow root cause\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f5d8e1ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udfaf Real-World Projects (Not Exercises!)\n",
    "\n",
    "Each project includes clear objectives, business value, and implementation guidance.\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "#### 1. \ud83c\udfed Automatic Test Program Organization System\n",
    "**Objective:** Build hierarchical taxonomy of 500+ parametric tests across multiple product generations, automatically maintaining structure as tests evolve.\n",
    "\n",
    "**Business Value:** $2M+ annual engineering efficiency (reduce test program maintenance from 100 hours/month \u2192 10 hours with auto-categorization).\n",
    "\n",
    "**Key Features:**\n",
    "- Input: 500 tests \u00d7 100K devices parametric data per product\n",
    "- Multi-level hierarchy: 3 top categories \u2192 15 mid-level \u2192 60 leaf groups\n",
    "- Dendrogram visualization: Interactive plot with zoom, hover tooltips showing test names\n",
    "- Redundancy detection: Flag test pairs with correlation >0.95 for elimination review\n",
    "- Temporal tracking: Monitor how test hierarchy changes across product generations\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use correlation distance metric (1 - Pearson correlation) for test similarity\n",
    "- Apply ward linkage for balanced categories\n",
    "- Store linkage matrix + cut heights in database for reproducibility\n",
    "- Build web dashboard with Plotly for dendrogram exploration\n",
    "- Alert system: Email when new test doesn't fit existing categories (novel functionality)\n",
    "\n",
    "**Success Metrics:** Achieve 90%+ agreement with manual expert categorization, reduce test program review time from 2 weeks \u2192 2 days per new product.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. \ud83d\udcca Failure Mode Taxonomy Builder\n",
    "**Objective:** Automatically discover hierarchical failure patterns from 10K+ failed device test signatures to accelerate root cause analysis.\n",
    "\n",
    "**Business Value:** $5M+ quarterly yield recovery by reducing failure analysis time from 3 days \u2192 4 hours per systematic excursion.\n",
    "\n",
    "**Key Features:**\n",
    "- Input: Test signatures from 10K failed devices (50-100 parametric tests)\n",
    "- Hierarchical failure clusters: Top level = failure domain (power/speed/leakage), mid level = specific parameter group, leaf = test combination\n",
    "- Signature matching: New failures automatically assigned to closest cluster (nearest neighbor in dendrogram)\n",
    "- Root cause hints: Each leaf cluster linked to known failure mechanisms from historical data\n",
    "- Visualization: Dendrogram colored by failure frequency (red = common, green = rare)\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Normalize test results to z-scores (failure magnitude independent)\n",
    "- Use complete linkage (tight failure mode definition)\n",
    "- Track cluster appearance over time (new clusters = novel failure modes)\n",
    "- Integrate with JIRA: Auto-create tickets for new failure clusters\n",
    "- Machine learning enhancement: Train classifier on cluster labels for fast triage\n",
    "\n",
    "**Success Metrics:** Classify 85%+ of failures into known categories within 30 seconds, reduce unknown failure investigation from 40% \u2192 15% of total FA time.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. \ud83d\udd17 Multi-Site Test Correlation Engine\n",
    "**Objective:** Discover hierarchical relationships between 4-6 test sites (wafer probe, final test, system level) to optimize test content and reduce redundancy.\n",
    "\n",
    "**Business Value:** $3M+ annual savings by eliminating 20-30% redundant tests across multi-site flow without yield impact.\n",
    "\n",
    "**Key Features:**\n",
    "- Input: Parametric data from 50K devices tested at 3 sites (wafer test, final test, board test)\n",
    "- Cross-site hierarchy: Cluster tests across all sites, identify which site tests overlap\n",
    "- Transfer function: For overlapping tests, model wafer\u2192final correlation (predict final from wafer)\n",
    "- Optimization: Recommend test moves (e.g., \"Move tests 15-20 from final\u2192wafer, saves 5s/device\")\n",
    "- Risk analysis: Quantify yield impact of removing redundant tests (confidence intervals)\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Concatenate test vectors from all sites (150-dimensional for 3 sites \u00d7 50 tests)\n",
    "- Use average linkage (robust to cross-site measurement noise)\n",
    "- Cophenetic correlation threshold: >0.75 indicates strong cross-site redundancy\n",
    "- Simulation: A/B test on 10K devices before production deployment\n",
    "- Business case calculator: TCO model (test time savings vs yield risk vs equipment cost)\n",
    "\n",
    "**Success Metrics:** Identify 15-25 redundant tests with 95%+ confidence, achieve 8-12 second test time reduction per device, maintain <0.1% yield loss.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. \ud83c\udf33 Die Similarity Tree for Spatial Analysis\n",
    "**Objective:** Build hierarchical tree of wafer die based on parametric profiles + spatial location to identify systematic spatial patterns.\n",
    "\n",
    "**Business Value:** $2M+ quarterly by detecting spatial yield patterns 3-5 days faster than manual wafer map inspection.\n",
    "\n",
    "**Key Features:**\n",
    "- Input: 300 die per wafer \u00d7 50 parametric tests + (x,y) coordinates\n",
    "- Hybrid distance: Combine parametric similarity (correlation) + spatial proximity (Euclidean)\n",
    "- Multi-level spatial clusters: Wafer zones \u2192 quadrants \u2192 local regions \u2192 individual die\n",
    "- Dendrogram coloring: Color by spatial location to visualize spatial coherence\n",
    "- Anomaly detection: Isolated die in dendrogram = outliers (potential yield loss)\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Feature engineering: [parametric_z_scores \u00d7 2, x_coordinate, y_coordinate] (weight parametric 2:1)\n",
    "- Use ward linkage with spatial connectivity constraints (sklearn kneighbors_graph)\n",
    "- Visualize: Side-by-side dendrogram + wafer map with cluster colors\n",
    "- Temporal analysis: Track how die similarity tree changes across wafer lots\n",
    "- Integration: Feed spatial clusters into failure analysis workflow\n",
    "\n",
    "**Success Metrics:** Detect 90%+ of spatial excursions within 2 hours of wafer test completion, reduce manual wafer map reviews from 200 \u2192 50 per week.\n",
    "\n",
    "---\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "#### 5. \ud83d\udcc4 Document Taxonomy for Knowledge Management\n",
    "**Objective:** Automatically organize 50K enterprise documents into hierarchical categories for improved search and knowledge discovery.\n",
    "\n",
    "**Business Value:** $3M+ annual productivity improvement (reduce document search time from 15 min \u2192 2 min avg per search \u00d7 100K searches/year).\n",
    "\n",
    "**Key Features:**\n",
    "- Input: 50K documents (PDFs, Word, emails) converted to TF-IDF vectors\n",
    "- Multi-level taxonomy: 10 top categories \u2192 50 mid-level \u2192 200 leaf categories\n",
    "- Auto-tagging: New documents automatically assigned to leaf categories\n",
    "- Search enhancement: Hierarchical navigation (drill down from broad \u2192 specific)\n",
    "- Anomaly detection: Documents that don't fit existing taxonomy = potential new topics\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use TF-IDF or sentence embeddings (BERT) for document vectors\n",
    "- Average linkage for balanced categories\n",
    "- Interactive dendrogram with document counts at each node\n",
    "- Store taxonomy in graph database (Neo4j) for fast hierarchical queries\n",
    "- User feedback loop: Allow manual reclassification to improve hierarchy\n",
    "\n",
    "**Success Metrics:** Achieve 80%+ precision/recall vs manual categorization, reduce \"document not found\" rate from 25% \u2192 8%.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. \ud83e\uddec Gene Expression Clustering for Bioinformatics\n",
    "**Objective:** Discover hierarchical relationships among 20K genes across 100 patient samples to identify disease subtypes and biomarker candidates.\n",
    "\n",
    "**Business Value:** $50M+ drug development acceleration by identifying target gene clusters 6-12 months faster than manual curation.\n",
    "\n",
    "**Key Features:**\n",
    "- Input: 20K genes \u00d7 100 patients expression matrix (RNA-seq data)\n",
    "- Two-way clustering: Cluster genes (rows) AND patients (columns)\n",
    "- Heatmap visualization: Dendrogram-ordered heatmap shows gene modules\n",
    "- Biomarker discovery: Genes in same leaf cluster = co-regulated \u2192 functional pathway\n",
    "- Patient stratification: Hierarchical patient clusters = disease subtypes\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Use correlation distance (genes with similar expression patterns cluster)\n",
    "- Average linkage for robust gene modules\n",
    "- Two dendrograms: One for genes (rows), one for patients (columns)\n",
    "- Statistical validation: Bootstrap resampling to assess cluster stability\n",
    "- Pathway enrichment: Link gene clusters to known biological pathways (KEGG, GO)\n",
    "\n",
    "**Success Metrics:** Discover 5-8 reproducible gene modules, identify 3+ novel biomarker candidates, stratify patients into 4-6 disease subtypes with clinical relevance.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. \ud83d\uded2 Product Catalog Hierarchy for E-Commerce\n",
    "**Objective:** Automatically build multi-level product taxonomy from 100K products based on attributes, descriptions, and purchase co-occurrence.\n",
    "\n",
    "**Business Value:** $10M+ annual revenue increase (improve product discovery, reduce search abandonment from 35% \u2192 22%).\n",
    "\n",
    "**Key Features:**\n",
    "- Input: 100K products with attributes (brand, price, category, description embeddings) + co-purchase matrix\n",
    "- 4-level hierarchy: Department \u2192 Category \u2192 Subcategory \u2192 Product clusters\n",
    "- Dynamic taxonomy: Updates weekly as new products added, trends shift\n",
    "- Recommendation enhancement: Products in same leaf cluster = \"similar items\"\n",
    "- Search relevance: Use hierarchy for query expansion (search \"laptop\" returns all devices in laptop cluster)\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Combine attribute similarity (Euclidean) + co-purchase affinity (Jaccard)\n",
    "- Ward linkage for balanced categories (each level has 5-20 children)\n",
    "- Store in hierarchical database (parent-child relationships)\n",
    "- A/B test: Hierarchical navigation vs flat search (conversion rate metric)\n",
    "- Seasonality handling: Recompute hierarchy quarterly to capture trend changes\n",
    "\n",
    "**Success Metrics:** Increase product discovery rate from 60% \u2192 80%, reduce category navigation depth from 5 clicks \u2192 3 clicks avg, improve conversion 8-12%.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. \ud83c\udfb5 Music Genre Taxonomy for Streaming Service\n",
    "**Objective:** Build hierarchical genre classification from 1M songs using audio features (tempo, key, spectral features) + user listening patterns.\n",
    "\n",
    "**Business Value:** $20M+ annual retention improvement (reduce churn 2% via better personalization) + $5M playlist curation efficiency.\n",
    "\n",
    "**Key Features:**\n",
    "- Input: 1M songs with audio features (Spotify API: tempo, key, energy, valence, etc.) + user co-listen matrix\n",
    "- Genre hierarchy: Top = broad genres (rock, pop, electronic) \u2192 subgenres \u2192 micro-genres\n",
    "- Playlist generation: Auto-create hierarchical playlists (\"Electronic \u2192 House \u2192 Deep House\")\n",
    "- Discovery algorithm: Recommend songs from neighboring leaf clusters (exploration within taste)\n",
    "- Temporal trends: Track how genre hierarchy evolves (new micro-genres emerge)\n",
    "\n",
    "**Implementation Hints:**\n",
    "- Standardize audio features (tempo in BPM, key in 0-11, etc.)\n",
    "- Average linkage for balanced genre tree\n",
    "- Validate with human curators: Genre labels should match industry taxonomy 70%+\n",
    "- Hybrid approach: Combine hierarchical clustering with user tags (collaborative + content-based)\n",
    "- Interactive visualization: D3.js tree with song samples at leaf nodes\n",
    "\n",
    "**Success Metrics:** Match human genre labels 75%+ accuracy, generate 1000+ playlists with 8+ satisfaction rating, increase avg listening time 12-15 minutes/user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edacd2d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## \ud83c\udf93 Key Takeaways & Best Practices\n",
    "\n",
    "### \u2705 When to Use Hierarchical Clustering\n",
    "\n",
    "1. **K is unknown**: Don't know how many clusters exist \u2192 dendrogram reveals natural groupings at multiple levels\n",
    "2. **Taxonomy needed**: Want hierarchical relationships (not just flat clusters) \u2192 e.g., test hierarchy, failure mode tree\n",
    "3. **Small-medium data (<5K points)**: O(n\u00b2) complexity manageable, dendrogram visualization useful\n",
    "4. **Deterministic results required**: Same distance matrix always produces same dendrogram (vs K-Means random init)\n",
    "5. **Exploratory analysis**: Understand data structure before committing to specific K\n",
    "6. **Multiple granularities**: Need clustering at different resolutions (e.g., 3 top categories, 10 subcategories)\n",
    "\n",
    "**Example Scenarios:**\n",
    "- \u2705 Test hierarchy discovery (500 tests, unknown natural groupings)\n",
    "- \u2705 Failure mode taxonomy (build tree of failure patterns)\n",
    "- \u2705 Document organization (discover multi-level topic structure)\n",
    "- \u2705 Gene expression analysis (identify gene modules, patient subtypes)\n",
    "\n",
    "### \u274c When NOT to Use Hierarchical Clustering\n",
    "\n",
    "1. **Large data (>10K points)**: O(n\u00b2 log n) complexity too slow, K-Means/MiniBatchKMeans 100\u00d7 faster\n",
    "2. **Spherical clusters with known K**: K-Means more efficient, produces similar results\n",
    "3. **Outliers dominate**: Hierarchical forces all points into clusters, DBSCAN better for noise handling\n",
    "4. **Real-time inference**: Clustering new points requires full recomputation, K-Means predicts instantly\n",
    "5. **Greedy merge problem**: Once clusters merge, can't un-merge \u2192 divisive clustering or K-Means may be better\n",
    "\n",
    "**Example Scenarios:**\n",
    "- \u274c Customer segmentation with 1M users (use K-Means or MiniBatchKMeans)\n",
    "- \u274c Real-time anomaly detection (use Isolation Forest or LOF)\n",
    "- \u274c Image clustering with 100K images (use K-Means after dimensionality reduction)\n",
    "- \u274c Geospatial clusters with noise (use DBSCAN)\n",
    "\n",
    "### \ud83d\udd0d Hierarchical vs K-Means vs DBSCAN - Decision Framework\n",
    "\n",
    "| **Use Hierarchical When...** | **Use K-Means When...** | **Use DBSCAN When...** |\n",
    "|------------------------------|------------------------|----------------------|\n",
    "| K unknown, need exploration | K known or easily determined | K unknown, outliers present |\n",
    "| Want hierarchical taxonomy | Want fast, scalable clustering | Want arbitrary cluster shapes |\n",
    "| <5K points, can afford O(n\u00b2) | 10K-1M+ points, need O(nkt) | Geospatial, density-based patterns |\n",
    "| Deterministic results critical | Random init acceptable | Noise handling critical |\n",
    "| Dendrogram visualization valuable | Centroids provide interpretability | No predefined distance threshold |\n",
    "\n",
    "**Post-Silicon Decision Tree:**\n",
    "```\n",
    "IF test hierarchy discovery (500 tests) \u2192 Hierarchical (ward/average)\n",
    "ELSE IF wafer clustering (50K die) \u2192 K-Means or MiniBatchKMeans\n",
    "ELSE IF spatial defect detection (outliers) \u2192 DBSCAN\n",
    "ELSE IF failure taxonomy (10K failures) \u2192 Hierarchical (complete linkage)\n",
    "ELSE IF real-time test triage \u2192 K-Means (pre-trained centroids)\n",
    "```\n",
    "\n",
    "### \ud83d\udd27 Linkage Method Selection Guide\n",
    "\n",
    "| **Linkage** | **Best For** | **Cluster Shape** | **Outlier Sensitivity** | **Post-Silicon Use Case** |\n",
    "|------------|-------------|-------------------|------------------------|--------------------------|\n",
    "| **Single** | Outlier detection, chain structures | Elongated, irregular | High (creates long chains) | Identify isolated tests that don't fit categories |\n",
    "| **Complete** | Tight, compact clusters | Spherical, well-separated | Low (compact groups) | Group highly correlated tests (>0.8) |\n",
    "| **Average** | General-purpose, robust | Balanced, moderate | Medium (robust to noise) | Default for test hierarchy (balanced categories) |\n",
    "| **Ward** | Balanced sizes, K-Means-like | Spherical, equal-sized | Low (variance minimization) | Organize tests into equal-sized groups for parallel execution |\n",
    "\n",
    "**Recommendation Matrix:**\n",
    "- **Test hierarchy**: Ward or Average (balanced categories, robust)\n",
    "- **Failure taxonomy**: Complete (tight failure mode definition)\n",
    "- **Die similarity**: Ward with spatial connectivity (balanced spatial clusters)\n",
    "- **Redundancy detection**: Average (robust to measurement noise)\n",
    "\n",
    "### \ud83d\udd27 Implementation Best Practices\n",
    "\n",
    "1. **Distance Metric Matters**: Choose based on data type\n",
    "   ```python\n",
    "   # For continuous features\n",
    "   linkage(X, method='ward', metric='euclidean')\n",
    "   \n",
    "   # For test correlation (test vectors)\n",
    "   linkage(pdist(X, metric='correlation'), method='average')\n",
    "   \n",
    "   # For binary features\n",
    "   linkage(pdist(X, metric='jaccard'), method='complete')\n",
    "   ```\n",
    "\n",
    "2. **Dendrogram Cut Height Selection**:\n",
    "   - **Visual inspection**: Look for large vertical gaps (natural boundaries)\n",
    "   - **Elbow method**: Plot number of clusters vs within-cluster variance\n",
    "   - **Cophenetic distance**: Cut where cophenetic correlation drops sharply\n",
    "   - **Domain knowledge**: Post-silicon example - 3 top categories (power/speed/leakage)\n",
    "\n",
    "3. **Cophenetic Correlation Validation**:\n",
    "   ```python\n",
    "   coph_corr, coph_dist = cophenet(linkage_matrix, pdist(X))\n",
    "   # >0.8: Excellent (dendrogram faithfully represents data)\n",
    "   # 0.6-0.8: Good\n",
    "   # <0.6: Poor (consider different linkage or distance metric)\n",
    "   ```\n",
    "\n",
    "4. **Memory Efficiency for Large Data**:\n",
    "   ```python\n",
    "   # For 5K-10K points, use condensed distance matrix\n",
    "   distances = pdist(X, metric='euclidean')  # Saves memory\n",
    "   linkage_matrix = linkage(distances, method='average')\n",
    "   \n",
    "   # For 10K+ points, use sklearn (more memory-efficient)\n",
    "   from sklearn.cluster import AgglomerativeClustering\n",
    "   hc = AgglomerativeClustering(n_clusters=None, distance_threshold=10)\n",
    "   ```\n",
    "\n",
    "5. **Multi-Level Hierarchy Extraction**:\n",
    "   ```python\n",
    "   # Top level: 3 categories\n",
    "   labels_L1 = fcluster(linkage_matrix, t=15, criterion='distance')\n",
    "   \n",
    "   # Mid level: 10 subcategories\n",
    "   labels_L2 = fcluster(linkage_matrix, t=10, criterion='distance')\n",
    "   \n",
    "   # Leaf level: 30 fine-grained groups\n",
    "   labels_L3 = fcluster(linkage_matrix, t=5, criterion='distance')\n",
    "   ```\n",
    "\n",
    "6. **Connectivity Constraints (Spatial Data)**:\n",
    "   ```python\n",
    "   from sklearn.neighbors import kneighbors_graph\n",
    "   \n",
    "   # Force spatially adjacent points to cluster together\n",
    "   connectivity = kneighbors_graph(X, n_neighbors=5, include_self=False)\n",
    "   hc = AgglomerativeClustering(n_clusters=3, connectivity=connectivity)\n",
    "   ```\n",
    "\n",
    "### \u26a0\ufe0f Common Pitfalls\n",
    "\n",
    "1. **Ignoring Distance Metric**: Using Euclidean for test correlation \u2192 use 'correlation' distance\n",
    "2. **Single Linkage Chaining**: Single linkage creates long chains for noisy data \u2192 use average/ward\n",
    "3. **No Dendrogram Inspection**: Cutting blindly without visual inspection \u2192 miss natural boundaries\n",
    "4. **Scalability Ignorance**: Applying to 100K+ points \u2192 use K-Means or sampling instead\n",
    "5. **Not Validating Cophenetic Correlation**: Low correlation (<0.6) means dendrogram doesn't represent data well\n",
    "\n",
    "### \ud83d\udcca Evaluation Metrics\n",
    "\n",
    "| **Metric** | **Formula/Method** | **Interpretation** | **Ideal Value** |\n",
    "|-----------|-------------------|-------------------|----------------|\n",
    "| **Cophenetic Correlation** | Correlation between original distances and cophenetic distances | Dendrogram faithfulness | >0.8 excellent, 0.6-0.8 good |\n",
    "| **Silhouette Score** | $\\frac{b - a}{\\max(a, b)}$ (cohesion vs separation) | Cluster quality at specific cut | 0.5-0.7 good, >0.7 excellent |\n",
    "| **Adjusted Rand Index** | Agreement with ground truth (if available) | External validation | 0.8-1.0 excellent |\n",
    "| **Dendrogram Height Gap** | Max vertical distance between merges | Natural cluster boundary | Large gaps = good separation |\n",
    "\n",
    "### \ud83d\ude80 Next Steps in Clustering Mastery\n",
    "\n",
    "1. **DBSCAN** (Notebook 028): Density-based clustering for outlier handling and arbitrary shapes\n",
    "2. **Gaussian Mixture Models** (Notebook 029): Probabilistic clustering with soft assignments\n",
    "3. **Dimensionality Reduction** (Notebook 030): PCA, t-SNE, UMAP for visualizing hierarchical clusters\n",
    "4. **Advanced Topics**: Divisive clustering, BIRCH (scalable hierarchical), HDBSCAN (hierarchical DBSCAN)\n",
    "\n",
    "### \ud83d\udca1 Final Thoughts\n",
    "\n",
    "**Hierarchical Clustering Strengths:**\n",
    "- No K required \u2192 dendrogram reveals natural structure\n",
    "- Multi-level granularity \u2192 taxonomy at multiple resolutions\n",
    "- Deterministic \u2192 same distance matrix = same tree\n",
    "- Interpretable \u2192 dendrogram visualizes relationships\n",
    "\n",
    "**Hierarchical Clustering Limitations:**\n",
    "- O(n\u00b2 log n) complexity \u2192 slow for large data (>10K points)\n",
    "- Greedy merges \u2192 can't undo poor early merges\n",
    "- Forces all points into clusters \u2192 poor outlier handling\n",
    "- Linkage-dependent \u2192 choice of linkage significantly affects results\n",
    "\n",
    "**Production Checklist:**\n",
    "- \u2705 Choose appropriate distance metric (correlation for test similarity, Euclidean for spatial)\n",
    "- \u2705 Select linkage method (ward/average for general, complete for tight clusters, single for outlier detection)\n",
    "- \u2705 Visualize dendrogram and validate cophenetic correlation (>0.7)\n",
    "- \u2705 Use domain knowledge to select cut height (look for large gaps)\n",
    "- \u2705 Extract multi-level hierarchy if needed (3 top categories, 10 subcategories, etc.)\n",
    "- \u2705 Consider sklearn for production (connectivity constraints, memory efficiency)\n",
    "- \u2705 For large data (>10K), use K-Means or sampling + hierarchical on samples\n",
    "\n",
    "**Post-Silicon Context:**\n",
    "- Hierarchical clustering excels at test hierarchy discovery (500 tests \u2192 3-10 categories)\n",
    "- Enables multi-level failure taxonomy (domain \u2192 parameter \u2192 test combination)\n",
    "- Critical for organizing complex test programs without manual categorization\n",
    "- Cophenetic correlation >0.75 indicates test relationships are well-captured\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf89 Congratulations!\n",
    "\n",
    "You've mastered hierarchical clustering - from agglomerative algorithm mechanics to dendrogram interpretation to production test hierarchy discovery. You can now:\n",
    "- \u2705 Implement agglomerative clustering from scratch with Lance-Williams distance updates\n",
    "- \u2705 Build and interpret dendrograms using scipy.cluster.hierarchy\n",
    "- \u2705 Select optimal cut height and linkage method based on data characteristics\n",
    "- \u2705 Apply hierarchical clustering to test hierarchy discovery and failure taxonomy\n",
    "- \u2705 Choose between hierarchical, K-Means, DBSCAN based on data size, shape, and goals\n",
    "\n",
    "**Next:** Explore DBSCAN (Notebook 028) for density-based clustering with automatic outlier detection!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}