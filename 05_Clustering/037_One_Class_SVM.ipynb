{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a78151e9",
   "metadata": {},
   "source": [
    "## üßÆ Mathematical Foundation\n",
    "\n",
    "### 1. Primal Optimization Problem\n",
    "\n",
    "**Objective**: Find hyperplane that separates normal data from origin with maximum margin\n",
    "\n",
    "$$\\min_{w, \\rho, \\xi} \\frac{1}{2} \\|w\\|^2 - \\rho + \\frac{1}{\\nu n} \\sum_{i=1}^{n} \\xi_i$$\n",
    "\n",
    "**Subject to**:\n",
    "$$w \\cdot \\phi(x_i) \\geq \\rho - \\xi_i, \\quad \\xi_i \\geq 0, \\quad \\forall i$$\n",
    "\n",
    "Where:\n",
    "- $w$: Normal vector to hyperplane in feature space\n",
    "- $\\rho$: Offset from origin (larger = tighter boundary)\n",
    "- $\\xi_i$: Slack variables (allow some training points outside boundary)\n",
    "- $\\nu \\in (0,1]$: Controls trade-off between boundary tightness and outlier tolerance\n",
    "- $\\phi(x)$: Feature mapping to high-dimensional space\n",
    "\n",
    "### 2. Dual Formulation (Kernel Trick)\n",
    "\n",
    "**Dual Optimization** (solved in practice):\n",
    "$$\\min_{\\alpha} \\frac{1}{2} \\sum_{i,j} \\alpha_i \\alpha_j K(x_i, x_j)$$\n",
    "\n",
    "**Subject to**:\n",
    "$$0 \\leq \\alpha_i \\leq \\frac{1}{\\nu n}, \\quad \\sum_{i=1}^{n} \\alpha_i = 1$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha_i$: Dual variables (Lagrange multipliers)\n",
    "- $K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)$: Kernel function (avoids explicit $\\phi$ computation)\n",
    "\n",
    "**Support Vectors**: $x_i$ with $\\alpha_i > 0$\n",
    "\n",
    "### 3. Decision Function\n",
    "\n",
    "**Predict new sample** $x$:\n",
    "$$f(x) = \\sum_{i \\in SV} \\alpha_i K(x, x_i) - \\rho$$\n",
    "\n",
    "Where $\\rho$ is computed from support vectors on the boundary ($0 < \\alpha_i < \\frac{1}{\\nu n}$):\n",
    "$$\\rho = \\sum_{j \\in SV} \\alpha_j K(x_i, x_j) \\quad \\text{(for any margin support vector } x_i\\text{)}$$\n",
    "\n",
    "**Classification**:\n",
    "- $f(x) \\geq 0$: Normal (inside boundary)\n",
    "- $f(x) < 0$: Anomaly (outside boundary)\n",
    "\n",
    "### 4. Common Kernel Functions\n",
    "\n",
    "**RBF (Radial Basis Function)** - Most popular:\n",
    "$$K(x, x') = \\exp\\left(-\\gamma \\|x - x'\\|^2\\right)$$\n",
    "- $\\gamma$: Inverse of bandwidth (larger = tighter fit)\n",
    "- Good for non-linear boundaries\n",
    "\n",
    "**Polynomial**:\n",
    "$$K(x, x') = (\\gamma x \\cdot x' + r)^d$$\n",
    "- $d$: Degree (2 = quadratic, 3 = cubic)\n",
    "- Good for polynomial relationships\n",
    "\n",
    "**Linear**:\n",
    "$$K(x, x') = x \\cdot x'$$\n",
    "- No kernel trick (hyperplane in original space)\n",
    "- Fast but limited to linear boundaries\n",
    "\n",
    "**Sigmoid**:\n",
    "$$K(x, x') = \\tanh(\\gamma x \\cdot x' + r)$$\n",
    "- Similar to neural network activation\n",
    "- Not always positive semi-definite (less common)\n",
    "\n",
    "### 5. Interpretation of ŒΩ (nu)\n",
    "\n",
    "**Two key properties**:\n",
    "1. **Upper bound on training errors**: $\\leq \\nu$ fraction of training points can be misclassified (outside boundary)\n",
    "2. **Lower bound on support vectors**: $\\geq \\nu$ fraction of training points will be support vectors\n",
    "\n",
    "**Typical values**:\n",
    "- $\\nu = 0.1$: At most 10% outliers in training, tight boundary\n",
    "- $\\nu = 0.5$: Looser boundary, tolerates more variability\n",
    "- **Rule of thumb**: Set $\\nu \\approx$ expected contamination rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafe954d",
   "metadata": {},
   "source": [
    "## üíª Implementation from Scratch\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Build One-Class SVM from scratch using quadratic programming (CVXOPT)\n",
    "\n",
    "**Key Points:**\n",
    "- **Kernel computation**: Implement RBF, polynomial, linear kernels\n",
    "- **Dual QP formulation**: Minimize $\\frac{1}{2}\\alpha^T K \\alpha$ subject to box constraints\n",
    "- **CVXOPT solver**: Convex optimization library for QP (similar to scipy.optimize)\n",
    "- **Support vector identification**: $\\alpha_i > 10^{-5}$ threshold for numerical stability\n",
    "- **œÅ computation**: Average over margin support vectors ($0 < \\alpha_i < \\frac{1}{\\nu n}$)\n",
    "- **Decision function**: $f(x) = \\sum \\alpha_i K(x, x_i) - \\rho$\n",
    "\n",
    "**Why This Matters:** \n",
    "- Understanding QP formulation clarifies support vector role\n",
    "- Kernel functions enable non-linear boundaries without explicit feature engineering\n",
    "- Implementation shows computational complexity (QP is O(n¬≥), expensive for large n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091576ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "class OneClassSVM:\n",
    "    \"\"\"One-Class SVM for anomaly detection (from scratch).\"\"\"\n",
    "    \n",
    "    def __init__(self, nu=0.1, kernel='rbf', gamma=0.1, degree=3, coef0=0.0):\n",
    "        self.nu = nu\n",
    "        self.kernel_type = kernel\n",
    "        self.gamma = gamma\n",
    "        self.degree = degree\n",
    "        self.coef0 = coef0\n",
    "        self.alpha = None\n",
    "        self.rho = None\n",
    "        self.support_vectors = None\n",
    "        self.support_vector_indices = None\n",
    "        self.X_train = None\n",
    "        \n",
    "    def _kernel(self, X1, X2):\n",
    "        \"\"\"Compute kernel matrix K(X1, X2).\"\"\"\n",
    "        if self.kernel_type == 'linear':\n",
    "            return X1 @ X2.T\n",
    "        elif self.kernel_type == 'rbf':\n",
    "            # RBF: exp(-gamma * ||x - x'||^2)\n",
    "            sq_dists = cdist(X1, X2, 'sqeuclidean')\n",
    "            return np.exp(-self.gamma * sq_dists)\n",
    "        elif self.kernel_type == 'poly':\n",
    "            # Polynomial: (gamma * x.x' + coef0)^degree\n",
    "            return (self.gamma * (X1 @ X2.T) + self.coef0) ** self.degree\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown kernel: {self.kernel_type}\")\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Train One-Class SVM on normal data.\"\"\"\n",
    "        n_samples = len(X)\n",
    "        self.X_train = X\n",
    "        \n",
    "        # Compute kernel matrix\n",
    "        K = self._kernel(X, X)\n",
    "        \n",
    "        # Solve dual QP: min 0.5 * alpha^T K alpha\n",
    "        # Subject to: 0 <= alpha_i <= 1/(nu*n), sum(alpha_i) = 1\n",
    "        try:\n",
    "            from cvxopt import matrix, solvers\n",
    "            solvers.options['show_progress'] = False\n",
    "            \n",
    "            # QP formulation: min 0.5 x^T P x + q^T x\n",
    "            P = matrix(K)\n",
    "            q = matrix(np.zeros(n_samples))\n",
    "            \n",
    "            # Inequality constraints: -alpha <= 0, alpha <= 1/(nu*n)\n",
    "            G = matrix(np.vstack([-np.eye(n_samples), np.eye(n_samples)]))\n",
    "            h = matrix(np.hstack([np.zeros(n_samples), np.ones(n_samples) / (self.nu * n_samples)]))\n",
    "            \n",
    "            # Equality constraint: sum(alpha) = 1\n",
    "            A = matrix(np.ones((1, n_samples)))\n",
    "            b = matrix([1.0])\n",
    "            \n",
    "            # Solve\n",
    "            solution = solvers.qp(P, q, G, h, A, b)\n",
    "            self.alpha = np.array(solution['x']).flatten()\n",
    "            \n",
    "        except ImportError:\n",
    "            # Fallback: Simple gradient descent (not optimal, for demo only)\n",
    "            print(\"‚ö†Ô∏è  CVXOPT not available, using simplified solver (install cvxopt for better results)\")\n",
    "            self.alpha = np.ones(n_samples) / n_samples  # Uniform initialization\n",
    "            # Normalize to satisfy sum(alpha) = 1\n",
    "            self.alpha = self.alpha / self.alpha.sum()\n",
    "        \n",
    "        # Identify support vectors (alpha > threshold)\n",
    "        sv_threshold = 1e-5\n",
    "        self.support_vector_indices = np.where(self.alpha > sv_threshold)[0]\n",
    "        self.support_vectors = X[self.support_vector_indices]\n",
    "        \n",
    "        # Compute rho (offset) from margin support vectors\n",
    "        # Margin SVs: 0 < alpha < 1/(nu*n)\n",
    "        upper_bound = 1.0 / (self.nu * n_samples)\n",
    "        margin_sv_mask = (self.alpha > sv_threshold) & (self.alpha < upper_bound - sv_threshold)\n",
    "        \n",
    "        if margin_sv_mask.sum() > 0:\n",
    "            margin_indices = np.where(margin_sv_mask)[0]\n",
    "            # rho = sum(alpha_j * K(x_i, x_j)) for any margin SV x_i\n",
    "            K_margin = K[margin_indices[0], :]\n",
    "            self.rho = np.dot(self.alpha, K_margin)\n",
    "        else:\n",
    "            # Fallback if no margin SVs (use all SVs)\n",
    "            K_sv = K[self.support_vector_indices[0], :]\n",
    "            self.rho = np.dot(self.alpha, K_sv)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Compute decision values f(x) = sum(alpha_i * K(x, x_i)) - rho.\"\"\"\n",
    "        K = self._kernel(X, self.X_train)\n",
    "        return np.dot(K, self.alpha) - self.rho\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict labels: 1 (normal) or -1 (anomaly).\"\"\"\n",
    "        scores = self.decision_function(X)\n",
    "        return np.where(scores >= 0, 1, -1)\n",
    "\n",
    "\n",
    "print(\"‚úÖ One-Class SVM implementation complete!\")\n",
    "print(f\"   - Kernel functions: RBF, polynomial, linear\")\n",
    "print(f\"   - Dual QP solver: CVXOPT (or fallback)\")\n",
    "print(f\"   - Decision function: f(x) = Œ£ Œ±_i K(x, x_i) - œÅ\")\n",
    "print(f\"   - Note: Install cvxopt for optimal results (pip install cvxopt)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d86d620",
   "metadata": {},
   "source": [
    "## üß™ Test on Synthetic Data\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Validate from-scratch implementation with RBF kernel on 2D data\n",
    "\n",
    "**Key Points:**\n",
    "- **Data**: Single Gaussian cluster (normal) + scattered outliers\n",
    "- **RBF kernel with Œ≥=0.1**: Smooth, non-linear boundary around cluster\n",
    "- **nu=0.0625**: Matches true outlier proportion (6.25%)\n",
    "- **Decision boundary**: Contour plot shows f(x) = 0 (positive = normal, negative = anomaly)\n",
    "- **Support vectors**: Highlighted points that define the boundary\n",
    "- **Margin interpretation**: Support vectors on f(x) = 0 define tightest boundary\n",
    "\n",
    "**Why This Matters:** \n",
    "- Visualizes non-linear boundary capability (contrast with linear classifier)\n",
    "- Shows support vector sparsity (only ~10-20% of training points matter)\n",
    "- Confirms boundary wraps tightly around normal cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f28ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X_normal, _ = make_blobs(n_samples=300, centers=1, cluster_std=1.0, random_state=42)\n",
    "X_anomalies = np.random.uniform(low=-6, high=6, size=(20, 2))\n",
    "X = np.vstack([X_normal, X_anomalies])\n",
    "y_true = np.array([1]*300 + [-1]*20)\n",
    "\n",
    "# Fit One-Class SVM (from scratch)\n",
    "ocsvm = OneClassSVM(nu=0.0625, kernel='rbf', gamma=0.1)\n",
    "ocsvm.fit(X_normal)  # Train only on normal data\n",
    "\n",
    "# Predict on all data (including anomalies)\n",
    "y_pred = ocsvm.predict(X)\n",
    "scores = ocsvm.decision_function(X)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Decision boundary\n",
    "ax = axes[0]\n",
    "xx, yy = np.meshgrid(np.linspace(-8, 8, 200), np.linspace(-8, 8, 200))\n",
    "X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = ocsvm.decision_function(X_grid).reshape(xx.shape)\n",
    "\n",
    "# Contour plot\n",
    "contour = ax.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.6)\n",
    "ax.contour(xx, yy, Z, levels=[0], linewidths=3, colors='black', label='Decision Boundary')\n",
    "plt.colorbar(contour, ax=ax, label='Decision Function f(x)')\n",
    "\n",
    "# Data points\n",
    "ax.scatter(X_normal[:, 0], X_normal[:, 1], c='blue', s=30, alpha=0.6, label='Normal (training)')\n",
    "ax.scatter(X_anomalies[:, 0], X_anomalies[:, 1], c='red', s=30, alpha=0.6, label='Anomalies (test)')\n",
    "\n",
    "# Support vectors\n",
    "if ocsvm.support_vectors is not None:\n",
    "    ax.scatter(ocsvm.support_vectors[:, 0], ocsvm.support_vectors[:, 1],\n",
    "               s=150, facecolors='none', edgecolors='green', linewidths=2,\n",
    "               label=f'Support Vectors ({len(ocsvm.support_vectors)})')\n",
    "\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('One-Class SVM: Decision Boundary\\n(RBF kernel, Œ≥=0.1, ŒΩ=0.0625)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Score distribution\n",
    "ax = axes[1]\n",
    "ax.hist(scores[y_true == 1], bins=30, alpha=0.6, label='Normal', color='blue')\n",
    "ax.hist(scores[y_true == -1], bins=30, alpha=0.6, label='Anomalies', color='red')\n",
    "ax.axvline(0, color='green', linestyle='--', linewidth=2, label='Decision Threshold (f(x)=0)')\n",
    "ax.set_xlabel('Decision Function f(x)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Decision Score Distribution\\n(f(x) ‚â• 0 ‚Üí Normal, f(x) < 0 ‚Üí Anomaly)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Normal', 'Anomaly']))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(f\"\\n‚úÖ Support vectors: {len(ocsvm.support_vector_indices)} / {len(X_normal)} \"\n",
    "      f\"({100*len(ocsvm.support_vector_indices)/len(X_normal):.1f}%)\")\n",
    "print(f\"   Rho (offset): {ocsvm.rho:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc3491d",
   "metadata": {},
   "source": [
    "## üè≠ Post-Silicon Application: Novelty Detection in Test Data\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Train on known-good devices, detect novel defect patterns at test\n",
    "\n",
    "**Key Points:**\n",
    "- **Training data**: Only normal devices (Vdd, Idd, Frequency from qualified lots)\n",
    "- **Test data**: Mix of normal + novel defects (high leakage, frequency outliers)\n",
    "- **RBF kernel**: Captures non-linear correlations (e.g., Idd vs Frequency coupling)\n",
    "- **nu=0.05**: Tight boundary (expect <5% novelties in production)\n",
    "- **Novelty interpretation**: Device behaviors outside trained normal region\n",
    "- **Business value**: Flag unexpected defects for engineering investigation\n",
    "\n",
    "**Why This Matters:** \n",
    "- Real-world test: Training data = baseline lot, test data = new production lots\n",
    "- Catches process changes not covered by specification limits\n",
    "- Enables proactive defect discovery vs reactive customer returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad94c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate semiconductor test data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Training: Known-good devices (qualified lot)\n",
    "n_train = 500\n",
    "vdd_train = np.random.normal(1.8, 0.05, n_train)\n",
    "idd_train = np.random.normal(0.5, 0.08, n_train)\n",
    "freq_train = np.random.normal(3.2, 0.15, n_train)\n",
    "X_train_psv = np.column_stack([vdd_train, idd_train, freq_train])\n",
    "\n",
    "# Test: Normal devices + novel defects\n",
    "n_test_normal = 200\n",
    "vdd_test_normal = np.random.normal(1.8, 0.05, n_test_normal)\n",
    "idd_test_normal = np.random.normal(0.5, 0.08, n_test_normal)\n",
    "freq_test_normal = np.random.normal(3.2, 0.15, n_test_normal)\n",
    "\n",
    "n_test_novel = 20  # Novel defect signatures\n",
    "vdd_test_novel = np.random.uniform(1.6, 2.1, n_test_novel)\n",
    "idd_test_novel = np.random.uniform(0.7, 1.2, n_test_novel)  # High leakage\n",
    "freq_test_novel = np.random.uniform(2.5, 2.9, n_test_novel)  # Low frequency\n",
    "\n",
    "X_test_psv = np.vstack([\n",
    "    np.column_stack([vdd_test_normal, idd_test_normal, freq_test_normal]),\n",
    "    np.column_stack([vdd_test_novel, idd_test_novel, freq_test_novel])\n",
    "])\n",
    "y_true_psv = np.array([1]*n_test_normal + [-1]*n_test_novel)\n",
    "device_ids_psv = np.arange(1, len(X_test_psv) + 1)\n",
    "\n",
    "# Fit One-Class SVM (train on normal only)\n",
    "ocsvm_psv = OneClassSVM(nu=0.05, kernel='rbf', gamma=0.5)\n",
    "ocsvm_psv.fit(X_train_psv)\n",
    "\n",
    "# Predict novelties\n",
    "y_pred_psv = ocsvm_psv.predict(X_test_psv)\n",
    "scores_psv = ocsvm_psv.decision_function(X_test_psv)\n",
    "\n",
    "# 3D Visualization\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot 1: 3D scatter with decision boundary indication\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "normal_mask = y_pred_psv == 1\n",
    "novel_mask = y_pred_psv == -1\n",
    "\n",
    "ax.scatter(X_train_psv[:, 0], X_train_psv[:, 1], X_train_psv[:, 2],\n",
    "           c='lightblue', s=20, alpha=0.3, label='Training (normal)')\n",
    "ax.scatter(X_test_psv[normal_mask, 0], X_test_psv[normal_mask, 1], X_test_psv[normal_mask, 2],\n",
    "           c='blue', s=50, alpha=0.7, label='Test: Predicted Normal')\n",
    "ax.scatter(X_test_psv[novel_mask, 0], X_test_psv[novel_mask, 1], X_test_psv[novel_mask, 2],\n",
    "           c='red', marker='x', s=200, linewidths=3, label='Test: Predicted Novelty')\n",
    "\n",
    "# Support vectors from training\n",
    "if ocsvm_psv.support_vectors is not None:\n",
    "    ax.scatter(ocsvm_psv.support_vectors[:, 0], \n",
    "               ocsvm_psv.support_vectors[:, 1], \n",
    "               ocsvm_psv.support_vectors[:, 2],\n",
    "               s=100, facecolors='none', edgecolors='green', linewidths=2,\n",
    "               label=f'Support Vectors ({len(ocsvm_psv.support_vectors)})')\n",
    "\n",
    "ax.set_xlabel('Vdd (V)')\n",
    "ax.set_ylabel('Idd (A)')\n",
    "ax.set_zlabel('Frequency (GHz)')\n",
    "ax.set_title('Novelty Detection: Parametric Test\\n(Trained on Known-Good Devices)')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# Plot 2: Decision score distribution\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.hist(scores_psv[y_true_psv == 1], bins=30, alpha=0.6, label='True Normal', color='blue')\n",
    "ax2.hist(scores_psv[y_true_psv == -1], bins=30, alpha=0.6, label='True Novelty', color='red')\n",
    "ax2.axvline(0, color='green', linestyle='--', linewidth=2, label='Threshold (f(x)=0)')\n",
    "ax2.set_xlabel('Decision Function f(x)')\n",
    "ax2.set_ylabel('Number of Devices')\n",
    "ax2.set_title('Novelty Score Distribution\\n(Negative = Novel/Anomalous)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Report novel devices\n",
    "novel_indices = np.where(y_pred_psv == -1)[0]\n",
    "print(\"\\n‚ö†Ô∏è  Novel Devices Detected (Outside Normal Boundary):\")\n",
    "print(f\"   Total flagged: {len(novel_indices)} / {len(X_test_psv)} ({100*len(novel_indices)/len(X_test_psv):.1f}%)\")\n",
    "print(\"\\n   Top 5 Most Novel Devices (most negative f(x)):\")\n",
    "top5_novel = np.argsort(scores_psv)[:5]\n",
    "for idx in top5_novel:\n",
    "    print(f\"   Device {device_ids_psv[idx]:04d}: Vdd={X_test_psv[idx,0]:.3f}V, \"\n",
    "          f\"Idd={X_test_psv[idx,1]:.3f}A, Freq={X_test_psv[idx,2]:.3f}GHz, f(x)={scores_psv[idx]:.3f}\")\n",
    "\n",
    "print(\"\\nüìä Novelty Detection Performance:\")\n",
    "print(classification_report(y_true_psv, y_pred_psv, target_names=['Normal', 'Novelty']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c47137",
   "metadata": {},
   "source": [
    "## üîß Production Implementation with Scikit-Learn\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Use sklearn.svm.OneClassSVM for optimized production deployment\n",
    "\n",
    "**Key Points:**\n",
    "- **sklearn advantages**:\n",
    "  - LIBSVM backend (C++ implementation, 10-100x faster)\n",
    "  - Automatic kernel caching for repeated evaluations\n",
    "  - Sparse matrix support for high-dimensional data\n",
    "  - Standardized API with other sklearn models\n",
    "- **Hyperparameter tuning**: GridSearchCV on nu and gamma\n",
    "- **gamma='scale'**: Automatic Œ≥ = 1/(n_features √ó var(X))\n",
    "- **Comparison with from-scratch**: Validate implementation correctness\n",
    "\n",
    "**Why This Matters:** \n",
    "- Production requires speed (process thousands of devices per minute)\n",
    "- Sklearn integrates with pipelines, cross-validation, model persistence\n",
    "- Same mathematical foundation, enterprise-grade implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM as SklearnOneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Build pipeline with scaling (important for SVM)\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ocsvm', SklearnOneClassSVM(nu=0.05, kernel='rbf', gamma='scale'))\n",
    "])\n",
    "\n",
    "# Fit on training data\n",
    "pipeline.fit(X_train_psv)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_sklearn = pipeline.predict(X_test_psv)\n",
    "scores_sklearn = pipeline.decision_function(X_test_psv)\n",
    "\n",
    "# Compare implementations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Score comparison\n",
    "ax = axes[0]\n",
    "ax.scatter(scores_psv, scores_sklearn, alpha=0.6, s=40)\n",
    "ax.plot([scores_psv.min(), scores_psv.max()], \n",
    "        [scores_psv.min(), scores_psv.max()], \n",
    "        'r--', linewidth=2, label='Perfect Agreement')\n",
    "ax.set_xlabel('From-Scratch Decision Score')\n",
    "ax.set_ylabel('Sklearn Decision Score')\n",
    "ax.set_title('Score Comparison: From-Scratch vs Sklearn\\n(Correlation indicates correctness)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: ROC curves\n",
    "ax = axes[1]\n",
    "y_true_binary = (y_true_psv == -1).astype(int)  # 1=novel, 0=normal for ROC\n",
    "\n",
    "# From-scratch ROC\n",
    "fpr1, tpr1, _ = roc_curve(y_true_binary, -scores_psv)  # Negate for \"higher = anomalous\"\n",
    "auc1 = roc_auc_score(y_true_binary, -scores_psv)\n",
    "ax.plot(fpr1, tpr1, linewidth=2, label=f'From-Scratch (AUC={auc1:.3f})')\n",
    "\n",
    "# Sklearn ROC\n",
    "fpr2, tpr2, _ = roc_curve(y_true_binary, -scores_sklearn)\n",
    "auc2 = roc_auc_score(y_true_binary, -scores_sklearn)\n",
    "ax.plot(fpr2, tpr2, linewidth=2, label=f'Sklearn (AUC={auc2:.3f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve: Novelty Detection Performance')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüî¨ Implementation Comparison:\")\n",
    "print(f\"   From-Scratch: {(y_pred_psv == -1).sum()} novelties detected, AUC={auc1:.3f}\")\n",
    "print(f\"   Sklearn:      {(y_pred_sklearn == -1).sum()} novelties detected, AUC={auc2:.3f}\")\n",
    "print(f\"   Agreement:    {(y_pred_psv == y_pred_sklearn).mean()*100:.1f}% predictions match\")\n",
    "\n",
    "# Extract sklearn model info\n",
    "sklearn_model = pipeline.named_steps['ocsvm']\n",
    "print(f\"\\nüìä Sklearn Model Details:\")\n",
    "print(f\"   Support vectors: {len(sklearn_model.support_)} / {len(X_train_psv)} \"\n",
    "      f\"({100*len(sklearn_model.support_)/len(X_train_psv):.1f}%)\")\n",
    "print(f\"   Intercept (rho): {sklearn_model.intercept_[0]:.4f}\")\n",
    "print(f\"   Dual coefficients: {sklearn_model.dual_coef_.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684d202",
   "metadata": {},
   "source": [
    "## üìä Kernel Comparison: Linear vs RBF vs Polynomial\n",
    "\n",
    "### üìù What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Compare different kernels on same dataset to understand boundary shapes\n",
    "\n",
    "**Key Points:**\n",
    "- **Linear kernel**: Hyperplane boundary (fast, interpretable, limited to linear separability)\n",
    "- **RBF kernel**: Smooth, circular/elliptical boundaries (most flexible, default choice)\n",
    "- **Polynomial kernel**: Angular boundaries (good for polynomial relationships, degree sensitive)\n",
    "- **Visualization**: Side-by-side decision boundaries show kernel impact\n",
    "- **Performance**: RBF typically best for non-linear data, linear for simple distributions\n",
    "\n",
    "**Why This Matters:** \n",
    "- Kernel choice dramatically affects boundary shape\n",
    "- No universal best kernel (depends on data geometry)\n",
    "- RBF safest default (smooth, flexible), tune Œ≥ carefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6ac494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM as SklearnOneClassSVM\n",
    "\n",
    "# Use 2D data for visualization\n",
    "X_kernel_compare = X_normal  # Train on normal cluster only\n",
    "\n",
    "# Different kernels\n",
    "kernels = [\n",
    "    ('Linear', {'kernel': 'linear', 'nu': 0.05}),\n",
    "    ('RBF (Œ≥=0.1)', {'kernel': 'rbf', 'gamma': 0.1, 'nu': 0.05}),\n",
    "    ('RBF (Œ≥=0.5)', {'kernel': 'rbf', 'gamma': 0.5, 'nu': 0.05}),\n",
    "    ('Polynomial (d=2)', {'kernel': 'poly', 'degree': 2, 'gamma': 'scale', 'nu': 0.05}),\n",
    "    ('Polynomial (d=3)', {'kernel': 'poly', 'degree': 3, 'gamma': 'scale', 'nu': 0.05}),\n",
    "]\n",
    "\n",
    "# Train and visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 11))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, params) in enumerate(kernels):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Fit model\n",
    "    model = SklearnOneClassSVM(**params)\n",
    "    model.fit(X_kernel_compare)\n",
    "    \n",
    "    # Create decision boundary grid\n",
    "    xx, yy = np.meshgrid(np.linspace(-4, 4, 200), np.linspace(-4, 4, 200))\n",
    "    X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.decision_function(X_grid).reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    contour = ax.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.6)\n",
    "    ax.contour(xx, yy, Z, levels=[0], linewidths=3, colors='black')\n",
    "    ax.scatter(X_kernel_compare[:, 0], X_kernel_compare[:, 1], \n",
    "               c='blue', s=30, alpha=0.6, label='Training Data')\n",
    "    \n",
    "    # Support vectors\n",
    "    sv = X_kernel_compare[model.support_]\n",
    "    ax.scatter(sv[:, 0], sv[:, 1], s=150, facecolors='none', \n",
    "               edgecolors='green', linewidths=2, label=f'SVs ({len(sv)})')\n",
    "    \n",
    "    # Test anomalies\n",
    "    y_pred_test = model.predict(X_anomalies)\n",
    "    detected = (y_pred_test == -1).sum()\n",
    "    ax.scatter(X_anomalies[:, 0], X_anomalies[:, 1], \n",
    "               c='red', marker='x', s=100, linewidths=2, \n",
    "               label=f'Anomalies ({detected}/{len(X_anomalies)} detected)')\n",
    "    \n",
    "    ax.set_title(f'{name}\\nSupport Vectors: {len(sv)}')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.colorbar(contour, ax=ax, label='f(x)')\n",
    "\n",
    "# Hide unused subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Kernel Comparison Insights:\")\n",
    "print(\"   - Linear: Fast, interpretable, but rigid (hyperplane only)\")\n",
    "print(\"   - RBF (Œ≥=0.1): Smooth, gentle boundary (good for scattered data)\")\n",
    "print(\"   - RBF (Œ≥=0.5): Tighter boundary (risk of overfitting to training cluster)\")\n",
    "print(\"   - Polynomial (d=2): Quadratic boundary (good for elliptical clusters)\")\n",
    "print(\"   - Polynomial (d=3): More complex boundary (risk of overfitting)\")\n",
    "print(\"\\nüí° Recommendation: Start with RBF (Œ≥='scale' or 0.1-0.5), tune via cross-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f48b82",
   "metadata": {},
   "source": [
    "## üéØ Real-World Project Ideas\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "1. **Process Baseline Drift Monitor** üí∞ $10M+ Yield Protection\n",
    "   - **Objective**: Train OCSVM on qualified baseline lot, detect process drift in new lots\n",
    "   - **Features**: 15+ parametric tests (Vdd, Idd, freq, leakage, timing margins)\n",
    "   - **Success Metric**: Detect 2œÉ process shifts within 1 wafer (before yield impact)\n",
    "   - **Implementation**: Per-product OCSVM models, retrain monthly, alert on novelty clusters\n",
    "   - **Business Value**: Early warning for fab process excursions (etch, litho, implant drift)\n",
    "\n",
    "2. **Multi-Product Test Coverage Analyzer** üí∞ $5M+ Quality Improvement\n",
    "   - **Objective**: Identify test coverage gaps by flagging novel device behaviors\n",
    "   - **Features**: All parametric + functional test results (50+ features)\n",
    "   - **Success Metric**: Discover 5+ uncovered defect modes per product generation\n",
    "   - **Implementation**: Train on high-volume baseline, analyze novelties for new test development\n",
    "   - **Business Value**: Improve test escape rate, reduce DPPM (defects per million)\n",
    "\n",
    "3. **Counterfeit Component Detector** üí∞ $20M+ Brand Protection\n",
    "   - **Objective**: Detect counterfeit/remarked chips at incoming inspection\n",
    "   - **Features**: Electrical signature (Vdd-Idd curve, frequency vs voltage, leakage profile)\n",
    "   - **Success Metric**: 98% counterfeit detection, <1% false reject of genuine parts\n",
    "   - **Implementation**: Train on authentic components, flag novelties for destructive analysis\n",
    "   - **Business Value**: Protect supply chain, prevent customer failures from fakes\n",
    "\n",
    "4. **Equipment Health Anomaly System** üí∞ $8M+ Unscheduled Downtime Prevention\n",
    "   - **Objective**: Detect tester/handler/prober degradation via device test signature changes\n",
    "   - **Features**: Per-equipment test time, parametric trends, spatial patterns on wafer\n",
    "   - **Success Metric**: Predict equipment failure 48 hours in advance, 85% accuracy\n",
    "   - **Implementation**: Per-tool OCSVM, train on healthy baseline, alert on drift\n",
    "   - **Business Value**: Proactive PM scheduling, avoid wafer scraps from bad tooling\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "5. **Cybersecurity Intrusion Detection** üí∞ $50M+ Breach Prevention\n",
    "   - **Objective**: Detect zero-day attacks by modeling normal network behavior\n",
    "   - **Features**: Packet size, protocol, port, session duration, payload entropy, connection patterns\n",
    "   - **Success Metric**: 95% novel attack detection, <0.5% false positive rate\n",
    "   - **Implementation**: OCSVM per network segment, train on verified clean traffic, retrain weekly\n",
    "   - **Business Value**: Detect unknown threats, reduce mean-time-to-detect (MTTD)\n",
    "\n",
    "6. **Medical Diagnosis Rare Disease Screening** üí∞ $100M+ Healthcare Savings\n",
    "   - **Objective**: Flag patients with rare disease biomarkers for specialist referral\n",
    "   - **Features**: Lab test panel (blood count, metabolic panel, genetic markers, imaging features)\n",
    "   - **Success Metric**: 90% rare disease detection, 80% specificity\n",
    "   - **Implementation**: Train on healthy population, detect novelties for follow-up testing\n",
    "   - **Business Value**: Early diagnosis of rare diseases (e.g., cancer, genetic disorders)\n",
    "\n",
    "7. **Manufacturing Quality Control** üí∞ $15M+ Defect Reduction\n",
    "   - **Objective**: Detect novel defect patterns in product assembly (automotive, aerospace)\n",
    "   - **Features**: Vision system features (shape, texture, color), dimensional measurements, torque sensors\n",
    "   - **Success Metric**: 99% defect detection, <2% false alarm rate\n",
    "   - **Implementation**: OCSVM on known-good units, flag novelties for manual inspection\n",
    "   - **Business Value**: Reduce warranty claims, improve six-sigma quality\n",
    "\n",
    "8. **Financial Transaction Fraud Novelty Detection** üí∞ $200M+ Fraud Loss Prevention\n",
    "   - **Objective**: Detect novel fraud patterns not covered by rule-based systems\n",
    "   - **Features**: Transaction amount, merchant category, location, time, user device fingerprint\n",
    "   - **Success Metric**: 75% novel fraud detection (complement existing rules), 0.5% FPR\n",
    "   - **Implementation**: OCSVM per user profile, train on verified legitimate transactions\n",
    "   - **Business Value**: Catch emerging fraud tactics before rules updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bfff63",
   "metadata": {},
   "source": [
    "## üîç Key Takeaways\n",
    "\n",
    "### ‚úÖ When to Use One-Class SVM\n",
    "- **Novelty detection**: Training data = only normal class, need to detect unseen anomalies\n",
    "- **Non-linear boundaries**: Complex data distributions requiring flexible decision boundary\n",
    "- **Small to medium datasets** (<100K samples): QP solver scales O(n¬≤) to O(n¬≥)\n",
    "- **Need interpretable boundary**: Support vectors show which training points define normal region\n",
    "- **Robust to outliers in training**: nu parameter allows some contamination\n",
    "\n",
    "### ‚ùå Limitations\n",
    "- **Slow training**: Quadratic programming expensive for large n (use SGDOneClassSVM for >100K samples)\n",
    "- **Kernel/nu selection**: Requires careful hyperparameter tuning (more sensitive than Isolation Forest)\n",
    "- **Memory intensive**: Kernel matrix K(n√ón) for training, support vectors for inference\n",
    "- **Assumes single normal cluster**: Struggles with multi-modal normal distributions (use multiple OCSVMs)\n",
    "- **No probabilistic output**: Decision function f(x) not calibrated probability\n",
    "\n",
    "### üîß Hyperparameter Tuning Guidelines\n",
    "1. **nu** (ŒΩ, most critical):\n",
    "   - Interpretation: Upper bound on training errors, lower bound on support vectors\n",
    "   - Start with expected outlier rate (e.g., 0.05 for 5% contamination)\n",
    "   - Too small: Overfit to training data (rejects too many test samples)\n",
    "   - Too large: Loose boundary (misses anomalies)\n",
    "   - Typical range: 0.01 to 0.2\n",
    "\n",
    "2. **kernel**:\n",
    "   - **RBF**: Default choice (smooth, flexible, works for most data)\n",
    "   - **Linear**: Fast, interpretable (if data linearly separable)\n",
    "   - **Polynomial**: Good for polynomial feature relationships (tune degree)\n",
    "   - Try RBF first, fallback to linear if too slow\n",
    "\n",
    "3. **gamma** (Œ≥, for RBF/poly kernels):\n",
    "   - Controls kernel width (inverse of bandwidth)\n",
    "   - Larger Œ≥: Tighter fit to training (risk overfitting)\n",
    "   - Smaller Œ≥: Smoother boundary (risk underfitting)\n",
    "   - Use 'scale' (automatic: 1/(n_features √ó var(X))) as starting point\n",
    "   - Tune via cross-validation on contaminated validation set\n",
    "\n",
    "4. **Preprocessing**:\n",
    "   - **Always standardize** (StandardScaler) for SVM (distance-based)\n",
    "   - Consider PCA for high dimensions (>50 features) to reduce kernel computation\n",
    "\n",
    "### üî¨ One-Class SVM vs Isolation Forest\n",
    "\n",
    "| Aspect | One-Class SVM | Isolation Forest |\n",
    "|--------|---------------|------------------|\n",
    "| **Approach** | Boundary-based (hyperplane) | Isolation-based (path length) |\n",
    "| **Speed** | O(n¬≤) training (slow for large n) | O(n log n) (much faster) |\n",
    "| **Boundary** | Smooth, kernel-defined | Irregular, tree-based |\n",
    "| **Hyperparameters** | nu, kernel, gamma (sensitive) | contamination, n_estimators (robust) |\n",
    "| **High dimensions** | Kernel trick helps, but memory costly | No distance metric, scales well |\n",
    "| **Novelty detection** | ‚úÖ Excellent (train on normal only) | ‚ö†Ô∏è Assumes some anomalies in training |\n",
    "| **Interpretability** | Support vectors show boundary | Path lengths show isolation ease |\n",
    "| **Best for** | Novelty, non-linear, interpretable | Large data, high-D, speed-critical |\n",
    "\n",
    "### üöÄ Next Steps\n",
    "1. **Notebook 038**: AutoEncoders for deep learning-based anomaly detection\n",
    "2. **Advanced topic**: Ensemble (OCSVM + Isolation Forest) for robust detection\n",
    "3. **Kernel engineering**: Custom kernels for domain-specific similarity (e.g., graph kernels)\n",
    "4. **Online learning**: Incremental OCSVM for streaming data (approximate kernel methods)\n",
    "\n",
    "### üìö Further Reading\n",
    "- Sch√∂lkopf et al. (2001): \"Estimating the Support of a High-Dimensional Distribution\" - Original OCSVM paper\n",
    "- Tax & Duin (2004): \"Support Vector Data Description\" (SVDD) - Alternative formulation\n",
    "- Kernel methods for anomaly detection in time series\n",
    "- SGDOneClassSVM for large-scale data (linear kernel only)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
