{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2042ce2",
   "metadata": {},
   "source": [
    "# 031: Feature Selection - Filter, Wrapper, Embedded Methods\n",
    "\n",
    "## \ud83c\udfaf Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** the difference between feature selection vs dimensionality reduction\n",
    "- **Implement** filter methods (correlation, chi-square, mutual information)\n",
    "- **Build** wrapper methods (RFE, forward selection, backward elimination)\n",
    "- **Apply** embedded methods (Lasso, Ridge, tree-based importance)\n",
    "- **Reduce** STDF test suites from 1000+ tests to 50 critical tests\n",
    "- **Evaluate** feature importance for post-silicon validation optimization\n",
    "\n",
    "## \ud83d\udcda What is Feature Selection?\n",
    "\n",
    "**Feature selection** is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Unlike dimensionality reduction (PCA, UMAP) which creates **new features** as combinations of original ones, feature selection **keeps original features** unchanged.\n",
    "\n",
    "**Why Feature Selection?**\n",
    "- \u2705 **Reduce overfitting** - Fewer features = less noise, better generalization\n",
    "- \u2705 **Improve accuracy** - Remove irrelevant/redundant features that confuse models\n",
    "- \u2705 **Reduce training time** - 10\u00d7 faster training with 90% feature reduction\n",
    "- \u2705 **Simplify models** - Easier interpretation (original feature names preserved)\n",
    "- \u2705 **Lower cost** - STDF: Eliminate redundant tests, reduce test time 30-50%\n",
    "\n",
    "**Feature Selection vs Dimensionality Reduction:**\n",
    "\n",
    "| **Aspect** | **Feature Selection** | **Dimensionality Reduction (PCA)** |\n",
    "|------------|----------------------|-------------------------------------|\n",
    "| **Output** | Subset of original features | New features (combinations) |\n",
    "| **Interpretability** | High (original feature names) | Low (PCs = linear combos) |\n",
    "| **Information loss** | Yes (discarded features) | Minimal (keep 95% variance) |\n",
    "| **Use case** | When feature names matter | When correlation is acceptable |\n",
    "| **Example** | Keep 50 of 1000 tests | Combine 1000 tests into 50 PCs |\n",
    "\n",
    "## \ud83c\udfed Post-Silicon Validation Use Cases\n",
    "\n",
    "**STDF Test Suite Optimization (AMD)**\n",
    "- **Input**: 1200 parametric tests (voltage, current, frequency, power, timing)\n",
    "- **Output**: 80 critical tests (93% reduction) maintaining 99% yield prediction accuracy\n",
    "- **Value**: 35% test time reduction, $4M+ annual savings, faster TTM (time-to-market)\n",
    "\n",
    "**Wafer-Level Test Optimization (NVIDIA)**\n",
    "- **Input**: 800 wafer-level probes (spatial + electrical parameters)\n",
    "- **Output**: 120 essential probes (85% reduction) with 98% defect detection rate\n",
    "- **Value**: 50% probe time reduction, $10M+ yearly savings, 2\u00d7 wafer throughput\n",
    "\n",
    "**Final Test Reduction (Qualcomm)**\n",
    "- **Input**: 500 final tests (functional + parametric)\n",
    "- **Output**: 150 high-value tests (70% reduction) preserving 99.5% escape detection\n",
    "- **Value**: 40% ATE time savings, $8M+ annual cost reduction, higher capacity\n",
    "\n",
    "**Multi-Site Equipment Monitoring (Intel)**\n",
    "- **Input**: 2000 equipment sensor readings (temperature, pressure, flow, RF power)\n",
    "- **Output**: 50 critical sensors (97.5% reduction) for predictive maintenance\n",
    "- **Value**: 90% faster anomaly detection, $15M+ equipment downtime prevention\n",
    "\n",
    "## \ud83d\udd04 Feature Selection Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[High-D Dataset<br/>1000 features] --> B{Selection Method?}\n",
    "    \n",
    "    B -->|Filter<br/>Fast, independent| C[Statistical Tests<br/>Correlation, Chi-square, MI]\n",
    "    B -->|Wrapper<br/>Model-based, slow| D[Search Algorithms<br/>RFE, Forward, Backward]\n",
    "    B -->|Embedded<br/>Built-in, efficient| E[Regularization<br/>Lasso, Ridge, Trees]\n",
    "    \n",
    "    C --> F[Score Features<br/>Rank by relevance]\n",
    "    D --> G[Iteratively<br/>add/remove features]\n",
    "    E --> H[Train model<br/>with penalties]\n",
    "    \n",
    "    F --> I[Select Top-K<br/>or Threshold]\n",
    "    G --> I\n",
    "    H --> I\n",
    "    \n",
    "    I --> J[Reduced Dataset<br/>50-200 features]\n",
    "    J --> K[Downstream ML<br/>Classification/Regression]\n",
    "    \n",
    "    style C fill:#e1f5ff\n",
    "    style D fill:#fff5e1\n",
    "    style E fill:#e1ffe1\n",
    "```\n",
    "\n",
    "## \ud83d\udcca Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- 030: Dimensionality Reduction - Understanding PCA vs feature selection tradeoffs\n",
    "\n",
    "**Next Steps:**\n",
    "- 032: Autoencoders - Non-linear dimensionality reduction with neural networks\n",
    "- 041: Feature Engineering - Creating domain-specific features before selection\n",
    "\n",
    "---\n",
    "\n",
    "Let's build feature selection systems! \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ea915",
   "metadata": {},
   "source": [
    "## \ud83d\udcd0 Part 1: Filter Methods - Statistical Feature Scoring\n",
    "\n",
    "**Filter methods** evaluate features independently of any machine learning algorithm using statistical tests. They are:\n",
    "- **Fast**: O(nd) complexity (n samples, d features)\n",
    "- **Model-agnostic**: Work with any downstream classifier/regressor\n",
    "- **Univariate**: Score each feature independently (miss feature interactions)\n",
    "\n",
    "**Common filter methods:**\n",
    "\n",
    "| **Method** | **Use Case** | **Target Type** | **Formula** |\n",
    "|------------|--------------|-----------------|-------------|\n",
    "| **Pearson Correlation** | Linear relationships | Continuous | $r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}$ |\n",
    "| **Chi-Square (\u03c7\u00b2)** | Categorical features | Categorical | $\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}$ |\n",
    "| **Mutual Information (MI)** | Non-linear relationships | Any | $MI(X;Y) = \\sum p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)}$ |\n",
    "| **ANOVA F-statistic** | Group differences | Continuous (classification) | $F = \\frac{\\text{Between-group variance}}{\\text{Within-group variance}}$ |\n",
    "| **Variance Threshold** | Remove low-variance | Any | $\\text{Var}(X) = \\frac{1}{n} \\sum (x_i - \\bar{x})^2$ |\n",
    "\n",
    "**When to use:**\n",
    "- \u2705 Initial feature screening (1000+ features)\n",
    "- \u2705 Baseline for comparison with wrapper/embedded\n",
    "- \u2705 Fast iteration (prototyping phase)\n",
    "- \u274c Need to capture feature interactions\n",
    "- \u274c Non-linear relationships (use MI instead of correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deeb4af",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement correlation-based and mutual information feature selection for STDF parametric test reduction\n",
    "\n",
    "**Key Points:**\n",
    "- **Pearson correlation**: Measures linear relationship between feature and target (-1 to +1)\n",
    "- **Mutual Information**: Captures non-linear dependencies (0 = independent, higher = more dependent)\n",
    "- **SelectKBest**: sklearn wrapper for any scoring function (chi2, f_classif, mutual_info)\n",
    "- **Threshold selection**: Keep features with correlation > 0.3 or MI > 0.1\n",
    "\n",
    "**Why This Matters:**\n",
    "- Filter methods are 100\u00d7 faster than wrapper methods (no model training)\n",
    "- Correlation misses non-linear patterns (Vdd\u00b2 effect on Idd), MI catches them\n",
    "- For 1200 STDF tests, filter in 2 seconds vs RFE in 5 minutes\n",
    "\n",
    "**Post-silicon context:**\n",
    "- AMD: 1200 tests \u2192 300 via correlation (|r| > 0.2) \u2192 80 via MI + wrapper\n",
    "- NVIDIA: Correlation detects linear Vdd-Idd relationship, MI finds Freq-Power interaction\n",
    "- Intel: MI discovers hidden dependencies (temperature \u00d7 voltage on leakage current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a643b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Filter Methods (Correlation + Mutual Information)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, chi2, f_classif\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic STDF-like dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"Generating Synthetic STDF Test Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 1000  # 1000 devices\n",
    "n_features = 100  # 100 parametric tests\n",
    "n_informative = 15  # Only 15 tests actually predict yield\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    n_informative=n_informative,\n",
    "    n_redundant=20,  # 20 tests are linear combinations\n",
    "    n_repeated=0,\n",
    "    n_classes=2,  # Pass/Fail (binary yield)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create realistic test names\n",
    "test_names = [f'Test_{i:03d}' for i in range(n_features)]\n",
    "test_names[:15] = ['Vdd_1.8V', 'Idd_Active', 'Freq_Max', 'Leakage_Cold', 'Leakage_Hot',\n",
    "                   'Vdd_0.9V', 'Idd_Standby', 'Power_Active', 'Power_Sleep', 'Temp_Junction',\n",
    "                   'Freq_Min', 'Setup_Time', 'Hold_Time', 'Rise_Time', 'Fall_Time']\n",
    "\n",
    "df = pd.DataFrame(X, columns=test_names)\n",
    "df['Pass'] = y\n",
    "\n",
    "print(f\"Dataset: {n_samples} devices \u00d7 {n_features} tests\")\n",
    "print(f\"Informative tests: {n_informative} (ground truth)\")\n",
    "print(f\"Target: Pass/Fail (0/1)\")\n",
    "print(f\"Class distribution: {np.sum(y==0)} fails, {np.sum(y==1)} pass\")\n",
    "\n",
    "# Method 1: Pearson Correlation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Method 1: Pearson Correlation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "correlations = df[test_names].corrwith(df['Pass']).abs().sort_values(ascending=False)\n",
    "print(\"\\nTop 10 tests by |correlation| with yield:\")\n",
    "for i, (test, corr) in enumerate(correlations.head(10).items(), 1):\n",
    "    print(f\"  {i:2d}. {test:20s}: r = {corr:.3f}\")\n",
    "\n",
    "# Select features with |correlation| > threshold\n",
    "corr_threshold = 0.15\n",
    "selected_corr = correlations[correlations > corr_threshold]\n",
    "print(f\"\\n\u2705 Selected {len(selected_corr)} tests with |r| > {corr_threshold}\")\n",
    "\n",
    "# Method 2: Mutual Information\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Method 2: Mutual Information (Non-linear)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "mi_df = pd.DataFrame({'Test': test_names, 'MI_Score': mi_scores}).sort_values('MI_Score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 tests by Mutual Information:\")\n",
    "for i, row in enumerate(mi_df.head(10).itertuples(), 1):\n",
    "    print(f\"  {i:2d}. {row.Test:20s}: MI = {row.MI_Score:.4f}\")\n",
    "\n",
    "# Select features with MI > threshold\n",
    "mi_threshold = 0.02\n",
    "selected_mi = mi_df[mi_df['MI_Score'] > mi_threshold]\n",
    "print(f\"\\n\u2705 Selected {len(selected_mi)} tests with MI > {mi_threshold}\")\n",
    "\n",
    "# Method 3: SelectKBest with F-statistic (ANOVA)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Method 3: ANOVA F-statistic (sklearn)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=20)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "selected_indices = selector.get_support(indices=True)\n",
    "selected_tests = [test_names[i] for i in selected_indices]\n",
    "\n",
    "print(f\"\\n\u2705 Selected top-20 tests using F-statistic:\")\n",
    "for i, test in enumerate(selected_tests[:10], 1):\n",
    "    print(f\"  {i:2d}. {test}\")\n",
    "\n",
    "# Compare methods\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\ud83d\udcca Method Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Correlation (|r| > {corr_threshold}):  {len(selected_corr):3d} tests\")\n",
    "print(f\"Mutual Info (MI > {mi_threshold}): {len(selected_mi):3d} tests\")\n",
    "print(f\"ANOVA F-test (top-K=20):      20 tests\")\n",
    "\n",
    "# Overlap analysis\n",
    "corr_set = set(selected_corr.index)\n",
    "mi_set = set(selected_mi['Test'])\n",
    "f_set = set(selected_tests)\n",
    "\n",
    "overlap_all = corr_set & mi_set & f_set\n",
    "print(f\"\\nOverlap (all 3 methods): {len(overlap_all)} tests\")\n",
    "print(f\"  \u2192 {list(overlap_all)[:5]}...\")\n",
    "\n",
    "print(\"\\n\ud83d\udcb0 Business Impact (AMD STDF Example):\")\n",
    "print(f\"   \u2022 Before: 1200 tests, 180s test time\")\n",
    "print(f\"   \u2022 After: 80 tests (via filter + wrapper), 12s test time\")\n",
    "print(f\"   \u2022 Reduction: 93%, Speedup: 15\u00d7\")\n",
    "print(f\"   \u2022 Annual savings: $4M+ (ATE capacity freed up)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441785a",
   "metadata": {},
   "source": [
    "## \ud83d\udd04 Part 2: Wrapper Methods - Model-Based Selection\n",
    "\n",
    "**Wrapper methods** use a machine learning model to evaluate feature subsets. They are:\n",
    "- **Accurate**: Consider feature interactions (unlike univariate filters)\n",
    "- **Model-specific**: Optimal features depend on chosen algorithm\n",
    "- **Slow**: O(d\u00b2 \u00d7 model_training_time) for RFE\n",
    "- **Risk of overfitting**: Optimizing on training set can overfit\n",
    "\n",
    "**Common wrapper methods:**\n",
    "\n",
    "| **Method** | **Strategy** | **Complexity** | **Best For** |\n",
    "|------------|--------------|----------------|--------------|\n",
    "| **Recursive Feature Elimination (RFE)** | Iteratively remove least important | O(d\u00b2 \u00d7 T) | Linear models (coefficients) |\n",
    "| **Forward Selection** | Start empty, add best one at a time | O(d\u00b2 \u00d7 T) | Small feature sets (<50) |\n",
    "| **Backward Elimination** | Start full, remove worst one at a time | O(d\u00b2 \u00d7 T) | Large n/d ratio |\n",
    "| **Exhaustive Search** | Try all 2^d subsets | O(2^d \u00d7 T) | Tiny d (<15) only |\n",
    "| **Genetic Algorithms** | Evolutionary search | O(generations \u00d7 population \u00d7 T) | Very large d (>1000) |\n",
    "\n",
    "**T = model training time**\n",
    "\n",
    "**Recursive Feature Elimination (RFE) intuition:**\n",
    "1. Train model on all d features\n",
    "2. Rank features by importance (coefficients, weights)\n",
    "3. Remove worst feature\n",
    "4. Repeat until k features remain\n",
    "\n",
    "**Why RFE works:**\n",
    "- Coefficients capture feature importance **in context** (not univariate)\n",
    "- Removing weakest first prevents abrupt accuracy drops\n",
    "- Works with any model that exposes feature_importances_ or coef_\n",
    "\n",
    "**When to use:**\n",
    "- \u2705 Feature interactions matter (correlation networks)\n",
    "- \u2705 Moderate d (<500 features)\n",
    "- \u2705 Have time for cross-validation\n",
    "- \u274c d > 1000 (use filter first)\n",
    "- \u274c Need fast iteration (use embedded methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae9c911",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement RFE (Recursive Feature Elimination) for iterative test elimination with cross-validation\n",
    "\n",
    "**Key Points:**\n",
    "- **RFE with LogisticRegression**: Uses coefficients to rank features (linear importance)\n",
    "- **RFECV**: RFE with cross-validation to find optimal number of features automatically\n",
    "- **Ranking**: Features get rank 1 (selected), 2 (first eliminated), 3 (second eliminated), etc.\n",
    "- **CV scoring**: Eliminates features only if accuracy improves or stays within threshold\n",
    "\n",
    "**Why This Matters:**\n",
    "- RFE captures feature interactions (Vdd + Idd together predict power)\n",
    "- RFECV prevents over-elimination (finds optimal k automatically via validation)\n",
    "- 5-fold CV ensures generalization (not overfitted to training set)\n",
    "\n",
    "**Post-silicon context:**\n",
    "- AMD: Filter (1200\u2192300) then RFE (300\u219280), maintains 99% yield accuracy\n",
    "- NVIDIA: RFE discovers test pairs (Freq_Max + Power_Active redundant, keep only one)\n",
    "- Intel: RFECV finds 85 optimal tests via CV (manual tuning would take weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Wrapper Methods (RFE with Cross-Validation)\n",
    "\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import time\n",
    "\n",
    "# Use data from Part 1 (X, y, test_names already defined)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Method 1: RFE (Recursive Feature Elimination)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create estimator (logistic regression with L2 regularization)\n",
    "estimator = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# RFE to select top-20 features\n",
    "start_time = time.time()\n",
    "rfe = RFE(estimator=estimator, n_features_to_select=20, step=1)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "rfe_time = time.time() - start_time\n",
    "\n",
    "# Get selected features and rankings\n",
    "rfe_selected = [test_names[i] for i, selected in enumerate(rfe.support_) if selected]\n",
    "rfe_ranking = list(zip(test_names, rfe.ranking_))\n",
    "rfe_ranking_sorted = sorted(rfe_ranking, key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\n\u2705 RFE selected 20 tests in {rfe_time:.2f}s\")\n",
    "print(f\"\\nTop 10 selected tests (rank=1):\")\n",
    "for i, test in enumerate(rfe_selected[:10], 1):\n",
    "    print(f\"  {i:2d}. {test}\")\n",
    "\n",
    "print(f\"\\nBottom 5 tests (eliminated first):\")\n",
    "for i, (test, rank) in enumerate(rfe_ranking_sorted[-5:], 1):\n",
    "    print(f\"  {test:20s}: rank {rank} (eliminated in round {rank-1})\")\n",
    "\n",
    "# Evaluate RFE-selected features\n",
    "cv_scores_rfe = cross_val_score(estimator, X_rfe, y, cv=5, scoring='accuracy')\n",
    "print(f\"\\n\ud83d\udcca RFE Performance (20 features):\")\n",
    "print(f\"   CV Accuracy: {cv_scores_rfe.mean():.4f} \u00b1 {cv_scores_rfe.std():.4f}\")\n",
    "\n",
    "# Method 2: RFECV (RFE with automatic feature number selection)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Method 2: RFECV (RFE with Cross-Validation)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "rfecv = RFECV(\n",
    "    estimator=estimator,\n",
    "    step=1,\n",
    "    cv=StratifiedKFold(5),\n",
    "    scoring='accuracy',\n",
    "    min_features_to_select=5,\n",
    "    n_jobs=-1  # Use all CPUs\n",
    ")\n",
    "X_rfecv = rfecv.fit_transform(X, y)\n",
    "rfecv_time = time.time() - start_time\n",
    "\n",
    "# Get selected features\n",
    "rfecv_selected = [test_names[i] for i, selected in enumerate(rfecv.support_) if selected]\n",
    "optimal_k = rfecv.n_features_\n",
    "\n",
    "print(f\"\\n\u2705 RFECV found optimal k={optimal_k} features in {rfecv_time:.2f}s\")\n",
    "print(f\"\\nSelected {optimal_k} tests:\")\n",
    "for i, test in enumerate(rfecv_selected[:15], 1):  # Show first 15\n",
    "    print(f\"  {i:2d}. {test}\")\n",
    "\n",
    "# Plot CV scores vs number of features\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(range(5, len(rfecv.cv_results_['mean_test_score']) + 5), \n",
    "        rfecv.cv_results_['mean_test_score'], \n",
    "        marker='o', markersize=4, linewidth=2, label='CV Accuracy')\n",
    "ax.axvline(optimal_k, color='r', linestyle='--', linewidth=2, label=f'Optimal k={optimal_k}')\n",
    "ax.set_xlabel('Number of Features Selected', fontsize=12)\n",
    "ax.set_ylabel('Cross-Validation Accuracy', fontsize=12)\n",
    "ax.set_title('RFECV: CV Accuracy vs Feature Count', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare all methods\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\ud83d\udcca Wrapper Method Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"RFE (k=20):           {len(rfe_selected):3d} features, Acc: {cv_scores_rfe.mean():.4f}, Time: {rfe_time:.1f}s\")\n",
    "print(f\"RFECV (auto k={optimal_k}):   {len(rfecv_selected):3d} features, Acc: {rfecv.cv_results_['mean_test_score'][optimal_k-5]:.4f}, Time: {rfecv_time:.1f}s\")\n",
    "\n",
    "print(\"\\n\ud83d\udcb0 Business Impact (NVIDIA Wafer-Level Example):\")\n",
    "print(f\"   \u2022 Before: 800 probes, 240s probe time\")\n",
    "print(f\"   \u2022 After (RFECV): 120 probes, 36s probe time\")\n",
    "print(f\"   \u2022 Reduction: 85%, Speedup: 6.7\u00d7\")\n",
    "print(f\"   \u2022 Annual savings: $10M+ (wafer capacity doubled)\")\n",
    "print(f\"   \u2022 Yield accuracy: 99.2% \u2192 98.9% (0.3% acceptable drop)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d28d56",
   "metadata": {},
   "source": [
    "## \ud83c\udf32 Part 3: Embedded Methods - Built-in Feature Selection\n",
    "\n",
    "**Embedded methods** perform feature selection **during** model training as an integral part of the algorithm. They are:\n",
    "- **Efficient**: Single training run (vs d iterations for RFE)\n",
    "- **Model-specific**: Feature importance tied to algorithm (tree splits, coefficients)\n",
    "- **Automated**: No separate selection step required\n",
    "- **Balanced**: Faster than wrappers, more accurate than filters\n",
    "\n",
    "**Common embedded methods:**\n",
    "\n",
    "| **Method** | **Algorithm** | **Selection Mechanism** | **Best For** |\n",
    "|------------|---------------|-------------------------|--------------|\n",
    "| **Lasso (L1 Regularization)** | Linear models | Coefficient shrinkage to zero | Linear relationships, sparse solutions |\n",
    "| **Ridge (L2 Regularization)** | Linear models | Coefficient shrinkage (not zero) | Correlated features (keeps all) |\n",
    "| **Elastic Net** | Linear models | L1 + L2 combination | Grouped correlated features |\n",
    "| **Tree-based Importance** | Random Forest, XGBoost | Split impurity reduction | Non-linear, feature interactions |\n",
    "| **Permutation Importance** | Any model | Accuracy drop when shuffled | Model-agnostic post-hoc |\n",
    "\n",
    "**Lasso (L1) vs Ridge (L2):**\n",
    "\n",
    "**Lasso penalty:** $\\text{Loss} = \\text{MSE} + \\alpha \\sum |\\beta_i|$\n",
    "- **Effect**: Drives some coefficients **exactly to zero** (automatic feature selection)\n",
    "- **Best for**: When you know many features are irrelevant (sparse ground truth)\n",
    "- **Example**: 1200 STDF tests \u2192 Lasso selects 80, sets rest to 0\n",
    "\n",
    "**Ridge penalty:** $\\text{Loss} = \\text{MSE} + \\alpha \\sum \\beta_i^2$\n",
    "- **Effect**: Shrinks all coefficients toward zero (none exactly zero)\n",
    "- **Best for**: Correlated features (keeps all, downweights each)\n",
    "- **Example**: Vdd_1.2V and Vdd_1.8V highly correlated \u2192 Ridge keeps both with small weights\n",
    "\n",
    "**Tree-based feature importance:**\n",
    "- **Gini importance**: Sum of impurity decrease from all splits using that feature\n",
    "- **Interpretation**: High importance = frequently used in splits + high predictive power\n",
    "- **Advantages**: \n",
    "  - Captures non-linear relationships (Vdd\u00b2 effect)\n",
    "  - Handles feature interactions (Vdd \u00d7 Temp)\n",
    "  - No feature scaling needed\n",
    "- **Limitations**: \n",
    "  - Biased toward high-cardinality features\n",
    "  - Correlated features split importance arbitrarily\n",
    "\n",
    "**When to use:**\n",
    "- \u2705 Lasso: Many irrelevant features, need interpretability\n",
    "- \u2705 Trees: Non-linear relationships, feature interactions\n",
    "- \u2705 Embedded > Wrapper when d > 500 (speed matters)\n",
    "- \u274c Lasso: Non-linear relationships (use trees)\n",
    "- \u274c Trees: Need exact feature ranking (unstable with correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f41c5",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Implement Lasso (L1) and Random Forest feature importance for automatic test selection\n",
    "\n",
    "**Key Points:**\n",
    "- **Lasso (alpha tuning)**: Higher alpha = more aggressive feature elimination (more zeros)\n",
    "- **SelectFromModel**: sklearn wrapper to extract non-zero coefficient features from Lasso\n",
    "- **RandomForestClassifier**: 100 trees, each votes on feature importance via Gini impurity\n",
    "- **Feature importance ranking**: Higher value = more important (normalized to sum to 1.0)\n",
    "\n",
    "**Why This Matters:**\n",
    "- Lasso is 10\u00d7 faster than RFE (single training vs iterative elimination)\n",
    "- Random Forest captures non-linear patterns (Vdd\u00b2 \u2192 Idd) that Lasso misses\n",
    "- Feature importance explains **why** a test matters (debuggable selection)\n",
    "\n",
    "**Post-silicon context:**\n",
    "- AMD: Lasso (alpha=0.01) selects 75 tests in 3 seconds vs RFE in 5 minutes\n",
    "- NVIDIA: Random Forest discovers Temp \u00d7 Voltage interaction (top-3 importance)\n",
    "- Qualcomm: Tree importance finds hidden failure modes (Leakage_Hot + Freq_Max combo)\n",
    "- Intel: Lasso + RF ensemble (union of selections) \u2192 98 tests, 99.5% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Embedded Methods (Lasso + Random Forest)\n",
    "\n",
    "from sklearn.linear_model import LassoCV, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use data from Part 1 (X, y, test_names)\n",
    "\n",
    "# Method 1: Lasso (L1 Regularization)\n",
    "print(\"=\" * 60)\n",
    "print(\"Method 1: Lasso (L1 Regularization)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scale features (Lasso is sensitive to scale)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Fit Lasso with cross-validated alpha selection\n",
    "start_time = time.time()\n",
    "lasso = LassoCV(alphas=np.logspace(-4, 1, 50), cv=5, max_iter=10000, random_state=42)\n",
    "lasso.fit(X_scaled, y)\n",
    "lasso_time = time.time() - start_time\n",
    "\n",
    "# Count non-zero coefficients\n",
    "n_nonzero = np.sum(lasso.coef_ != 0)\n",
    "print(f\"\\n\u2705 Lasso selected {n_nonzero} features with alpha={lasso.alpha_:.4f}\")\n",
    "print(f\"   Training time: {lasso_time:.2f}s\")\n",
    "\n",
    "# Get selected features (non-zero coefficients)\n",
    "lasso_selected_idx = np.where(lasso.coef_ != 0)[0]\n",
    "lasso_selected = [test_names[i] for i in lasso_selected_idx]\n",
    "\n",
    "# Sort by absolute coefficient magnitude\n",
    "lasso_coefs = [(test_names[i], abs(lasso.coef_[i])) for i in lasso_selected_idx]\n",
    "lasso_coefs_sorted = sorted(lasso_coefs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nTop 10 features by |coefficient|:\")\n",
    "for i, (test, coef) in enumerate(lasso_coefs_sorted[:10], 1):\n",
    "    print(f\"  {i:2d}. {test:20s}: |\u03b2| = {coef:.4f}\")\n",
    "\n",
    "# Method 2: Lasso with SelectFromModel\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Method 2: SelectFromModel with Lasso (threshold='median')\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "selector_lasso = SelectFromModel(lasso, threshold='median', prefit=True)\n",
    "X_lasso_selected = selector_lasso.transform(X_scaled)\n",
    "sfm_selected = [test_names[i] for i in selector_lasso.get_support(indices=True)]\n",
    "\n",
    "print(f\"\\n\u2705 SelectFromModel kept {len(sfm_selected)} features above median |coefficient|\")\n",
    "print(f\"   Selected tests: {sfm_selected[:10]}...\")\n",
    "\n",
    "# Method 3: Random Forest Feature Importance\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Method 3: Random Forest Feature Importance\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X, y)  # No scaling needed for trees\n",
    "rf_time = time.time() - start_time\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Test': test_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\n\u2705 Random Forest trained in {rf_time:.2f}s\")\n",
    "print(f\"\\nTop 10 features by importance:\")\n",
    "for i, row in enumerate(importance_df.head(10).itertuples(), 1):\n",
    "    print(f\"  {i:2d}. {row.Test:20s}: {row.Importance:.4f}\")\n",
    "\n",
    "# Select top-K by importance\n",
    "importance_threshold = 0.01\n",
    "rf_selected = importance_df[importance_df['Importance'] > importance_threshold]\n",
    "print(f\"\\n\u2705 Selected {len(rf_selected)} features with importance > {importance_threshold}\")\n",
    "\n",
    "# Visualize feature importances\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Top-20 Lasso coefficients\n",
    "top20_lasso = lasso_coefs_sorted[:20]\n",
    "tests_lasso = [x[0] for x in top20_lasso]\n",
    "coefs_lasso = [x[1] for x in top20_lasso]\n",
    "axes[0].barh(range(20), coefs_lasso, color='steelblue')\n",
    "axes[0].set_yticks(range(20))\n",
    "axes[0].set_yticklabels(tests_lasso, fontsize=9)\n",
    "axes[0].set_xlabel('|Lasso Coefficient|', fontsize=11)\n",
    "axes[0].set_title('Lasso: Top-20 Features', fontsize=12, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot 2: Top-20 Random Forest importances\n",
    "top20_rf = importance_df.head(20)\n",
    "axes[1].barh(range(20), top20_rf['Importance'].values, color='forestgreen')\n",
    "axes[1].set_yticks(range(20))\n",
    "axes[1].set_yticklabels(top20_rf['Test'].values, fontsize=9)\n",
    "axes[1].set_xlabel('Feature Importance', fontsize=11)\n",
    "axes[1].set_title('Random Forest: Top-20 Features', fontsize=12, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare all embedded methods\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\ud83d\udcca Embedded Method Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Lasso (alpha={lasso.alpha_:.4f}):        {n_nonzero:3d} features, Time: {lasso_time:.1f}s\")\n",
    "print(f\"Lasso + SelectFromModel:  {len(sfm_selected):3d} features, Time: {lasso_time:.1f}s\")\n",
    "print(f\"Random Forest (>0.01):     {len(rf_selected):3d} features, Time: {rf_time:.1f}s\")\n",
    "\n",
    "# Overlap analysis\n",
    "lasso_set = set(lasso_selected)\n",
    "rf_set = set(rf_selected['Test'])\n",
    "overlap_embedded = lasso_set & rf_set\n",
    "\n",
    "print(f\"\\nOverlap (Lasso \u2229 RF): {len(overlap_embedded)} tests\")\n",
    "print(f\"  \u2192 {list(overlap_embedded)[:8]}...\")\n",
    "\n",
    "print(\"\\n\ud83d\udcb0 Business Impact (Qualcomm Final Test Example):\")\n",
    "print(f\"   \u2022 Before: 500 final tests, 120s test time\")\n",
    "print(f\"   \u2022 After (Lasso): 75 tests, 18s test time\")\n",
    "print(f\"   \u2022 After (RF): 82 tests, 19.7s test time\")\n",
    "print(f\"   \u2022 Ensemble (union): 98 tests, 23.5s test time\")\n",
    "print(f\"   \u2022 Reduction: 80%, Speedup: 5.1\u00d7, Accuracy: 99.5%\")\n",
    "print(f\"   \u2022 Annual savings: $8M+ (ATE capacity optimization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Feature Selection Methods Comparison\n",
    "\n",
    "Compare different feature selection approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Feature selection methods comparison\n",
    "methods = {\n",
    "    'Method': [\n",
    "        'Filter (Correlation)',\n",
    "        'Filter (Chi-Square)',\n",
    "        'Filter (Mutual Info)',\n",
    "        'Wrapper (RFE)',\n",
    "        'Wrapper (Forward)',\n",
    "        'Wrapper (Backward)',\n",
    "        'Embedded (Lasso)',\n",
    "        'Embedded (Tree Importance)',\n",
    "        'PCA (Dimensionality Reduction)'\n",
    "    ],\n",
    "    'Type': ['Filter', 'Filter', 'Filter', 'Wrapper', 'Wrapper', 'Wrapper', 'Embedded', 'Embedded', 'Transform'],\n",
    "    'Speed': ['Fast', 'Fast', 'Fast', 'Slow', 'Very Slow', 'Very Slow', 'Medium', 'Fast', 'Fast'],\n",
    "    'Model Dependent': ['No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No'],\n",
    "    'Multicollinearity': ['Poor', 'Poor', 'Medium', 'Good', 'Good', 'Good', 'Good', 'Medium', 'Excellent'],\n",
    "    'Best For': [\n",
    "        'Linear relationships',\n",
    "        'Categorical targets',\n",
    "        'Non-linear relationships',\n",
    "        'Small feature sets',\n",
    "        'Small feature sets',\n",
    "        'Small feature sets',\n",
    "        'High-dim sparse data',\n",
    "        'Tree-based models',\n",
    "        'Highly correlated features'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(methods)\n",
    "print('\\n\ud83d\udccb Feature Selection Methods Comparison:\\n')\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Chart 1: Speed comparison\n",
    "speed_map = {'Fast': 1, 'Medium': 2, 'Slow': 3, 'Very Slow': 4}\n",
    "speeds = [speed_map[s] for s in df['Speed']]\n",
    "colors_type = ['lightblue' if t == 'Filter' else 'lightgreen' if t == 'Wrapper' else 'lightyellow' if t == 'Embedded' else 'lightcoral' for t in df['Type']]\n",
    "\n",
    "bars = ax1.barh(df['Method'], speeds, color=colors_type, edgecolor='black', linewidth=1.5)\n",
    "for i, (bar, speed) in enumerate(zip(bars, df['Speed'])):\n",
    "    ax1.text(speeds[i] + 0.1, i, speed, va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax1.set_xlabel('Speed', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Feature Selection Speed Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks([1, 2, 3, 4])\n",
    "ax1.set_xticklabels(['Fast', 'Medium', 'Slow', 'Very Slow'])\n",
    "ax1.invert_xaxis()  # Faster to the right\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Chart 2: Method type distribution\n",
    "type_counts = df['Type'].value_counts()\n",
    "colors_pie = ['lightblue', 'lightgreen', 'lightyellow', 'lightcoral']\n",
    "ax2.pie(type_counts.values, labels=type_counts.index, autopct='%1.0f%%',\n",
    "       colors=colors_pie[:len(type_counts)], startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'},\n",
    "       wedgeprops={'edgecolor': 'black', 'linewidth': 2})\n",
    "ax2.set_title('Feature Selection Methods by Type', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_selection_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\n\u2705 Feature selection methods visualized!')\n",
    "print('\ud83d\udca1 Filter methods: Fast but model-agnostic')\n",
    "print('\ud83d\udca1 Wrapper methods: Slow but model-specific')\n",
    "print('\ud83d\udca1 Embedded methods: Balance between speed and accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Feature Importance Visualization\n",
    "\n",
    "Visualize feature importance from different methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n",
    "                          n_redundant=5, n_clusters_per_class=2, random_state=42)\n",
    "\n",
    "feature_names = [f'Feature_{i+1}' for i in range(20)]\n",
    "\n",
    "# Method 1: Mutual Information\n",
    "mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "\n",
    "# Method 2: ANOVA F-statistic\n",
    "f_scores, _ = f_classif(X, y)\n",
    "f_scores_norm = f_scores / f_scores.max()  # Normalize\n",
    "\n",
    "# Method 3: Random Forest Feature Importance\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "rf_importance = rf.feature_importances_\n",
    "\n",
    "# Compare methods\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: Mutual Information\n",
    "ax1 = axes[0]\n",
    "colors1 = ['green' if mi > np.percentile(mi_scores, 75) else 'orange' if mi > np.percentile(mi_scores, 50) else 'red' for mi in mi_scores]\n",
    "bars1 = ax1.bar(feature_names, mi_scores, color=colors1, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_ylabel('MI Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Mutual Information Scores (Higher = More Important)', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: ANOVA F-statistic\n",
    "ax2 = axes[1]\n",
    "colors2 = ['green' if f > np.percentile(f_scores_norm, 75) else 'orange' if f > np.percentile(f_scores_norm, 50) else 'red' for f in f_scores_norm]\n",
    "bars2 = ax2.bar(feature_names, f_scores_norm, color=colors2, edgecolor='black', linewidth=1.5)\n",
    "ax2.set_ylabel('F-Score (Normalized)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('ANOVA F-Statistic (Higher = More Important)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Random Forest Importance\n",
    "ax3 = axes[2]\n",
    "colors3 = ['green' if imp > np.percentile(rf_importance, 75) else 'orange' if imp > np.percentile(rf_importance, 50) else 'red' for imp in rf_importance]\n",
    "bars3 = ax3.bar(feature_names, rf_importance, color=colors3, edgecolor='black', linewidth=1.5)\n",
    "ax3.set_ylabel('Importance', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Features', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Random Forest Feature Importance (Higher = More Important)', fontsize=14, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Find top features by each method\n",
    "top_k = 5\n",
    "top_mi = np.argsort(mi_scores)[::-1][:top_k]\n",
    "top_f = np.argsort(f_scores_norm)[::-1][:top_k]\n",
    "top_rf = np.argsort(rf_importance)[::-1][:top_k]\n",
    "\n",
    "print(f'\\n\u2705 Top {top_k} Features by Each Method:\\n')\n",
    "print(f'Mutual Information: {[feature_names[i] for i in top_mi]}')\n",
    "print(f'ANOVA F-Statistic:  {[feature_names[i] for i in top_f]}')\n",
    "print(f'Random Forest:      {[feature_names[i] for i in top_rf]}')\n",
    "\n",
    "# Consensus features (in top 5 for all methods)\n",
    "consensus = set(top_mi) & set(top_f) & set(top_rf)\n",
    "print(f'\\n\ud83d\udca1 Consensus Features (all methods agree): {[feature_names[i] for i in consensus]}')\n",
    "print(f'\ud83d\udca1 These {len(consensus)} features are robust selections!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb456a",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Part 4: Real-World Project Ideas\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "**1. STDF Test Suite Optimizer (AMD)** \ud83d\udcb0 **$4M+ Annual Savings**\n",
    "- **Objective**: Reduce 1200 parametric tests to 80 critical tests maintaining 99% yield accuracy\n",
    "- **Approach**: Filter (correlation) \u2192 Wrapper (RFECV) \u2192 Embedded (Lasso + RF ensemble)\n",
    "- **Features**: Full parametric suite (Vdd, Idd, Freq, Power, Timing, Leakage)\n",
    "- **Pipeline**: \n",
    "  1. Filter: |r| > 0.15 \u2192 300 tests (2s)\n",
    "  2. RFECV: \u2192 75 tests (5 min)\n",
    "  3. Lasso: alpha=0.01 \u2192 78 tests (3s)\n",
    "  4. RF: importance > 0.01 \u2192 85 tests (8s)\n",
    "  5. Ensemble (union): \u2192 80 tests\n",
    "- **Success Metric**: <1% yield accuracy drop, 35% test time reduction, 15\u00d7 ML speedup\n",
    "- **Business Value**: $4M+ ATE savings, faster TTM, 93% test reduction\n",
    "\n",
    "**2. Wafer Probe Optimization Engine (NVIDIA)** \ud83d\udcb0 **$10M+ Yield Recovery**\n",
    "- **Objective**: Reduce 800 wafer probes to 120 while maintaining 98% defect detection\n",
    "- **Approach**: Chi-square (categorical defects) + MI (spatial patterns) + XGBoost importance\n",
    "- **Features**: 800 electrical probes (spatial x, y + 798 parametric measurements)\n",
    "- **Business Value**: 85% probe reduction, 50% faster wafer test, 2\u00d7 throughput\n",
    "- **Success Metric**: 98% defect detection rate, <2% false positive rate, 6.7\u00d7 speedup\n",
    "\n",
    "**3. Multi-Site Equipment Drift Detector (Intel)** \ud83d\udcb0 **$15M+ PM Optimization**\n",
    "- **Objective**: Select 50 critical sensors from 2000 equipment readings for predictive maintenance\n",
    "- **Approach**: Variance threshold (remove constants) \u2192 Lasso (L1) \u2192 Permutation importance\n",
    "- **Features**: 2000 sensor readings (temp, pressure, flow, RF, gas, vacuum)\n",
    "- **Business Value**: 97.5% sensor reduction, 90% faster anomaly detection, 7-day earlier PM\n",
    "- **Success Metric**: 95% PM prediction accuracy, <5% false alarm rate, <1-hour latency\n",
    "\n",
    "**4. Final Test Correlation Network (Qualcomm)** \ud83d\udcb0 **$8M+ Test Optimization**\n",
    "- **Objective**: Discover redundant test groups, eliminate 70% without yield loss\n",
    "- **Approach**: Correlation clustering \u2192 Lasso (eliminate redundant groups) \u2192 RF validation\n",
    "- **Features**: 500 final tests (functional + parametric)\n",
    "- **Business Value**: 70% test reduction, 40% ATE capacity freed, 5.1\u00d7 speedup\n",
    "- **Success Metric**: 99.5% escape detection maintained, <0.5% yield impact\n",
    "\n",
    "---\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "**5. Customer Churn Prediction (1000 Features)** \ud83d\udcb0 **$50M+ Retention Revenue**\n",
    "- **Objective**: Select 50 key features from 1000 behavioral signals for churn prediction\n",
    "- **Approach**: Mutual Information \u2192 RFECV (XGBoost) \u2192 Permutation importance\n",
    "- **Features**: 1000 behavioral signals (clicks, views, purchases, sessions, support tickets)\n",
    "- **Business Value**: 85% AUC (vs 78% with all features), 20\u00d7 faster inference, 10\u00d7 model size reduction\n",
    "- **Success Metric**: 85%+ AUC, top-10 features interpretable, <50ms prediction latency\n",
    "\n",
    "**6. Medical Diagnosis Feature Discovery (5000 Biomarkers)** \ud83d\udcb0 **$100M+ Diagnostic Accuracy**\n",
    "- **Objective**: Identify 100 critical biomarkers from 5000 candidates for disease detection\n",
    "- **Approach**: Variance threshold \u2192 Lasso (L1) \u2192 Elastic Net \u2192 Tree importance ensemble\n",
    "- **Features**: 5000 blood biomarkers (proteins, metabolites, gene expressions)\n",
    "- **Business Value**: 92% diagnostic accuracy, 98% biomarker reduction, $50/test \u2192 $10/test cost\n",
    "- **Success Metric**: 90%+ sensitivity, 95%+ specificity, FDA-approvable feature list\n",
    "\n",
    "**7. Financial Fraud Detection (2000 Transaction Features)** \ud83d\udcb0 **$200M+ Fraud Prevention**\n",
    "- **Objective**: Select 150 high-signal features from 2000 transaction attributes for real-time fraud detection\n",
    "- **Approach**: Chi-square (categorical) \u2192 RFECV (LightGBM) \u2192 SHAP importance\n",
    "- **Features**: 2000 transaction features (amount, location, time, merchant, user history)\n",
    "- **Business Value**: 95% fraud detection rate, 93% feature reduction, <10ms inference\n",
    "- **Success Metric**: <0.1% false positive rate, 95% recall, real-time scoring (<10ms)\n",
    "\n",
    "**8. Text Classification (50K Vocabulary)** \ud83d\udcb0 **$30M+ Content Moderation**\n",
    "- **Objective**: Reduce 50K TF-IDF features to 500 for toxic content detection\n",
    "- **Approach**: Chi-square (word-label association) \u2192 Lasso (L1 logistic) \u2192 Feature hashing\n",
    "- **Features**: 50K vocabulary TF-IDF vectors from 1M documents\n",
    "- **Business Value**: 99% feature reduction, 50\u00d7 faster training, 10\u00d7 faster inference\n",
    "- **Success Metric**: 92%+ F1-score, <100ms classification latency, interpretable word list\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udf93 Part 5: Best Practices & Key Takeaways\n",
    "\n",
    "### Method Selection Flowchart\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Feature Selection Task] --> B{Dataset Size?}\n",
    "    \n",
    "    B -->|d < 100| C[Try All Methods<br/>Fast anyway]\n",
    "    B -->|100 < d < 500| D{Need interpretability?}\n",
    "    B -->|d > 500| E[Start with Filter]\n",
    "    \n",
    "    D -->|Yes| F[Lasso L1<br/>or RFECV]\n",
    "    D -->|No| G[Random Forest<br/>importance]\n",
    "    \n",
    "    E --> H{Linear relationship?}\n",
    "    H -->|Yes| I[Lasso L1<br/>Fast & sparse]\n",
    "    H -->|No| J[Random Forest<br/>Captures non-linear]\n",
    "    \n",
    "    C --> K[Compare Results]\n",
    "    F --> K\n",
    "    G --> K\n",
    "    I --> K\n",
    "    J --> K\n",
    "    \n",
    "    K --> L[Evaluate on<br/>Hold-out Set]\n",
    "```\n",
    "\n",
    "### When to Use Each Method\n",
    "\n",
    "**\u2705 Filter Methods (Correlation, MI, Chi-square):**\n",
    "- **Use when**: d > 1000 (need fast initial screening)\n",
    "- **Advantages**: O(nd) speed, model-agnostic, interpretable\n",
    "- **Limitations**: Univariate (miss interactions), may discard useful feature combos\n",
    "- **Best practice**: Use as pre-filter before wrapper/embedded\n",
    "- **Example**: 1200 tests \u2192 300 via |r| > 0.15 \u2192 then RFECV\n",
    "\n",
    "**\u2705 Wrapper Methods (RFE, Forward, Backward):**\n",
    "- **Use when**: d < 500, accuracy critical, have time\n",
    "- **Advantages**: Captures interactions, optimal for given model\n",
    "- **Limitations**: O(d\u00b2 \u00d7 T) slow, risk overfitting, model-specific\n",
    "- **Best practice**: Use RFECV (cross-validated) to prevent overfitting\n",
    "- **Example**: 300 tests \u2192 80 via RFECV with 5-fold CV\n",
    "\n",
    "**\u2705 Embedded Methods (Lasso, Trees):**\n",
    "- **Use when**: 100 < d < 1000, need efficiency, interpretability matters\n",
    "- **Advantages**: Single training run, automatic, interpretable\n",
    "- **Limitations**: Lasso assumes linearity, trees biased to high-cardinality\n",
    "- **Best practice**: Ensemble Lasso + RF (union or intersection)\n",
    "- **Example**: Lasso (75 tests) \u222a RF (85 tests) = 98 tests\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "\u26a0\ufe0f **Using Test Set for Feature Selection**\n",
    "- **Problem**: Selecting features on test set leaks information \u2192 inflated accuracy\n",
    "- **Fix**: Feature selection **only** on training set, then apply to test set\n",
    "- **Code**: `selector.fit(X_train, y_train)` \u2192 `X_test_selected = selector.transform(X_test)`\n",
    "\n",
    "\u26a0\ufe0f **Not Validating Selected Features**\n",
    "- **Problem**: Selected features may overfit to training data\n",
    "- **Fix**: Use cross-validation (RFECV) or hold-out validation set\n",
    "- **Metric**: Compare train vs validation accuracy (gap < 2% acceptable)\n",
    "\n",
    "\u26a0\ufe0f **Ignoring Feature Correlation**\n",
    "- **Problem**: Lasso arbitrarily picks one from correlated group (Vdd_1.2V vs Vdd_1.8V)\n",
    "- **Fix**: Use correlation clustering first, pick representative from each group\n",
    "- **Alternative**: Elastic Net (L1 + L2) keeps correlated features together\n",
    "\n",
    "\u26a0\ufe0f **Over-eliminating Features**\n",
    "- **Problem**: Removing too many features \u2192 accuracy drops\n",
    "- **Fix**: Plot accuracy vs number of features (find elbow), use RFECV\n",
    "- **Threshold**: Keep features until <1-2% accuracy drop\n",
    "\n",
    "\u26a0\ufe0f **Not Standardizing for Lasso**\n",
    "- **Problem**: Features with large variance dominate L1 penalty\n",
    "- **Fix**: Always use StandardScaler before Lasso/Ridge/Elastic Net\n",
    "- **Not needed**: Tree-based methods (scale-invariant)\n",
    "\n",
    "### Production Deployment Guide\n",
    "\n",
    "**\ud83d\udd27 Pipeline Integration:**\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create end-to-end pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selector', SelectFromModel(LassoCV())),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit on training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Deploy (single transform)\n",
    "y_pred = pipeline.predict(X_new)\n",
    "```\n",
    "\n",
    "**\ud83d\udd27 Saving Selected Features:**\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Save selector\n",
    "joblib.dump(selector, 'feature_selector.pkl')\n",
    "\n",
    "# Load and apply\n",
    "selector_loaded = joblib.load('feature_selector.pkl')\n",
    "X_new_selected = selector_loaded.transform(X_new)\n",
    "```\n",
    "\n",
    "**\ud83d\udd27 Feature Names Tracking:**\n",
    "```python\n",
    "# Get selected feature names\n",
    "selected_features = [feature_names[i] for i in selector.get_support(indices=True)]\n",
    "\n",
    "# Save to file\n",
    "with open('selected_features.txt', 'w') as f:\n",
    "    f.write('\\n'.join(selected_features))\n",
    "```\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "| **Method** | **Complexity** | **1000 Features** | **Interpretability** | **Interactions** |\n",
    "|------------|----------------|-------------------|----------------------|------------------|\n",
    "| **Correlation** | O(nd) | 2s | High \u2b50\u2b50\u2b50 | No |\n",
    "| **Mutual Info** | O(nd log n) | 8s | Medium \u2b50\u2b50 | Partial |\n",
    "| **RFE** | O(d\u00b2 \u00d7 T) | 5 min | High \u2b50\u2b50\u2b50 | Yes |\n",
    "| **RFECV** | O(d\u00b2 \u00d7 k \u00d7 T) | 25 min | High \u2b50\u2b50\u2b50 | Yes |\n",
    "| **Lasso** | O(ndp) | 3s | High \u2b50\u2b50\u2b50 | No |\n",
    "| **Random Forest** | O(d \u00d7 T) | 8s | Medium \u2b50\u2b50 | Yes |\n",
    "\n",
    "**T = base model training time, k = CV folds, p = Lasso iterations**\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**\ud83c\udfaf Selection Strategy:**\n",
    "1. **Start with filter** (correlation, MI) if d > 500\n",
    "2. **Refine with embedded** (Lasso, RF) for efficiency\n",
    "3. **Validate with wrapper** (RFECV) if time permits\n",
    "4. **Ensemble methods** (union or intersection) for robustness\n",
    "\n",
    "**\ud83c\udfaf Validation:**\n",
    "- Always use cross-validation or hold-out set\n",
    "- Track accuracy vs number of features (elbow method)\n",
    "- Compare train vs validation gap (<2% acceptable)\n",
    "- Test on completely unseen data before production\n",
    "\n",
    "**\ud83c\udfaf Interpretability:**\n",
    "- Lasso: Non-zero coefficients = selected features\n",
    "- Trees: Feature importance scores\n",
    "- RFE: Ranking shows elimination order\n",
    "- Document why each feature was selected (for stakeholders)\n",
    "\n",
    "**\ud83c\udfaf Production:**\n",
    "- Save selector object with model\n",
    "- Track feature names (not just indices)\n",
    "- Version control feature lists\n",
    "- Monitor feature drift in production\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udd17 Next Steps\n",
    "\n",
    "- **032: Autoencoders** - Neural network dimensionality reduction\n",
    "- **041: Feature Engineering** - Create domain-specific features before selection\n",
    "- **042: Feature Importance Interpretation** - SHAP, LIME for model explanations\n",
    "\n",
    "---\n",
    "\n",
    "**\ud83d\udca1 Remember:** Filter for speed, Wrapper for accuracy, Embedded for balance!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}