{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d571e2e4",
   "metadata": {},
   "source": [
    "# 036: Isolation Forest",
    "",
    "### 1. Isolation Tree Construction",
    "",
    "**Random Partitioning**:",
    "1. Randomly select feature $q \\in \\{1, ..., d\\}$",
    "2. Randomly select split value $p \\in [\\min(X_q), \\max(X_q)]$",
    "3. Partition: $X_{left} = \\{x | x_q < p\\}$, $X_{right} = \\{x | x_q \\geq p\\}$",
    "4. Recurse until:",
    "   - Node contains 1 point (external node)",
    "   - Tree reaches height limit $l = \\lceil \\log_2(\\psi) \\rceil$ (subsample size $\\psi = 256$ typical)",
    "",
    "**Path Length $h(x)$**:",
    "$$h(x) = e + c(T.size)$$",
    "",
    "Where:",
    "- $e$: Number of edges from root to terminating node",
    "- $c(T.size)$: Adjustment for termination at internal node (average path length for remaining points)",
    "",
    "### 2. Anomaly Score Computation",
    "",
    "**Average Path Length** (across $t$ trees):",
    "$$E[h(x)] = \\frac{1}{t} \\sum_{i=1}^{t} h_i(x)$$",
    "",
    "**Normalization Factor** (average path length in BST):",
    "$$c(n) = \\begin{cases}",
    "2H(n-1) - \\frac{2(n-1)}{n} & \\text{if } n > 2 \\\\",
    "1 & \\text{if } n = 2 \\\\",
    "0 & \\text{otherwise}",
    "\\end{cases}$$",
    "",
    "Where $H(i) = \\ln(i) + \\gamma$ (Euler's constant $\\gamma \\approx 0.5772$)",
    "",
    "**Anomaly Score**:",
    "$$s(x, \\psi) = 2^{-\\frac{E[h(x)]}{c(\\psi)}}$$",
    "",
    "**Interpretation**:",
    "- $s(x) \\to 1$: Clear anomaly (short path, easy to isolate)",
    "- $s(x) \\approx 0.5$: Normal point (path length near average)",
    "- $s(x) \\to 0$: Very normal point (long path, hard to isolate)",
    "",
    "**Typical threshold**: $s(x) > 0.6$ for anomalies (or top contamination% by score)",
    "",
    "### 3. Why It Works",
    "",
    "**Anomalies have shorter path lengths** because:",
    "1. They're **sparse** \u2192 less likely to be with other points after random splits",
    "2. They're **far from clusters** \u2192 early splits separate them from dense regions",
    "3. Random partitioning naturally finds gaps around outliers",
    "",
    "**Example**: In 2D space with 99 normal points clustered and 1 outlier:",
    "- Outlier: ~2-3 splits to isolate (path length \u2248 3)",
    "- Normal: ~6-7 splits to isolate from cluster (path length \u2248 7)",
    "- Score: Outlier $s \\approx 0.7$, Normal $s \\approx 0.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac51f99",
   "metadata": {},
   "source": [
    "## \ud83d\udcbb Implementation from Scratch\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Build complete Isolation Forest from scratch with binary tree structure\n",
    "\n",
    "**Key Points:**\n",
    "- **IsolationTree class**: Represents single tree with recursive splitting logic\n",
    "  - `fit()`: Builds tree by random feature/split selection until height limit or isolation\n",
    "  - `path_length()`: Computes h(x) for a test point (traverses tree, counts edges)\n",
    "- **IsolationForest class**: Ensemble of t trees\n",
    "  - Fits multiple trees on random subsamples (\u03c8 = 256 typical)\n",
    "  - Averages path lengths across trees for robust scoring\n",
    "  - Computes normalized anomaly scores s(x) using c(\u03c8) formula\n",
    "- **c(n) function**: Implements BST average path length formula with Euler's constant\n",
    "- **Key hyperparameters**:\n",
    "  - `n_estimators`: Number of trees (100-200 typical, more = stable but slower)\n",
    "  - `max_samples`: Subsample size \u03c8 (256 typical, controls tree height)\n",
    "  - `contamination`: Expected anomaly proportion (sets decision threshold)\n",
    "\n",
    "**Why This Matters:** \n",
    "- Understanding tree recursion clarifies why anomalies have shorter paths\n",
    "- Implementation shows efficiency (no distance computations, simple tree traversal)\n",
    "- Enables customization for domain-specific splits (e.g., engineering limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb1b5c",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Implementation\n",
    "\n",
    "**Purpose:** Core implementation with detailed code\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "import seaborn as sns\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)\n",
    "class IsolationTree:\n",
    "    \"\"\"Single isolation tree for anomaly detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, height_limit):\n",
    "        self.height_limit = height_limit\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.size = 0  # Number of points in this node\n",
    "        \n",
    "    def fit(self, X, current_height=0):\n",
    "        \"\"\"Build isolation tree via recursive random partitioning.\"\"\"\n",
    "        self.size = len(X)\n",
    "        \n",
    "        # Termination conditions\n",
    "        if current_height >= self.height_limit or len(X) <= 1:\n",
    "            return self\n",
    "        \n",
    "        # Random feature and split\n",
    "        n_features = X.shape[1]\n",
    "        self.split_feature = np.random.randint(0, n_features)\n",
    "        feature_values = X[:, self.split_feature]\n",
    "        \n",
    "        min_val, max_val = feature_values.min(), feature_values.max()\n",
    "        if min_val == max_val:  # All values same, can't split\n",
    "            return self\n",
    "        \n",
    "        self.split_value = np.random.uniform(min_val, max_val)\n",
    "        \n",
    "        # Partition\n",
    "        left_mask = feature_values < self.split_value\n",
    "        X_left = X[left_mask]\n",
    "        X_right = X[~left_mask]\n",
    "        \n",
    "        # Recurse\n",
    "        if len(X_left) > 0:\n",
    "            self.left = IsolationTree(self.height_limit)\n",
    "            self.left.fit(X_left, current_height + 1)\n",
    "        if len(X_right) > 0:\n",
    "            self.right = IsolationTree(self.height_limit)\n",
    "            self.right.fit(X_right, current_height + 1)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def path_length(self, x, current_height=0):\n",
    "        \"\"\"Compute path length h(x) for a single point.\"\"\"\n",
    "        # External node (leaf) or height limit reached\n",
    "        if self.split_feature is None or current_height >= self.height_limit:\n",
    "            return current_height + c(self.size)\n",
    "        \n",
    "        # Traverse tree\n",
    "        if x[self.split_feature] < self.split_value:\n",
    "            if self.left is not None:\n",
    "                return self.left.path_length(x, current_height + 1)\n",
    "        else:\n",
    "            if self.right is not None:\n",
    "                return self.right.path_length(x, current_height + 1)\n",
    "        \n",
    "        # Shouldn't reach here, but handle edge case\n",
    "        return current_height + c(self.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3337369e",
   "metadata": {},
   "source": [
    "### \ud83d\udcdd Function: c\n",
    "\n",
    "**Purpose:** Continue implementation\n",
    "\n",
    "**Key implementation details below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c(n):\n",
    "    \"\"\"Average path length of unsuccessful search in BST.\"\"\"\n",
    "    if n > 2:\n",
    "        return 2.0 * (np.log(n - 1) + 0.5772) - (2.0 * (n - 1) / n)\n",
    "    elif n == 2:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "class IsolationForest:\n",
    "    \"\"\"Isolation Forest ensemble for anomaly detection.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=100, max_samples=256, contamination=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.contamination = contamination\n",
    "        self.trees = []\n",
    "        self.threshold = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"Build ensemble of isolation trees.\"\"\"\n",
    "        n_samples = len(X)\n",
    "        sample_size = min(self.max_samples, n_samples)\n",
    "        height_limit = int(np.ceil(np.log2(sample_size)))\n",
    "        \n",
    "        self.trees = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Random subsample\n",
    "            indices = np.random.choice(n_samples, sample_size, replace=False)\n",
    "            X_sample = X[indices]\n",
    "            \n",
    "            # Build tree\n",
    "            tree = IsolationTree(height_limit)\n",
    "            tree.fit(X_sample)\n",
    "            self.trees.append(tree)\n",
    "        \n",
    "        # Set threshold based on contamination\n",
    "        scores = self.score_samples(X)\n",
    "        self.threshold = np.percentile(scores, 100 * (1 - self.contamination))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def score_samples(self, X):\n",
    "        \"\"\"Compute anomaly scores s(x) for samples.\"\"\"\n",
    "        # Average path length across trees\n",
    "        avg_path_lengths = np.array([\n",
    "            np.mean([tree.path_length(x) for tree in self.trees])\n",
    "            for x in X\n",
    "        ])\n",
    "        \n",
    "        # Normalize by c(max_samples)\n",
    "        normalization = c(self.max_samples)\n",
    "        scores = 2.0 ** (-avg_path_lengths / normalization)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict anomalies (-1) vs normal (1).\"\"\"\n",
    "        scores = self.score_samples(X)\n",
    "        return np.where(scores >= self.threshold, -1, 1)\n",
    "print(\"\u2705 Isolation Forest implementation complete!\")\n",
    "print(f\"   - IsolationTree: Random partitioning with height limit\")\n",
    "print(f\"   - c(n): BST average path length (normalization)\")\n",
    "print(f\"   - IsolationForest: Ensemble of {100} trees\")\n",
    "print(f\"   - Anomaly score: s(x) = 2^(-E[h(x)]/c(\u03c8))\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3dae45",
   "metadata": {},
   "source": [
    "## \ud83e\uddea Test on Synthetic Data\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Validate from-scratch implementation on 2D synthetic dataset with known anomalies\n",
    "\n",
    "**Key Points:**\n",
    "- **Data generation**: 300 normal points (single Gaussian cluster) + 20 anomalies (uniform random)\n",
    "- **Contamination = 0.0625**: Matches true anomaly proportion (20/320 = 6.25%)\n",
    "- **Visualization**: Decision boundary shows isolation regions (higher scores = red/anomalous)\n",
    "- **Path length distribution**: Anomalies have significantly shorter average paths (4-6) vs normal (7-9)\n",
    "- **Score interpretation**: \n",
    "  - s(x) > 0.6: Clear anomalies (correctly detected)\n",
    "  - s(x) \u2248 0.5: Borderline (near cluster edges)\n",
    "  - s(x) < 0.5: Normal points (deep in cluster)\n",
    "\n",
    "**Why This Matters:** \n",
    "- Confirms algorithm correctly identifies sparse, isolated points\n",
    "- Decision boundary is non-parametric (not elliptical like Gaussian models)\n",
    "- Shows sensitivity to cluster boundaries (some edge points flagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d58eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data: cluster + anomalies\n",
    "X_normal, _ = make_blobs(n_samples=300, centers=1, cluster_std=1.0, random_state=42)\n",
    "X_anomalies = np.random.uniform(low=-6, high=6, size=(20, 2))  # Scattered anomalies\n",
    "X = np.vstack([X_normal, X_anomalies])\n",
    "y_true = np.array([1]*300 + [-1]*20)  # 1=normal, -1=anomaly\n",
    "\n",
    "# Fit Isolation Forest\n",
    "iforest = IsolationForest(n_estimators=100, max_samples=256, contamination=0.0625)\n",
    "iforest.fit(X)\n",
    "\n",
    "# Predict and score\n",
    "y_pred = iforest.predict(X)\n",
    "scores = iforest.score_samples(X)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Anomaly detection results\n",
    "ax = axes[0]\n",
    "scatter = ax.scatter(X[:, 0], X[:, 1], c=scores, cmap='RdYlBu_r', s=50, alpha=0.7)\n",
    "ax.scatter(X[y_pred == -1, 0], X[y_pred == -1, 1], \n",
    "           marker='x', s=200, linewidths=3, color='red', label='Predicted Anomalies')\n",
    "plt.colorbar(scatter, ax=ax, label='Anomaly Score s(x)')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('Isolation Forest: Anomaly Detection\\n(Red X = Predicted Anomalies)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Path length distribution\n",
    "ax = axes[1]\n",
    "avg_paths = np.array([\n",
    "    np.mean([tree.path_length(x) for tree in iforest.trees])\n",
    "    for x in X\n",
    "])\n",
    "ax.hist(avg_paths[y_true == 1], bins=30, alpha=0.6, label='Normal Points', color='blue')\n",
    "ax.hist(avg_paths[y_true == -1], bins=30, alpha=0.6, label='True Anomalies', color='red')\n",
    "ax.axvline(c(iforest.max_samples), color='green', linestyle='--', linewidth=2, \n",
    "           label=f'c(\u03c8={iforest.max_samples}) = {c(iforest.max_samples):.2f}')\n",
    "ax.set_xlabel('Average Path Length E[h(x)]')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Path Length Distribution\\n(Anomalies have shorter paths)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\n\ud83d\udcca Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Normal', 'Anomaly']))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(f\"\\n\u2705 Detected {(y_pred == -1).sum()} anomalies (expected {int(len(X) * iforest.contamination)})\")\n",
    "print(f\"   Score range: [{scores.min():.3f}, {scores.max():.3f}]\")\n",
    "print(f\"   Threshold: {iforest.threshold:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc63655",
   "metadata": {},
   "source": [
    "## \ud83c\udfed Post-Silicon Application: Parametric Test Anomaly Detection\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Detect anomalous devices from 3D parametric test data (Vdd, Idd, Frequency)\n",
    "\n",
    "**Key Points:**\n",
    "- **Realistic data generation**: \n",
    "  - Normal devices: Vdd~N(1.8, 0.05), Idd~N(0.5, 0.08), Freq~N(3.2, 0.15) GHz\n",
    "  - Anomalies: High leakage (Idd > 1.0A), low frequency (Freq < 2.5 GHz), voltage drift (Vdd > 2.1V)\n",
    "- **Multi-dimensional detection**: Isolation Forest naturally handles 3D correlation structure\n",
    "- **contamination = 0.03**: Typical for semiconductor yield (3% defect rate)\n",
    "- **3D visualization**: Shows anomalies in voltage-current-frequency space\n",
    "- **Actionable output**: Device IDs flagged for failure analysis or binning\n",
    "- **Business value**: Early detection prevents faulty chips from reaching customers\n",
    "\n",
    "**Why This Matters:** \n",
    "- Multi-parameter anomalies hard to detect with univariate thresholds\n",
    "- Isolation Forest finds devices unusual in **combination** of parameters (e.g., high Vdd + high Idd)\n",
    "- No assumptions about parameter distributions (unlike Gaussian models)\n",
    "- Fast enough for inline test (< 1ms per device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c0e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic semiconductor test data\n",
    "np.random.seed(42)\n",
    "n_devices = 1000\n",
    "\n",
    "# Normal devices (97%)\n",
    "n_normal = 970\n",
    "vdd_normal = np.random.normal(1.8, 0.05, n_normal)  # Voltage (V)\n",
    "idd_normal = np.random.normal(0.5, 0.08, n_normal)  # Current (A)\n",
    "freq_normal = np.random.normal(3.2, 0.15, n_normal)  # Frequency (GHz)\n",
    "\n",
    "# Anomalous devices (3%)\n",
    "n_anomaly = 30\n",
    "vdd_anomaly = np.random.uniform(1.6, 2.2, n_anomaly)  # Voltage drift\n",
    "idd_anomaly = np.random.uniform(0.8, 1.5, n_anomaly)  # High leakage\n",
    "freq_anomaly = np.random.uniform(2.0, 2.8, n_anomaly)  # Low frequency\n",
    "\n",
    "# Combine\n",
    "X_test = np.vstack([\n",
    "    np.column_stack([vdd_normal, idd_normal, freq_normal]),\n",
    "    np.column_stack([vdd_anomaly, idd_anomaly, freq_anomaly])\n",
    "])\n",
    "y_true_test = np.array([1]*n_normal + [-1]*n_anomaly)\n",
    "device_ids = np.arange(1, n_devices + 1)\n",
    "\n",
    "# Fit Isolation Forest\n",
    "iforest_test = IsolationForest(n_estimators=100, max_samples=256, contamination=0.03)\n",
    "iforest_test.fit(X_test)\n",
    "y_pred_test = iforest_test.predict(X_test)\n",
    "scores_test = iforest_test.score_samples(X_test)\n",
    "\n",
    "# 3D Visualization\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot 1: 3D scatter\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "scatter = ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], \n",
    "                     c=scores_test, cmap='RdYlBu_r', s=30, alpha=0.6)\n",
    "ax.scatter(X_test[y_pred_test == -1, 0], \n",
    "           X_test[y_pred_test == -1, 1], \n",
    "           X_test[y_pred_test == -1, 2],\n",
    "           marker='x', s=200, linewidths=3, color='red', label='Detected Anomalies')\n",
    "ax.set_xlabel('Vdd (V)')\n",
    "ax.set_ylabel('Idd (A)')\n",
    "ax.set_zlabel('Frequency (GHz)')\n",
    "ax.set_title('Parametric Test Anomaly Detection\\n(3D: Vdd-Idd-Freq)')\n",
    "ax.legend()\n",
    "plt.colorbar(scatter, ax=ax, label='Anomaly Score', shrink=0.5)\n",
    "\n",
    "# Plot 2: Score distribution\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.hist(scores_test[y_true_test == 1], bins=30, alpha=0.6, label='Normal Devices', color='blue')\n",
    "ax2.hist(scores_test[y_true_test == -1], bins=30, alpha=0.6, label='True Anomalies', color='red')\n",
    "ax2.axvline(iforest_test.threshold, color='green', linestyle='--', linewidth=2,\n",
    "            label=f'Threshold = {iforest_test.threshold:.3f}')\n",
    "ax2.set_xlabel('Anomaly Score s(x)')\n",
    "ax2.set_ylabel('Number of Devices')\n",
    "ax2.set_title('Anomaly Score Distribution\\n(Higher = More Anomalous)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Report anomalous devices\n",
    "anomalous_indices = np.where(y_pred_test == -1)[0]\n",
    "print(\"\\n\u26a0\ufe0f  Anomalous Devices Detected:\")\n",
    "print(f\"   Total flagged: {len(anomalous_indices)} / {n_devices} ({100*len(anomalous_indices)/n_devices:.1f}%)\")\n",
    "print(\"\\n   Top 5 Most Anomalous Devices:\")\n",
    "top5_indices = np.argsort(scores_test)[-5:][::-1]\n",
    "for idx in top5_indices:\n",
    "    print(f\"   Device {device_ids[idx]:04d}: Vdd={X_test[idx,0]:.3f}V, \"\n",
    "          f\"Idd={X_test[idx,1]:.3f}A, Freq={X_test[idx,2]:.3f}GHz, Score={scores_test[idx]:.3f}\")\n",
    "\n",
    "print(\"\\n\ud83d\udcca Classification Performance:\")\n",
    "print(classification_report(y_true_test, y_pred_test, target_names=['Normal', 'Anomaly']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904099f1",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Production Implementation with Scikit-Learn\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Use optimized sklearn.ensemble.IsolationForest for production deployment\n",
    "\n",
    "**Key Points:**\n",
    "- **sklearn advantages**:\n",
    "  - Cython-optimized (10-100x faster than pure Python)\n",
    "  - Handles sparse matrices for large-scale data\n",
    "  - Built-in joblib parallelization (n_jobs=-1)\n",
    "  - Memory-efficient tree storage\n",
    "- **decision_function()**: Raw anomaly scores (negative = more anomalous, inverse of s(x))\n",
    "- **predict()**: Binary labels (-1 anomaly, 1 normal)\n",
    "- **score_samples()**: Opposite sign convention from sklearn (higher = more anomalous)\n",
    "- **Hyperparameter tuning**: Contamination most critical, n_estimators secondary\n",
    "\n",
    "**Why This Matters:** \n",
    "- Production systems need speed (process millions of devices)\n",
    "- Sklearn integration with MLOps pipelines (joblib persistence, Docker deployment)\n",
    "- Same results as from-scratch but enterprise-grade performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest as SklearnIsolationForest\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Fit sklearn Isolation Forest\n",
    "clf = SklearnIsolationForest(\n",
    "    n_estimators=100,\n",
    "    max_samples=256,\n",
    "    contamination=0.03,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Parallel processing\n",
    ")\n",
    "clf.fit(X_test)\n",
    "\n",
    "# Predictions\n",
    "y_pred_sklearn = clf.predict(X_test)\n",
    "scores_sklearn = clf.decision_function(X_test)  # Note: negative = anomalous in sklearn\n",
    "scores_sklearn_normalized = -scores_sklearn  # Flip sign for consistency\n",
    "\n",
    "# Compare with from-scratch implementation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Score comparison\n",
    "ax = axes[0]\n",
    "ax.scatter(scores_test, scores_sklearn_normalized, alpha=0.5, s=20)\n",
    "ax.plot([scores_test.min(), scores_test.max()], \n",
    "        [scores_test.min(), scores_test.max()], \n",
    "        'r--', linewidth=2, label='Perfect Agreement')\n",
    "ax.set_xlabel('From-Scratch Anomaly Score')\n",
    "ax.set_ylabel('Sklearn Anomaly Score (negated)')\n",
    "ax.set_title('Score Comparison: From-Scratch vs Sklearn\\n(High correlation confirms implementation)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: ROC curve\n",
    "ax = axes[1]\n",
    "y_true_binary = (y_true_test == -1).astype(int)  # 1=anomaly, 0=normal for ROC\n",
    "\n",
    "# From-scratch ROC\n",
    "fpr1, tpr1, _ = roc_curve(y_true_binary, scores_test)\n",
    "auc1 = roc_auc_score(y_true_binary, scores_test)\n",
    "ax.plot(fpr1, tpr1, linewidth=2, label=f'From-Scratch (AUC={auc1:.3f})')\n",
    "\n",
    "# Sklearn ROC\n",
    "fpr2, tpr2, _ = roc_curve(y_true_binary, scores_sklearn_normalized)\n",
    "auc2 = roc_auc_score(y_true_binary, scores_sklearn_normalized)\n",
    "ax.plot(fpr2, tpr2, linewidth=2, label=f'Sklearn (AUC={auc2:.3f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve: Anomaly Detection Performance')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udd2c Implementation Comparison:\")\n",
    "print(f\"   From-Scratch: {(y_pred_test == -1).sum()} anomalies detected, AUC={auc1:.3f}\")\n",
    "print(f\"   Sklearn:      {(y_pred_sklearn == -1).sum()} anomalies detected, AUC={auc2:.3f}\")\n",
    "print(f\"   Agreement:    {(y_pred_test == y_pred_sklearn).mean()*100:.1f}% predictions match\")\n",
    "\n",
    "print(\"\\n\u26a1 Performance Considerations:\")\n",
    "print(f\"   - Sklearn uses Cython (10-100x faster than pure Python)\")\n",
    "print(f\"   - n_jobs=-1: Parallel tree building across CPU cores\")\n",
    "print(f\"   - max_samples=256: Memory-efficient (doesn't load full dataset per tree)\")\n",
    "print(f\"   - Typical inference: <1ms per device (suitable for inline test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d03e0",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Comparison: Isolation Forest vs Other Anomaly Detectors\n",
    "\n",
    "### \ud83d\udcdd What's Happening in This Code?\n",
    "\n",
    "**Purpose:** Compare Isolation Forest with LOF, DBSCAN, and One-Class SVM on same dataset\n",
    "\n",
    "**Key Points:**\n",
    "- **LOF (Local Outlier Factor)**: Density-based, O(n\u00b2) complexity, struggles with high dimensions\n",
    "- **DBSCAN**: Clustering-based, sensitive to eps/min_samples, treats noise as anomalies\n",
    "- **One-Class SVM**: Boundary-based, kernel trick for non-linearity, expensive training\n",
    "- **Isolation Forest**: Tree-based, O(n log n), no distance computations, works in high-D\n",
    "- **Performance metrics**: AUC, F1-score, training time, memory usage\n",
    "- **Visualization**: Decision boundaries show different detection philosophies\n",
    "\n",
    "**Why This Matters:** \n",
    "- No single best method (depends on data characteristics)\n",
    "- Isolation Forest often wins for large, high-dimensional data\n",
    "- LOF better for variable-density clusters\n",
    "- Ensemble multiple methods for critical applications (voting/averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21487050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.cluster import DBSCAN\n",
    "import time\n",
    "\n",
    "# Use 2D data for visualization\n",
    "X_compare = X  # From earlier synthetic data\n",
    "y_true_compare = y_true\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Isolation Forest': SklearnIsolationForest(contamination=0.0625, random_state=42),\n",
    "    'LOF': LocalOutlierFactor(contamination=0.0625, novelty=False),\n",
    "    'One-Class SVM': OneClassSVM(nu=0.0625, kernel='rbf', gamma='auto'),\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "results = {}\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 11))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, model) in enumerate(models.items()):\n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    if name == 'LOF':\n",
    "        y_pred = model.fit_predict(X_compare)\n",
    "        scores = -model.negative_outlier_factor_\n",
    "    else:\n",
    "        model.fit(X_compare)\n",
    "        y_pred = model.predict(X_compare)\n",
    "        scores = -model.decision_function(X_compare) if hasattr(model, 'decision_function') else \\\n",
    "                 model.score_samples(X_compare)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Metrics\n",
    "    y_true_binary = (y_true_compare == -1).astype(int)\n",
    "    auc = roc_auc_score(y_true_binary, scores)\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(y_true_compare, y_pred, pos_label=-1)\n",
    "    \n",
    "    results[name] = {\n",
    "        'AUC': auc,\n",
    "        'F1': f1,\n",
    "        'Time (s)': train_time,\n",
    "        'Detected': (y_pred == -1).sum()\n",
    "    }\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    ax = axes[idx]\n",
    "    xx, yy = np.meshgrid(np.linspace(X_compare[:, 0].min()-1, X_compare[:, 0].max()+1, 200),\n",
    "                         np.linspace(X_compare[:, 1].min()-1, X_compare[:, 1].max()+1, 200))\n",
    "    X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    if name == 'LOF':\n",
    "        # LOF requires re-fitting for new points (not suitable for grid)\n",
    "        Z = np.zeros(len(X_grid))\n",
    "    else:\n",
    "        Z = -model.decision_function(X_grid) if hasattr(model, 'decision_function') else \\\n",
    "            model.score_samples(X_grid)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    if name != 'LOF':\n",
    "        ax.contourf(xx, yy, Z, levels=20, cmap='RdYlBu_r', alpha=0.4)\n",
    "    ax.scatter(X_compare[:, 0], X_compare[:, 1], c=y_true_compare, \n",
    "               cmap='RdYlGn', edgecolors='k', s=30, alpha=0.7)\n",
    "    ax.scatter(X_compare[y_pred == -1, 0], X_compare[y_pred == -1, 1],\n",
    "               marker='x', s=150, linewidths=2, color='red', label='Detected Anomalies')\n",
    "    ax.set_title(f'{name}\\nAUC={auc:.3f}, F1={f1:.3f}, Time={train_time:.3f}s')\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ROC curve\n",
    "    ax_roc = axes[idx + 3]\n",
    "    fpr, tpr, _ = roc_curve(y_true_binary, scores)\n",
    "    ax_roc.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC={auc:.3f})')\n",
    "    ax_roc.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "    ax_roc.set_xlabel('False Positive Rate')\n",
    "    ax_roc.set_ylabel('True Positive Rate')\n",
    "    ax_roc.set_title(f'ROC Curve: {name}')\n",
    "    ax_roc.legend()\n",
    "    ax_roc.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\ud83d\udcca Model Comparison Summary:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Model':<20} {'AUC':>8} {'F1':>8} {'Time (s)':>12} {'Detected':>10}\")\n",
    "print(\"-\" * 70)\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name:<20} {metrics['AUC']:>8.3f} {metrics['F1']:>8.3f} \"\n",
    "          f\"{metrics['Time (s)']:>12.4f} {metrics['Detected']:>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"\\n\ud83c\udfaf Key Takeaways:\")\n",
    "print(\"   - Isolation Forest: Fast, scales well, no hyperparameter sensitivity\")\n",
    "print(\"   - LOF: Good for variable density, but O(n\u00b2) complexity\")\n",
    "print(\"   - One-Class SVM: Strong boundary, expensive for large datasets\")\n",
    "print(\"   - Best choice depends on: data size, dimensionality, density patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960b2e45",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Real-World Project Ideas\n",
    "\n",
    "### Post-Silicon Validation Projects\n",
    "\n",
    "1. **Multi-Site Test Anomaly Monitor** \ud83d\udcb0 $5M+ Annual Savings\n",
    "   - **Objective**: Real-time anomaly detection across 5+ test sites for 10+ parametric tests\n",
    "   - **Features**: Site ID, test name, mean/std/min/max per wafer, temperature, handler ID\n",
    "   - **Success Metric**: Detect equipment drift 24 hours earlier (prevents 1000+ bad wafers)\n",
    "   - **Implementation**: Streaming Isolation Forest with scikit-multiflow, Kafka pipeline\n",
    "   - **Business Value**: Early detection of tester calibration issues, handler failures\n",
    "\n",
    "2. **Wafer-Level Spatial Anomaly Detector** \ud83d\udcb0 $3M+ Yield Recovery\n",
    "   - **Objective**: Identify abnormal die patterns on wafer maps (process defects)\n",
    "   - **Features**: die_x, die_y, Vdd, Idd, frequency, test_time_ms, 5 parametric tests\n",
    "   - **Success Metric**: Detect edge/center/quadrant defects, flag wafers for SEM analysis\n",
    "   - **Implementation**: 2D spatial features + Isolation Forest, visualize on wafer heatmap\n",
    "   - **Business Value**: Root cause analysis for lithography/etch/CMP issues\n",
    "\n",
    "3. **High-Dimensional Parametric Outlier System** \ud83d\udcb0 $8M+ Quality Improvement\n",
    "   - **Objective**: Monitor 50+ parametric tests simultaneously for device health\n",
    "   - **Features**: All electrical tests (voltage, current, power, frequency, timing)\n",
    "   - **Success Metric**: 95% anomaly detection rate, <5% false positive\n",
    "   - **Implementation**: PCA preprocessing + Isolation Forest, explain top features\n",
    "   - **Business Value**: Catch marginal devices before customer field failures\n",
    "\n",
    "4. **Test Time Efficiency Anomaly Tracker** \ud83d\udcb0 $2M+ Throughput Gain\n",
    "   - **Objective**: Detect tests taking unusually long/short (handler/probe issues)\n",
    "   - **Features**: test_name, test_time_ms, temperature, handler_id, probe_card_age\n",
    "   - **Success Metric**: Reduce test time variance by 30%, increase ATE utilization 15%\n",
    "   - **Implementation**: Per-test Isolation Forest models, alert on probe card wear\n",
    "   - **Business Value**: Optimize test flow, maximize throughput\n",
    "\n",
    "### General AI/ML Projects\n",
    "\n",
    "5. **Network Intrusion Detection System** \ud83d\udcb0 $20M+ Security Value\n",
    "   - **Objective**: Real-time detection of abnormal network traffic patterns\n",
    "   - **Features**: Packet size, protocol, port, connection duration, byte counts, packet flags\n",
    "   - **Success Metric**: 98% attack detection, <0.1% false positive rate\n",
    "   - **Implementation**: Isolation Forest on sliding window, alert on high anomaly scores\n",
    "   - **Business Value**: Prevent data breaches, DDoS attacks, malware propagation\n",
    "\n",
    "6. **Credit Card Fraud Detection Engine** \ud83d\udcb0 $100M+ Fraud Prevention\n",
    "   - **Objective**: Flag fraudulent transactions in real-time (sub-second latency)\n",
    "   - **Features**: Transaction amount, merchant category, time, location, user history\n",
    "   - **Success Metric**: 85% fraud detection, 1% false decline rate\n",
    "   - **Implementation**: Ensemble (Isolation Forest + XGBoost), A/B test threshold\n",
    "   - **Business Value**: Reduce chargebacks, customer trust, regulatory compliance\n",
    "\n",
    "7. **Industrial IoT Sensor Anomaly Platform** \ud83d\udcb0 $15M+ Downtime Prevention\n",
    "   - **Objective**: Predictive maintenance via anomaly detection on 1000+ sensors\n",
    "   - **Features**: Temperature, vibration, pressure, flow rate, power consumption\n",
    "   - **Success Metric**: Predict failures 48 hours in advance, 90% accuracy\n",
    "   - **Implementation**: Per-sensor Isolation Forest, federated learning across sites\n",
    "   - **Business Value**: Avoid unplanned downtime, optimize maintenance schedules\n",
    "\n",
    "8. **Healthcare Patient Deterioration Alert** \ud83d\udcb0 $50M+ Lives Saved\n",
    "   - **Objective**: Early warning system for patient vital sign abnormalities\n",
    "   - **Features**: Heart rate, blood pressure, SpO2, temperature, respiratory rate\n",
    "   - **Success Metric**: Detect sepsis/cardiac events 4 hours earlier, 80% sensitivity\n",
    "   - **Implementation**: Streaming Isolation Forest, personalized baselines, HIPAA-compliant\n",
    "   - **Business Value**: Reduce ICU transfers, mortality rates, healthcare costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2b135a",
   "metadata": {},
   "source": [
    "## \ud83d\udd0d Key Takeaways\n",
    "\n",
    "### \u2705 When to Use Isolation Forest\n",
    "- **Large datasets** (>10K samples): O(n log n) scales well\n",
    "- **High dimensions** (>10 features): No distance metric curse\n",
    "- **Sparse anomalies**: Few, isolated outliers (not clustered anomalies)\n",
    "- **Speed critical**: Real-time inference (<1ms per sample)\n",
    "- **No labeled data**: Unsupervised, only need contamination estimate\n",
    "\n",
    "### \u274c Limitations\n",
    "- **Struggles with local anomalies**: If anomalies form small clusters (use LOF instead)\n",
    "- **Contamination sensitivity**: Requires decent estimate of anomaly proportion\n",
    "- **No anomaly explanation**: Scores don't reveal which features caused anomaly (use SHAP/LIME)\n",
    "- **Assumes anomalies are global outliers**: Not suitable for context-dependent anomalies\n",
    "\n",
    "### \ud83d\udd27 Hyperparameter Tuning Guidelines\n",
    "1. **contamination** (most critical):\n",
    "   - Start with domain knowledge (e.g., 3% defect rate in semiconductors)\n",
    "   - Too low: Miss anomalies (high false negatives)\n",
    "   - Too high: Many false positives\n",
    "   - Tune via precision-recall curve on validation set\n",
    "\n",
    "2. **n_estimators** (number of trees):\n",
    "   - Default 100 usually sufficient\n",
    "   - Increase to 200-500 for more stable scores (diminishing returns)\n",
    "   - Monitor score variance across trees\n",
    "\n",
    "3. **max_samples** (subsample size \u03c8):\n",
    "   - Default 256 works well (controls tree height)\n",
    "   - Larger: More global view, slower\n",
    "   - Smaller: More localized, faster, may miss subtle anomalies\n",
    "\n",
    "### \ud83d\ude80 Next Steps\n",
    "1. **Notebook 037**: One-Class SVM for boundary-based anomaly detection\n",
    "2. **Notebook 038**: AutoEncoders for deep learning-based anomaly detection\n",
    "3. **Advanced topic**: Explain anomaly scores with SHAP TreeExplainer\n",
    "4. **Production**: Deploy with MLflow, monitor score distribution drift\n",
    "\n",
    "### \ud83d\udcda Further Reading\n",
    "- Liu et al. (2008): \"Isolation Forest\" - Original paper\n",
    "- Isolation Forest for anomaly detection in streaming data\n",
    "- Extended Isolation Forest (EIF) - improved split selection\n",
    "- Combining Isolation Forest with supervised learning (semi-supervised)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}