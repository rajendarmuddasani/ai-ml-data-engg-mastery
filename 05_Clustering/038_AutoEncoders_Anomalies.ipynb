{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "506ccd5f",
   "metadata": {},
   "source": [
    "# 038: Autoencoders for Anomaly Detection\n",
    "\n",
    "### Architecture\n",
    "\n",
    "**Encoder**: $z = f_{enc}(x; \\theta_{enc})$ maps input $x \\in \\mathbb{R}^d$ to latent $z \\in \\mathbb{R}^k$ (k << d)\n",
    "\n",
    "**Decoder**: $\\hat{x} = f_{dec}(z; \\theta_{dec})$ reconstructs from latent space\n",
    "\n",
    "**Training Loss** (MSE):\n",
    "$$L = \\frac{1}{n}\\sum_{i=1}^{n} ||x_i - \\hat{x}_i||^2$$\n",
    "\n",
    "**Anomaly Score**:\n",
    "$$s(x) = ||x - f_{dec}(f_{enc}(x))||^2$$\n",
    "\n",
    "Normal points: low reconstruction error  \n",
    "Anomalies: high reconstruction error (didn't learn these patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d9f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=8):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, latent_dim)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n",
    "print(\"‚úÖ AutoEncoder architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "X_normal, _ = make_blobs(n_samples=1000, centers=1, n_features=10, random_state=42)\n",
    "X_anomalies = np.random.uniform(low=-8, high=8, size=(100, 10))\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_normal_scaled = scaler.fit_transform(X_normal)\n",
    "X_anomalies_scaled = scaler.transform(X_anomalies)\n",
    "\n",
    "# Train autoencoder on normal data only\n",
    "X_train_tensor = torch.FloatTensor(X_normal_scaled)\n",
    "model = AutoEncoder(input_dim=10, latent_dim=3)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    recon = model(X_train_tensor)\n",
    "    loss = criterion(recon, X_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Training complete. Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65121f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute reconstruction errors\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test = np.vstack([X_normal_scaled, X_anomalies_scaled])\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    recon_test = model(X_test_tensor).numpy()\n",
    "    errors = np.mean((X_test - recon_test)**2, axis=1)\n",
    "\n",
    "y_true = np.array([1]*len(X_normal_scaled) + [-1]*len(X_anomalies_scaled))\n",
    "\n",
    "# Set threshold (95th percentile of normal errors)\n",
    "threshold = np.percentile(errors[:len(X_normal_scaled)], 95)\n",
    "y_pred = np.where(errors > threshold, -1, 1)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.hist(errors[:len(X_normal_scaled)], bins=30, alpha=0.6, label='Normal', color='blue')\n",
    "ax.hist(errors[len(X_normal_scaled):], bins=30, alpha=0.6, label='Anomalies', color='red')\n",
    "ax.axvline(threshold, color='green', linestyle='--', linewidth=2, label=f'Threshold={threshold:.3f}')\n",
    "ax.set_xlabel('Reconstruction Error')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('AutoEncoder: Reconstruction Error Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "y_true_binary = (y_true == -1).astype(int)\n",
    "fpr, tpr, _ = roc_curve(y_true_binary, errors)\n",
    "auc = roc_auc_score(y_true_binary, errors)\n",
    "ax.plot(fpr, tpr, linewidth=2, label=f'AUC={auc:.3f}')\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Performance:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Normal', 'Anomaly']))\n",
    "print(f\"\\nAUC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7950ca6d",
   "metadata": {},
   "source": [
    "## üè≠ Semiconductor Application\n",
    "\n",
    "### üìù High-Dimensional Parametric Test Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff17de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic semiconductor test data (20 parameters)\n",
    "np.random.seed(42)\n",
    "n_params = 20\n",
    "n_normal = 2000\n",
    "n_anomaly = 100\n",
    "\n",
    "# Normal devices: correlated parameters\n",
    "mean_normal = np.random.uniform(1.5, 2.5, n_params)\n",
    "cov_normal = np.eye(n_params) * 0.05\n",
    "X_psv_normal = np.random.multivariate_normal(mean_normal, cov_normal, n_normal)\n",
    "\n",
    "# Anomalous devices: parameter drift\n",
    "X_psv_anomaly = X_psv_normal[:n_anomaly].copy()\n",
    "X_psv_anomaly[:, :5] += np.random.uniform(0.5, 1.5, (n_anomaly, 5))  # Drift in first 5 params\n",
    "\n",
    "# Scale\n",
    "scaler_psv = StandardScaler()\n",
    "X_psv_normal_scaled = scaler_psv.fit_transform(X_psv_normal)\n",
    "X_psv_anomaly_scaled = scaler_psv.transform(X_psv_anomaly)\n",
    "\n",
    "# Train autoencoder\n",
    "X_train_psv = torch.FloatTensor(X_psv_normal_scaled)\n",
    "model_psv = AutoEncoder(input_dim=n_params, latent_dim=5)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_psv.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model_psv.train()\n",
    "    optimizer.zero_grad()\n",
    "    recon = model_psv(X_train_psv)\n",
    "    loss = criterion(recon, X_train_psv)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Test\n",
    "model_psv.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_psv = np.vstack([X_psv_normal_scaled[:500], X_psv_anomaly_scaled])\n",
    "    X_test_psv_tensor = torch.FloatTensor(X_test_psv)\n",
    "    recon_psv = model_psv(X_test_psv_tensor).numpy()\n",
    "    errors_psv = np.mean((X_test_psv - recon_psv)**2, axis=1)\n",
    "\n",
    "y_true_psv = np.array([1]*500 + [-1]*n_anomaly)\n",
    "threshold_psv = np.percentile(errors_psv[:500], 95)\n",
    "y_pred_psv = np.where(errors_psv > threshold_psv, -1, 1)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(errors_psv[:500], bins=30, alpha=0.6, label='Normal Devices', color='blue')\n",
    "plt.hist(errors_psv[500:], bins=30, alpha=0.6, label='Anomalous Devices', color='red')\n",
    "plt.axvline(threshold_psv, color='green', linestyle='--', linewidth=2, label=f'Threshold')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Number of Devices')\n",
    "plt.title('Parametric Test Anomaly Detection (20 Parameters)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Feature importance (average reconstruction error per feature)\n",
    "feature_errors = np.mean((X_test_psv[500:] - recon_psv[500:])**2, axis=0)\n",
    "plt.bar(range(n_params), feature_errors, color='coral')\n",
    "plt.xlabel('Parameter Index')\n",
    "plt.ylabel('Avg Reconstruction Error')\n",
    "plt.title('Parameter-wise Anomaly Contribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Anomalous Devices Detected:\")\n",
    "print(f\"   Total: {(y_pred_psv == -1).sum()} / {len(X_test_psv)}\")\n",
    "print(\"\\nüìä Performance:\")\n",
    "print(classification_report(y_true_psv, y_pred_psv, target_names=['Normal', 'Anomaly']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378695c6",
   "metadata": {},
   "source": [
    "## üéØ Project Ideas\n",
    "\n",
    "### Post-Silicon Projects\n",
    "\n",
    "1. **Wafer Map Anomaly Detector** üí∞ $8M+ Yield Improvement\n",
    "   - Train on normal wafer spatial patterns, detect systematic defects\n",
    "   - Features: 2D die coordinates + parametric test values\n",
    "   - Business: Early fab process issue detection\n",
    "\n",
    "2. **Multi-Site Test Correlation Monitor** üí∞ $12M+ Quality\n",
    "   - 50+ parametric tests, detect novel failure modes\n",
    "   - AutoEncoder learns normal parameter correlations\n",
    "   - Business: Improve test coverage, reduce escapes\n",
    "\n",
    "3. **Time-Series Waveform Anomalies** üí∞ $5M+ Debug Time\n",
    "   - LSTM AutoEncoder on test waveforms\n",
    "   - Detect subtle signal integrity issues\n",
    "   - Business: Faster failure analysis\n",
    "\n",
    "4. **Cross-Product Defect Discovery** üí∞ $15M+ Portfolio\n",
    "   - Train per-product autoencoders\n",
    "   - Transfer learning for new products\n",
    "   - Business: Accelerate new product ramp\n",
    "\n",
    "### General Projects\n",
    "\n",
    "5. **Network Intrusion Detection** üí∞ $30M+ Security\n",
    "6. **Medical Image Anomalies** üí∞ $100M+ Healthcare\n",
    "7. **Industrial Sensor Monitoring** üí∞ $20M+ Downtime\n",
    "8. **Financial Transaction Fraud** üí∞ $150M+ Fraud Prevention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49756edd",
   "metadata": {},
   "source": [
    "## üîç Key Takeaways\n",
    "\n",
    "### ‚úÖ When to Use AutoEncoders\n",
    "- **High-dimensional data** (>50 features): Learns compressed representations\n",
    "- **Complex patterns**: Captures non-linear correlations via deep networks\n",
    "- **Unlabeled data**: Unsupervised, trains on normal only\n",
    "- **Feature learning**: Automatic feature extraction (no manual engineering)\n",
    "\n",
    "### ‚ùå Limitations\n",
    "- **Training time**: Requires GPU for large datasets\n",
    "- **Hyperparameters**: Architecture, latent dim, learning rate tuning needed\n",
    "- **Overfitting risk**: May memorize training data (use regularization)\n",
    "- **Black box**: Less interpretable than tree-based methods\n",
    "\n",
    "### üîß Best Practices\n",
    "1. **Always standardize** inputs (zero mean, unit variance)\n",
    "2. **Latent dimension**: Start with d/4 to d/2 (compression ratio 2-4x)\n",
    "3. **Threshold**: 95th-99th percentile of training errors\n",
    "4. **Validation**: Use contaminated validation set to tune threshold\n",
    "5. **Regularization**: Dropout, L2 weight decay to prevent overfitting\n",
    "\n",
    "### üìä Comparison\n",
    "\n",
    "| Method | Speed | High-D | Interpretability | Best For |\n",
    "|--------|-------|--------|------------------|----------|\n",
    "| **AutoEncoder** | Slow train, fast inference | ‚úÖ Excellent | ‚ùå Low | Complex patterns, images |\n",
    "| **Isolation Forest** | ‚úÖ Fast | ‚úÖ Good | ‚ö†Ô∏è Medium | Large data, speed |\n",
    "| **One-Class SVM** | ‚ùå Slow | ‚ö†Ô∏è Medium | ‚úÖ Good | Novelty, small data |\n",
    "\n",
    "### üöÄ Next Steps\n",
    "- Variational AutoEncoders (VAE) for probabilistic anomaly scores\n",
    "- LSTM AutoEncoders for time-series anomalies\n",
    "- Convolutional AutoEncoders for image/wafer map anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d12e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Generate synthetic semiconductor test data with anomalies\n",
    "np.random.seed(42)\n",
    "\n",
    "# Normal data (95% of dataset)\n",
    "n_normal = 1900\n",
    "n_features = 15\n",
    "X_normal = np.random.randn(n_normal, n_features)\n",
    "\n",
    "# Anomalous data (5% of dataset) - shifted distribution\n",
    "n_anomalies = 100\n",
    "X_anomalies = np.random.randn(n_anomalies, n_features) + 3.0  # Shifted mean\n",
    "\n",
    "# Combine and create labels\n",
    "X = np.vstack([X_normal, X_anomalies])\n",
    "y = np.concatenate([np.zeros(n_normal), np.ones(n_anomalies)])\n",
    "\n",
    "# Shuffle\n",
    "shuffle_idx = np.random.permutation(len(X))\n",
    "X, y = X[shuffle_idx], y[shuffle_idx]\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split: train only on normal data\n",
    "train_idx = int(0.8 * n_normal)\n",
    "X_train = X_scaled[y == 0][:train_idx]  # Only normal data\n",
    "X_test = X_scaled\n",
    "y_test = y\n",
    "\n",
    "print(\"üéØ Advanced Anomaly Detection with Autoencoders\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Normal samples: {n_normal} ({n_normal/(n_normal+n_anomalies)*100:.1f}%)\")\n",
    "print(f\"Anomalous samples: {n_anomalies} ({n_anomalies/(n_normal+n_anomalies)*100:.1f}%)\")\n",
    "print(f\"Training on: {len(X_train)} normal samples only\")\n",
    "print(f\"Testing on: {len(X_test)} mixed samples\\n\")\n",
    "\n",
    "# Build autoencoder for anomaly detection\n",
    "input_dim = n_features\n",
    "encoding_dim = 6\n",
    "\n",
    "autoencoder = keras.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(encoding_dim, activation='relu', name='bottleneck'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(input_dim, activation='linear')\n",
    "], name='anomaly_detector')\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train on normal data only\n",
    "history = autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model trained successfully\")\n",
    "print(f\"   Final training loss: {history.history['loss'][-1]:.6f}\")\n",
    "print(f\"   Final validation loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "\n",
    "# Compute reconstruction errors\n",
    "X_test_pred = autoencoder.predict(X_test, verbose=0)\n",
    "reconstruction_errors = np.mean((X_test - X_test_pred) ** 2, axis=1)\n",
    "\n",
    "print(f\"\\nüìä Reconstruction Error Statistics:\")\n",
    "normal_errors = reconstruction_errors[y_test == 0]\n",
    "anomaly_errors = reconstruction_errors[y_test == 1]\n",
    "\n",
    "print(f\"   Normal - Mean: {np.mean(normal_errors):.6f}, Std: {np.std(normal_errors):.6f}\")\n",
    "print(f\"   Anomaly - Mean: {np.mean(anomaly_errors):.6f}, Std: {np.std(anomaly_errors):.6f}\")\n",
    "print(f\"   Separation: {np.mean(anomaly_errors) / np.mean(normal_errors):.2f}x\")\n",
    "\n",
    "# Threshold tuning using percentile method\n",
    "percentiles = [90, 95, 99, 99.5, 99.9]\n",
    "print(f\"\\nüéöÔ∏è Threshold Tuning (using normal data percentiles):\")\n",
    "print(f\"{'Percentile':<12} {'Threshold':<12} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "best_f1 = 0\n",
    "best_threshold = 0\n",
    "\n",
    "for p in percentiles:\n",
    "    threshold = np.percentile(normal_errors, p)\n",
    "    predictions = (reconstruction_errors > threshold).astype(int)\n",
    "    \n",
    "    tp = np.sum((predictions == 1) & (y_test == 1))\n",
    "    fp = np.sum((predictions == 1) & (y_test == 0))\n",
    "    fn = np.sum((predictions == 0) & (y_test == 1))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"{p:<12.1f} {threshold:<12.6f} {precision:<12.3f} {recall:<12.3f} {f1:<12.3f}\")\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\n‚úÖ Best threshold: {best_threshold:.6f} (F1: {best_f1:.3f})\")\n",
    "\n",
    "# ROC curve analysis\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, reconstruction_errors)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\"\\nüìà ROC Analysis:\")\n",
    "print(f\"   AUC: {roc_auc:.4f}\")\n",
    "print(f\"   Performance: {'Excellent' if roc_auc > 0.95 else 'Good' if roc_auc > 0.85 else 'Fair'}\")\n",
    "\n",
    "# Precision-Recall curve\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_test, reconstruction_errors)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "print(f\"\\nüìä Precision-Recall Analysis:\")\n",
    "print(f\"   AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# Final predictions with best threshold\n",
    "final_predictions = (reconstruction_errors > best_threshold).astype(int)\n",
    "cm = confusion_matrix(y_test, final_predictions)\n",
    "\n",
    "print(f\"\\nüìã Confusion Matrix:\")\n",
    "print(f\"   TN: {cm[0,0]:<6} FP: {cm[0,1]:<6}\")\n",
    "print(f\"   FN: {cm[1,0]:<6} TP: {cm[1,1]:<6}\")\n",
    "\n",
    "accuracy = (cm[0,0] + cm[1,1]) / np.sum(cm)\n",
    "print(f\"\\n‚úÖ Final Performance:\")\n",
    "print(f\"   Accuracy: {accuracy:.3f}\")\n",
    "print(f\"   Precision: {cm[1,1]/(cm[1,1]+cm[0,1]):.3f}\")\n",
    "print(f\"   Recall: {cm[1,1]/(cm[1,1]+cm[1,0]):.3f}\")\n",
    "print(f\"   F1 Score: {best_f1:.3f}\")\n",
    "\n",
    "print(f\"\\nüè≠ Post-Silicon Validation Application:\")\n",
    "print(f\"   Detected {cm[1,1]} out of {np.sum(y_test==1)} anomalous wafers\")\n",
    "print(f\"   False alarms: {cm[0,1]} out of {np.sum(y_test==0)} normal wafers\")\n",
    "print(f\"   Catch rate: {cm[1,1]/np.sum(y_test==1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec05baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import time\n",
    "\n",
    "class RealTimeAnomalyDetector:\n",
    "    \"\"\"\n",
    "    Real-time anomaly detection system for streaming semiconductor test data.\n",
    "    \n",
    "    Features:\n",
    "    - Sliding window detection\n",
    "    - Adaptive threshold updating\n",
    "    - Concept drift handling\n",
    "    - Streaming pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, autoencoder, initial_threshold, window_size=100):\n",
    "        self.autoencoder = autoencoder\n",
    "        self.threshold = initial_threshold\n",
    "        self.window = deque(maxlen=window_size)\n",
    "        self.error_history = deque(maxlen=1000)\n",
    "        self.drift_detected = False\n",
    "        \n",
    "    def update_threshold(self, alpha=0.1):\n",
    "        \"\"\"Exponential moving average threshold update\"\"\"\n",
    "        if len(self.error_history) > 50:\n",
    "            recent_errors = list(self.error_history)[-50:]\n",
    "            new_threshold = np.percentile(recent_errors, 95)\n",
    "            self.threshold = alpha * new_threshold + (1 - alpha) * self.threshold\n",
    "            \n",
    "    def detect_drift(self):\n",
    "        \"\"\"Detect concept drift using error distribution changes\"\"\"\n",
    "        if len(self.error_history) < 200:\n",
    "            return False\n",
    "        \n",
    "        recent = list(self.error_history)[-100:]\n",
    "        historical = list(self.error_history)[-200:-100]\n",
    "        \n",
    "        # Compare distributions using mean and std\n",
    "        recent_mean = np.mean(recent)\n",
    "        hist_mean = np.mean(historical)\n",
    "        hist_std = np.std(historical)\n",
    "        \n",
    "        # Drift if recent mean shifts significantly\n",
    "        drift = abs(recent_mean - hist_mean) > 2 * hist_std\n",
    "        \n",
    "        if drift:\n",
    "            self.drift_detected = True\n",
    "            print(f\"‚ö†Ô∏è Concept drift detected! Recent mean: {recent_mean:.6f}, Historical: {hist_mean:.6f}\")\n",
    "            \n",
    "        return drift\n",
    "        \n",
    "    def process_sample(self, sample):\n",
    "        \"\"\"Process a single incoming sample\"\"\"\n",
    "        # Normalize\n",
    "        sample_scaled = scaler.transform(sample.reshape(1, -1))\n",
    "        \n",
    "        # Predict and compute error\n",
    "        reconstruction = self.autoencoder.predict(sample_scaled, verbose=0)\n",
    "        error = np.mean((sample_scaled - reconstruction) ** 2)\n",
    "        \n",
    "        # Update history\n",
    "        self.error_history.append(error)\n",
    "        self.window.append(error)\n",
    "        \n",
    "        # Detect anomaly\n",
    "        is_anomaly = error > self.threshold\n",
    "        \n",
    "        # Periodically update threshold and check drift\n",
    "        if len(self.error_history) % 50 == 0:\n",
    "            self.update_threshold()\n",
    "            self.detect_drift()\n",
    "            \n",
    "        return {\n",
    "            'is_anomaly': is_anomaly,\n",
    "            'error': error,\n",
    "            'threshold': self.threshold,\n",
    "            'confidence': min(1.0, error / self.threshold if is_anomaly else self.threshold / error)\n",
    "        }\n",
    "\n",
    "# Initialize real-time detector\n",
    "rt_detector = RealTimeAnomalyDetector(autoencoder, best_threshold)\n",
    "\n",
    "print(\"üöÄ Real-Time Anomaly Detection System\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simulate streaming data\n",
    "print(\"\\nüìä Processing streaming test data...\")\n",
    "print(f\"{'Sample #':<10} {'Error':<15} {'Threshold':<15} {'Status':<10} {'Confidence':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "stream_results = []\n",
    "anomaly_count = 0\n",
    "\n",
    "# Simulate 50 samples\n",
    "for i in range(50):\n",
    "    # Randomly choose normal or anomaly\n",
    "    if np.random.random() < 0.9:\n",
    "        sample = np.random.randn(n_features)  # Normal\n",
    "        true_label = 0\n",
    "    else:\n",
    "        sample = np.random.randn(n_features) + 3.0  # Anomaly\n",
    "        true_label = 1\n",
    "    \n",
    "    result = rt_detector.process_sample(sample)\n",
    "    stream_results.append((result['is_anomaly'], true_label))\n",
    "    \n",
    "    if result['is_anomaly']:\n",
    "        anomaly_count += 1\n",
    "        \n",
    "    # Print every 10th sample\n",
    "    if i % 10 == 9:\n",
    "        status = \"üö® ANOMALY\" if result['is_anomaly'] else \"‚úÖ Normal\"\n",
    "        print(f\"{i+1:<10} {result['error']:<15.6f} {result['threshold']:<15.6f} {status:<10} {result['confidence']:<12.3f}\")\n",
    "\n",
    "# Performance metrics\n",
    "stream_preds = [r[0] for r in stream_results]\n",
    "stream_labels = [r[1] for r in stream_results]\n",
    "stream_accuracy = np.mean([p == l for p, l in zip(stream_preds, stream_labels)])\n",
    "\n",
    "print(f\"\\n‚úÖ Real-Time Detection Performance:\")\n",
    "print(f\"   Samples processed: {len(stream_results)}\")\n",
    "print(f\"   Anomalies detected: {anomaly_count}\")\n",
    "print(f\"   Accuracy: {stream_accuracy:.3f}\")\n",
    "print(f\"   Final threshold: {rt_detector.threshold:.6f}\")\n",
    "print(f\"   Threshold adjusted: {abs(rt_detector.threshold - best_threshold) > 0.01}\")\n",
    "\n",
    "print(f\"\\nüè≠ Production Deployment Considerations:\")\n",
    "print(f\"   Latency: <10ms per sample (single prediction)\")\n",
    "print(f\"   Memory: ~{len(rt_detector.error_history) * 8 / 1024:.1f}KB (error history)\")\n",
    "print(f\"   Throughput: ~100 samples/sec (single thread)\")\n",
    "print(f\"   Adaptation: Threshold updates every 50 samples\")\n",
    "print(f\"   Drift detection: Checked every 50 samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea44bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "print(\"üåê Multivariate Anomaly Detection Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Method 1: Autoencoder (already computed)\n",
    "ae_predictions = (reconstruction_errors > best_threshold).astype(int)\n",
    "ae_accuracy = np.mean(ae_predictions == y_test)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ Autoencoder Method:\")\n",
    "print(f\"   Accuracy: {ae_accuracy:.3f}\")\n",
    "print(f\"   Advantages: Learns complex nonlinear patterns, good for high-dim data\")\n",
    "print(f\"   Disadvantages: Needs training, hyperparameter tuning\")\n",
    "\n",
    "# Method 2: Mahalanobis Distance\n",
    "print(\"\\n2Ô∏è‚É£ Mahalanobis Distance:\")\n",
    "X_train_normal = X_scaled[y == 0][:train_idx]\n",
    "mean = np.mean(X_train_normal, axis=0)\n",
    "cov = np.cov(X_train_normal, rowvar=False)\n",
    "cov_inv = np.linalg.pinv(cov)  # Pseudo-inverse for stability\n",
    "\n",
    "mahal_distances = np.array([\n",
    "    mahalanobis(x, mean, cov_inv) for x in X_test\n",
    "])\n",
    "\n",
    "# Threshold using chi-squared distribution (95th percentile)\n",
    "mahal_threshold = np.percentile(mahal_distances[y_test == 0], 99)\n",
    "mahal_predictions = (mahal_distances > mahal_threshold).astype(int)\n",
    "mahal_accuracy = np.mean(mahal_predictions == y_test)\n",
    "\n",
    "print(f\"   Accuracy: {mahal_accuracy:.3f}\")\n",
    "print(f\"   Threshold: {mahal_threshold:.2f}\")\n",
    "print(f\"   Advantages: Statistical foundation, interpretable, fast\")\n",
    "print(f\"   Disadvantages: Assumes Gaussian, sensitive to outliers in training\")\n",
    "\n",
    "# Method 3: Isolation Forest\n",
    "print(\"\\n3Ô∏è‚É£ Isolation Forest:\")\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.05,  # Expected proportion of anomalies\n",
    "    random_state=42,\n",
    "    n_estimators=100\n",
    ")\n",
    "iso_forest.fit(X_train_normal)\n",
    "iso_predictions = iso_forest.predict(X_test)\n",
    "iso_predictions = (iso_predictions == -1).astype(int)  # -1 = anomaly\n",
    "iso_accuracy = np.mean(iso_predictions == y_test)\n",
    "\n",
    "print(f\"   Accuracy: {iso_accuracy:.3f}\")\n",
    "print(f\"   Contamination: 5%\")\n",
    "print(f\"   Advantages: No assumptions, handles outliers, fast\")\n",
    "print(f\"   Disadvantages: Less interpretable, needs contamination estimate\")\n",
    "\n",
    "# Method 4: Local Outlier Factor\n",
    "print(\"\\n4Ô∏è‚É£ Local Outlier Factor (LOF):\")\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=20,\n",
    "    contamination=0.05,\n",
    "    novelty=True  # For use on test data\n",
    ")\n",
    "lof.fit(X_train_normal)\n",
    "lof_predictions = lof.predict(X_test)\n",
    "lof_predictions = (lof_predictions == -1).astype(int)\n",
    "lof_accuracy = np.mean(lof_predictions == y_test)\n",
    "\n",
    "print(f\"   Accuracy: {lof_accuracy:.3f}\")\n",
    "print(f\"   Neighbors: 20\")\n",
    "print(f\"   Advantages: Finds local density anomalies, no global assumptions\")\n",
    "print(f\"   Disadvantages: Computationally expensive, needs n_neighbors tuning\")\n",
    "\n",
    "# Comparison summary\n",
    "print(\"\\nüìä Method Comparison Summary:\")\n",
    "print(f\"{'Method':<25} {'Accuracy':<12} {'Speed':<15} {'Best For':<30}\")\n",
    "print(\"-\" * 85)\n",
    "print(f\"{'Autoencoder':<25} {ae_accuracy:<12.3f} {'Medium':<15} {'Complex patterns, high-dim':<30}\")\n",
    "print(f\"{'Mahalanobis Distance':<25} {mahal_accuracy:<12.3f} {'Fast':<15} {'Gaussian data, real-time':<30}\")\n",
    "print(f\"{'Isolation Forest':<25} {iso_accuracy:<12.3f} {'Fast':<15} {'Mixed distributions':<30}\")\n",
    "print(f\"{'Local Outlier Factor':<25} {lof_accuracy:<12.3f} {'Slow':<15} {'Local density anomalies':<30}\")\n",
    "\n",
    "# Ensemble approach\n",
    "print(\"\\nüîÄ Ensemble Approach (Voting):\")\n",
    "ensemble_predictions = (\n",
    "    ae_predictions + \n",
    "    mahal_predictions + \n",
    "    iso_predictions + \n",
    "    lof_predictions\n",
    ") >= 2  # At least 2 methods agree\n",
    "\n",
    "ensemble_accuracy = np.mean(ensemble_predictions == y_test)\n",
    "print(f\"   Accuracy: {ensemble_accuracy:.3f}\")\n",
    "print(f\"   Strategy: Majority voting (‚â•2 out of 4 methods)\")\n",
    "print(f\"   Advantage: More robust, reduces false positives\")\n",
    "\n",
    "# Feature correlation analysis\n",
    "print(\"\\nüîó Feature Correlation Impact:\")\n",
    "corr_matrix = np.corrcoef(X_train_normal.T)\n",
    "avg_corr = np.mean(np.abs(corr_matrix[np.triu_indices_from(corr_matrix, k=1)]))\n",
    "print(f\"   Average feature correlation: {avg_corr:.3f}\")\n",
    "print(f\"   High correlation (>{avg_corr:.2f}): {np.sum(np.abs(corr_matrix) > avg_corr) // 2} pairs\")\n",
    "print(f\"   Impact: {'High' if avg_corr > 0.5 else 'Moderate' if avg_corr > 0.3 else 'Low'} - \"\n",
    "      f\"{'Autoencoders excel' if avg_corr > 0.5 else 'All methods viable'}\")\n",
    "\n",
    "print(f\"\\nüè≠ Post-Silicon Application Guidance:\")\n",
    "print(f\"   ‚úÖ Use Autoencoder when: High-dimensional parametric test data (>20 params)\")\n",
    "print(f\"   ‚úÖ Use Mahalanobis when: Real-time detection needed, data ~Gaussian\")\n",
    "print(f\"   ‚úÖ Use Isolation Forest when: Unknown anomaly patterns, mixed distributions\")\n",
    "print(f\"   ‚úÖ Use LOF when: Detecting wafer map spatial clusters\")\n",
    "print(f\"   ‚úÖ Use Ensemble when: Critical decisions (false positives costly)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c1a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "print(\"‚è±Ô∏è Time-Series Anomaly Detection with LSTM Autoencoder\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Generate synthetic time-series data\n",
    "np.random.seed(42)\n",
    "n_timesteps = 1000\n",
    "n_features_ts = 5\n",
    "\n",
    "# Create normal pattern (sine wave with noise)\n",
    "t = np.linspace(0, 100, n_timesteps)\n",
    "normal_pattern = np.column_stack([\n",
    "    np.sin(t / 5 + i) + np.random.randn(n_timesteps) * 0.1\n",
    "    for i in range(n_features_ts)\n",
    "])\n",
    "\n",
    "# Inject anomalies (sudden spikes)\n",
    "anomaly_indices = [200, 450, 750]\n",
    "for idx in anomaly_indices:\n",
    "    normal_pattern[idx:idx+10] += 5.0  # Spike anomaly\n",
    "\n",
    "# Normalize\n",
    "ts_scaler = StandardScaler()\n",
    "ts_data = ts_scaler.fit_transform(normal_pattern)\n",
    "\n",
    "# Create sequences (sliding window)\n",
    "sequence_length = 20\n",
    "X_sequences = []\n",
    "y_labels = []\n",
    "\n",
    "for i in range(len(ts_data) - sequence_length):\n",
    "    X_sequences.append(ts_data[i:i+sequence_length])\n",
    "    # Label as anomaly if any point in sequence is anomalous\n",
    "    is_anomaly = any(abs(i - idx) < 10 for idx in anomaly_indices)\n",
    "    y_labels.append(1 if is_anomaly else 0)\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_labels = np.array(y_labels)\n",
    "\n",
    "print(f\"üìä Time-Series Dataset:\")\n",
    "print(f\"   Total timesteps: {n_timesteps}\")\n",
    "print(f\"   Features per timestep: {n_features_ts}\")\n",
    "print(f\"   Sequence length: {sequence_length}\")\n",
    "print(f\"   Number of sequences: {len(X_sequences)}\")\n",
    "print(f\"   Anomalous sequences: {np.sum(y_labels)} ({np.sum(y_labels)/len(y_labels)*100:.1f}%)\")\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.7 * len(X_sequences))\n",
    "X_train_ts = X_sequences[:train_size]\n",
    "X_test_ts = X_sequences[train_size:]\n",
    "y_test_ts = y_labels[train_size:]\n",
    "\n",
    "# Build LSTM Autoencoder\n",
    "latent_dim = 10\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = layers.Input(shape=(sequence_length, n_features_ts))\n",
    "x = layers.LSTM(64, activation='relu', return_sequences=True)(encoder_inputs)\n",
    "x = layers.LSTM(32, activation='relu', return_sequences=False)(x)\n",
    "latent = layers.Dense(latent_dim, activation='relu', name='latent')(x)\n",
    "\n",
    "# Decoder\n",
    "x = layers.RepeatVector(sequence_length)(latent)\n",
    "x = layers.LSTM(32, activation='relu', return_sequences=True)(x)\n",
    "x = layers.LSTM(64, activation='relu', return_sequences=True)(x)\n",
    "decoder_outputs = layers.TimeDistributed(layers.Dense(n_features_ts))(x)\n",
    "\n",
    "# Full autoencoder\n",
    "lstm_autoencoder = Model(encoder_inputs, decoder_outputs, name='lstm_autoencoder')\n",
    "lstm_autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(f\"\\nüèóÔ∏è LSTM Autoencoder Architecture:\")\n",
    "print(f\"   Encoder: Input({sequence_length}, {n_features_ts}) ‚Üí LSTM(64) ‚Üí LSTM(32) ‚Üí Dense({latent_dim})\")\n",
    "print(f\"   Decoder: RepeatVector({sequence_length}) ‚Üí LSTM(32) ‚Üí LSTM(64) ‚Üí TimeDistributed(Dense({n_features_ts}))\")\n",
    "print(f\"   Parameters: {lstm_autoencoder.count_params():,}\")\n",
    "\n",
    "# Train\n",
    "history_ts = lstm_autoencoder.fit(\n",
    "    X_train_ts, X_train_ts,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete:\")\n",
    "print(f\"   Final loss: {history_ts.history['loss'][-1]:.6f}\")\n",
    "print(f\"   Final val_loss: {history_ts.history['val_loss'][-1]:.6f}\")\n",
    "\n",
    "# Predict and compute sequence-level reconstruction errors\n",
    "X_test_pred_ts = lstm_autoencoder.predict(X_test_ts, verbose=0)\n",
    "sequence_errors = np.mean((X_test_ts - X_test_pred_ts) ** 2, axis=(1, 2))\n",
    "\n",
    "# Threshold determination\n",
    "normal_seq_errors = sequence_errors[y_test_ts == 0]\n",
    "ts_threshold = np.percentile(normal_seq_errors, 95)\n",
    "\n",
    "# Predictions\n",
    "ts_predictions = (sequence_errors > ts_threshold).astype(int)\n",
    "ts_accuracy = np.mean(ts_predictions == y_test_ts)\n",
    "\n",
    "print(f\"\\nüìä Time-Series Detection Performance:\")\n",
    "print(f\"   Threshold: {ts_threshold:.6f}\")\n",
    "print(f\"   Accuracy: {ts_accuracy:.3f}\")\n",
    "\n",
    "# Temporal pattern analysis\n",
    "print(f\"\\nüîç Temporal Pattern Analysis:\")\n",
    "print(f\"   Normal sequence error - Mean: {np.mean(normal_seq_errors):.6f}, Std: {np.std(normal_seq_errors):.6f}\")\n",
    "print(f\"   Anomaly sequence error - Mean: {np.mean(sequence_errors[y_test_ts==1]):.6f}, Std: {np.std(sequence_errors[y_test_ts==1]):.6f}\")\n",
    "print(f\"   Separation ratio: {np.mean(sequence_errors[y_test_ts==1]) / np.mean(normal_seq_errors):.2f}x\")\n",
    "\n",
    "# Change point detection\n",
    "print(f\"\\nüìç Change Point Detection:\")\n",
    "window_size = 50\n",
    "change_points = []\n",
    "\n",
    "for i in range(len(sequence_errors) - window_size):\n",
    "    window = sequence_errors[i:i+window_size]\n",
    "    if np.mean(window) > 3 * np.std(normal_seq_errors):\n",
    "        change_points.append(i + train_size)\n",
    "        \n",
    "print(f\"   Detected {len(change_points)} change points\")\n",
    "if change_points:\n",
    "    print(f\"   First change point at sequence {change_points[0]} (timestep ~{change_points[0] + sequence_length})\")\n",
    "\n",
    "print(f\"\\nüè≠ Post-Silicon Time-Series Applications:\")\n",
    "print(f\"   ‚úÖ Equipment drift monitoring: Detect gradual parameter shifts\")\n",
    "print(f\"   ‚úÖ Test station anomalies: Identify sudden calibration issues\")\n",
    "print(f\"   ‚úÖ Yield trend analysis: Flag unexpected yield drops\")\n",
    "print(f\"   ‚úÖ Thermal cycling tests: Detect abnormal temperature patterns\")\n",
    "print(f\"   ‚úÖ Burn-in failures: Early prediction from power consumption trends\")\n",
    "\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "print(f\"   ‚Ä¢ LSTM captures temporal dependencies (sequence context)\")\n",
    "print(f\"   ‚Ä¢ Sequence-level errors smoother than point-wise\")\n",
    "print(f\"   ‚Ä¢ Good for: Gradual drifts, periodic patterns, multi-step anomalies\")\n",
    "print(f\"   ‚Ä¢ Latency: ~{sequence_length} timesteps (need full sequence)\")\n",
    "print(f\"   ‚Ä¢ Trade-off: Longer sequences = better context, but higher latency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: Reconstruction Error Distribution with Threshold\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "bins = np.linspace(0, max(reconstruction_errors), 50)\n",
    "ax1.hist(reconstruction_errors[y_test==0], bins=bins, alpha=0.6, label='Normal', color='#2ecc71', edgecolor='black')\n",
    "ax1.hist(reconstruction_errors[y_test==1], bins=bins, alpha=0.6, label='Anomaly', color='#e74c3c', edgecolor='black')\n",
    "ax1.axvline(best_threshold, color='#f39c12', linestyle='--', linewidth=2, label=f'Threshold = {best_threshold:.4f}')\n",
    "ax1.set_xlabel('Reconstruction Error', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Reconstruction Error Distribution', fontsize=13, fontweight='bold', pad=15)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add statistics text\n",
    "normal_median = np.median(reconstruction_errors[y_test==0])\n",
    "anomaly_median = np.median(reconstruction_errors[y_test==1])\n",
    "ax1.text(0.98, 0.97, f'Normal median: {normal_median:.4f}\\nAnomaly median: {anomaly_median:.4f}\\nSeparation: {anomaly_median/normal_median:.1f}x',\n",
    "         transform=ax1.transAxes, ha='right', va='top', fontsize=9,\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "# Plot 2: ROC and Precision-Recall Curves\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(fpr, tpr, color='#3498db', linewidth=2.5, label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', linewidth=1.5, alpha=0.5)\n",
    "ax2_twin = ax2.twinx()\n",
    "ax2_twin.plot(recall, precision, color='#9b59b6', linewidth=2.5, linestyle='--', label=f'PR (AUC = {pr_auc:.3f})')\n",
    "ax2.set_xlabel('False Positive Rate / Recall', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold', color='#3498db')\n",
    "ax2_twin.set_ylabel('Precision', fontsize=11, fontweight='bold', color='#9b59b6')\n",
    "ax2.set_title('ROC & Precision-Recall Curves', fontsize=13, fontweight='bold', pad=15)\n",
    "ax2.legend(loc='lower right', fontsize=10)\n",
    "ax2_twin.legend(loc='upper right', fontsize=10)\n",
    "ax2.grid(alpha=0.3, linestyle='--')\n",
    "ax2.tick_params(axis='y', labelcolor='#3498db')\n",
    "ax2_twin.tick_params(axis='y', labelcolor='#9b59b6')\n",
    "\n",
    "# Plot 3: Time-Series Detection Timeline\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "timeline = np.arange(len(sequence_errors)) + train_size\n",
    "ax3.plot(timeline, sequence_errors, color='#34495e', linewidth=1, alpha=0.7, label='Reconstruction Error')\n",
    "ax3.axhline(ts_threshold, color='#e74c3c', linestyle='--', linewidth=2, label=f'Threshold = {ts_threshold:.4f}')\n",
    "anomaly_mask = y_test_ts == 1\n",
    "ax3.scatter(timeline[anomaly_mask], sequence_errors[anomaly_mask], \n",
    "           color='#e74c3c', s=50, zorder=5, label='True Anomalies', marker='X')\n",
    "detected_mask = ts_predictions == 1\n",
    "ax3.scatter(timeline[detected_mask], sequence_errors[detected_mask],\n",
    "           facecolors='none', edgecolors='#f39c12', s=100, linewidths=2, \n",
    "           zorder=4, label='Detected', marker='o')\n",
    "ax3.set_xlabel('Sequence Index', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Reconstruction Error', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Time-Series Anomaly Detection Timeline', fontsize=13, fontweight='bold', pad=15)\n",
    "ax3.legend(fontsize=10, loc='upper left')\n",
    "ax3.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# Highlight change points\n",
    "for cp in change_points[:3]:  # Show first 3\n",
    "    ax3.axvline(cp, color='#95a5a6', linestyle=':', linewidth=1.5, alpha=0.6)\n",
    "    ax3.text(cp, ax3.get_ylim()[1]*0.95, 'Change', rotation=90, fontsize=8, ha='right', va='top')\n",
    "\n",
    "# Plot 4: Method Comparison (Confusion Matrices)\n",
    "ax4 = fig.add_subplot(gs[2, 0])\n",
    "methods = ['Autoencoder', 'Mahalanobis', 'Iso Forest', 'LOF']\n",
    "accuracies = [ae_accuracy, mahal_accuracy, iso_accuracy, lof_accuracy]\n",
    "colors_bar = ['#3498db', '#2ecc71', '#f39c12', '#9b59b6']\n",
    "bars = ax4.barh(methods, accuracies, color=colors_bar, edgecolor='black', linewidth=1.5)\n",
    "ax4.set_xlabel('Accuracy', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('Method Comparison', fontsize=13, fontweight='bold', pad=15)\n",
    "ax4.set_xlim(0, 1)\n",
    "ax4.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
    "    ax4.text(acc + 0.02, i, f'{acc:.3f}', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Add best indicator\n",
    "best_idx = np.argmax(accuracies)\n",
    "bars[best_idx].set_edgecolor('#e74c3c')\n",
    "bars[best_idx].set_linewidth(3)\n",
    "\n",
    "# Plot 5: Ensemble Confusion Matrix\n",
    "ax5 = fig.add_subplot(gs[2, 1])\n",
    "cm_ensemble = confusion_matrix(y_test, ensemble_predictions)\n",
    "sns.heatmap(cm_ensemble, annot=True, fmt='d', cmap='Blues', cbar=True, \n",
    "            xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'],\n",
    "            ax=ax5, linewidths=2, linecolor='black', annot_kws={'fontsize': 14, 'fontweight': 'bold'})\n",
    "ax5.set_xlabel('Predicted', fontsize=11, fontweight='bold')\n",
    "ax5.set_ylabel('Actual', fontsize=11, fontweight='bold')\n",
    "ax5.set_title(f'Ensemble Confusion Matrix (Acc: {ensemble_accuracy:.3f})', fontsize=13, fontweight='bold', pad=15)\n",
    "\n",
    "# Add performance metrics text\n",
    "tn, fp, fn, tp = cm_ensemble.ravel()\n",
    "precision_ens = tp / (tp + fp)\n",
    "recall_ens = tp / (tp + fn)\n",
    "f1_ens = 2 * precision_ens * recall_ens / (precision_ens + recall_ens)\n",
    "\n",
    "metrics_text = f'Precision: {precision_ens:.3f}\\nRecall: {recall_ens:.3f}\\nF1: {f1_ens:.3f}\\nFPR: {fp/(fp+tn):.3f}'\n",
    "ax5.text(1.35, 0.5, metrics_text, transform=ax5.transAxes, fontsize=10,\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n",
    "        verticalalignment='center')\n",
    "\n",
    "plt.suptitle('üîç Autoencoder Anomaly Detection - Comprehensive Analysis', \n",
    "            fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.savefig('autoencoder_anomaly_detection_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved as 'autoencoder_anomaly_detection_analysis.png'\")\n",
    "print(\"\\nüìä Analysis Summary:\")\n",
    "print(f\"   Best individual method: {methods[best_idx]} ({accuracies[best_idx]:.3f})\")\n",
    "print(f\"   Ensemble accuracy: {ensemble_accuracy:.3f}\")\n",
    "print(f\"   Improvement: {(ensemble_accuracy - accuracies[best_idx])*100:+.1f}%\")\n",
    "print(f\"   False positive rate: {fp/(fp+tn):.3f} (critical for production)\")\n",
    "print(f\"   Time-series accuracy: {ts_accuracy:.3f}\")\n",
    "print(f\"   ROC AUC: {roc_auc:.3f} - {'Excellent' if roc_auc > 0.95 else 'Good' if roc_auc > 0.85 else 'Fair'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85fc593",
   "metadata": {},
   "source": [
    "## üöÄ Real-World Projects\n",
    "\n",
    "### Project 1: Equipment Drift Detection System üè≠\n",
    "**Objective:** Real-time monitoring of test equipment drift to prevent false failures  \n",
    "**Business Value:** $5M annual savings from reduced unnecessary equipment maintenance and false rejects\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "STDF Stream ‚Üí Feature Extraction ‚Üí LSTM Autoencoder ‚Üí Drift Score ‚Üí Alert Dashboard\n",
    "                      ‚Üì\n",
    "              Historical Baseline\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- Multi-parameter monitoring (Vdd, Idd, frequency, temperature)\n",
    "- Adaptive threshold with seasonal adjustment\n",
    "- Equipment-specific baseline models\n",
    "- Automated calibration recommendations\n",
    "- ROI: Detect drift 3-5 days before failures, 85% reduction in unplanned downtime\n",
    "\n",
    "**Implementation Tips:**\n",
    "- Sequence length: 100-200 test cycles (~1-2 hours)\n",
    "- Update baseline weekly with concept drift detection\n",
    "- Use ensemble (Autoencoder + Mahalanobis) for robustness\n",
    "- Alert severity levels: Warning (95th%), Critical (99th%)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 2: Wafer-Level Outlier Detection üî¨\n",
    "**Objective:** Identify anomalous wafers before costly packaging  \n",
    "**Business Value:** 99.2% accuracy, prevent $2M/year in packaging costs for defective wafers\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Parametric Test Data ‚Üí Spatial Feature Engineering ‚Üí Autoencoder ‚Üí Outlier Score ‚Üí Bin Decision\n",
    "         ‚Üì                        ‚Üì\n",
    "   Die-level stats          Wafer map patterns\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- 50+ parametric test features per die\n",
    "- Spatial correlation features (neighbor statistics)\n",
    "- Multi-level detection (die, wafer, lot)\n",
    "- Integration with MES system for auto-binning\n",
    "- ROI: Catch 98% of problem wafers pre-packaging\n",
    "\n",
    "**Implementation Tips:**\n",
    "- Train separate models per product family\n",
    "- Feature engineering: mean, std, spatial gradients, edge effects\n",
    "- Ensemble with Isolation Forest for robustness\n",
    "- Retrain monthly with production data feedback\n",
    "\n",
    "---\n",
    "\n",
    "### Project 3: Sensor Fault Detection in Test Stations üì°\n",
    "**Objective:** Detect faulty sensors causing test accuracy degradation  \n",
    "**Business Value:** 30% reduction in false test escapes, improved product quality\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Multi-Sensor Stream ‚Üí Correlation Analysis ‚Üí VAE ‚Üí Sensor Health Score ‚Üí Maintenance Queue\n",
    "                              ‚Üì\n",
    "                    Cross-sensor validation\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- Monitors 20+ sensors per test station (voltage, current, temp, pressure)\n",
    "- Cross-correlation anomaly detection\n",
    "- Predictive maintenance scheduling\n",
    "- Sensor-specific degradation curves\n",
    "- ROI: Proactive replacement before critical failures\n",
    "\n",
    "**Implementation Tips:**\n",
    "- Use VAE for probabilistic outlier scoring\n",
    "- Compare sensor readings across multiple test stations\n",
    "- Time-series analysis for gradual drift\n",
    "- Alert when 2+ sensors show correlated anomalies\n",
    "\n",
    "---\n",
    "\n",
    "### Project 4: Production Line Anomaly Alerting System üö®\n",
    "**Objective:** Real-time detection of production line issues with automated escalation  \n",
    "**Business Value:** 40% faster incident response, $3M annual yield improvement\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Edge Devices ‚Üí Streaming Pipeline (Kafka) ‚Üí Real-Time AE ‚Üí Anomaly DB ‚Üí Dashboard + Alerts\n",
    "                                                   ‚Üì\n",
    "                                          Context-aware rules\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- Sub-second detection latency (<500ms)\n",
    "- Context-aware alerting (shift, product, line)\n",
    "- Automated escalation workflow\n",
    "- Root cause analysis suggestions\n",
    "- ROI: Reduce yield loss by catching issues within 15 minutes\n",
    "\n",
    "**Implementation Tips:**\n",
    "- Deploy model on edge for ultra-low latency\n",
    "- Use lightweight autoencoder (quantized, pruned)\n",
    "- Batch inference for throughput (1000 samples/sec)\n",
    "- Integrate with existing SCADA/MES systems\n",
    "- Multi-tier alerting: Email ‚Üí SMS ‚Üí Pager for severity levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b5b75d",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways & Best Practices\n",
    "\n",
    "### üìã Threshold Selection Decision Matrix\n",
    "\n",
    "| **Scenario** | **Method** | **Rationale** | **Typical Value** |\n",
    "|-------------|-----------|--------------|------------------|\n",
    "| High imbalance (>99% normal) | Percentile (99-99.9%) | Robust to extreme outliers | 99th percentile of normal |\n",
    "| Real-time, low latency | MAD (Median Absolute Deviation) | Fast, robust | Median + 3√óMAD |\n",
    "| Time-series with drift | Adaptive (EMA) | Tracks distribution changes | Œ±=0.1, recompute every 50 samples |\n",
    "| Critical applications | Ensemble voting | Reduces false positives | ‚â•2/4 methods agree |\n",
    "| Known contamination rate | Contamination-based | Matches expected anomaly % | Set contamination=0.05 for 5% |\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è Architecture Design Principles\n",
    "\n",
    "**1. Compression Ratio Selection:**\n",
    "- **Rule of thumb:** 5-10x compression for anomaly detection\n",
    "- **Example:** 50 features ‚Üí 5-10 latent dimensions\n",
    "- **Too aggressive (>15x):** Loss of discriminative information\n",
    "- **Too conservative (<3x):** Model memorizes anomalies\n",
    "\n",
    "**2. Training Strategy:**\n",
    "- ‚úÖ **DO:** Train only on normal data (clean baseline)\n",
    "- ‚úÖ **DO:** Use validation set to tune bottleneck size\n",
    "- ‚ùå **DON'T:** Include anomalies in training (contaminates baseline)\n",
    "- ‚ùå **DON'T:** Overtrain (leads to memorization)\n",
    "\n",
    "**3. Network Depth:**\n",
    "- **Simple patterns:** 2-3 hidden layers (Input ‚Üí 64 ‚Üí 32 ‚Üí 8 ‚Üí 32 ‚Üí 64 ‚Üí Output)\n",
    "- **Complex patterns:** 4-5 layers with skip connections\n",
    "- **Time-series:** LSTM autoencoder with 2-3 LSTM layers\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Training Best Practices\n",
    "\n",
    "**Data Preparation:**\n",
    "```python\n",
    "# 1. Normalize features (critical for autoencoders)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_normal)  # Only normal data\n",
    "\n",
    "# 2. Train/val split (only normal)\n",
    "X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "```\n",
    "\n",
    "**Hyperparameters:**\n",
    "- **Epochs:** 50-100 (with early stopping)\n",
    "- **Batch size:** 32-128 (larger for stable gradients)\n",
    "- **Learning rate:** 0.001 (Adam optimizer)\n",
    "- **Activation:** ReLU for hidden layers, linear for output\n",
    "- **Loss:** MSE for reconstruction\n",
    "\n",
    "**Monitoring:**\n",
    "- Track validation loss (should plateau, not increase)\n",
    "- Visualize reconstruction quality on normal samples\n",
    "- Check latent space distribution (should be compact)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Common Pitfalls & Solutions\n",
    "\n",
    "**Pitfall 1: Training on contaminated data**\n",
    "- **Symptom:** Low recall, model learns anomalies as normal\n",
    "- **Solution:** Carefully clean training data, use domain knowledge to filter outliers\n",
    "- **Code:**\n",
    "```python\n",
    "# Remove outliers from training using IQR\n",
    "Q1 = np.percentile(X_train, 25, axis=0)\n",
    "Q3 = np.percentile(X_train, 75, axis=0)\n",
    "IQR = Q3 - Q1\n",
    "mask = np.all((X_train > Q1 - 1.5*IQR) & (X_train < Q3 + 1.5*IQR), axis=1)\n",
    "X_train_clean = X_train[mask]\n",
    "```\n",
    "\n",
    "**Pitfall 2: Fixed threshold in production**\n",
    "- **Symptom:** Increasing false positives over time (concept drift)\n",
    "- **Solution:** Implement adaptive thresholding with periodic retraining\n",
    "- **Code:**\n",
    "```python\n",
    "# Exponential moving average threshold\n",
    "threshold_ema = alpha * new_threshold + (1 - alpha) * threshold_ema\n",
    "# Retrain trigger: if drift detected or monthly schedule\n",
    "```\n",
    "\n",
    "**Pitfall 3: Ignoring imbalanced evaluation**\n",
    "- **Symptom:** High accuracy but poor recall (missing anomalies)\n",
    "- **Solution:** Use F1, ROC-AUC, and PR-AUC instead of accuracy\n",
    "- **Code:**\n",
    "```python\n",
    "# Comprehensive evaluation\n",
    "roc_auc = roc_auc_score(y_true, anomaly_scores)\n",
    "pr_auc = average_precision_score(y_true, anomaly_scores)\n",
    "f1 = f1_score(y_true, predictions)\n",
    "```\n",
    "\n",
    "**Pitfall 4: Overfitting to training data**\n",
    "- **Symptom:** Low training error, high validation error\n",
    "- **Solution:** Regularization, dropout, early stopping\n",
    "- **Code:**\n",
    "```python\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(1e-4)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(1e-4)),\n",
    "    # ...\n",
    "])\n",
    "```\n",
    "\n",
    "**Pitfall 5: Not handling time-series dependencies**\n",
    "- **Symptom:** Poor performance on temporal anomalies\n",
    "- **Solution:** Use LSTM/GRU autoencoder instead of feedforward\n",
    "- **Code:**\n",
    "```python\n",
    "# LSTM for time-series\n",
    "encoder = LSTM(64, return_sequences=True)(input)\n",
    "latent = LSTM(32, return_sequences=False)(encoder)\n",
    "decoder = RepeatVector(sequence_length)(latent)\n",
    "output = LSTM(64, return_sequences=True)(decoder)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**1. Parametric Test Outlier Detection:**\n",
    "- **Data:** Vdd, Idd, frequency, power measurements\n",
    "- **Approach:** Dense autoencoder with 50+ features ‚Üí 5D latent\n",
    "- **Threshold:** 99.5th percentile (high yield products)\n",
    "- **Impact:** Identify marginal devices, improve guardbands\n",
    "\n",
    "**2. Wafer Map Spatial Anomalies:**\n",
    "- **Data:** Die-level pass/fail + parametric data\n",
    "- **Approach:** CNN autoencoder for spatial patterns + dense for parametrics\n",
    "- **Threshold:** Per-wafer adaptive (accounts for process variation)\n",
    "- **Impact:** Early lot disposition, yield learning\n",
    "\n",
    "**3. Test Time Anomalies:**\n",
    "- **Data:** Test execution times per test\n",
    "- **Approach:** Time-series LSTM autoencoder\n",
    "- **Threshold:** 95th percentile (test time less critical)\n",
    "- **Impact:** Detect equipment slowdowns, optimize test flow\n",
    "\n",
    "**4. Multi-Site Correlation Anomalies:**\n",
    "- **Data:** Same DUT tested on multiple test stations\n",
    "- **Approach:** Variational autoencoder for cross-site patterns\n",
    "- **Threshold:** Mahalanobis distance on latent space\n",
    "- **Impact:** Identify rogue test stations, improve test repeatability\n",
    "\n",
    "**5. Burn-In Failure Prediction:**\n",
    "- **Data:** Power, temperature, voltage over 48-168 hours\n",
    "- **Approach:** LSTM autoencoder with 1-hour windows\n",
    "- **Threshold:** Adaptive (updated every 24 hours)\n",
    "- **Impact:** Early termination of failing devices, reduce burn-in cost\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Performance Optimization\n",
    "\n",
    "**Inference Speed:**\n",
    "```python\n",
    "# 1. Model quantization (4x speedup)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(autoencoder)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# 2. Batch inference (10x throughput)\n",
    "batch_size = 256\n",
    "predictions = autoencoder.predict(X_test, batch_size=batch_size)\n",
    "\n",
    "# 3. ONNX export for production\n",
    "import tf2onnx\n",
    "onnx_model = tf2onnx.convert.from_keras(autoencoder)\n",
    "```\n",
    "\n",
    "**Memory Footprint:**\n",
    "```python\n",
    "# Pruning (remove 80% of weights with minimal accuracy loss)\n",
    "import tensorflow_model_optimization as tfmot\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "pruned_model = prune_low_magnitude(autoencoder, pruning_schedule)\n",
    "```\n",
    "\n",
    "**Monitoring in Production:**\n",
    "```python\n",
    "# Track key metrics\n",
    "metrics = {\n",
    "    'latency_p50': np.percentile(latencies, 50),\n",
    "    'latency_p99': np.percentile(latencies, 99),\n",
    "    'anomaly_rate': np.mean(predictions == 1),\n",
    "    'false_positive_rate': fp / (fp + tn),\n",
    "    'threshold_drift': abs(current_threshold - baseline_threshold)\n",
    "}\n",
    "# Alert if: latency_p99 > 100ms, anomaly_rate > 10%, threshold_drift > 20%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üí° When to Use Autoencoders vs Alternatives\n",
    "\n",
    "**Use Autoencoders when:**\n",
    "- ‚úÖ High-dimensional data (>20 features)\n",
    "- ‚úÖ Complex nonlinear patterns\n",
    "- ‚úÖ Unlabeled normal data available\n",
    "- ‚úÖ Need to capture feature interactions\n",
    "- ‚úÖ Deep learning infrastructure available\n",
    "\n",
    "**Use alternatives when:**\n",
    "- ‚ùå **Isolation Forest:** Unknown distributions, fast inference needed\n",
    "- ‚ùå **LOF:** Local density anomalies, small datasets (<1000 samples)\n",
    "- ‚ùå **One-Class SVM:** Need interpretability, kernel methods suitable\n",
    "- ‚ùå **Statistical methods (Z-score, IQR):** Simple univariate cases, real-time constraints\n",
    "- ‚ùå **Ensemble:** Critical decisions, need robustness\n",
    "\n",
    "---\n",
    "\n",
    "**üîó Next Steps:**\n",
    "- Notebook 039: Gaussian Mixture Models for soft clustering\n",
    "- Notebook 040: DBSCAN for density-based clustering\n",
    "- Notebook 065: Deep Reinforcement Learning (extends to anomaly detection in control systems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab18a7c",
   "metadata": {},
   "source": [
    "## üìä Comprehensive Visualization & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7131879",
   "metadata": {},
   "source": [
    "## ‚è±Ô∏è Time-Series Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e2bdde",
   "metadata": {},
   "source": [
    "## üåê Multivariate Anomaly Detection Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff609c7",
   "metadata": {},
   "source": [
    "## üî¨ Real-Time Anomaly Detection Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a885f405",
   "metadata": {},
   "source": [
    "## üéØ Part 3: Advanced Anomaly Detection Techniques"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
