{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "506ccd5f",
   "metadata": {},
   "source": [
    "## ğŸ§® Mathematical Foundation\n",
    "\n",
    "### Architecture\n",
    "\n",
    "**Encoder**: $z = f_{enc}(x; \\theta_{enc})$ maps input $x \\in \\mathbb{R}^d$ to latent $z \\in \\mathbb{R}^k$ (k << d)\n",
    "\n",
    "**Decoder**: $\\hat{x} = f_{dec}(z; \\theta_{dec})$ reconstructs from latent space\n",
    "\n",
    "**Training Loss** (MSE):\n",
    "$$L = \\frac{1}{n}\\sum_{i=1}^{n} ||x_i - \\hat{x}_i||^2$$\n",
    "\n",
    "**Anomaly Score**:\n",
    "$$s(x) = ||x - f_{dec}(f_{enc}(x))||^2$$\n",
    "\n",
    "Normal points: low reconstruction error  \n",
    "Anomalies: high reconstruction error (didn't learn these patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d9f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=8):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, latent_dim)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n",
    "print(\"âœ… AutoEncoder architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "X_normal, _ = make_blobs(n_samples=1000, centers=1, n_features=10, random_state=42)\n",
    "X_anomalies = np.random.uniform(low=-8, high=8, size=(100, 10))\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_normal_scaled = scaler.fit_transform(X_normal)\n",
    "X_anomalies_scaled = scaler.transform(X_anomalies)\n",
    "\n",
    "# Train autoencoder on normal data only\n",
    "X_train_tensor = torch.FloatTensor(X_normal_scaled)\n",
    "model = AutoEncoder(input_dim=10, latent_dim=3)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    recon = model(X_train_tensor)\n",
    "    loss = criterion(recon, X_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Training complete. Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65121f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute reconstruction errors\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test = np.vstack([X_normal_scaled, X_anomalies_scaled])\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    recon_test = model(X_test_tensor).numpy()\n",
    "    errors = np.mean((X_test - recon_test)**2, axis=1)\n",
    "\n",
    "y_true = np.array([1]*len(X_normal_scaled) + [-1]*len(X_anomalies_scaled))\n",
    "\n",
    "# Set threshold (95th percentile of normal errors)\n",
    "threshold = np.percentile(errors[:len(X_normal_scaled)], 95)\n",
    "y_pred = np.where(errors > threshold, -1, 1)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.hist(errors[:len(X_normal_scaled)], bins=30, alpha=0.6, label='Normal', color='blue')\n",
    "ax.hist(errors[len(X_normal_scaled):], bins=30, alpha=0.6, label='Anomalies', color='red')\n",
    "ax.axvline(threshold, color='green', linestyle='--', linewidth=2, label=f'Threshold={threshold:.3f}')\n",
    "ax.set_xlabel('Reconstruction Error')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('AutoEncoder: Reconstruction Error Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "y_true_binary = (y_true == -1).astype(int)\n",
    "fpr, tpr, _ = roc_curve(y_true_binary, errors)\n",
    "auc = roc_auc_score(y_true_binary, errors)\n",
    "ax.plot(fpr, tpr, linewidth=2, label=f'AUC={auc:.3f}')\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Performance:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Normal', 'Anomaly']))\n",
    "print(f\"\\nAUC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7950ca6d",
   "metadata": {},
   "source": [
    "## ğŸ­ Semiconductor Application\n",
    "\n",
    "### ğŸ“ High-Dimensional Parametric Test Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff17de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic semiconductor test data (20 parameters)\n",
    "np.random.seed(42)\n",
    "n_params = 20\n",
    "n_normal = 2000\n",
    "n_anomaly = 100\n",
    "\n",
    "# Normal devices: correlated parameters\n",
    "mean_normal = np.random.uniform(1.5, 2.5, n_params)\n",
    "cov_normal = np.eye(n_params) * 0.05\n",
    "X_psv_normal = np.random.multivariate_normal(mean_normal, cov_normal, n_normal)\n",
    "\n",
    "# Anomalous devices: parameter drift\n",
    "X_psv_anomaly = X_psv_normal[:n_anomaly].copy()\n",
    "X_psv_anomaly[:, :5] += np.random.uniform(0.5, 1.5, (n_anomaly, 5))  # Drift in first 5 params\n",
    "\n",
    "# Scale\n",
    "scaler_psv = StandardScaler()\n",
    "X_psv_normal_scaled = scaler_psv.fit_transform(X_psv_normal)\n",
    "X_psv_anomaly_scaled = scaler_psv.transform(X_psv_anomaly)\n",
    "\n",
    "# Train autoencoder\n",
    "X_train_psv = torch.FloatTensor(X_psv_normal_scaled)\n",
    "model_psv = AutoEncoder(input_dim=n_params, latent_dim=5)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_psv.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model_psv.train()\n",
    "    optimizer.zero_grad()\n",
    "    recon = model_psv(X_train_psv)\n",
    "    loss = criterion(recon, X_train_psv)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Test\n",
    "model_psv.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_psv = np.vstack([X_psv_normal_scaled[:500], X_psv_anomaly_scaled])\n",
    "    X_test_psv_tensor = torch.FloatTensor(X_test_psv)\n",
    "    recon_psv = model_psv(X_test_psv_tensor).numpy()\n",
    "    errors_psv = np.mean((X_test_psv - recon_psv)**2, axis=1)\n",
    "\n",
    "y_true_psv = np.array([1]*500 + [-1]*n_anomaly)\n",
    "threshold_psv = np.percentile(errors_psv[:500], 95)\n",
    "y_pred_psv = np.where(errors_psv > threshold_psv, -1, 1)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(errors_psv[:500], bins=30, alpha=0.6, label='Normal Devices', color='blue')\n",
    "plt.hist(errors_psv[500:], bins=30, alpha=0.6, label='Anomalous Devices', color='red')\n",
    "plt.axvline(threshold_psv, color='green', linestyle='--', linewidth=2, label=f'Threshold')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Number of Devices')\n",
    "plt.title('Parametric Test Anomaly Detection (20 Parameters)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Feature importance (average reconstruction error per feature)\n",
    "feature_errors = np.mean((X_test_psv[500:] - recon_psv[500:])**2, axis=0)\n",
    "plt.bar(range(n_params), feature_errors, color='coral')\n",
    "plt.xlabel('Parameter Index')\n",
    "plt.ylabel('Avg Reconstruction Error')\n",
    "plt.title('Parameter-wise Anomaly Contribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâš ï¸ Anomalous Devices Detected:\")\n",
    "print(f\"   Total: {(y_pred_psv == -1).sum()} / {len(X_test_psv)}\")\n",
    "print(\"\\nğŸ“Š Performance:\")\n",
    "print(classification_report(y_true_psv, y_pred_psv, target_names=['Normal', 'Anomaly']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378695c6",
   "metadata": {},
   "source": [
    "## ğŸ¯ Project Ideas\n",
    "\n",
    "### Post-Silicon Projects\n",
    "\n",
    "1. **Wafer Map Anomaly Detector** ğŸ’° $8M+ Yield Improvement\n",
    "   - Train on normal wafer spatial patterns, detect systematic defects\n",
    "   - Features: 2D die coordinates + parametric test values\n",
    "   - Business: Early fab process issue detection\n",
    "\n",
    "2. **Multi-Site Test Correlation Monitor** ğŸ’° $12M+ Quality\n",
    "   - 50+ parametric tests, detect novel failure modes\n",
    "   - AutoEncoder learns normal parameter correlations\n",
    "   - Business: Improve test coverage, reduce escapes\n",
    "\n",
    "3. **Time-Series Waveform Anomalies** ğŸ’° $5M+ Debug Time\n",
    "   - LSTM AutoEncoder on test waveforms\n",
    "   - Detect subtle signal integrity issues\n",
    "   - Business: Faster failure analysis\n",
    "\n",
    "4. **Cross-Product Defect Discovery** ğŸ’° $15M+ Portfolio\n",
    "   - Train per-product autoencoders\n",
    "   - Transfer learning for new products\n",
    "   - Business: Accelerate new product ramp\n",
    "\n",
    "### General Projects\n",
    "\n",
    "5. **Network Intrusion Detection** ğŸ’° $30M+ Security\n",
    "6. **Medical Image Anomalies** ğŸ’° $100M+ Healthcare\n",
    "7. **Industrial Sensor Monitoring** ğŸ’° $20M+ Downtime\n",
    "8. **Financial Transaction Fraud** ğŸ’° $150M+ Fraud Prevention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49756edd",
   "metadata": {},
   "source": [
    "## ğŸ” Key Takeaways\n",
    "\n",
    "### âœ… When to Use AutoEncoders\n",
    "- **High-dimensional data** (>50 features): Learns compressed representations\n",
    "- **Complex patterns**: Captures non-linear correlations via deep networks\n",
    "- **Unlabeled data**: Unsupervised, trains on normal only\n",
    "- **Feature learning**: Automatic feature extraction (no manual engineering)\n",
    "\n",
    "### âŒ Limitations\n",
    "- **Training time**: Requires GPU for large datasets\n",
    "- **Hyperparameters**: Architecture, latent dim, learning rate tuning needed\n",
    "- **Overfitting risk**: May memorize training data (use regularization)\n",
    "- **Black box**: Less interpretable than tree-based methods\n",
    "\n",
    "### ğŸ”§ Best Practices\n",
    "1. **Always standardize** inputs (zero mean, unit variance)\n",
    "2. **Latent dimension**: Start with d/4 to d/2 (compression ratio 2-4x)\n",
    "3. **Threshold**: 95th-99th percentile of training errors\n",
    "4. **Validation**: Use contaminated validation set to tune threshold\n",
    "5. **Regularization**: Dropout, L2 weight decay to prevent overfitting\n",
    "\n",
    "### ğŸ“Š Comparison\n",
    "\n",
    "| Method | Speed | High-D | Interpretability | Best For |\n",
    "|--------|-------|--------|------------------|----------|\n",
    "| **AutoEncoder** | Slow train, fast inference | âœ… Excellent | âŒ Low | Complex patterns, images |\n",
    "| **Isolation Forest** | âœ… Fast | âœ… Good | âš ï¸ Medium | Large data, speed |\n",
    "| **One-Class SVM** | âŒ Slow | âš ï¸ Medium | âœ… Good | Novelty, small data |\n",
    "\n",
    "### ğŸš€ Next Steps\n",
    "- Variational AutoEncoders (VAE) for probabilistic anomaly scores\n",
    "- LSTM AutoEncoders for time-series anomalies\n",
    "- Convolutional AutoEncoders for image/wafer map anomalies"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
