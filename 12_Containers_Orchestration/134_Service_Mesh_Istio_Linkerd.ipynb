{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6952213",
   "metadata": {},
   "source": [
    "# 134: Service Mesh for ML - Istio and Linkerd\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** service mesh architecture (control plane manages config, data plane handles traffic)\n",
    "- **Implement** traffic management with Istio/Linkerd (canary releases, A/B testing, traffic splitting)\n",
    "- **Master** observability patterns (distributed tracing with Jaeger, service graphs with Kiali)\n",
    "- **Apply** mTLS security to post-silicon validation pipelines (automatic certificate management)\n",
    "- **Build** resilience patterns (circuit breakers, retries with exponential backoff, timeouts)\n",
    "- **Deploy** production ML systems with service mesh (multi-model inference pipelines)\n",
    "\n",
    "## üìö What is a Service Mesh?\n",
    "\n",
    "A **service mesh** is an infrastructure layer that handles service-to-service communication in microservices architectures. Instead of each service managing its own networking concerns (retries, timeouts, encryption, load balancing), a **sidecar proxy** is injected into each pod to handle these cross-cutting concerns. The result: **zero code changes** to enable mTLS, distributed tracing, circuit breakers, and advanced traffic management.\n",
    "\n",
    "**Without Service Mesh:**\n",
    "- Each service implements its own retry logic (inconsistent, hard to update)\n",
    "- No automatic mTLS (security team manually manages certificates)\n",
    "- No distributed tracing (debugging latency issues takes hours)\n",
    "- Traffic management requires code changes (deploy new version to shift traffic)\n",
    "\n",
    "**With Service Mesh (Istio/Linkerd):**\n",
    "- **Sidecar proxies** (Envoy, Linkerd2-proxy) handle all networking automatically\n",
    "- **Control plane** (Pilot, Citadel) manages configuration centrally\n",
    "- **mTLS automatic** (certificates issued, rotated every 24 hours without manual intervention)\n",
    "- **Distributed tracing** (Jaeger shows request flow across 10+ services)\n",
    "- **Traffic splitting** (route 10% traffic to new model version without code changes)\n",
    "\n",
    "**Why Service Mesh?**\n",
    "- ‚úÖ **Zero Code Changes**: Add mTLS, tracing, retries without modifying application code\n",
    "- ‚úÖ **Centralized Policy Management**: Control all traffic routing from one place (control plane)\n",
    "- ‚úÖ **Automatic Observability**: Every request traced, metrics exported (no manual instrumentation)\n",
    "- ‚úÖ **Security by Default**: mTLS enabled for all service-to-service traffic (zero-trust networking)\n",
    "- ‚úÖ **Progressive Delivery**: Canary releases with automatic rollback on metric degradation\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Use Case 1: Multi-Model Inference Pipeline with Traffic Splitting**\n",
    "- **Input**: 5-service ML pipeline (feature extraction ‚Üí wafer map analysis ‚Üí parametric model ‚Üí spatial model ‚Üí ensemble)\n",
    "- **Output**: Istio routes 10% traffic to new ensemble model v2.5, monitors accuracy for 1 hour\n",
    "- **Value**: Safe deployment - if accuracy <99%, automatic rollback to v2.4 (no downtime)\n",
    "- **Business Impact**: **$180K/year savings** (prevent bad model deployments, reduce rollback time 95%)\n",
    "\n",
    "**Use Case 2: Zero-Trust STDF Processing with Automatic mTLS**\n",
    "- **Input**: STDF parser ‚Üí feature extractor ‚Üí outlier detector ‚Üí results storage (4 services)\n",
    "- **Output**: Linkerd automatically encrypts all traffic, rotates certificates every 24 hours\n",
    "- **Value**: Compliance with data security regulations (SOC 2, ISO 27001) without manual certificate management\n",
    "- **Business Impact**: **$95K/year savings** (eliminate manual certificate rotation, pass security audits)\n",
    "\n",
    "**Use Case 3: Resilient Wafer Analysis with Circuit Breakers**\n",
    "- **Input**: Wafer map analyzer calls external defect classification API (3rd party service, sometimes slow)\n",
    "- **Output**: Istio circuit breaker opens after 5 failures, prevents cascade failures across pipeline\n",
    "- **Value**: Pipeline continues processing other wafers (graceful degradation vs complete failure)\n",
    "- **Business Impact**: **$340K/year savings** (prevent pipeline downtime, maintain 99.9% availability)\n",
    "\n",
    "**Use Case 4: A/B Testing for Yield Prediction Models**\n",
    "- **Input**: Yield predictor v3.0 (new transformer architecture) vs v2.8 (baseline GBM)\n",
    "- **Output**: Istio routes premium customers to v3.0, regular customers to v2.8 (header-based routing)\n",
    "- **Value**: Compare model accuracy on real production traffic (not just test data)\n",
    "- **Business Impact**: **$2.8M/year savings** (v3.0 improves yield prediction by 0.5% ‚Üí reduce manufacturing waste)\n",
    "\n",
    "## üîÑ Service Mesh Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    subgraph \"Control Plane\"\n",
    "        A[Pilot<br/>Service Discovery<br/>Traffic Rules]\n",
    "        B[Citadel<br/>Certificate Authority<br/>mTLS Certs]\n",
    "        C[Galley<br/>Configuration<br/>Validation]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Data Plane - Application Pods\"\n",
    "        D[Feature Service Pod<br/>App Container + Envoy Proxy]\n",
    "        E[Model Service Pod<br/>App Container + Envoy Proxy]\n",
    "        F[Ensemble Service Pod<br/>App Container + Envoy Proxy]\n",
    "    end\n",
    "    \n",
    "    A -->|Config Distribution| D\n",
    "    A -->|Config Distribution| E\n",
    "    A -->|Config Distribution| F\n",
    "    \n",
    "    B -->|Issue Certificates| D\n",
    "    B -->|Issue Certificates| E\n",
    "    B -->|Issue Certificates| F\n",
    "    \n",
    "    D -->|mTLS Encrypted| E\n",
    "    E -->|mTLS Encrypted| F\n",
    "    \n",
    "    D -->|Metrics/Traces| G[Prometheus<br/>Jaeger]\n",
    "    E -->|Metrics/Traces| G\n",
    "    F -->|Metrics/Traces| G\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#ffe1e1\n",
    "    style C fill:#fff4e1\n",
    "    style G fill:#e1ffe1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **Notebook 131**: Docker for ML (containerization fundamentals)\n",
    "- **Notebook 132**: Kubernetes Fundamentals (deployments, services, pods)\n",
    "- **Notebook 133**: Kubernetes Advanced (operators, CRDs, StatefulSets)\n",
    "\n",
    "**Next Steps:**\n",
    "- **Notebook 135**: GitOps (ArgoCD, Flux for declarative deployments)\n",
    "- **Notebook 136**: CI/CD for ML (Tekton, GitHub Actions with service mesh)\n",
    "- **Notebook 137**: Infrastructure as Code (Terraform for Kubernetes + Istio)\n",
    "\n",
    "---\n",
    "\n",
    "Let's build service mesh systems for ML! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3248f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "from enum import Enum\n",
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Environment ready for service mesh simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbde1ec",
   "metadata": {},
   "source": [
    "## 2. üèóÔ∏è Service Mesh Architecture - Control Plane and Data Plane\n",
    "\n",
    "### üìù What's Happening in This Section?\n",
    "\n",
    "**Purpose:** Understand the two-layer architecture of service meshes (control plane manages configuration, data plane handles actual traffic)\n",
    "\n",
    "**Key Points:**\n",
    "- **Control Plane**: Manages configuration, distributes policies, handles certificate issuance (Pilot, Citadel, Galley)\n",
    "- **Data Plane**: Sidecar proxies intercept all traffic, enforce policies, collect metrics (Envoy, Linkerd2-proxy)\n",
    "- **Sidecar Pattern**: Each pod gets additional container (proxy) that handles networking\n",
    "- **Service Discovery**: Control plane tells proxies about all services and their endpoints\n",
    "- **Policy Distribution**: Control plane pushes routing rules, security policies to all proxies\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Decoupling**: Application code doesn't handle mTLS, retries, metrics (separation of concerns)\n",
    "- **Centralized Control**: Change traffic routing for all services from one place (control plane)\n",
    "- **Zero Code Changes**: Add service mesh to existing applications without modifying code\n",
    "- **Observability**: Proxies automatically export metrics, traces, logs (no manual instrumentation)\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "STDF parsing pipeline (4 services: Parser ‚Üí Extractor ‚Üí Analyzer ‚Üí Storage) gets automatic mTLS, distributed tracing, circuit breakers by deploying Istio (inject sidecar proxies via annotation `sidecar.istio.io/inject: \"true\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service Mesh Architecture Simulation\n",
    "\n",
    "@dataclass\n",
    "class Certificate:\n",
    "    \"\"\"TLS certificate for mTLS.\"\"\"\n",
    "    service_name: str\n",
    "    issued_at: datetime\n",
    "    expires_at: datetime\n",
    "    certificate_id: str = field(default_factory=lambda: uuid.uuid4().hex[:12])\n",
    "    \n",
    "    def is_valid(self) -> bool:\n",
    "        \"\"\"Check if certificate is still valid.\"\"\"\n",
    "        now = datetime.now()\n",
    "        return self.issued_at <= now <= self.expires_at\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ServiceEndpoint:\n",
    "    \"\"\"Represents a service endpoint (pod).\"\"\"\n",
    "    service_name: str\n",
    "    pod_name: str\n",
    "    ip: str\n",
    "    port: int\n",
    "    version: str = \"v1\"\n",
    "    healthy: bool = True\n",
    "    \n",
    "    def get_address(self) -> str:\n",
    "        \"\"\"Get full address.\"\"\"\n",
    "        return f\"{self.ip}:{self.port}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RoutingRule:\n",
    "    \"\"\"Traffic routing rule.\"\"\"\n",
    "    source_service: str\n",
    "    destination_service: str\n",
    "    version_weights: Dict[str, int]  # {\"v1\": 90, \"v2\": 10}\n",
    "    headers: Optional[Dict[str, str]] = None  # Header-based routing\n",
    "    \n",
    "    def select_version(self, request_headers: Dict[str, str] = None) -> str:\n",
    "        \"\"\"Select version based on weights or headers.\"\"\"\n",
    "        # Header-based routing takes precedence\n",
    "        if self.headers and request_headers:\n",
    "            for header_key, header_value in self.headers.items():\n",
    "                if request_headers.get(header_key) == header_value:\n",
    "                    # Find version for this header\n",
    "                    for version in self.version_weights:\n",
    "                        if version != \"v1\":  # Assume non-v1 is the test version\n",
    "                            return version\n",
    "        \n",
    "        # Weight-based routing\n",
    "        versions = list(self.version_weights.keys())\n",
    "        weights = list(self.version_weights.values())\n",
    "        return np.random.choice(versions, p=[w/sum(weights) for w in weights])\n",
    "\n",
    "\n",
    "class ControlPlane:\n",
    "    \"\"\"Service mesh control plane (Istio Pilot + Citadel).\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"istio-control-plane\"):\n",
    "        self.name = name\n",
    "        self.services: Dict[str, List[ServiceEndpoint]] = {}\n",
    "        self.routing_rules: List[RoutingRule] = []\n",
    "        self.certificates: Dict[str, Certificate] = {}\n",
    "        self.certificate_lifetime_hours: int = 24\n",
    "    \n",
    "    def register_service(self, endpoint: ServiceEndpoint):\n",
    "        \"\"\"Register service endpoint.\"\"\"\n",
    "        if endpoint.service_name not in self.services:\n",
    "            self.services[endpoint.service_name] = []\n",
    "        self.services[endpoint.service_name].append(endpoint)\n",
    "        print(f\"üìù Registered: {endpoint.service_name} ({endpoint.pod_name}) at {endpoint.get_address()}\")\n",
    "    \n",
    "    def add_routing_rule(self, rule: RoutingRule):\n",
    "        \"\"\"Add traffic routing rule.\"\"\"\n",
    "        self.routing_rules.append(rule)\n",
    "        print(f\"üîÄ Routing rule: {rule.source_service} ‚Üí {rule.destination_service} \"\n",
    "              f\"(weights: {rule.version_weights})\")\n",
    "    \n",
    "    def issue_certificate(self, service_name: str) -> Certificate:\n",
    "        \"\"\"Issue TLS certificate for service (Citadel functionality).\"\"\"\n",
    "        cert = Certificate(\n",
    "            service_name=service_name,\n",
    "            issued_at=datetime.now(),\n",
    "            expires_at=datetime.now() + timedelta(hours=self.certificate_lifetime_hours)\n",
    "        )\n",
    "        self.certificates[service_name] = cert\n",
    "        print(f\"üîí Certificate issued: {service_name} (expires in {self.certificate_lifetime_hours}h)\")\n",
    "        return cert\n",
    "    \n",
    "    def get_endpoints(self, service_name: str, version: str = None) -> List[ServiceEndpoint]:\n",
    "        \"\"\"Get service endpoints (optionally filtered by version).\"\"\"\n",
    "        endpoints = self.services.get(service_name, [])\n",
    "        \n",
    "        if version:\n",
    "            endpoints = [ep for ep in endpoints if ep.version == version]\n",
    "        \n",
    "        # Filter out unhealthy endpoints\n",
    "        endpoints = [ep for ep in endpoints if ep.healthy]\n",
    "        \n",
    "        return endpoints\n",
    "    \n",
    "    def get_routing_rule(self, source: str, destination: str) -> Optional[RoutingRule]:\n",
    "        \"\"\"Get routing rule for source ‚Üí destination.\"\"\"\n",
    "        for rule in self.routing_rules:\n",
    "            if rule.source_service == source and rule.destination_service == destination:\n",
    "                return rule\n",
    "        return None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Request:\n",
    "    \"\"\"HTTP request.\"\"\"\n",
    "    request_id: str = field(default_factory=lambda: uuid.uuid4().hex[:8])\n",
    "    source_service: str = \"\"\n",
    "    destination_service: str = \"\"\n",
    "    headers: Dict[str, str] = field(default_factory=dict)\n",
    "    trace_id: str = field(default_factory=lambda: uuid.uuid4().hex[:16])\n",
    "    span_id: str = field(default_factory=lambda: uuid.uuid4().hex[:8])\n",
    "    timestamp: datetime = field(default_factory=datetime.now)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Response:\n",
    "    \"\"\"HTTP response.\"\"\"\n",
    "    request_id: str\n",
    "    status_code: int = 200\n",
    "    latency_ms: float = 0.0\n",
    "    endpoint: Optional[ServiceEndpoint] = None\n",
    "    encrypted: bool = False\n",
    "\n",
    "\n",
    "class SidecarProxy:\n",
    "    \"\"\"Envoy sidecar proxy (data plane).\"\"\"\n",
    "    \n",
    "    def __init__(self, service_name: str, pod_name: str, control_plane: ControlPlane):\n",
    "        self.service_name = service_name\n",
    "        self.pod_name = pod_name\n",
    "        self.control_plane = control_plane\n",
    "        self.certificate: Optional[Certificate] = None\n",
    "        self.request_count: int = 0\n",
    "        self.error_count: int = 0\n",
    "        self.total_latency_ms: float = 0.0\n",
    "        \n",
    "        # Request certificate from control plane\n",
    "        self.certificate = self.control_plane.issue_certificate(self.service_name)\n",
    "    \n",
    "    def forward_request(self, request: Request) -> Response:\n",
    "        \"\"\"Forward request to destination service.\"\"\"\n",
    "        self.request_count += 1\n",
    "        \n",
    "        # Get routing rule\n",
    "        rule = self.control_plane.get_routing_rule(\n",
    "            self.service_name, \n",
    "            request.destination_service\n",
    "        )\n",
    "        \n",
    "        # Select version based on routing rule\n",
    "        if rule:\n",
    "            version = rule.select_version(request.headers)\n",
    "        else:\n",
    "            version = \"v1\"  # Default\n",
    "        \n",
    "        # Get endpoints for selected version\n",
    "        endpoints = self.control_plane.get_endpoints(\n",
    "            request.destination_service, \n",
    "            version\n",
    "        )\n",
    "        \n",
    "        if not endpoints:\n",
    "            self.error_count += 1\n",
    "            return Response(\n",
    "                request_id=request.request_id,\n",
    "                status_code=503,  # Service Unavailable\n",
    "                latency_ms=5.0\n",
    "            )\n",
    "        \n",
    "        # Load balance across endpoints (random)\n",
    "        endpoint = np.random.choice(endpoints)\n",
    "        \n",
    "        # Simulate latency\n",
    "        base_latency = np.random.uniform(10, 50)\n",
    "        if version == \"v2\":\n",
    "            base_latency *= 1.2  # v2 slightly slower\n",
    "        \n",
    "        # Simulate encryption overhead (mTLS)\n",
    "        if self.certificate and self.certificate.is_valid():\n",
    "            base_latency += 2.0  # mTLS overhead\n",
    "            encrypted = True\n",
    "        else:\n",
    "            encrypted = False\n",
    "        \n",
    "        latency = base_latency\n",
    "        self.total_latency_ms += latency\n",
    "        \n",
    "        return Response(\n",
    "            request_id=request.request_id,\n",
    "            status_code=200,\n",
    "            latency_ms=latency,\n",
    "            endpoint=endpoint,\n",
    "            encrypted=encrypted\n",
    "        )\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"Get golden metrics (requests, errors, latency).\"\"\"\n",
    "        avg_latency = self.total_latency_ms / self.request_count if self.request_count > 0 else 0\n",
    "        error_rate = self.error_count / self.request_count if self.request_count > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"request_count\": self.request_count,\n",
    "            \"error_count\": self.error_count,\n",
    "            \"error_rate\": error_rate,\n",
    "            \"avg_latency_ms\": avg_latency\n",
    "        }\n",
    "\n",
    "\n",
    "# Example 1: Service Mesh Setup\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE 1: Service Mesh Architecture Setup\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create control plane\n",
    "control_plane = ControlPlane()\n",
    "\n",
    "# Register services (ML inference pipeline)\n",
    "# Feature Service\n",
    "control_plane.register_service(ServiceEndpoint(\n",
    "    service_name=\"feature-service\",\n",
    "    pod_name=\"feature-service-0\",\n",
    "    ip=\"10.0.1.10\",\n",
    "    port=8080,\n",
    "    version=\"v1\"\n",
    "))\n",
    "\n",
    "# Model Service v1 (3 replicas)\n",
    "for i in range(3):\n",
    "    control_plane.register_service(ServiceEndpoint(\n",
    "        service_name=\"model-service\",\n",
    "        pod_name=f\"model-service-v1-{i}\",\n",
    "        ip=f\"10.0.2.{10+i}\",\n",
    "        port=8080,\n",
    "        version=\"v1\"\n",
    "    ))\n",
    "\n",
    "# Model Service v2 (1 replica, canary)\n",
    "control_plane.register_service(ServiceEndpoint(\n",
    "    service_name=\"model-service\",\n",
    "    pod_name=\"model-service-v2-0\",\n",
    "    ip=\"10.0.2.20\",\n",
    "    port=8080,\n",
    "    version=\"v2\"\n",
    "))\n",
    "\n",
    "# Ensemble Service\n",
    "control_plane.register_service(ServiceEndpoint(\n",
    "    service_name=\"ensemble-service\",\n",
    "    pod_name=\"ensemble-service-0\",\n",
    "    ip=\"10.0.3.10\",\n",
    "    port=8080,\n",
    "    version=\"v1\"\n",
    "))\n",
    "\n",
    "print(f\"\\nüìä Services Registered: {len(control_plane.services)}\")\n",
    "print(f\"   ‚Ä¢ feature-service: {len(control_plane.get_endpoints('feature-service'))} endpoints\")\n",
    "print(f\"   ‚Ä¢ model-service: {len(control_plane.get_endpoints('model-service'))} endpoints\")\n",
    "print(f\"     - v1: {len(control_plane.get_endpoints('model-service', 'v1'))} replicas\")\n",
    "print(f\"     - v2: {len(control_plane.get_endpoints('model-service', 'v2'))} replicas\")\n",
    "print(f\"   ‚Ä¢ ensemble-service: {len(control_plane.get_endpoints('ensemble-service'))} endpoints\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE 2: Traffic Routing Rules (Canary Release)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Add canary routing rule: 90% to v1, 10% to v2\n",
    "canary_rule = RoutingRule(\n",
    "    source_service=\"feature-service\",\n",
    "    destination_service=\"model-service\",\n",
    "    version_weights={\"v1\": 90, \"v2\": 10}\n",
    ")\n",
    "control_plane.add_routing_rule(canary_rule)\n",
    "\n",
    "# Create sidecar proxies\n",
    "feature_proxy = SidecarProxy(\"feature-service\", \"feature-service-0\", control_plane)\n",
    "\n",
    "print(\"\\nüí° Canary Release: 90% traffic to model-v1, 10% to model-v2\")\n",
    "\n",
    "# Simulate 100 requests\n",
    "print(\"\\nüîÑ Simulating 100 requests...\")\n",
    "v1_count = 0\n",
    "v2_count = 0\n",
    "latencies_v1 = []\n",
    "latencies_v2 = []\n",
    "\n",
    "for i in range(100):\n",
    "    request = Request(\n",
    "        source_service=\"feature-service\",\n",
    "        destination_service=\"model-service\"\n",
    "    )\n",
    "    \n",
    "    response = feature_proxy.forward_request(request)\n",
    "    \n",
    "    if response.endpoint:\n",
    "        if response.endpoint.version == \"v1\":\n",
    "            v1_count += 1\n",
    "            latencies_v1.append(response.latency_ms)\n",
    "        else:\n",
    "            v2_count += 1\n",
    "            latencies_v2.append(response.latency_ms)\n",
    "\n",
    "print(f\"\\nüìä Traffic Distribution:\")\n",
    "print(f\"   ‚Ä¢ model-v1: {v1_count} requests ({v1_count}%)\")\n",
    "print(f\"   ‚Ä¢ model-v2: {v2_count} requests ({v2_count}%)\")\n",
    "print(f\"\\nüìà Latency:\")\n",
    "print(f\"   ‚Ä¢ model-v1: {np.mean(latencies_v1):.2f} ms (avg)\")\n",
    "print(f\"   ‚Ä¢ model-v2: {np.mean(latencies_v2):.2f} ms (avg)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE 3: mTLS Security (Automatic Certificate Issuance)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüîí Certificates Issued:\")\n",
    "for service_name, cert in control_plane.certificates.items():\n",
    "    valid_for = (cert.expires_at - datetime.now()).total_seconds() / 3600\n",
    "    print(f\"   ‚Ä¢ {service_name}\")\n",
    "    print(f\"     - Certificate ID: {cert.certificate_id}\")\n",
    "    print(f\"     - Valid for: {valid_for:.1f} hours\")\n",
    "    print(f\"     - Status: {'‚úÖ Valid' if cert.is_valid() else '‚ùå Expired'}\")\n",
    "\n",
    "# Check encryption status\n",
    "sample_request = Request(\n",
    "    source_service=\"feature-service\",\n",
    "    destination_service=\"model-service\"\n",
    ")\n",
    "sample_response = feature_proxy.forward_request(sample_request)\n",
    "\n",
    "print(f\"\\nüîê Request Encryption:\")\n",
    "print(f\"   ‚Ä¢ Encrypted: {'‚úÖ Yes (mTLS)' if sample_response.encrypted else '‚ùå No'}\")\n",
    "print(f\"   ‚Ä¢ Latency overhead: ~2ms (TLS handshake + encryption)\")\n",
    "\n",
    "print(\"\\nüí° Service Mesh Benefits Demonstrated:\")\n",
    "print(\"   ‚úÖ Automatic service discovery (control plane knows all endpoints)\")\n",
    "print(\"   ‚úÖ Traffic splitting (90/10 canary release)\")\n",
    "print(\"   ‚úÖ mTLS encryption (automatic certificate issuance)\")\n",
    "print(\"   ‚úÖ Load balancing (random selection across endpoints)\")\n",
    "print(\"   ‚úÖ Metrics collection (requests, errors, latency)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b71b7e",
   "metadata": {},
   "source": [
    "## 3. üîÄ Traffic Management - Canary Releases and A/B Testing\n",
    "\n",
    "### üìù What's Happening in This Section?\n",
    "\n",
    "**Purpose:** Control traffic flow between service versions for safe deployments and experimentation\n",
    "\n",
    "**Key Points:**\n",
    "- **Canary Release**: Gradually shift traffic from old version to new (5% ‚Üí 10% ‚Üí 25% ‚Üí 50% ‚Üí 100%)\n",
    "- **A/B Testing**: Route traffic based on user attributes (header, cookie, IP) to compare versions\n",
    "- **Blue-Green Deployment**: Maintain two identical environments (switch traffic instantly)\n",
    "- **Traffic Mirroring**: Send copy of production traffic to new version (test without user impact)\n",
    "- **Weight-Based Routing**: Percentage-based distribution (80% v1, 20% v2)\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Risk Mitigation**: Detect issues with 5% traffic before affecting all users\n",
    "- **Fast Rollback**: Revert to v1 by changing weights (seconds vs hours for full redeployment)\n",
    "- **Data-Driven Decisions**: Compare metrics (latency, errors, accuracy) between versions\n",
    "- **Zero Downtime**: Gradual migration ensures always enough healthy pods serving traffic\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "Deploy new wafer yield model v2.5 (95.5% validation accuracy) with canary: 5% production traffic to v2.5, monitor for 24 hours (if error rate <0.5% and accuracy ‚â•95%, increase to 100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8b4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traffic Management Simulation\n",
    "\n",
    "class CanaryDeployment:\n",
    "    \"\"\"Manage gradual canary rollout.\"\"\"\n",
    "    \n",
    "    def __init__(self, control_plane: ControlPlane, source: str, destination: str):\n",
    "        self.control_plane = control_plane\n",
    "        self.source = source\n",
    "        self.destination = destination\n",
    "        self.current_weight_v1 = 100\n",
    "        self.current_weight_v2 = 0\n",
    "        self.history: List[Dict] = []\n",
    "    \n",
    "    def update_weights(self, v2_percentage: int):\n",
    "        \"\"\"Update traffic weights.\"\"\"\n",
    "        self.current_weight_v1 = 100 - v2_percentage\n",
    "        self.current_weight_v2 = v2_percentage\n",
    "        \n",
    "        # Update routing rule\n",
    "        for rule in self.control_plane.routing_rules:\n",
    "            if (rule.source_service == self.source and \n",
    "                rule.destination_service == self.destination):\n",
    "                rule.version_weights = {\"v1\": self.current_weight_v1, \"v2\": self.current_weight_v2}\n",
    "                break\n",
    "        \n",
    "        print(f\"üîÑ Updated traffic weights: v1={self.current_weight_v1}%, v2={self.current_weight_v2}%\")\n",
    "    \n",
    "    def evaluate_metrics(self, proxy: SidecarProxy, num_requests: int = 1000) -> Dict:\n",
    "        \"\"\"Send requests and evaluate version metrics.\"\"\"\n",
    "        v1_requests = []\n",
    "        v2_requests = []\n",
    "        \n",
    "        for _ in range(num_requests):\n",
    "            request = Request(\n",
    "                source_service=self.source,\n",
    "                destination_service=self.destination\n",
    "            )\n",
    "            response = proxy.forward_request(request)\n",
    "            \n",
    "            if response.endpoint:\n",
    "                if response.endpoint.version == \"v1\":\n",
    "                    v1_requests.append(response)\n",
    "                else:\n",
    "                    v2_requests.append(response)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            \"v1\": {\n",
    "                \"count\": len(v1_requests),\n",
    "                \"avg_latency\": np.mean([r.latency_ms for r in v1_requests]) if v1_requests else 0,\n",
    "                \"error_rate\": sum(1 for r in v1_requests if r.status_code >= 400) / len(v1_requests) if v1_requests else 0\n",
    "            },\n",
    "            \"v2\": {\n",
    "                \"count\": len(v2_requests),\n",
    "                \"avg_latency\": np.mean([r.latency_ms for r in v2_requests]) if v2_requests else 0,\n",
    "                \"error_rate\": sum(1 for r in v2_requests if r.status_code >= 400) / len(v2_requests) if v2_requests else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.history.append({\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"v2_weight\": self.current_weight_v2,\n",
    "            \"metrics\": metrics\n",
    "        })\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def gradual_rollout(self, proxy: SidecarProxy, stages: List[int], requests_per_stage: int = 1000):\n",
    "        \"\"\"Execute gradual canary rollout.\"\"\"\n",
    "        print(f\"\\nüöÄ Starting Canary Rollout: {self.destination}\")\n",
    "        print(f\"   Stages: {stages}%\")\n",
    "        print(f\"   Requests per stage: {requests_per_stage}\\n\")\n",
    "        \n",
    "        for stage_percentage in stages:\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"STAGE: {stage_percentage}% traffic to v2\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Update weights\n",
    "            self.update_weights(stage_percentage)\n",
    "            \n",
    "            # Evaluate metrics\n",
    "            metrics = self.evaluate_metrics(proxy, requests_per_stage)\n",
    "            \n",
    "            print(f\"\\nüìä Metrics for stage {stage_percentage}%:\")\n",
    "            print(f\"   v1: {metrics['v1']['count']} requests, \"\n",
    "                  f\"{metrics['v1']['avg_latency']:.2f} ms avg latency, \"\n",
    "                  f\"{metrics['v1']['error_rate']*100:.2f}% errors\")\n",
    "            print(f\"   v2: {metrics['v2']['count']} requests, \"\n",
    "                  f\"{metrics['v2']['avg_latency']:.2f} ms avg latency, \"\n",
    "                  f\"{metrics['v2']['error_rate']*100:.2f}% errors\")\n",
    "            \n",
    "            # Decision logic\n",
    "            if metrics['v2']['error_rate'] > 0.05:  # >5% errors\n",
    "                print(f\"\\n‚ùå ERROR RATE TOO HIGH! Rolling back to v1...\")\n",
    "                self.update_weights(0)  # Rollback to 100% v1\n",
    "                break\n",
    "            \n",
    "            if metrics['v2']['avg_latency'] > metrics['v1']['avg_latency'] * 1.5:  # 50% slower\n",
    "                print(f\"\\n‚ö†Ô∏è  LATENCY REGRESSION! Pausing rollout...\")\n",
    "                break\n",
    "            \n",
    "            print(f\"‚úÖ Stage {stage_percentage}% successful, proceeding...\")\n",
    "            time.sleep(0.1)  # Simulate monitoring period\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üéâ Canary rollout complete!\")\n",
    "        print(f\"   Final weight: v1={self.current_weight_v1}%, v2={self.current_weight_v2}%\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "\n",
    "class ABTestManager:\n",
    "    \"\"\"Manage A/B testing with header-based routing.\"\"\"\n",
    "    \n",
    "    def __init__(self, control_plane: ControlPlane):\n",
    "        self.control_plane = control_plane\n",
    "        self.experiments: Dict[str, Dict] = {}\n",
    "    \n",
    "    def create_experiment(self, name: str, source: str, destination: str, \n",
    "                         test_header: Dict[str, str], control_version: str = \"v1\", \n",
    "                         treatment_version: str = \"v2\"):\n",
    "        \"\"\"Create A/B test experiment.\"\"\"\n",
    "        # Add routing rule with header matching\n",
    "        rule = RoutingRule(\n",
    "            source_service=source,\n",
    "            destination_service=destination,\n",
    "            version_weights={control_version: 50, treatment_version: 50},\n",
    "            headers=test_header\n",
    "        )\n",
    "        self.control_plane.add_routing_rule(rule)\n",
    "        \n",
    "        self.experiments[name] = {\n",
    "            \"source\": source,\n",
    "            \"destination\": destination,\n",
    "            \"test_header\": test_header,\n",
    "            \"control_version\": control_version,\n",
    "            \"treatment_version\": treatment_version,\n",
    "            \"control_metrics\": [],\n",
    "            \"treatment_metrics\": []\n",
    "        }\n",
    "        \n",
    "        print(f\"üß™ A/B Test Created: {name}\")\n",
    "        print(f\"   Control: {control_version}\")\n",
    "        print(f\"   Treatment: {treatment_version}\")\n",
    "        print(f\"   Header: {test_header}\")\n",
    "    \n",
    "    def run_experiment(self, name: str, proxy: SidecarProxy, num_requests: int = 1000):\n",
    "        \"\"\"Run A/B test experiment.\"\"\"\n",
    "        exp = self.experiments[name]\n",
    "        \n",
    "        control_responses = []\n",
    "        treatment_responses = []\n",
    "        \n",
    "        for i in range(num_requests):\n",
    "            # Alternate between control and treatment groups\n",
    "            if i % 2 == 0:\n",
    "                # Control group (no special header)\n",
    "                request = Request(\n",
    "                    source_service=exp[\"source\"],\n",
    "                    destination_service=exp[\"destination\"],\n",
    "                    headers={}\n",
    "                )\n",
    "            else:\n",
    "                # Treatment group (with test header)\n",
    "                request = Request(\n",
    "                    source_service=exp[\"source\"],\n",
    "                    destination_service=exp[\"destination\"],\n",
    "                    headers=exp[\"test_header\"]\n",
    "                )\n",
    "            \n",
    "            response = proxy.forward_request(request)\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                control_responses.append(response)\n",
    "            else:\n",
    "                treatment_responses.append(response)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        control_latency = np.mean([r.latency_ms for r in control_responses])\n",
    "        treatment_latency = np.mean([r.latency_ms for r in treatment_responses])\n",
    "        \n",
    "        control_errors = sum(1 for r in control_responses if r.status_code >= 400) / len(control_responses)\n",
    "        treatment_errors = sum(1 for r in treatment_responses if r.status_code >= 400) / len(treatment_responses)\n",
    "        \n",
    "        exp[\"control_metrics\"].append({\n",
    "            \"latency\": control_latency,\n",
    "            \"error_rate\": control_errors\n",
    "        })\n",
    "        exp[\"treatment_metrics\"].append({\n",
    "            \"latency\": treatment_latency,\n",
    "            \"error_rate\": treatment_errors\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nüìä A/B Test Results: {name}\")\n",
    "        print(f\"   Control ({exp['control_version']}):\")\n",
    "        print(f\"     - Requests: {len(control_responses)}\")\n",
    "        print(f\"     - Avg Latency: {control_latency:.2f} ms\")\n",
    "        print(f\"     - Error Rate: {control_errors*100:.2f}%\")\n",
    "        print(f\"   Treatment ({exp['treatment_version']}):\")\n",
    "        print(f\"     - Requests: {len(treatment_responses)}\")\n",
    "        print(f\"     - Avg Latency: {treatment_latency:.2f} ms\")\n",
    "        print(f\"     - Error Rate: {treatment_errors*100:.2f}%\")\n",
    "        \n",
    "        # Statistical significance (simplified)\n",
    "        latency_diff = ((treatment_latency - control_latency) / control_latency) * 100\n",
    "        print(f\"\\nüìà Latency Impact: {latency_diff:+.2f}%\")\n",
    "        \n",
    "        if abs(latency_diff) < 5:\n",
    "            print(\"   ‚úÖ No significant latency difference\")\n",
    "        elif latency_diff < 0:\n",
    "            print(\"   ‚úÖ Treatment is faster!\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  Treatment is slower\")\n",
    "        \n",
    "        return {\n",
    "            \"control_latency\": control_latency,\n",
    "            \"treatment_latency\": treatment_latency,\n",
    "            \"control_errors\": control_errors,\n",
    "            \"treatment_errors\": treatment_errors,\n",
    "            \"latency_impact_pct\": latency_diff\n",
    "        }\n",
    "\n",
    "\n",
    "# Example 1: Canary Deployment with Gradual Rollout\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE 1: Canary Deployment - Gradual Rollout\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Setup\n",
    "canary = CanaryDeployment(control_plane, \"feature-service\", \"model-service\")\n",
    "proxy = SidecarProxy(\"feature-service\", \"feature-service-0\", control_plane)\n",
    "\n",
    "# Execute gradual rollout: 5% ‚Üí 10% ‚Üí 25% ‚Üí 50% ‚Üí 100%\n",
    "canary.gradual_rollout(proxy, stages=[5, 10, 25, 50, 100], requests_per_stage=500)\n",
    "\n",
    "# Visualize rollout history\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE 2: Canary Rollout Visualization\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "stages = [h[\"v2_weight\"] for h in canary.history]\n",
    "v1_latencies = [h[\"metrics\"][\"v1\"][\"avg_latency\"] for h in canary.history]\n",
    "v2_latencies = [h[\"metrics\"][\"v2\"][\"avg_latency\"] for h in canary.history]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Traffic distribution\n",
    "ax1.plot(stages, [100-s for s in stages], marker='o', linewidth=2.5, markersize=10, \n",
    "         label='v1 (stable)', color='#4ECDC4')\n",
    "ax1.plot(stages, stages, marker='s', linewidth=2.5, markersize=10, \n",
    "         label='v2 (canary)', color='#FF6B6B')\n",
    "ax1.fill_between(stages, 0, [100-s for s in stages], alpha=0.3, color='#4ECDC4')\n",
    "ax1.fill_between(stages, 0, stages, alpha=0.3, color='#FF6B6B')\n",
    "ax1.set_xlabel(\"Rollout Stage (%)\", fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel(\"Traffic Percentage (%)\", fontsize=12, fontweight='bold')\n",
    "ax1.set_title(\"Canary Release: Gradual Traffic Shift\\n(v1 ‚Üí v2)\", \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "ax1.legend(fontsize=11, loc='center left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 105)\n",
    "\n",
    "# Plot 2: Latency comparison\n",
    "ax2.plot(stages, v1_latencies, marker='o', linewidth=2.5, markersize=10, \n",
    "         label='v1 latency', color='#4ECDC4')\n",
    "ax2.plot(stages, v2_latencies, marker='s', linewidth=2.5, markersize=10, \n",
    "         label='v2 latency', color='#FF6B6B')\n",
    "ax2.set_xlabel(\"Rollout Stage (%)\", fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel(\"Avg Latency (ms)\", fontsize=12, fontweight='bold')\n",
    "ax2.set_title(\"Latency Monitoring During Rollout\\n(Check for regressions)\", \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE 3: A/B Testing with Header-Based Routing\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create A/B test\n",
    "ab_test = ABTestManager(control_plane)\n",
    "ab_test.create_experiment(\n",
    "    name=\"wafer_yield_model_v2_test\",\n",
    "    source=\"feature-service\",\n",
    "    destination=\"model-service\",\n",
    "    test_header={\"x-user-segment\": \"premium\"},\n",
    "    control_version=\"v1\",\n",
    "    treatment_version=\"v2\"\n",
    ")\n",
    "\n",
    "# Run experiment\n",
    "ab_proxy = SidecarProxy(\"feature-service\", \"feature-service-1\", control_plane)\n",
    "results = ab_test.run_experiment(\"wafer_yield_model_v2_test\", ab_proxy, num_requests=1000)\n",
    "\n",
    "print(\"\\nüí° Traffic Management Capabilities Demonstrated:\")\n",
    "print(\"   ‚úÖ Canary release (gradual 5% ‚Üí 100% rollout)\")\n",
    "print(\"   ‚úÖ Automatic rollback (if error rate >5%)\")\n",
    "print(\"   ‚úÖ A/B testing (header-based routing)\")\n",
    "print(\"   ‚úÖ Metrics comparison (latency, errors between versions)\")\n",
    "print(\"   ‚úÖ Risk mitigation (detect issues with small % of traffic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10954154",
   "metadata": {},
   "source": [
    "## 4. üõ°Ô∏è Resilience Patterns - Circuit Breakers, Retries, Timeouts\n",
    "\n",
    "### üìù What's Happening in This Section?\n",
    "\n",
    "**Purpose:** Protect ML pipelines from cascading failures using resilience patterns\n",
    "\n",
    "**Key Points:**\n",
    "- **Circuit Breaker**: Stop calling failing service (open circuit after 50% error rate, retry after 30s)\n",
    "- **Automatic Retries**: Retry transient failures (network blips, temporary overload) with exponential backoff\n",
    "- **Timeouts**: Prevent hanging requests (fail fast after 10s instead of waiting 5 minutes)\n",
    "- **Bulkhead Pattern**: Isolate failures (separate thread pools for critical vs non-critical services)\n",
    "- **Fallback**: Return cached/default result when service unavailable\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Prevent Cascade Failures**: One slow service doesn't bring down entire pipeline\n",
    "- **Improve Availability**: Auto-retry succeeds 80% of the time for transient errors\n",
    "- **Resource Protection**: Timeouts free up connection pools, prevent resource exhaustion\n",
    "- **Graceful Degradation**: Return lower-quality result instead of complete failure\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "Wafer map analysis pipeline: Spatial analyzer calls external defect classification API (2% error rate) ‚Üí circuit breaker opens after 5 consecutive failures ‚Üí return cached classification ‚Üí auto-retry after 30s ‚Üí restore service when API healthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resilience Patterns Simulation\n",
    "\n",
    "class CircuitBreakerState(Enum):\n",
    "    \"\"\"Circuit breaker states.\"\"\"\n",
    "    CLOSED = \"closed\"  # Normal operation\n",
    "    OPEN = \"open\"      # Blocking requests (service unhealthy)\n",
    "    HALF_OPEN = \"half_open\"  # Testing if service recovered\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CircuitBreaker:\n",
    "    \"\"\"Circuit breaker for resilience.\"\"\"\n",
    "    service_name: str\n",
    "    failure_threshold: int = 5  # Open after N consecutive failures\n",
    "    success_threshold: int = 2  # Close after N consecutive successes (in half-open)\n",
    "    timeout_seconds: int = 30   # Time before half-open\n",
    "    \n",
    "    state: CircuitBreakerState = CircuitBreakerState.CLOSED\n",
    "    consecutive_failures: int = 0\n",
    "    consecutive_successes: int = 0\n",
    "    last_failure_time: Optional[datetime] = None\n",
    "    total_requests: int = 0\n",
    "    blocked_requests: int = 0\n",
    "    \n",
    "    def call(self, success: bool) -> bool:\n",
    "        \"\"\"Attempt call through circuit breaker.\"\"\"\n",
    "        self.total_requests += 1\n",
    "        \n",
    "        # Check if circuit should transition from OPEN ‚Üí HALF_OPEN\n",
    "        if self.state == CircuitBreakerState.OPEN:\n",
    "            if self.last_failure_time:\n",
    "                elapsed = (datetime.now() - self.last_failure_time).total_seconds()\n",
    "                if elapsed >= self.timeout_seconds:\n",
    "                    print(f\"‚è∞ Circuit breaker timeout elapsed, entering HALF_OPEN state\")\n",
    "                    self.state = CircuitBreakerState.HALF_OPEN\n",
    "                    self.consecutive_successes = 0\n",
    "                    self.consecutive_failures = 0\n",
    "                else:\n",
    "                    # Still open, block request\n",
    "                    self.blocked_requests += 1\n",
    "                    print(f\"üö´ Circuit OPEN: Request blocked ({self.blocked_requests} total)\")\n",
    "                    return False\n",
    "        \n",
    "        # State: CLOSED or HALF_OPEN, allow request\n",
    "        if success:\n",
    "            self.consecutive_failures = 0\n",
    "            self.consecutive_successes += 1\n",
    "            \n",
    "            # Transition: HALF_OPEN ‚Üí CLOSED\n",
    "            if (self.state == CircuitBreakerState.HALF_OPEN and \n",
    "                self.consecutive_successes >= self.success_threshold):\n",
    "                print(f\"‚úÖ Circuit breaker CLOSED: Service recovered ({self.consecutive_successes} successes)\")\n",
    "                self.state = CircuitBreakerState.CLOSED\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            self.consecutive_successes = 0\n",
    "            self.consecutive_failures += 1\n",
    "            self.last_failure_time = datetime.now()\n",
    "            \n",
    "            # Transition: CLOSED ‚Üí OPEN\n",
    "            if (self.state == CircuitBreakerState.CLOSED and \n",
    "                self.consecutive_failures >= self.failure_threshold):\n",
    "                print(f\"‚ùå Circuit breaker OPEN: Too many failures ({self.consecutive_failures}/{self.failure_threshold})\")\n",
    "                self.state = CircuitBreakerState.OPEN\n",
    "                self.blocked_requests += 1\n",
    "                return False\n",
    "            \n",
    "            # Transition: HALF_OPEN ‚Üí OPEN (service still unhealthy)\n",
    "            if self.state == CircuitBreakerState.HALF_OPEN:\n",
    "                print(f\"‚ùå Circuit breaker OPEN: Service still failing\")\n",
    "                self.state = CircuitBreakerState.OPEN\n",
    "                self.blocked_requests += 1\n",
    "                return False\n",
    "            \n",
    "            return False\n",
    "\n",
    "\n",
    "class RetryPolicy:\n",
    "    \"\"\"Retry policy with exponential backoff.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_retries: int = 3, base_delay_ms: float = 100, max_delay_ms: float = 5000):\n",
    "        self.max_retries = max_retries\n",
    "        self.base_delay_ms = base_delay_ms\n",
    "        self.max_delay_ms = max_delay_ms\n",
    "        self.total_retries = 0\n",
    "        self.successful_retries = 0\n",
    "    \n",
    "    def execute(self, func, *args, **kwargs):\n",
    "        \"\"\"Execute function with retries.\"\"\"\n",
    "        for attempt in range(self.max_retries + 1):\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                \n",
    "                if attempt > 0:\n",
    "                    self.successful_retries += 1\n",
    "                    print(f\"   ‚úÖ Retry {attempt} succeeded\")\n",
    "                \n",
    "                return result\n",
    "            except Exception as e:\n",
    "                if attempt < self.max_retries:\n",
    "                    self.total_retries += 1\n",
    "                    \n",
    "                    # Exponential backoff: 100ms, 200ms, 400ms, 800ms, ...\n",
    "                    delay_ms = min(self.base_delay_ms * (2 ** attempt), self.max_delay_ms)\n",
    "                    print(f\"   ‚ö†Ô∏è  Attempt {attempt + 1} failed: {e}\")\n",
    "                    print(f\"   ‚è≥ Retrying in {delay_ms:.0f}ms...\")\n",
    "                    time.sleep(delay_ms / 1000)\n",
    "                else:\n",
    "                    print(f\"   ‚ùå All {self.max_retries} retries exhausted\")\n",
    "                    raise\n",
    "\n",
    "\n",
    "class TimeoutPolicy:\n",
    "    \"\"\"Request timeout policy.\"\"\"\n",
    "    \n",
    "    def __init__(self, timeout_seconds: float = 10.0):\n",
    "        self.timeout_seconds = timeout_seconds\n",
    "        self.timeout_count = 0\n",
    "    \n",
    "    def execute(self, func, latency_ms: float, *args, **kwargs):\n",
    "        \"\"\"Execute function with timeout.\"\"\"\n",
    "        if latency_ms > self.timeout_seconds * 1000:\n",
    "            self.timeout_count += 1\n",
    "            raise TimeoutError(f\"Request exceeded timeout ({latency_ms:.0f}ms > {self.timeout_seconds*1000:.0f}ms)\")\n",
    "        \n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "\n",
    "# Example 1: Circuit Breaker Pattern\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE 1: Circuit Breaker - Protect from Cascading Failures\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "circuit_breaker = CircuitBreaker(\n",
    "    service_name=\"spatial-analyzer-api\",\n",
    "    failure_threshold=5,\n",
    "    success_threshold=2,\n",
    "    timeout_seconds=30\n",
    ")\n",
    "\n",
    "print(f\"\\nüîß Circuit Breaker Configuration:\")\n",
    "print(f\"   ‚Ä¢ Failure threshold: {circuit_breaker.failure_threshold} (open after N failures)\")\n",
    "print(f\"   ‚Ä¢ Success threshold: {circuit_breaker.success_threshold} (close after N successes)\")\n",
    "print(f\"   ‚Ä¢ Timeout: {circuit_breaker.timeout_seconds}s (before retry)\")\n",
    "\n",
    "# Simulate API calls with failures\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Simulating API calls (50% error rate)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "success_count = 0\n",
    "failure_count = 0\n",
    "blocked_count = 0\n",
    "\n",
    "for i in range(20):\n",
    "    # Simulate 50% error rate for first 10 calls\n",
    "    if i < 10:\n",
    "        success = np.random.random() > 0.5\n",
    "    else:\n",
    "        # API recovered, 95% success rate\n",
    "        success = np.random.random() > 0.05\n",
    "    \n",
    "    print(f\"\\nRequest {i+1}:\")\n",
    "    allowed = circuit_breaker.call(success)\n",
    "    \n",
    "    if not allowed:\n",
    "        blocked_count += 1\n",
    "    elif success:\n",
    "        success_count += 1\n",
    "        print(f\"   ‚úÖ Success\")\n",
    "    else:\n",
    "        failure_count += 1\n",
    "        print(f\"   ‚ùå Failed\")\n",
    "    \n",
    "    print(f\"   State: {circuit_breaker.state.value.upper()}\")\n",
    "    time.sleep(0.05)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Circuit Breaker Summary:\")\n",
    "print(f\"   ‚Ä¢ Total requests: {circuit_breaker.total_requests}\")\n",
    "print(f\"   ‚Ä¢ Successful: {success_count}\")\n",
    "print(f\"   ‚Ä¢ Failed: {failure_count}\")\n",
    "print(f\"   ‚Ä¢ Blocked: {blocked_count}\")\n",
    "print(f\"   ‚Ä¢ Final state: {circuit_breaker.state.value.upper()}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE 2: Retry Policy with Exponential Backoff\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "retry_policy = RetryPolicy(max_retries=3, base_delay_ms=100, max_delay_ms=1000)\n",
    "\n",
    "# Simulate function that fails first 2 times, succeeds on 3rd\n",
    "def unreliable_api_call(attempt_tracker: List[int]):\n",
    "    \"\"\"Simulates unreliable API.\"\"\"\n",
    "    attempt_tracker[0] += 1\n",
    "    \n",
    "    if attempt_tracker[0] <= 2:\n",
    "        raise Exception(f\"Network error (attempt {attempt_tracker[0]})\")\n",
    "    \n",
    "    return {\"status\": \"success\", \"data\": \"wafer_analysis_results\"}\n",
    "\n",
    "print(f\"\\nüîß Retry Policy Configuration:\")\n",
    "print(f\"   ‚Ä¢ Max retries: {retry_policy.max_retries}\")\n",
    "print(f\"   ‚Ä¢ Base delay: {retry_policy.base_delay_ms}ms\")\n",
    "print(f\"   ‚Ä¢ Max delay: {retry_policy.max_delay_ms}ms\")\n",
    "print(f\"   ‚Ä¢ Backoff: Exponential (100ms ‚Üí 200ms ‚Üí 400ms ‚Üí ...)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Calling unreliable API (fails first 2 times)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "attempt_tracker = [0]\n",
    "try:\n",
    "    result = retry_policy.execute(unreliable_api_call, attempt_tracker)\n",
    "    print(f\"\\n‚úÖ Final result: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Final failure: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Retry Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total retries: {retry_policy.total_retries}\")\n",
    "print(f\"   ‚Ä¢ Successful retries: {retry_policy.successful_retries}\")\n",
    "print(f\"   ‚Ä¢ Success rate: {retry_policy.successful_retries / retry_policy.total_retries * 100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE 3: Timeout Policy - Fail Fast\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "timeout_policy = TimeoutPolicy(timeout_seconds=1.0)\n",
    "\n",
    "print(f\"\\nüîß Timeout Policy Configuration:\")\n",
    "print(f\"   ‚Ä¢ Timeout: {timeout_policy.timeout_seconds}s\")\n",
    "print(f\"   ‚Ä¢ Strategy: Fail fast (don't wait indefinitely)\")\n",
    "\n",
    "# Simulate requests with varying latencies\n",
    "latencies = [50, 150, 300, 800, 1200, 2000, 100]  # ms\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Simulating requests with varying latencies\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for i, latency in enumerate(latencies):\n",
    "    print(f\"Request {i+1}: latency={latency}ms\")\n",
    "    \n",
    "    try:\n",
    "        def dummy_request():\n",
    "            return {\"status\": \"success\"}\n",
    "        \n",
    "        timeout_policy.execute(dummy_request, latency)\n",
    "        print(f\"   ‚úÖ Success ({latency}ms < {timeout_policy.timeout_seconds*1000}ms)\")\n",
    "    except TimeoutError as e:\n",
    "        print(f\"   ‚ùå Timeout: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Timeout Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total requests: {len(latencies)}\")\n",
    "print(f\"   ‚Ä¢ Timeouts: {timeout_policy.timeout_count}\")\n",
    "print(f\"   ‚Ä¢ Timeout rate: {timeout_policy.timeout_count / len(latencies) * 100:.1f}%\")\n",
    "\n",
    "# Visualize resilience patterns impact\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE 4: Resilience Patterns Impact Visualization\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simulate service with/without resilience patterns\n",
    "np.random.seed(42)\n",
    "\n",
    "# Scenario: External API with 20% error rate\n",
    "num_requests = 100\n",
    "base_error_rate = 0.20\n",
    "\n",
    "# Without resilience (direct calls)\n",
    "without_resilience_successes = []\n",
    "for _ in range(num_requests):\n",
    "    success = np.random.random() > base_error_rate\n",
    "    without_resilience_successes.append(1 if success else 0)\n",
    "\n",
    "without_resilience_success_rate = np.mean(without_resilience_successes) * 100\n",
    "\n",
    "# With resilience (retries + circuit breaker)\n",
    "with_resilience_successes = []\n",
    "cb = CircuitBreaker(service_name=\"test\", failure_threshold=5, timeout_seconds=5)\n",
    "retry = RetryPolicy(max_retries=2)\n",
    "\n",
    "for _ in range(num_requests):\n",
    "    # Simulate with retries\n",
    "    success_attempts = []\n",
    "    for attempt in range(3):  # 1 initial + 2 retries\n",
    "        success = np.random.random() > base_error_rate\n",
    "        success_attempts.append(success)\n",
    "        if success:\n",
    "            break\n",
    "    \n",
    "    final_success = any(success_attempts)\n",
    "    \n",
    "    # Circuit breaker check\n",
    "    allowed = cb.call(final_success)\n",
    "    \n",
    "    with_resilience_successes.append(1 if (allowed and final_success) else 0)\n",
    "\n",
    "with_resilience_success_rate = np.mean(with_resilience_successes) * 100\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot 1: Success rate comparison\n",
    "categories = ['Without\\nResilience', 'With Retries +\\nCircuit Breaker']\n",
    "success_rates = [without_resilience_success_rate, with_resilience_success_rate]\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "\n",
    "bars = ax1.bar(categories, success_rates, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.axhline(y=95, color='green', linestyle='--', linewidth=2, label='Target (95%)')\n",
    "ax1.set_ylabel(\"Success Rate (%)\", fontsize=12, fontweight='bold')\n",
    "ax1.set_title(\"Service Availability Improvement\\n(20% base error rate)\", \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "ax1.set_ylim(0, 105)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, rate in zip(bars, success_rates):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "             f'{rate:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 2: Cumulative success over time\n",
    "ax2.plot(np.cumsum(without_resilience_successes), linewidth=2.5, \n",
    "         label='Without Resilience', color='#FF6B6B')\n",
    "ax2.plot(np.cumsum(with_resilience_successes), linewidth=2.5, \n",
    "         label='With Resilience', color='#4ECDC4')\n",
    "ax2.set_xlabel(\"Request Number\", fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel(\"Cumulative Successes\", fontsize=12, fontweight='bold')\n",
    "ax2.set_title(\"Cumulative Success Over Time\\n(Resilience reduces failures)\", \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Improvement breakdown\n",
    "improvements = {\n",
    "    'Base\\nSuccess': 100 - base_error_rate * 100,\n",
    "    '+Retries': (with_resilience_success_rate - without_resilience_success_rate) * 0.7,\n",
    "    '+Circuit\\nBreaker': (with_resilience_success_rate - without_resilience_success_rate) * 0.3\n",
    "}\n",
    "\n",
    "x_pos = np.arange(len(improvements))\n",
    "bars3 = ax3.bar(improvements.keys(), improvements.values(), \n",
    "                color=['#95E1D3', '#F38181', '#AA96DA'], alpha=0.8,\n",
    "                edgecolor='black', linewidth=2)\n",
    "ax3.set_ylabel(\"Contribution (%)\", fontsize=12, fontweight='bold')\n",
    "ax3.set_title(\"Resilience Pattern Contributions\\n(to overall success rate)\", \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "             f'{height:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Resilience Impact:\")\n",
    "print(f\"   Without resilience: {without_resilience_success_rate:.1f}% success\")\n",
    "print(f\"   With resilience: {with_resilience_success_rate:.1f}% success\")\n",
    "print(f\"   Improvement: +{with_resilience_success_rate - without_resilience_success_rate:.1f} percentage points\")\n",
    "\n",
    "print(\"\\nüí° Resilience Patterns Demonstrated:\")\n",
    "print(\"   ‚úÖ Circuit breaker (prevent cascade failures)\")\n",
    "print(\"   ‚úÖ Automatic retries (exponential backoff)\")\n",
    "print(\"   ‚úÖ Timeouts (fail fast, free resources)\")\n",
    "print(\"   ‚úÖ Success rate improvement (80% ‚Üí 90%+)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bfdafa",
   "metadata": {},
   "source": [
    "## 5. üöÄ Real-World Projects Using Service Mesh\n",
    "\n",
    "Build production ML services with Istio and Linkerd:\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 1: Multi-Model Inference Pipeline with Canary Releases** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "**Objective:** Deploy 5-service ML inference pipeline with automatic canary releases and rollback\n",
    "\n",
    "**Business Value:**  \n",
    "- $280K/year savings (catch regressions before full rollout, reduce downtime from bad deployments)\n",
    "- 99.95% uptime (circuit breakers prevent cascade failures)\n",
    "- 3x faster deployments (gradual rollouts with automatic rollback)\n",
    "\n",
    "**Success Criteria:**\n",
    "- ‚úÖ Pipeline handles 1000 req/sec with <100ms p99 latency\n",
    "- ‚úÖ Canary release completes 5% ‚Üí 100% in 24 hours (no manual intervention)\n",
    "- ‚úÖ Automatic rollback triggered if error rate >1% or latency >150ms\n",
    "- ‚úÖ Distributed tracing shows end-to-end request flow across all services\n",
    "\n",
    "**Pipeline Services:**\n",
    "1. **Feature Engineering Service:** Extract 50 features from raw wafer test data\n",
    "2. **Wafer Map Analyzer:** Spatial pattern detection (defects, hotspots)\n",
    "3. **Parametric Model:** Predict yield from electrical parameters\n",
    "4. **Spatial Model:** Predict yield from wafer map patterns\n",
    "5. **Ensemble Combiner:** Weighted voting (parametric 60%, spatial 40%)\n",
    "\n",
    "**Service Mesh Features:**\n",
    "- **Istio VirtualService:** Traffic splitting (v2.4: 95%, v2.5: 5%)\n",
    "- **DestinationRule:** Circuit breaker (max connections: 100, consecutive errors: 5)\n",
    "- **ServiceEntry:** External defect classification API with timeout (10s)\n",
    "- **Prometheus metrics:** Request rate, latency, error rate (automatic)\n",
    "- **Jaeger tracing:** Distributed traces across all 5 services\n",
    "\n",
    "**Implementation Hints:**\n",
    "```yaml\n",
    "# Canary VirtualService\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: VirtualService\n",
    "metadata:\n",
    "  name: ensemble-combiner\n",
    "spec:\n",
    "  hosts:\n",
    "  - ensemble-combiner\n",
    "  http:\n",
    "  - match:\n",
    "    - headers:\n",
    "        x-canary:\n",
    "          exact: \"true\"\n",
    "    route:\n",
    "    - destination:\n",
    "        host: ensemble-combiner\n",
    "        subset: v2-5\n",
    "      weight: 100\n",
    "  - route:\n",
    "    - destination:\n",
    "        host: ensemble-combiner\n",
    "        subset: v2-4\n",
    "      weight: 95\n",
    "    - destination:\n",
    "        host: ensemble-combiner\n",
    "        subset: v2-5\n",
    "      weight: 5\n",
    "---\n",
    "# Circuit Breaker DestinationRule\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: DestinationRule\n",
    "metadata:\n",
    "  name: wafer-map-analyzer\n",
    "spec:\n",
    "  host: wafer-map-analyzer\n",
    "  trafficPolicy:\n",
    "    connectionPool:\n",
    "      tcp:\n",
    "        maxConnections: 100\n",
    "      http:\n",
    "        http1MaxPendingRequests: 50\n",
    "        maxRequestsPerConnection: 2\n",
    "    outlierDetection:\n",
    "      consecutiveErrors: 5\n",
    "      interval: 30s\n",
    "      baseEjectionTime: 60s\n",
    "```\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Deploy new ensemble model v2.5 (96.5% accuracy) with canary release, monitor for 24 hours, auto-rollback if p99 latency >150ms or accuracy drops below 95%\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 2: Zero-Trust mTLS for STDF Processing** ‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "**Objective:** Secure STDF parsing pipeline with automatic mTLS and authorization policies\n",
    "\n",
    "**Business Value:**  \n",
    "- $150K/year savings (eliminate manual certificate management, pass security audits)\n",
    "- Compliance with data security requirements (all traffic encrypted)\n",
    "- 30-minute incident response (detect unauthorized access via service graph)\n",
    "\n",
    "**Success Criteria:**\n",
    "- ‚úÖ All service-to-service traffic encrypted with mTLS (100% coverage)\n",
    "- ‚úÖ Certificates auto-rotated every 24 hours (zero manual intervention)\n",
    "- ‚úÖ Authorization policies enforce least-privilege (parser can't call storage directly)\n",
    "- ‚úÖ Security audit passes (all traffic logged, encrypted, authorized)\n",
    "\n",
    "**Pipeline Services:**\n",
    "1. **STDF Parser:** Parse binary STDF files (IEEE 1505 format)\n",
    "2. **Feature Extractor:** Extract parametric features (Vdd, Idd, frequency)\n",
    "3. **Outlier Detector:** Detect parametric anomalies (z-score >3)\n",
    "4. **Results Storage:** Store parsed data in PostgreSQL\n",
    "\n",
    "**Service Mesh Security:**\n",
    "- **Linkerd automatic mTLS:** All traffic encrypted without code changes\n",
    "- **AuthorizationPolicy:** Parser ‚Üí Extractor (allow), Parser ‚Üí Storage (deny)\n",
    "- **PeerAuthentication:** Require mTLS for all services (STRICT mode)\n",
    "- **Certificate rotation:** Auto-rotate every 24 hours (Linkerd identity)\n",
    "\n",
    "**Implementation Hints:**\n",
    "```yaml\n",
    "# Linkerd automatic mTLS (annotation on namespace)\n",
    "apiVersion: v1\n",
    "kind: Namespace\n",
    "metadata:\n",
    "  name: stdf-processing\n",
    "  annotations:\n",
    "    linkerd.io/inject: enabled\n",
    "---\n",
    "# Istio AuthorizationPolicy\n",
    "apiVersion: security.istio.io/v1beta1\n",
    "kind: AuthorizationPolicy\n",
    "metadata:\n",
    "  name: stdf-parser-policy\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: feature-extractor\n",
    "  action: ALLOW\n",
    "  rules:\n",
    "  - from:\n",
    "    - source:\n",
    "        principals: [\"cluster.local/ns/stdf-processing/sa/stdf-parser\"]\n",
    "---\n",
    "# Deny direct storage access\n",
    "apiVersion: security.istio.io/v1beta1\n",
    "kind: AuthorizationPolicy\n",
    "metadata:\n",
    "  name: storage-deny-parser\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: results-storage\n",
    "  action: DENY\n",
    "  rules:\n",
    "  - from:\n",
    "    - source:\n",
    "        principals: [\"cluster.local/ns/stdf-processing/sa/stdf-parser\"]\n",
    "```\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Secure proprietary wafer test data (trade secrets) with end-to-end mTLS, prevent parser from directly accessing storage (defense in depth)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 3: Chaos Engineering with Fault Injection** ‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "**Objective:** Test ML pipeline resilience by injecting faults (delays, errors, aborts)\n",
    "\n",
    "**Business Value:**  \n",
    "- $220K/year savings (proactively find weaknesses before production incidents)\n",
    "- 60% reduction in MTTR (mean time to recovery, from 4 hours ‚Üí 1.5 hours)\n",
    "- 99.9% availability improvement (find and fix single points of failure)\n",
    "\n",
    "**Success Criteria:**\n",
    "- ‚úÖ Pipeline survives 50% error injection on non-critical services\n",
    "- ‚úÖ Circuit breakers open after 5 consecutive failures (prevent cascade)\n",
    "- ‚úÖ Automatic retries succeed 80% of the time for transient errors\n",
    "- ‚úÖ Critical services (payment, logging) have fallback mechanisms\n",
    "\n",
    "**Chaos Experiments:**\n",
    "1. **Latency Injection:** Add 5s delay to feature service (test timeouts)\n",
    "2. **Error Injection:** 50% error rate on spatial analyzer (test circuit breakers)\n",
    "3. **Abort Injection:** Kill ensemble combiner pod (test pod restart)\n",
    "4. **Network Partition:** Block traffic between services (test retry logic)\n",
    "\n",
    "**Service Mesh Fault Injection:**\n",
    "- **VirtualService with delays:** Inject 5s delay on 20% of requests\n",
    "- **VirtualService with aborts:** Return HTTP 500 on 50% of requests\n",
    "- **DestinationRule with retries:** Retry 3x with exponential backoff\n",
    "\n",
    "**Implementation Hints:**\n",
    "```yaml\n",
    "# Latency injection\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: VirtualService\n",
    "metadata:\n",
    "  name: feature-service-fault\n",
    "spec:\n",
    "  hosts:\n",
    "  - feature-service\n",
    "  http:\n",
    "  - fault:\n",
    "      delay:\n",
    "        percentage:\n",
    "          value: 20.0\n",
    "        fixedDelay: 5s\n",
    "    route:\n",
    "    - destination:\n",
    "        host: feature-service\n",
    "---\n",
    "# Error injection\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: VirtualService\n",
    "metadata:\n",
    "  name: spatial-analyzer-fault\n",
    "spec:\n",
    "  hosts:\n",
    "  - spatial-analyzer\n",
    "  http:\n",
    "  - fault:\n",
    "      abort:\n",
    "        percentage:\n",
    "          value: 50.0\n",
    "        httpStatus: 500\n",
    "    route:\n",
    "    - destination:\n",
    "        host: spatial-analyzer\n",
    "---\n",
    "# Retry policy\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: VirtualService\n",
    "metadata:\n",
    "  name: ensemble-combiner-retry\n",
    "spec:\n",
    "  hosts:\n",
    "  - ensemble-combiner\n",
    "  http:\n",
    "  - retries:\n",
    "      attempts: 3\n",
    "      perTryTimeout: 2s\n",
    "      retryOn: 5xx\n",
    "    route:\n",
    "    - destination:\n",
    "        host: ensemble-combiner\n",
    "```\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Test wafer analysis pipeline resilience: inject 50% errors on external defect API, verify circuit breaker opens and cached classifications used\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 4: Distributed Tracing for Performance Debugging** ‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "**Objective:** Debug slow ML predictions using distributed tracing (Jaeger, Zipkin)\n",
    "\n",
    "**Business Value:**  \n",
    "- $95K/year savings (reduce debugging time from 8 hours/week ‚Üí 2 hours/week)\n",
    "- 40% latency reduction (identify and optimize slowest services)\n",
    "- 2x faster root cause analysis (trace shows exact service adding latency)\n",
    "\n",
    "**Success Criteria:**\n",
    "- ‚úÖ 100% of requests traced end-to-end (no sampling gaps)\n",
    "- ‚úÖ Traces show service-by-service latency breakdown\n",
    "- ‚úÖ Critical path identified (feature extraction adds 45ms, optimize first)\n",
    "- ‚úÖ Anomaly detection (flag requests >500ms for investigation)\n",
    "\n",
    "**Tracing Features:**\n",
    "- **Automatic span creation:** Istio/Linkerd proxies create spans for each service call\n",
    "- **Trace ID propagation:** Headers (x-request-id, x-b3-traceid) passed through pipeline\n",
    "- **Service graph:** Visualize request flow (Feature ‚Üí Model A ‚Üí Model B ‚Üí Ensemble)\n",
    "- **Latency heatmap:** Find p99 latency hotspots\n",
    "\n",
    "**Implementation Hints:**\n",
    "```python\n",
    "# Application code (propagate trace headers)\n",
    "def call_next_service(request_headers):\n",
    "    trace_headers = {\n",
    "        'x-request-id': request_headers.get('x-request-id'),\n",
    "        'x-b3-traceid': request_headers.get('x-b3-traceid'),\n",
    "        'x-b3-spanid': request_headers.get('x-b3-spanid'),\n",
    "        'x-b3-parentspanid': request_headers.get('x-b3-parentspanid'),\n",
    "        'x-b3-sampled': request_headers.get('x-b3-sampled')\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        'http://next-service:8080/predict',\n",
    "        headers=trace_headers,\n",
    "        json=data\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Jaeger query (find slow requests)\n",
    "# UI: http://jaeger:16686\n",
    "# Query: service=ensemble-combiner duration>500ms\n",
    "# Result: Traces sorted by latency, click to see service-by-service breakdown\n",
    "```\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Debug slow wafer yield predictions (p99 latency 350ms, target 100ms), trace shows feature extraction adds 120ms ‚Üí optimize by caching computed features\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 5: Blue-Green Deployment with Instant Rollback** ‚≠ê‚≠ê‚≠ê\n",
    "**Objective:** Deploy new model version to separate environment, switch traffic instantly\n",
    "\n",
    "**Business Value:**  \n",
    "- $180K/year savings (zero downtime during deployments)\n",
    "- 10-second rollback (vs 15 minutes for rolling update rollback)\n",
    "- 100% confidence in new version (test with production traffic before cutover)\n",
    "\n",
    "**Success Criteria:**\n",
    "- ‚úÖ Blue and green environments identical (same replicas, resources)\n",
    "- ‚úÖ Traffic switch completes in <10 seconds (VirtualService update)\n",
    "- ‚úÖ Rollback completes in <10 seconds (revert VirtualService)\n",
    "- ‚úÖ Zero dropped requests during cutover\n",
    "\n",
    "**Implementation Hints:**\n",
    "```yaml\n",
    "# Blue environment (production)\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: yield-model-blue\n",
    "spec:\n",
    "  replicas: 10\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: yield-model\n",
    "        version: blue\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: model\n",
    "        image: yield-model:v2.4\n",
    "---\n",
    "# Green environment (new version)\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: yield-model-green\n",
    "spec:\n",
    "  replicas: 10\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: yield-model\n",
    "        version: green\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: model\n",
    "        image: yield-model:v2.5\n",
    "---\n",
    "# VirtualService (switch traffic)\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: VirtualService\n",
    "metadata:\n",
    "  name: yield-model\n",
    "spec:\n",
    "  hosts:\n",
    "  - yield-model\n",
    "  http:\n",
    "  - route:\n",
    "    - destination:\n",
    "        host: yield-model\n",
    "        subset: green  # Switch to green (change to blue for rollback)\n",
    "      weight: 100\n",
    "```\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Deploy wafer yield model v2.5 to green environment, test with mirror traffic (10%), switch production traffic to green, rollback to blue if issues detected\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 6: Multi-Cluster Service Mesh (Global Load Balancing)** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "**Objective:** Deploy ML services across 3 regions (US, EU, Asia) with global load balancing\n",
    "\n",
    "**Business Value:**  \n",
    "- $420K/year savings (eliminate manual multi-region deployments)\n",
    "- 99.99% availability (survive entire region failure)\n",
    "- 60% latency reduction (route to nearest region)\n",
    "\n",
    "**Success Criteria:**\n",
    "- ‚úÖ Services deployed to 3 Kubernetes clusters (us-west, eu-central, asia-east)\n",
    "- ‚úÖ Cross-cluster service discovery (us-west can call eu-central)\n",
    "- ‚úÖ Locality-aware load balancing (route to nearest healthy region)\n",
    "- ‚úÖ Automatic failover (if us-west fails, route to eu-central)\n",
    "\n",
    "**Implementation Hints:**\n",
    "```bash\n",
    "# Install Istio multi-cluster (shared control plane)\n",
    "istioctl install --set profile=demo --set values.global.multiCluster.enabled=true\n",
    "\n",
    "# Link clusters\n",
    "istioctl x create-remote-secret --name=cluster-us-west | kubectl apply -f -\n",
    "istioctl x create-remote-secret --name=cluster-eu-central | kubectl apply -f -\n",
    "istioctl x create-remote-secret --name=cluster-asia-east | kubectl apply -f -\n",
    "\n",
    "# Deploy service to all clusters\n",
    "kubectl apply -f yield-model-deployment.yaml --context=us-west\n",
    "kubectl apply -f yield-model-deployment.yaml --context=eu-central\n",
    "kubectl apply -f yield-model-deployment.yaml --context=asia-east\n",
    "```\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Global wafer yield prediction service: US fabs route to us-west cluster, EU fabs to eu-central, Asia fabs to asia-east (reduce latency from 300ms ‚Üí 50ms)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 7: Rate Limiting and Quota Management** ‚≠ê‚≠ê‚≠ê\n",
    "**Objective:** Protect ML services from overload with rate limiting (100 req/sec per user)\n",
    "\n",
    "**Business Value:**  \n",
    "- $125K/year savings (prevent service overload, maintain SLA for premium users)\n",
    "- Fair resource allocation (no single user monopolizes resources)\n",
    "- DDoS protection (automatic throttling of abusive traffic)\n",
    "\n",
    "**Success Criteria:**\n",
    "- ‚úÖ Rate limit enforced: 100 req/sec per user (HTTP 429 if exceeded)\n",
    "- ‚úÖ Premium users get 500 req/sec quota (tiered limits)\n",
    "- ‚úÖ Global rate limits: 10,000 req/sec cluster-wide (prevent overload)\n",
    "- ‚úÖ Smooth degradation (throttle gradually, not hard cutoff)\n",
    "\n",
    "**Implementation Hints:**\n",
    "```yaml\n",
    "# Envoy rate limit config\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: EnvoyFilter\n",
    "metadata:\n",
    "  name: rate-limit-filter\n",
    "spec:\n",
    "  configPatches:\n",
    "  - applyTo: HTTP_FILTER\n",
    "    patch:\n",
    "      operation: INSERT_BEFORE\n",
    "      value:\n",
    "        name: envoy.filters.http.ratelimit\n",
    "        typed_config:\n",
    "          \"@type\": type.googleapis.com/envoy.extensions.filters.http.ratelimit.v3.RateLimit\n",
    "          domain: yield-model\n",
    "          rate_limit_service:\n",
    "            grpc_service:\n",
    "              envoy_grpc:\n",
    "                cluster_name: rate_limit_cluster\n",
    "---\n",
    "# Rate limit descriptor\n",
    "domain: yield-model\n",
    "descriptors:\n",
    "  - key: user_id\n",
    "    rate_limit:\n",
    "      unit: second\n",
    "      requests_per_unit: 100\n",
    "  - key: premium_user\n",
    "    rate_limit:\n",
    "      unit: second\n",
    "      requests_per_unit: 500\n",
    "```\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Protect STDF parser service from overload (test equipment can generate 500 files/sec burst), rate limit to 100 files/sec per fab, queue excess\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 8: Service Mesh Observability Dashboard** ‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "**Objective:** Build unified observability dashboard (Grafana + Prometheus + Kiali)\n",
    "\n",
    "**Business Value:**  \n",
    "- $95K/year savings (single pane of glass, eliminate tool-hopping)\n",
    "- 50% faster incident response (all metrics in one place)\n",
    "- Proactive monitoring (alerts fire before users notice issues)\n",
    "\n",
    "**Success Criteria:**\n",
    "- ‚úÖ Golden metrics (requests, errors, latency, saturation) for all services\n",
    "- ‚úÖ Service dependency graph (visualize traffic flow)\n",
    "- ‚úÖ Alerts configured (error rate >1%, latency p99 >200ms, saturation >80%)\n",
    "- ‚úÖ Historical data retention (30 days for trend analysis)\n",
    "\n",
    "**Dashboard Metrics:**\n",
    "- **Request Rate:** Requests/sec per service\n",
    "- **Error Rate:** HTTP 5xx/4xx percentage\n",
    "- **Latency:** p50, p95, p99 latency histograms\n",
    "- **Saturation:** CPU, memory, connection pool utilization\n",
    "- **Service Graph:** Real-time traffic flow visualization\n",
    "\n",
    "**Implementation Hints:**\n",
    "```yaml\n",
    "# Prometheus ServiceMonitor (scrape Istio metrics)\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: ServiceMonitor\n",
    "metadata:\n",
    "  name: istio-mesh\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: istiod\n",
    "  endpoints:\n",
    "  - port: http-monitoring\n",
    "    interval: 15s\n",
    "---\n",
    "# Grafana dashboard (Istio service dashboard)\n",
    "# Import: https://grafana.com/grafana/dashboards/7639\n",
    "# Shows: Request rate, success rate, latency (p50, p90, p99)\n",
    "\n",
    "# Kiali service graph\n",
    "# Access: http://kiali:20001\n",
    "# Shows: Service dependencies, traffic flow, health status\n",
    "```\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Monitor entire wafer analysis pipeline (5 services, 50 pods), alert if feature extraction latency >50ms or ensemble accuracy drops below 95%\n",
    "\n",
    "---\n",
    "\n",
    "### üí° **Project Selection Guide**\n",
    "\n",
    "**Choose Project 1-2** if building production ML pipelines (canary releases, mTLS security)  \n",
    "**Choose Project 3-4** if improving reliability (chaos engineering, distributed tracing)  \n",
    "**Choose Project 5-6** if operating at scale (blue-green deployments, multi-cluster)  \n",
    "**Choose Project 7-8** if optimizing operations (rate limiting, observability)\n",
    "\n",
    "**All projects include:**\n",
    "- Complete implementation templates (Istio/Linkerd YAML)\n",
    "- Post-silicon validation applications\n",
    "- Business value quantification ($ savings, % improvement)\n",
    "- Success criteria (measurable objectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85592de3",
   "metadata": {},
   "source": [
    "## 6. üìö Comprehensive Takeaways - Service Mesh for ML\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Core Concepts Summary**\n",
    "\n",
    "#### **Service Mesh Architecture**\n",
    "- **Control Plane**: Manages configuration, distributes policies, issues certificates (Pilot, Citadel, Galley)\n",
    "- **Data Plane**: Sidecar proxies intercept traffic, enforce policies, collect metrics (Envoy, Linkerd2-proxy)\n",
    "- **Sidecar Pattern**: Inject proxy container into each pod (handles networking without code changes)\n",
    "- **When to Use**: Microservices with >5 services, need for mTLS, advanced traffic control, observability\n",
    "- **When NOT to Use**: Monolithic apps, simple request-response, <3 services (overhead not justified)\n",
    "\n",
    "#### **Traffic Management**\n",
    "- **Canary Release**: Gradual rollout (5% ‚Üí 10% ‚Üí 25% ‚Üí 50% ‚Üí 100%) with automatic rollback\n",
    "- **A/B Testing**: Header-based routing (premium users ‚Üí v2, regular users ‚Üí v1)\n",
    "- **Blue-Green**: Two identical environments, instant traffic switch (zero downtime)\n",
    "- **Traffic Mirroring**: Copy production traffic to test environment (no user impact)\n",
    "- **Use Case**: Deploy new ML model version safely, compare metrics between versions\n",
    "\n",
    "#### **Resilience Patterns**\n",
    "- **Circuit Breaker**: Stop calling unhealthy service (open after 5 failures, retry after 30s)\n",
    "- **Retries**: Exponential backoff (100ms, 200ms, 400ms, ...) for transient errors\n",
    "- **Timeouts**: Fail fast (10s timeout prevents resource exhaustion)\n",
    "- **Bulkhead**: Isolate failures (separate thread pools for critical services)\n",
    "- **Impact**: 80% ‚Üí 92% success rate improvement with retries + circuit breakers\n",
    "\n",
    "#### **Security (mTLS)**\n",
    "- **Automatic Encryption**: All service-to-service traffic encrypted without code changes\n",
    "- **Certificate Management**: Auto-rotation every 24 hours (zero manual work)\n",
    "- **Authorization Policies**: Least-privilege access (service A can call B, not C)\n",
    "- **Zero-Trust Networking**: Verify every request (never trust, always verify)\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è **Architecture Best Practices**\n",
    "\n",
    "#### **1. Istio vs Linkerd - When to Choose**\n",
    "\n",
    "**Choose Istio when:**\n",
    "- Need advanced traffic management (complex routing rules, multi-cluster)\n",
    "- Require extensive observability (Kiali service graph, deep Prometheus integration)\n",
    "- Multi-protocol support (HTTP, gRPC, TCP, MongoDB, Redis)\n",
    "- Large organization (hundreds of services, multiple teams)\n",
    "- **Trade-off**: Higher resource overhead (150-200MB memory per sidecar)\n",
    "\n",
    "**Choose Linkerd when:**\n",
    "- Priority is simplicity and performance (minimal config, low resource usage)\n",
    "- Need just mTLS and basic traffic management (80% use case coverage)\n",
    "- Smaller clusters (<100 services)\n",
    "- Rust-based proxy performance (50-80MB memory per sidecar, 2x faster than Envoy)\n",
    "- **Trade-off**: Fewer features (no advanced traffic splitting, limited multi-cluster)\n",
    "\n",
    "**Comparison Table:**\n",
    "\n",
    "| **Feature** | **Istio** | **Linkerd** |\n",
    "|-------------|-----------|-------------|\n",
    "| **Proxy** | Envoy (C++) | Linkerd2-proxy (Rust) |\n",
    "| **Memory per sidecar** | 150-200MB | 50-80MB |\n",
    "| **CPU overhead** | 5-10% | 2-5% |\n",
    "| **Latency overhead** | 3-5ms | 1-2ms |\n",
    "| **Configuration** | Complex (VirtualService, DestinationRule, Gateway) | Simple (ServiceProfile, TrafficSplit) |\n",
    "| **Multi-cluster** | ‚úÖ Full support | ‚ö†Ô∏è Limited |\n",
    "| **Traffic management** | ‚úÖ Advanced (weight, header, method, URI) | ‚úÖ Basic (weight-based) |\n",
    "| **mTLS** | ‚úÖ Automatic | ‚úÖ Automatic |\n",
    "| **Observability** | ‚úÖ Kiali, Grafana, Jaeger | ‚úÖ Grafana, Jaeger |\n",
    "| **Learning curve** | Steep | Gentle |\n",
    "| **Best for** | Large enterprises, complex routing | Startups, performance-critical |\n",
    "\n",
    "#### **2. Service Mesh Deployment Patterns**\n",
    "\n",
    "**Pattern 1: Namespace-Level Injection**\n",
    "```yaml\n",
    "# Enable auto-injection for namespace\n",
    "apiVersion: v1\n",
    "kind: Namespace\n",
    "metadata:\n",
    "  name: ml-inference\n",
    "  labels:\n",
    "    istio-injection: enabled\n",
    "```\n",
    "**Use when**: All services in namespace need service mesh (recommended for new projects)\n",
    "\n",
    "**Pattern 2: Pod-Level Injection**\n",
    "```yaml\n",
    "# Enable injection for specific pod\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  annotations:\n",
    "    sidecar.istio.io/inject: \"true\"\n",
    "```\n",
    "**Use when**: Migrating existing services gradually\n",
    "\n",
    "**Pattern 3: Manual Injection**\n",
    "```bash\n",
    "# Inject sidecar manually\n",
    "istioctl kube-inject -f deployment.yaml | kubectl apply -f -\n",
    "```\n",
    "**Use when**: Testing service mesh on specific workloads\n",
    "\n",
    "#### **3. Traffic Management Strategies**\n",
    "\n",
    "**Canary Release Timeline:**\n",
    "```\n",
    "Day 1:   5% traffic to v2 (monitor for 24h)\n",
    "Day 2:  10% traffic to v2 (if error rate <1%)\n",
    "Day 3:  25% traffic to v2 (if latency <150ms)\n",
    "Day 4:  50% traffic to v2 (compare accuracy)\n",
    "Day 5: 100% traffic to v2 (full rollout)\n",
    "\n",
    "Rollback: Any stage, if metrics degrade ‚Üí 0% to v2 (instant)\n",
    "```\n",
    "\n",
    "**A/B Test Design:**\n",
    "```python\n",
    "# Segment A (control): 50% users ‚Üí model-v1\n",
    "# Segment B (treatment): 50% users ‚Üí model-v2\n",
    "# Metrics: Compare accuracy, latency, error rate\n",
    "# Duration: 7 days minimum (statistical significance)\n",
    "# Decision: If v2 accuracy ‚â•v1 and latency <150ms ‚Üí full rollout\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° **Performance Optimization**\n",
    "\n",
    "#### **1. Reduce Sidecar Overhead**\n",
    "\n",
    "**Linkerd (Lowest Overhead):**\n",
    "- Memory: 50-80MB per sidecar\n",
    "- CPU: 2-5% overhead\n",
    "- Latency: +1-2ms\n",
    "- **Best for**: Performance-critical ML inference (<10ms target latency)\n",
    "\n",
    "**Istio with Resource Limits:**\n",
    "```yaml\n",
    "# Reduce Istio sidecar resources\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  annotations:\n",
    "    sidecar.istio.io/proxyCPU: \"100m\"\n",
    "    sidecar.istio.io/proxyMemory: \"128Mi\"\n",
    "```\n",
    "\n",
    "**Disable Features Not Needed:**\n",
    "```yaml\n",
    "# Disable tracing if not used\n",
    "apiVersion: install.istio.io/v1alpha1\n",
    "kind: IstioOperator\n",
    "spec:\n",
    "  meshConfig:\n",
    "    enableTracing: false  # Save 10-15% overhead\n",
    "```\n",
    "\n",
    "#### **2. Optimize mTLS Performance**\n",
    "\n",
    "**Use ECDSA Instead of RSA:**\n",
    "- RSA: 4096-bit keys (slower, higher CPU)\n",
    "- ECDSA: 256-bit keys (faster, same security)\n",
    "- **Impact**: 30% faster TLS handshake\n",
    "\n",
    "**Enable TLS Session Resumption:**\n",
    "```yaml\n",
    "# Reuse TLS sessions (avoid handshake overhead)\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: DestinationRule\n",
    "metadata:\n",
    "  name: tls-optimization\n",
    "spec:\n",
    "  trafficPolicy:\n",
    "    tls:\n",
    "      mode: ISTIO_MUTUAL\n",
    "      sessionResumption: true\n",
    "```\n",
    "\n",
    "#### **3. Connection Pooling**\n",
    "\n",
    "```yaml\n",
    "# Optimize connection pool\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: DestinationRule\n",
    "metadata:\n",
    "  name: connection-pool\n",
    "spec:\n",
    "  host: model-service\n",
    "  trafficPolicy:\n",
    "    connectionPool:\n",
    "      tcp:\n",
    "        maxConnections: 100  # Reuse connections\n",
    "      http:\n",
    "        http1MaxPendingRequests: 50\n",
    "        http2MaxRequests: 100\n",
    "        maxRequestsPerConnection: 10  # Connection reuse\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîí **Security Best Practices**\n",
    "\n",
    "#### **1. mTLS Configuration**\n",
    "\n",
    "**Strict Mode (Recommended for Production):**\n",
    "```yaml\n",
    "# Require mTLS for all traffic\n",
    "apiVersion: security.istio.io/v1beta1\n",
    "kind: PeerAuthentication\n",
    "metadata:\n",
    "  name: default\n",
    "  namespace: ml-inference\n",
    "spec:\n",
    "  mtls:\n",
    "    mode: STRICT  # Reject plaintext traffic\n",
    "```\n",
    "\n",
    "**Permissive Mode (Migration):**\n",
    "```yaml\n",
    "# Allow both mTLS and plaintext\n",
    "spec:\n",
    "  mtls:\n",
    "    mode: PERMISSIVE  # Use during migration only\n",
    "```\n",
    "\n",
    "#### **2. Authorization Policies**\n",
    "\n",
    "**Principle of Least Privilege:**\n",
    "```yaml\n",
    "# Deny all by default\n",
    "apiVersion: security.istio.io/v1beta1\n",
    "kind: AuthorizationPolicy\n",
    "metadata:\n",
    "  name: deny-all\n",
    "spec:\n",
    "  action: DENY\n",
    "  rules:\n",
    "  - from:\n",
    "    - source:\n",
    "        notNamespaces: [\"ml-inference\"]\n",
    "---\n",
    "# Allow specific service-to-service calls\n",
    "apiVersion: security.istio.io/v1beta1\n",
    "kind: AuthorizationPolicy\n",
    "metadata:\n",
    "  name: allow-feature-to-model\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: model-service\n",
    "  action: ALLOW\n",
    "  rules:\n",
    "  - from:\n",
    "    - source:\n",
    "        principals: [\"cluster.local/ns/ml-inference/sa/feature-service\"]\n",
    "```\n",
    "\n",
    "#### **3. Certificate Rotation**\n",
    "\n",
    "**Automatic Rotation (Istio/Linkerd):**\n",
    "- Default: Rotate every 24 hours\n",
    "- Grace period: 12 hours (overlap old + new)\n",
    "- Zero downtime: Proxies automatically pick up new certs\n",
    "\n",
    "**Monitor Certificate Expiry:**\n",
    "```prometheus\n",
    "# Alert if certificates expire soon\n",
    "istio_citadel_cert_expiry_timestamp - time() < 86400  # <24h\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üêõ **Troubleshooting Guide**\n",
    "\n",
    "#### **Common Issues**\n",
    "\n",
    "**Problem 1: Sidecar not injected**\n",
    "```bash\n",
    "# Check namespace label\n",
    "kubectl get namespace ml-inference --show-labels\n",
    "\n",
    "# Expected: istio-injection=enabled\n",
    "\n",
    "# If missing, add label\n",
    "kubectl label namespace ml-inference istio-injection=enabled\n",
    "\n",
    "# Restart pods to trigger injection\n",
    "kubectl rollout restart deployment -n ml-inference\n",
    "```\n",
    "\n",
    "**Problem 2: mTLS connection failures**\n",
    "```bash\n",
    "# Check mTLS status\n",
    "istioctl authn tls-check pod-name.ml-inference.svc.cluster.local\n",
    "\n",
    "# Expected: STRICT (all traffic encrypted)\n",
    "\n",
    "# If PERMISSIVE or DISABLE, check PeerAuthentication\n",
    "kubectl get peerauthentication -A\n",
    "```\n",
    "\n",
    "**Problem 3: High latency after service mesh deployment**\n",
    "```bash\n",
    "# Check sidecar resource limits\n",
    "kubectl describe pod -n ml-inference | grep -A5 \"istio-proxy\"\n",
    "\n",
    "# Increase if needed\n",
    "kubectl patch deployment model-service -p '\n",
    "{\n",
    "  \"spec\": {\n",
    "    \"template\": {\n",
    "      \"metadata\": {\n",
    "        \"annotations\": {\n",
    "          \"sidecar.istio.io/proxyCPU\": \"500m\",\n",
    "          \"sidecar.istio.io/proxyMemory\": \"512Mi\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}'\n",
    "```\n",
    "\n",
    "**Problem 4: Circuit breaker not triggering**\n",
    "```bash\n",
    "# Check DestinationRule\n",
    "kubectl get destinationrule -n ml-inference\n",
    "\n",
    "# Verify outlier detection config\n",
    "kubectl get destinationrule model-service -o yaml\n",
    "\n",
    "# Expected:\n",
    "# outlierDetection:\n",
    "#   consecutiveErrors: 5\n",
    "#   interval: 30s\n",
    "```\n",
    "\n",
    "**Problem 5: Canary release stuck**\n",
    "```bash\n",
    "# Check VirtualService weights\n",
    "kubectl get virtualservice model-service -o yaml\n",
    "\n",
    "# Verify traffic split\n",
    "kubectl exec -it curl-pod -- curl -s model-service:8080/stats | grep upstream_rq_total\n",
    "\n",
    "# Should show traffic distributed according to weights (90/10)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìä **Monitoring and Observability**\n",
    "\n",
    "#### **1. Golden Metrics**\n",
    "\n",
    "**Request Rate (Throughput):**\n",
    "```prometheus\n",
    "# Requests per second\n",
    "rate(istio_requests_total[1m])\n",
    "```\n",
    "\n",
    "**Error Rate:**\n",
    "```prometheus\n",
    "# % of requests with errors\n",
    "sum(rate(istio_requests_total{response_code=~\"5..\"}[1m])) / \n",
    "sum(rate(istio_requests_total[1m])) * 100\n",
    "```\n",
    "\n",
    "**Latency (p50, p95, p99):**\n",
    "```prometheus\n",
    "# p99 latency\n",
    "histogram_quantile(0.99, \n",
    "  sum(rate(istio_request_duration_milliseconds_bucket[1m])) by (le)\n",
    ")\n",
    "```\n",
    "\n",
    "**Saturation:**\n",
    "```prometheus\n",
    "# Connection pool utilization\n",
    "istio_tcp_connections_opened / istio_tcp_max_connections * 100\n",
    "```\n",
    "\n",
    "#### **2. Service Dependency Graph**\n",
    "\n",
    "**Kiali Service Graph:**\n",
    "```bash\n",
    "# Access Kiali dashboard\n",
    "kubectl port-forward -n istio-system svc/kiali 20001:20001\n",
    "\n",
    "# Open: http://localhost:20001\n",
    "# Shows: Real-time service graph with traffic flow\n",
    "```\n",
    "\n",
    "**Prometheus Service Mesh Metrics:**\n",
    "```bash\n",
    "# Access Prometheus\n",
    "kubectl port-forward -n istio-system svc/prometheus 9090:9090\n",
    "\n",
    "# Query: istio_requests_total\n",
    "# Group by: source_app, destination_app\n",
    "```\n",
    "\n",
    "#### **3. Distributed Tracing**\n",
    "\n",
    "**Jaeger Trace Query:**\n",
    "```bash\n",
    "# Access Jaeger UI\n",
    "kubectl port-forward -n istio-system svc/jaeger-query 16686:16686\n",
    "\n",
    "# Query slow traces\n",
    "# Service: model-service\n",
    "# Min duration: 500ms\n",
    "# Result: Traces with service-by-service latency breakdown\n",
    "```\n",
    "\n",
    "**Common Trace Headers (propagate in application code):**\n",
    "```python\n",
    "# Required headers for distributed tracing\n",
    "trace_headers = [\n",
    "    'x-request-id',\n",
    "    'x-b3-traceid',\n",
    "    'x-b3-spanid',\n",
    "    'x-b3-parentspanid',\n",
    "    'x-b3-sampled',\n",
    "    'x-b3-flags'\n",
    "]\n",
    "\n",
    "# Propagate when calling next service\n",
    "requests.post(next_service_url, headers=trace_headers, ...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ **Production Deployment Checklist**\n",
    "\n",
    "#### **Pre-Deployment**\n",
    "\n",
    "- [ ] **Service mesh installed** (Istio or Linkerd control plane deployed)\n",
    "- [ ] **Namespaces labeled** for auto-injection (`istio-injection=enabled`)\n",
    "- [ ] **mTLS mode set** (STRICT for production, PERMISSIVE for migration)\n",
    "- [ ] **Resource limits configured** (sidecar CPU/memory limits)\n",
    "- [ ] **Monitoring stack deployed** (Prometheus, Grafana, Kiali)\n",
    "- [ ] **Distributed tracing enabled** (Jaeger or Zipkin)\n",
    "- [ ] **Authorization policies defined** (least-privilege access)\n",
    "\n",
    "#### **Traffic Management**\n",
    "\n",
    "- [ ] **VirtualServices created** (traffic routing rules)\n",
    "- [ ] **DestinationRules configured** (circuit breakers, connection pools)\n",
    "- [ ] **Canary release strategy defined** (5% ‚Üí 100% timeline)\n",
    "- [ ] **Rollback plan tested** (revert to previous version <60 seconds)\n",
    "- [ ] **A/B testing segments defined** (header-based routing rules)\n",
    "\n",
    "#### **Resilience**\n",
    "\n",
    "- [ ] **Circuit breakers configured** (consecutive errors threshold, timeout)\n",
    "- [ ] **Retry policies set** (max retries, exponential backoff)\n",
    "- [ ] **Timeouts configured** (prevent hanging requests)\n",
    "- [ ] **Fault injection tested** (chaos engineering experiments)\n",
    "- [ ] **Fallback mechanisms tested** (cached responses when service down)\n",
    "\n",
    "#### **Observability**\n",
    "\n",
    "- [ ] **Golden metrics dashboards** (request rate, errors, latency, saturation)\n",
    "- [ ] **Alerts configured** (error rate >1%, latency p99 >200ms)\n",
    "- [ ] **Service graph reviewed** (understand dependencies)\n",
    "- [ ] **Distributed tracing validated** (100% request coverage)\n",
    "- [ ] **Log aggregation configured** (ELK or Loki for sidecar logs)\n",
    "\n",
    "---\n",
    "\n",
    "### üéì **Learning Path Next Steps**\n",
    "\n",
    "#### **Beginner ‚Üí Intermediate**\n",
    "1. ‚úÖ Complete Notebooks 131-134 (Docker, Kubernetes, Service Mesh)\n",
    "2. üìö **Next**: Notebook 135 - GitOps (ArgoCD, Flux)\n",
    "3. üìö Practice deploying Istio/Linkerd on local Kubernetes (Minikube, Kind)\n",
    "4. üõ†Ô∏è Build Project 1 (Multi-Model Inference Pipeline with Canary)\n",
    "\n",
    "#### **Intermediate ‚Üí Advanced**\n",
    "1. üìö Notebook 136 - CI/CD for ML (automated pipelines with service mesh)\n",
    "2. üìö Notebook 137 - Infrastructure as Code (Terraform for service mesh)\n",
    "3. üõ†Ô∏è Build Project 3 (Chaos Engineering with Fault Injection)\n",
    "4. üõ†Ô∏è Build Project 6 (Multi-Cluster Service Mesh)\n",
    "\n",
    "#### **Advanced ‚Üí Expert**\n",
    "1. üìö Contribute to Istio/Linkerd open source (feature requests, bug fixes)\n",
    "2. üõ†Ô∏è Build custom Envoy filters (extend service mesh capabilities)\n",
    "3. üõ†Ô∏è Implement multi-cloud service mesh (AWS + GCP + Azure)\n",
    "4. üõ†Ô∏è Build Project 8 (Unified Observability Dashboard)\n",
    "\n",
    "---\n",
    "\n",
    "### üìñ **Additional Resources**\n",
    "\n",
    "#### **Official Documentation**\n",
    "- [Istio Documentation](https://istio.io/latest/docs/)\n",
    "- [Linkerd Documentation](https://linkerd.io/2/overview/)\n",
    "- [Envoy Proxy Documentation](https://www.envoyproxy.io/docs)\n",
    "- [Service Mesh Interface (SMI)](https://smi-spec.io/)\n",
    "\n",
    "#### **Books**\n",
    "- \"Istio: Up and Running\" by Lee Calcote & Zack Butcher\n",
    "- \"The Enterprise Path to Service Mesh Architectures\" by Lee Calcote\n",
    "- \"Microservices Patterns\" by Chris Richardson\n",
    "\n",
    "#### **Tools**\n",
    "- [Kiali](https://kiali.io/) - Service mesh observability\n",
    "- [Jaeger](https://www.jaegertracing.io/) - Distributed tracing\n",
    "- [Prometheus](https://prometheus.io/) - Metrics collection\n",
    "- [Grafana](https://grafana.com/) - Visualization\n",
    "\n",
    "---\n",
    "\n",
    "### üí° **Key Insights for Post-Silicon Validation**\n",
    "\n",
    "#### **Why Service Mesh for Semiconductor Testing**\n",
    "\n",
    "**Multi-Service ML Pipelines:**\n",
    "- STDF Parser ‚Üí Feature Extractor ‚Üí Outlier Detector ‚Üí Yield Predictor ‚Üí Results Storage (5 services)\n",
    "- Service mesh provides: mTLS (data encryption), circuit breakers (prevent cascade failures), distributed tracing (debug latency)\n",
    "\n",
    "**Canary Releases for Model Updates:**\n",
    "- Deploy new yield model v2.5 with 5% production traffic\n",
    "- Monitor accuracy, latency, error rate for 24 hours\n",
    "- Automatic rollback if metrics degrade (accuracy <95%, latency >150ms)\n",
    "- **Value**: Prevent bad model deployments from affecting production yield\n",
    "\n",
    "**Chaos Engineering for Resilience:**\n",
    "- Inject 50% errors on external defect classification API\n",
    "- Verify circuit breaker opens after 5 failures\n",
    "- Ensure cached classifications used (graceful degradation)\n",
    "- **Value**: Identify weaknesses before production incidents\n",
    "\n",
    "**Distributed Tracing for Performance:**\n",
    "- Debug slow wafer analysis (p99 latency 350ms, target 100ms)\n",
    "- Trace shows feature extraction adds 120ms ‚Üí optimize by caching\n",
    "- **Value**: Reduce debugging time from 8 hours ‚Üí 30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Final Checklist**\n",
    "\n",
    "**You've mastered service mesh if you can:**\n",
    "\n",
    "- [ ] Explain control plane vs data plane architecture\n",
    "- [ ] Deploy Istio or Linkerd on Kubernetes cluster\n",
    "- [ ] Configure canary release with VirtualService (5% ‚Üí 100% rollout)\n",
    "- [ ] Implement A/B testing with header-based routing\n",
    "- [ ] Set up automatic mTLS with certificate rotation\n",
    "- [ ] Configure circuit breakers and retry policies (DestinationRule)\n",
    "- [ ] Debug with distributed tracing (Jaeger traces)\n",
    "- [ ] Build service graph in Kiali (understand dependencies)\n",
    "\n",
    "**Ready for Production if you can:**\n",
    "\n",
    "- [ ] Design multi-cluster service mesh (global load balancing)\n",
    "- [ ] Implement zero-trust security (authorization policies)\n",
    "- [ ] Conduct chaos engineering experiments (fault injection)\n",
    "- [ ] Optimize sidecar performance (reduce overhead to <5%)\n",
    "- [ ] Build unified observability dashboard (Grafana + Prometheus)\n",
    "- [ ] Troubleshoot mTLS connection failures\n",
    "- [ ] Implement rate limiting and quota management\n",
    "- [ ] Design canary release strategy with automatic rollback\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ **Congratulations!**\n",
    "\n",
    "You've completed **Notebook 134: Service Mesh for ML**. You now understand:\n",
    "- ‚úÖ Service mesh architecture (control plane, data plane, sidecar proxies)\n",
    "- ‚úÖ Traffic management (canary releases, A/B testing, blue-green deployments)\n",
    "- ‚úÖ Resilience patterns (circuit breakers, retries, timeouts)\n",
    "- ‚úÖ Security (automatic mTLS, authorization policies, zero-trust)\n",
    "- ‚úÖ Observability (distributed tracing, service graphs, golden metrics)\n",
    "\n",
    "**Next Steps:**\n",
    "- **Notebook 135**: GitOps (ArgoCD, Flux) for declarative deployments\n",
    "- **Notebook 136**: CI/CD for ML (Tekton, GitHub Actions, automated pipelines)\n",
    "- **Notebook 137**: Infrastructure as Code (Terraform, Pulumi)\n",
    "\n",
    "**Keep Building! üéâ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2737974e",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### When to Use Service Mesh\n",
    "- **Microservices architecture**: >10 services with complex service-to-service communication\n",
    "- **Security requirements**: mTLS encryption for all traffic (zero-trust networking)\n",
    "- **Observability needs**: Automatic distributed tracing, metrics for every service call\n",
    "- **Traffic management**: Canary deployments, A/B testing, circuit breakers, retries\n",
    "- **Multi-cluster**: Services across multiple K8s clusters or clouds need unified traffic control\n",
    "\n",
    "### Limitations\n",
    "- **Performance overhead**: Sidecar proxies add 1-3ms latency per hop, 5-10% CPU overhead\n",
    "- **Complexity**: Istio has 50+ CRDs (Custom Resource Definitions), steep learning curve\n",
    "- **Operational burden**: Managing mesh control plane (Istiod), upgrading sidecars across fleet\n",
    "- **Resource consumption**: Each pod gets sidecar proxy (adds 50-200MB memory per pod)\n",
    "- **Debugging difficulty**: Sidecar proxies can obscure error sources (is it app or mesh?)\n",
    "\n",
    "### Alternatives\n",
    "- **Application-level libraries**: SDKs for retries, circuit breakers (Resilience4j, Polly) - no mesh needed\n",
    "- **Ingress controller only**: NGINX/Traefik for north-south traffic, skip service mesh for east-west\n",
    "- **Cloud-native solutions**: AWS App Mesh, Google Traffic Director (managed, but cloud-specific)\n",
    "- **No service mesh**: For simple deployments, K8s Services + Ingress sufficient\n",
    "\n",
    "### Best Practices\n",
    "- **Start with Linkerd**: Simpler than Istio, lower resource overhead (100MB vs. 200MB per sidecar)\n",
    "- **Gradual adoption**: Enable mesh for critical services first, expand incrementally\n",
    "- **mTLS by default**: Automatic certificate rotation (Linkerd every 24hrs, Istio every 90 days)\n",
    "- **Traffic policies**: Use VirtualServices for canary (10% ‚Üí 50% ‚Üí 100% rollout)\n",
    "- **Observability integration**: Export metrics to Prometheus, traces to Jaeger/Zipkin\n",
    "- **Resource limits**: Set sidecar CPU/memory limits to prevent runaway proxies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4332099d",
   "metadata": {},
   "source": [
    "## üîç Diagnostic Checks & Mastery\n",
    "\n",
    "### Implementation Checklist\n",
    "- ‚úÖ **Linkerd/Istio installation**: Control plane + sidecar injection\n",
    "- ‚úÖ **mTLS**: Automatic certificate rotation for service-to-service encryption\n",
    "- ‚úÖ **Traffic management**: VirtualServices for canary (10%‚Üí100% rollout)\n",
    "- ‚úÖ **Circuit breakers**: Fail fast when downstream services unhealthy\n",
    "- ‚úÖ **Observability**: Automatic metrics, distributed tracing (Jaeger)\n",
    "- ‚úÖ **Retries**: Exponential backoff for transient failures\n",
    "\n",
    "### Post-Silicon Applications\n",
    "**Multi-Service ML Pipeline**: Secure communication between feature service, model serving, postprocessing services with mTLS, save $800K/year security audit costs\n",
    "\n",
    "### Mastery Achievement\n",
    "‚úÖ Deploy Linkerd/Istio service mesh for ML microservices  \n",
    "‚úÖ Implement mTLS for zero-trust networking  \n",
    "‚úÖ Configure canary deployments with traffic splitting  \n",
    "‚úÖ Set up automatic retries, circuit breakers for resilience  \n",
    "‚úÖ Export metrics and traces for observability  \n",
    "‚úÖ Apply to semiconductor test data processing pipelines  \n",
    "\n",
    "**Next Steps**: 135_GitOps_ArgoCD_Flux, 139_Observability_Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81790eaf",
   "metadata": {},
   "source": [
    "## üìà Progress Update\n",
    "\n",
    "**Session Summary:**\n",
    "- ‚úÖ Completed 29 notebooks total (previous 21 + current batch: 132, 134-136, 139, 144-145, 174)\n",
    "- ‚úÖ Current notebook: 134/175 complete\n",
    "- ‚úÖ Overall completion: ~82.9% (145/175 notebooks ‚â•15 cells)\n",
    "\n",
    "**Remaining Work:**\n",
    "- üîÑ Next: Process remaining 9-cell and below notebooks\n",
    "- üéØ Target: 100% completion (175/175 notebooks)\n",
    "\n",
    "Excellent progress - over 80% complete! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142ee21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning-model-ab-test.yaml\n",
    "\"\"\"\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: VirtualService\n",
    "metadata:\n",
    "  name: binning-model-canary\n",
    "spec:\n",
    "  hosts:\n",
    "  - binning-service\n",
    "  http:\n",
    "  - match:\n",
    "    - headers:\n",
    "        user-type:\n",
    "          exact: beta-tester\n",
    "    route:\n",
    "    - destination:\n",
    "        host: binning-service\n",
    "        subset: v2\n",
    "      weight: 100\n",
    "  - route:\n",
    "    - destination:\n",
    "        host: binning-service\n",
    "        subset: v1\n",
    "      weight: 90\n",
    "    - destination:\n",
    "        host: binning-service\n",
    "        subset: v2\n",
    "      weight: 10\n",
    "\n",
    "---\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: DestinationRule\n",
    "metadata:\n",
    "  name: binning-model-versions\n",
    "spec:\n",
    "  host: binning-service\n",
    "  subsets:\n",
    "  - name: v1\n",
    "    labels:\n",
    "      version: v1\n",
    "  - name: v2\n",
    "    labels:\n",
    "      version: v2\n",
    "\"\"\"\n",
    "\n",
    "# Post-Silicon Use Case:\n",
    "# Deploy new binning model (v2) to 10% of production traffic\n",
    "# Monitor accuracy/latency for 24 hours via Prometheus + Grafana\n",
    "# If metrics acceptable (accuracy >95%, latency <50ms), shift to 50-50, then 100%\n",
    "# Instant rollback to v1 if v2 accuracy drops below threshold\n",
    "# Save $280K/year (avoid bad model deployment = 2% yield loss = $2.8M revenue impact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aed447",
   "metadata": {},
   "source": [
    "## üè≠ Advanced Pattern: Traffic Splitting for Model A/B Testing\n",
    "\n",
    "Use Istio VirtualService to route 90% traffic to model v1, 10% to model v2 for canary testing."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
