{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "065a7b8a",
   "metadata": {},
   "source": [
    "# 133: Kubernetes Advanced Patterns for ML\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** StatefulSets for stateful ML workloads (stable identities, persistent storage, ordered operations)\n",
    "- **Deploy** DaemonSets for cluster-wide services (GPU drivers, monitoring agents, logging)\n",
    "- **Implement** Kubernetes Operators (custom controllers, reconciliation loops, domain-specific automation)\n",
    "- **Apply** Custom Resource Definitions (CRDs) to extend Kubernetes API for ML workflows\n",
    "- **Master** advanced scheduling patterns (affinity, taints, tolerations, custom schedulers)\n",
    "- **Build** production ML platforms (Kubeflow, KServe, multi-tenant systems)\n",
    "\n",
    "## ðŸ“š What Are Kubernetes Advanced Patterns?\n",
    "\n",
    "While basic Kubernetes (Deployments, Services) works for stateless applications, **ML workloads require advanced patterns** to handle:\n",
    "\n",
    "**The Gap in Basic Kubernetes:**\n",
    "```\n",
    "Basic Deployment (Stateless):\n",
    "âœ… Random pod names: yield-model-7f4d8-xyz\n",
    "âœ… Ephemeral storage: data lost on pod restart\n",
    "âœ… Random scheduling: pod-1 may land on any node\n",
    "âœ… Parallel scaling: all replicas created simultaneously\n",
    "\n",
    "ML Challenges:\n",
    "âŒ Distributed training: worker-0 needs stable DNS to coordinate with worker-1\n",
    "âŒ Stateful services: Database primary vs replicas need persistent identities\n",
    "âŒ GPU drivers: Every GPU node needs NVIDIA driver (not just some nodes)\n",
    "âŒ Custom operations: Auto-scaling based on queue depth (not just CPU)\n",
    "```\n",
    "\n",
    "**Advanced Patterns Solution:**\n",
    "```\n",
    "StatefulSets:\n",
    "âœ… Stable pod names: redis-0, redis-1, redis-2\n",
    "âœ… Persistent volumes: Each pod gets dedicated PVC (survives restarts)\n",
    "âœ… Ordered operations: Sequential creation (0â†’1â†’2), reverse deletion (2â†’1â†’0)\n",
    "âœ… Stable DNS: worker-0.service.namespace.svc.cluster.local\n",
    "\n",
    "DaemonSets:\n",
    "âœ… One pod per node: GPU driver on ALL GPU nodes automatically\n",
    "âœ… Node selectors: Only schedule on nodes with label gpu=nvidia\n",
    "âœ… Auto-scheduling: New node joins â†’ DaemonSet pod deployed instantly\n",
    "\n",
    "Operators:\n",
    "âœ… Custom controllers: Watch TrainingJob CRD, create StatefulSet automatically\n",
    "âœ… Reconciliation loop: Ensure desired state = actual state (self-healing)\n",
    "âœ… Domain knowledge: Auto-retry failed training, checkpoint management\n",
    "\n",
    "Custom Resource Definitions (CRDs):\n",
    "âœ… Extend Kubernetes API: TrainingJob, ModelServer, HyperparameterTuning\n",
    "âœ… Declarative: Users create YAML, operator handles complexity\n",
    "âœ… Native kubectl: kubectl get trainingjobs, kubectl describe modelserver\n",
    "```\n",
    "\n",
    "**Why Advanced Patterns for ML?**\n",
    "- âœ… **Distributed Training:** PyTorch DDP, Horovod require stable pod names and network IDs\n",
    "- âœ… **Stateful Services:** Databases, caches, message queues need persistent identities\n",
    "- âœ… **GPU Management:** DaemonSets ensure every GPU node has drivers, monitoring\n",
    "- âœ… **ML-Specific Operations:** Operators encode best practices (checkpointing, auto-retry, hyperparameter tuning)\n",
    "- âœ… **Self-Service ML:** Data scientists create TrainingJob CRD, operator handles pod creation, GPU allocation, monitoring\n",
    "- âœ… **Automated Operations:** Operators reduce manual work (auto-scale, auto-heal, auto-optimize)\n",
    "\n",
    "## ðŸ­ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Use Case 1: Distributed Wafer Map Analysis (StatefulSet)**\n",
    "- **Input:** 10,000 wafer maps (spatial defect patterns, 300MB each)\n",
    "- **Deployment:** StatefulSet with 5 analyzer pods (`analyzer-0` to `analyzer-4`)\n",
    "- **Stable Identity:** Each pod processes specific wafer range (shard-0: wafers 0-1999, shard-1: 2000-3999, etc.)\n",
    "- **Persistent Storage:** Each pod has 100GB PVC for intermediate spatial correlation matrices (survives restarts)\n",
    "- **Output:** Aggregated defect patterns (hotspots, systematic failures, spatial trends)\n",
    "- **Value:** 8x faster analysis (5 pods in parallel vs sequential), $340K/year savings\n",
    "\n",
    "**Use Case 2: GPU Driver Installation (DaemonSet)**\n",
    "- **Input:** 10 GPU nodes with NVIDIA A100 GPUs (no drivers installed initially)\n",
    "- **Deployment:** DaemonSet with node selector `gpu=nvidia` (only GPU nodes)\n",
    "- **Init Container:** Install NVIDIA driver 525.xx, configure CUDA 12.1, enable GPU resource plugin\n",
    "- **Monitoring:** DaemonSet also deploys GPU metrics exporter (utilization, temperature, memory)\n",
    "- **Output:** All GPU nodes ready for ML inference/training workloads\n",
    "- **Value:** Zero manual driver management, auto-deployment on new nodes, 99.8% GPU uptime\n",
    "\n",
    "**Use Case 3: STDF Parser Operator (Kubernetes Operator)**\n",
    "- **Input:** Test equipment streams STDF files (variable rate: 10-100 wafers/hour)\n",
    "- **CRD:** Data scientists create `STDFParserJob` custom resource (specify input bucket, output format)\n",
    "- **Operator:** Watches `STDFParserJob`, creates pods based on queue depth (auto-scale 1-20 workers)\n",
    "- **Self-Healing:** Failed jobs auto-retry (up to 3 attempts), corrupted files quarantined automatically\n",
    "- **Output:** Parsed data in Parquet format (sub-5 second p95 latency)\n",
    "- **Value:** $125K/year savings (eliminate manual job management), 60% faster processing\n",
    "\n",
    "**Use Case 4: Multi-Model Ensemble (Custom Resource Definition)**\n",
    "- **Input:** 5 yield prediction models (Random Forest, XGBoost, LightGBM, CatBoost, Neural Net)\n",
    "- **CRD:** `EnsembleModel` custom resource defines models, weights, voting strategy\n",
    "- **Operator:** Deploys each model as separate pod, creates ensemble combiner, monitors performance\n",
    "- **Auto-Scaling:** Each model scales independently based on traffic (Random Forest: 3 pods, Neural Net: 8 pods with GPU)\n",
    "- **Output:** Single prediction endpoint (weighted voting, 96.5% accuracy vs 92% for single model)\n",
    "- **Value:** $4.2M/year savings (fewer false negatives â†’ less yield loss)\n",
    "\n",
    "## ðŸ”„ Kubernetes Advanced Patterns Workflow\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Data Scientist] -->|Create TrainingJob CRD| B[Kubernetes API]\n",
    "    B -->|Store| C[etcd]\n",
    "    D[Operator] -->|Watch| B\n",
    "    D -->|Detect New TrainingJob| E[Reconcile]\n",
    "    E -->|Create| F[StatefulSet: 4 workers]\n",
    "    F -->|Deploy| G[worker-0: master]\n",
    "    F -->|Deploy| H[worker-1: replica]\n",
    "    F -->|Deploy| I[worker-2: replica]\n",
    "    F -->|Deploy| J[worker-3: replica]\n",
    "    G -->|Stable DNS| K[Distributed Training]\n",
    "    H -->|Stable DNS| K\n",
    "    I -->|Stable DNS| K\n",
    "    J -->|Stable DNS| K\n",
    "    K -->|Metrics| L[Prometheus]\n",
    "    L -->|Trigger| M[Auto-Scale to 8 workers]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#fff4e1\n",
    "    style D fill:#e1ffe1\n",
    "    style E fill:#ffe1e1\n",
    "    style F fill:#f0e1ff\n",
    "    style G fill:#ffe1f5\n",
    "    style H fill:#ffe1f5\n",
    "    style I fill:#ffe1f5\n",
    "    style J fill:#ffe1f5\n",
    "    style K fill:#e1ffff\n",
    "    style M fill:#ffe1e1\n",
    "```\n",
    "\n",
    "## ðŸ“Š Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **Notebook 131:** Docker for ML (containerization fundamentals, multi-stage builds)\n",
    "- **Notebook 132:** Kubernetes Fundamentals (architecture, deployments, services, HPA, rolling updates)\n",
    "\n",
    "**Current Notebook:**\n",
    "- **Notebook 133:** Kubernetes Advanced Patterns (StatefulSets, DaemonSets, Operators, CRDs)\n",
    "\n",
    "**Next Steps:**\n",
    "- **Notebook 134:** Service Mesh (Istio, Linkerd for advanced networking and observability)\n",
    "- **Notebook 135:** GitOps (ArgoCD, Flux for declarative deployments)\n",
    "- **Notebook 136:** CI/CD for ML (Tekton, GitHub Actions, automated pipelines)\n",
    "\n",
    "---\n",
    "\n",
    "Let's master advanced Kubernetes patterns for production ML! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab0cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Any\n",
    "from enum import Enum\n",
    "import uuid\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Environment ready for Kubernetes advanced patterns simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0015ef",
   "metadata": {},
   "source": [
    "## 2. ðŸ—„ï¸ StatefulSets - Stable Identities for Stateful ML Workloads\n",
    "\n",
    "**Purpose:** Deploy applications requiring stable network identities, ordered deployment/scaling, and persistent storage per pod.\n",
    "\n",
    "**Key Points:**\n",
    "- **Stable Pod Names:** Pods get predictable names (`redis-0`, `redis-1`, `redis-2`, not random hashes)\n",
    "- **Stable Network IDs:** Each pod gets DNS entry (`redis-0.redis.default.svc.cluster.local`)\n",
    "- **Ordered Operations:** Pods created sequentially (0â†’1â†’2), deleted in reverse (2â†’1â†’0)\n",
    "- **Persistent Volumes:** Each pod gets own PVC (survives pod deletion/restart)\n",
    "- **Headless Service:** Service without ClusterIP (DNS for individual pods)\n",
    "\n",
    "**Why It Matters:**\n",
    "- **Distributed training:** PyTorch DDP worker-0 needs to know worker-1's address (stable DNS)\n",
    "- **Databases:** PostgreSQL primary (`postgres-0`) vs replicas (`postgres-1`, `postgres-2`)\n",
    "- **Consensus systems:** Kafka, ZooKeeper need stable IDs for leader election\n",
    "- **Data locality:** Pod always mounts same PVC (training checkpoints, model artifacts)\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "Distributed wafer map analysis: 5 analyzer pods (`analyzer-0` to `analyzer-4`), each processes specific wafer range (0-1999, 2000-3999, etc.), stable names allow consistent shard assignment, each pod has 100GB PVC for intermediate spatial correlation matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71498fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StatefulSet Simulation\n",
    "\n",
    "@dataclass\n",
    "class PersistentVolumeClaim:\n",
    "    \"\"\"Represents a Kubernetes PVC.\"\"\"\n",
    "    name: str\n",
    "    size_gb: int\n",
    "    storage_class: str = \"fast-ssd\"\n",
    "    status: str = \"Bound\"\n",
    "    mount_path: str = \"/data\"\n",
    "    data_size_gb: float = 0.0  # Current data size\n",
    "    \n",
    "    def write_data(self, size_gb: float):\n",
    "        \"\"\"Simulate writing data to PVC.\"\"\"\n",
    "        self.data_size_gb += size_gb\n",
    "        if self.data_size_gb > self.size_gb:\n",
    "            raise Exception(f\"PVC full: {self.data_size_gb:.1f}GB > {self.size_gb}GB\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StatefulPod:\n",
    "    \"\"\"Represents a pod in StatefulSet with stable identity.\"\"\"\n",
    "    name: str  # Predictable: redis-0, redis-1, etc.\n",
    "    ordinal: int  # 0, 1, 2, ...\n",
    "    node: str\n",
    "    status: str = \"Running\"\n",
    "    ip: str = \"\"\n",
    "    pvc: Optional[PersistentVolumeClaim] = None\n",
    "    role: str = \"replica\"  # master or replica\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.ip:\n",
    "            self.ip = f\"10.244.{np.random.randint(0, 255)}.{ordinal}\"\n",
    "    \n",
    "    def get_dns_name(self, service_name: str, namespace: str = \"default\") -> str:\n",
    "        \"\"\"Get stable DNS name for pod.\"\"\"\n",
    "        return f\"{self.name}.{service_name}.{namespace}.svc.cluster.local\"\n",
    "    \n",
    "    def attach_pvc(self, pvc: PersistentVolumeClaim):\n",
    "        \"\"\"Attach PVC to pod.\"\"\"\n",
    "        self.pvc = pvc\n",
    "        print(f\"âœ… PVC {pvc.name} attached to {self.name} (mount: {pvc.mount_path})\")\n",
    "\n",
    "\n",
    "class StatefulSet:\n",
    "    \"\"\"Simulates Kubernetes StatefulSet.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, service_name: str, replicas: int, \n",
    "                 pvc_size_gb: int = 10):\n",
    "        self.name = name\n",
    "        self.service_name = service_name\n",
    "        self.desired_replicas = replicas\n",
    "        self.pvc_size_gb = pvc_size_gb\n",
    "        self.pods: List[StatefulPod] = []\n",
    "        self.pvcs: List[PersistentVolumeClaim] = []\n",
    "        \n",
    "    def create(self):\n",
    "        \"\"\"Create StatefulSet pods sequentially.\"\"\"\n",
    "        print(f\"\\nðŸš€ Creating StatefulSet '{self.name}' (replicas: {self.desired_replicas})\")\n",
    "        print(f\"   Service: {self.service_name}\")\n",
    "        print(f\"   PVC per pod: {self.pvc_size_gb}GB\")\n",
    "        print()\n",
    "        \n",
    "        for i in range(self.desired_replicas):\n",
    "            # Create PVC first\n",
    "            pvc = PersistentVolumeClaim(\n",
    "                name=f\"{self.name}-data-{self.name}-{i}\",\n",
    "                size_gb=self.pvc_size_gb\n",
    "            )\n",
    "            self.pvcs.append(pvc)\n",
    "            \n",
    "            # Create pod with predictable name\n",
    "            pod = StatefulPod(\n",
    "                name=f\"{self.name}-{i}\",\n",
    "                ordinal=i,\n",
    "                node=f\"node-{(i % 3) + 1}\",\n",
    "                role=\"master\" if i == 0 else \"replica\"\n",
    "            )\n",
    "            \n",
    "            # Attach PVC\n",
    "            pod.attach_pvc(pvc)\n",
    "            \n",
    "            # Add to StatefulSet\n",
    "            self.pods.append(pod)\n",
    "            \n",
    "            # Get DNS name\n",
    "            dns_name = pod.get_dns_name(self.service_name)\n",
    "            print(f\"âœ… Pod {pod.name} created on {pod.node}\")\n",
    "            print(f\"   IP: {pod.ip}\")\n",
    "            print(f\"   DNS: {dns_name}\")\n",
    "            print(f\"   Role: {pod.role}\")\n",
    "            print(f\"   PVC: {pvc.name} ({pvc.size_gb}GB)\")\n",
    "            print()\n",
    "            \n",
    "            # Simulate sequential startup delay\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        print(f\"âœ… StatefulSet {self.name} ready: {len(self.pods)} pods running\\n\")\n",
    "    \n",
    "    def scale(self, new_replicas: int):\n",
    "        \"\"\"Scale StatefulSet (ordered operations).\"\"\"\n",
    "        current_replicas = len(self.pods)\n",
    "        \n",
    "        if new_replicas > current_replicas:\n",
    "            # Scale up: add pods sequentially\n",
    "            print(f\"ðŸ“ˆ Scaling UP: {current_replicas} â†’ {new_replicas}\")\n",
    "            for i in range(current_replicas, new_replicas):\n",
    "                pvc = PersistentVolumeClaim(\n",
    "                    name=f\"{self.name}-data-{self.name}-{i}\",\n",
    "                    size_gb=self.pvc_size_gb\n",
    "                )\n",
    "                self.pvcs.append(pvc)\n",
    "                \n",
    "                pod = StatefulPod(\n",
    "                    name=f\"{self.name}-{i}\",\n",
    "                    ordinal=i,\n",
    "                    node=f\"node-{(i % 3) + 1}\"\n",
    "                )\n",
    "                pod.attach_pvc(pvc)\n",
    "                self.pods.append(pod)\n",
    "                print(f\"âœ… Added pod {pod.name}\")\n",
    "        \n",
    "        elif new_replicas < current_replicas:\n",
    "            # Scale down: remove pods in reverse order\n",
    "            print(f\"ðŸ“‰ Scaling DOWN: {current_replicas} â†’ {new_replicas}\")\n",
    "            for i in range(current_replicas - 1, new_replicas - 1, -1):\n",
    "                pod = self.pods.pop()\n",
    "                print(f\"ðŸ—‘ï¸  Removed pod {pod.name} (PVC {pod.pvc.name} retained)\")\n",
    "        \n",
    "        self.desired_replicas = new_replicas\n",
    "        print(f\"âœ… Scale complete: {len(self.pods)} pods running\\n\")\n",
    "    \n",
    "    def get_pod_by_name(self, name: str) -> Optional[StatefulPod]:\n",
    "        \"\"\"Get pod by stable name.\"\"\"\n",
    "        for pod in self.pods:\n",
    "            if pod.name == name:\n",
    "                return pod\n",
    "        return None\n",
    "    \n",
    "    def simulate_data_write(self, pod_name: str, data_gb: float):\n",
    "        \"\"\"Simulate writing data to pod's PVC.\"\"\"\n",
    "        pod = self.get_pod_by_name(pod_name)\n",
    "        if pod and pod.pvc:\n",
    "            pod.pvc.write_data(data_gb)\n",
    "            print(f\"ðŸ’¾ {pod_name}: Wrote {data_gb}GB to {pod.pvc.name}\")\n",
    "            print(f\"   Total data: {pod.pvc.data_size_gb:.1f}GB / {pod.pvc.size_gb}GB\")\n",
    "    \n",
    "    def get_status(self) -> Dict:\n",
    "        \"\"\"Get StatefulSet status.\"\"\"\n",
    "        total_storage = sum(pvc.size_gb for pvc in self.pvcs)\n",
    "        used_storage = sum(pvc.data_size_gb for pvc in self.pvcs)\n",
    "        \n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"replicas\": len(self.pods),\n",
    "            \"pods\": [p.name for p in self.pods],\n",
    "            \"total_storage_gb\": total_storage,\n",
    "            \"used_storage_gb\": used_storage,\n",
    "            \"storage_utilization\": (used_storage / total_storage * 100) if total_storage > 0 else 0\n",
    "        }\n",
    "\n",
    "\n",
    "# Example 1: Create Redis StatefulSet (caching layer for ML predictions)\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE 1: Redis StatefulSet for ML Prediction Cache\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "redis_sts = StatefulSet(\n",
    "    name=\"redis\",\n",
    "    service_name=\"redis\",\n",
    "    replicas=3,\n",
    "    pvc_size_gb=50\n",
    ")\n",
    "\n",
    "redis_sts.create()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE 2: Stable DNS Names for Service Discovery\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"ðŸ“¡ DNS Resolution for Redis Pods:\\n\")\n",
    "for pod in redis_sts.pods:\n",
    "    dns = pod.get_dns_name(\"redis\")\n",
    "    print(f\"   {pod.name} â†’ {dns}\")\n",
    "    print(f\"      Role: {pod.role}\")\n",
    "    print(f\"      IP: {pod.ip}\")\n",
    "    print()\n",
    "\n",
    "print(\"ðŸ’¡ Use Case: ML service connects to redis-0.redis.default.svc.cluster.local\")\n",
    "print(\"   (Stable DNS, always resolves to master pod)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE 3: Persistent Storage Per Pod\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simulate data writes to each pod\n",
    "print(\"\\nðŸ’¾ Simulating cache writes to Redis pods:\\n\")\n",
    "redis_sts.simulate_data_write(\"redis-0\", 15.5)  # Master gets most writes\n",
    "redis_sts.simulate_data_write(\"redis-1\", 8.2)   # Replica\n",
    "redis_sts.simulate_data_write(\"redis-2\", 6.7)   # Replica\n",
    "\n",
    "status = redis_sts.get_status()\n",
    "print(f\"\\nðŸ“Š StatefulSet Status:\")\n",
    "print(f\"   Pods: {status['replicas']}\")\n",
    "print(f\"   Total Storage: {status['total_storage_gb']}GB\")\n",
    "print(f\"   Used Storage: {status['used_storage_gb']:.1f}GB\")\n",
    "print(f\"   Utilization: {status['storage_utilization']:.1f}%\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight: Each pod has own PVC, data survives pod restart\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE 4: Ordered Scaling\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Scale up from 3 to 5 replicas\n",
    "redis_sts.scale(5)\n",
    "\n",
    "# Show new pods\n",
    "print(\"ðŸ“Š Current Pods:\")\n",
    "for pod in redis_sts.pods:\n",
    "    print(f\"   {pod.name} on {pod.node} (PVC: {pod.pvc.name})\")\n",
    "\n",
    "# Scale down from 5 to 3 replicas\n",
    "print()\n",
    "redis_sts.scale(3)\n",
    "\n",
    "print(\"ðŸ’¡ Observations:\")\n",
    "print(\"   â€¢ Scale-up: Pods added sequentially (redis-3, then redis-4)\")\n",
    "print(\"   â€¢ Scale-down: Pods removed in reverse (redis-4, then redis-3)\")\n",
    "print(\"   â€¢ PVCs retained: Can reattach if scaled back up\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE 5: StatefulSet vs Deployment Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_data = {\n",
    "    \"Feature\": [\n",
    "        \"Pod Names\",\n",
    "        \"Network Identity\",\n",
    "        \"Storage\",\n",
    "        \"Scaling Order\",\n",
    "        \"Use Cases\"\n",
    "    ],\n",
    "    \"Deployment\": [\n",
    "        \"Random (model-7f4d8)\",\n",
    "        \"Dynamic IPs\",\n",
    "        \"Shared or no PVC\",\n",
    "        \"Parallel\",\n",
    "        \"Stateless apps (web servers, ML APIs)\"\n",
    "    ],\n",
    "    \"StatefulSet\": [\n",
    "        \"Predictable (redis-0, redis-1)\",\n",
    "        \"Stable DNS names\",\n",
    "        \"PVC per pod\",\n",
    "        \"Sequential (0â†’1â†’2)\",\n",
    "        \"Databases, caches, distributed training\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ’¡ When to Use StatefulSet for ML:\")\n",
    "print(\"   âœ… Distributed training (PyTorch DDP, Horovod)\")\n",
    "print(\"   âœ… Caching layer (Redis, Memcached)\")\n",
    "print(\"   âœ… Message queues (Kafka, RabbitMQ)\")\n",
    "print(\"   âœ… Databases (PostgreSQL, MongoDB)\")\n",
    "print(\"   âœ… Model artifact storage (per-worker checkpoints)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b46644",
   "metadata": {},
   "source": [
    "## 3. ðŸŒ DaemonSets - One Pod Per Node for Cluster-Wide Services\n",
    "\n",
    "**Purpose:** Ensure exactly one pod runs on every node (or selected nodes) for infrastructure services.\n",
    "\n",
    "**Key Points:**\n",
    "- **One pod per node:** Automatically schedules pod on each node (including new nodes)\n",
    "- **Node Selectors:** Run only on nodes matching labels (`gpu=nvidia`, `workload=ml`)\n",
    "- **Tolerations:** Run on tainted nodes (e.g., GPU nodes tainted to prevent non-GPU workloads)\n",
    "- **Init Containers:** Run setup tasks before main container (install drivers, configure system)\n",
    "- **Auto-scaling:** New node added â†’ DaemonSet pod automatically scheduled\n",
    "\n",
    "**Why It Matters:**\n",
    "- **GPU drivers:** Every GPU node needs NVIDIA drivers (DaemonSet installs automatically)\n",
    "- **Monitoring:** Prometheus node-exporter on every node (collects CPU, memory, disk metrics)\n",
    "- **Logging:** Fluentd on every node (collects logs, ships to Elasticsearch)\n",
    "- **Networking:** CNI plugins, kube-proxy run as DaemonSets\n",
    "- **Security:** Falco security agent on every node (detects anomalies)\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "GPU driver DaemonSet: Runs on 10 GPU nodes (label: `gpu=nvidia`), installs NVIDIA driver v525, configures CUDA toolkit, enables `nvidia.com/gpu` resource, skips 10 CPU-only nodes (no label match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc7fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DaemonSet Simulation\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    \"\"\"Represents a Kubernetes node.\"\"\"\n",
    "    name: str\n",
    "    labels: Dict[str, str] = field(default_factory=dict)\n",
    "    taints: List[str] = field(default_factory=list)\n",
    "    cpu_capacity: float = 8.0\n",
    "    memory_capacity: int = 16384\n",
    "    has_gpu: bool = False\n",
    "    \n",
    "    def matches_selector(self, node_selector: Dict[str, str]) -> bool:\n",
    "        \"\"\"Check if node matches selector.\"\"\"\n",
    "        for key, value in node_selector.items():\n",
    "            if self.labels.get(key) != value:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def has_toleration_for_taints(self, tolerations: List[str]) -> bool:\n",
    "        \"\"\"Check if pod can tolerate node taints.\"\"\"\n",
    "        for taint in self.taints:\n",
    "            if taint not in tolerations:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DaemonPod:\n",
    "    \"\"\"Represents a pod scheduled by DaemonSet.\"\"\"\n",
    "    name: str\n",
    "    node: str\n",
    "    status: str = \"Running\"\n",
    "    init_complete: bool = False\n",
    "    driver_version: Optional[str] = None\n",
    "    \n",
    "    def run_init_container(self, task: str) -> str:\n",
    "        \"\"\"Simulate init container execution.\"\"\"\n",
    "        self.init_complete = True\n",
    "        return f\"Init: {task} completed\"\n",
    "\n",
    "\n",
    "class DaemonSet:\n",
    "    \"\"\"Simulates Kubernetes DaemonSet.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, image: str, \n",
    "                 node_selector: Optional[Dict[str, str]] = None,\n",
    "                 tolerations: Optional[List[str]] = None,\n",
    "                 init_tasks: Optional[List[str]] = None):\n",
    "        self.name = name\n",
    "        self.image = image\n",
    "        self.node_selector = node_selector or {}\n",
    "        self.tolerations = tolerations or []\n",
    "        self.init_tasks = init_tasks or []\n",
    "        self.pods: List[DaemonPod] = []\n",
    "    \n",
    "    def deploy(self, nodes: List[Node]):\n",
    "        \"\"\"Deploy DaemonSet (one pod per matching node).\"\"\"\n",
    "        print(f\"\\nðŸš€ Deploying DaemonSet '{self.name}'\")\n",
    "        print(f\"   Image: {self.image}\")\n",
    "        if self.node_selector:\n",
    "            print(f\"   Node Selector: {self.node_selector}\")\n",
    "        if self.tolerations:\n",
    "            print(f\"   Tolerations: {self.tolerations}\")\n",
    "        print()\n",
    "        \n",
    "        scheduled_count = 0\n",
    "        skipped_count = 0\n",
    "        \n",
    "        for node in nodes:\n",
    "            # Check node selector\n",
    "            if self.node_selector and not node.matches_selector(self.node_selector):\n",
    "                print(f\"â­ï¸  Skipped {node.name}: Node selector mismatch\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Check taints\n",
    "            if node.taints and not node.has_toleration_for_taints(self.tolerations):\n",
    "                print(f\"â­ï¸  Skipped {node.name}: Missing toleration for taints {node.taints}\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Schedule pod\n",
    "            pod = DaemonPod(\n",
    "                name=f\"{self.name}-{node.name}\",\n",
    "                node=node.name\n",
    "            )\n",
    "            \n",
    "            # Run init containers\n",
    "            if self.init_tasks:\n",
    "                print(f\"ðŸ”§ {pod.name} on {node.name}:\")\n",
    "                for task in self.init_tasks:\n",
    "                    result = pod.run_init_container(task)\n",
    "                    print(f\"   {result}\")\n",
    "                print(f\"   Main container started\")\n",
    "            else:\n",
    "                print(f\"âœ… {pod.name} scheduled on {node.name}\")\n",
    "            \n",
    "            self.pods.append(pod)\n",
    "            scheduled_count += 1\n",
    "        \n",
    "        print(f\"\\nâœ… DaemonSet {self.name} deployed:\")\n",
    "        print(f\"   Pods scheduled: {scheduled_count}\")\n",
    "        print(f\"   Nodes skipped: {skipped_count}\")\n",
    "        print(f\"   Total pods: {len(self.pods)}\\n\")\n",
    "    \n",
    "    def handle_new_node(self, node: Node):\n",
    "        \"\"\"Automatically schedule pod on new node.\"\"\"\n",
    "        print(f\"\\nðŸ†• New node detected: {node.name}\")\n",
    "        \n",
    "        # Check if should schedule\n",
    "        if self.node_selector and not node.matches_selector(self.node_selector):\n",
    "            print(f\"   â­ï¸ Skipped: Node selector mismatch\")\n",
    "            return\n",
    "        \n",
    "        if node.taints and not node.has_toleration_for_taints(self.tolerations):\n",
    "            print(f\"   â­ï¸ Skipped: Missing toleration\")\n",
    "            return\n",
    "        \n",
    "        # Schedule pod\n",
    "        pod = DaemonPod(\n",
    "            name=f\"{self.name}-{node.name}\",\n",
    "            node=node.name\n",
    "        )\n",
    "        \n",
    "        # Run init tasks\n",
    "        for task in self.init_tasks:\n",
    "            pod.run_init_container(task)\n",
    "        \n",
    "        self.pods.append(pod)\n",
    "        print(f\"   âœ… DaemonSet pod {pod.name} scheduled automatically\")\n",
    "    \n",
    "    def get_status(self) -> Dict:\n",
    "        \"\"\"Get DaemonSet status.\"\"\"\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"desired_pods\": len(self.pods),\n",
    "            \"ready_pods\": sum(1 for p in self.pods if p.status == \"Running\"),\n",
    "            \"nodes_scheduled\": [p.node for p in self.pods]\n",
    "        }\n",
    "\n",
    "\n",
    "# Example 1: Create cluster with GPU and CPU nodes\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE 1: Cluster Setup - GPU and CPU Nodes\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "nodes = [\n",
    "    # GPU nodes (labeled and tainted)\n",
    "    Node(name=\"gpu-node-1\", labels={\"gpu\": \"nvidia\", \"gpu-model\": \"t4\"}, \n",
    "         taints=[\"gpu-only\"], has_gpu=True),\n",
    "    Node(name=\"gpu-node-2\", labels={\"gpu\": \"nvidia\", \"gpu-model\": \"t4\"}, \n",
    "         taints=[\"gpu-only\"], has_gpu=True),\n",
    "    Node(name=\"gpu-node-3\", labels={\"gpu\": \"nvidia\", \"gpu-model\": \"v100\"}, \n",
    "         taints=[\"gpu-only\"], has_gpu=True),\n",
    "    \n",
    "    # CPU-only nodes (no special labels)\n",
    "    Node(name=\"cpu-node-1\", labels={\"workload\": \"general\"}),\n",
    "    Node(name=\"cpu-node-2\", labels={\"workload\": \"general\"}),\n",
    "    Node(name=\"cpu-node-3\", labels={\"workload\": \"general\"}),\n",
    "]\n",
    "\n",
    "print(\"ðŸ“Š Cluster Nodes:\\n\")\n",
    "for node in nodes:\n",
    "    gpu_info = \"GPU âœ…\" if node.has_gpu else \"CPU only\"\n",
    "    labels_str = \", \".join([f\"{k}={v}\" for k, v in node.labels.items()])\n",
    "    taints_str = \", \".join(node.taints) if node.taints else \"None\"\n",
    "    print(f\"   {node.name}: {gpu_info}\")\n",
    "    print(f\"      Labels: {labels_str}\")\n",
    "    print(f\"      Taints: {taints_str}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE 2: GPU Driver DaemonSet (GPU Nodes Only)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# DaemonSet for GPU drivers (only on GPU nodes)\n",
    "gpu_driver_ds = DaemonSet(\n",
    "    name=\"nvidia-driver-installer\",\n",
    "    image=\"nvidia/driver:525.60.13\",\n",
    "    node_selector={\"gpu\": \"nvidia\"},  # Only GPU nodes\n",
    "    tolerations=[\"gpu-only\"],  # Tolerate GPU taint\n",
    "    init_tasks=[\n",
    "        \"Install NVIDIA driver v525.60.13\",\n",
    "        \"Configure CUDA toolkit 12.0\",\n",
    "        \"Enable nvidia.com/gpu resource\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "gpu_driver_ds.deploy(nodes)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE 3: Monitoring DaemonSet (All Nodes)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# DaemonSet for monitoring (all nodes)\n",
    "prometheus_ds = DaemonSet(\n",
    "    name=\"node-exporter\",\n",
    "    image=\"prom/node-exporter:latest\",\n",
    "    node_selector={},  # No selector = all nodes\n",
    "    tolerations=[\"gpu-only\"]  # Tolerate GPU taint to run on GPU nodes too\n",
    ")\n",
    "\n",
    "prometheus_ds.deploy(nodes)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE 4: Auto-Scheduling on New Node\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Add new GPU node\n",
    "new_gpu_node = Node(\n",
    "    name=\"gpu-node-4\",\n",
    "    labels={\"gpu\": \"nvidia\", \"gpu-model\": \"a100\"},\n",
    "    taints=[\"gpu-only\"],\n",
    "    has_gpu=True\n",
    ")\n",
    "\n",
    "print(\"ðŸ†• Adding new GPU node to cluster...\\n\")\n",
    "nodes.append(new_gpu_node)\n",
    "\n",
    "# DaemonSets automatically schedule pods\n",
    "gpu_driver_ds.handle_new_node(new_gpu_node)\n",
    "prometheus_ds.handle_new_node(new_gpu_node)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE 5: DaemonSet Status Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# GPU driver status\n",
    "gpu_status = gpu_driver_ds.get_status()\n",
    "print(f\"ðŸ“Š {gpu_status['name']}:\")\n",
    "print(f\"   Desired Pods: {gpu_status['desired_pods']}\")\n",
    "print(f\"   Ready Pods: {gpu_status['ready_pods']}\")\n",
    "print(f\"   Nodes: {', '.join(gpu_status['nodes_scheduled'])}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Monitoring status\n",
    "mon_status = prometheus_ds.get_status()\n",
    "print(f\"ðŸ“Š {mon_status['name']}:\")\n",
    "print(f\"   Desired Pods: {mon_status['desired_pods']}\")\n",
    "print(f\"   Ready Pods: {mon_status['ready_pods']}\")\n",
    "print(f\"   Nodes: {', '.join(mon_status['nodes_scheduled'])}\")\n",
    "\n",
    "# Visualize DaemonSet deployment\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE 6: DaemonSet Deployment Visualization\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create visualization data\n",
    "node_names = [n.name for n in nodes]\n",
    "gpu_driver_scheduled = [1 if n.has_gpu else 0 for n in nodes]\n",
    "node_exporter_scheduled = [1 for _ in nodes]  # All nodes\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: GPU Driver DaemonSet\n",
    "colors_gpu = ['#4ECDC4' if v == 1 else '#95E1D3' for v in gpu_driver_scheduled]\n",
    "bars1 = ax1.bar(node_names, gpu_driver_scheduled, color=colors_gpu, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_ylabel(\"Pod Scheduled (1=Yes, 0=No)\", fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel(\"Node Name\", fontsize=12, fontweight='bold')\n",
    "ax1.set_title(\"GPU Driver DaemonSet\\n(GPU Nodes Only)\", fontsize=14, fontweight='bold', pad=20)\n",
    "ax1.set_ylim(0, 1.2)\n",
    "ax1.set_xticklabels(node_names, rotation=45, ha='right')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add labels\n",
    "for i, (bar, val) in enumerate(zip(bars1, gpu_driver_scheduled)):\n",
    "    label = \"âœ… Scheduled\" if val == 1 else \"â­ï¸ Skipped\"\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, val + 0.05, label, \n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 2: Node Exporter DaemonSet\n",
    "bars2 = ax2.bar(node_names, node_exporter_scheduled, color='#FF6B6B', edgecolor='black', linewidth=1.5)\n",
    "ax2.set_ylabel(\"Pod Scheduled (1=Yes, 0=No)\", fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel(\"Node Name\", fontsize=12, fontweight='bold')\n",
    "ax2.set_title(\"Node Exporter DaemonSet\\n(All Nodes)\", fontsize=14, fontweight='bold', pad=20)\n",
    "ax2.set_ylim(0, 1.2)\n",
    "ax2.set_xticklabels(node_names, rotation=45, ha='right')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add labels\n",
    "for bar in bars2:\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, 1.05, \"âœ… Scheduled\", \n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Key Insights:\")\n",
    "print(\"   â€¢ GPU Driver DaemonSet: 4/7 nodes (only GPU nodes)\")\n",
    "print(\"   â€¢ Node Exporter DaemonSet: 7/7 nodes (all nodes)\")\n",
    "print(\"   â€¢ New nodes: Pods automatically scheduled\")\n",
    "print(\"   â€¢ Node selectors: Filter nodes by labels\")\n",
    "print(\"   â€¢ Tolerations: Run on tainted nodes\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Common DaemonSet Use Cases for ML:\")\n",
    "print(\"   âœ… GPU drivers (NVIDIA, AMD)\")\n",
    "print(\"   âœ… Monitoring agents (Prometheus node-exporter, Datadog)\")\n",
    "print(\"   âœ… Log collectors (Fluentd, Filebeat)\")\n",
    "print(\"   âœ… Security agents (Falco, Sysdig)\")\n",
    "print(\"   âœ… Network plugins (Calico, Weave)\")\n",
    "print(\"   âœ… Storage drivers (Ceph, GlusterFS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68389f52",
   "metadata": {},
   "source": [
    "## 4. ðŸ¤– Kubernetes Operators - Encoding Operational Knowledge\n",
    "\n",
    "**Purpose:** Automate complex application lifecycle management by extending Kubernetes with custom controllers that watch resources and take actions.\n",
    "\n",
    "**Key Points:**\n",
    "- **Custom Resource Definitions (CRDs):** Define custom resources (`TrainingJob`, `ModelServer`, `HyperparameterTuning`)\n",
    "- **Controller:** Watches CRDs, reconciles desired vs actual state (creates pods, services, manages lifecycle)\n",
    "- **Reconciliation Loop:** Continuously ensures desired state (`spec`) matches actual state (`status`)\n",
    "- **Domain Knowledge:** Operator encodes best practices (backup strategies, scaling logic, failure handling)\n",
    "- **Self-Healing:** Automatically recovers from failures (restarts training, scales down unhealthy pods)\n",
    "\n",
    "**Why It Matters:**\n",
    "- **Simplicity:** User creates `TrainingJob` CRD, operator handles complexity (pods, volumes, services)\n",
    "- **Consistency:** Operator ensures best practices (always use GPU scheduling, always save checkpoints)\n",
    "- **Automation:** Operator watches metrics, auto-scales, auto-retries, auto-backs-up\n",
    "- **Extensibility:** Add ML-specific features to Kubernetes (model versioning, A/B testing, drift detection)\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "STDF Parser Operator: User creates `STDFParserJob` CRD with file list, operator creates parser pods (scales based on queue depth), monitors progress (tracks files parsed/failed), handles failures (retries failed files up to 3 times), updates job status (parsed=9500, failed=500, completion=95%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff6ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kubernetes Operator Simulation\n",
    "\n",
    "class JobStatus(Enum):\n",
    "    \"\"\"Training job status.\"\"\"\n",
    "    PENDING = \"Pending\"\n",
    "    RUNNING = \"Running\"\n",
    "    SUCCEEDED = \"Succeeded\"\n",
    "    FAILED = \"Failed\"\n",
    "    RETRYING = \"Retrying\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingJobSpec:\n",
    "    \"\"\"Desired state for training job (user-defined).\"\"\"\n",
    "    model_name: str\n",
    "    dataset: str\n",
    "    epochs: int = 10\n",
    "    batch_size: int = 32\n",
    "    replicas: int = 1  # Distributed training workers\n",
    "    gpu_per_worker: int = 1\n",
    "    max_retries: int = 3\n",
    "    checkpoint_interval: int = 5  # Save every N epochs\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingJobStatus:\n",
    "    \"\"\"Actual state for training job (operator-managed).\"\"\"\n",
    "    phase: JobStatus = JobStatus.PENDING\n",
    "    current_epoch: int = 0\n",
    "    accuracy: float = 0.0\n",
    "    loss: float = 0.0\n",
    "    retries: int = 0\n",
    "    pods_ready: int = 0\n",
    "    start_time: Optional[str] = None\n",
    "    end_time: Optional[str] = None\n",
    "    message: str = \"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingJobCRD:\n",
    "    \"\"\"Custom Resource Definition for ML training job.\"\"\"\n",
    "    api_version: str = \"ml.kubeflow.org/v1\"\n",
    "    kind: str = \"TrainingJob\"\n",
    "    metadata: Dict[str, str] = field(default_factory=dict)\n",
    "    spec: TrainingJobSpec = None\n",
    "    status: TrainingJobStatus = field(default_factory=TrainingJobStatus)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.metadata:\n",
    "            self.metadata = {\"name\": f\"training-{uuid.uuid4().hex[:8]}\"}\n",
    "\n",
    "\n",
    "class MLTrainingOperator:\n",
    "    \"\"\"Kubernetes Operator for ML training jobs.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"ml-training-operator\"):\n",
    "        self.name = name\n",
    "        self.watched_jobs: List[TrainingJobCRD] = []\n",
    "        self.reconciliation_history: List[Dict] = []\n",
    "    \n",
    "    def watch_job(self, job: TrainingJobCRD):\n",
    "        \"\"\"Add job to watch list.\"\"\"\n",
    "        self.watched_jobs.append(job)\n",
    "        print(f\"ðŸ‘€ Operator watching job: {job.metadata['name']}\")\n",
    "    \n",
    "    def reconcile(self, job: TrainingJobCRD) -> Dict:\n",
    "        \"\"\"Reconciliation loop: ensure desired state matches actual state.\"\"\"\n",
    "        reconciliation = {\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"job_name\": job.metadata['name'],\n",
    "            \"actions\": []\n",
    "        }\n",
    "        \n",
    "        # Phase: PENDING â†’ RUNNING\n",
    "        if job.status.phase == JobStatus.PENDING:\n",
    "            print(f\"\\nðŸ”„ Reconciling job: {job.metadata['name']}\")\n",
    "            print(f\"   Current state: {job.status.phase.value}\")\n",
    "            print(f\"   Desired state: Training with {job.spec.replicas} workers\")\n",
    "            \n",
    "            # Create pods for training\n",
    "            job.status.pods_ready = job.spec.replicas\n",
    "            job.status.phase = JobStatus.RUNNING\n",
    "            job.status.start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            job.status.message = f\"Created {job.spec.replicas} worker pods\"\n",
    "            \n",
    "            reconciliation[\"actions\"].append(\n",
    "                f\"Created {job.spec.replicas} pods with {job.spec.gpu_per_worker} GPU each\"\n",
    "            )\n",
    "            reconciliation[\"actions\"].append(f\"Started training: {job.spec.model_name}\")\n",
    "            \n",
    "            print(f\"   âœ… Actions: {reconciliation['actions']}\")\n",
    "        \n",
    "        # Phase: RUNNING â†’ simulate training progress\n",
    "        elif job.status.phase == JobStatus.RUNNING:\n",
    "            # Simulate epoch progress\n",
    "            if job.status.current_epoch < job.spec.epochs:\n",
    "                job.status.current_epoch += 1\n",
    "                job.status.accuracy = 0.50 + (job.status.current_epoch / job.spec.epochs) * 0.45\n",
    "                job.status.loss = 2.0 - (job.status.current_epoch / job.spec.epochs) * 1.5\n",
    "                \n",
    "                reconciliation[\"actions\"].append(\n",
    "                    f\"Epoch {job.status.current_epoch}/{job.spec.epochs}: \"\n",
    "                    f\"accuracy={job.status.accuracy:.3f}, loss={job.status.loss:.3f}\"\n",
    "                )\n",
    "                \n",
    "                # Checkpoint every N epochs\n",
    "                if job.status.current_epoch % job.spec.checkpoint_interval == 0:\n",
    "                    reconciliation[\"actions\"].append(\n",
    "                        f\"Saved checkpoint at epoch {job.status.current_epoch}\"\n",
    "                    )\n",
    "            \n",
    "            # Training complete\n",
    "            if job.status.current_epoch >= job.spec.epochs:\n",
    "                job.status.phase = JobStatus.SUCCEEDED\n",
    "                job.status.end_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                job.status.message = f\"Training completed: {job.spec.epochs} epochs\"\n",
    "                reconciliation[\"actions\"].append(\"Training succeeded âœ…\")\n",
    "        \n",
    "        # Phase: FAILED â†’ retry logic\n",
    "        elif job.status.phase == JobStatus.FAILED:\n",
    "            if job.status.retries < job.spec.max_retries:\n",
    "                job.status.retries += 1\n",
    "                job.status.phase = JobStatus.RETRYING\n",
    "                job.status.current_epoch = 0  # Restart from last checkpoint\n",
    "                reconciliation[\"actions\"].append(\n",
    "                    f\"Retry {job.status.retries}/{job.spec.max_retries}: Restarting training\"\n",
    "                )\n",
    "            else:\n",
    "                job.status.message = f\"Max retries exceeded ({job.spec.max_retries})\"\n",
    "                reconciliation[\"actions\"].append(\"Max retries reached, job failed permanently âŒ\")\n",
    "        \n",
    "        self.reconciliation_history.append(reconciliation)\n",
    "        return reconciliation\n",
    "    \n",
    "    def simulate_failure(self, job: TrainingJobCRD, reason: str = \"OOM\"):\n",
    "        \"\"\"Simulate training failure.\"\"\"\n",
    "        job.status.phase = JobStatus.FAILED\n",
    "        job.status.message = f\"Training failed: {reason}\"\n",
    "        print(f\"\\nâŒ Simulated failure: {job.metadata['name']} ({reason})\")\n",
    "    \n",
    "    def get_job_status(self, job_name: str) -> Optional[TrainingJobCRD]:\n",
    "        \"\"\"Get job status.\"\"\"\n",
    "        for job in self.watched_jobs:\n",
    "            if job.metadata['name'] == job_name:\n",
    "                return job\n",
    "        return None\n",
    "    \n",
    "    def run_reconciliation_loop(self, iterations: int = 15):\n",
    "        \"\"\"Run reconciliation loop for all watched jobs.\"\"\"\n",
    "        print(f\"\\nðŸ”„ Starting reconciliation loop ({iterations} iterations)...\\n\")\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            print(f\"--- Iteration {i+1}/{iterations} ---\")\n",
    "            \n",
    "            for job in self.watched_jobs:\n",
    "                if job.status.phase not in [JobStatus.SUCCEEDED, JobStatus.FAILED]:\n",
    "                    self.reconcile(job)\n",
    "            \n",
    "            time.sleep(0.1)  # Simulate time passing\n",
    "            \n",
    "            # Check if all jobs done\n",
    "            all_done = all(job.status.phase in [JobStatus.SUCCEEDED, JobStatus.FAILED] \n",
    "                          for job in self.watched_jobs)\n",
    "            if all_done:\n",
    "                print(f\"\\nâœ… All jobs completed\")\n",
    "                break\n",
    "\n",
    "\n",
    "# Example 1: Create Training Job CRD\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE 1: Define TrainingJob Custom Resource\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# User creates TrainingJob CRD (like kubectl apply -f training-job.yaml)\n",
    "wafer_training_job = TrainingJobCRD(\n",
    "    metadata={\"name\": \"wafer-yield-training-v1\"},\n",
    "    spec=TrainingJobSpec(\n",
    "        model_name=\"wafer_yield_predictor\",\n",
    "        dataset=\"wafer_test_data_2024_q4\",\n",
    "        epochs=20,\n",
    "        batch_size=64,\n",
    "        replicas=4,  # 4 workers for distributed training\n",
    "        gpu_per_worker=1,\n",
    "        max_retries=3,\n",
    "        checkpoint_interval=5\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"ðŸ“„ TrainingJob CRD:\")\n",
    "print(f\"   Name: {wafer_training_job.metadata['name']}\")\n",
    "print(f\"   Model: {wafer_training_job.spec.model_name}\")\n",
    "print(f\"   Dataset: {wafer_training_job.spec.dataset}\")\n",
    "print(f\"   Epochs: {wafer_training_job.spec.epochs}\")\n",
    "print(f\"   Workers: {wafer_training_job.spec.replicas}\")\n",
    "print(f\"   GPU per worker: {wafer_training_job.spec.gpu_per_worker}\")\n",
    "print(f\"   Status: {wafer_training_job.status.phase.value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE 2: Operator Watches and Reconciles Job\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create operator\n",
    "operator = MLTrainingOperator()\n",
    "\n",
    "# Operator watches job\n",
    "operator.watch_job(wafer_training_job)\n",
    "\n",
    "# Simulate reconciliation loop (operator runs continuously)\n",
    "operator.run_reconciliation_loop(iterations=25)\n",
    "\n",
    "# Check final status\n",
    "final_job = operator.get_job_status(\"wafer-yield-training-v1\")\n",
    "print(f\"\\nðŸ“Š Final Job Status:\")\n",
    "print(f\"   Phase: {final_job.status.phase.value}\")\n",
    "print(f\"   Epochs Completed: {final_job.status.current_epoch}/{final_job.spec.epochs}\")\n",
    "print(f\"   Final Accuracy: {final_job.status.accuracy:.3f}\")\n",
    "print(f\"   Final Loss: {final_job.status.loss:.3f}\")\n",
    "print(f\"   Start Time: {final_job.status.start_time}\")\n",
    "print(f\"   End Time: {final_job.status.end_time}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE 3: Operator Handles Failures with Auto-Retry\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create another job\n",
    "stdf_training_job = TrainingJobCRD(\n",
    "    metadata={\"name\": \"stdf-parser-training-v2\"},\n",
    "    spec=TrainingJobSpec(\n",
    "        model_name=\"stdf_anomaly_detector\",\n",
    "        dataset=\"stdf_historical_2024\",\n",
    "        epochs=15,\n",
    "        batch_size=32,\n",
    "        replicas=2,\n",
    "        gpu_per_worker=1,\n",
    "        max_retries=3\n",
    "    )\n",
    ")\n",
    "\n",
    "operator.watch_job(stdf_training_job)\n",
    "\n",
    "# Run for 5 iterations\n",
    "for i in range(5):\n",
    "    operator.reconcile(stdf_training_job)\n",
    "    print(f\"   Epoch {stdf_training_job.status.current_epoch}: \"\n",
    "          f\"accuracy={stdf_training_job.status.accuracy:.3f}\")\n",
    "    time.sleep(0.05)\n",
    "\n",
    "# Simulate OOM failure\n",
    "operator.simulate_failure(stdf_training_job, reason=\"OutOfMemory (GPU OOM)\")\n",
    "\n",
    "# Operator auto-retries\n",
    "print(\"\\nðŸ”„ Operator detecting failure, initiating auto-retry...\")\n",
    "operator.reconcile(stdf_training_job)\n",
    "\n",
    "print(f\"\\nðŸ“Š Job Status After Retry:\")\n",
    "print(f\"   Phase: {stdf_training_job.status.phase.value}\")\n",
    "print(f\"   Retries: {stdf_training_job.status.retries}/{stdf_training_job.spec.max_retries}\")\n",
    "print(f\"   Message: {stdf_training_job.status.message}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight: Operator automatically retries failed jobs (no manual intervention)\")\n",
    "\n",
    "# Visualize training progress\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE 4: Training Progress Visualization\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extract metrics from successful job\n",
    "epochs = list(range(1, final_job.status.current_epoch + 1))\n",
    "accuracies = [0.50 + (e / final_job.spec.epochs) * 0.45 for e in epochs]\n",
    "losses = [2.0 - (e / final_job.spec.epochs) * 1.5 for e in epochs]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Accuracy\n",
    "ax1.plot(epochs, accuracies, marker='o', linewidth=2.5, markersize=8, color='#4ECDC4')\n",
    "ax1.fill_between(epochs, 0, accuracies, alpha=0.3, color='#4ECDC4')\n",
    "ax1.axhline(y=0.95, color='green', linestyle='--', linewidth=2, label='Target (95%)')\n",
    "ax1.set_xlabel(\"Epoch\", fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel(\"Accuracy\", fontsize=12, fontweight='bold')\n",
    "ax1.set_title(\"Training Accuracy Over Time\\n(Wafer Yield Predictor)\", \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Mark checkpoints\n",
    "checkpoint_epochs = [e for e in epochs if e % final_job.spec.checkpoint_interval == 0]\n",
    "checkpoint_accs = [accuracies[e-1] for e in checkpoint_epochs]\n",
    "ax1.scatter(checkpoint_epochs, checkpoint_accs, s=200, c='red', marker='s', \n",
    "            edgecolors='black', linewidths=2, label='Checkpoint', zorder=5)\n",
    "\n",
    "# Plot 2: Loss\n",
    "ax2.plot(epochs, losses, marker='s', linewidth=2.5, markersize=8, color='#FF6B6B')\n",
    "ax2.fill_between(epochs, 0, losses, alpha=0.3, color='#FF6B6B')\n",
    "ax2.set_xlabel(\"Epoch\", fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel(\"Loss\", fontsize=12, fontweight='bold')\n",
    "ax2.set_title(\"Training Loss Over Time\\n(Wafer Yield Predictor)\", \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 2.5)\n",
    "\n",
    "# Mark checkpoints\n",
    "checkpoint_losses = [losses[e-1] for e in checkpoint_epochs]\n",
    "ax2.scatter(checkpoint_epochs, checkpoint_losses, s=200, c='red', marker='s', \n",
    "            edgecolors='black', linewidths=2, label='Checkpoint', zorder=5)\n",
    "ax2.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Operator Features Demonstrated:\")\n",
    "print(\"   âœ… Watches TrainingJob CRDs (reconciliation loop)\")\n",
    "print(\"   âœ… Creates pods and resources automatically\")\n",
    "print(\"   âœ… Monitors training progress (updates status)\")\n",
    "print(\"   âœ… Saves checkpoints periodically\")\n",
    "print(\"   âœ… Auto-retries on failure (up to max_retries)\")\n",
    "print(\"   âœ… Updates job status (phase, accuracy, loss)\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Popular ML Operators:\")\n",
    "print(\"   â€¢ Kubeflow Training Operator (TensorFlow, PyTorch, MXNet)\")\n",
    "print(\"   â€¢ KServe (Model serving with auto-scaling)\")\n",
    "print(\"   â€¢ Seldon Core Operator (Advanced model serving)\")\n",
    "print(\"   â€¢ Argo Workflows (DAG-based ML pipelines)\")\n",
    "print(\"   â€¢ MLflow Operator (Experiment tracking integration)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e7cbdf",
   "metadata": {},
   "source": [
    "## 5. ðŸ“‹ Custom Resource Definitions (CRDs) - Extending Kubernetes API\n",
    "\n",
    "### ðŸ“ What Are CRDs?\n",
    "\n",
    "**Purpose:** Extend Kubernetes API with custom resource types for ML workflows\n",
    "\n",
    "**Key Points:**\n",
    "- **Custom Resources**: Define domain-specific objects (TrainingJob, ModelServer, HyperparameterTuning)\n",
    "- **API Extension**: Kubernetes treats CRDs like built-in resources (Pod, Service, Deployment)\n",
    "- **Declarative**: Users define desired state in YAML, operators ensure actual state matches\n",
    "- **Validation**: CRDs support schema validation, default values, version management\n",
    "- **CRUD Operations**: kubectl get/describe/delete work with CRDs just like native resources\n",
    "\n",
    "**Why CRDs Matter:**\n",
    "- âœ… **Simplify ML Workflows**: Data scientists use `kubectl apply -f training-job.yaml` instead of complex scripts\n",
    "- âœ… **Self-Service**: Users create resources without understanding Kubernetes internals\n",
    "- âœ… **Standardization**: Consistent API across teams and projects\n",
    "- âœ… **Automation**: Operators watch CRDs and automate complex operations\n",
    "\n",
    "**Post-Silicon Validation Application:**\n",
    "\n",
    "**Multi-Model Ensemble for Wafer Yield Prediction:**\n",
    "- **Input**: Create `EnsembleModel` CRD with 5 base models (Random Forest, XGBoost, LightGBM, CatBoost, Neural Net)\n",
    "- **Operator**: Deploys each model as separate pod, creates ensemble combiner, monitors performance\n",
    "- **Output**: Single prediction endpoint combining all models (improves accuracy from 92% to 96.5%)\n",
    "- **Value**: $4.2M annual savings (fewer false negatives reducing yield loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48084432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Resource Definitions (CRDs) Simulation\n",
    "\n",
    "@dataclass\n",
    "class ModelServerSpec:\n",
    "    \"\"\"Desired state for model server.\"\"\"\n",
    "    model_name: str\n",
    "    model_path: str\n",
    "    framework: str  # tensorflow, pytorch, sklearn\n",
    "    replicas: int = 3\n",
    "    gpu_enabled: bool = False\n",
    "    min_replicas: int = 1\n",
    "    max_replicas: int = 10\n",
    "    target_cpu_utilization: int = 70  # Auto-scale at 70% CPU\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelServerStatus:\n",
    "    \"\"\"Actual state for model server.\"\"\"\n",
    "    replicas: int = 0\n",
    "    ready_replicas: int = 0\n",
    "    endpoint: str = \"\"\n",
    "    requests_per_second: float = 0.0\n",
    "    avg_latency_ms: float = 0.0\n",
    "    error_rate: float = 0.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelServerCRD:\n",
    "    \"\"\"CRD for model serving.\"\"\"\n",
    "    api_version: str = \"serving.kubeflow.org/v1\"\n",
    "    kind: str = \"ModelServer\"\n",
    "    metadata: Dict[str, str] = field(default_factory=dict)\n",
    "    spec: ModelServerSpec = None\n",
    "    status: ModelServerStatus = field(default_factory=ModelServerStatus)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HyperparameterTuningSpec:\n",
    "    \"\"\"Desired state for hyperparameter tuning job.\"\"\"\n",
    "    model_name: str\n",
    "    algorithm: str  # random, grid, bayesian\n",
    "    max_trials: int = 50\n",
    "    max_parallel_trials: int = 5\n",
    "    objective_metric: str = \"accuracy\"\n",
    "    objective_type: str = \"maximize\"  # maximize or minimize\n",
    "    parameters: Dict[str, Dict] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Trial:\n",
    "    \"\"\"Single hyperparameter trial.\"\"\"\n",
    "    trial_id: str\n",
    "    parameters: Dict[str, float]\n",
    "    status: str = \"Pending\"  # Pending, Running, Succeeded, Failed\n",
    "    objective_value: Optional[float] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HyperparameterTuningStatus:\n",
    "    \"\"\"Actual state for hyperparameter tuning.\"\"\"\n",
    "    trials_completed: int = 0\n",
    "    trials_failed: int = 0\n",
    "    best_trial: Optional[Trial] = None\n",
    "    current_trials: List[Trial] = field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HyperparameterTuningCRD:\n",
    "    \"\"\"CRD for hyperparameter tuning.\"\"\"\n",
    "    api_version: str = \"katib.kubeflow.org/v1\"\n",
    "    kind: str = \"HyperparameterTuning\"\n",
    "    metadata: Dict[str, str] = field(default_factory=dict)\n",
    "    spec: HyperparameterTuningSpec = None\n",
    "    status: HyperparameterTuningStatus = field(default_factory=HyperparameterTuningStatus)\n",
    "\n",
    "\n",
    "# Example 1: ModelServer CRD for Wafer Yield Prediction\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE 1: ModelServer CRD - Deploy ML Model with Auto-Scaling\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "wafer_model_server = ModelServerCRD(\n",
    "    metadata={\"name\": \"wafer-yield-predictor-v3\"},\n",
    "    spec=ModelServerSpec(\n",
    "        model_name=\"wafer_yield_xgboost\",\n",
    "        model_path=\"s3://ml-models/wafer/xgboost-v3.pkl\",\n",
    "        framework=\"sklearn\",\n",
    "        replicas=3,\n",
    "        gpu_enabled=False,\n",
    "        min_replicas=2,\n",
    "        max_replicas=8,\n",
    "        target_cpu_utilization=75\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"ðŸ“„ ModelServer CRD:\")\n",
    "print(f\"   Name: {wafer_model_server.metadata['name']}\")\n",
    "print(f\"   Model: {wafer_model_server.spec.model_name}\")\n",
    "print(f\"   Framework: {wafer_model_server.spec.framework}\")\n",
    "print(f\"   Replicas: {wafer_model_server.spec.replicas}\")\n",
    "print(f\"   Auto-scaling: {wafer_model_server.spec.min_replicas}-{wafer_model_server.spec.max_replicas} replicas\")\n",
    "print(f\"   GPU: {'Enabled' if wafer_model_server.spec.gpu_enabled else 'Disabled'}\")\n",
    "\n",
    "# Simulate operator creating resources\n",
    "wafer_model_server.status.replicas = wafer_model_server.spec.replicas\n",
    "wafer_model_server.status.ready_replicas = wafer_model_server.spec.replicas\n",
    "wafer_model_server.status.endpoint = f\"http://{wafer_model_server.metadata['name']}.default.svc.cluster.local/v1/predict\"\n",
    "wafer_model_server.status.requests_per_second = 245.5\n",
    "wafer_model_server.status.avg_latency_ms = 12.3\n",
    "wafer_model_server.status.error_rate = 0.002\n",
    "\n",
    "print(f\"\\nðŸ“Š ModelServer Status:\")\n",
    "print(f\"   Replicas: {wafer_model_server.status.ready_replicas}/{wafer_model_server.status.replicas}\")\n",
    "print(f\"   Endpoint: {wafer_model_server.status.endpoint}\")\n",
    "print(f\"   Requests/sec: {wafer_model_server.status.requests_per_second}\")\n",
    "print(f\"   Avg Latency: {wafer_model_server.status.avg_latency_ms} ms\")\n",
    "print(f\"   Error Rate: {wafer_model_server.status.error_rate * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nðŸ’¡ What Operator Does:\")\n",
    "print(\"   1. Creates Deployment with 3 replicas\")\n",
    "print(\"   2. Creates Service for load balancing\")\n",
    "print(\"   3. Creates HPA (HorizontalPodAutoscaler) for auto-scaling\")\n",
    "print(\"   4. Monitors metrics (requests/sec, latency, errors)\")\n",
    "print(\"   5. Auto-scales 2-8 replicas based on CPU utilization\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE 2: HyperparameterTuning CRD - Automated HPO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "hpo_job = HyperparameterTuningCRD(\n",
    "    metadata={\"name\": \"wafer-yield-hpo-v1\"},\n",
    "    spec=HyperparameterTuningSpec(\n",
    "        model_name=\"wafer_yield_neural_net\",\n",
    "        algorithm=\"bayesian\",\n",
    "        max_trials=30,\n",
    "        max_parallel_trials=4,\n",
    "        objective_metric=\"f1_score\",\n",
    "        objective_type=\"maximize\",\n",
    "        parameters={\n",
    "            \"learning_rate\": {\"min\": 0.0001, \"max\": 0.01, \"type\": \"double\"},\n",
    "            \"hidden_units\": {\"min\": 64, \"max\": 512, \"type\": \"int\"},\n",
    "            \"dropout\": {\"min\": 0.1, \"max\": 0.5, \"type\": \"double\"},\n",
    "            \"batch_size\": {\"values\": [32, 64, 128], \"type\": \"categorical\"}\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"ðŸ“„ HyperparameterTuning CRD:\")\n",
    "print(f\"   Name: {hpo_job.metadata['name']}\")\n",
    "print(f\"   Model: {hpo_job.spec.model_name}\")\n",
    "print(f\"   Algorithm: {hpo_job.spec.algorithm}\")\n",
    "print(f\"   Max Trials: {hpo_job.spec.max_trials}\")\n",
    "print(f\"   Parallel Trials: {hpo_job.spec.max_parallel_trials}\")\n",
    "print(f\"   Objective: {hpo_job.spec.objective_type} {hpo_job.spec.objective_metric}\")\n",
    "\n",
    "print(f\"\\nðŸ”¬ Hyperparameter Search Space:\")\n",
    "for param, config in hpo_job.spec.parameters.items():\n",
    "    if config[\"type\"] == \"categorical\":\n",
    "        print(f\"   â€¢ {param}: {config['values']}\")\n",
    "    else:\n",
    "        print(f\"   â€¢ {param}: [{config['min']}, {config['max']}] ({config['type']})\")\n",
    "\n",
    "# Simulate running trials\n",
    "print(f\"\\nðŸ”„ Simulating Bayesian Optimization...\")\n",
    "\n",
    "trials = []\n",
    "for i in range(10):\n",
    "    trial = Trial(\n",
    "        trial_id=f\"trial-{i+1:03d}\",\n",
    "        parameters={\n",
    "            \"learning_rate\": np.random.uniform(0.0001, 0.01),\n",
    "            \"hidden_units\": int(np.random.uniform(64, 512)),\n",
    "            \"dropout\": np.random.uniform(0.1, 0.5),\n",
    "            \"batch_size\": np.random.choice([32, 64, 128])\n",
    "        },\n",
    "        status=\"Succeeded\",\n",
    "        objective_value=np.random.uniform(0.85, 0.97)\n",
    "    )\n",
    "    trials.append(trial)\n",
    "    hpo_job.status.trials_completed += 1\n",
    "\n",
    "# Find best trial\n",
    "best_trial = max(trials, key=lambda t: t.objective_value)\n",
    "hpo_job.status.best_trial = best_trial\n",
    "\n",
    "print(f\"\\nðŸ“Š HPO Results (10/{hpo_job.spec.max_trials} trials completed):\")\n",
    "print(f\"   Trials Completed: {hpo_job.status.trials_completed}\")\n",
    "print(f\"   Trials Failed: {hpo_job.status.trials_failed}\")\n",
    "\n",
    "print(f\"\\nðŸ† Best Trial: {best_trial.trial_id}\")\n",
    "print(f\"   F1 Score: {best_trial.objective_value:.4f}\")\n",
    "print(f\"   Parameters:\")\n",
    "print(f\"     â€¢ learning_rate: {best_trial.parameters['learning_rate']:.5f}\")\n",
    "print(f\"     â€¢ hidden_units: {best_trial.parameters['hidden_units']}\")\n",
    "print(f\"     â€¢ dropout: {best_trial.parameters['dropout']:.3f}\")\n",
    "print(f\"     â€¢ batch_size: {best_trial.parameters['batch_size']}\")\n",
    "\n",
    "# Visualize trial results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXAMPLE 3: HPO Trial Results Visualization\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: F1 Score vs Learning Rate\n",
    "ax1 = axes[0, 0]\n",
    "lrs = [t.parameters['learning_rate'] for t in trials]\n",
    "f1s = [t.objective_value for t in trials]\n",
    "scatter1 = ax1.scatter(lrs, f1s, s=150, c=f1s, cmap='viridis', edgecolors='black', linewidths=1.5)\n",
    "ax1.scatter(best_trial.parameters['learning_rate'], best_trial.objective_value, \n",
    "            s=400, c='red', marker='*', edgecolors='black', linewidths=2, label='Best Trial', zorder=5)\n",
    "ax1.set_xlabel(\"Learning Rate\", fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel(\"F1 Score\", fontsize=12, fontweight='bold')\n",
    "ax1.set_title(\"F1 Score vs Learning Rate\", fontsize=14, fontweight='bold', pad=15)\n",
    "ax1.set_xscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=10)\n",
    "plt.colorbar(scatter1, ax=ax1, label='F1 Score')\n",
    "\n",
    "# Plot 2: F1 Score vs Hidden Units\n",
    "ax2 = axes[0, 1]\n",
    "hidden = [t.parameters['hidden_units'] for t in trials]\n",
    "scatter2 = ax2.scatter(hidden, f1s, s=150, c=f1s, cmap='viridis', edgecolors='black', linewidths=1.5)\n",
    "ax2.scatter(best_trial.parameters['hidden_units'], best_trial.objective_value, \n",
    "            s=400, c='red', marker='*', edgecolors='black', linewidths=2, label='Best Trial', zorder=5)\n",
    "ax2.set_xlabel(\"Hidden Units\", fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel(\"F1 Score\", fontsize=12, fontweight='bold')\n",
    "ax2.set_title(\"F1 Score vs Hidden Units\", fontsize=14, fontweight='bold', pad=15)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=10)\n",
    "plt.colorbar(scatter2, ax=ax2, label='F1 Score')\n",
    "\n",
    "# Plot 3: F1 Score vs Dropout\n",
    "ax3 = axes[1, 0]\n",
    "dropouts = [t.parameters['dropout'] for t in trials]\n",
    "scatter3 = ax3.scatter(dropouts, f1s, s=150, c=f1s, cmap='viridis', edgecolors='black', linewidths=1.5)\n",
    "ax3.scatter(best_trial.parameters['dropout'], best_trial.objective_value, \n",
    "            s=400, c='red', marker='*', edgecolors='black', linewidths=2, label='Best Trial', zorder=5)\n",
    "ax3.set_xlabel(\"Dropout Rate\", fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel(\"F1 Score\", fontsize=12, fontweight='bold')\n",
    "ax3.set_title(\"F1 Score vs Dropout Rate\", fontsize=14, fontweight='bold', pad=15)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend(fontsize=10)\n",
    "plt.colorbar(scatter3, ax=ax3, label='F1 Score')\n",
    "\n",
    "# Plot 4: Batch Size Distribution\n",
    "ax4 = axes[1, 1]\n",
    "batch_sizes = [t.parameters['batch_size'] for t in trials]\n",
    "batch_f1s = {}\n",
    "for bs, f1 in zip(batch_sizes, f1s):\n",
    "    if bs not in batch_f1s:\n",
    "        batch_f1s[bs] = []\n",
    "    batch_f1s[bs].append(f1)\n",
    "\n",
    "# Create box plot\n",
    "bp = ax4.boxplot([batch_f1s[bs] for bs in sorted(batch_f1s.keys())], \n",
    "                   labels=[str(bs) for bs in sorted(batch_f1s.keys())],\n",
    "                   patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], ['#4ECDC4', '#FFD93D', '#FF6B6B']):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax4.set_xlabel(\"Batch Size\", fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel(\"F1 Score\", fontsize=12, fontweight='bold')\n",
    "ax4.set_title(\"F1 Score Distribution by Batch Size\", fontsize=14, fontweight='bold', pad=15)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Popular CRDs in ML/Kubernetes Ecosystem:\")\n",
    "print(\"\\nðŸ¤– Kubeflow:\")\n",
    "print(\"   â€¢ TrainingJob (TFJob, PyTorchJob, MXJob) - Distributed training\")\n",
    "print(\"   â€¢ Notebook - Jupyter notebook servers\")\n",
    "print(\"   â€¢ Experiment - ML experiment tracking\")\n",
    "print(\"   â€¢ Pipeline - ML pipeline workflows\")\n",
    "\n",
    "print(\"\\nðŸš€ KServe (InferenceService):\")\n",
    "print(\"   â€¢ Predictor - Model serving with auto-scaling\")\n",
    "print(\"   â€¢ Transformer - Pre/post-processing pipelines\")\n",
    "print(\"   â€¢ Explainer - Model explainability\")\n",
    "\n",
    "print(\"\\nðŸ”¬ Katib:\")\n",
    "print(\"   â€¢ Experiment - Hyperparameter tuning jobs\")\n",
    "print(\"   â€¢ Suggestion - HPO algorithm configuration\")\n",
    "print(\"   â€¢ Trial - Individual training trial\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Custom CRDs for Post-Silicon:\")\n",
    "print(\"   â€¢ STDFParserJob - Parse and analyze STDF files\")\n",
    "print(\"   â€¢ WaferMapAnalysis - Spatial pattern detection\")\n",
    "print(\"   â€¢ YieldPredictor - Yield forecasting models\")\n",
    "print(\"   â€¢ ParametricOutlierDetection - Anomaly detection on test data\")\n",
    "print(\"   â€¢ BinOptimization - Optimal binning strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd956a36",
   "metadata": {},
   "source": [
    "## 6. ðŸš€ Real-World Projects Using Kubernetes Advanced Patterns\n",
    "\n",
    "Build production ML systems with StatefulSets, DaemonSets, Operators, and CRDs:\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 1: Distributed PyTorch Training with StatefulSets** â­â­â­â­\n",
    "**Objective:** Build distributed deep learning system for wafer defect detection using PyTorch DDP (DistributedDataParallel)\n",
    "\n",
    "**Business Value:**  \n",
    "- 5x faster training (8 hours â†’ 1.6 hours for 1M wafer images)\n",
    "- $125K/year savings in compute costs (GPU utilization up from 60% â†’ 92%)\n",
    "- Enables real-time model updates (daily retraining on fresh data)\n",
    "\n",
    "**Success Criteria:**\n",
    "- âœ… Training completes successfully across 8 GPU workers\n",
    "- âœ… Linear scaling efficiency >85% (8 GPUs = 6.8x speedup vs 1 GPU)\n",
    "- âœ… Automatic recovery from worker failures (checkpoint restoration)\n",
    "- âœ… Sub-10 minute recovery time from total cluster failure\n",
    "\n",
    "**Features:**\n",
    "- **StatefulSet with 8 replicas** (worker-0 to worker-7, stable DNS names)\n",
    "- **Master-worker architecture** (worker-0 coordinates via stable DNS)\n",
    "- **Shared PersistentVolume** for checkpoints (NFS or S3)\n",
    "- **Init container** to download dataset shards\n",
    "- **Headless Service** for worker-to-worker communication\n",
    "- **Auto-checkpointing** every 500 steps (survives restarts)\n",
    "\n",
    "**Implementation Hints:**\n",
    "```python\n",
    "# StatefulSet YAML structure:\n",
    "apiVersion: apps/v1\n",
    "kind: StatefulSet\n",
    "metadata:\n",
    "  name: pytorch-distributed-training\n",
    "spec:\n",
    "  serviceName: \"pytorch-workers\"\n",
    "  replicas: 8\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: pytorch-training\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: worker\n",
    "        image: pytorch/pytorch:2.0-gpu\n",
    "        env:\n",
    "        - name: RANK\n",
    "          valueFrom:\n",
    "            fieldRef:\n",
    "              fieldPath: metadata.name  # worker-0, worker-1, ...\n",
    "        - name: MASTER_ADDR\n",
    "          value: \"pytorch-workers-0.pytorch-workers\"  # Stable DNS\n",
    "        - name: WORLD_SIZE\n",
    "          value: \"8\"\n",
    "        volumeMounts:\n",
    "        - name: checkpoint-storage\n",
    "          mountPath: /checkpoints\n",
    "  volumeClaimTemplates:\n",
    "  - metadata:\n",
    "      name: checkpoint-storage\n",
    "    spec:\n",
    "      accessModes: [\"ReadWriteOnce\"]\n",
    "      resources:\n",
    "        requests:\n",
    "          storage: 100Gi\n",
    "```\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Train ResNet-50 on 1M wafer defect images (spatial patterns, scratch detection, particle contamination), deploy model to edge devices for real-time inspection\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 2: GPU Driver Management with DaemonSet** â­â­â­\n",
    "**Objective:** Automate NVIDIA GPU driver installation and monitoring across 50-node GPU cluster\n",
    "\n",
    "**Business Value:**  \n",
    "- $80K/year savings (eliminate manual driver updates, 3 DevOps engineers â†’ 1)\n",
    "- 99.8% GPU uptime (automatic driver recovery vs 96.2% manual)\n",
    "- Zero-downtime driver updates (rolling updates on tainted nodes)\n",
    "\n",
    "**Success Criteria:**\n",
    "- âœ… GPU drivers installed on all GPU nodes automatically\n",
    "- âœ… Non-GPU nodes unaffected (node selector filters)\n",
    "- âœ… New GPU nodes get drivers within 5 minutes of joining cluster\n",
    "- âœ… Driver upgrades complete with zero ML job interruptions\n",
    "\n",
    "**Features:**\n",
    "- **DaemonSet with node selector** (gpu=nvidia)\n",
    "- **Init container** to install NVIDIA driver kernel modules\n",
    "- **Tolerations** for GPU taints (gpu-workload=true:NoSchedule)\n",
    "- **Host mounts** for /dev, /sys, /proc (driver access)\n",
    "- **ConfigMap** for driver version pinning (525.xx)\n",
    "- **Prometheus metrics** for GPU utilization monitoring\n",
    "\n",
    "**Implementation Hints:**\n",
    "```python\n",
    "# DaemonSet YAML structure:\n",
    "apiVersion: apps/v1\n",
    "kind: DaemonSet\n",
    "metadata:\n",
    "  name: nvidia-driver-installer\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      name: nvidia-driver\n",
    "  template:\n",
    "    spec:\n",
    "      nodeSelector:\n",
    "        gpu: nvidia  # Only GPU nodes\n",
    "      tolerations:\n",
    "      - key: gpu-workload\n",
    "        operator: Exists\n",
    "        effect: NoSchedule\n",
    "      initContainers:\n",
    "      - name: driver-installer\n",
    "        image: nvidia/driver:525.105.17-ubuntu22.04\n",
    "        securityContext:\n",
    "          privileged: true\n",
    "        volumeMounts:\n",
    "        - name: dev\n",
    "          mountPath: /dev\n",
    "        - name: nvidia-install-dir\n",
    "          mountPath: /usr/local/nvidia\n",
    "      containers:\n",
    "      - name: nvidia-device-plugin\n",
    "        image: nvidia/k8s-device-plugin:v0.14.0\n",
    "      volumes:\n",
    "      - name: dev\n",
    "        hostPath:\n",
    "          path: /dev\n",
    "      - name: nvidia-install-dir\n",
    "        hostPath:\n",
    "          path: /usr/local/nvidia\n",
    "```\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Ensure all GPU nodes have consistent CUDA 12.1 drivers for ML inference workloads (YOLOv8 wafer defect detection requires specific driver version)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 3: ML Training Operator for Auto-Scaling** â­â­â­â­â­\n",
    "**Objective:** Build Kubernetes Operator that auto-scales training jobs based on queue depth and GPU availability\n",
    "\n",
    "**Business Value:**  \n",
    "- $220K/year savings (reduce idle GPU time from 35% â†’ 8%)\n",
    "- 3x more experiments per day (automatic queue processing)\n",
    "- 40% faster time-to-model (parallel training when GPUs available)\n",
    "\n",
    "**Success Criteria:**\n",
    "- âœ… Operator watches TrainingJob CRDs and reconciles state\n",
    "- âœ… Auto-scales from 0 â†’ 8 workers when queue depth > 5\n",
    "- âœ… Scales down to 0 when no jobs queued (save costs)\n",
    "- âœ… Automatic retry on failure (up to 3 retries with exponential backoff)\n",
    "\n",
    "**Features:**\n",
    "- **Custom Resource Definition** (TrainingJob with spec: model, dataset, epochs)\n",
    "- **Operator controller** with reconciliation loop (every 10 seconds)\n",
    "- **Queue depth monitoring** (scale up when >5 jobs waiting)\n",
    "- **GPU availability checks** (don't schedule if no GPUs free)\n",
    "- **Checkpoint management** (save every N epochs, restore on retry)\n",
    "- **Metrics collection** (accuracy, loss, training time)\n",
    "\n",
    "**Implementation Hints:**\n",
    "```python\n",
    "# Operator reconciliation logic:\n",
    "def reconcile(training_job):\n",
    "    # Get desired state from CRD\n",
    "    desired_workers = training_job.spec.replicas\n",
    "    \n",
    "    # Get actual state from cluster\n",
    "    actual_workers = len(get_pods(training_job.name))\n",
    "    \n",
    "    # Reconcile: create missing workers\n",
    "    if actual_workers < desired_workers:\n",
    "        for i in range(actual_workers, desired_workers):\n",
    "            create_pod(f\"{training_job.name}-worker-{i}\")\n",
    "    \n",
    "    # Reconcile: delete extra workers\n",
    "    elif actual_workers > desired_workers:\n",
    "        for i in range(desired_workers, actual_workers):\n",
    "            delete_pod(f\"{training_job.name}-worker-{i}\")\n",
    "    \n",
    "    # Auto-scale based on queue depth\n",
    "    queue_depth = get_queue_depth()\n",
    "    if queue_depth > 5 and gpu_available():\n",
    "        training_job.spec.replicas = min(8, queue_depth)\n",
    "    elif queue_depth == 0:\n",
    "        training_job.spec.replicas = 0  # Scale to zero\n",
    "```\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Auto-scale STDF parsing jobs based on wafer test data ingestion rate (10 wafers/hour â†’ 2 workers, 100 wafers/hour â†’ 8 workers)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 4: KServe Multi-Model Ensemble Deployment** â­â­â­â­\n",
    "**Objective:** Deploy ensemble of 5 models for wafer yield prediction with canary releases and A/B testing\n",
    "\n",
    "**Business Value:**  \n",
    "- 4.2% accuracy improvement (92.3% â†’ 96.5%, ensemble voting)\n",
    "- $4.2M/year savings (fewer false negatives â†’ less yield loss)\n",
    "- Zero-downtime model updates (canary releases with 5% traffic)\n",
    "\n",
    "**Success Criteria:**\n",
    "- âœ… All 5 models deployed and healthy (Random Forest, XGBoost, LightGBM, CatBoost, Neural Net)\n",
    "- âœ… Ensemble combiner aggregates predictions (weighted voting)\n",
    "- âœ… Canary release completes with <0.5% error rate increase\n",
    "- âœ… Auto-rollback if canary metrics degrade >10%\n",
    "\n",
    "**Features:**\n",
    "- **InferenceService CRD** for each model\n",
    "- **Ensemble combiner** (weighted voting based on validation accuracy)\n",
    "- **Canary deployment** (route 5% traffic to new version)\n",
    "- **Prometheus metrics** (latency, throughput, error rate)\n",
    "- **Auto-scaling** (2-10 replicas based on request rate)\n",
    "- **GPU acceleration** for Neural Net model only\n",
    "\n",
    "**Implementation Hints:**\n",
    "```python\n",
    "# InferenceService CRD for Random Forest:\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: wafer-yield-rf\n",
    "spec:\n",
    "  predictor:\n",
    "    sklearn:\n",
    "      storageUri: s3://ml-models/wafer-yield/rf-v2.pkl\n",
    "      resources:\n",
    "        limits:\n",
    "          cpu: 2\n",
    "          memory: 4Gi\n",
    "  scaleTarget: 3\n",
    "  scaleMetric: rps\n",
    "  canaryTrafficPercent: 5  # Canary release\n",
    "\n",
    "# Ensemble combiner:\n",
    "@dataclass\n",
    "class EnsemblePredictor:\n",
    "    models: List[InferenceService]\n",
    "    weights: List[float]  # Based on validation accuracy\n",
    "    \n",
    "    def predict(self, features):\n",
    "        predictions = [model.predict(features) for model in self.models]\n",
    "        return np.average(predictions, weights=self.weights)\n",
    "```\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Combine 5 models for wafer yield prediction (each model trained on different feature subsets: electrical params, spatial features, test sequence, temperature, lot history)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 5: Kubeflow Pipelines for End-to-End ML** â­â­â­â­â­\n",
    "**Objective:** Build automated ML pipeline from STDF parsing â†’ feature engineering â†’ training â†’ deployment\n",
    "\n",
    "**Business Value:**  \n",
    "- $340K/year savings (reduce data scientist time from 30 hours/week â†’ 8 hours/week)\n",
    "- 5x faster model iteration (2 weeks â†’ 3 days from data to production)\n",
    "- 100% reproducibility (versioned pipelines, no manual steps)\n",
    "\n",
    "**Success Criteria:**\n",
    "- âœ… Pipeline executes all 7 steps automatically (parse STDF â†’ deploy model)\n",
    "- âœ… Each step cached (re-run only changed components)\n",
    "- âœ… Full lineage tracking (model â†’ training data â†’ STDF files)\n",
    "- âœ… One-click rollback to previous pipeline version\n",
    "\n",
    "**Features:**\n",
    "- **Kubeflow Pipeline DAG** (7 steps with dependencies)\n",
    "- **Component caching** (skip unchanged steps)\n",
    "- **Artifact versioning** (MLflow integration)\n",
    "- **Conditional execution** (skip training if accuracy >95%)\n",
    "- **Parallel execution** (5 feature engineering jobs in parallel)\n",
    "- **Notifications** (Slack alerts on failure/success)\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1. **STDF Parser**: Parse 1000 STDF files â†’ Parquet (parallelized, 10 workers)\n",
    "2. **Feature Engineering**: 50 derived features (electrical + spatial)\n",
    "3. **Train/Test Split**: 80/20 stratified split\n",
    "4. **Hyperparameter Tuning**: Bayesian optimization (30 trials)\n",
    "5. **Model Training**: Best hyperparameters, full dataset\n",
    "6. **Model Validation**: Test set evaluation (accuracy, F1, AUC)\n",
    "7. **Model Deployment**: KServe InferenceService (canary release)\n",
    "\n",
    "**Implementation Hints:**\n",
    "```python\n",
    "from kfp import dsl\n",
    "\n",
    "@dsl.pipeline(name=\"Wafer Yield Prediction Pipeline\")\n",
    "def wafer_yield_pipeline(stdf_bucket: str, model_version: str):\n",
    "    # Step 1: Parse STDF files\n",
    "    parse_op = dsl.ContainerOp(\n",
    "        name=\"Parse STDF Files\",\n",
    "        image=\"wafer-ml/stdf-parser:v2\",\n",
    "        arguments=[\"--bucket\", stdf_bucket, \"--output\", \"/data/parsed\"]\n",
    "    )\n",
    "    \n",
    "    # Step 2: Feature engineering (depends on parse_op)\n",
    "    feature_op = dsl.ContainerOp(\n",
    "        name=\"Feature Engineering\",\n",
    "        image=\"wafer-ml/feature-eng:v1\",\n",
    "        arguments=[\"--input\", parse_op.outputs[\"data_path\"]]\n",
    "    )\n",
    "    \n",
    "    # Step 3-7: Training, validation, deployment\n",
    "    # ...\n",
    "    \n",
    "    # Conditional: only deploy if validation accuracy > 95%\n",
    "    with dsl.Condition(validation_op.outputs[\"accuracy\"] > 0.95):\n",
    "        deploy_op = dsl.ContainerOp(...)\n",
    "```\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Automate weekly model retraining pipeline (Friday night: parse week's STDF data â†’ retrain â†’ deploy by Monday morning)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 6: Custom GPU Scheduler for Cost Optimization** â­â­â­â­\n",
    "**Objective:** Build custom Kubernetes scheduler that packs ML jobs efficiently on GPU nodes (bin packing)\n",
    "\n",
    "**Business Value:**  \n",
    "- $180K/year savings (reduce GPU nodes from 50 â†’ 38 via better packing)\n",
    "- 25% better GPU utilization (76% â†’ 95% average)\n",
    "- 3x more jobs per GPU (time-slicing for small inference jobs)\n",
    "\n",
    "**Success Criteria:**\n",
    "- âœ… Scheduler achieves >90% GPU utilization across cluster\n",
    "- âœ… Bin packing reduces wasted GPU memory by 40%\n",
    "- âœ… Latency-sensitive jobs get priority (p99 latency <50ms)\n",
    "- âœ… Training jobs preemptible by inference jobs (cost optimization)\n",
    "\n",
    "**Features:**\n",
    "- **Custom scheduler** (implements Kubernetes scheduler interface)\n",
    "- **Bin packing algorithm** (first-fit-decreasing by GPU memory)\n",
    "- **Priority classes** (inference > training)\n",
    "- **GPU time-slicing** (MIG for small inference jobs)\n",
    "- **Anti-affinity** (spread replicas across nodes for HA)\n",
    "- **Cost-aware scheduling** (prefer spot instances for training)\n",
    "\n",
    "**Implementation Hints:**\n",
    "```python\n",
    "class GPUScheduler:\n",
    "    def filter_nodes(self, pod, nodes):\n",
    "        \"\"\"Filter nodes that can run this pod.\"\"\"\n",
    "        viable = []\n",
    "        for node in nodes:\n",
    "            # Check GPU availability\n",
    "            if pod.gpu_required > node.gpu_available:\n",
    "                continue\n",
    "            \n",
    "            # Check GPU memory\n",
    "            if pod.gpu_memory_required > node.gpu_memory_available:\n",
    "                continue\n",
    "            \n",
    "            viable.append(node)\n",
    "        \n",
    "        return viable\n",
    "    \n",
    "    def score_nodes(self, pod, nodes):\n",
    "        \"\"\"Score nodes (higher = better).\"\"\"\n",
    "        scores = {}\n",
    "        for node in nodes:\n",
    "            # Bin packing: prefer fuller nodes (reduce fragmentation)\n",
    "            utilization = node.gpu_used / node.gpu_total\n",
    "            scores[node] = utilization * 100\n",
    "            \n",
    "            # Bonus: spot instances for preemptible jobs\n",
    "            if pod.preemptible and node.is_spot:\n",
    "                scores[node] += 50\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def bind_pod(self, pod, node):\n",
    "        \"\"\"Bind pod to selected node.\"\"\"\n",
    "        node.gpu_available -= pod.gpu_required\n",
    "        node.gpu_memory_available -= pod.gpu_memory_required\n",
    "```\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Pack 20 small STDF parser jobs (0.5 GPU each) + 5 large training jobs (4 GPU each) on 12 GPU nodes (A100 80GB, 8 GPUs/node)\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 7: Multi-Tenant ML Platform with Namespace Isolation** â­â­â­â­\n",
    "**Objective:** Build shared ML platform for 5 engineering teams with resource quotas and network isolation\n",
    "\n",
    "**Business Value:**  \n",
    "- $420K/year savings (shared infrastructure vs per-team clusters)\n",
    "- 60% better resource utilization (team-1 borrows GPUs from idle team-2)\n",
    "- 99.9% tenant isolation (no cross-team data leakage)\n",
    "\n",
    "**Success Criteria:**\n",
    "- âœ… Each team has dedicated namespace with ResourceQuota\n",
    "- âœ… Network policies prevent cross-team traffic\n",
    "- âœ… Fair scheduling (no team monopolizes GPUs)\n",
    "- âœ… Chargebacks based on actual usage (GPU-hours, storage GB)\n",
    "\n",
    "**Features:**\n",
    "- **Namespace per team** (team-design, team-test, team-validation, team-analytics, team-packaging)\n",
    "- **ResourceQuota** (max 10 GPUs, 500GB RAM, 2TB storage per team)\n",
    "- **LimitRange** (min/max resources per pod)\n",
    "- **NetworkPolicy** (deny all cross-namespace traffic except API server)\n",
    "- **PodSecurityPolicy** (prevent privileged containers)\n",
    "- **Chargeback tracking** (Prometheus metrics â†’ cost allocation)\n",
    "\n",
    "**Implementation Hints:**\n",
    "```yaml\n",
    "# Namespace with ResourceQuota:\n",
    "apiVersion: v1\n",
    "kind: Namespace\n",
    "metadata:\n",
    "  name: team-design\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ResourceQuota\n",
    "metadata:\n",
    "  name: team-design-quota\n",
    "  namespace: team-design\n",
    "spec:\n",
    "  hard:\n",
    "    requests.nvidia.com/gpu: \"10\"  # Max 10 GPUs\n",
    "    requests.memory: \"500Gi\"\n",
    "    persistentvolumeclaims: \"20\"\n",
    "    requests.storage: \"2Ti\"\n",
    "---\n",
    "# NetworkPolicy: deny all ingress except from same namespace\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: NetworkPolicy\n",
    "metadata:\n",
    "  name: deny-cross-namespace\n",
    "  namespace: team-design\n",
    "spec:\n",
    "  podSelector: {}\n",
    "  policyTypes:\n",
    "  - Ingress\n",
    "  ingress:\n",
    "  - from:\n",
    "    - podSelector: {}  # Only same namespace\n",
    "```\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Design team (10 users), Test team (15 users), Validation team (8 users), Analytics team (5 users), Packaging team (3 users) share 50-node GPU cluster\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 8: Production Monitoring with Custom Operator** â­â­â­â­â­\n",
    "**Objective:** Build operator that auto-deploys Prometheus, Grafana, and Alertmanager for each ML application\n",
    "\n",
    "**Business Value:**  \n",
    "- $95K/year savings (eliminate manual monitoring setup, 2 SRE engineers â†’ 0.5)\n",
    "- 99.5% model uptime (auto-alerts on degradation vs 94% manual)\n",
    "- 15-minute MTTR (mean time to recovery, down from 4 hours)\n",
    "\n",
    "**Success Criteria:**\n",
    "- âœ… Operator deploys full monitoring stack in <5 minutes\n",
    "- âœ… Auto-configured dashboards for model metrics (latency, throughput, accuracy)\n",
    "- âœ… Alerts fire within 60 seconds of anomaly detection\n",
    "- âœ… Self-healing: operator recreates failed Prometheus pods\n",
    "\n",
    "**Features:**\n",
    "- **MonitoringStack CRD** (defines Prometheus, Grafana, Alertmanager config)\n",
    "- **Operator** watches MonitoringStack and reconciles state\n",
    "- **Service monitors** (auto-discover model endpoints)\n",
    "- **Alerting rules** (p99 latency >100ms, error rate >1%, accuracy drop >5%)\n",
    "- **Grafana dashboards** (auto-generated from CRD)\n",
    "- **PersistentVolume** for metrics retention (30 days)\n",
    "\n",
    "**Implementation Hints:**\n",
    "```python\n",
    "@dataclass\n",
    "class MonitoringStackCRD:\n",
    "    api_version: str = \"monitoring.ml/v1\"\n",
    "    kind: str = \"MonitoringStack\"\n",
    "    spec: Dict = field(default_factory=dict)\n",
    "\n",
    "class MonitoringOperator:\n",
    "    def reconcile(self, stack: MonitoringStackCRD):\n",
    "        # Deploy Prometheus\n",
    "        if not self.prometheus_exists(stack.name):\n",
    "            self.create_prometheus(stack)\n",
    "        \n",
    "        # Deploy Grafana\n",
    "        if not self.grafana_exists(stack.name):\n",
    "            self.create_grafana(stack)\n",
    "        \n",
    "        # Configure ServiceMonitors\n",
    "        for model in stack.spec[\"models\"]:\n",
    "            self.create_service_monitor(model)\n",
    "        \n",
    "        # Configure AlertingRules\n",
    "        for rule in stack.spec[\"alert_rules\"]:\n",
    "            self.create_alerting_rule(rule)\n",
    "```\n",
    "\n",
    "**Post-Silicon Application:**  \n",
    "Monitor 25 deployed ML models (wafer yield, test time, parametric outliers, bin optimization, defect detection) with unified observability\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ **Project Selection Guide**\n",
    "\n",
    "**Choose Project 1-2** if learning Kubernetes basics (StatefulSets, DaemonSets)  \n",
    "**Choose Project 3-5** if building production ML pipelines (Operators, Kubeflow)  \n",
    "**Choose Project 6-8** if optimizing infrastructure (scheduling, multi-tenancy, monitoring)\n",
    "\n",
    "**All projects include:**\n",
    "- Complete implementation templates (YAML + Python code)\n",
    "- Post-silicon validation applications\n",
    "- Business value quantification ($ savings, % improvement)\n",
    "- Success criteria (measurable objectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b66fe",
   "metadata": {},
   "source": [
    "## 7. ðŸ“š Comprehensive Takeaways - Kubernetes Advanced Patterns\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ **Core Concepts Summary**\n",
    "\n",
    "#### **StatefulSets**\n",
    "- **Purpose**: Provide stable, unique identities for pods (predictable names, stable DNS, persistent storage)\n",
    "- **When to Use**: Databases (MySQL, PostgreSQL, MongoDB), distributed training (PyTorch DDP, Horovod), caches (Redis cluster), consensus systems (etcd, ZooKeeper)\n",
    "- **Key Features**: Ordered creation/deletion (0â†’1â†’2, 2â†’1â†’0), stable network IDs (pod-0.service.ns.svc.cluster.local), PersistentVolumeClaim per pod\n",
    "- **Anti-Pattern**: Using StatefulSets for stateless applications (use Deployments instead)\n",
    "\n",
    "#### **DaemonSets**\n",
    "- **Purpose**: Run one pod per node (or matching nodes) for cluster-wide services\n",
    "- **When to Use**: GPU drivers, monitoring agents (Prometheus node-exporter), logging (Fluentd, Filebeat), networking (Calico, Cilium), security (Falco)\n",
    "- **Key Features**: Auto-scheduling on new nodes, node selectors (gpu=nvidia), tolerations (run on tainted nodes)\n",
    "- **Anti-Pattern**: Using DaemonSets for application workloads (use Deployments with pod anti-affinity)\n",
    "\n",
    "#### **Operators**\n",
    "- **Purpose**: Automate complex application lifecycle management (encode operational knowledge as code)\n",
    "- **When to Use**: ML training automation, database backups, certificate management, custom scaling logic\n",
    "- **Key Features**: Watch CRDs, reconciliation loop (ensure desired state = actual state), self-healing, domain-specific operations\n",
    "- **Anti-Pattern**: Using operators for simple tasks (shell scripts or CronJobs suffice)\n",
    "\n",
    "#### **Custom Resource Definitions (CRDs)**\n",
    "- **Purpose**: Extend Kubernetes API with custom resource types (TrainingJob, ModelServer, Experiment)\n",
    "- **When to Use**: ML platforms (Kubeflow, KServe), CI/CD (Tekton, Argo), databases (CockroachDB, Vitess), custom controllers\n",
    "- **Key Features**: Schema validation, versioning, defaulting, conversion webhooks, CRUD operations via kubectl\n",
    "- **Anti-Pattern**: Creating CRDs for one-off tasks (use ConfigMaps or Jobs)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ—ï¸ **Architecture Best Practices**\n",
    "\n",
    "#### **1. StatefulSet Design Patterns**\n",
    "\n",
    "**Master-Worker Architecture:**\n",
    "```yaml\n",
    "# worker-0 is master (coordinator)\n",
    "# worker-1, worker-2, ... are workers\n",
    "env:\n",
    "- name: RANK\n",
    "  value: \"0\"  # From pod ordinal\n",
    "- name: MASTER_ADDR\n",
    "  value: \"training-workers-0.training-workers\"  # Stable DNS\n",
    "```\n",
    "\n",
    "**Storage Management:**\n",
    "- Use PersistentVolumeClaims for stateful data (survives pod restarts)\n",
    "- Use emptyDir for temporary data (deleted on pod termination)\n",
    "- Use NFS/S3 for shared data (all pods access same files)\n",
    "\n",
    "**Headless Services:**\n",
    "```yaml\n",
    "# Required for StatefulSets to provide stable DNS\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: training-workers\n",
    "spec:\n",
    "  clusterIP: None  # Headless\n",
    "  selector:\n",
    "    app: training\n",
    "```\n",
    "\n",
    "#### **2. DaemonSet Design Patterns**\n",
    "\n",
    "**Node Affinity vs Node Selector:**\n",
    "- **Node Selector**: Simple label matching (`gpu: nvidia`)\n",
    "- **Node Affinity**: Complex rules (`requiredDuringScheduling`, `preferredDuringScheduling`)\n",
    "\n",
    "**Tolerations:**\n",
    "```yaml\n",
    "# Run on tainted nodes\n",
    "tolerations:\n",
    "- key: gpu-workload\n",
    "  operator: Exists\n",
    "  effect: NoSchedule\n",
    "```\n",
    "\n",
    "**Update Strategies:**\n",
    "- **RollingUpdate** (default): Update one pod at a time (zero downtime)\n",
    "- **OnDelete**: Manual update (delete pod to trigger update)\n",
    "\n",
    "#### **3. Operator Design Patterns**\n",
    "\n",
    "**Reconciliation Loop:**\n",
    "```python\n",
    "while True:\n",
    "    for resource in watch_resources():\n",
    "        desired_state = resource.spec\n",
    "        actual_state = get_actual_state(resource)\n",
    "        \n",
    "        if desired_state != actual_state:\n",
    "            reconcile(resource, desired_state, actual_state)\n",
    "    \n",
    "    time.sleep(reconcile_interval)\n",
    "```\n",
    "\n",
    "**Idempotency:**\n",
    "- Reconciliation must be idempotent (calling multiple times = same result)\n",
    "- Check if resource exists before creating\n",
    "- Use status subresource to track state\n",
    "\n",
    "**Error Handling:**\n",
    "- Exponential backoff for retries (1s, 2s, 4s, 8s, ...)\n",
    "- Max retries limit (3-5 retries)\n",
    "- Update resource status with error message\n",
    "\n",
    "#### **4. CRD Design Patterns**\n",
    "\n",
    "**Spec vs Status:**\n",
    "- **Spec**: User-defined desired state (immutable after creation)\n",
    "- **Status**: System-managed actual state (updated by controller)\n",
    "\n",
    "**Versioning:**\n",
    "```yaml\n",
    "apiVersion: ml.kubeflow.org/v1beta1  # Version in API group\n",
    "kind: TrainingJob\n",
    "spec:\n",
    "  # v1beta1 fields\n",
    "status:\n",
    "  # Managed by operator\n",
    "```\n",
    "\n",
    "**Validation:**\n",
    "```yaml\n",
    "# OpenAPI schema validation\n",
    "validation:\n",
    "  openAPIV3Schema:\n",
    "    properties:\n",
    "      spec:\n",
    "        properties:\n",
    "          replicas:\n",
    "            type: integer\n",
    "            minimum: 1\n",
    "            maximum: 100\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### âš¡ **Performance Optimization**\n",
    "\n",
    "#### **1. StatefulSet Scaling**\n",
    "\n",
    "**Parallel Scaling (Kubernetes 1.26+):**\n",
    "```yaml\n",
    "# Scale multiple pods simultaneously\n",
    "spec:\n",
    "  podManagementPolicy: Parallel  # Default: OrderedReady\n",
    "```\n",
    "\n",
    "**Performance Impact:**\n",
    "- OrderedReady: Sequential (0â†’1â†’2, slower but safer)\n",
    "- Parallel: All pods at once (faster but may cause resource contention)\n",
    "\n",
    "#### **2. DaemonSet Resource Limits**\n",
    "\n",
    "**Prevent Node Overload:**\n",
    "```yaml\n",
    "resources:\n",
    "  requests:\n",
    "    cpu: 100m\n",
    "    memory: 200Mi\n",
    "  limits:\n",
    "    cpu: 200m\n",
    "    memory: 500Mi\n",
    "```\n",
    "\n",
    "**Priority Classes:**\n",
    "```yaml\n",
    "# Prevent DaemonSet eviction\n",
    "priorityClassName: system-node-critical  # Highest priority\n",
    "```\n",
    "\n",
    "#### **3. Operator Efficiency**\n",
    "\n",
    "**Watch vs Polling:**\n",
    "- Use **Watch API** (event-driven, efficient)\n",
    "- Avoid **Polling** (wasteful, high API server load)\n",
    "\n",
    "**Leader Election:**\n",
    "```python\n",
    "# Only one operator replica reconciles (avoid conflicts)\n",
    "from kubernetes import client, config\n",
    "\n",
    "lock = client.V1Lease(...)\n",
    "if acquire_lock(lock):\n",
    "    run_reconciliation_loop()\n",
    "```\n",
    "\n",
    "**Batch Reconciliation:**\n",
    "```python\n",
    "# Process multiple resources in one reconciliation\n",
    "pending_jobs = [job for job in jobs if job.status.phase == \"Pending\"]\n",
    "for job in pending_jobs[:10]:  # Batch of 10\n",
    "    reconcile(job)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”’ **Security Best Practices**\n",
    "\n",
    "#### **1. RBAC for Operators**\n",
    "\n",
    "**Principle of Least Privilege:**\n",
    "```yaml\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: Role\n",
    "metadata:\n",
    "  name: training-operator\n",
    "rules:\n",
    "- apiGroups: [\"ml.kubeflow.org\"]\n",
    "  resources: [\"trainingjobs\"]\n",
    "  verbs: [\"get\", \"list\", \"watch\", \"update\"]\n",
    "- apiGroups: [\"\"]\n",
    "  resources: [\"pods\"]\n",
    "  verbs: [\"create\", \"delete\", \"get\", \"list\"]\n",
    "```\n",
    "\n",
    "**Avoid Cluster-Admin:**\n",
    "- Never grant `cluster-admin` to operators\n",
    "- Use `Role` (namespace-scoped) or `ClusterRole` (cluster-wide) with minimal permissions\n",
    "\n",
    "#### **2. Pod Security Policies**\n",
    "\n",
    "**StatefulSet Security:**\n",
    "```yaml\n",
    "securityContext:\n",
    "  runAsNonRoot: true\n",
    "  runAsUser: 1000\n",
    "  fsGroup: 2000\n",
    "  capabilities:\n",
    "    drop:\n",
    "    - ALL\n",
    "```\n",
    "\n",
    "**DaemonSet Privileged Containers:**\n",
    "- Only use `privileged: true` for GPU drivers, networking (absolutely necessary)\n",
    "- Use securityContext to drop unnecessary capabilities\n",
    "\n",
    "#### **3. Network Policies**\n",
    "\n",
    "**Isolate StatefulSets:**\n",
    "```yaml\n",
    "# Only allow traffic from same StatefulSet\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: NetworkPolicy\n",
    "metadata:\n",
    "  name: allow-same-statefulset\n",
    "spec:\n",
    "  podSelector:\n",
    "    matchLabels:\n",
    "      app: redis-cluster\n",
    "  ingress:\n",
    "  - from:\n",
    "    - podSelector:\n",
    "        matchLabels:\n",
    "          app: redis-cluster\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ› **Troubleshooting Guide**\n",
    "\n",
    "#### **StatefulSet Issues**\n",
    "\n",
    "**Problem: Pods stuck in Pending**\n",
    "- **Cause**: PersistentVolumeClaim not bound\n",
    "- **Fix**: Check PV availability (`kubectl get pv`), verify StorageClass exists\n",
    "\n",
    "**Problem: Pods restarting frequently**\n",
    "- **Cause**: Liveness probe failing\n",
    "- **Fix**: Increase `initialDelaySeconds`, check application logs\n",
    "\n",
    "**Problem: Scaling stuck (pods not created)**\n",
    "- **Cause**: Pod disruption budget blocking\n",
    "- **Fix**: Update PodDisruptionBudget or use `kubectl delete pdb`\n",
    "\n",
    "#### **DaemonSet Issues**\n",
    "\n",
    "**Problem: DaemonSet pods not scheduled on all nodes**\n",
    "- **Cause**: Node selector not matching, taints not tolerated\n",
    "- **Fix**: Verify node labels (`kubectl get nodes --show-labels`), add tolerations\n",
    "\n",
    "**Problem: DaemonSet update stuck**\n",
    "- **Cause**: maxUnavailable too conservative\n",
    "- **Fix**: Increase `maxUnavailable` in RollingUpdate strategy\n",
    "\n",
    "**Problem: Init container failing**\n",
    "- **Cause**: Missing host mount, insufficient permissions\n",
    "- **Fix**: Check `securityContext`, verify hostPath volumes\n",
    "\n",
    "#### **Operator Issues**\n",
    "\n",
    "**Problem: Operator not reconciling**\n",
    "- **Cause**: RBAC permissions missing, leader election conflict\n",
    "- **Fix**: Check ServiceAccount permissions, verify only one leader\n",
    "\n",
    "**Problem: Infinite reconciliation loop**\n",
    "- **Cause**: Status updates triggering new reconciliations\n",
    "- **Fix**: Use `metadata.generation` to detect spec changes only\n",
    "\n",
    "**Problem: CRD not found**\n",
    "- **Cause**: CRD not installed or wrong API version\n",
    "- **Fix**: Install CRD (`kubectl apply -f crd.yaml`), verify version\n",
    "\n",
    "#### **CRD Issues**\n",
    "\n",
    "**Problem: Validation errors on create**\n",
    "- **Cause**: Schema validation failing\n",
    "- **Fix**: Check CRD schema, ensure required fields present\n",
    "\n",
    "**Problem: CRD version conversion failing**\n",
    "- **Cause**: Conversion webhook not configured\n",
    "- **Fix**: Deploy conversion webhook, update CRD with webhook config\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š **Monitoring and Observability**\n",
    "\n",
    "#### **1. StatefulSet Metrics**\n",
    "\n",
    "**Key Metrics:**\n",
    "- `kube_statefulset_status_replicas` (desired replicas)\n",
    "- `kube_statefulset_status_replicas_ready` (ready replicas)\n",
    "- `kube_statefulset_status_replicas_current` (current version replicas)\n",
    "- `kube_statefulset_status_replicas_updated` (updated replicas)\n",
    "\n",
    "**Alerts:**\n",
    "```yaml\n",
    "# StatefulSet replicas not ready\n",
    "- alert: StatefulSetNotReady\n",
    "  expr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas\n",
    "  for: 5m\n",
    "```\n",
    "\n",
    "#### **2. DaemonSet Metrics**\n",
    "\n",
    "**Key Metrics:**\n",
    "- `kube_daemonset_status_desired_number_scheduled` (should be = number of nodes)\n",
    "- `kube_daemonset_status_number_ready` (ready pods)\n",
    "- `kube_daemonset_status_number_misscheduled` (shouldn't run but running)\n",
    "\n",
    "**Alerts:**\n",
    "```yaml\n",
    "# DaemonSet not deployed on all nodes\n",
    "- alert: DaemonSetNotFullyDeployed\n",
    "  expr: kube_daemonset_status_number_ready < kube_daemonset_status_desired_number_scheduled\n",
    "  for: 10m\n",
    "```\n",
    "\n",
    "#### **3. Operator Metrics**\n",
    "\n",
    "**Custom Metrics:**\n",
    "```python\n",
    "from prometheus_client import Counter, Gauge\n",
    "\n",
    "reconciliation_total = Counter('operator_reconciliations_total', 'Total reconciliations')\n",
    "reconciliation_errors = Counter('operator_reconciliation_errors_total', 'Failed reconciliations')\n",
    "resources_managed = Gauge('operator_resources_managed', 'Number of resources managed')\n",
    "\n",
    "def reconcile(resource):\n",
    "    reconciliation_total.inc()\n",
    "    try:\n",
    "        # Reconciliation logic\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        reconciliation_errors.inc()\n",
    "```\n",
    "\n",
    "**Alerts:**\n",
    "```yaml\n",
    "# High reconciliation error rate\n",
    "- alert: OperatorHighErrorRate\n",
    "  expr: rate(operator_reconciliation_errors_total[5m]) > 0.1\n",
    "  for: 5m\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ **Production Deployment Checklist**\n",
    "\n",
    "#### **Pre-Deployment**\n",
    "\n",
    "- [ ] **Resource requests/limits set** (prevent node overload)\n",
    "- [ ] **Health checks configured** (liveness, readiness, startup probes)\n",
    "- [ ] **RBAC configured** (ServiceAccount, Role, RoleBinding)\n",
    "- [ ] **Network policies defined** (restrict traffic)\n",
    "- [ ] **PodDisruptionBudget created** (prevent downtime during node maintenance)\n",
    "- [ ] **Monitoring configured** (Prometheus metrics, Grafana dashboards)\n",
    "- [ ] **Alerts configured** (PagerDuty, Slack integration)\n",
    "- [ ] **Backup strategy defined** (for StatefulSets with persistent data)\n",
    "\n",
    "#### **StatefulSet Specific**\n",
    "\n",
    "- [ ] **PersistentVolumeClaims configured** (with sufficient storage)\n",
    "- [ ] **Headless Service created** (required for stable DNS)\n",
    "- [ ] **Update strategy defined** (RollingUpdate with partition for canary)\n",
    "- [ ] **Pod management policy set** (OrderedReady vs Parallel)\n",
    "- [ ] **Persistent data backup tested** (restore from backup verified)\n",
    "\n",
    "#### **DaemonSet Specific**\n",
    "\n",
    "- [ ] **Node selector configured** (if not all nodes)\n",
    "- [ ] **Tolerations configured** (for tainted nodes)\n",
    "- [ ] **Update strategy defined** (maxUnavailable set appropriately)\n",
    "- [ ] **Priority class set** (prevent eviction)\n",
    "- [ ] **Resource limits set** (prevent node resource exhaustion)\n",
    "\n",
    "#### **Operator Specific**\n",
    "\n",
    "- [ ] **CRD installed** (before deploying operator)\n",
    "- [ ] **Leader election enabled** (for multi-replica operators)\n",
    "- [ ] **Reconciliation interval tuned** (balance responsiveness vs API load)\n",
    "- [ ] **Error handling tested** (retries, exponential backoff)\n",
    "- [ ] **Webhook certificates configured** (if using admission/conversion webhooks)\n",
    "- [ ] **Operator versioning strategy** (for CRD version upgrades)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ“ **Learning Path Next Steps**\n",
    "\n",
    "#### **Beginner â†’ Intermediate**\n",
    "1. âœ… Complete Notebooks 131-133 (Docker, Kubernetes Fundamentals, Advanced Patterns)\n",
    "2. ðŸ“š **Next**: Notebook 134 - Service Mesh (Istio, Linkerd for microservices)\n",
    "3. ðŸ“š Study Kubeflow components (Training Operator, KServe, Katib)\n",
    "4. ðŸ› ï¸ Build Project 1 (Distributed PyTorch Training with StatefulSets)\n",
    "\n",
    "#### **Intermediate â†’ Advanced**\n",
    "1. ðŸ“š Notebook 135 - GitOps (ArgoCD, Flux for declarative deployments)\n",
    "2. ðŸ“š Notebook 136 - CI/CD for ML (Tekton, GitHub Actions, ML pipelines)\n",
    "3. ðŸ› ï¸ Build Project 3 (ML Training Operator for Auto-Scaling)\n",
    "4. ðŸ› ï¸ Build Project 5 (Kubeflow Pipelines End-to-End)\n",
    "\n",
    "#### **Advanced â†’ Expert**\n",
    "1. ðŸ“š Contribute to open-source operators (Kubeflow, KServe)\n",
    "2. ðŸ› ï¸ Build custom CRDs for domain-specific ML workflows\n",
    "3. ðŸ› ï¸ Build Project 6 (Custom GPU Scheduler)\n",
    "4. ðŸ› ï¸ Build Project 8 (Production Monitoring Operator)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“– **Additional Resources**\n",
    "\n",
    "#### **Official Documentation**\n",
    "- [Kubernetes StatefulSets](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/)\n",
    "- [Kubernetes DaemonSets](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/)\n",
    "- [Kubernetes Operators](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/)\n",
    "- [Custom Resource Definitions](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/)\n",
    "\n",
    "#### **Operator Frameworks**\n",
    "- [Kubebuilder](https://book.kubebuilder.io/) - Go-based operator framework\n",
    "- [Operator SDK](https://sdk.operatorframework.io/) - Multi-language operator framework\n",
    "- [Kopf](https://kopf.readthedocs.io/) - Python-based operator framework\n",
    "- [KUDO](https://kudo.dev/) - Declarative operator framework\n",
    "\n",
    "#### **ML Platforms**\n",
    "- [Kubeflow](https://www.kubeflow.org/) - End-to-end ML platform\n",
    "- [KServe](https://kserve.github.io/website/) - Model serving (successor to KFServing)\n",
    "- [Katib](https://www.kubeflow.org/docs/components/katib/) - Hyperparameter tuning\n",
    "- [Seldon Core](https://www.seldon.io/solutions/open-source-projects/core) - Advanced model serving\n",
    "\n",
    "#### **Books**\n",
    "- \"Programming Kubernetes\" by Michael Hausenblas & Stefan Schimanski\n",
    "- \"Kubernetes Operators\" by Jason Dobies & Joshua Wood\n",
    "- \"Kubernetes Patterns\" by Bilgin Ibryam & Roland HuÃŸ\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ **Key Insights for Post-Silicon Validation**\n",
    "\n",
    "#### **Why Advanced Patterns Matter for Semiconductor Testing**\n",
    "\n",
    "**StatefulSets for Distributed Wafer Analysis:**\n",
    "- Stable pod names enable sharding (wafer-0 processes lot A, wafer-1 processes lot B)\n",
    "- Persistent storage retains intermediate results (survive pod restarts)\n",
    "- Ordered scaling prevents data corruption (complete shard 0 before starting shard 1)\n",
    "\n",
    "**DaemonSets for GPU Driver Management:**\n",
    "- Every GPU node needs NVIDIA driver 525.xx (consistency critical for inference)\n",
    "- Auto-deployment on new nodes (scale from 10 â†’ 50 GPU nodes with zero manual work)\n",
    "- Rolling updates enable zero-downtime driver upgrades\n",
    "\n",
    "**Operators for STDF Parsing Automation:**\n",
    "- Data scientists create `STDFParserJob` CRD (no Kubernetes expertise needed)\n",
    "- Operator auto-scales workers based on queue depth (10 wafers â†’ 2 workers, 100 wafers â†’ 8 workers)\n",
    "- Auto-retry on failure (handle corrupted STDF files gracefully)\n",
    "\n",
    "**CRDs for ML Workflow Standardization:**\n",
    "- `YieldPredictorJob` CRD standardizes wafer yield prediction across teams\n",
    "- `WaferMapAnalysis` CRD encodes spatial analysis best practices\n",
    "- `BinOptimizationJob` CRD automates binning strategy experiments\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ **When to Use Each Pattern**\n",
    "\n",
    "| **Pattern** | **Use Case** | **Example** | **Complexity** |\n",
    "|-------------|--------------|-------------|----------------|\n",
    "| **StatefulSet** | Stable identities, persistent storage, ordered operations | Distributed training, databases, Redis cluster | â­â­â­ |\n",
    "| **DaemonSet** | One pod per node, cluster-wide services | GPU drivers, monitoring, logging | â­â­ |\n",
    "| **Operator** | Complex lifecycle management, domain-specific automation | ML training automation, backup/restore | â­â­â­â­â­ |\n",
    "| **CRD** | Custom resources, API extension | TrainingJob, ModelServer, Experiment | â­â­â­â­ |\n",
    "| **Deployment** | Stateless applications, simple scaling | Model serving (stateless), web apps | â­ |\n",
    "| **Job** | One-time tasks, batch processing | STDF parsing, ETL pipelines | â­ |\n",
    "| **CronJob** | Scheduled tasks | Daily model retraining, backup jobs | â­â­ |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Final Checklist**\n",
    "\n",
    "**You've mastered Kubernetes Advanced Patterns if you can:**\n",
    "\n",
    "- [ ] Explain when to use StatefulSet vs Deployment (and provide 3 examples)\n",
    "- [ ] Design a DaemonSet with node selectors and tolerations\n",
    "- [ ] Build a simple operator with reconciliation loop\n",
    "- [ ] Create a CRD with schema validation and versioning\n",
    "- [ ] Debug StatefulSet scaling issues (PVC not bound, PDB blocking)\n",
    "- [ ] Configure monitoring for operators (Prometheus metrics, alerts)\n",
    "- [ ] Implement distributed training with StatefulSets (PyTorch DDP)\n",
    "- [ ] Deploy GPU drivers with DaemonSets (NVIDIA driver installation)\n",
    "\n",
    "**Ready for Production if you can:**\n",
    "\n",
    "- [ ] Design multi-tenant ML platform with namespace isolation\n",
    "- [ ] Build custom scheduler for GPU bin packing\n",
    "- [ ] Implement canary releases for model serving (KServe)\n",
    "- [ ] Create end-to-end ML pipeline (Kubeflow Pipelines)\n",
    "- [ ] Troubleshoot operator infinite reconciliation loops\n",
    "- [ ] Secure operators with RBAC and network policies\n",
    "- [ ] Monitor cluster health (DaemonSet coverage, StatefulSet readiness)\n",
    "- [ ] Implement auto-scaling based on custom metrics (queue depth, accuracy)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ **Congratulations!**\n",
    "\n",
    "You've completed **Notebook 133: Kubernetes Advanced Patterns for ML**. You now understand:\n",
    "- âœ… StatefulSets for stable identities and persistent storage\n",
    "- âœ… DaemonSets for cluster-wide services\n",
    "- âœ… Operators for automating complex workflows\n",
    "- âœ… CRDs for extending Kubernetes API\n",
    "\n",
    "**Next Steps:**\n",
    "- **Notebook 134**: Service Mesh (Istio, Linkerd) for advanced networking\n",
    "- **Notebook 135**: GitOps (ArgoCD, Flux) for declarative deployments\n",
    "- **Notebook 136**: CI/CD for ML (Tekton, GitHub Actions)\n",
    "\n",
    "**Keep Building! ðŸŽ‰**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ffada7",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "### When to Use Kubernetes Advanced Patterns\n",
    "- **Sidecar pattern**: Add capabilities (logging, monitoring, service mesh) without modifying main container (ML model + Prometheus exporter sidecar)\n",
    "- **Ambassador pattern**: Proxy connections to external services (database connection pooling, circuit breaking)\n",
    "- **Adapter pattern**: Standardize outputs from heterogeneous containers (normalize logs from different ML frameworks)\n",
    "- **Init containers**: Run setup tasks before main container starts (download model artifacts, database migrations)\n",
    "- **StatefulSets**: Deploy stateful applications requiring stable network IDs and persistent storage (feature stores, vector databases)\n",
    "- **DaemonSets**: Run one pod per node for node-level tasks (log collection, GPU monitoring on every inference node)\n",
    "\n",
    "### Limitations\n",
    "- **Complexity overhead**: Advanced patterns add YAML configuration, debugging difficulty vs. simple deployments\n",
    "- **Resource consumption**: Sidecars/adapters consume CPU/memory on every pod (2-5% overhead typical)\n",
    "- **Networking complexity**: Service meshes (Istio/Linkerd) add latency (1-5ms p99) and operational burden\n",
    "- **Learning curve**: Teams need deep Kubernetes knowledge (pod lifecycles, volumes, networking)\n",
    "\n",
    "### Alternatives\n",
    "- **Monolithic containers**: Package all functionality in single container (simpler, but less flexible)\n",
    "- **VM-based deployments**: Traditional VMs for stateful apps (easier state management, higher resource overhead)\n",
    "- **Serverless (Lambda/Cloud Run)**: For stateless inference workloads (no Kubernetes needed, vendor lock-in risk)\n",
    "- **Docker Compose**: Local/dev environments (simpler than K8s, doesn't scale to production)\n",
    "\n",
    "### Best Practices\n",
    "- **Resource limits**: Always set CPU/memory requests and limits to prevent pod evictions\n",
    "- **Health checks**: Implement liveness (restart unhealthy pods) and readiness (traffic routing) probes\n",
    "- **Rolling updates**: Use RollingUpdate strategy with maxUnavailable=1 for zero-downtime deployments\n",
    "- **Pod disruption budgets**: Ensure minimum availability during node maintenance/upgrades\n",
    "- **Network policies**: Restrict pod-to-pod traffic for security (ML inference pods can't access training data stores)\n",
    "- **Horizontal Pod Autoscaling**: Scale based on custom metrics (inference latency p95, GPU utilization) not just CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a247d922",
   "metadata": {},
   "source": [
    "## ðŸ“Š Diagnostic Checks Summary\n",
    "\n",
    "### Implementation Checklist\n",
    "âœ… **Sidecar Pattern**\n",
    "- Logging sidecar: Fluentd/Filebeat collects logs from main container shared volume\n",
    "- Monitoring sidecar: Prometheus exporter scrapes model metrics (latency, throughput, error rate)\n",
    "- Service mesh sidecar: Envoy proxy handles mTLS, retries, circuit breaking\n",
    "\n",
    "âœ… **Ambassador Pattern**\n",
    "- Database proxy: PgBouncer pools connections, reduces connection overhead\n",
    "- Circuit breaker: Hystrix prevents cascading failures to downstream services\n",
    "- Rate limiter: Token bucket limits requests to expensive GPU inference\n",
    "\n",
    "âœ… **Adapter Pattern**\n",
    "- Log normalizer: Convert framework-specific logs (TensorFlow, PyTorch) to standard JSON format\n",
    "- Metrics adapter: Transform model-specific metrics to Prometheus format\n",
    "- API adapter: Convert legacy REST API responses to new GraphQL schema\n",
    "\n",
    "âœ… **Init Containers**\n",
    "- Model artifact downloader: Fetch model weights from S3/GCS before inference pod starts\n",
    "- Database schema migrator: Apply schema updates before app deployment\n",
    "- Config validator: Check ConfigMaps/Secrets before starting main container\n",
    "\n",
    "âœ… **StatefulSets**\n",
    "- Stable network IDs: Pods get predictable names (redis-0, redis-1) for peer discovery\n",
    "- Persistent volumes: Data survives pod restarts (feature store, vector database)\n",
    "- Ordered deployment: Pods created/deleted in sequence (master-slave database setup)\n",
    "\n",
    "âœ… **DaemonSets**\n",
    "- Node monitoring: GPU utilization, temperature tracking on every inference node\n",
    "- Log collection: Fluentd on every node ships logs to centralized Elasticsearch\n",
    "- Network monitoring: Packet capture for debugging distributed training\n",
    "\n",
    "### Quality Metrics\n",
    "- **Pod startup time**: <30s for inference pods (model download + health check)\n",
    "- **Resource overhead**: Sidecars consume <10% CPU, <200MB memory per pod\n",
    "- **Service mesh latency**: p99 <5ms added by Envoy proxy\n",
    "- **StatefulSet availability**: >99.9% uptime for stateful services (Redis, Postgres)\n",
    "\n",
    "### Post-Silicon Validation Applications\n",
    "**1. Sidecar Pattern for ATE Test Data Streaming**\n",
    "- Main container: Test execution engine (ATE controller)\n",
    "- Sidecar: Real-time STDF parser + Kafka producer\n",
    "- Use case: Stream parametric test results to centralized yield database\n",
    "- Business value: Real-time yield dashboards enable immediate excursion response (2-4hr faster root cause)\n",
    "\n",
    "**2. Ambassador Pattern for Test Floor Database Connections**\n",
    "- Main container: Yield prediction service (ML inference)\n",
    "- Ambassador: PgBouncer connection pool to wafer test database\n",
    "- Use case: Reduce database connection overhead from 5000 pods hitting PostgreSQL\n",
    "- Business value: Database cost reduction 40-60% (fewer connections = smaller RDS instance)\n",
    "\n",
    "**3. Init Container for Model Artifact Management**\n",
    "- Init container: Download yield prediction model from S3 (200MB XGBoost model)\n",
    "- Main container: Inference service starts after model loaded\n",
    "- Use case: Ensure latest model deployed before accepting inference requests\n",
    "- Business value: Zero-downtime model updates, rollback in <2min if accuracy drops\n",
    "\n",
    "### Business ROI Estimation\n",
    "\n",
    "**Scenario 1: Medium-Scale Kubernetes Cluster (50 nodes, 500 pods)**\n",
    "- Sidecar logging/monitoring: $1.5M/year observability value (faster debugging)\n",
    "- Ambassador pattern DB pooling: $400K/year reduced database costs\n",
    "- Init containers for artifact mgmt: $800K/year faster deployments (10min â†’ 2min)\n",
    "- **Total ROI: $2.7M/year** (cost: $200K learning + $100K tooling = $2.4M net)\n",
    "\n",
    "**Scenario 2: Large-Scale Production Cluster (200 nodes, 3000 pods)**\n",
    "- Service mesh (Istio): $5M/year improved reliability (circuit breaking, retries)\n",
    "- StatefulSets for feature stores: $3M/year data persistence guarantees\n",
    "- DaemonSets for GPU monitoring: $2M/year reduced GPU failures (proactive thermal management)\n",
    "- **Total ROI: $10M/year** (cost: $1.2M infrastructure + $800K ops = $8M net)\n",
    "\n",
    "**Scenario 3: Multi-Cluster Global Deployment (500+ nodes across 3 regions)**\n",
    "- Advanced patterns across all clusters: $15M/year standardized operations\n",
    "- Cross-cluster service mesh: $8M/year improved cross-region latency (traffic shaping)\n",
    "- Disaster recovery with StatefulSets: $12M/year downtime reduction\n",
    "- **Total ROI: $35M/year** (cost: $5M infrastructure + $3M team = $27M net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9516eba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ Mastery Achievement\n",
    "\n",
    "**You now have production-grade expertise in:**\n",
    "- âœ… Implementing sidecar, ambassador, and adapter patterns for separation of concerns in Kubernetes\n",
    "- âœ… Using init containers for pre-deployment setup tasks (model downloads, schema migrations)\n",
    "- âœ… Deploying StatefulSets with persistent storage for stateful ML applications (feature stores, vector DBs)\n",
    "- âœ… Running DaemonSets for node-level tasks (GPU monitoring, log collection)\n",
    "- âœ… Applying K8s patterns to semiconductor test data streaming, database optimization, and model deployment\n",
    "\n",
    "**Next Steps:**\n",
    "- **Service Mesh Deep Dive**: Istio/Linkerd for advanced traffic management, observability, security\n",
    "- **Custom Resource Definitions (CRDs)**: Extend Kubernetes API for ML-specific resources (TFJob, PyTorchJob)\n",
    "- **Kubernetes Operators**: Automate complex application lifecycle management (database backups, model retraining triggers)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
