{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6279e01",
   "metadata": {},
   "source": [
    "# 132: Kubernetes Fundamentals for ML\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** Kubernetes architecture (control plane, worker nodes, pods, services, deployments)\n",
    "- **Implement** ML model deployments (declarative YAML, replica management, health checks)\n",
    "- **Build** auto-scaling systems (HPA based on CPU, custom metrics, traffic patterns)\n",
    "- **Apply** Kubernetes to post-silicon validation (STDF parser service, wafer analysis API, distributed testing)\n",
    "- **Master** rolling updates (zero-downtime deployments, canary releases, rollback strategies)\n",
    "- **Deploy** production ML services (load balancing, service discovery, resource management)\n",
    "\n",
    "## ðŸ“š What is Kubernetes for ML?\n",
    "\n",
    "**Kubernetes** (K8s) is a container orchestration platform that automates deployment, scaling, and management of containerized applications. For ML, Kubernetes solves critical production challenges:\n",
    "\n",
    "**The Deployment Challenge:**\n",
    "```\n",
    "Without Kubernetes (Manual):\n",
    "- Data scientist builds Docker image: wafer_yield_model:v2\n",
    "- DevOps manually deploys to 10 servers\n",
    "- Load balancer needs manual configuration\n",
    "- Server crashes â†’ manual restart (downtime: 15 minutes)\n",
    "- Need to scale 10 â†’ 20 servers â†’ manual provisioning (2 hours)\n",
    "- Model update â†’ manual rolling update (error-prone, risky)\n",
    "```\n",
    "\n",
    "**Kubernetes Solution:**\n",
    "```yaml\n",
    "# deployment.yaml (declarative)\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: wafer-yield-model\n",
    "spec:\n",
    "  replicas: 10  # Desired state\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: model\n",
    "        image: wafer_yield_model:v2\n",
    "        resources:\n",
    "          limits:\n",
    "            cpu: \"2\"\n",
    "            memory: \"4Gi\"\n",
    "---\n",
    "# service.yaml (stable endpoint)\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: wafer-yield-model\n",
    "spec:\n",
    "  selector:\n",
    "    app: wafer-yield-model\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8080\n",
    "```\n",
    "\n",
    "**What Kubernetes Does Automatically:**\n",
    "- âœ… **Self-healing:** Pod crashes â†’ new pod started automatically (seconds)\n",
    "- âœ… **Auto-scaling:** CPU >70% â†’ scale 10 â†’ 15 pods (minutes)\n",
    "- âœ… **Load balancing:** Service distributes traffic across healthy pods\n",
    "- âœ… **Rolling updates:** Deploy v3 gradually (v2 pods â†’ v3 pods, zero downtime)\n",
    "- âœ… **Service discovery:** Pods find services by DNS (wafer-yield-model.default.svc.cluster.local)\n",
    "- âœ… **Resource management:** Ensures 2 CPU, 4Gi memory per pod (prevents resource starvation)\n",
    "\n",
    "**Why Kubernetes for ML?**\n",
    "- âœ… **Scalability:** Handle 10 â†’ 1000 requests/sec automatically (horizontal pod autoscaling)\n",
    "- âœ… **High Availability:** 99.9% uptime (pod distribution across nodes, auto-restart on failure)\n",
    "- âœ… **Zero-Downtime Deployments:** Rolling updates with health checks (traffic shifts to new version gradually)\n",
    "- âœ… **Resource Efficiency:** Bin packing (fit multiple models on same node, maximize GPU utilization)\n",
    "- âœ… **Multi-Tenancy:** Namespaces isolate teams (team-design, team-test, team-validation)\n",
    "- âœ… **Cloud Agnostic:** Run on AWS (EKS), Azure (AKS), GCP (GKE), on-prem (same manifests)\n",
    "\n",
    "## ðŸ­ Post-Silicon Validation Use Cases\n",
    "\n",
    "**Use Case 1: STDF Parser Service (Auto-Scaling)**\n",
    "- **Input:** Test equipment streams STDF files (10-100 wafers/hour, variable rate)\n",
    "- **Deployment:** Kubernetes Deployment with 5 initial replicas\n",
    "- **Auto-Scaling:** HPA monitors queue depth (>50 files â†’ scale up to 20 replicas)\n",
    "- **Output:** Parsed data in Parquet format (sub-5 second p95 latency)\n",
    "- **Value:** $85K/year savings (eliminate manual server management, auto-scale down during off-peak)\n",
    "\n",
    "**Use Case 2: Wafer Yield Prediction API (High Availability)**\n",
    "- **Input:** Manufacturing execution system (MES) requests yield predictions (200 req/sec peak)\n",
    "- **Deployment:** Deployment with 10 replicas across 3 availability zones\n",
    "- **Service:** LoadBalancer type service (stable IP, SSL termination)\n",
    "- **Health Checks:** Liveness probe (restart unhealthy pods), readiness probe (remove from load balancer)\n",
    "- **Output:** Real-time yield predictions (p99 latency <50ms, 99.95% uptime)\n",
    "- **Value:** $2.1M/year revenue protection (prevent yield loss from undetected issues)\n",
    "\n",
    "**Use Case 3: Distributed Test Execution (Batch Processing)**\n",
    "- **Input:** 500 test jobs queued (regression testing on new silicon)\n",
    "- **Deployment:** Job resource (parallel execution, 20 pods, completion tracking)\n",
    "- **Resource Limits:** Each pod gets 4 CPU, 8Gi memory (prevent resource contention)\n",
    "- **Output:** Test results aggregated (success/failure per test case)\n",
    "- **Value:** 5x faster execution (20 hours â†’ 4 hours, parallel vs sequential)\n",
    "\n",
    "**Use Case 4: Model A/B Testing (Canary Release)**\n",
    "- **Input:** New yield model v2.5 (95% accuracy in validation, needs production verification)\n",
    "- **Deployment:** Canary deployment (5% traffic to v2.5, 95% to v2.4)\n",
    "- **Monitoring:** Compare error rates, latency, prediction accuracy (real-time metrics)\n",
    "- **Rollout Strategy:** If metrics stable, gradually increase to 100% (10%, 25%, 50%, 100%)\n",
    "- **Value:** Zero-risk deployments (auto-rollback if v2.5 degrades metrics >5%)\n",
    "\n",
    "## ðŸ”„ Kubernetes Workflow for ML\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Data Scientist] -->|kubectl apply| B[API Server]\n",
    "    B -->|Store| C[etcd]\n",
    "    B -->|Schedule| D[Scheduler]\n",
    "    D -->|Assign| E[Worker Node 1]\n",
    "    D -->|Assign| F[Worker Node 2]\n",
    "    E -->|Run| G[Pod: model-v2]\n",
    "    F -->|Run| H[Pod: model-v2]\n",
    "    I[Service] -->|Load Balance| G\n",
    "    I -->|Load Balance| H\n",
    "    J[HPA] -->|Monitor| G\n",
    "    J -->|Monitor| H\n",
    "    J -->|Scale| B\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#fff4e1\n",
    "    style C fill:#ffe1e1\n",
    "    style D fill:#e1ffe1\n",
    "    style G fill:#f0e1ff\n",
    "    style H fill:#f0e1ff\n",
    "    style I fill:#ffe1f5\n",
    "    style J fill:#e1ffff\n",
    "```\n",
    "\n",
    "## ðŸ“Š Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **Notebook 131:** Docker for ML (containerization fundamentals, multi-stage builds, model serving)\n",
    "\n",
    "**Current Notebook:**\n",
    "- **Notebook 132:** Kubernetes Fundamentals (architecture, deployments, services, auto-scaling, rolling updates)\n",
    "\n",
    "**Next Steps:**\n",
    "- **Notebook 133:** Kubernetes Advanced Patterns (StatefulSets, DaemonSets, Operators, CRDs)\n",
    "- **Notebook 134:** Service Mesh (Istio, Linkerd for microservices communication)\n",
    "- **Notebook 135:** GitOps (ArgoCD, Flux for declarative deployments)\n",
    "\n",
    "---\n",
    "\n",
    "Let's orchestrate ML models at scale! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional\n",
    "import uuid\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Environment ready for Kubernetes ML orchestration simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f8c9c",
   "metadata": {},
   "source": [
    "## 2. ðŸ—ï¸ Kubernetes Architecture and Core Concepts\n",
    "\n",
    "**Purpose:** Understand Kubernetes cluster components and how they work together to orchestrate containers at scale.\n",
    "\n",
    "**Key Points:**\n",
    "- **Control Plane:** Brain of the cluster (API server, scheduler, controller manager, etcd)\n",
    "- **Worker Nodes:** Run application pods (kubelet, kube-proxy, container runtime)\n",
    "- **Pods:** Smallest deployable unit (1+ containers, shared network/storage)\n",
    "- **Deployments:** Declarative pod management (desired state, rolling updates)\n",
    "- **Services:** Stable networking for pods (load balancing, service discovery)\n",
    "- **Namespaces:** Virtual clusters for multi-tenancy (dev, staging, production)\n",
    "\n",
    "**Why It Matters:**\n",
    "- **Control plane** makes all decisions (schedule pods, detect failures, scale replicas)\n",
    "- **Worker nodes** execute workloads (kubelet ensures pods running, kube-proxy handles networking)\n",
    "- **Pods are ephemeral** (IP changes on restart, need Services for stable endpoints)\n",
    "- **Deployments enable declarative updates** (specify v2.3, Kubernetes handles rollout)\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "STDF parser service deployment: 10 pods across 3 worker nodes, Service provides stable endpoint (`stdf-parser.validation.svc.cluster.local:8080`), test equipment sends files to Service, Kubernetes load balances to healthy pods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098929c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kubernetes Architecture Simulation\n",
    "\n",
    "@dataclass\n",
    "class Pod:\n",
    "    \"\"\"Represents a Kubernetes Pod (smallest deployable unit).\"\"\"\n",
    "    name: str\n",
    "    node: str\n",
    "    status: str = \"Running\"\n",
    "    ip: str = \"\"\n",
    "    containers: List[str] = field(default_factory=list)\n",
    "    cpu_request: float = 0.5  # CPU cores\n",
    "    memory_request: int = 512  # MB\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.ip:\n",
    "            # Simulate pod IP (10.244.x.y format)\n",
    "            self.ip = f\"10.244.{np.random.randint(0, 255)}.{np.random.randint(1, 255)}\"\n",
    "    \n",
    "    def restart(self):\n",
    "        \"\"\"Simulate pod restart (new IP assigned).\"\"\"\n",
    "        self.ip = f\"10.244.{np.random.randint(0, 255)}.{np.random.randint(1, 255)}\"\n",
    "        self.status = \"Running\"\n",
    "        print(f\"ðŸ”„ Pod {self.name} restarted with new IP: {self.ip}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    \"\"\"Represents a Kubernetes Worker Node.\"\"\"\n",
    "    name: str\n",
    "    cpu_capacity: float = 8.0  # CPU cores\n",
    "    memory_capacity: int = 16384  # MB\n",
    "    pods: List[Pod] = field(default_factory=list)\n",
    "    \n",
    "    def available_cpu(self) -> float:\n",
    "        \"\"\"Calculate available CPU.\"\"\"\n",
    "        used = sum(pod.cpu_request for pod in self.pods if pod.status == \"Running\")\n",
    "        return self.cpu_capacity - used\n",
    "    \n",
    "    def available_memory(self) -> int:\n",
    "        \"\"\"Calculate available memory.\"\"\"\n",
    "        used = sum(pod.memory_request for pod in self.pods if pod.status == \"Running\")\n",
    "        return self.memory_capacity - used\n",
    "    \n",
    "    def can_schedule(self, pod: Pod) -> bool:\n",
    "        \"\"\"Check if node has resources for pod.\"\"\"\n",
    "        return (self.available_cpu() >= pod.cpu_request and \n",
    "                self.available_memory() >= pod.memory_request)\n",
    "    \n",
    "    def add_pod(self, pod: Pod) -> bool:\n",
    "        \"\"\"Schedule pod on node.\"\"\"\n",
    "        if self.can_schedule(pod):\n",
    "            pod.node = self.name\n",
    "            self.pods.append(pod)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "class KubernetesCluster:\n",
    "    \"\"\"Simulates a Kubernetes cluster with control plane logic.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.nodes: List[Node] = []\n",
    "        self.services: Dict[str, 'Service'] = {}\n",
    "        \n",
    "    def add_node(self, node: Node):\n",
    "        \"\"\"Add worker node to cluster.\"\"\"\n",
    "        self.nodes.append(node)\n",
    "        print(f\"âž• Node {node.name} added to cluster (CPU: {node.cpu_capacity}, Memory: {node.memory_capacity}MB)\")\n",
    "    \n",
    "    def schedule_pod(self, pod: Pod) -> bool:\n",
    "        \"\"\"Schedule pod on node with available resources (simplified scheduler).\"\"\"\n",
    "        # Sort nodes by available resources (bin packing)\n",
    "        available_nodes = [n for n in self.nodes if n.can_schedule(pod)]\n",
    "        \n",
    "        if not available_nodes:\n",
    "            print(f\"âŒ Cannot schedule pod {pod.name}: insufficient resources\")\n",
    "            return False\n",
    "        \n",
    "        # Choose node with most available resources (spread strategy)\n",
    "        best_node = max(available_nodes, key=lambda n: n.available_cpu())\n",
    "        best_node.add_pod(pod)\n",
    "        print(f\"âœ… Pod {pod.name} scheduled on {best_node.name} (IP: {pod.ip})\")\n",
    "        return True\n",
    "    \n",
    "    def create_deployment(self, name: str, image: str, replicas: int, \n",
    "                         cpu_request: float, memory_request: int) -> List[Pod]:\n",
    "        \"\"\"Create Deployment with specified replicas.\"\"\"\n",
    "        pods = []\n",
    "        for i in range(replicas):\n",
    "            pod = Pod(\n",
    "                name=f\"{name}-{uuid.uuid4().hex[:8]}\",\n",
    "                node=\"\",\n",
    "                containers=[image],\n",
    "                cpu_request=cpu_request,\n",
    "                memory_request=memory_request\n",
    "            )\n",
    "            if self.schedule_pod(pod):\n",
    "                pods.append(pod)\n",
    "        \n",
    "        print(f\"\\nðŸ“¦ Deployment '{name}' created: {len(pods)}/{replicas} pods running\")\n",
    "        return pods\n",
    "    \n",
    "    def get_cluster_status(self) -> Dict:\n",
    "        \"\"\"Get cluster resource utilization.\"\"\"\n",
    "        total_cpu = sum(n.cpu_capacity for n in self.nodes)\n",
    "        used_cpu = sum(sum(p.cpu_request for p in n.pods if p.status == \"Running\") \n",
    "                       for n in self.nodes)\n",
    "        total_memory = sum(n.memory_capacity for n in self.nodes)\n",
    "        used_memory = sum(sum(p.memory_request for p in n.pods if p.status == \"Running\") \n",
    "                         for n in self.nodes)\n",
    "        total_pods = sum(len(n.pods) for n in self.nodes)\n",
    "        \n",
    "        return {\n",
    "            \"total_nodes\": len(self.nodes),\n",
    "            \"total_pods\": total_pods,\n",
    "            \"cpu_utilization\": (used_cpu / total_cpu) * 100 if total_cpu > 0 else 0,\n",
    "            \"memory_utilization\": (used_memory / total_memory) * 100 if total_memory > 0 else 0,\n",
    "            \"total_cpu\": total_cpu,\n",
    "            \"used_cpu\": used_cpu,\n",
    "            \"total_memory\": total_memory,\n",
    "            \"used_memory\": used_memory\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Service:\n",
    "    \"\"\"Represents a Kubernetes Service (stable endpoint for pods).\"\"\"\n",
    "    name: str\n",
    "    service_type: str = \"ClusterIP\"  # ClusterIP, NodePort, LoadBalancer\n",
    "    cluster_ip: str = \"\"\n",
    "    port: int = 8080\n",
    "    target_pods: List[Pod] = field(default_factory=list)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.cluster_ip:\n",
    "            # Simulate cluster IP (10.96.x.y format)\n",
    "            self.cluster_ip = f\"10.96.{np.random.randint(0, 255)}.{np.random.randint(1, 255)}\"\n",
    "    \n",
    "    def add_pod(self, pod: Pod):\n",
    "        \"\"\"Add pod to service backend.\"\"\"\n",
    "        self.target_pods.append(pod)\n",
    "    \n",
    "    def get_endpoint(self) -> Optional[Pod]:\n",
    "        \"\"\"Load balance request to healthy pod (round-robin simulation).\"\"\"\n",
    "        healthy_pods = [p for p in self.target_pods if p.status == \"Running\"]\n",
    "        if not healthy_pods:\n",
    "            return None\n",
    "        return np.random.choice(healthy_pods)\n",
    "    \n",
    "    def handle_request(self) -> Dict:\n",
    "        \"\"\"Simulate service request (load balance to pod).\"\"\"\n",
    "        pod = self.get_endpoint()\n",
    "        if not pod:\n",
    "            return {\"status\": \"error\", \"message\": \"No healthy pods available\"}\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"service\": self.name,\n",
    "            \"cluster_ip\": self.cluster_ip,\n",
    "            \"pod_name\": pod.name,\n",
    "            \"pod_ip\": pod.ip,\n",
    "            \"node\": pod.node\n",
    "        }\n",
    "\n",
    "\n",
    "# Example 1: Create Kubernetes cluster with 3 worker nodes\n",
    "print(\"=\" * 70)\n",
    "print(\"EXAMPLE 1: Kubernetes Cluster Creation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cluster = KubernetesCluster(name=\"prod-ml-cluster\")\n",
    "\n",
    "# Add 3 worker nodes\n",
    "cluster.add_node(Node(name=\"node-1\", cpu_capacity=8.0, memory_capacity=16384))\n",
    "cluster.add_node(Node(name=\"node-2\", cpu_capacity=8.0, memory_capacity=16384))\n",
    "cluster.add_node(Node(name=\"node-3\", cpu_capacity=8.0, memory_capacity=16384))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 2: Deploy ML Model Service (5 replicas)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create deployment: wafer yield predictor (5 replicas)\n",
    "pods = cluster.create_deployment(\n",
    "    name=\"wafer-yield-predictor\",\n",
    "    image=\"registry.io/wafer-model:v2.3\",\n",
    "    replicas=5,\n",
    "    cpu_request=1.0,\n",
    "    memory_request=1024\n",
    ")\n",
    "\n",
    "# Show cluster status\n",
    "status = cluster.get_cluster_status()\n",
    "print(f\"\\nðŸ“Š Cluster Status:\")\n",
    "print(f\"   Nodes: {status['total_nodes']}\")\n",
    "print(f\"   Pods: {status['total_pods']}\")\n",
    "print(f\"   CPU Utilization: {status['cpu_utilization']:.1f}% ({status['used_cpu']:.1f}/{status['total_cpu']:.1f} cores)\")\n",
    "print(f\"   Memory Utilization: {status['memory_utilization']:.1f}% ({status['used_memory']}/{status['total_memory']} MB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 3: Create Service for Load Balancing\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create Service (stable endpoint for pods)\n",
    "service = Service(name=\"wafer-predictor-svc\", port=8080)\n",
    "for pod in pods:\n",
    "    service.add_pod(pod)\n",
    "\n",
    "print(f\"âœ… Service created: {service.name}\")\n",
    "print(f\"   Type: {service.service_type}\")\n",
    "print(f\"   Cluster IP: {service.cluster_ip}:{service.port}\")\n",
    "print(f\"   Backend Pods: {len(service.target_pods)}\")\n",
    "\n",
    "# Simulate 10 requests to service (load balancing)\n",
    "print(f\"\\nðŸ”„ Simulating 10 requests to service {service.cluster_ip}:8080...\")\n",
    "pod_request_counts = {}\n",
    "\n",
    "for i in range(10):\n",
    "    response = service.handle_request()\n",
    "    if response[\"status\"] == \"success\":\n",
    "        pod_name = response[\"pod_name\"]\n",
    "        pod_request_counts[pod_name] = pod_request_counts.get(pod_name, 0) + 1\n",
    "\n",
    "print(f\"\\nðŸ“Š Request Distribution Across Pods:\")\n",
    "for pod_name, count in sorted(pod_request_counts.items()):\n",
    "    print(f\"   {pod_name}: {count} requests\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight: Service distributes traffic across all healthy pods (load balancing)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 4: Self-Healing - Pod Restart\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simulate pod failure and restart\n",
    "failed_pod = pods[0]\n",
    "print(f\"âŒ Pod {failed_pod.name} crashed (IP: {failed_pod.ip})\")\n",
    "print(f\"   Kubernetes detects failure and restarts pod...\")\n",
    "\n",
    "old_ip = failed_pod.ip\n",
    "failed_pod.restart()\n",
    "\n",
    "print(f\"âœ… Pod restarted successfully\")\n",
    "print(f\"   Old IP: {old_ip}\")\n",
    "print(f\"   New IP: {failed_pod.ip}\")\n",
    "print(f\"   Service automatically routes to new IP (no manual intervention)\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight: Pods are ephemeral (IP changes), Service provides stable endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e419bd",
   "metadata": {},
   "source": [
    "## 3. ðŸ“ˆ Horizontal Pod Autoscaling (HPA) for ML Workloads\n",
    "\n",
    "**Purpose:** Automatically scale ML model replicas based on CPU, memory, or custom metrics (requests per second, queue depth).\n",
    "\n",
    "**Key Points:**\n",
    "- **HPA (Horizontal Pod Autoscaler):** Adjusts replica count based on metrics (5 â†’ 50 pods during traffic spike)\n",
    "- **Metrics:** CPU utilization (target: 70%), memory, custom metrics (RPS, latency, queue length)\n",
    "- **Scale-up:** When CPU >70%, add pods (max: 50 replicas)\n",
    "- **Scale-down:** When CPU <50%, remove pods (min: 2 replicas, gradual cooldown)\n",
    "- **Target tracking:** Maintains target metric (e.g., 70% CPU across all pods)\n",
    "\n",
    "**Why It Matters:**\n",
    "- **Cost efficiency:** Scale down during low traffic (50 pods â†’ 5 pods saves 90% infrastructure cost)\n",
    "- **Performance:** Scale up during peaks (handle 10,000 RPS without degradation)\n",
    "- **Automatic:** No manual intervention (responds to traffic in 30 seconds)\n",
    "- **Business continuity:** Prevent overload (traffic spike doesn't crash service)\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "STDF parser HPA: Normal load 100 files/hour (5 pods sufficient), wafer test spike 1000 files/hour (HPA scales to 50 pods in 2 minutes), after spike HPA gradually scales down to 5 pods (15-minute cooldown prevents flapping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bce082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal Pod Autoscaler (HPA) Simulation\n",
    "\n",
    "class HorizontalPodAutoscaler:\n",
    "    \"\"\"Simulates Kubernetes HPA for automatic scaling.\"\"\"\n",
    "    \n",
    "    def __init__(self, deployment_name: str, min_replicas: int = 2, \n",
    "                 max_replicas: int = 50, target_cpu_percent: float = 70.0):\n",
    "        self.deployment_name = deployment_name\n",
    "        self.min_replicas = min_replicas\n",
    "        self.max_replicas = max_replicas\n",
    "        self.target_cpu_percent = target_cpu_percent\n",
    "        self.current_replicas = min_replicas\n",
    "        self.scale_history = []\n",
    "        \n",
    "    def calculate_desired_replicas(self, current_cpu_percent: float) -> int:\n",
    "        \"\"\"Calculate desired replica count based on current CPU utilization.\"\"\"\n",
    "        if current_cpu_percent == 0:\n",
    "            return self.current_replicas\n",
    "        \n",
    "        # HPA formula: desired = ceil(current_replicas * (current_metric / target_metric))\n",
    "        desired = int(np.ceil(self.current_replicas * (current_cpu_percent / self.target_cpu_percent)))\n",
    "        \n",
    "        # Apply min/max constraints\n",
    "        desired = max(self.min_replicas, min(desired, self.max_replicas))\n",
    "        \n",
    "        return desired\n",
    "    \n",
    "    def scale(self, current_cpu_percent: float, timestamp: str) -> Dict:\n",
    "        \"\"\"Execute scaling decision based on current CPU.\"\"\"\n",
    "        desired = self.calculate_desired_replicas(current_cpu_percent)\n",
    "        \n",
    "        action = \"no_change\"\n",
    "        if desired > self.current_replicas:\n",
    "            action = \"scale_up\"\n",
    "        elif desired < self.current_replicas:\n",
    "            action = \"scale_down\"\n",
    "        \n",
    "        old_replicas = self.current_replicas\n",
    "        self.current_replicas = desired\n",
    "        \n",
    "        scale_event = {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"current_cpu\": current_cpu_percent,\n",
    "            \"target_cpu\": self.target_cpu_percent,\n",
    "            \"old_replicas\": old_replicas,\n",
    "            \"new_replicas\": desired,\n",
    "            \"action\": action\n",
    "        }\n",
    "        self.scale_history.append(scale_event)\n",
    "        \n",
    "        if action != \"no_change\":\n",
    "            symbol = \"ðŸ“ˆ\" if action == \"scale_up\" else \"ðŸ“‰\"\n",
    "            print(f\"{symbol} {timestamp}: CPU {current_cpu_percent:.1f}% â†’ \"\n",
    "                  f\"Scale {old_replicas} â†’ {desired} replicas ({action})\")\n",
    "        \n",
    "        return scale_event\n",
    "    \n",
    "    def get_scaling_summary(self) -> Dict:\n",
    "        \"\"\"Summarize scaling activity.\"\"\"\n",
    "        scale_ups = sum(1 for e in self.scale_history if e[\"action\"] == \"scale_up\")\n",
    "        scale_downs = sum(1 for e in self.scale_history if e[\"action\"] == \"scale_down\")\n",
    "        max_replicas_used = max(e[\"new_replicas\"] for e in self.scale_history) if self.scale_history else 0\n",
    "        \n",
    "        return {\n",
    "            \"total_events\": len(self.scale_history),\n",
    "            \"scale_ups\": scale_ups,\n",
    "            \"scale_downs\": scale_downs,\n",
    "            \"max_replicas\": max_replicas_used,\n",
    "            \"final_replicas\": self.current_replicas\n",
    "        }\n",
    "\n",
    "\n",
    "def simulate_traffic_pattern(hours: int = 24) -> pd.DataFrame:\n",
    "    \"\"\"Simulate realistic traffic pattern (daily cycle with spikes).\"\"\"\n",
    "    timestamps = pd.date_range(start=\"2024-01-01 00:00\", periods=hours, freq=\"h\")\n",
    "    \n",
    "    # Base traffic pattern (sinusoidal daily cycle)\n",
    "    base_load = 50 + 30 * np.sin(np.linspace(0, 2*np.pi, hours))\n",
    "    \n",
    "    # Add random spikes\n",
    "    spikes = np.zeros(hours)\n",
    "    spike_hours = np.random.choice(hours, size=3, replace=False)\n",
    "    for hour in spike_hours:\n",
    "        spikes[hour] = np.random.uniform(40, 60)\n",
    "    \n",
    "    # Combine base + spikes + noise\n",
    "    cpu_utilization = base_load + spikes + np.random.normal(0, 5, hours)\n",
    "    cpu_utilization = np.clip(cpu_utilization, 10, 95)  # Keep within 10-95%\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"timestamp\": timestamps,\n",
    "        \"cpu_percent\": cpu_utilization\n",
    "    })\n",
    "\n",
    "\n",
    "# Example 1: Simulate 24-hour autoscaling with traffic pattern\n",
    "print(\"=\" * 70)\n",
    "print(\"EXAMPLE 1: HPA in Action - 24 Hour Traffic Pattern\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create HPA\n",
    "hpa = HorizontalPodAutoscaler(\n",
    "    deployment_name=\"wafer-yield-predictor\",\n",
    "    min_replicas=5,\n",
    "    max_replicas=50,\n",
    "    target_cpu_percent=70.0\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š HPA Configuration:\")\n",
    "print(f\"   Deployment: {hpa.deployment_name}\")\n",
    "print(f\"   Min Replicas: {hpa.min_replicas}\")\n",
    "print(f\"   Max Replicas: {hpa.max_replicas}\")\n",
    "print(f\"   Target CPU: {hpa.target_cpu_percent}%\")\n",
    "\n",
    "# Generate traffic pattern\n",
    "traffic_df = simulate_traffic_pattern(hours=24)\n",
    "\n",
    "print(f\"\\nðŸ”„ Starting autoscaling simulation (24 hours)...\\n\")\n",
    "\n",
    "# Simulate HPA decisions every hour\n",
    "for _, row in traffic_df.iterrows():\n",
    "    hpa.scale(\n",
    "        current_cpu_percent=row[\"cpu_percent\"],\n",
    "        timestamp=row[\"timestamp\"].strftime(\"%Y-%m-%d %H:%M\")\n",
    "    )\n",
    "\n",
    "# Get summary\n",
    "summary = hpa.get_scaling_summary()\n",
    "print(f\"\\nðŸ“Š Scaling Summary (24 hours):\")\n",
    "print(f\"   Total Scaling Events: {summary['total_events']}\")\n",
    "print(f\"   Scale-Ups: {summary['scale_ups']}\")\n",
    "print(f\"   Scale-Downs: {summary['scale_downs']}\")\n",
    "print(f\"   Max Replicas Used: {summary['max_replicas']}\")\n",
    "print(f\"   Final Replicas: {summary['final_replicas']}\")\n",
    "\n",
    "# Visualize autoscaling behavior\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "# Plot 1: CPU utilization over time\n",
    "ax1.plot(traffic_df[\"timestamp\"], traffic_df[\"cpu_percent\"], \n",
    "         linewidth=2, label=\"CPU Utilization\", color='#FF6B6B')\n",
    "ax1.axhline(y=hpa.target_cpu_percent, color='green', linestyle='--', \n",
    "            linewidth=2, label=f'Target ({hpa.target_cpu_percent}%)')\n",
    "ax1.fill_between(traffic_df[\"timestamp\"], 0, traffic_df[\"cpu_percent\"], \n",
    "                  alpha=0.3, color='#FF6B6B')\n",
    "ax1.set_ylabel(\"CPU Utilization (%)\", fontsize=12, fontweight='bold')\n",
    "ax1.set_title(\"Horizontal Pod Autoscaling - CPU and Replica Count\", \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "ax1.legend(loc='upper left', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 100)\n",
    "\n",
    "# Plot 2: Replica count over time\n",
    "replica_counts = [e[\"new_replicas\"] for e in hpa.scale_history]\n",
    "timestamps = [pd.to_datetime(e[\"timestamp\"]) for e in hpa.scale_history]\n",
    "\n",
    "ax2.plot(timestamps, replica_counts, linewidth=2.5, \n",
    "         marker='o', markersize=6, label=\"Replica Count\", color='#4ECDC4')\n",
    "ax2.fill_between(timestamps, 0, replica_counts, alpha=0.3, color='#4ECDC4')\n",
    "ax2.axhline(y=hpa.min_replicas, color='orange', linestyle=':', \n",
    "            linewidth=2, label=f'Min ({hpa.min_replicas})')\n",
    "ax2.axhline(y=hpa.max_replicas, color='red', linestyle=':', \n",
    "            linewidth=2, label=f'Max ({hpa.max_replicas})')\n",
    "ax2.set_xlabel(\"Time\", fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel(\"Pod Replicas\", fontsize=12, fontweight='bold')\n",
    "ax2.legend(loc='upper left', fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, hpa.max_replicas + 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insights:\")\n",
    "print(\"   â€¢ HPA scales UP when CPU > 70% (handle increased load)\")\n",
    "print(\"   â€¢ HPA scales DOWN when CPU < 70% (save costs)\")\n",
    "print(\"   â€¢ Scaling is gradual (prevents flapping)\")\n",
    "print(\"   â€¢ Min/max constraints prevent extreme scaling\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 2: Cost Savings from Autoscaling\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate cost savings\n",
    "avg_replicas_static = 50  # Static deployment (always 50 pods for peak)\n",
    "avg_replicas_hpa = np.mean(replica_counts)\n",
    "cost_per_pod_hour = 0.05  # $0.05/pod/hour\n",
    "\n",
    "static_cost_24h = avg_replicas_static * 24 * cost_per_pod_hour\n",
    "hpa_cost_24h = avg_replicas_hpa * 24 * cost_per_pod_hour\n",
    "savings_24h = static_cost_24h - hpa_cost_24h\n",
    "savings_percent = (savings_24h / static_cost_24h) * 100\n",
    "\n",
    "print(f\"ðŸ’° Cost Analysis (24 hours):\")\n",
    "print(f\"   Static Deployment (50 pods always):\")\n",
    "print(f\"      Cost: ${static_cost_24h:.2f}\")\n",
    "print(f\"   \")\n",
    "print(f\"   HPA Deployment (avg {avg_replicas_hpa:.1f} pods):\")\n",
    "print(f\"      Cost: ${hpa_cost_24h:.2f}\")\n",
    "print(f\"   \")\n",
    "print(f\"   Savings: ${savings_24h:.2f} ({savings_percent:.1f}%)\")\n",
    "print(f\"   Monthly Savings: ${savings_24h * 30:.2f}\")\n",
    "print(f\"   Annual Savings: ${savings_24h * 365:.2f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Business Impact:\")\n",
    "print(f\"   â€¢ Same performance during peaks (50 pods available)\")\n",
    "print(f\"   â€¢ {savings_percent:.1f}% cost reduction (scale down during low traffic)\")\n",
    "print(f\"   â€¢ Fully automated (no manual scaling required)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddee381",
   "metadata": {},
   "source": [
    "## 4. ðŸ”„ Rolling Updates and Rollbacks - Zero-Downtime Deployments\n",
    "\n",
    "**Purpose:** Update ML models to new versions without service interruption (v2.3 â†’ v2.4 with zero dropped requests).\n",
    "\n",
    "**Key Points:**\n",
    "- **Rolling Update Strategy:** Update pods gradually (1 at a time, wait for readiness before next)\n",
    "- **MaxUnavailable:** Maximum pods down during update (1 out of 10 = 90% capacity maintained)\n",
    "- **MaxSurge:** Extra pods during update (surge=1 means 11 pods temporarily, smooth transition)\n",
    "- **Readiness Probe:** Health check determines when new pod ready (wait 30s after startup)\n",
    "- **Rollback:** Revert to previous version instantly (`kubectl rollout undo`)\n",
    "\n",
    "**Why It Matters:**\n",
    "- **Zero downtime:** Users don't notice deployment (service continuously available)\n",
    "- **Gradual rollout:** Canary pattern (update 1 pod, monitor, then rest)\n",
    "- **Safety:** Rollback in 30 seconds if new version has bugs\n",
    "- **Version history:** Keep last 10 revisions (audit trail, easy rollback)\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "Wafer yield predictor v2.3 â†’ v2.4 deployment: 10 pods total, rolling update with maxUnavailable=1, update 1 pod every 2 minutes (monitor error rate), total deployment time 20 minutes, if v2.4 error rate >1%, automatic rollback to v2.3 in 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling Update Simulation\n",
    "\n",
    "class RollingUpdateDeployment:\n",
    "    \"\"\"Simulates Kubernetes rolling update strategy.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, replicas: int, image: str, \n",
    "                 max_unavailable: int = 1, max_surge: int = 1):\n",
    "        self.name = name\n",
    "        self.desired_replicas = replicas\n",
    "        self.image = image\n",
    "        self.max_unavailable = max_unavailable\n",
    "        self.max_surge = max_surge\n",
    "        self.pods: List[Pod] = []\n",
    "        self.revision_history = []\n",
    "        \n",
    "        # Create initial pods\n",
    "        for i in range(replicas):\n",
    "            self.pods.append(Pod(\n",
    "                name=f\"{name}-{uuid.uuid4().hex[:8]}\",\n",
    "                node=f\"node-{(i % 3) + 1}\",\n",
    "                containers=[image]\n",
    "            ))\n",
    "        \n",
    "        self.revision_history.append({\n",
    "            \"revision\": 1,\n",
    "            \"image\": image,\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "    \n",
    "    def update_to_version(self, new_image: str, simulate_delay: bool = False) -> Dict:\n",
    "        \"\"\"Perform rolling update to new image version.\"\"\"\n",
    "        print(f\"\\nðŸ”„ Starting rolling update: {self.image} â†’ {new_image}\")\n",
    "        print(f\"   Strategy: maxUnavailable={self.max_unavailable}, maxSurge={self.max_surge}\")\n",
    "        \n",
    "        update_log = []\n",
    "        old_pods = self.pods.copy()\n",
    "        new_pods = []\n",
    "        \n",
    "        # Create surge pods first (extra capacity during update)\n",
    "        surge_count = min(self.max_surge, self.desired_replicas)\n",
    "        for i in range(surge_count):\n",
    "            new_pod = Pod(\n",
    "                name=f\"{self.name}-{uuid.uuid4().hex[:8]}\",\n",
    "                node=f\"node-{np.random.randint(1, 4)}\",\n",
    "                containers=[new_image]\n",
    "            )\n",
    "            new_pods.append(new_pod)\n",
    "            self.pods.append(new_pod)\n",
    "            update_log.append(f\"âœ… Created new pod {new_pod.name} ({new_image})\")\n",
    "            if simulate_delay:\n",
    "                time.sleep(0.1)\n",
    "        \n",
    "        # Rolling update: replace old pods one by one\n",
    "        for i, old_pod in enumerate(old_pods):\n",
    "            # Terminate old pod\n",
    "            old_pod.status = \"Terminating\"\n",
    "            update_log.append(f\"ðŸ”» Terminating old pod {old_pod.name} ({self.image})\")\n",
    "            \n",
    "            # If we haven't created enough surge pods, create new pod now\n",
    "            if i >= surge_count:\n",
    "                new_pod = Pod(\n",
    "                    name=f\"{self.name}-{uuid.uuid4().hex[:8]}\",\n",
    "                    node=old_pod.node,  # Schedule on same node\n",
    "                    containers=[new_image]\n",
    "                )\n",
    "                new_pods.append(new_pod)\n",
    "                update_log.append(f\"âœ… Created new pod {new_pod.name} ({new_image})\")\n",
    "                if simulate_delay:\n",
    "                    time.sleep(0.1)\n",
    "            \n",
    "            # Wait for readiness probe (simulated)\n",
    "            update_log.append(f\"â³ Waiting for readiness probe...\")\n",
    "            if simulate_delay:\n",
    "                time.sleep(0.05)\n",
    "            \n",
    "            # Remove old pod\n",
    "            self.pods.remove(old_pod)\n",
    "            \n",
    "            # Print progress\n",
    "            progress = ((i + 1) / len(old_pods)) * 100\n",
    "            print(f\"   Progress: {progress:.0f}% ({i+1}/{len(old_pods)} pods updated)\")\n",
    "        \n",
    "        # Remove extra surge pods\n",
    "        while len(self.pods) > self.desired_replicas:\n",
    "            surplus_pod = self.pods.pop()\n",
    "            update_log.append(f\"ðŸ”» Removing surge pod {surplus_pod.name}\")\n",
    "        \n",
    "        # Update image and revision history\n",
    "        old_image = self.image\n",
    "        self.image = new_image\n",
    "        self.revision_history.append({\n",
    "            \"revision\": len(self.revision_history) + 1,\n",
    "            \"image\": new_image,\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nâœ… Rolling update completed successfully\")\n",
    "        print(f\"   Old version: {old_image}\")\n",
    "        print(f\"   New version: {new_image}\")\n",
    "        print(f\"   Pods updated: {len(old_pods)}\")\n",
    "        print(f\"   Final pod count: {len(self.pods)}\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"old_image\": old_image,\n",
    "            \"new_image\": new_image,\n",
    "            \"pods_updated\": len(old_pods),\n",
    "            \"update_log\": update_log\n",
    "        }\n",
    "    \n",
    "    def rollback(self) -> Dict:\n",
    "        \"\"\"Rollback to previous revision.\"\"\"\n",
    "        if len(self.revision_history) < 2:\n",
    "            return {\"status\": \"error\", \"message\": \"No previous revision to rollback to\"}\n",
    "        \n",
    "        current_revision = self.revision_history[-1]\n",
    "        previous_revision = self.revision_history[-2]\n",
    "        \n",
    "        print(f\"\\nâª Rolling back deployment...\")\n",
    "        print(f\"   Current: {current_revision['image']} (revision {current_revision['revision']})\")\n",
    "        print(f\"   Target: {previous_revision['image']} (revision {previous_revision['revision']})\")\n",
    "        \n",
    "        # Perform rollback (update to previous image)\n",
    "        result = self.update_to_version(previous_revision['image'])\n",
    "        \n",
    "        print(f\"\\nâœ… Rollback completed in 30 seconds\")\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"reverted_from\": current_revision['image'],\n",
    "            \"reverted_to\": previous_revision['image']\n",
    "        }\n",
    "    \n",
    "    def get_revision_history(self) -> List[Dict]:\n",
    "        \"\"\"Get deployment revision history.\"\"\"\n",
    "        return self.revision_history\n",
    "\n",
    "\n",
    "# Example 1: Rolling update simulation\n",
    "print(\"=\" * 70)\n",
    "print(\"EXAMPLE 1: Rolling Update - v2.3 â†’ v2.4\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create deployment with v2.3\n",
    "deployment = RollingUpdateDeployment(\n",
    "    name=\"wafer-yield-predictor\",\n",
    "    replicas=10,\n",
    "    image=\"registry.io/wafer-model:v2.3\",\n",
    "    max_unavailable=1,\n",
    "    max_surge=1\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“¦ Initial Deployment:\")\n",
    "print(f\"   Name: {deployment.name}\")\n",
    "print(f\"   Replicas: {len(deployment.pods)}\")\n",
    "print(f\"   Image: {deployment.image}\")\n",
    "\n",
    "# Perform rolling update to v2.4\n",
    "result = deployment.update_to_version(\"registry.io/wafer-model:v2.4\", simulate_delay=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 2: Revision History\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "history = deployment.get_revision_history()\n",
    "print(f\"ðŸ“œ Deployment Revision History:\")\n",
    "for rev in history:\n",
    "    print(f\"   Revision {rev['revision']}: {rev['image']} (deployed at {rev['timestamp']})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 3: Rollback to Previous Version\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simulate bug in v2.4 - rollback to v2.3\n",
    "print(\"âŒ Bug detected in v2.4 (error rate increased to 5%)\")\n",
    "print(\"ðŸš¨ Initiating automatic rollback...\")\n",
    "\n",
    "rollback_result = deployment.rollback()\n",
    "\n",
    "print(f\"\\nðŸ“Š Post-Rollback Status:\")\n",
    "print(f\"   Current Image: {deployment.image}\")\n",
    "print(f\"   Active Pods: {len(deployment.pods)}\")\n",
    "print(f\"   All pods healthy: âœ…\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insights:\")\n",
    "print(\"   â€¢ Rolling update: Zero downtime (always â‰¥9 pods running)\")\n",
    "print(\"   â€¢ MaxUnavailable=1: 90% capacity during update (9/10 pods)\")\n",
    "print(\"   â€¢ MaxSurge=1: Temporarily 11 pods (smooth transition)\")\n",
    "print(\"   â€¢ Rollback: 30 seconds to revert (vs hours of manual rollback)\")\n",
    "print(\"   â€¢ Version history: Audit trail for compliance\")\n",
    "\n",
    "# Visualize rolling update timeline\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 4: Rolling Update Timeline Visualization\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simulate detailed timeline\n",
    "timeline_data = {\n",
    "    \"time_seconds\": [0, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300],\n",
    "    \"v2.3_pods\": [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n",
    "    \"v2.4_pods\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"total_pods\": [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n",
    "}\n",
    "\n",
    "timeline_df = pd.DataFrame(timeline_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.plot(timeline_df[\"time_seconds\"], timeline_df[\"v2.3_pods\"], \n",
    "        marker='o', linewidth=3, markersize=8, label=\"v2.3 Pods (Old)\", color='#FF6B6B')\n",
    "ax.plot(timeline_df[\"time_seconds\"], timeline_df[\"v2.4_pods\"], \n",
    "        marker='s', linewidth=3, markersize=8, label=\"v2.4 Pods (New)\", color='#4ECDC4')\n",
    "ax.plot(timeline_df[\"time_seconds\"], timeline_df[\"total_pods\"], \n",
    "        linestyle='--', linewidth=2.5, label=\"Total Pods\", color='#95E1D3')\n",
    "\n",
    "ax.fill_between(timeline_df[\"time_seconds\"], 0, timeline_df[\"v2.3_pods\"], \n",
    "                 alpha=0.3, color='#FF6B6B')\n",
    "ax.fill_between(timeline_df[\"time_seconds\"], 0, timeline_df[\"v2.4_pods\"], \n",
    "                 alpha=0.3, color='#4ECDC4')\n",
    "\n",
    "ax.set_xlabel(\"Time (seconds)\", fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel(\"Number of Pods\", fontsize=12, fontweight='bold')\n",
    "ax.set_title(\"Rolling Update Timeline - Zero Downtime Deployment\", \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='center right', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 12)\n",
    "\n",
    "# Add annotations\n",
    "ax.annotate('Update Start', xy=(0, 10), xytext=(20, 11),\n",
    "            arrowprops=dict(arrowstyle='->', color='black', lw=1.5),\n",
    "            fontsize=10, fontweight='bold')\n",
    "ax.annotate('50% Complete', xy=(150, 5), xytext=(100, 8),\n",
    "            arrowprops=dict(arrowstyle='->', color='black', lw=1.5),\n",
    "            fontsize=10, fontweight='bold')\n",
    "ax.annotate('Update Complete', xy=(300, 10), xytext=(240, 11),\n",
    "            arrowprops=dict(arrowstyle='->', color='black', lw=1.5),\n",
    "            fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Timeline Analysis:\")\n",
    "print(\"   â€¢ Duration: 5 minutes (300 seconds)\")\n",
    "print(\"   â€¢ Pod replacement rate: 1 pod every 30 seconds\")\n",
    "print(\"   â€¢ Service availability: 100% (always 10 pods running)\")\n",
    "print(\"   â€¢ Traffic distribution: Gradual shift from v2.3 â†’ v2.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7e61ed",
   "metadata": {},
   "source": [
    "## 5. ðŸš€ Real-World Project Templates\n",
    "\n",
    "---\n",
    "\n",
    "### Project 1: Auto-Scaling STDF Parser Service on Kubernetes\n",
    "\n",
    "**Objective:** Deploy STDF binary file parser on Kubernetes with HPA for auto-scaling (5-50 pods based on queue depth).\n",
    "\n",
    "**Business Value:**\n",
    "- **Cost efficiency:** Scale down to 5 pods during off-hours (save 90% infrastructure cost)\n",
    "- **Performance:** Handle wafer test spikes (1000 files/hour) without latency degradation\n",
    "- **Reliability:** 3 availability zones, survive datacenter failures\n",
    "\n",
    "**Features to Implement:**\n",
    "- **Deployment:** 5 replicas initially, HPA target: 80% CPU or 100 messages in SQS queue\n",
    "- **Service:** LoadBalancer type (external IP for test equipment)\n",
    "- **ConfigMap:** Parser configuration (chunk_size, output_format, compression)\n",
    "- **Secret:** Database credentials (PostgreSQL connection string)\n",
    "- **PersistentVolume:** Store parsed data temporarily (1TB SSD)\n",
    "- **Resource limits:** CPU: 1 core, Memory: 2GB per pod\n",
    "\n",
    "**Success Criteria:**\n",
    "- âœ… Auto-scale from 5 â†’ 50 pods in 3 minutes during spike\n",
    "- âœ… Parse latency <2 seconds per STDF file (10K devices)\n",
    "- âœ… 99.9% uptime (8.76 hours downtime/year max)\n",
    "- âœ… Zero data loss (persistent queue, retries)\n",
    "- âœ… Cost <$500/month (vs $5000/month static 50 pods)\n",
    "\n",
    "**Kubernetes YAML Skeleton:**\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: stdf-parser\n",
    "spec:\n",
    "  replicas: 5\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: parser\n",
    "        image: registry.io/stdf-parser:v1.2\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: \"1\"\n",
    "            memory: \"2Gi\"\n",
    "        env:\n",
    "        - name: DB_CONNECTION\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: db-credentials\n",
    "              key: connection-string\n",
    "---\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: stdf-parser-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    kind: Deployment\n",
    "    name: stdf-parser\n",
    "  minReplicas: 5\n",
    "  maxReplicas: 50\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Project 2: High-Availability ML Model API with Multi-Region Deployment\n",
    "\n",
    "**Objective:** Deploy wafer yield prediction API across 3 AWS regions (us-west-2, us-east-1, eu-west-1) with global load balancing.\n",
    "\n",
    "**Business Value:**\n",
    "- **Latency:** <50ms response time worldwide (serve from nearest region)\n",
    "- **Availability:** 99.99% uptime (4 regions, survive region failures)\n",
    "- **Compliance:** EU data stays in EU (GDPR compliance)\n",
    "\n",
    "**Features to Implement:**\n",
    "- **Multi-cluster:** 3 Kubernetes clusters (EKS in us-west-2, us-east-1, eu-west-1)\n",
    "- **Global load balancer:** AWS Route53 geo-routing (direct traffic to nearest region)\n",
    "- **StatefulSet:** Model pods with persistent model storage (EBS volumes)\n",
    "- **Cross-region replication:** Model updates replicate to all regions (S3 sync)\n",
    "- **Health checks:** Readiness/liveness probes, Route53 health checks\n",
    "- **Monitoring:** Prometheus federated across regions, Grafana global dashboard\n",
    "\n",
    "**Success Criteria:**\n",
    "- âœ… Latency: <50ms p99 from any location\n",
    "- âœ… Failover: <10 seconds when region fails\n",
    "- âœ… Model sync: New version deployed to all regions within 15 minutes\n",
    "- âœ… Cost: <$2000/month (3 regions, auto-scaling)\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "               â”‚ Route53 Global LB   â”‚\n",
    "               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚                 â”‚                 â”‚\n",
    "   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”\n",
    "   â”‚ us-west-2â”‚       â”‚us-east-1â”‚       â”‚ eu-west-1â”‚\n",
    "   â”‚   EKS   â”‚       â”‚   EKS   â”‚       â”‚   EKS   â”‚\n",
    "   â”‚ 10 pods â”‚       â”‚ 10 pods â”‚       â”‚ 10 pods â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Project 3: GPU-Accelerated Deep Learning Training on Kubernetes\n",
    "\n",
    "**Objective:** Run distributed PyTorch training on Kubernetes with 4 NVIDIA V100 GPUs (wafer defect detection model).\n",
    "\n",
    "**Business Value:**\n",
    "- **Training speed:** 10 hours â†’ 2.5 hours (4 GPUs, distributed training)\n",
    "- **Cost efficiency:** Spot instances for 70% discount (save $5000/month)\n",
    "- **Scalability:** Auto-provision GPUs, terminate when training complete\n",
    "\n",
    "**Features to Implement:**\n",
    "- **Job:** Kubernetes Job for training (not Deployment, terminates when complete)\n",
    "- **GPU scheduling:** Request 4 GPUs (nvidia.com/gpu: 4)\n",
    "- **Distributed training:** PyTorch DistributedDataParallel (DDP)\n",
    "- **Node affinity:** Schedule on p3.8xlarge instances (4 V100 GPUs)\n",
    "- **Volume:** Mount training data from EFS (100GB SEM images)\n",
    "- **Checkpointing:** Save checkpoints to S3 every epoch (resume on failure)\n",
    "- **TensorBoard:** Service for monitoring training (port-forward for access)\n",
    "\n",
    "**Success Criteria:**\n",
    "- âœ… Training time: <3 hours (vs 10 hours single GPU)\n",
    "- âœ… GPU utilization: >90% (efficient data pipeline)\n",
    "- âœ… Cost: <$50 per training run (spot instances)\n",
    "- âœ… Automatic retry: Resume from checkpoint on preemption\n",
    "- âœ… Model accuracy: >98% (detect scratches, particles, voids)\n",
    "\n",
    "**Kubernetes Job YAML:**\n",
    "```yaml\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: wafer-defect-training\n",
    "spec:\n",
    "  parallelism: 1  # Single job, 4 GPUs\n",
    "  template:\n",
    "    spec:\n",
    "      restartPolicy: OnFailure\n",
    "      containers:\n",
    "      - name: pytorch-training\n",
    "        image: pytorch/pytorch:2.0.0-cuda11.8-cudnn8\n",
    "        command: [\"python\", \"train.py\"]\n",
    "        resources:\n",
    "          limits:\n",
    "            nvidia.com/gpu: 4\n",
    "        volumeMounts:\n",
    "        - name: training-data\n",
    "          mountPath: /data\n",
    "        - name: checkpoints\n",
    "          mountPath: /checkpoints\n",
    "      volumes:\n",
    "      - name: training-data\n",
    "        persistentVolumeClaim:\n",
    "          claimName: efs-training-data\n",
    "      - name: checkpoints\n",
    "        emptyDir: {}\n",
    "      nodeSelector:\n",
    "        node.kubernetes.io/instance-type: p3.8xlarge\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Project 4: Batch Processing Pipeline with Kubernetes CronJobs\n",
    "\n",
    "**Objective:** Nightly wafer map analysis (process 1000 wafers, generate spatial correlation reports, email stakeholders).\n",
    "\n",
    "**Business Value:**\n",
    "- **Automation:** Zero manual effort (runs every night at 2 AM)\n",
    "- **Cost:** Only pay for compute during job execution (terminate after completion)\n",
    "- **Scalability:** Process 1000 wafers in parallel (100 concurrent pods)\n",
    "\n",
    "**Features to Implement:**\n",
    "- **CronJob:** Schedule daily at 2 AM UTC (`0 2 * * *`)\n",
    "- **Parallel Jobs:** 100 parallel pods (each processes 10 wafers)\n",
    "- **Job completion:** Terminate pods when all wafers processed\n",
    "- **Output:** Write reports to S3, send email via SES\n",
    "- **Failure handling:** Retry failed wafers up to 3 times\n",
    "- **Resource limits:** CPU: 2 cores, Memory: 4GB per pod\n",
    "- **Spot instances:** Use spot nodes for 70% cost savings\n",
    "\n",
    "**Success Criteria:**\n",
    "- âœ… Processing time: <2 hours for 1000 wafers (vs 20 hours sequential)\n",
    "- âœ… Parallelism: 100 concurrent pods (elastic scaling)\n",
    "- âœ… Cost: <$20 per run (spot instances, terminate after completion)\n",
    "- âœ… Reliability: 99.9% success rate (automatic retries)\n",
    "- âœ… Alerting: Email notification on completion or failure\n",
    "\n",
    "**CronJob YAML:**\n",
    "```yaml\n",
    "apiVersion: batch/v1\n",
    "kind: CronJob\n",
    "metadata:\n",
    "  name: nightly-wafer-analysis\n",
    "spec:\n",
    "  schedule: \"0 2 * * *\"  # 2 AM UTC daily\n",
    "  jobTemplate:\n",
    "    spec:\n",
    "      parallelism: 100\n",
    "      completions: 100\n",
    "      template:\n",
    "        spec:\n",
    "          restartPolicy: OnFailure\n",
    "          containers:\n",
    "          - name: wafer-analyzer\n",
    "            image: registry.io/wafer-analyzer:v1.0\n",
    "            args: [\"--batch-size\", \"10\"]\n",
    "            resources:\n",
    "              requests:\n",
    "                cpu: \"2\"\n",
    "                memory: \"4Gi\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Project 5: Blue-Green Deployment for Risk-Free Model Updates\n",
    "\n",
    "**Objective:** Deploy model updates with instant rollback capability (blue environment v2.3, green environment v2.4, switch traffic in 10 seconds).\n",
    "\n",
    "**Business Value:**\n",
    "- **Risk mitigation:** Test v2.4 in production without affecting users (blue still serves traffic)\n",
    "- **Instant rollback:** Revert in 10 seconds if v2.4 has issues (vs 30 minutes rolling update rollback)\n",
    "- **A/B testing:** Send 10% traffic to green, compare metrics, then switch 100%\n",
    "\n",
    "**Features to Implement:**\n",
    "- **Two Deployments:** blue (v2.3) and green (v2.4) running simultaneously\n",
    "- **Service:** Switch traffic via selector label (version: blue â†’ version: green)\n",
    "- **Resource duplication:** 2x infrastructure temporarily (10 blue pods + 10 green pods)\n",
    "- **Smoke tests:** Automated tests on green before switching traffic\n",
    "- **Metrics comparison:** Compare error rate, latency between blue and green\n",
    "- **Quick switch:** Update Service selector label (`kubectl patch`)\n",
    "\n",
    "**Success Criteria:**\n",
    "- âœ… Deployment time: 15 minutes (deploy green, test, switch)\n",
    "- âœ… Rollback time: 10 seconds (change Service selector)\n",
    "- âœ… Zero downtime: 100% (always 10 pods serving traffic)\n",
    "- âœ… Cost: 2x infrastructure for 15 minutes only\n",
    "- âœ… Confidence: Full production testing before switch\n",
    "\n",
    "**Blue-Green Switch Process:**\n",
    "```bash\n",
    "# Step 1: Deploy green (v2.4) alongside blue (v2.3)\n",
    "kubectl apply -f deployment-green.yaml\n",
    "\n",
    "# Step 2: Run smoke tests on green\n",
    "curl http://green-service:8080/health\n",
    "curl http://green-service:8080/predict -d '{\"features\": [...]}'\n",
    "\n",
    "# Step 3: Compare metrics (Prometheus)\n",
    "# Blue: Error rate 0.1%, Latency 45ms p99\n",
    "# Green: Error rate 0.1%, Latency 42ms p99 âœ…\n",
    "\n",
    "# Step 4: Switch traffic (update Service selector)\n",
    "kubectl patch service wafer-predictor -p '{\"spec\":{\"selector\":{\"version\":\"green\"}}}'\n",
    "\n",
    "# Traffic now flows to green (v2.4)\n",
    "# Blue (v2.3) still running, instant rollback if needed\n",
    "\n",
    "# Step 5: If green is stable after 1 hour, delete blue\n",
    "kubectl delete deployment wafer-predictor-blue\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Project 6: StatefulSet for Model Caching Layer with Redis\n",
    "\n",
    "**Objective:** Deploy Redis cluster on Kubernetes for caching model predictions (reduce redundant inference by 80%).\n",
    "\n",
    "**Business Value:**\n",
    "- **Latency:** 2ms cache hit vs 50ms model inference (25x speedup)\n",
    "- **Cost:** Reduce inference load (80% cache hit rate, 5x fewer model pods needed)\n",
    "- **Scalability:** Handle 50,000 RPS (cache absorbs load)\n",
    "\n",
    "**Features to Implement:**\n",
    "- **StatefulSet:** 3 Redis pods with stable network identities (redis-0, redis-1, redis-2)\n",
    "- **PersistentVolumes:** Each Redis pod gets 50GB SSD (data survives pod restart)\n",
    "- **Headless Service:** DNS entries for each pod (redis-0.redis.svc.cluster.local)\n",
    "- **Redis Cluster:** Sharded data across 3 pods (automatic failover)\n",
    "- **TTL:** Cache entries expire after 1 hour (fresh predictions)\n",
    "- **Monitoring:** Redis Exporter for Prometheus (hit rate, memory usage)\n",
    "\n",
    "**Success Criteria:**\n",
    "- âœ… Cache hit rate: >80% (most predictions cached)\n",
    "- âœ… Latency: <5ms p99 for cache hits\n",
    "- âœ… Availability: 99.99% (Redis cluster, automatic failover)\n",
    "- âœ… Cost savings: 80% reduction in model inference (5 model pods vs 25 pods)\n",
    "\n",
    "**StatefulSet YAML:**\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: StatefulSet\n",
    "metadata:\n",
    "  name: redis\n",
    "spec:\n",
    "  serviceName: redis\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: redis\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: redis\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: redis\n",
    "        image: redis:7-alpine\n",
    "        ports:\n",
    "        - containerPort: 6379\n",
    "        volumeMounts:\n",
    "        - name: redis-data\n",
    "          mountPath: /data\n",
    "  volumeClaimTemplates:\n",
    "  - metadata:\n",
    "      name: redis-data\n",
    "    spec:\n",
    "      accessModes: [\"ReadWriteOnce\"]\n",
    "      resources:\n",
    "        requests:\n",
    "          storage: 50Gi\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Project 7: Kubernetes Ingress for External Access and TLS Termination\n",
    "\n",
    "**Objective:** Expose ML API to internet with HTTPS, domain name (api.wafer-validation.com), rate limiting, and authentication.\n",
    "\n",
    "**Business Value:**\n",
    "- **Security:** TLS encryption (HTTPS), prevent eavesdropping\n",
    "- **Usability:** Custom domain (vs random IP address)\n",
    "- **Protection:** Rate limiting (1000 req/min per IP), prevent abuse\n",
    "- **Cost:** Single load balancer for all services (vs separate LB per service)\n",
    "\n",
    "**Features to Implement:**\n",
    "- **Ingress:** NGINX Ingress Controller\n",
    "- **TLS Certificate:** Let's Encrypt auto-renewal (free certificates)\n",
    "- **Domain:** api.wafer-validation.com â†’ Service\n",
    "- **Rate limiting:** 1000 requests/minute per IP\n",
    "- **Authentication:** API key validation (Ingress middleware)\n",
    "- **Path routing:** /predict â†’ model service, /parse â†’ STDF parser service\n",
    "\n",
    "**Success Criteria:**\n",
    "- âœ… HTTPS: All traffic encrypted (A+ SSL Labs rating)\n",
    "- âœ… Auto-renewal: Certificates renew before expiry\n",
    "- âœ… Rate limiting: Block IPs exceeding 1000 req/min\n",
    "- âœ… Cost: $20/month (single load balancer for all services)\n",
    "\n",
    "**Ingress YAML:**\n",
    "```yaml\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: ml-api-ingress\n",
    "  annotations:\n",
    "    cert-manager.io/cluster-issuer: letsencrypt-prod\n",
    "    nginx.ingress.kubernetes.io/rate-limit: \"1000\"\n",
    "spec:\n",
    "  tls:\n",
    "  - hosts:\n",
    "    - api.wafer-validation.com\n",
    "    secretName: api-tls-cert\n",
    "  rules:\n",
    "  - host: api.wafer-validation.com\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /predict\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: wafer-predictor\n",
    "            port:\n",
    "              number: 8080\n",
    "      - path: /parse\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: stdf-parser\n",
    "            port:\n",
    "              number: 8080\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Project 8: Kubernetes Monitoring Stack with Prometheus and Grafana\n",
    "\n",
    "**Objective:** Complete observability for ML workloads (metrics, logs, traces, dashboards, alerting).\n",
    "\n",
    "**Business Value:**\n",
    "- **Visibility:** Real-time metrics (request rate, latency, error rate, resource usage)\n",
    "- **Alerting:** PagerDuty notification when error rate >1% or latency >100ms\n",
    "- **Troubleshooting:** Correlate logs, metrics, traces (find root cause in 5 minutes vs 2 hours)\n",
    "- **Capacity planning:** Historical trends (predict when to scale infrastructure)\n",
    "\n",
    "**Features to Implement:**\n",
    "- **Prometheus:** Scrape metrics from all pods (CPU, memory, custom metrics)\n",
    "- **Grafana:** Dashboards for ML metrics (predictions/sec, accuracy, latency percentiles)\n",
    "- **Alertmanager:** Route alerts to PagerDuty, Slack, email\n",
    "- **Loki:** Log aggregation (ELK alternative, Grafana integration)\n",
    "- **Jaeger:** Distributed tracing (track requests across microservices)\n",
    "- **Dashboards:** Pre-built for Kubernetes, ML models, business metrics\n",
    "\n",
    "**Success Criteria:**\n",
    "- âœ… Metrics retention: 90 days (Prometheus + Thanos)\n",
    "- âœ… Dashboard count: 10+ (cluster health, model performance, business KPIs)\n",
    "- âœ… Alert latency: <1 minute (from incident to notification)\n",
    "- âœ… Cost: <$100/month (storage, compute for monitoring stack)\n",
    "\n",
    "**Monitoring Architecture:**\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           Kubernetes Cluster                 â”‚\n",
    "â”‚                                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n",
    "â”‚  â”‚ Pod 1  â”‚  â”‚ Pod 2  â”‚  â”‚ Pod 3  â”‚        â”‚\n",
    "â”‚  â”‚ /metricsâ”‚  â”‚ /metricsâ”‚  â”‚ /metricsâ”‚        â”‚\n",
    "â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â”‚\n",
    "â”‚      â”‚           â”‚           â”‚              â”‚\n",
    "â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
    "â”‚                  â”‚                          â”‚\n",
    "â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\n",
    "â”‚          â”‚  Prometheus    â”‚                 â”‚\n",
    "â”‚          â”‚  (Scrape       â”‚                 â”‚\n",
    "â”‚          â”‚   every 15s)   â”‚                 â”‚\n",
    "â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "â”‚                  â”‚                          â”‚\n",
    "â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\n",
    "â”‚          â”‚   Grafana      â”‚                 â”‚\n",
    "â”‚          â”‚  (Dashboards)  â”‚                 â”‚\n",
    "â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "â”‚                  â”‚                          â”‚\n",
    "â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚\n",
    "â”‚          â”‚ Alertmanager   â”‚                 â”‚\n",
    "â”‚          â”‚ â†’ PagerDuty    â”‚                 â”‚\n",
    "â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a74c1a",
   "metadata": {},
   "source": [
    "## 6. ðŸ“‹ Comprehensive Takeaways - Kubernetes ML Mastery\n",
    "\n",
    "---\n",
    "\n",
    "### Section 1: Kubernetes Architecture Fundamentals\n",
    "\n",
    "**Control Plane Components:**\n",
    "- **API Server (kube-apiserver):** Entry point for all operations (kubectl â†’ API server)\n",
    "- **Scheduler (kube-scheduler):** Assigns pods to nodes (bin packing, resource constraints)\n",
    "- **Controller Manager:** Maintains desired state (5 replicas specified â†’ ensures 5 running)\n",
    "- **etcd:** Distributed key-value store (cluster state, configuration)\n",
    "\n",
    "**Worker Node Components:**\n",
    "- **kubelet:** Agent on each node (starts/stops pods, reports status)\n",
    "- **kube-proxy:** Network proxy (implements Services, load balancing)\n",
    "- **Container Runtime:** Runs containers (Docker, containerd, CRI-O)\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Pod:** Smallest deployable unit (1+ containers, shared network/storage, ephemeral)\n",
    "- **Deployment:** Manages replicas (declarative, rolling updates, rollbacks)\n",
    "- **Service:** Stable networking (ClusterIP, NodePort, LoadBalancer types)\n",
    "- **Namespace:** Virtual cluster (dev, staging, production isolation)\n",
    "\n",
    "**Why This Architecture:**\n",
    "- **Declarative:** Specify desired state (5 replicas), Kubernetes maintains it\n",
    "- **Self-healing:** Pod crashes â†’ kubelet restarts, node fails â†’ scheduler reschedules\n",
    "- **Scalable:** Control plane handles 1000s of nodes, 100,000s of pods\n",
    "\n",
    "---\n",
    "\n",
    "### Section 2: Pod Lifecycle and Health Checks\n",
    "\n",
    "**Pod Phases:**\n",
    "1. **Pending:** Waiting to be scheduled (no node available yet)\n",
    "2. **Running:** Pod scheduled, containers started\n",
    "3. **Succeeded:** All containers exited successfully (Job completed)\n",
    "4. **Failed:** At least one container exited with error\n",
    "5. **Unknown:** Node communication lost (kubelet not responding)\n",
    "\n",
    "**Probes (Health Checks):**\n",
    "```yaml\n",
    "livenessProbe:  # Is container running? (Restart if fails)\n",
    "  httpGet:\n",
    "    path: /health\n",
    "    port: 8080\n",
    "  initialDelaySeconds: 30\n",
    "  periodSeconds: 10\n",
    "  \n",
    "readinessProbe:  # Is container ready for traffic? (Remove from Service if fails)\n",
    "  httpGet:\n",
    "    path: /ready\n",
    "    port: 8080\n",
    "  initialDelaySeconds: 10\n",
    "  periodSeconds: 5\n",
    "```\n",
    "\n",
    "**Restart Policies:**\n",
    "- **Always:** Default, always restart (for long-running services)\n",
    "- **OnFailure:** Restart only if exits with error (for Jobs)\n",
    "- **Never:** Never restart (for one-time tasks)\n",
    "\n",
    "**Best Practices:**\n",
    "- âœ… **Liveness probe:** Detect deadlocks (app running but unresponsive)\n",
    "- âœ… **Readiness probe:** Prevent routing to pods during startup (wait for model loading)\n",
    "- âœ… **Separate probes:** Different endpoints (liveness: /health, readiness: /ready)\n",
    "- âœ… **Graceful shutdown:** Handle SIGTERM (finish in-flight requests before exit)\n",
    "\n",
    "---\n",
    "\n",
    "### Section 3: Services and Networking\n",
    "\n",
    "**Service Types:**\n",
    "```yaml\n",
    "# ClusterIP (default): Internal only (pod-to-pod communication)\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: model-internal\n",
    "spec:\n",
    "  type: ClusterIP\n",
    "  selector:\n",
    "    app: model\n",
    "  ports:\n",
    "  - port: 8080\n",
    "    targetPort: 8080\n",
    "\n",
    "# NodePort: External access via <NodeIP>:<NodePort>\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: model-external\n",
    "spec:\n",
    "  type: NodePort\n",
    "  selector:\n",
    "    app: model\n",
    "  ports:\n",
    "  - port: 8080\n",
    "    targetPort: 8080\n",
    "    nodePort: 30080  # Accessible at any-node-ip:30080\n",
    "\n",
    "# LoadBalancer: Cloud load balancer (AWS ELB, GCP LB)\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: model-lb\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  selector:\n",
    "    app: model\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8080\n",
    "```\n",
    "\n",
    "**Service Discovery:**\n",
    "- **DNS:** Every Service gets DNS name (`model-svc.default.svc.cluster.local`)\n",
    "- **Environment Variables:** Injected into pods (`MODEL_SVC_SERVICE_HOST=10.96.1.5`)\n",
    "\n",
    "**Load Balancing:**\n",
    "- **Round-robin:** Default (distribute evenly across pods)\n",
    "- **Session affinity:** Sticky sessions (`sessionAffinity: ClientIP`)\n",
    "\n",
    "**Key Insight:**\n",
    "Pods have dynamic IPs (change on restart), Services provide stable endpoint. Always use Service DNS name, never hardcode Pod IP.\n",
    "\n",
    "---\n",
    "\n",
    "### Section 4: Deployments and ReplicaSets\n",
    "\n",
    "**Deployment Structure:**\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: wafer-model\n",
    "spec:\n",
    "  replicas: 10\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: wafer-model\n",
    "  template:  # Pod template\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: wafer-model\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: model\n",
    "        image: registry.io/wafer-model:v2.3\n",
    "        resources:\n",
    "          requests:  # Guaranteed resources\n",
    "            cpu: \"1\"\n",
    "            memory: \"2Gi\"\n",
    "          limits:  # Maximum allowed\n",
    "            cpu: \"2\"\n",
    "            memory: \"4Gi\"\n",
    "```\n",
    "\n",
    "**Rolling Update Strategy:**\n",
    "```yaml\n",
    "strategy:\n",
    "  type: RollingUpdate\n",
    "  rollingUpdate:\n",
    "    maxUnavailable: 1  # Max pods down during update\n",
    "    maxSurge: 1        # Extra pods during update\n",
    "```\n",
    "\n",
    "**Update Commands:**\n",
    "```bash\n",
    "# Update image\n",
    "kubectl set image deployment/wafer-model model=registry.io/wafer-model:v2.4\n",
    "\n",
    "# Check rollout status\n",
    "kubectl rollout status deployment/wafer-model\n",
    "\n",
    "# View rollout history\n",
    "kubectl rollout history deployment/wafer-model\n",
    "\n",
    "# Rollback to previous version\n",
    "kubectl rollout undo deployment/wafer-model\n",
    "\n",
    "# Rollback to specific revision\n",
    "kubectl rollout undo deployment/wafer-model --to-revision=3\n",
    "\n",
    "# Pause rollout (canary deployment)\n",
    "kubectl rollout pause deployment/wafer-model\n",
    "\n",
    "# Resume rollout\n",
    "kubectl rollout resume deployment/wafer-model\n",
    "```\n",
    "\n",
    "**ReplicaSet:**\n",
    "- Deployment creates ReplicaSet (Deployment â†’ ReplicaSet â†’ Pods)\n",
    "- ReplicaSet ensures desired replica count (5 pods specified â†’ maintains 5 running)\n",
    "- Don't create ReplicaSets directly (use Deployments for version management)\n",
    "\n",
    "---\n",
    "\n",
    "### Section 5: Horizontal Pod Autoscaling (HPA)\n",
    "\n",
    "**HPA Metrics:**\n",
    "```yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: wafer-model-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: wafer-model\n",
    "  minReplicas: 5\n",
    "  maxReplicas: 50\n",
    "  metrics:\n",
    "  # CPU-based scaling\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  # Memory-based scaling\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80\n",
    "  # Custom metric (Prometheus)\n",
    "  - type: Pods\n",
    "    pods:\n",
    "      metric:\n",
    "        name: http_requests_per_second\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: \"1000\"\n",
    "```\n",
    "\n",
    "**HPA Formula:**\n",
    "```\n",
    "desired_replicas = ceil(current_replicas * (current_metric / target_metric))\n",
    "\n",
    "Example:\n",
    "current_replicas = 10\n",
    "current_cpu = 85%\n",
    "target_cpu = 70%\n",
    "desired_replicas = ceil(10 * (85 / 70)) = ceil(12.14) = 13\n",
    "```\n",
    "\n",
    "**Scale-Down Cooldown:**\n",
    "- Default: 5 minutes (prevent flapping)\n",
    "- Gradual scale-down (don't drop from 50 â†’ 5 instantly)\n",
    "\n",
    "**Best Practices:**\n",
    "- âœ… **Set appropriate targets:** CPU 70-80% (not 90%, leaves no buffer)\n",
    "- âœ… **Define min/max:** Prevent scaling to 0 or infinity\n",
    "- âœ… **Use custom metrics:** Better than CPU (e.g., queue depth, RPS)\n",
    "- âœ… **Monitor scaling events:** Alert on frequent scaling (indicates instability)\n",
    "\n",
    "---\n",
    "\n",
    "### Section 6: ConfigMaps and Secrets\n",
    "\n",
    "**ConfigMaps (Non-Sensitive Configuration):**\n",
    "```yaml\n",
    "# Create ConfigMap\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: model-config\n",
    "data:\n",
    "  model_path: \"/models/yield_v2.3.pkl\"\n",
    "  batch_size: \"32\"\n",
    "  confidence_threshold: \"0.85\"\n",
    "  feature_names: |\n",
    "    Vdd\n",
    "    Idd\n",
    "    frequency\n",
    "    temperature\n",
    "    power\n",
    "    yield\n",
    "\n",
    "# Use in Pod\n",
    "spec:\n",
    "  containers:\n",
    "  - name: model\n",
    "    env:\n",
    "    - name: MODEL_PATH\n",
    "      valueFrom:\n",
    "        configMapKeyRef:\n",
    "          name: model-config\n",
    "          key: model_path\n",
    "    # Or mount as volume\n",
    "    volumeMounts:\n",
    "    - name: config\n",
    "      mountPath: /config\n",
    "  volumes:\n",
    "  - name: config\n",
    "    configMap:\n",
    "      name: model-config\n",
    "```\n",
    "\n",
    "**Secrets (Sensitive Data):**\n",
    "```yaml\n",
    "# Create Secret\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: db-credentials\n",
    "type: Opaque\n",
    "data:\n",
    "  username: YWRtaW4=  # Base64: admin\n",
    "  password: cGFzc3dvcmQxMjM=  # Base64: password123\n",
    "\n",
    "# Use in Pod\n",
    "spec:\n",
    "  containers:\n",
    "  - name: model\n",
    "    env:\n",
    "    - name: DB_USERNAME\n",
    "      valueFrom:\n",
    "        secretKeyRef:\n",
    "          name: db-credentials\n",
    "          key: username\n",
    "    - name: DB_PASSWORD\n",
    "      valueFrom:\n",
    "        secretKeyRef:\n",
    "          name: db-credentials\n",
    "          key: password\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "- âœ… **Never commit secrets to Git** (use sealed-secrets, external secrets operator)\n",
    "- âœ… **Use RBAC:** Restrict who can view secrets\n",
    "- âœ… **Encrypt at rest:** Enable etcd encryption\n",
    "- âœ… **Rotate secrets:** Change passwords regularly, update Secrets\n",
    "- âœ… **ConfigMaps for config:** Secrets only for sensitive data\n",
    "\n",
    "---\n",
    "\n",
    "### Section 7: Persistent Storage - Volumes and PersistentVolumeClaims\n",
    "\n",
    "**Volume Types:**\n",
    "```yaml\n",
    "# emptyDir: Temporary, deleted when pod deleted\n",
    "volumes:\n",
    "- name: cache\n",
    "  emptyDir: {}\n",
    "\n",
    "# hostPath: Mount from node filesystem (not portable)\n",
    "volumes:\n",
    "- name: logs\n",
    "  hostPath:\n",
    "    path: /var/log/pods\n",
    "\n",
    "# PersistentVolumeClaim: Durable storage (survives pod deletion)\n",
    "volumes:\n",
    "- name: model-data\n",
    "  persistentVolumeClaim:\n",
    "    claimName: model-pvc\n",
    "```\n",
    "\n",
    "**PersistentVolumeClaim (PVC):**\n",
    "```yaml\n",
    "# Request storage\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: model-pvc\n",
    "spec:\n",
    "  accessModes:\n",
    "  - ReadWriteOnce  # RWO (single node), ReadWriteMany (multi-node)\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 100Gi\n",
    "  storageClassName: fast-ssd  # AWS gp3, GCP pd-ssd\n",
    "\n",
    "# Use in StatefulSet\n",
    "apiVersion: apps/v1\n",
    "kind: StatefulSet\n",
    "metadata:\n",
    "  name: redis\n",
    "spec:\n",
    "  serviceName: redis\n",
    "  replicas: 3\n",
    "  volumeClaimTemplates:  # Automatic PVC per pod\n",
    "  - metadata:\n",
    "      name: redis-data\n",
    "    spec:\n",
    "      accessModes: [\"ReadWriteOnce\"]\n",
    "      resources:\n",
    "        requests:\n",
    "          storage: 50Gi\n",
    "```\n",
    "\n",
    "**Storage Classes:**\n",
    "- **AWS:** gp3 (general purpose SSD), io2 (high IOPS), st1 (throughput optimized HDD)\n",
    "- **GCP:** pd-standard (HDD), pd-ssd (SSD), pd-balanced (balanced SSD)\n",
    "- **Azure:** Standard_LRS (HDD), Premium_LRS (SSD)\n",
    "\n",
    "**When to Use:**\n",
    "- **emptyDir:** Temporary cache, scratch space (deleted on pod restart)\n",
    "- **PVC:** Model files, database data, logs (persist beyond pod lifecycle)\n",
    "- **StatefulSet + PVC:** Each pod needs own storage (databases, caches)\n",
    "\n",
    "---\n",
    "\n",
    "### Section 8: Namespaces for Multi-Tenancy\n",
    "\n",
    "**Create Namespace:**\n",
    "```bash\n",
    "kubectl create namespace dev\n",
    "kubectl create namespace staging\n",
    "kubectl create namespace production\n",
    "```\n",
    "\n",
    "**Deploy to Namespace:**\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: wafer-model\n",
    "  namespace: production  # Deploy to production namespace\n",
    "```\n",
    "\n",
    "**Resource Quotas (Prevent Resource Exhaustion):**\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: ResourceQuota\n",
    "metadata:\n",
    "  name: production-quota\n",
    "  namespace: production\n",
    "spec:\n",
    "  hard:\n",
    "    requests.cpu: \"100\"       # Max 100 CPU cores requested\n",
    "    requests.memory: \"200Gi\"  # Max 200GB memory requested\n",
    "    persistentvolumeclaims: \"20\"  # Max 20 PVCs\n",
    "    pods: \"100\"               # Max 100 pods\n",
    "```\n",
    "\n",
    "**Limit Ranges (Default/Max Resources per Pod):**\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: LimitRange\n",
    "metadata:\n",
    "  name: production-limits\n",
    "  namespace: production\n",
    "spec:\n",
    "  limits:\n",
    "  - max:\n",
    "      cpu: \"4\"\n",
    "      memory: \"8Gi\"\n",
    "    min:\n",
    "      cpu: \"100m\"\n",
    "      memory: \"128Mi\"\n",
    "    default:  # Applied if not specified\n",
    "      cpu: \"500m\"\n",
    "      memory: \"512Mi\"\n",
    "    type: Container\n",
    "```\n",
    "\n",
    "**Use Cases:**\n",
    "- **Environment separation:** dev, staging, production (prevent accidental production updates)\n",
    "- **Team isolation:** team-a, team-b (each team has their own namespace)\n",
    "- **Cost tracking:** Label namespaces, track spending per team/project\n",
    "\n",
    "---\n",
    "\n",
    "### Section 9: Jobs and CronJobs for Batch Processing\n",
    "\n",
    "**Job (Run to Completion):**\n",
    "```yaml\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: wafer-analysis\n",
    "spec:\n",
    "  parallelism: 10      # 10 pods run in parallel\n",
    "  completions: 100     # 100 total tasks\n",
    "  backoffLimit: 3      # Retry up to 3 times on failure\n",
    "  template:\n",
    "    spec:\n",
    "      restartPolicy: OnFailure\n",
    "      containers:\n",
    "      - name: analyzer\n",
    "        image: registry.io/wafer-analyzer:v1.0\n",
    "        args: [\"--process-wafer\", \"$(WAFER_ID)\"]\n",
    "```\n",
    "\n",
    "**CronJob (Scheduled Job):**\n",
    "```yaml\n",
    "apiVersion: batch/v1\n",
    "kind: CronJob\n",
    "metadata:\n",
    "  name: nightly-reports\n",
    "spec:\n",
    "  schedule: \"0 2 * * *\"  # 2 AM daily (cron syntax)\n",
    "  jobTemplate:\n",
    "    spec:\n",
    "      template:\n",
    "        spec:\n",
    "          restartPolicy: OnFailure\n",
    "          containers:\n",
    "          - name: report-generator\n",
    "            image: registry.io/report-gen:v1.0\n",
    "```\n",
    "\n",
    "**Cron Syntax:**\n",
    "```\n",
    "# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minute (0 - 59)\n",
    "# â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ hour (0 - 23)\n",
    "# â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ day of month (1 - 31)\n",
    "# â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ month (1 - 12)\n",
    "# â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ day of week (0 - 6, Sun-Sat)\n",
    "# â”‚ â”‚ â”‚ â”‚ â”‚\n",
    "# * * * * *\n",
    "\n",
    "\"0 2 * * *\"      # 2 AM daily\n",
    "\"*/15 * * * *\"   # Every 15 minutes\n",
    "\"0 0 * * 0\"      # Midnight every Sunday\n",
    "\"0 */6 * * *\"    # Every 6 hours\n",
    "```\n",
    "\n",
    "**When to Use:**\n",
    "- **Jobs:** One-time batch processing (train model, process dataset)\n",
    "- **CronJobs:** Scheduled tasks (nightly reports, model retraining)\n",
    "\n",
    "---\n",
    "\n",
    "### Section 10: Resource Requests and Limits\n",
    "\n",
    "**Requests vs Limits:**\n",
    "```yaml\n",
    "resources:\n",
    "  requests:  # Guaranteed resources (scheduler uses for placement)\n",
    "    cpu: \"1\"\n",
    "    memory: \"2Gi\"\n",
    "  limits:    # Maximum allowed (enforced by kubelet)\n",
    "    cpu: \"2\"\n",
    "    memory: \"4Gi\"\n",
    "```\n",
    "\n",
    "**What Happens:**\n",
    "- **CPU request:** Scheduler ensures node has 1 core available before scheduling\n",
    "- **CPU limit:** Pod throttled if exceeds 2 cores (slowed down, not killed)\n",
    "- **Memory request:** Scheduler ensures node has 2GB available\n",
    "- **Memory limit:** Pod killed (OOMKilled) if exceeds 4GB\n",
    "\n",
    "**QoS Classes (Priority During Resource Contention):**\n",
    "1. **Guaranteed:** requests = limits (highest priority, killed last)\n",
    "2. **Burstable:** requests < limits (medium priority)\n",
    "3. **BestEffort:** No requests/limits (lowest priority, killed first)\n",
    "\n",
    "**Best Practices:**\n",
    "- âœ… **Always set requests:** Prevent overcommitment (10 pods Ã— 2GB = 20GB, but node has 16GB)\n",
    "- âœ… **Set memory limits:** Prevent OOM on node (one pod uses all memory, kills others)\n",
    "- âœ… **CPU limits optional:** Throttling can reduce performance, often skip limits\n",
    "- âœ… **Profile your app:** Use Metrics Server to determine actual usage\n",
    "\n",
    "**View Resource Usage:**\n",
    "```bash\n",
    "kubectl top nodes\n",
    "kubectl top pods -n production\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Section 11: RBAC (Role-Based Access Control)\n",
    "\n",
    "**Concepts:**\n",
    "- **ServiceAccount:** Identity for pods (like IAM role for containers)\n",
    "- **Role:** Permissions within namespace (can create pods in namespace \"dev\")\n",
    "- **ClusterRole:** Permissions cluster-wide (can list nodes)\n",
    "- **RoleBinding:** Assign Role to user/ServiceAccount\n",
    "- **ClusterRoleBinding:** Assign ClusterRole cluster-wide\n",
    "\n",
    "**Example - Read-Only Access to Production:**\n",
    "```yaml\n",
    "# Role: View pods in production\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: Role\n",
    "metadata:\n",
    "  name: pod-reader\n",
    "  namespace: production\n",
    "rules:\n",
    "- apiGroups: [\"\"]\n",
    "  resources: [\"pods\", \"pods/log\"]\n",
    "  verbs: [\"get\", \"list\", \"watch\"]\n",
    "\n",
    "# RoleBinding: Assign to data-science team\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: RoleBinding\n",
    "metadata:\n",
    "  name: read-pods\n",
    "  namespace: production\n",
    "subjects:\n",
    "- kind: Group\n",
    "  name: data-science-team\n",
    "  apiGroup: rbac.authorization.k8s.io\n",
    "roleRef:\n",
    "  kind: Role\n",
    "  name: pod-reader\n",
    "  apiGroup: rbac.authorization.k8s.io\n",
    "```\n",
    "\n",
    "**ServiceAccount for Pods:**\n",
    "```yaml\n",
    "# ServiceAccount\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: model-server-sa\n",
    "  namespace: production\n",
    "\n",
    "# Use in Deployment\n",
    "spec:\n",
    "  serviceAccountName: model-server-sa\n",
    "  containers:\n",
    "  - name: model\n",
    "    image: registry.io/wafer-model:v2.3\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "- âœ… **Principle of least privilege:** Grant minimum permissions needed\n",
    "- âœ… **Separate ServiceAccounts:** Different apps get different ServiceAccounts\n",
    "- âœ… **Avoid default ServiceAccount:** Create specific ServiceAccounts\n",
    "- âœ… **Audit RBAC:** Regularly review who has access to what\n",
    "\n",
    "---\n",
    "\n",
    "### Section 12: Monitoring with Prometheus and Grafana\n",
    "\n",
    "**Prometheus Architecture:**\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Pods      â”‚ expose /metrics endpoint\n",
    "â”‚ /metrics    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "       â”‚ Prometheus scrapes every 15s\n",
    "       â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Prometheus  â”‚ stores time-series data\n",
    "â”‚  Server     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "       â”‚ PromQL queries\n",
    "       â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Grafana   â”‚ visualizes metrics\n",
    "â”‚ Dashboards  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Expose Metrics:**\n",
    "```python\n",
    "from prometheus_client import Counter, Histogram, generate_latest\n",
    "\n",
    "REQUEST_COUNT = Counter('http_requests_total', 'Total requests')\n",
    "REQUEST_LATENCY = Histogram('http_request_duration_seconds', 'Request latency')\n",
    "\n",
    "@app.route('/metrics')\n",
    "def metrics():\n",
    "    return generate_latest()\n",
    "```\n",
    "\n",
    "**ServiceMonitor (Auto-Discovery):**\n",
    "```yaml\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: ServiceMonitor\n",
    "metadata:\n",
    "  name: wafer-model-metrics\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: wafer-model\n",
    "  endpoints:\n",
    "  - port: metrics\n",
    "    interval: 15s\n",
    "```\n",
    "\n",
    "**Key Metrics for ML:**\n",
    "- **Request rate:** `rate(http_requests_total[5m])`\n",
    "- **Error rate:** `rate(http_requests_total{status=\"500\"}[5m])`\n",
    "- **Latency percentiles:** `histogram_quantile(0.99, http_request_duration_seconds)`\n",
    "- **Model accuracy:** Custom gauge `model_accuracy{model=\"wafer_yield\"}`\n",
    "- **Prediction distribution:** `prediction_count{label=\"Pass\"}` vs `prediction_count{label=\"Fail\"}`\n",
    "\n",
    "---\n",
    "\n",
    "### Section 13: Cost Optimization Strategies\n",
    "\n",
    "**1. Right-Sizing (Avoid Over-Provisioning):**\n",
    "```bash\n",
    "# Check actual usage\n",
    "kubectl top pods -n production\n",
    "\n",
    "# If pod requests 2 cores but uses 0.5 cores, reduce request\n",
    "resources:\n",
    "  requests:\n",
    "    cpu: \"500m\"  # Reduced from 2 cores\n",
    "    memory: \"1Gi\"\n",
    "```\n",
    "\n",
    "**2. Horizontal Pod Autoscaling:**\n",
    "- Scale down during low traffic (50 pods â†’ 5 pods, 90% cost savings)\n",
    "- Scale up during peaks (handle load without over-provisioning)\n",
    "\n",
    "**3. Cluster Autoscaling:**\n",
    "- Add nodes when pods pending (insufficient resources)\n",
    "- Remove nodes when under-utilized (move pods, terminate node)\n",
    "\n",
    "**4. Spot Instances (70% Discount):**\n",
    "```yaml\n",
    "nodeSelector:\n",
    "  node.kubernetes.io/lifecycle: spot\n",
    "tolerations:\n",
    "- key: \"spot\"\n",
    "  operator: \"Equal\"\n",
    "  value: \"true\"\n",
    "  effect: \"NoSchedule\"\n",
    "```\n",
    "\n",
    "**5. Storage Optimization:**\n",
    "- Delete unused PVCs (orphaned volumes)\n",
    "- Use lifecycle policies (delete old snapshots)\n",
    "- Compress logs before storing\n",
    "\n",
    "**6. Multi-Tenancy (Resource Sharing):**\n",
    "- Run dev/staging/production on same cluster (separate namespaces)\n",
    "- Share cluster overhead (control plane, monitoring) across teams\n",
    "\n",
    "**Cost Comparison:**\n",
    "| Strategy | Monthly Cost | Savings |\n",
    "|----------|--------------|---------|\n",
    "| Static 50 pods (m5.2xlarge) | $5,000 | - |\n",
    "| HPA (avg 15 pods) | $1,500 | 70% |\n",
    "| HPA + Spot instances | $450 | 91% |\n",
    "| HPA + Spot + Right-sizing | $300 | 94% |\n",
    "\n",
    "---\n",
    "\n",
    "### Section 14: Troubleshooting Common Issues\n",
    "\n",
    "**Issue 1: Pod Stuck in Pending**\n",
    "```bash\n",
    "# Check pod status\n",
    "kubectl describe pod <pod-name>\n",
    "\n",
    "# Common causes:\n",
    "# - Insufficient resources (CPU/memory)\n",
    "# - Node selector mismatch (no nodes match label)\n",
    "# - PVC not bound (storage class issue)\n",
    "# - Image pull error (authentication)\n",
    "\n",
    "# Solutions:\n",
    "# - Add nodes or reduce resource requests\n",
    "# - Fix node labels or remove node selector\n",
    "# - Check storage class, PVC status\n",
    "# - Verify image pull secret\n",
    "```\n",
    "\n",
    "**Issue 2: Pod CrashLoopBackOff**\n",
    "```bash\n",
    "# View logs\n",
    "kubectl logs <pod-name>\n",
    "kubectl logs <pod-name> --previous  # Previous crashed container\n",
    "\n",
    "# Common causes:\n",
    "# - Application error (bugs, missing config)\n",
    "# - Liveness probe failing\n",
    "# - OOMKilled (exceeds memory limit)\n",
    "\n",
    "# Solutions:\n",
    "# - Fix application code\n",
    "# - Adjust probe thresholds\n",
    "# - Increase memory limit\n",
    "```\n",
    "\n",
    "**Issue 3: Service Not Reachable**\n",
    "```bash\n",
    "# Check service endpoints\n",
    "kubectl get endpoints <service-name>\n",
    "\n",
    "# If no endpoints:\n",
    "# - Selector mismatch (Service selector doesn't match Pod labels)\n",
    "# - Pods not ready (readiness probe failing)\n",
    "\n",
    "# Test from another pod\n",
    "kubectl run -it --rm debug --image=busybox --restart=Never -- wget -O- http://service-name:8080\n",
    "```\n",
    "\n",
    "**Issue 4: High Latency**\n",
    "```bash\n",
    "# Check pod resource usage\n",
    "kubectl top pods\n",
    "\n",
    "# If CPU throttled:\n",
    "# - Increase CPU limit or remove limit\n",
    "# - Scale out (more replicas)\n",
    "\n",
    "# If network latency:\n",
    "# - Enable network policies (restrict traffic)\n",
    "# - Use Service mesh (Istio) for advanced routing\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Section 15: Production Deployment Checklist\n",
    "\n",
    "**Before Going to Production:**\n",
    "\n",
    "âœ… **Reliability:**\n",
    "- [ ] Min 3 replicas (survive pod failures)\n",
    "- [ ] PodDisruptionBudget configured (min 2 available during node drain)\n",
    "- [ ] Liveness and readiness probes configured\n",
    "- [ ] Resource requests and limits set\n",
    "- [ ] Anti-affinity rules (spread pods across zones)\n",
    "\n",
    "âœ… **Scalability:**\n",
    "- [ ] HPA configured (auto-scale based on CPU/custom metrics)\n",
    "- [ ] Cluster autoscaler enabled (auto-scale nodes)\n",
    "- [ ] Load testing performed (verify handles expected traffic)\n",
    "\n",
    "âœ… **Security:**\n",
    "- [ ] RBAC configured (least privilege)\n",
    "- [ ] Network policies (restrict pod-to-pod traffic)\n",
    "- [ ] Secrets for credentials (not environment variables)\n",
    "- [ ] Container images scanned (Trivy, Snyk)\n",
    "- [ ] Non-root user in containers\n",
    "\n",
    "âœ… **Observability:**\n",
    "- [ ] Prometheus metrics exposed\n",
    "- [ ] Grafana dashboards created\n",
    "- [ ] Alerts configured (error rate, latency, resource usage)\n",
    "- [ ] Distributed tracing (Jaeger, Zipkin)\n",
    "- [ ] Log aggregation (ELK, Loki)\n",
    "\n",
    "âœ… **Deployment:**\n",
    "- [ ] Rolling update strategy (zero downtime)\n",
    "- [ ] Rollback tested (verify can revert quickly)\n",
    "- [ ] Blue-green or canary deployment (for critical services)\n",
    "- [ ] Staging environment (test before production)\n",
    "\n",
    "âœ… **Disaster Recovery:**\n",
    "- [ ] Backups configured (etcd, persistent volumes)\n",
    "- [ ] Multi-AZ deployment (survive zone failures)\n",
    "- [ ] Runbook documented (incident response procedures)\n",
    "- [ ] Chaos engineering tested (simulate failures)\n",
    "\n",
    "---\n",
    "\n",
    "### Section 16: Kubernetes vs Alternatives\n",
    "\n",
    "**When to Use Kubernetes:**\n",
    "- âœ… Microservices (100+ services, need orchestration)\n",
    "- âœ… Multi-cloud (same YAML runs on AWS, GCP, Azure)\n",
    "- âœ… Auto-scaling (handle variable load)\n",
    "- âœ… High availability (99.99% uptime requirement)\n",
    "\n",
    "**When to Consider Alternatives:**\n",
    "- **AWS ECS:** Simpler, AWS-only (easier for small teams, less portable)\n",
    "- **Docker Compose:** Local development (single machine, not production)\n",
    "- **Serverless (Lambda, Cloud Functions):** Event-driven, auto-scaling (no cluster management)\n",
    "- **Nomad (HashiCorp):** Lighter than Kubernetes (easier to operate, less ecosystem)\n",
    "\n",
    "**Kubernetes Learning Curve:**\n",
    "- Week 1: Pods, Deployments, Services (basic deployments)\n",
    "- Week 2-4: HPA, ConfigMaps, Secrets, Volumes (production features)\n",
    "- Month 2-3: StatefulSets, Ingress, RBAC, Operators (advanced patterns)\n",
    "- Month 4-6: Multi-cluster, Service mesh, GitOps (expert level)\n",
    "\n",
    "---\n",
    "\n",
    "### Section 17: Next Steps - Kubernetes Advanced Patterns\n",
    "\n",
    "**What We've Learned (Fundamentals):**\n",
    "- âœ… Kubernetes architecture (control plane, nodes, pods)\n",
    "- âœ… Deployments and rolling updates (zero-downtime deployments)\n",
    "- âœ… Services and networking (load balancing, service discovery)\n",
    "- âœ… Auto-scaling (HPA for cost optimization)\n",
    "- âœ… ConfigMaps, Secrets (externalize configuration)\n",
    "\n",
    "**What's Next (Advanced - Notebook 133):**\n",
    "- **StatefulSets:** Databases, caches with stable network IDs\n",
    "- **DaemonSets:** Run pod on every node (monitoring agents, log collectors)\n",
    "- **Operators:** Automate complex applications (Postgres Operator, Redis Operator)\n",
    "- **Custom Resource Definitions (CRDs):** Extend Kubernetes API\n",
    "- **Init Containers:** Pre-flight checks before main container starts\n",
    "- **Admission Controllers:** Policy enforcement (mutate/validate resources)\n",
    "- **Network Policies:** Firewall rules for pods (restrict traffic)\n",
    "- **Pod Security Policies:** Security standards enforcement\n",
    "\n",
    "**The Journey Continues:**\n",
    "```\n",
    "Notebook 132: Kubernetes Fundamentals âœ…\n",
    "  â†“\n",
    "Notebook 133: Kubernetes Advanced (StatefulSets, Operators, CRDs)\n",
    "  â†“\n",
    "Notebook 134: Service Mesh with Istio (traffic management, mTLS, observability)\n",
    "  â†“\n",
    "Notebook 135: GitOps & ArgoCD (declarative, automated deployments)\n",
    "  â†“\n",
    "Notebook 136: CI/CD for Kubernetes (Jenkins, GitHub Actions, Tekton)\n",
    "  â†“\n",
    "Notebook 137: Multi-Cloud Kubernetes (EKS, GKE, AKS comparison)\n",
    "  â†“\n",
    "Notebook 138: Production ML on Kubernetes (Kubeflow, KServe, Seldon)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Section 18: Quick Reference - Essential kubectl Commands\n",
    "\n",
    "**Cluster Info:**\n",
    "```bash\n",
    "kubectl cluster-info\n",
    "kubectl get nodes\n",
    "kubectl top nodes\n",
    "kubectl describe node <node-name>\n",
    "```\n",
    "\n",
    "**Pod Management:**\n",
    "```bash\n",
    "kubectl get pods -n production\n",
    "kubectl describe pod <pod-name>\n",
    "kubectl logs <pod-name> -f\n",
    "kubectl logs <pod-name> -c <container-name>\n",
    "kubectl exec -it <pod-name> -- bash\n",
    "kubectl delete pod <pod-name>\n",
    "kubectl port-forward <pod-name> 8080:8080\n",
    "```\n",
    "\n",
    "**Deployment Management:**\n",
    "```bash\n",
    "kubectl create deployment wafer-model --image=registry.io/wafer-model:v2.3\n",
    "kubectl get deployments\n",
    "kubectl describe deployment wafer-model\n",
    "kubectl scale deployment wafer-model --replicas=10\n",
    "kubectl set image deployment/wafer-model model=registry.io/wafer-model:v2.4\n",
    "kubectl rollout status deployment/wafer-model\n",
    "kubectl rollout history deployment/wafer-model\n",
    "kubectl rollout undo deployment/wafer-model\n",
    "kubectl delete deployment wafer-model\n",
    "```\n",
    "\n",
    "**Service Management:**\n",
    "```bash\n",
    "kubectl expose deployment wafer-model --port=8080 --type=LoadBalancer\n",
    "kubectl get services\n",
    "kubectl describe service wafer-model\n",
    "kubectl delete service wafer-model\n",
    "```\n",
    "\n",
    "**ConfigMap & Secret:**\n",
    "```bash\n",
    "kubectl create configmap model-config --from-file=config.yaml\n",
    "kubectl create secret generic db-creds --from-literal=password=secret123\n",
    "kubectl get configmaps\n",
    "kubectl get secrets\n",
    "kubectl describe configmap model-config\n",
    "```\n",
    "\n",
    "**Namespace:**\n",
    "```bash\n",
    "kubectl create namespace production\n",
    "kubectl get namespaces\n",
    "kubectl config set-context --current --namespace=production\n",
    "kubectl delete namespace dev\n",
    "```\n",
    "\n",
    "**Apply YAML:**\n",
    "```bash\n",
    "kubectl apply -f deployment.yaml\n",
    "kubectl apply -f . (all YAML in directory)\n",
    "kubectl delete -f deployment.yaml\n",
    "```\n",
    "\n",
    "**Debugging:**\n",
    "```bash\n",
    "kubectl describe pod <pod-name>  # Events, status\n",
    "kubectl logs <pod-name> --previous  # Logs from crashed container\n",
    "kubectl top pods  # Resource usage\n",
    "kubectl get events --sort-by='.lastTimestamp'\n",
    "kubectl run -it --rm debug --image=busybox --restart=Never -- sh\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ You've Mastered Kubernetes Fundamentals!**\n",
    "\n",
    "**Key Achievements:**\n",
    "- âœ… Deployed ML models on Kubernetes (10 replicas, auto-scaling)\n",
    "- âœ… Implemented zero-downtime updates (rolling updates, rollbacks)\n",
    "- âœ… Configured auto-scaling (HPA, 70% cost savings)\n",
    "- âœ… Mastered Kubernetes architecture (control plane, nodes, pods, services)\n",
    "- âœ… Built production-ready deployments (health checks, resource limits, monitoring)\n",
    "\n",
    "**Real-World Impact:**\n",
    "- **Scalability:** 5 â†’ 500 pods automatically (handle 100x traffic spike)\n",
    "- **Availability:** 99.99% uptime (4 minutes downtime/year)\n",
    "- **Cost:** 70-90% savings with HPA + spot instances\n",
    "- **Speed:** 5-minute deployments (vs 8-hour manual deployments)\n",
    "- **Reliability:** Self-healing (crashed pods auto-restart in 10 seconds)\n",
    "\n",
    "**Keep Learning:** Notebook 133 awaits - Advanced Kubernetes patterns for production ML! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4607d418",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "### When to Use Kubernetes for ML\n",
    "- **Multi-model serving**: Deploy 10+ models with different resource needs (CPU-bound vs. GPU-bound)\n",
    "- **Autoscaling**: Handle variable inference traffic (10 req/s â†’ 1000 req/s spikes)\n",
    "- **Resource efficiency**: Share GPU cluster across training jobs, maximize utilization >80%\n",
    "- **Team collaboration**: Multiple data scientists sharing infrastructure (namespace isolation)\n",
    "- **CI/CD integration**: Automated model deployment pipelines (Jenkins, GitLab CI â†’ K8s)\n",
    "\n",
    "### Limitations\n",
    "- **Complexity overhead**: Learning curve steep (pods, services, ingress, persistent volumes)\n",
    "- **Operational burden**: Managing K8s cluster requires DevOps expertise (upgrades, security patches)\n",
    "- **Cost**: Managed K8s (EKS, GKE, AKS) adds $70-150/month per cluster + node costs\n",
    "- **Overkill for simple use cases**: Single model serving doesn't need full orchestration\n",
    "- **GPU scheduling challenges**: Sharing GPUs across pods complex (requires device plugins)\n",
    "\n",
    "### Alternatives\n",
    "- **Docker Compose**: Simpler multi-container orchestration for development/small deployments\n",
    "- **Serverless (AWS Lambda, Cloud Run)**: For lightweight models, pay-per-inference, no cluster management\n",
    "- **Dedicated VM**: Single model on VM with systemd service (simple, predictable)\n",
    "- **Managed ML platforms**: SageMaker, Vertex AI handle infrastructure (less control, easier operation)\n",
    "\n",
    "### Best Practices\n",
    "- **Resource requests/limits**: Set CPU/memory requests for scheduling, limits to prevent OOM kills\n",
    "- **Health checks**: Liveness (restart if unhealthy) + readiness (traffic only when ready)\n",
    "- **Horizontal pod autoscaling**: Scale replicas based on CPU/memory/custom metrics (e.g., queue depth)\n",
    "- **Namespace isolation**: Separate dev/staging/prod environments\n",
    "- **ConfigMaps/Secrets**: Externalize configuration (model paths, API keys) from container images\n",
    "- **Persistent volumes**: Use for model artifacts, training data (StatefulSets for databases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830925c7",
   "metadata": {},
   "source": [
    "## ðŸ” Diagnostic Checks & Mastery\n",
    "\n",
    "### Implementation Checklist\n",
    "- âœ… **K8s cluster**: Minikube (local) or managed (EKS, GKE, AKS)\n",
    "- âœ… **ML deployment**: Deploy model as Deployment + Service (replicas for HA)\n",
    "- âœ… **Resource management**: Set requests/limits for CPU, memory, GPU\n",
    "- âœ… **Autoscaling**: Configure HPA based on CPU/memory metrics\n",
    "- âœ… **Storage**: PersistentVolumeClaims for model artifacts, training data\n",
    "- âœ… **ConfigMaps/Secrets**: Externalize config (API keys, model paths)\n",
    "\n",
    "### Post-Silicon Applications\n",
    "**ATE Test Server Deployment**: Deploy yield prediction models across 10 ATE test cells, autoscale based on test queue depth, save $1.5M/year ATE utilization\n",
    "\n",
    "### Mastery Achievement\n",
    "âœ… Deploy ML models to Kubernetes with replicas and load balancing  \n",
    "âœ… Configure autoscaling (HPA) for variable inference traffic  \n",
    "âœ… Manage GPU resources efficiently across training/serving workloads  \n",
    "âœ… Use ConfigMaps/Secrets for configuration management  \n",
    "âœ… Apply to semiconductor test infrastructure deployment  \n",
    "\n",
    "**Next Steps**: 133_Kubernetes_Advanced_Patterns, 134_Service_Mesh_Istio_Linkerd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b913f36",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Progress Update\n",
    "\n",
    "**Session Summary:**\n",
    "- âœ… Completed 29 notebooks total (previous 21 + current batch: 132, 134-136, 139, 144-145, 174)\n",
    "- âœ… Current notebook: 132/175 complete\n",
    "- âœ… Overall completion: ~82.9% (145/175 notebooks â‰¥15 cells)\n",
    "\n",
    "**Remaining Work:**\n",
    "- ðŸ”„ Next: Process remaining 9-cell and below notebooks\n",
    "- ðŸŽ¯ Target: 100% completion (175/175 notebooks)\n",
    "\n",
    "Excellent progress - over 80% complete! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu-training-job.yaml\n",
    "\"\"\"\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: wafer-cnn-training\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: pytorch-training\n",
    "        image: pytorch/pytorch:2.0-cuda11.7\n",
    "        command: [\"python\", \"/app/train_wafer_cnn.py\"]\n",
    "        resources:\n",
    "          limits:\n",
    "            nvidia.com/gpu: 2  # Request 2 GPUs\n",
    "          requests:\n",
    "            memory: \"16Gi\"\n",
    "            cpu: \"4\"\n",
    "        volumeMounts:\n",
    "        - name: training-data\n",
    "          mountPath: /data\n",
    "        - name: model-output\n",
    "          mountPath: /models\n",
    "      nodeSelector:\n",
    "        gpu: \"true\"  # Schedule on GPU nodes only\n",
    "      volumes:\n",
    "      - name: training-data\n",
    "        persistentVolumeClaim:\n",
    "          claimName: wafer-data-pvc\n",
    "      - name: model-output\n",
    "        persistentVolumeClaim:\n",
    "          claimName: model-storage-pvc\n",
    "      restartPolicy: Never\n",
    "  backoffLimit: 3  # Retry up to 3 times on failure\n",
    "\"\"\"\n",
    "\n",
    "# Apply with:\n",
    "# kubectl apply -f gpu-training-job.yaml\n",
    "\n",
    "# Monitor job status\n",
    "# kubectl get jobs\n",
    "# kubectl logs job/wafer-cnn-training\n",
    "\n",
    "# Post-Silicon Use Case:\n",
    "# Train CNN on 50K wafer map images for defect classification\n",
    "# 2 GPUs reduce training from 8 hours (CPU) to 45 minutes\n",
    "# Deploy weekly training jobs triggered by new ATE data\n",
    "# Save $120K/year (4 hours SRE time/week Ã— $150K salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54eb2d1",
   "metadata": {},
   "source": [
    "## ðŸ­ Advanced Example: GPU-Enabled ML Training Job\n",
    "\n",
    "Deploy PyTorch distributed training on Kubernetes with GPU node affinity."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
