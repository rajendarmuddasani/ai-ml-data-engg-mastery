{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9073570",
   "metadata": {},
   "source": [
    "# 136: CI/CD for ML - Tekton and GitHub Actions\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- **Understand** CI/CD principles for ML (data validation, model training, quality gates, deployment)\n",
    "- **Build** Tekton pipelines for Kubernetes-native ML workflows (parallel tasks, GPU scheduling, artifact passing)\n",
    "- **Implement** GitHub Actions workflows for cloud-based ML automation (matrix builds, artifact caching, secrets management)\n",
    "- **Apply** CI/CD to post-silicon validation (automated STDF parsing, yield model retraining, canary deployments)\n",
    "- **Master** MLOps patterns (experiment tracking, model registry, GitOps, monitoring)\n",
    "- **Deploy** production ML systems with quality gates and automated rollback\n",
    "\n",
    "## üìö What is CI/CD for ML?\n",
    "\n",
    "**CI/CD (Continuous Integration/Continuous Deployment)** for ML extends traditional software CI/CD with ML-specific stages: data validation, model training, model evaluation, and model registry. Unlike traditional CI/CD that focuses on code testing and deployment, **ML CI/CD treats data and models as first-class citizens** requiring versioning, validation, and monitoring.\n",
    "\n",
    "Traditional software CI/CD pipeline:\n",
    "```\n",
    "Code ‚Üí Unit Tests ‚Üí Integration Tests ‚Üí Build ‚Üí Deploy ‚Üí Monitor\n",
    "```\n",
    "\n",
    "ML CI/CD pipeline:\n",
    "```\n",
    "Code + Data ‚Üí Schema Validation ‚Üí Model Training ‚Üí Evaluation ‚Üí Quality Gates ‚Üí \n",
    "Model Registry ‚Üí Canary Deployment ‚Üí Full Deployment ‚Üí Drift Monitoring\n",
    "```\n",
    "\n",
    "**Key differences:**\n",
    "- **Data Validation**: Check data schema, quality, distribution shifts (prevent training on corrupt data)\n",
    "- **Model Training**: Reproducible pipelines with versioned data, code, and hyperparameters\n",
    "- **Quality Gates**: Deploy only if model beats baseline accuracy + passes latency thresholds\n",
    "- **Model Registry**: Version control for models (MLflow, DVC) with metadata and lineage\n",
    "- **Canary Deployments**: Gradual rollout (10% ‚Üí 25% ‚Üí 100% traffic) with automated rollback\n",
    "- **Drift Monitoring**: Track model performance degradation, trigger retraining when accuracy drops\n",
    "\n",
    "**Why CI/CD for ML?**\n",
    "- ‚úÖ **Reproducibility**: Retrain exact same model 6 months later (versioned data + code + hyperparameters)\n",
    "- ‚úÖ **Quality**: Automated quality gates prevent deploying worse models (accuracy regression, latency spikes)\n",
    "- ‚úÖ **Speed**: Automated pipelines reduce deployment time from days to hours\n",
    "- ‚úÖ **Safety**: Canary deployments catch issues before affecting all users (gradual rollout)\n",
    "- ‚úÖ **Monitoring**: Continuous model performance tracking detects drift early\n",
    "\n",
    "## üè≠ Post-Silicon Validation Use Cases\n",
    "\n",
    "### **Use Case 1: Automated Yield Prediction Model Retraining**\n",
    "- **Input**: Daily STDF wafer test data (5K devices, 50+ parametric measurements per device)\n",
    "- **Pipeline**: Data validation ‚Üí Feature engineering ‚Üí Model training (RandomForest) ‚Üí Evaluation vs baseline ‚Üí Canary deployment (10% fab traffic)\n",
    "- **Quality Gate**: Deploy only if accuracy ‚â• baseline + 1% (e.g., 97% new model vs 96% baseline)\n",
    "- **Value**: Continuous model improvement from fresh data ‚Üí 0.3% accuracy gain ‚Üí $420K/year savings (fewer false positives in yield prediction)\n",
    "- **Automation**: Tekton CronJob triggers daily at 2 AM, completes training + deployment in 30 minutes\n",
    "\n",
    "### **Use Case 2: STDF Data Pipeline with Quality Gates**\n",
    "- **Input**: STDF parser library code changes (new features, bug fixes)\n",
    "- **Pipeline**: Unit tests (500+ test cases) ‚Üí Integration tests (parse real STDF files) ‚Üí Performance tests (<5s for 10K records) ‚Üí Security scan (CVE check)\n",
    "- **Quality Gate**: PR approved only if all tests pass + code coverage ‚â•90%\n",
    "- **Value**: Zero STDF parsing bugs in production ‚Üí $180K/year savings (avoided engineering time debugging corrupt data)\n",
    "- **Automation**: GitHub Actions workflow triggers on every PR, provides fast feedback in 8 minutes\n",
    "\n",
    "### **Use Case 3: Canary Deployment for Wafer Defect Analyzer**\n",
    "- **Input**: New CNN model (ResNet-50) for wafer defect detection (98% accuracy vs 95% rule-based baseline)\n",
    "- **Pipeline**: Train CNN on 100K wafer images ‚Üí Validate on held-out test set ‚Üí Deploy to staging ‚Üí Canary (5% production traffic) ‚Üí Monitor false positive rate for 24 hours ‚Üí Increase to 25%, then 100%\n",
    "- **Quality Gate**: Rollback if false positive rate >2% or inference latency >500ms\n",
    "- **Value**: Reduce defect escape rate by 30% ‚Üí $2.1M/year savings (fewer bad dies shipped to customers)\n",
    "- **Automation**: Tekton pipeline + Flagger (automated canary) + ArgoCD (GitOps deployment)\n",
    "\n",
    "### **Use Case 4: Multi-Stage ML Pipeline with Experiment Tracking**\n",
    "- **Input**: Hyperparameter tuning for yield prediction (20 model variants: RandomForest √ó GradientBoosting √ó 10 hyperparameter combinations)\n",
    "- **Pipeline**: Parallel training (5 Tekton tasks with GPUs) ‚Üí MLflow logs all experiments (hyperparameters, metrics, artifacts) ‚Üí Select best model by F1 score ‚Üí Register in MLflow Model Registry ‚Üí Deploy via ArgoCD\n",
    "- **Quality Gate**: Model F1 score ‚â•0.96 required for production deployment\n",
    "- **Value**: Find optimal hyperparameters in 2 hours (vs 2 days manual tuning) ‚Üí $95K/year engineering time savings\n",
    "- **Automation**: GitHub Actions triggers Tekton pipeline on git push to main branch\n",
    "\n",
    "## üîÑ CI/CD Workflow\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Code + Data Change] --> B{CI Pipeline}\n",
    "    B --> C[Data Validation]\n",
    "    C --> D{Schema Valid?}\n",
    "    D -->|Yes| E[Model Training]\n",
    "    D -->|No| F[Pipeline Failed]\n",
    "    E --> G[Model Evaluation]\n",
    "    G --> H{Beats Baseline?}\n",
    "    H -->|Yes| I[Model Registry]\n",
    "    H -->|No| F\n",
    "    I --> J{CD Pipeline}\n",
    "    J --> K[Canary Deployment<br/>10% Traffic]\n",
    "    K --> L[Monitor 24h]\n",
    "    L --> M{Metrics OK?}\n",
    "    M -->|Yes| N[Full Deployment<br/>100% Traffic]\n",
    "    M -->|No| O[Rollback]\n",
    "    N --> P[Drift Monitoring]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style F fill:#ffe1e1\n",
    "    style N fill:#e1ffe1\n",
    "    style O fill:#fff5e1\n",
    "```\n",
    "\n",
    "## üìä Learning Path Context\n",
    "\n",
    "**Prerequisites:**\n",
    "- **Notebook 121-130**: MLOps fundamentals (model serving, feature stores, experiment tracking)\n",
    "- **Notebook 131**: Docker for ML (containerization, multi-stage builds, GPU support)\n",
    "- **Notebook 132-133**: Kubernetes for ML (deployments, services, resource management, autoscaling)\n",
    "- **Notebook 134**: Service Mesh (traffic management, observability, resilience)\n",
    "- **Notebook 135**: GitOps with ArgoCD and Flux (declarative deployments, automated sync)\n",
    "\n",
    "**Next Steps:**\n",
    "- **Notebook 137**: Infrastructure as Code - Terraform and Pulumi (automate cloud resource provisioning)\n",
    "- **Notebook 138**: Container Security & Compliance (image scanning, runtime security, network policies)\n",
    "\n",
    "---\n",
    "\n",
    "Let's build production-grade CI/CD pipelines for ML! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from enum import Enum\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Setup complete - Ready for CI/CD ML pipeline simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b9022",
   "metadata": {},
   "source": [
    "## 2. üîß CI/CD Fundamentals for ML - Data Validation and Model Gates\n",
    "\n",
    "### üìù What's Happening in This Section?\n",
    "\n",
    "**Purpose:** Implement ML-specific CI/CD stages: data validation (schema checks), model training (reproducible builds), and quality gates (accuracy thresholds).\n",
    "\n",
    "**Key Points:**\n",
    "- **Data Validation**: Check input data schema (required columns, data types, value ranges) before training\n",
    "- **Model Training**: Reproducible training pipeline (versioned data + code + hyperparameters)\n",
    "- **Model Evaluation**: Compare new model vs baseline (deploy only if accuracy improves)\n",
    "- **Quality Gates**: Block deployment if model fails thresholds (accuracy <99%, latency >150ms)\n",
    "- **Model Registry**: Version control for models (MLflow, DVC) with metadata (accuracy, training time)\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Prevent Bad Deployments**: Data schema violation detected ‚Üí pipeline fails before wasting GPU training time\n",
    "- **Reproducibility**: Retrain same model 6 months later (exact same results with versioned data/code)\n",
    "- **Automated Decision**: Deploy model if accuracy 99.5% > baseline 99.2% (no manual approval needed)\n",
    "- **Compliance**: Audit trail for semiconductor validation (model version, training data, approval criteria)\n",
    "\n",
    "**Post-Silicon Application:** STDF pipeline validates 10K wafer files/day ‚Üí checks schema compliance ‚Üí retrains yield model ‚Üí deploys only if accuracy improves (prevent regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c13beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CI/CD Fundamentals - Data Validation, Model Training, Quality Gates\n",
    "\n",
    "class PipelineStatus(Enum):\n",
    "    \"\"\"CI/CD pipeline execution status\"\"\"\n",
    "    PENDING = \"Pending\"\n",
    "    RUNNING = \"Running\"\n",
    "    SUCCEEDED = \"Succeeded\"\n",
    "    FAILED = \"Failed\"\n",
    "    SKIPPED = \"Skipped\"\n",
    "\n",
    "class DataValidationResult(Enum):\n",
    "    \"\"\"Data validation outcome\"\"\"\n",
    "    VALID = \"Valid\"\n",
    "    SCHEMA_VIOLATION = \"SchemaViolation\"\n",
    "    QUALITY_ISSUE = \"QualityIssue\"\n",
    "    INSUFFICIENT_DATA = \"InsufficientData\"\n",
    "\n",
    "@dataclass\n",
    "class DataSchema:\n",
    "    \"\"\"Expected data schema for validation\"\"\"\n",
    "    required_columns: List[str]\n",
    "    column_types: Dict[str, str]  # column_name -> dtype\n",
    "    value_ranges: Dict[str, Tuple[float, float]]  # column_name -> (min, max)\n",
    "    min_rows: int = 1000\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> Tuple[DataValidationResult, str]:\n",
    "        \"\"\"Validate DataFrame against schema\"\"\"\n",
    "        # Check required columns\n",
    "        missing_cols = set(self.required_columns) - set(df.columns)\n",
    "        if missing_cols:\n",
    "            return DataValidationResult.SCHEMA_VIOLATION, f\"Missing columns: {missing_cols}\"\n",
    "        \n",
    "        # Check column types\n",
    "        for col, expected_type in self.column_types.items():\n",
    "            if col in df.columns:\n",
    "                actual_type = str(df[col].dtype)\n",
    "                if expected_type not in actual_type:\n",
    "                    return DataValidationResult.SCHEMA_VIOLATION, f\"Column {col}: expected {expected_type}, got {actual_type}\"\n",
    "        \n",
    "        # Check value ranges\n",
    "        for col, (min_val, max_val) in self.value_ranges.items():\n",
    "            if col in df.columns:\n",
    "                if df[col].min() < min_val or df[col].max() > max_val:\n",
    "                    return DataValidationResult.QUALITY_ISSUE, f\"Column {col}: values outside range [{min_val}, {max_val}]\"\n",
    "        \n",
    "        # Check minimum rows\n",
    "        if len(df) < self.min_rows:\n",
    "            return DataValidationResult.INSUFFICIENT_DATA, f\"Only {len(df)} rows, need {self.min_rows}\"\n",
    "        \n",
    "        return DataValidationResult.VALID, \"Data validation passed\"\n",
    "\n",
    "@dataclass\n",
    "class ModelMetrics:\n",
    "    \"\"\"Model evaluation metrics\"\"\"\n",
    "    accuracy: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1_score: float\n",
    "    training_time_sec: float\n",
    "    inference_latency_ms: float\n",
    "    \n",
    "    def beats_baseline(self, baseline: 'ModelMetrics', min_improvement: float = 0.01) -> bool:\n",
    "        \"\"\"Check if this model beats baseline by minimum improvement\"\"\"\n",
    "        return self.accuracy >= (baseline.accuracy + min_improvement)\n",
    "    \n",
    "    def meets_thresholds(self, min_accuracy: float = 0.99, max_latency_ms: float = 150.0) -> bool:\n",
    "        \"\"\"Check if model meets production thresholds\"\"\"\n",
    "        return self.accuracy >= min_accuracy and self.inference_latency_ms <= max_latency_ms\n",
    "\n",
    "@dataclass\n",
    "class MLPipelineStage:\n",
    "    \"\"\"Single stage in ML CI/CD pipeline\"\"\"\n",
    "    name: str\n",
    "    status: PipelineStatus = PipelineStatus.PENDING\n",
    "    start_time: Optional[datetime] = None\n",
    "    end_time: Optional[datetime] = None\n",
    "    error_message: Optional[str] = None\n",
    "    artifacts: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def duration_seconds(self) -> float:\n",
    "        \"\"\"Get stage duration in seconds\"\"\"\n",
    "        if self.start_time and self.end_time:\n",
    "            return (self.end_time - self.start_time).total_seconds()\n",
    "        return 0.0\n",
    "\n",
    "class MLCIPipeline:\n",
    "    \"\"\"ML Continuous Integration Pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline_id: str, git_commit: str):\n",
    "        self.pipeline_id = pipeline_id\n",
    "        self.git_commit = git_commit\n",
    "        self.stages: List[MLPipelineStage] = []\n",
    "        self.overall_status = PipelineStatus.PENDING\n",
    "        self.start_time = datetime.now()\n",
    "        self.end_time: Optional[datetime] = None\n",
    "    \n",
    "    def add_stage(self, stage: MLPipelineStage):\n",
    "        \"\"\"Add pipeline stage\"\"\"\n",
    "        self.stages.append(stage)\n",
    "    \n",
    "    def run_data_validation(self, data: pd.DataFrame, schema: DataSchema) -> MLPipelineStage:\n",
    "        \"\"\"Stage 1: Validate input data\"\"\"\n",
    "        stage = MLPipelineStage(name=\"Data Validation\")\n",
    "        stage.start_time = datetime.now()\n",
    "        stage.status = PipelineStatus.RUNNING\n",
    "        \n",
    "        print(f\"\\nüîç Stage 1: Data Validation\")\n",
    "        print(f\"   Validating {len(data)} rows against schema...\")\n",
    "        \n",
    "        result, message = schema.validate(data)\n",
    "        \n",
    "        if result == DataValidationResult.VALID:\n",
    "            stage.status = PipelineStatus.SUCCEEDED\n",
    "            stage.artifacts['validation_result'] = result.value\n",
    "            print(f\"   ‚úÖ {message}\")\n",
    "        else:\n",
    "            stage.status = PipelineStatus.FAILED\n",
    "            stage.error_message = message\n",
    "            stage.artifacts['validation_result'] = result.value\n",
    "            print(f\"   ‚ùå Validation failed: {message}\")\n",
    "        \n",
    "        stage.end_time = datetime.now()\n",
    "        self.add_stage(stage)\n",
    "        return stage\n",
    "    \n",
    "    def run_model_training(self, X_train: np.ndarray, y_train: np.ndarray, \n",
    "                          model_class, hyperparams: Dict) -> MLPipelineStage:\n",
    "        \"\"\"Stage 2: Train ML model\"\"\"\n",
    "        stage = MLPipelineStage(name=\"Model Training\")\n",
    "        stage.start_time = datetime.now()\n",
    "        stage.status = PipelineStatus.RUNNING\n",
    "        \n",
    "        print(f\"\\nüèãÔ∏è Stage 2: Model Training\")\n",
    "        print(f\"   Training {model_class.__name__} with {len(X_train)} samples...\")\n",
    "        \n",
    "        try:\n",
    "            model = model_class(**hyperparams)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            training_time = (datetime.now() - stage.start_time).total_seconds()\n",
    "            \n",
    "            stage.status = PipelineStatus.SUCCEEDED\n",
    "            stage.artifacts['model'] = model\n",
    "            stage.artifacts['training_time_sec'] = training_time\n",
    "            stage.artifacts['hyperparams'] = hyperparams\n",
    "            \n",
    "            print(f\"   ‚úÖ Training completed in {training_time:.2f}s\")\n",
    "        except Exception as e:\n",
    "            stage.status = PipelineStatus.FAILED\n",
    "            stage.error_message = str(e)\n",
    "            print(f\"   ‚ùå Training failed: {e}\")\n",
    "        \n",
    "        stage.end_time = datetime.now()\n",
    "        self.add_stage(stage)\n",
    "        return stage\n",
    "    \n",
    "    def run_model_evaluation(self, model, X_test: np.ndarray, y_test: np.ndarray,\n",
    "                            baseline_metrics: Optional[ModelMetrics] = None) -> MLPipelineStage:\n",
    "        \"\"\"Stage 3: Evaluate model and compare to baseline\"\"\"\n",
    "        stage = MLPipelineStage(name=\"Model Evaluation\")\n",
    "        stage.start_time = datetime.now()\n",
    "        stage.status = PipelineStatus.RUNNING\n",
    "        \n",
    "        print(f\"\\nüìä Stage 3: Model Evaluation\")\n",
    "        \n",
    "        try:\n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = ModelMetrics(\n",
    "                accuracy=accuracy_score(y_test, y_pred),\n",
    "                precision=precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "                recall=recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "                f1_score=f1_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "                training_time_sec=self.stages[-1].artifacts.get('training_time_sec', 0.0),\n",
    "                inference_latency_ms=np.random.uniform(50, 120)  # Simulated latency\n",
    "            )\n",
    "            \n",
    "            print(f\"   Accuracy:  {metrics.accuracy:.4f}\")\n",
    "            print(f\"   Precision: {metrics.precision:.4f}\")\n",
    "            print(f\"   Recall:    {metrics.recall:.4f}\")\n",
    "            print(f\"   F1 Score:  {metrics.f1_score:.4f}\")\n",
    "            print(f\"   Latency:   {metrics.inference_latency_ms:.1f}ms\")\n",
    "            \n",
    "            stage.artifacts['metrics'] = metrics\n",
    "            \n",
    "            # Compare to baseline\n",
    "            if baseline_metrics:\n",
    "                print(f\"\\n   üìà Baseline Comparison:\")\n",
    "                print(f\"      Baseline Accuracy: {baseline_metrics.accuracy:.4f}\")\n",
    "                print(f\"      New Model Accuracy: {metrics.accuracy:.4f}\")\n",
    "                print(f\"      Improvement: {(metrics.accuracy - baseline_metrics.accuracy)*100:.2f}%\")\n",
    "                \n",
    "                if metrics.beats_baseline(baseline_metrics):\n",
    "                    print(f\"   ‚úÖ New model beats baseline!\")\n",
    "                    stage.status = PipelineStatus.SUCCEEDED\n",
    "                else:\n",
    "                    print(f\"   ‚ùå New model does not beat baseline (failed quality gate)\")\n",
    "                    stage.status = PipelineStatus.FAILED\n",
    "                    stage.error_message = \"Model accuracy below baseline threshold\"\n",
    "            else:\n",
    "                # No baseline - check absolute thresholds\n",
    "                if metrics.meets_thresholds():\n",
    "                    print(f\"   ‚úÖ Model meets production thresholds\")\n",
    "                    stage.status = PipelineStatus.SUCCEEDED\n",
    "                else:\n",
    "                    print(f\"   ‚ùå Model fails production thresholds\")\n",
    "                    stage.status = PipelineStatus.FAILED\n",
    "                    stage.error_message = \"Model below production quality thresholds\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            stage.status = PipelineStatus.FAILED\n",
    "            stage.error_message = str(e)\n",
    "            print(f\"   ‚ùå Evaluation failed: {e}\")\n",
    "        \n",
    "        stage.end_time = datetime.now()\n",
    "        self.add_stage(stage)\n",
    "        return stage\n",
    "    \n",
    "    def run_model_registration(self, model, metrics: ModelMetrics, model_name: str, version: str) -> MLPipelineStage:\n",
    "        \"\"\"Stage 4: Register model in model registry (MLflow/DVC simulation)\"\"\"\n",
    "        stage = MLPipelineStage(name=\"Model Registration\")\n",
    "        stage.start_time = datetime.now()\n",
    "        stage.status = PipelineStatus.RUNNING\n",
    "        \n",
    "        print(f\"\\nüì¶ Stage 4: Model Registration\")\n",
    "        \n",
    "        try:\n",
    "            model_metadata = {\n",
    "                'name': model_name,\n",
    "                'version': version,\n",
    "                'git_commit': self.git_commit,\n",
    "                'accuracy': metrics.accuracy,\n",
    "                'precision': metrics.precision,\n",
    "                'recall': metrics.recall,\n",
    "                'f1_score': metrics.f1_score,\n",
    "                'training_time_sec': metrics.training_time_sec,\n",
    "                'inference_latency_ms': metrics.inference_latency_ms,\n",
    "                'registered_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            stage.artifacts['model'] = model\n",
    "            stage.artifacts['metadata'] = model_metadata\n",
    "            stage.status = PipelineStatus.SUCCEEDED\n",
    "            \n",
    "            print(f\"   ‚úÖ Model registered: {model_name} v{version}\")\n",
    "            print(f\"      Git Commit: {self.git_commit}\")\n",
    "            print(f\"      Accuracy: {metrics.accuracy:.4f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            stage.status = PipelineStatus.FAILED\n",
    "            stage.error_message = str(e)\n",
    "            print(f\"   ‚ùå Registration failed: {e}\")\n",
    "        \n",
    "        stage.end_time = datetime.now()\n",
    "        self.add_stage(stage)\n",
    "        return stage\n",
    "    \n",
    "    def finalize(self):\n",
    "        \"\"\"Finalize pipeline execution\"\"\"\n",
    "        self.end_time = datetime.now()\n",
    "        \n",
    "        # Determine overall status\n",
    "        if any(stage.status == PipelineStatus.FAILED for stage in self.stages):\n",
    "            self.overall_status = PipelineStatus.FAILED\n",
    "        elif all(stage.status == PipelineStatus.SUCCEEDED for stage in self.stages):\n",
    "            self.overall_status = PipelineStatus.SUCCEEDED\n",
    "        else:\n",
    "            self.overall_status = PipelineStatus.FAILED\n",
    "    \n",
    "    def get_summary(self) -> Dict:\n",
    "        \"\"\"Get pipeline execution summary\"\"\"\n",
    "        total_duration = (self.end_time - self.start_time).total_seconds() if self.end_time else 0.0\n",
    "        \n",
    "        return {\n",
    "            'pipeline_id': self.pipeline_id,\n",
    "            'git_commit': self.git_commit,\n",
    "            'overall_status': self.overall_status.value,\n",
    "            'total_duration_sec': total_duration,\n",
    "            'stages': [\n",
    "                {\n",
    "                    'name': stage.name,\n",
    "                    'status': stage.status.value,\n",
    "                    'duration_sec': stage.duration_seconds(),\n",
    "                    'error': stage.error_message\n",
    "                }\n",
    "                for stage in self.stages\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Example 1: Successful ML CI Pipeline (Data Valid, Model Beats Baseline)\n",
    "print(\"=\" * 70)\n",
    "print(\"Example 1: Successful ML CI Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate synthetic STDF data (wafer test results)\n",
    "np.random.seed(42)\n",
    "n_samples = 5000\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'wafer_id': [f'W{i:04d}' for i in range(n_samples)],\n",
    "    'die_x': np.random.randint(0, 50, n_samples),\n",
    "    'die_y': np.random.randint(0, 50, n_samples),\n",
    "    'voltage_v': np.random.uniform(1.0, 1.2, n_samples),\n",
    "    'current_ma': np.random.uniform(50, 150, n_samples),\n",
    "    'frequency_mhz': np.random.uniform(2400, 2600, n_samples),\n",
    "    'temperature_c': np.random.uniform(20, 30, n_samples),\n",
    "    'yield_class': np.random.choice([0, 1], n_samples, p=[0.05, 0.95])  # 95% yield\n",
    "})\n",
    "\n",
    "# Define data schema\n",
    "schema = DataSchema(\n",
    "    required_columns=['wafer_id', 'voltage_v', 'current_ma', 'frequency_mhz', 'yield_class'],\n",
    "    column_types={\n",
    "        'voltage_v': 'float',\n",
    "        'current_ma': 'float',\n",
    "        'frequency_mhz': 'float',\n",
    "        'yield_class': 'int'\n",
    "    },\n",
    "    value_ranges={\n",
    "        'voltage_v': (0.8, 1.5),\n",
    "        'current_ma': (10, 200),\n",
    "        'frequency_mhz': (2000, 3000)\n",
    "    },\n",
    "    min_rows=1000\n",
    ")\n",
    "\n",
    "# Create ML CI pipeline\n",
    "pipeline = MLCIPipeline(\n",
    "    pipeline_id=f\"pipeline-{uuid.uuid4().hex[:8]}\",\n",
    "    git_commit=\"a3f2c1b\"\n",
    ")\n",
    "\n",
    "# Stage 1: Data Validation\n",
    "stage1 = pipeline.run_data_validation(data, schema)\n",
    "\n",
    "if stage1.status == PipelineStatus.SUCCEEDED:\n",
    "    # Prepare training data\n",
    "    X = data[['die_x', 'die_y', 'voltage_v', 'current_ma', 'frequency_mhz', 'temperature_c']].values\n",
    "    y = data['yield_class'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Stage 2: Model Training\n",
    "    stage2 = pipeline.run_model_training(\n",
    "        X_train, y_train,\n",
    "        model_class=RandomForestClassifier,\n",
    "        hyperparams={'n_estimators': 100, 'max_depth': 10, 'random_state': 42}\n",
    "    )\n",
    "    \n",
    "    if stage2.status == PipelineStatus.SUCCEEDED:\n",
    "        model = stage2.artifacts['model']\n",
    "        \n",
    "        # Baseline metrics (previous model v1.0)\n",
    "        baseline = ModelMetrics(\n",
    "            accuracy=0.965,\n",
    "            precision=0.960,\n",
    "            recall=0.965,\n",
    "            f1_score=0.962,\n",
    "            training_time_sec=12.5,\n",
    "            inference_latency_ms=85.0\n",
    "        )\n",
    "        \n",
    "        # Stage 3: Model Evaluation\n",
    "        stage3 = pipeline.run_model_evaluation(model, X_test, y_test, baseline_metrics=baseline)\n",
    "        \n",
    "        if stage3.status == PipelineStatus.SUCCEEDED:\n",
    "            metrics = stage3.artifacts['metrics']\n",
    "            \n",
    "            # Stage 4: Model Registration\n",
    "            stage4 = pipeline.run_model_registration(\n",
    "                model, metrics,\n",
    "                model_name=\"yield-predictor\",\n",
    "                version=\"v1.1\"\n",
    "            )\n",
    "\n",
    "pipeline.finalize()\n",
    "\n",
    "# Print pipeline summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Pipeline Execution Summary\")\n",
    "print(\"=\" * 70)\n",
    "summary = pipeline.get_summary()\n",
    "print(json.dumps(summary, indent=2))\n",
    "\n",
    "# Example 2: Failed Pipeline - Data Validation Failure\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Example 2: Failed Pipeline - Data Schema Violation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create bad data (missing required column)\n",
    "bad_data = pd.DataFrame({\n",
    "    'wafer_id': [f'W{i:04d}' for i in range(2000)],\n",
    "    'voltage_v': np.random.uniform(1.0, 1.2, 2000),\n",
    "    # Missing 'current_ma', 'frequency_mhz', 'yield_class'\n",
    "})\n",
    "\n",
    "pipeline2 = MLCIPipeline(\n",
    "    pipeline_id=f\"pipeline-{uuid.uuid4().hex[:8]}\",\n",
    "    git_commit=\"b7e4d2c\"\n",
    ")\n",
    "\n",
    "stage1_bad = pipeline2.run_data_validation(bad_data, schema)\n",
    "pipeline2.finalize()\n",
    "\n",
    "print(f\"\\n‚ùå Pipeline Status: {pipeline2.overall_status.value}\")\n",
    "print(f\"   Error: {stage1_bad.error_message}\")\n",
    "\n",
    "# Example 3: Failed Pipeline - Model Below Baseline\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Example 3: Failed Pipeline - Model Accuracy Below Baseline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train weaker model (fewer trees)\n",
    "pipeline3 = MLCIPipeline(\n",
    "    pipeline_id=f\"pipeline-{uuid.uuid4().hex[:8]}\",\n",
    "    git_commit=\"c9f5e3d\"\n",
    ")\n",
    "\n",
    "stage1_v3 = pipeline3.run_data_validation(data, schema)\n",
    "\n",
    "if stage1_v3.status == PipelineStatus.SUCCEEDED:\n",
    "    stage2_v3 = pipeline3.run_model_training(\n",
    "        X_train, y_train,\n",
    "        model_class=RandomForestClassifier,\n",
    "        hyperparams={'n_estimators': 10, 'max_depth': 3, 'random_state': 42}  # Weak model\n",
    "    )\n",
    "    \n",
    "    if stage2_v3.status == PipelineStatus.SUCCEEDED:\n",
    "        model_weak = stage2_v3.artifacts['model']\n",
    "        stage3_v3 = pipeline3.run_model_evaluation(model_weak, X_test, y_test, baseline_metrics=baseline)\n",
    "\n",
    "pipeline3.finalize()\n",
    "\n",
    "print(f\"\\n‚ùå Pipeline Status: {pipeline3.overall_status.value}\")\n",
    "if pipeline3.stages[-1].error_message:\n",
    "    print(f\"   Error: {pipeline3.stages[-1].error_message}\")\n",
    "\n",
    "print(f\"\\n‚úÖ CI/CD fundamentals demonstrated: Data validation, model gates, quality checks!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ce524",
   "metadata": {},
   "source": [
    "## 3. üéØ Tekton Pipelines - Kubernetes-Native CI/CD for ML\n",
    "\n",
    "### üìù What's Happening in This Section?\n",
    "\n",
    "**Purpose:** Implement Tekton pipelines (Kubernetes CRDs) for ML workflows with parallel task execution and artifact passing.\n",
    "\n",
    "**Key Points:**\n",
    "- **Task CRD**: Single unit of work (data validation, model training, evaluation) running in a pod\n",
    "- **Pipeline CRD**: DAG of tasks (define dependencies: training runs after validation)\n",
    "- **PipelineRun**: Execution instance of pipeline (like workflow run in GitHub Actions)\n",
    "- **Workspaces**: Shared storage between tasks (pass training data from validation ‚Üí training ‚Üí evaluation)\n",
    "- **Parameters**: Runtime inputs (model hyperparameters, data version, Git commit)\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Kubernetes-Native**: Runs on same cluster as ML models (no external CI/CD server needed)\n",
    "- **Parallel Execution**: Train 5 model variants in parallel (A/B test hyperparameters)\n",
    "- **Resource Control**: Training task gets 4 GPUs, evaluation task gets 1 CPU (Kubernetes resource limits)\n",
    "- **Artifact Passing**: Validation outputs data summary ‚Üí training uses it for stratified split\n",
    "\n",
    "**Post-Silicon Application:** Tekton pipeline validates STDF data ‚Üí trains yield model on GPU cluster ‚Üí evaluates accuracy ‚Üí registers model in MLflow ‚Üí triggers ArgoCD deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e63f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tekton Pipelines - Kubernetes-Native ML CI/CD\n",
    "\n",
    "class TaskStatus(Enum):\n",
    "    \"\"\"Tekton Task status\"\"\"\n",
    "    PENDING = \"Pending\"\n",
    "    RUNNING = \"Running\"\n",
    "    SUCCEEDED = \"Succeeded\"\n",
    "    FAILED = \"Failed\"\n",
    "    SKIPPED = \"Skipped\"\n",
    "\n",
    "@dataclass\n",
    "class TektonTask:\n",
    "    \"\"\"Tekton Task CRD - single unit of work\"\"\"\n",
    "    name: str\n",
    "    image: str  # Container image (e.g., python:3.12, pytorch/pytorch:2.0)\n",
    "    script: str  # Shell script to execute\n",
    "    params: Dict[str, Any] = field(default_factory=dict)\n",
    "    resources: Dict[str, str] = field(default_factory=dict)  # CPU, memory, GPU\n",
    "    workspaces: List[str] = field(default_factory=list)  # Shared volumes\n",
    "    \n",
    "    # Execution state\n",
    "    status: TaskStatus = TaskStatus.PENDING\n",
    "    start_time: Optional[datetime] = None\n",
    "    end_time: Optional[datetime] = None\n",
    "    outputs: Dict[str, Any] = field(default_factory=dict)\n",
    "    logs: List[str] = field(default_factory=list)\n",
    "    \n",
    "    def run(self, workspace_data: Dict[str, Any]) -> TaskStatus:\n",
    "        \"\"\"Execute task (simulated)\"\"\"\n",
    "        self.status = TaskStatus.RUNNING\n",
    "        self.start_time = datetime.now()\n",
    "        self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] Task started: {self.name}\")\n",
    "        self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] Using image: {self.image}\")\n",
    "        \n",
    "        try:\n",
    "            # Simulate task execution\n",
    "            if \"data-validation\" in self.name:\n",
    "                self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] Validating data schema...\")\n",
    "                time.sleep(0.5)\n",
    "                self.outputs['validation_result'] = 'VALID'\n",
    "                self.outputs['row_count'] = workspace_data.get('data_rows', 5000)\n",
    "                self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] ‚úÖ Validation passed\")\n",
    "            \n",
    "            elif \"model-training\" in self.name:\n",
    "                self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] Training model...\")\n",
    "                self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] Hyperparameters: {self.params}\")\n",
    "                time.sleep(1.0)\n",
    "                self.outputs['model_accuracy'] = np.random.uniform(0.96, 0.98)\n",
    "                self.outputs['model_path'] = f\"/models/yield-predictor-{uuid.uuid4().hex[:8]}.pkl\"\n",
    "                self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] ‚úÖ Training completed\")\n",
    "            \n",
    "            elif \"model-evaluation\" in self.name:\n",
    "                self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] Evaluating model...\")\n",
    "                time.sleep(0.3)\n",
    "                baseline_acc = self.params.get('baseline_accuracy', 0.96)\n",
    "                model_acc = workspace_data.get('model_accuracy', 0.97)\n",
    "                self.outputs['accuracy'] = model_acc\n",
    "                self.outputs['beats_baseline'] = model_acc > baseline_acc\n",
    "                self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] Model: {model_acc:.4f}, Baseline: {baseline_acc:.4f}\")\n",
    "                if model_acc > baseline_acc:\n",
    "                    self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] ‚úÖ Model beats baseline\")\n",
    "                else:\n",
    "                    self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] ‚ùå Model below baseline\")\n",
    "                    self.status = TaskStatus.FAILED\n",
    "                    return TaskStatus.FAILED\n",
    "            \n",
    "            elif \"model-registry\" in self.name:\n",
    "                self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] Registering model in MLflow...\")\n",
    "                time.sleep(0.2)\n",
    "                self.outputs['model_version'] = f\"v{np.random.randint(10, 50)}.{np.random.randint(0, 10)}\"\n",
    "                self.outputs['registry_url'] = \"mlflow://models/yield-predictor\"\n",
    "                self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] ‚úÖ Model registered\")\n",
    "            \n",
    "            self.status = TaskStatus.SUCCEEDED\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.status = TaskStatus.FAILED\n",
    "            self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] ‚ùå Task failed: {e}\")\n",
    "        \n",
    "        self.end_time = datetime.now()\n",
    "        duration = (self.end_time - self.start_time).total_seconds()\n",
    "        self.logs.append(f\"[{self.end_time.strftime('%H:%M:%S')}] Task completed in {duration:.2f}s\")\n",
    "        \n",
    "        return self.status\n",
    "\n",
    "@dataclass\n",
    "class TektonPipeline:\n",
    "    \"\"\"Tekton Pipeline CRD - DAG of tasks\"\"\"\n",
    "    name: str\n",
    "    tasks: List[TektonTask]\n",
    "    params: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def get_task_dependencies(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Get task dependencies (DAG)\"\"\"\n",
    "        # Simple dependency inference: tasks run sequentially unless specified\n",
    "        dependencies = {}\n",
    "        for i, task in enumerate(self.tasks):\n",
    "            if i == 0:\n",
    "                dependencies[task.name] = []\n",
    "            else:\n",
    "                dependencies[task.name] = [self.tasks[i-1].name]\n",
    "        return dependencies\n",
    "\n",
    "@dataclass\n",
    "class TektonPipelineRun:\n",
    "    \"\"\"Tekton PipelineRun - execution instance of pipeline\"\"\"\n",
    "    pipeline: TektonPipeline\n",
    "    run_id: str\n",
    "    git_commit: str\n",
    "    params: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    # Execution state\n",
    "    status: PipelineStatus = PipelineStatus.PENDING\n",
    "    start_time: Optional[datetime] = None\n",
    "    end_time: Optional[datetime] = None\n",
    "    workspace_data: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def execute(self) -> PipelineStatus:\n",
    "        \"\"\"Execute all pipeline tasks\"\"\"\n",
    "        self.status = PipelineStatus.RUNNING\n",
    "        self.start_time = datetime.now()\n",
    "        \n",
    "        print(f\"\\nüöÄ PipelineRun Started: {self.run_id}\")\n",
    "        print(f\"   Pipeline: {self.pipeline.name}\")\n",
    "        print(f\"   Git Commit: {self.git_commit}\")\n",
    "        print(f\"   Parameters: {self.params}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Execute tasks sequentially (in real Tekton, respects DAG dependencies)\n",
    "        for task in self.pipeline.tasks:\n",
    "            print(f\"\\nüì¶ Executing Task: {task.name}\")\n",
    "            print(f\"   Image: {task.image}\")\n",
    "            print(f\"   Resources: {task.resources}\")\n",
    "            \n",
    "            # Merge pipeline params with task params\n",
    "            task.params.update(self.params)\n",
    "            \n",
    "            # Run task\n",
    "            task_status = task.run(self.workspace_data)\n",
    "            \n",
    "            # Update workspace with task outputs\n",
    "            self.workspace_data.update(task.outputs)\n",
    "            \n",
    "            # Print task logs\n",
    "            for log in task.logs:\n",
    "                print(f\"   {log}\")\n",
    "            \n",
    "            # Check task status\n",
    "            if task_status == TaskStatus.FAILED:\n",
    "                print(f\"\\n‚ùå Task failed: {task.name}\")\n",
    "                self.status = PipelineStatus.FAILED\n",
    "                self.end_time = datetime.now()\n",
    "                return self.status\n",
    "        \n",
    "        self.status = PipelineStatus.SUCCEEDED\n",
    "        self.end_time = datetime.now()\n",
    "        \n",
    "        total_duration = (self.end_time - self.start_time).total_seconds()\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"‚úÖ PipelineRun Completed: {self.run_id}\")\n",
    "        print(f\"   Status: {self.status.value}\")\n",
    "        print(f\"   Duration: {total_duration:.2f}s\")\n",
    "        \n",
    "        return self.status\n",
    "    \n",
    "    def get_summary(self) -> Dict:\n",
    "        \"\"\"Get pipeline run summary\"\"\"\n",
    "        return {\n",
    "            'run_id': self.run_id,\n",
    "            'pipeline': self.pipeline.name,\n",
    "            'git_commit': self.git_commit,\n",
    "            'status': self.status.value,\n",
    "            'duration_sec': (self.end_time - self.start_time).total_seconds() if self.end_time else 0,\n",
    "            'tasks': [\n",
    "                {\n",
    "                    'name': task.name,\n",
    "                    'status': task.status.value,\n",
    "                    'duration_sec': (task.end_time - task.start_time).total_seconds() if task.end_time and task.start_time else 0,\n",
    "                    'outputs': task.outputs\n",
    "                }\n",
    "                for task in self.pipeline.tasks\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Example 1: Tekton ML Pipeline - Data Validation ‚Üí Training ‚Üí Evaluation ‚Üí Registry\n",
    "print(\"=\" * 70)\n",
    "print(\"Example 1: Tekton ML Training Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define pipeline tasks\n",
    "task1_validate = TektonTask(\n",
    "    name=\"data-validation\",\n",
    "    image=\"python:3.12-slim\",\n",
    "    script=\"python validate_data.py --schema schema.yaml --input data.csv\",\n",
    "    resources={'cpu': '1', 'memory': '2Gi'},\n",
    "    workspaces=['data', 'config']\n",
    ")\n",
    "\n",
    "task2_train = TektonTask(\n",
    "    name=\"model-training\",\n",
    "    image=\"pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime\",\n",
    "    script=\"python train_model.py --data data.csv --model-type rf --output model.pkl\",\n",
    "    params={'n_estimators': 100, 'max_depth': 10},\n",
    "    resources={'cpu': '4', 'memory': '16Gi', 'nvidia.com/gpu': '1'},\n",
    "    workspaces=['data', 'models']\n",
    ")\n",
    "\n",
    "task3_evaluate = TektonTask(\n",
    "    name=\"model-evaluation\",\n",
    "    image=\"python:3.12-slim\",\n",
    "    script=\"python evaluate_model.py --model model.pkl --test-data test.csv\",\n",
    "    params={'baseline_accuracy': 0.965},\n",
    "    resources={'cpu': '2', 'memory': '4Gi'},\n",
    "    workspaces=['models', 'metrics']\n",
    ")\n",
    "\n",
    "task4_register = TektonTask(\n",
    "    name=\"model-registry\",\n",
    "    image=\"python:3.12-slim\",\n",
    "    script=\"python register_model.py --model model.pkl --registry mlflow\",\n",
    "    resources={'cpu': '1', 'memory': '2Gi'},\n",
    "    workspaces=['models']\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "ml_pipeline = TektonPipeline(\n",
    "    name=\"ml-training-pipeline\",\n",
    "    tasks=[task1_validate, task2_train, task3_evaluate, task4_register]\n",
    ")\n",
    "\n",
    "# Execute pipeline run\n",
    "pipeline_run = TektonPipelineRun(\n",
    "    pipeline=ml_pipeline,\n",
    "    run_id=f\"run-{uuid.uuid4().hex[:8]}\",\n",
    "    git_commit=\"a3f2c1b\",\n",
    "    params={'data_version': 'v2024.12.10', 'model_name': 'yield-predictor'}\n",
    ")\n",
    "\n",
    "# Initialize workspace with data\n",
    "pipeline_run.workspace_data['data_rows'] = 5000\n",
    "pipeline_run.workspace_data['model_accuracy'] = 0.972  # Simulated training result\n",
    "\n",
    "status = pipeline_run.execute()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Pipeline Run Summary\")\n",
    "print(\"=\" * 70)\n",
    "summary = pipeline_run.get_summary()\n",
    "print(json.dumps(summary, indent=2))\n",
    "\n",
    "# Example 2: Parallel Task Execution (Hyperparameter Tuning)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Example 2: Parallel Hyperparameter Tuning with Tekton\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create 3 parallel training tasks with different hyperparameters\n",
    "training_tasks = []\n",
    "for i, (n_est, depth) in enumerate([(50, 5), (100, 10), (200, 15)]):\n",
    "    task = TektonTask(\n",
    "        name=f\"model-training-{i+1}\",\n",
    "        image=\"pytorch/pytorch:2.0.0-cuda11.7-cudnn8-runtime\",\n",
    "        script=f\"python train_model.py --n-estimators {n_est} --max-depth {depth}\",\n",
    "        params={'n_estimators': n_est, 'max_depth': depth},\n",
    "        resources={'cpu': '4', 'memory': '16Gi', 'nvidia.com/gpu': '1'},\n",
    "    )\n",
    "    training_tasks.append(task)\n",
    "\n",
    "print(f\"üîÄ Executing {len(training_tasks)} training tasks in parallel...\")\n",
    "print(f\"   (In real Tekton, these run simultaneously on different nodes)\")\n",
    "\n",
    "# Simulate parallel execution\n",
    "results = []\n",
    "for task in training_tasks:\n",
    "    task.run({})\n",
    "    results.append({\n",
    "        'hyperparams': task.params,\n",
    "        'accuracy': task.outputs.get('model_accuracy', 0.0),\n",
    "        'model_path': task.outputs.get('model_path', '')\n",
    "    })\n",
    "    print(f\"\\n   Task: {task.name}\")\n",
    "    print(f\"      Hyperparams: n_estimators={task.params['n_estimators']}, max_depth={task.params['max_depth']}\")\n",
    "    print(f\"      Accuracy: {task.outputs.get('model_accuracy', 0.0):.4f}\")\n",
    "\n",
    "# Select best model\n",
    "best_result = max(results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\n‚úÖ Best Model Selected:\")\n",
    "print(f\"   Hyperparameters: {best_result['hyperparams']}\")\n",
    "print(f\"   Accuracy: {best_result['accuracy']:.4f}\")\n",
    "print(f\"   Model Path: {best_result['model_path']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Tekton pipelines demonstrated: Sequential tasks, parallel execution, resource control!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1706b111",
   "metadata": {},
   "source": [
    "## 4. üîß GitHub Actions - Cloud-Based CI/CD for ML\n",
    "\n",
    "**Purpose:** Implement CI/CD workflows using GitHub Actions for automated ML pipeline execution.\n",
    "\n",
    "**Key Points:**\n",
    "- **Workflows**: YAML-defined automation (triggered by git push, PR, schedule)\n",
    "- **Jobs & Steps**: Jobs run in parallel, steps execute sequentially within a job\n",
    "- **Matrix Builds**: Test multiple Python versions, OS, or hyperparameters in parallel\n",
    "- **Secrets Management**: Store API keys, cloud credentials, model registry tokens securely\n",
    "- **Artifact Caching**: Cache dependencies, datasets, models to speed up workflows\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Cloud-Native**: No infrastructure setup (GitHub-hosted runners, automatic scaling)\n",
    "- **Git Integration**: Tight coupling with code changes (trigger on push, PR review, merge)\n",
    "- **Cost-Effective**: Free for public repos, pay-per-minute for private repos\n",
    "- **Ecosystem**: Thousands of pre-built actions (Docker build, cloud deploy, Slack notify)\n",
    "\n",
    "**Post-Silicon Application:**\n",
    "GitHub Actions workflow triggers when STDF parser code pushed to main branch:\n",
    "1. **Step 1**: Validate STDF schema (check required fields, data types)\n",
    "2. **Step 2**: Run unit tests on parser logic (edge cases, malformed data)\n",
    "3. **Step 3**: Train yield prediction model on latest STDF data (RandomForest)\n",
    "4. **Step 4**: Deploy model to staging (Kubernetes cluster via kubectl)\n",
    "5. **Step 5**: Run integration tests (API health check, prediction accuracy)\n",
    "6. **Step 6**: Promote to production if tests pass (update Kubernetes deployment)\n",
    "\n",
    "This ensures every code change validated before production deployment, reducing STDF parser bugs by 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b21d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub Actions - Cloud-Based ML CI/CD Workflows\n",
    "\n",
    "@dataclass\n",
    "class WorkflowStep:\n",
    "    \"\"\"GitHub Actions step\"\"\"\n",
    "    name: str\n",
    "    run: str  # Shell command or action\n",
    "    uses: Optional[str] = None  # Action (e.g., actions/checkout@v3)\n",
    "    with_params: Dict[str, Any] = field(default_factory=dict)\n",
    "    env: Dict[str, str] = field(default_factory=dict)\n",
    "    \n",
    "    # Execution state\n",
    "    status: TaskStatus = TaskStatus.PENDING\n",
    "    start_time: Optional[datetime] = None\n",
    "    end_time: Optional[datetime] = None\n",
    "    outputs: Dict[str, Any] = field(default_factory=dict)\n",
    "    logs: List[str] = field(default_factory=list)\n",
    "    \n",
    "    def execute(self, context: Dict[str, Any]) -> TaskStatus:\n",
    "        \"\"\"Execute step\"\"\"\n",
    "        self.status = TaskStatus.RUNNING\n",
    "        self.start_time = datetime.now()\n",
    "        self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] Step: {self.name}\")\n",
    "        \n",
    "        try:\n",
    "            if self.uses:\n",
    "                # Using pre-built action\n",
    "                self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] Using action: {self.uses}\")\n",
    "                \n",
    "                if \"checkout\" in self.uses:\n",
    "                    self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] ‚úÖ Repository checked out\")\n",
    "                    self.outputs['repo_path'] = \"/github/workspace\"\n",
    "                \n",
    "                elif \"setup-python\" in self.uses:\n",
    "                    python_version = self.with_params.get('python-version', '3.12')\n",
    "                    self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] ‚úÖ Python {python_version} installed\")\n",
    "                    self.outputs['python_version'] = python_version\n",
    "                \n",
    "                elif \"upload-artifact\" in self.uses:\n",
    "                    artifact_name = self.with_params.get('name', 'artifact')\n",
    "                    self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] ‚úÖ Artifact '{artifact_name}' uploaded\")\n",
    "                    self.outputs['artifact_url'] = f\"https://github.com/actions/runs/artifacts/{artifact_name}\"\n",
    "            \n",
    "            elif self.run:\n",
    "                # Running shell command\n",
    "                self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] Running: {self.run}\")\n",
    "                time.sleep(0.2)\n",
    "                \n",
    "                if \"pytest\" in self.run:\n",
    "                    self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] ============================= test session starts ==============================\")\n",
    "                    self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] collected 45 items\")\n",
    "                    self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] tests/test_data_validation.py::test_schema_validation PASSED [  2%]\")\n",
    "                    self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] tests/test_model_training.py::test_rf_training PASSED [ 11%]\")\n",
    "                    self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] tests/test_model_evaluation.py::test_baseline_comparison PASSED [ 24%]\")\n",
    "                    self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] ============================== 45 passed in 2.34s ===============================\")\n",
    "                    self.outputs['tests_passed'] = 45\n",
    "                    self.outputs['tests_failed'] = 0\n",
    "                \n",
    "                elif \"train\" in self.run:\n",
    "                    self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] Training model...\")\n",
    "                    time.sleep(0.5)\n",
    "                    accuracy = np.random.uniform(0.96, 0.98)\n",
    "                    self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] Model accuracy: {accuracy:.4f}\")\n",
    "                    self.outputs['model_accuracy'] = accuracy\n",
    "                    self.outputs['model_path'] = \"models/yield-predictor.pkl\"\n",
    "                \n",
    "                elif \"kubectl\" in self.run:\n",
    "                    self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] Deploying to Kubernetes...\")\n",
    "                    time.sleep(0.3)\n",
    "                    self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] deployment.apps/yield-predictor-api configured\")\n",
    "                    self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] service/yield-predictor-api unchanged\")\n",
    "                    self.outputs['deployment_status'] = 'success'\n",
    "                \n",
    "                self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] ‚úÖ Command executed successfully\")\n",
    "            \n",
    "            self.status = TaskStatus.SUCCEEDED\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.status = TaskStatus.FAILED\n",
    "            self.logs.append(f\"[{self.start_time.strftime('%H:%M:%S')}] ‚ùå Step failed: {e}\")\n",
    "        \n",
    "        self.end_time = datetime.now()\n",
    "        return self.status\n",
    "\n",
    "@dataclass\n",
    "class WorkflowJob:\n",
    "    \"\"\"GitHub Actions job\"\"\"\n",
    "    name: str\n",
    "    runs_on: str  # ubuntu-latest, macos-latest, self-hosted\n",
    "    steps: List[WorkflowStep]\n",
    "    needs: List[str] = field(default_factory=list)  # Job dependencies\n",
    "    strategy: Optional[Dict] = None  # Matrix builds\n",
    "    \n",
    "    # Execution state\n",
    "    status: TaskStatus = TaskStatus.PENDING\n",
    "    start_time: Optional[datetime] = None\n",
    "    end_time: Optional[datetime] = None\n",
    "    \n",
    "    def execute(self, context: Dict[str, Any]) -> TaskStatus:\n",
    "        \"\"\"Execute all steps in job\"\"\"\n",
    "        self.status = TaskStatus.RUNNING\n",
    "        self.start_time = datetime.now()\n",
    "        \n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"üîß Job: {self.name}\")\n",
    "        print(f\"   Runs on: {self.runs_on}\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        \n",
    "        for step in self.steps:\n",
    "            step_status = step.execute(context)\n",
    "            \n",
    "            # Print step logs\n",
    "            for log in step.logs:\n",
    "                print(f\"   {log}\")\n",
    "            \n",
    "            # Update context with step outputs\n",
    "            context.update(step.outputs)\n",
    "            \n",
    "            if step_status == TaskStatus.FAILED:\n",
    "                print(f\"\\n‚ùå Job failed at step: {step.name}\")\n",
    "                self.status = TaskStatus.FAILED\n",
    "                self.end_time = datetime.now()\n",
    "                return self.status\n",
    "        \n",
    "        self.status = TaskStatus.SUCCEEDED\n",
    "        self.end_time = datetime.now()\n",
    "        \n",
    "        duration = (self.end_time - self.start_time).total_seconds()\n",
    "        print(f\"\\n‚úÖ Job completed in {duration:.2f}s\")\n",
    "        \n",
    "        return self.status\n",
    "\n",
    "@dataclass\n",
    "class GitHubWorkflow:\n",
    "    \"\"\"GitHub Actions workflow\"\"\"\n",
    "    name: str\n",
    "    on: List[str]  # push, pull_request, schedule, workflow_dispatch\n",
    "    jobs: List[WorkflowJob]\n",
    "    env: Dict[str, str] = field(default_factory=dict)\n",
    "    \n",
    "    # Execution state\n",
    "    run_id: str = field(default_factory=lambda: uuid.uuid4().hex[:8])\n",
    "    status: PipelineStatus = PipelineStatus.PENDING\n",
    "    start_time: Optional[datetime] = None\n",
    "    end_time: Optional[datetime] = None\n",
    "    \n",
    "    def execute(self) -> PipelineStatus:\n",
    "        \"\"\"Execute workflow\"\"\"\n",
    "        self.status = PipelineStatus.RUNNING\n",
    "        self.start_time = datetime.now()\n",
    "        \n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"üöÄ GitHub Actions Workflow: {self.name}\")\n",
    "        print(f\"   Run ID: {self.run_id}\")\n",
    "        print(f\"   Triggered by: {', '.join(self.on)}\")\n",
    "        print(f\"   Environment: {self.env}\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        \n",
    "        context = {}\n",
    "        \n",
    "        # Execute jobs (simplified - assumes sequential execution)\n",
    "        for job in self.jobs:\n",
    "            job_status = job.execute(context)\n",
    "            \n",
    "            if job_status == TaskStatus.FAILED:\n",
    "                print(f\"\\n‚ùå Workflow failed at job: {job.name}\")\n",
    "                self.status = PipelineStatus.FAILED\n",
    "                self.end_time = datetime.now()\n",
    "                return self.status\n",
    "        \n",
    "        self.status = PipelineStatus.SUCCEEDED\n",
    "        self.end_time = datetime.now()\n",
    "        \n",
    "        total_duration = (self.end_time - self.start_time).total_seconds()\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"‚úÖ Workflow Completed: {self.name}\")\n",
    "        print(f\"   Status: {self.status.value}\")\n",
    "        print(f\"   Duration: {total_duration:.2f}s\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        \n",
    "        return self.status\n",
    "\n",
    "# Example 1: Complete ML CI/CD Workflow with GitHub Actions\n",
    "print(\"=\" * 70)\n",
    "print(\"Example 1: Complete ML CI/CD Workflow\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Job 1: Test\n",
    "test_job = WorkflowJob(\n",
    "    name=\"test\",\n",
    "    runs_on=\"ubuntu-latest\",\n",
    "    steps=[\n",
    "        WorkflowStep(name=\"Checkout code\", uses=\"actions/checkout@v3\"),\n",
    "        WorkflowStep(name=\"Set up Python\", uses=\"actions/setup-python@v4\", with_params={'python-version': '3.12'}),\n",
    "        WorkflowStep(name=\"Install dependencies\", run=\"pip install -r requirements.txt\"),\n",
    "        WorkflowStep(name=\"Run tests\", run=\"pytest tests/ --cov=src --cov-report=xml\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Job 2: Train (depends on test passing)\n",
    "train_job = WorkflowJob(\n",
    "    name=\"train\",\n",
    "    runs_on=\"ubuntu-latest\",\n",
    "    needs=[\"test\"],\n",
    "    steps=[\n",
    "        WorkflowStep(name=\"Checkout code\", uses=\"actions/checkout@v3\"),\n",
    "        WorkflowStep(name=\"Set up Python\", uses=\"actions/setup-python@v4\", with_params={'python-version': '3.12'}),\n",
    "        WorkflowStep(name=\"Train model\", run=\"python train_model.py --data data/stdf_wafer_test.csv --output models/\"),\n",
    "        WorkflowStep(name=\"Upload model artifact\", uses=\"actions/upload-artifact@v3\", with_params={'name': 'trained-model', 'path': 'models/'}),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Job 3: Deploy (depends on train passing)\n",
    "deploy_job = WorkflowJob(\n",
    "    name=\"deploy\",\n",
    "    runs_on=\"ubuntu-latest\",\n",
    "    needs=[\"train\"],\n",
    "    steps=[\n",
    "        WorkflowStep(name=\"Checkout code\", uses=\"actions/checkout@v3\"),\n",
    "        WorkflowStep(name=\"Download model artifact\", uses=\"actions/download-artifact@v3\", with_params={'name': 'trained-model'}),\n",
    "        WorkflowStep(name=\"Deploy to Kubernetes\", run=\"kubectl apply -f k8s/deployment.yaml\"),\n",
    "        WorkflowStep(name=\"Verify deployment\", run=\"kubectl rollout status deployment/yield-predictor-api\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create workflow\n",
    "ml_workflow = GitHubWorkflow(\n",
    "    name=\"ML Training and Deployment\",\n",
    "    on=[\"push\", \"pull_request\"],\n",
    "    jobs=[test_job, train_job, deploy_job],\n",
    "    env={'PYTHONPATH': '/github/workspace', 'MLFLOW_TRACKING_URI': 'https://mlflow.example.com'}\n",
    ")\n",
    "\n",
    "# Execute workflow\n",
    "status = ml_workflow.execute()\n",
    "\n",
    "# Example 2: Matrix Build - Test Multiple Python Versions\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Example 2: Matrix Build - Test Multiple Python Versions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Matrix strategy: Test Python 3.10, 3.11, 3.12\n",
    "for python_version in ['3.10', '3.11', '3.12']:\n",
    "    print(f\"\\nüîÑ Testing with Python {python_version}\")\n",
    "    \n",
    "    test_job_matrix = WorkflowJob(\n",
    "        name=f\"test-py{python_version}\",\n",
    "        runs_on=\"ubuntu-latest\",\n",
    "        steps=[\n",
    "            WorkflowStep(name=\"Checkout code\", uses=\"actions/checkout@v3\"),\n",
    "            WorkflowStep(name=\"Set up Python\", uses=\"actions/setup-python@v4\", with_params={'python-version': python_version}),\n",
    "            WorkflowStep(name=\"Install dependencies\", run=\"pip install -r requirements.txt\"),\n",
    "            WorkflowStep(name=\"Run tests\", run=\"pytest tests/\"),\n",
    "        ],\n",
    "        strategy={'matrix': {'python-version': python_version}}\n",
    "    )\n",
    "    \n",
    "    context = {}\n",
    "    job_status = test_job_matrix.execute(context)\n",
    "    \n",
    "    if job_status == TaskStatus.SUCCEEDED:\n",
    "        print(f\"   ‚úÖ Tests passed with Python {python_version}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Tests failed with Python {python_version}\")\n",
    "\n",
    "print(f\"\\n‚úÖ GitHub Actions workflows demonstrated: Multi-job CI/CD, matrix builds, artifact management!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536044d4",
   "metadata": {},
   "source": [
    "## 5. üè≠ Real-World Projects: CI/CD for ML in Production\n",
    "\n",
    "### Project 1: Automated Yield Prediction Retraining Pipeline üéØ\n",
    "\n",
    "**Objective:** Build CI/CD pipeline that automatically retrains yield prediction model when new STDF data arrives.\n",
    "\n",
    "**Business Value:** Continuous model improvement with latest wafer test data ‚Üí 0.5% accuracy gains ‚Üí $600K/year savings (fewer false positives in yield prediction).\n",
    "\n",
    "**Implementation Plan:**\n",
    "1. **Data Ingestion**: Schedule daily STDF file collection from test equipment (cron: 0 2 * * *)\n",
    "2. **Data Validation**: Tekton Task validates schema (required columns, data types, value ranges)\n",
    "3. **Model Retraining**: Train RandomForest on last 30 days of data (5000+ devices)\n",
    "4. **Baseline Comparison**: Deploy only if new model accuracy ‚â• baseline + 1%\n",
    "5. **Canary Deployment**: Route 10% traffic to new model, monitor for 24 hours\n",
    "6. **Full Rollout**: Promote to 100% traffic if canary metrics acceptable\n",
    "\n",
    "**Tekton Pipeline Structure:**\n",
    "- Task 1: `stdf-data-ingestion` (runs daily at 2 AM)\n",
    "- Task 2: `data-validation` (schema checks, quality gates)\n",
    "- Task 3: `model-training` (RandomForest with Grid Search)\n",
    "- Task 4: `model-evaluation` (accuracy, precision, recall vs baseline)\n",
    "- Task 5: `canary-deployment` (10% traffic to new model)\n",
    "- Task 6: `full-deployment` (100% traffic after 24h monitoring)\n",
    "\n",
    "**Key Technologies:** Tekton, Kubernetes, MLflow, Prometheus (metrics), ArgoCD (GitOps)\n",
    "\n",
    "**Success Metrics:**\n",
    "- ‚úÖ Model retraining frequency: Daily (down from weekly manual process)\n",
    "- ‚úÖ Accuracy improvement rate: 0.5% per quarter (continuous learning)\n",
    "- ‚úÖ Deployment time: 15 minutes automated (down from 4 hours manual)\n",
    "- ‚úÖ Failed deployments: <2% (quality gates prevent regression)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 2: STDF Data Pipeline with Quality Gates üîç\n",
    "\n",
    "**Objective:** CI/CD pipeline for STDF parser library ensuring zero data corruption bugs.\n",
    "\n",
    "**Business Value:** Prevent STDF parsing errors that cause incorrect yield reports ‚Üí $180K/year savings (avoided engineering time debugging bad data).\n",
    "\n",
    "**Implementation Plan:**\n",
    "1. **GitHub Actions Workflow**: Triggers on PR to main branch\n",
    "2. **Unit Tests**: Test STDF parser on 500+ edge cases (malformed headers, missing fields, corrupt data)\n",
    "3. **Integration Tests**: Parse real STDF files (wafer test, final test) and validate output schema\n",
    "4. **Performance Tests**: Ensure parser handles 10K+ device records in <5 seconds\n",
    "5. **Security Scan**: Check dependencies for CVE vulnerabilities (Snyk, Dependabot)\n",
    "6. **Merge Gate**: PR approved only if all tests pass + code coverage ‚â• 90%\n",
    "\n",
    "**GitHub Actions Workflow Structure:**\n",
    "```yaml\n",
    "name: STDF Parser CI\n",
    "on: [pull_request]\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.12'\n",
    "      - run: pip install -r requirements.txt\n",
    "      - run: pytest tests/ --cov=src --cov-report=xml\n",
    "      - run: python benchmark_parser.py --stdf-file data/wafer_test.stdf\n",
    "  security:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - uses: snyk/actions/python@master\n",
    "        with:\n",
    "          command: test\n",
    "```\n",
    "\n",
    "**Key Technologies:** GitHub Actions, pytest, pytest-cov, Snyk, Codecov\n",
    "\n",
    "**Success Metrics:**\n",
    "- ‚úÖ Code coverage: 95%+ (comprehensive edge case testing)\n",
    "- ‚úÖ Parser bugs in production: 0 (quality gates prevent bad code)\n",
    "- ‚úÖ PR merge time: 8 minutes (automated testing)\n",
    "- ‚úÖ False positive test failures: <1% (stable CI pipeline)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 3: Canary Deployment for Wafer Defect Analyzer üõ°Ô∏è\n",
    "\n",
    "**Objective:** Safely deploy new CNN model for wafer defect detection using canary strategy.\n",
    "\n",
    "**Business Value:** Reduce defect escape rate (bad dies shipped to customers) by 30% ‚Üí $2.1M/year savings (fewer field returns, improved quality).\n",
    "\n",
    "**Implementation Plan:**\n",
    "1. **Baseline Model**: Rule-based defect classifier (95% accuracy, fast but limited)\n",
    "2. **New Model**: CNN (ResNet-50) trained on 100K wafer images (98% accuracy, slower but comprehensive)\n",
    "3. **Canary Strategy**:\n",
    "   - Stage 1: Route 5% production traffic to CNN model (low-risk validation)\n",
    "   - Stage 2: Monitor false positive rate, latency, memory usage (24 hours)\n",
    "   - Stage 3: If metrics acceptable, increase to 25% traffic (48 hours)\n",
    "   - Stage 4: If metrics acceptable, increase to 100% traffic (full rollout)\n",
    "   - Rollback: If false positives ‚Üë or latency >500ms, rollback to baseline\n",
    "4. **Monitoring**: Prometheus metrics (defect_detection_accuracy, inference_latency_ms, memory_usage_mb)\n",
    "5. **GitOps**: ArgoCD manages Kubernetes deployment versions (automated rollout, rollback)\n",
    "\n",
    "**Tekton/GitHub Actions Workflow:**\n",
    "- Step 1: Train CNN on latest wafer images\n",
    "- Step 2: Validate model accuracy on held-out test set (‚â•98% required)\n",
    "- Step 3: Deploy to staging Kubernetes cluster (functional testing)\n",
    "- Step 4: Create canary deployment YAML (5% traffic to new model, 95% to baseline)\n",
    "- Step 5: ArgoCD syncs canary deployment to production cluster\n",
    "- Step 6: Monitor Prometheus metrics for 24 hours\n",
    "- Step 7: If acceptable, increase traffic to 25%, then 100%\n",
    "\n",
    "**Key Technologies:** Tekton, ArgoCD, Flagger (automated canary), Prometheus, Grafana, Kubernetes\n",
    "\n",
    "**Success Metrics:**\n",
    "- ‚úÖ Defect detection accuracy: 98% (up from 95% baseline)\n",
    "- ‚úÖ Canary rollout time: 72 hours (safe, gradual deployment)\n",
    "- ‚úÖ Production incidents: 0 (canary prevents bad deployments)\n",
    "- ‚úÖ Rollback time: <5 minutes (automated Flagger rollback)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 4: Multi-Stage ML Pipeline with Experiment Tracking üß™\n",
    "\n",
    "**Objective:** ML pipeline with integrated experiment tracking for hyperparameter tuning.\n",
    "\n",
    "**Business Value:** Find optimal model configuration faster ‚Üí $95K/year savings (reduced engineering time on manual hyperparameter tuning).\n",
    "\n",
    "**Implementation Plan:**\n",
    "1. **Experiment Tracking**: MLflow tracks all training runs (hyperparameters, metrics, artifacts)\n",
    "2. **Grid Search**: Train 20 model variants (RandomForest, GradientBoosting √ó 10 hyperparameter combinations)\n",
    "3. **Parallel Execution**: Tekton runs 5 training tasks in parallel (GPU nodes)\n",
    "4. **Model Selection**: Select best model by F1 score (balanced precision/recall)\n",
    "5. **Model Registry**: Register best model in MLflow Model Registry (versioned, tagged)\n",
    "6. **CI/CD Integration**: GitHub Actions triggers pipeline on git push to main\n",
    "\n",
    "**Tekton Pipeline Structure:**\n",
    "- Task 1: `data-preprocessing` (feature engineering, train/test split)\n",
    "- Task 2-21: `model-training-{i}` (parallel tasks, each with different hyperparameters)\n",
    "  - Example: Task 2: RandomForest(n_estimators=50, max_depth=5)\n",
    "  - Example: Task 3: RandomForest(n_estimators=100, max_depth=10)\n",
    "  - Example: Task 12: GradientBoosting(n_estimators=50, learning_rate=0.01)\n",
    "- Task 22: `model-selection` (compare MLflow runs, select best by F1 score)\n",
    "- Task 23: `model-registration` (register in MLflow Model Registry)\n",
    "- Task 24: `deployment` (deploy best model to Kubernetes via ArgoCD)\n",
    "\n",
    "**MLflow Integration:**\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "with mlflow.start_run(run_name=f\"rf-{n_estimators}-{max_depth}\"):\n",
    "    mlflow.log_params({'n_estimators': n_estimators, 'max_depth': max_depth})\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    mlflow.log_metric('f1_score', f1)\n",
    "    mlflow.sklearn.log_model(model, 'model')\n",
    "```\n",
    "\n",
    "**Key Technologies:** Tekton, MLflow, Kubernetes (GPU nodes), GitHub Actions, ArgoCD\n",
    "\n",
    "**Success Metrics:**\n",
    "- ‚úÖ Hyperparameter tuning time: 2 hours (down from 2 days manual)\n",
    "- ‚úÖ Model F1 score: 0.96+ (optimal hyperparameters found)\n",
    "- ‚úÖ Experiment reproducibility: 100% (MLflow tracks all runs)\n",
    "- ‚úÖ Engineering time savings: 80% (automated grid search)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 5: GitOps-Driven Model Deployment Pipeline üîÑ\n",
    "\n",
    "**Objective:** Use GitOps (ArgoCD) to manage ML model deployments declaratively.\n",
    "\n",
    "**Business Value:** Eliminate deployment drift (production matches Git definitions) ‚Üí $75K/year savings (fewer configuration errors, faster rollbacks).\n",
    "\n",
    "**Implementation Plan:**\n",
    "1. **Git as Source of Truth**: All Kubernetes manifests stored in Git (deployment, service, ConfigMap)\n",
    "2. **ArgoCD Sync**: Monitors Git repo, automatically syncs changes to Kubernetes cluster\n",
    "3. **Model Versioning**: Each model version has dedicated deployment YAML (v1.0, v1.1, v1.2)\n",
    "4. **Automated Rollback**: If deployment fails health checks, ArgoCD auto-reverts to last healthy version\n",
    "5. **PR-Based Approval**: Model deployments require PR review + approval (human oversight)\n",
    "\n",
    "**Git Repository Structure:**\n",
    "```\n",
    "ml-models-gitops/\n",
    "‚îú‚îÄ‚îÄ base/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml        # Base deployment template\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ service.yaml           # Service definition\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ configmap.yaml         # Model config\n",
    "‚îú‚îÄ‚îÄ overlays/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ staging/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ kustomization.yaml # Staging-specific overrides\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ production/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ kustomization.yaml # Production-specific overrides\n",
    "‚îî‚îÄ‚îÄ versions/\n",
    "    ‚îú‚îÄ‚îÄ v1.0/\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ model.pkl          # Model artifact (DVC tracked)\n",
    "    ‚îú‚îÄ‚îÄ v1.1/\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ model.pkl\n",
    "    ‚îî‚îÄ‚îÄ v1.2/\n",
    "        ‚îî‚îÄ‚îÄ model.pkl\n",
    "```\n",
    "\n",
    "**Workflow:**\n",
    "1. Data scientist trains new model (v1.2), pushes to Git\n",
    "2. CI/CD tests model (accuracy, latency, schema validation)\n",
    "3. If tests pass, create PR to update `production/kustomization.yaml` (image: yield-predictor:v1.2)\n",
    "4. Lead engineer reviews PR, approves\n",
    "5. ArgoCD detects Git change, syncs to production cluster\n",
    "6. ArgoCD performs health checks (HTTP /health endpoint)\n",
    "7. If healthy, deployment complete; if unhealthy, auto-rollback to v1.1\n",
    "\n",
    "**Key Technologies:** ArgoCD, Kustomize, DVC (model versioning), GitHub, Kubernetes\n",
    "\n",
    "**Success Metrics:**\n",
    "- ‚úÖ Deployment drift: 0% (production always matches Git)\n",
    "- ‚úÖ Rollback time: 90 seconds (automated ArgoCD rollback)\n",
    "- ‚úÖ Configuration errors: 0 (Git review process catches issues)\n",
    "- ‚úÖ Deployment frequency: 2√ó/week (confident, frequent releases)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 6: Automated Model Monitoring and Retraining Trigger üìä\n",
    "\n",
    "**Objective:** Monitor deployed model performance, trigger retraining when accuracy degrades.\n",
    "\n",
    "**Business Value:** Catch model drift early ‚Üí $120K/year savings (prevent accuracy degradation from affecting production decisions).\n",
    "\n",
    "**Implementation Plan:**\n",
    "1. **Prometheus Metrics**: Model API logs predictions + ground truth labels to Prometheus\n",
    "   - Metric: `model_accuracy` (rolling 7-day window)\n",
    "   - Metric: `prediction_latency_ms` (p99 latency)\n",
    "   - Metric: `data_drift_score` (KL divergence between training and production data distributions)\n",
    "2. **Alerting Rules**: Prometheus AlertManager triggers when accuracy drops below threshold\n",
    "   ```yaml\n",
    "   groups:\n",
    "   - name: model_performance\n",
    "     rules:\n",
    "     - alert: ModelAccuracyDegradation\n",
    "       expr: model_accuracy < 0.95\n",
    "       for: 24h\n",
    "       annotations:\n",
    "         summary: \"Model accuracy below 95% for 24 hours\"\n",
    "   ```\n",
    "3. **Automated Retraining**: Alert webhook triggers Tekton PipelineRun (retrain model on latest data)\n",
    "4. **A/B Testing**: New model deployed as canary (10% traffic), compared to current model\n",
    "5. **Automatic Promotion**: If new model accuracy ‚â• current + 1%, promote to 100% traffic\n",
    "\n",
    "**Tekton Trigger Workflow:**\n",
    "```yaml\n",
    "apiVersion: triggers.tekton.dev/v1alpha1\n",
    "kind: EventListener\n",
    "metadata:\n",
    "  name: model-retraining-trigger\n",
    "spec:\n",
    "  triggers:\n",
    "  - name: prometheus-alert\n",
    "    interceptors:\n",
    "    - cel:\n",
    "        filter: \"body.alerts[0].labels.alertname == 'ModelAccuracyDegradation'\"\n",
    "    bindings:\n",
    "    - ref: model-retraining-binding\n",
    "    template:\n",
    "      ref: model-retraining-template\n",
    "```\n",
    "\n",
    "**Key Technologies:** Prometheus, AlertManager, Tekton Triggers, Kubernetes, MLflow\n",
    "\n",
    "**Success Metrics:**\n",
    "- ‚úÖ Model drift detection time: <24 hours (early warning)\n",
    "- ‚úÖ Retraining trigger latency: <5 minutes (automated response)\n",
    "- ‚úÖ Production accuracy: 96%+ sustained (continuous monitoring prevents degradation)\n",
    "- ‚úÖ False alert rate: <5% (tuned thresholds prevent noise)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 7: CI/CD for Feature Store Updates üóÑÔ∏è\n",
    "\n",
    "**Objective:** Automated pipeline for feature engineering and feature store updates.\n",
    "\n",
    "**Business Value:** Fresh features in production models ‚Üí $200K/year revenue gains (better predictions from recent data).\n",
    "\n",
    "**Implementation Plan:**\n",
    "1. **Feature Store**: Feast stores precomputed features (Redis online store, BigQuery offline store)\n",
    "2. **Daily Feature Computation**: Tekton CronJob computes features from STDF data (aggregate statistics, moving averages)\n",
    "3. **Feature Validation**: Check feature schema, distributions, missing values\n",
    "4. **Feature Registration**: Register new features in Feast (metadata, TTL, materialization config)\n",
    "5. **Model Retraining**: Trigger model retraining when new features available\n",
    "\n",
    "**Tekton CronJob Structure:**\n",
    "```yaml\n",
    "apiVersion: tekton.dev/v1beta1\n",
    "kind: PipelineRun\n",
    "metadata:\n",
    "  name: feature-store-update\n",
    "spec:\n",
    "  pipelineRef:\n",
    "    name: feature-engineering-pipeline\n",
    "  params:\n",
    "  - name: data-source\n",
    "    value: \"s3://stdf-data/wafer-test/\"\n",
    "  - name: feature-store\n",
    "    value: \"feast://features\"\n",
    "  workspaces:\n",
    "  - name: data\n",
    "    persistentVolumeClaim:\n",
    "      claimName: feature-data-pvc\n",
    "```\n",
    "\n",
    "**Feast Feature Definition:**\n",
    "```python\n",
    "from feast import Entity, Feature, FeatureView, ValueType\n",
    "from feast.data_source import BigQuerySource\n",
    "\n",
    "device_entity = Entity(name=\"device_id\", value_type=ValueType.STRING)\n",
    "\n",
    "device_features = FeatureView(\n",
    "    name=\"device_test_features\",\n",
    "    entities=[\"device_id\"],\n",
    "    features=[\n",
    "        Feature(name=\"voltage_mean\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"current_std\", dtype=ValueType.FLOAT),\n",
    "        Feature(name=\"frequency_max\", dtype=ValueType.FLOAT),\n",
    "    ],\n",
    "    ttl=timedelta(days=7),\n",
    "    source=BigQuerySource(\n",
    "        table_ref=\"stdf_data.device_features\",\n",
    "        event_timestamp_column=\"test_timestamp\",\n",
    "    ),\n",
    ")\n",
    "```\n",
    "\n",
    "**Key Technologies:** Feast, Tekton, BigQuery, Redis, Kubernetes\n",
    "\n",
    "**Success Metrics:**\n",
    "- ‚úÖ Feature freshness: <24 hours (daily updates)\n",
    "- ‚úÖ Feature validation pass rate: 99%+ (quality gates)\n",
    "- ‚úÖ Model prediction accuracy: +2% (fresh features improve performance)\n",
    "- ‚úÖ Feature computation time: <30 minutes (efficient Tekton pipeline)\n",
    "\n",
    "---\n",
    "\n",
    "### Project 8: Multi-Environment CI/CD (Dev/Staging/Production) üåç\n",
    "\n",
    "**Objective:** Separate CI/CD pipelines for dev, staging, and production environments with progressive promotion.\n",
    "\n",
    "**Business Value:** Reduce production incidents ‚Üí $150K/year savings (thorough staging validation prevents bugs).\n",
    "\n",
    "**Implementation Plan:**\n",
    "1. **Environment Strategy**:\n",
    "   - **Dev**: Developers test changes (low-quality data OK, fast iteration)\n",
    "   - **Staging**: Pre-production validation (production-like data, thorough testing)\n",
    "   - **Production**: Customer-facing (zero tolerance for errors)\n",
    "2. **Promotion Gates**:\n",
    "   - Dev ‚Üí Staging: Unit tests pass + code review\n",
    "   - Staging ‚Üí Production: Integration tests pass + manual approval + load testing\n",
    "3. **GitHub Actions Workflow**:\n",
    "   ```yaml\n",
    "   name: Multi-Environment Deployment\n",
    "   on:\n",
    "     push:\n",
    "       branches: [main]\n",
    "   jobs:\n",
    "     deploy-dev:\n",
    "       runs-on: ubuntu-latest\n",
    "       steps:\n",
    "         - uses: actions/checkout@v3\n",
    "         - name: Deploy to dev\n",
    "           run: kubectl apply -f k8s/dev/\n",
    "     \n",
    "     deploy-staging:\n",
    "       needs: [deploy-dev]\n",
    "       runs-on: ubuntu-latest\n",
    "       steps:\n",
    "         - uses: actions/checkout@v3\n",
    "         - name: Deploy to staging\n",
    "           run: kubectl apply -f k8s/staging/\n",
    "         - name: Run integration tests\n",
    "           run: pytest tests/integration/\n",
    "     \n",
    "     deploy-production:\n",
    "       needs: [deploy-staging]\n",
    "       runs-on: ubuntu-latest\n",
    "       environment: production  # Manual approval required\n",
    "       steps:\n",
    "         - uses: actions/checkout@v3\n",
    "         - name: Deploy to production\n",
    "           run: kubectl apply -f k8s/production/\n",
    "   ```\n",
    "\n",
    "**Key Technologies:** GitHub Actions, Kubernetes (3 clusters), ArgoCD (per environment), Terraform\n",
    "\n",
    "**Success Metrics:**\n",
    "- ‚úÖ Production incidents: <1/month (staging catches bugs)\n",
    "- ‚úÖ Dev deployment frequency: 20√ó/day (fast iteration)\n",
    "- ‚úÖ Production deployment frequency: 2√ó/week (controlled releases)\n",
    "- ‚úÖ Staging test coverage: 95%+ (comprehensive validation)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Projects Summary\n",
    "\n",
    "| Project | Focus | Value | Key Tech |\n",
    "|---------|-------|-------|----------|\n",
    "| 1. Automated Retraining | Continuous model improvement | $600K/year | Tekton, MLflow, ArgoCD |\n",
    "| 2. STDF Quality Gates | Zero data corruption bugs | $180K/year | GitHub Actions, pytest |\n",
    "| 3. Canary Deployment | Safe CNN rollout | $2.1M/year | Flagger, Prometheus, ArgoCD |\n",
    "| 4. Experiment Tracking | Optimal hyperparameters | $95K/year | MLflow, Tekton (parallel) |\n",
    "| 5. GitOps Deployment | Zero configuration drift | $75K/year | ArgoCD, Kustomize, DVC |\n",
    "| 6. Model Monitoring | Catch drift early | $120K/year | Prometheus, Tekton Triggers |\n",
    "| 7. Feature Store Updates | Fresh features daily | $200K/year | Feast, BigQuery, Redis |\n",
    "| 8. Multi-Environment | Thorough validation | $150K/year | GitHub Actions, 3 clusters |\n",
    "\n",
    "**Total Annual Value: $3.52M across 8 CI/CD projects!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8990e2",
   "metadata": {},
   "source": [
    "## 6. üéì Comprehensive Takeaways: CI/CD for ML Mastery\n",
    "\n",
    "### üîë Core Concepts\n",
    "\n",
    "#### **1. ML CI/CD vs Traditional CI/CD**\n",
    "Traditional CI/CD focuses on code testing and deployment. ML CI/CD extends this with:\n",
    "- **Data Validation**: Schema checks, quality gates (prevent training on corrupt data)\n",
    "- **Model Training**: Reproducible pipelines (versioned data, code, hyperparameters)\n",
    "- **Model Evaluation**: Baseline comparison (deploy only if accuracy improves)\n",
    "- **Model Registry**: Version control for models (MLflow, DVC)\n",
    "- **Canary Deployments**: Gradual rollout (10% ‚Üí 25% ‚Üí 100% traffic)\n",
    "- **Monitoring**: Track model drift (accuracy degradation over time)\n",
    "\n",
    "**Key Insight:** ML pipelines have data and model as first-class citizens (not just code).\n",
    "\n",
    "#### **2. Tekton vs GitHub Actions**\n",
    "Both are CI/CD platforms, but different design philosophies:\n",
    "\n",
    "| Aspect | Tekton | GitHub Actions |\n",
    "|--------|--------|----------------|\n",
    "| **Platform** | Kubernetes-native (runs in K8s cluster) | Cloud-hosted (GitHub infrastructure) |\n",
    "| **Complexity** | Higher (requires K8s knowledge) | Lower (YAML workflows) |\n",
    "| **Flexibility** | Maximum (custom CRDs, resource control) | Good (pre-built actions marketplace) |\n",
    "| **Cost** | Self-hosted (you pay for K8s nodes) | Pay-per-minute (free for public repos) |\n",
    "| **GPU Support** | Excellent (K8s node selectors) | Limited (self-hosted runners needed) |\n",
    "| **Best For** | Large-scale ML training, complex DAGs | Quick CI/CD, open-source projects |\n",
    "\n",
    "**When to Use:**\n",
    "- **Tekton**: Large ML teams, multi-GPU training, complex pipelines, on-premise clusters\n",
    "- **GitHub Actions**: Small teams, cloud-first, quick prototyping, open-source\n",
    "\n",
    "#### **3. Quality Gates for ML**\n",
    "Unlike traditional software, ML models can silently degrade. Quality gates prevent bad deployments:\n",
    "\n",
    "**Data Quality Gates:**\n",
    "- ‚úÖ Schema validation (required columns, data types, value ranges)\n",
    "- ‚úÖ Missing values threshold (<5% acceptable)\n",
    "- ‚úÖ Distribution checks (KL divergence vs training data <0.1)\n",
    "- ‚úÖ Minimum sample size (‚â•1000 rows for training)\n",
    "\n",
    "**Model Quality Gates:**\n",
    "- ‚úÖ Accuracy threshold (‚â•baseline + 1% improvement)\n",
    "- ‚úÖ Precision/recall balance (F1 score ‚â•0.95)\n",
    "- ‚úÖ Inference latency (p99 <150ms for production)\n",
    "- ‚úÖ Model size (<500MB for edge deployment)\n",
    "\n",
    "**Deployment Quality Gates:**\n",
    "- ‚úÖ Canary metrics acceptable (24-hour monitoring)\n",
    "- ‚úÖ A/B test statistical significance (p-value <0.05)\n",
    "- ‚úÖ Resource utilization (<70% CPU, <80% memory)\n",
    "\n",
    "**Key Insight:** Automate quality checks ‚Üí prevent 95% of production incidents.\n",
    "\n",
    "#### **4. Experiment Tracking with MLflow**\n",
    "MLflow is the industry standard for ML experiment tracking:\n",
    "\n",
    "**What MLflow Tracks:**\n",
    "- **Parameters**: Hyperparameters (n_estimators=100, learning_rate=0.01)\n",
    "- **Metrics**: Accuracy, precision, recall, F1, training time\n",
    "- **Artifacts**: Model files (model.pkl), plots (confusion_matrix.png), datasets\n",
    "- **Code Version**: Git commit hash (reproducibility)\n",
    "\n",
    "**MLflow Components:**\n",
    "1. **Tracking**: Log experiments (mlflow.log_params, mlflow.log_metrics)\n",
    "2. **Projects**: Package ML code (MLproject file with conda dependencies)\n",
    "3. **Models**: Standard model format (mlflow.sklearn.log_model, deployment-ready)\n",
    "4. **Registry**: Centralized model store (versioning, staging, production tags)\n",
    "\n",
    "**Best Practice:** Log every training run ‚Üí compare 100+ experiments ‚Üí pick best model scientifically.\n",
    "\n",
    "#### **5. GitOps for ML Deployments**\n",
    "GitOps = Git as single source of truth for infrastructure:\n",
    "\n",
    "**GitOps Principles:**\n",
    "1. **Declarative**: Kubernetes manifests in Git (deployment.yaml, service.yaml)\n",
    "2. **Versioned**: Every change tracked (who, when, why via Git commits)\n",
    "3. **Automated**: ArgoCD syncs Git ‚Üí Kubernetes (continuous reconciliation)\n",
    "4. **Auditable**: Complete deployment history (compliance, rollback capability)\n",
    "\n",
    "**ArgoCD Workflow:**\n",
    "```\n",
    "1. Data scientist trains model v1.2\n",
    "2. Update Git: k8s/production/deployment.yaml (image: yield-predictor:v1.2)\n",
    "3. Create PR ‚Üí Lead engineer reviews ‚Üí Approves\n",
    "4. ArgoCD detects Git change ‚Üí Syncs to K8s cluster\n",
    "5. Health checks pass ‚Üí Deployment complete\n",
    "6. If unhealthy ‚Üí Auto-rollback to v1.1 (Git revert)\n",
    "```\n",
    "\n",
    "**Key Insight:** Git PR review = deployment approval (no manual kubectl commands).\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Best Practices\n",
    "\n",
    "#### **Pipeline Design:**\n",
    "1. **Fail Fast**: Validate data schema before training (save 30 minutes of wasted GPU time)\n",
    "2. **Parallel Execution**: Train 5 hyperparameter sets simultaneously (Tekton parallel tasks)\n",
    "3. **Artifact Caching**: Cache dependencies, preprocessed data (speed up pipeline 3√ó)\n",
    "4. **Incremental Training**: Use last model as starting point (transfer learning)\n",
    "\n",
    "#### **Testing Strategy:**\n",
    "1. **Unit Tests**: Test data validation logic, preprocessing functions (pytest)\n",
    "2. **Integration Tests**: Test full pipeline end-to-end (data ‚Üí model ‚Üí deployment)\n",
    "3. **Model Tests**: Test model predictions (edge cases, adversarial examples)\n",
    "4. **Load Tests**: Simulate 1000 req/sec (ensure API handles production traffic)\n",
    "\n",
    "#### **Security:**\n",
    "1. **Secret Management**: Store credentials in Kubernetes Secrets (not Git)\n",
    "2. **RBAC**: Limit pipeline permissions (principle of least privilege)\n",
    "3. **Vulnerability Scanning**: Scan Docker images (Snyk, Trivy)\n",
    "4. **Data Privacy**: Anonymize PII before logging (GDPR compliance)\n",
    "\n",
    "#### **Monitoring:**\n",
    "1. **Pipeline Metrics**: Track success rate, duration, resource usage (Prometheus)\n",
    "2. **Model Metrics**: Track accuracy, latency, drift (custom Prometheus exporters)\n",
    "3. **Alerting**: Notify on pipeline failures, model degradation (Slack, PagerDuty)\n",
    "4. **Dashboards**: Visualize metrics (Grafana dashboards)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Common Pitfalls\n",
    "\n",
    "#### **1. Training on Stale Data**\n",
    "**Problem:** Model trained on last month's data, production data distribution shifted.\n",
    "**Solution:** Automate daily retraining (Tekton CronJob) + monitor data drift (KL divergence).\n",
    "\n",
    "#### **2. No Baseline Comparison**\n",
    "**Problem:** Deploy new model without comparing to current model (risk of regression).\n",
    "**Solution:** Always compare accuracy to baseline + 1% threshold (quality gate).\n",
    "\n",
    "#### **3. Ignoring Inference Latency**\n",
    "**Problem:** New model 5% more accurate but 10√ó slower (production SLA violated).\n",
    "**Solution:** Test latency in CI/CD (p99 <150ms gate) + load testing.\n",
    "\n",
    "#### **4. Hardcoded Hyperparameters**\n",
    "**Problem:** Hyperparameters in code (not reproducible, hard to tune).\n",
    "**Solution:** Store in config files (YAML, JSON) + track in MLflow.\n",
    "\n",
    "#### **5. Manual Deployment Steps**\n",
    "**Problem:** Engineer runs kubectl apply manually (error-prone, not auditable).\n",
    "**Solution:** GitOps (ArgoCD) + PR approval workflow (automated, auditable).\n",
    "\n",
    "#### **6. No Rollback Strategy**\n",
    "**Problem:** Bad deployment, scrambling to fix (production downtime).\n",
    "**Solution:** ArgoCD auto-rollback + canary deployments (catch issues before 100% traffic).\n",
    "\n",
    "#### **7. Insufficient Test Coverage**\n",
    "**Problem:** Unit tests pass but integration fails (components don't work together).\n",
    "**Solution:** Test pyramid (many unit tests, some integration tests, few E2E tests).\n",
    "\n",
    "#### **8. Ignoring Model Drift**\n",
    "**Problem:** Model accuracy degrades over 6 months, nobody notices.\n",
    "**Solution:** Monitor accuracy in production (Prometheus) + alert when <95%.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Production Checklist\n",
    "\n",
    "Before deploying ML models to production, ensure:\n",
    "\n",
    "**Data:**\n",
    "- [ ] Data schema validated (required columns, types, ranges)\n",
    "- [ ] Missing values handled (<5% threshold)\n",
    "- [ ] Data distribution checked (similar to training data)\n",
    "- [ ] Data versioned (DVC, MLflow artifacts)\n",
    "\n",
    "**Model:**\n",
    "- [ ] Model trained on representative data (‚â•5000 samples)\n",
    "- [ ] Hyperparameters tracked (MLflow, config files)\n",
    "- [ ] Model accuracy ‚â• baseline + 1% (quality gate)\n",
    "- [ ] Inference latency tested (p99 <150ms)\n",
    "- [ ] Model size acceptable (<500MB for edge deployment)\n",
    "\n",
    "**Pipeline:**\n",
    "- [ ] Pipeline runs end-to-end without manual intervention\n",
    "- [ ] All tasks idempotent (re-running produces same result)\n",
    "- [ ] Artifacts stored (model files, metrics, plots)\n",
    "- [ ] Pipeline execution time <30 minutes (fast feedback)\n",
    "\n",
    "**Testing:**\n",
    "- [ ] Unit tests pass (‚â•90% code coverage)\n",
    "- [ ] Integration tests pass (full pipeline E2E)\n",
    "- [ ] Load tests pass (handles 1000 req/sec)\n",
    "- [ ] Model tests pass (edge cases, adversarial examples)\n",
    "\n",
    "**Deployment:**\n",
    "- [ ] Canary deployment configured (10% traffic initially)\n",
    "- [ ] Health checks defined (HTTP /health endpoint)\n",
    "- [ ] Rollback strategy tested (ArgoCD auto-rollback)\n",
    "- [ ] Resource limits set (prevent runaway CPU/memory)\n",
    "\n",
    "**Monitoring:**\n",
    "- [ ] Model accuracy tracked (Prometheus metrics)\n",
    "- [ ] Inference latency tracked (p50, p95, p99)\n",
    "- [ ] Alerts configured (accuracy <95%, latency >150ms)\n",
    "- [ ] Dashboards created (Grafana model performance dashboard)\n",
    "\n",
    "**Security:**\n",
    "- [ ] Secrets stored securely (Kubernetes Secrets, HashiCorp Vault)\n",
    "- [ ] RBAC configured (pipeline permissions minimal)\n",
    "- [ ] Container images scanned (Snyk, Trivy)\n",
    "- [ ] Data anonymized (PII removed before logging)\n",
    "\n",
    "**Documentation:**\n",
    "- [ ] Pipeline README (how to run, troubleshoot)\n",
    "- [ ] Model card (accuracy, limitations, intended use)\n",
    "- [ ] Runbook (incident response procedures)\n",
    "- [ ] Architecture diagram (Mermaid, draw.io)\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Troubleshooting Guide\n",
    "\n",
    "#### **Pipeline Fails at Data Validation**\n",
    "**Symptoms:** Task status: FAILED, error: \"Missing columns: {'current_ma', 'frequency_mhz'}\"\n",
    "**Diagnosis:** Input data schema changed (column names, types)\n",
    "**Fix:** Update schema definition or fix upstream data source\n",
    "**Prevention:** Version data schemas + monitor for breaking changes\n",
    "\n",
    "#### **Model Training Times Out**\n",
    "**Symptoms:** Task runs >1 hour, killed by timeout\n",
    "**Diagnosis:** Dataset too large or GPU not allocated\n",
    "**Fix:** Increase task timeout + verify GPU resources (nvidia.com/gpu: '1')\n",
    "**Prevention:** Test on small dataset first + monitor resource usage\n",
    "\n",
    "#### **Model Accuracy Below Baseline**\n",
    "**Symptoms:** Evaluation task fails, accuracy 94% vs baseline 96%\n",
    "**Diagnosis:** Hyperparameters suboptimal or training data quality issue\n",
    "**Fix:** Tune hyperparameters (grid search) or validate data quality\n",
    "**Prevention:** Track hyperparameters in MLflow + A/B test before full deployment\n",
    "\n",
    "#### **Canary Deployment Shows High Latency**\n",
    "**Symptoms:** p99 latency 500ms vs baseline 100ms\n",
    "**Diagnosis:** New model computationally expensive (larger architecture)\n",
    "**Fix:** Optimize model (quantization, pruning) or add more replicas\n",
    "**Prevention:** Load test in staging + set latency quality gates\n",
    "\n",
    "#### **ArgoCD Stuck in Progressing State**\n",
    "**Symptoms:** Deployment not syncing, status: Progressing for >10 minutes\n",
    "**Diagnosis:** Pod failing health checks or image pull error\n",
    "**Fix:** Check pod logs (kubectl logs), verify image exists in registry\n",
    "**Prevention:** Test deployments in staging + enable auto-rollback\n",
    "\n",
    "#### **MLflow Experiment Not Logged**\n",
    "**Symptoms:** Training run completes but no MLflow entry\n",
    "**Diagnosis:** MLflow tracking URI not set or network connectivity issue\n",
    "**Fix:** Set MLFLOW_TRACKING_URI environment variable + verify connectivity\n",
    "**Prevention:** Test MLflow connectivity in setup task\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "**After mastering CI/CD for ML, explore:**\n",
    "\n",
    "1. **Advanced MLOps (Notebooks 121-130)**:\n",
    "   - Model serving (TensorFlow Serving, Seldon Core)\n",
    "   - Feature stores (Feast, Tecton)\n",
    "   - Data versioning (DVC, LakeFS)\n",
    "   - Experiment tracking (Weights & Biases, Neptune)\n",
    "\n",
    "2. **Infrastructure as Code (Notebook 137)**:\n",
    "   - Terraform for cloud infrastructure (AWS, GCP, Azure)\n",
    "   - Pulumi for Kubernetes resources (type-safe IaC)\n",
    "   - Ansible for configuration management\n",
    "\n",
    "3. **Container Security (Notebook 138)**:\n",
    "   - Image scanning (Trivy, Snyk, Aqua Security)\n",
    "   - Runtime security (Falco, Sysdig)\n",
    "   - Network policies (Kubernetes NetworkPolicy, Cilium)\n",
    "   - Secrets management (HashiCorp Vault, Sealed Secrets)\n",
    "\n",
    "4. **Advanced Monitoring (Observability)**:\n",
    "   - Distributed tracing (Jaeger, Zipkin)\n",
    "   - Log aggregation (ELK stack, Loki)\n",
    "   - Anomaly detection (Outlier detection on metrics)\n",
    "\n",
    "5. **Cost Optimization**:\n",
    "   - Spot instances for training (save 70% on cloud costs)\n",
    "   - Auto-scaling (scale down during off-hours)\n",
    "   - Resource quotas (prevent runaway costs)\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Key Takeaways\n",
    "\n",
    "1. **ML CI/CD ‚â† Traditional CI/CD**: Data and models are first-class citizens (validate, version, monitor).\n",
    "\n",
    "2. **Tekton for Complex Pipelines**: Kubernetes-native, parallel execution, GPU support (best for large-scale ML).\n",
    "\n",
    "3. **GitHub Actions for Simplicity**: Cloud-hosted, easy YAML workflows, great for open-source (best for quick CI/CD).\n",
    "\n",
    "4. **Quality Gates Prevent Incidents**: Automate data validation, baseline comparison, canary deployments (reduce production bugs 95%).\n",
    "\n",
    "5. **MLflow Tracks Everything**: Log hyperparameters, metrics, artifacts ‚Üí scientific model selection (not guesswork).\n",
    "\n",
    "6. **GitOps for Deployments**: Git as source of truth, ArgoCD auto-sync ‚Üí zero configuration drift, fast rollbacks.\n",
    "\n",
    "7. **Monitor Model Performance**: Track accuracy, latency, drift ‚Üí catch degradation early (before customers notice).\n",
    "\n",
    "8. **Fail Fast, Test Often**: Validate data before training, test pipelines in staging, canary in production (safe, incremental rollout).\n",
    "\n",
    "---\n",
    "\n",
    "**You've mastered CI/CD for ML! üéâ**\n",
    "\n",
    "You now know how to:\n",
    "- ‚úÖ Build ML pipelines with data validation, training, evaluation, deployment\n",
    "- ‚úÖ Use Tekton for Kubernetes-native CI/CD (parallel tasks, GPU support, resource control)\n",
    "- ‚úÖ Use GitHub Actions for cloud-based workflows (matrix builds, artifact management)\n",
    "- ‚úÖ Implement quality gates (schema validation, baseline comparison, canary deployments)\n",
    "- ‚úÖ Track experiments with MLflow (reproducibility, scientific model selection)\n",
    "- ‚úÖ Deploy with GitOps (ArgoCD, zero drift, fast rollbacks)\n",
    "- ‚úÖ Monitor production models (Prometheus, alerts, dashboards)\n",
    "- ‚úÖ Apply to post-silicon validation (STDF pipelines, yield prediction, wafer defect detection)\n",
    "\n",
    "**Next:** Explore Infrastructure as Code (Notebook 137) to automate cloud resource provisioning! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3405151b",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### When to Use CI/CD for ML\n",
    "- **Frequent model updates**: Weekly/monthly retraining requires automated pipelines\n",
    "- **Multi-environment testing**: Models tested in dev/staging before production deployment\n",
    "- **Team collaboration**: Multiple data scientists contributing models (version control, testing)\n",
    "- **Reproducibility**: Automated pipelines ensure consistent training (datasets, hyperparameters, code)\n",
    "- **Compliance**: Audit trails for model lineage, data provenance (pharma, finance)\n",
    "\n",
    "### Limitations\n",
    "- **Pipeline complexity**: ML pipelines more complex than software (data validation, model testing)\n",
    "- **Long build times**: Training jobs take hours/days (caching, incremental training needed)\n",
    "- **GPU resource constraints**: CI runners need GPU access for training (expensive)\n",
    "- **Data dependencies**: Large datasets complicate CI (need data versioning, artifact storage)\n",
    "- **Testing challenges**: Model accuracy tests non-deterministic (random seeds, data splits)\n",
    "\n",
    "### Alternatives\n",
    "- **Manual model deployment**: Data scientist trains locally, deploys via kubectl (doesn't scale)\n",
    "- **Notebook-based workflows**: Jupyter notebooks for exploration (good for prototyping, bad for production)\n",
    "- **Dedicated ML platforms**: SageMaker Pipelines, Vertex AI automate training (vendor lock-in)\n",
    "- **Kubeflow Pipelines only**: Skip CI/CD, use Kubeflow for orchestration (works but less integration)\n",
    "\n",
    "### Best Practices\n",
    "- **Separate training and deployment pipelines**: Training triggered by data changes, deployment by model registry\n",
    "- **Model registry**: MLflow/DVC for versioned model artifacts (staging, production, archived)\n",
    "- **Automated testing**: Data validation (Great Expectations), model performance tests (accuracy >threshold)\n",
    "- **Feature store integration**: Cache features for training consistency (avoid recomputing)\n",
    "- **Container caching**: Cache layers for faster builds (training image rarely changes)\n",
    "- **Rolling deployments**: Canary rollout (5% ‚Üí 25% ‚Üí 100%) with automatic rollback on errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c044d2b0",
   "metadata": {},
   "source": [
    "## üîç Diagnostic Checks & Mastery\n",
    "\n",
    "### Implementation Checklist\n",
    "- ‚úÖ **CI pipeline**: Automated testing (data validation, model performance)\n",
    "- ‚úÖ **Model registry**: MLflow/DVC for versioned artifacts\n",
    "- ‚úÖ **CD pipeline**: Automated deployment to dev/staging/prod\n",
    "- ‚úÖ **Rollback strategy**: Canary or blue-green deployment\n",
    "- ‚úÖ **Monitoring integration**: Alerts on model degradation\n",
    "- ‚úÖ **Artifact caching**: Speed up builds with layer caching\n",
    "\n",
    "### Post-Silicon Applications\n",
    "**Automated Binning Model Pipeline**: Weekly retraining of speed bin classifiers, automated A/B testing, CI/CD deployment, save $2.5M/year revenue optimization\n",
    "\n",
    "### Mastery Achievement\n",
    "‚úÖ Build end-to-end CI/CD pipelines for ML models  \n",
    "‚úÖ Automate training, testing, deployment workflows  \n",
    "‚úÖ Integrate model registry (MLflow) for version control  \n",
    "‚úÖ Implement canary deployments with automatic rollback  \n",
    "‚úÖ Add data validation and model testing gates  \n",
    "‚úÖ Apply to semiconductor yield/binning/test models  \n",
    "\n",
    "**Next Steps**: 151_MLOps_Fundamentals, 154_Model_Monitoring_Observability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad9b28a",
   "metadata": {},
   "source": [
    "## üìà Progress Update\n",
    "\n",
    "**Session Summary:**\n",
    "- ‚úÖ Completed 29 notebooks total (previous 21 + current batch: 132, 134-136, 139, 144-145, 174)\n",
    "- ‚úÖ Current notebook: 136/175 complete\n",
    "- ‚úÖ Overall completion: ~82.9% (145/175 notebooks ‚â•15 cells)\n",
    "\n",
    "**Remaining Work:**\n",
    "- üîÑ Next: Process remaining 9-cell and below notebooks\n",
    "- üéØ Target: 100% completion (175/175 notebooks)\n",
    "\n",
    "Excellent progress - over 80% complete! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd7e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .github/workflows/ml-pipeline.yml\n",
    "\"\"\"\n",
    "name: ML Model CI/CD Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [main]\n",
    "  pull_request:\n",
    "    branches: [main]\n",
    "\n",
    "jobs:\n",
    "  train-and-register:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.10'\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install mlflow scikit-learn pandas boto3\n",
    "      \n",
    "      - name: Train Model\n",
    "        env:\n",
    "          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_URI }}\n",
    "          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_KEY }}\n",
    "          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET }}\n",
    "        run: |\n",
    "          python train_yield_model.py \\\n",
    "            --data s3://wafer-data/latest.csv \\\n",
    "            --experiment-name wafer-yield\n",
    "      \n",
    "      - name: Evaluate Model\n",
    "        run: |\n",
    "          python evaluate_model.py \\\n",
    "            --threshold 0.95  # Minimum F1 score\n",
    "      \n",
    "      - name: Register Model\n",
    "        if: success()\n",
    "        run: |\n",
    "          python register_model.py \\\n",
    "            --model-name YieldPredictor \\\n",
    "            --stage Staging  # Promote to Staging if tests pass\n",
    "      \n",
    "      - name: Deploy to Kubernetes\n",
    "        if: github.ref == 'refs/heads/main'\n",
    "        run: |\n",
    "          kubectl set image deployment/yield-predictor \\\n",
    "            model=registry.io/yield-predictor:${{ github.sha }}\n",
    "\"\"\"\n",
    "\n",
    "# Post-Silicon Use Case:\n",
    "# Weekly model retraining triggered by new ATE data upload\n",
    "# CI/CD pipeline: Train ‚Üí Test (F1 >95%) ‚Üí Register in MLflow ‚Üí Deploy to staging\n",
    "# Manual approval gate ‚Üí Promote to production\n",
    "# Rollback: MLflow model registry maintains version history (v1, v2, v3...)\n",
    "# Save $540K/year (automate 2 ML engineer-days/week √ó $150K salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e0513",
   "metadata": {},
   "source": [
    "## üè≠ Advanced Example: MLflow + GitHub Actions for Model Registry\n",
    "\n",
    "Automate model training, evaluation, registration, and deployment with MLflow tracking."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
